2006.amta-papers.1,H01-1052,0,0.0121506,"ick rampup machine translation from languages with few machinetractable resources (online monolingual corpora, bilingual corpora, lexicons, and analyzers) into English. Development of ma chine translation systems requires such resources and one could arguably make the case that the quality of the resulting MT system is greatly de pendent on the quality and quantity of this lan guage data. For example, the quality of MT sys tems developed by statistical methods is dependent on the size and quality of the bilingual parallel corpus. This belief is supported by recent experi mental work by Banko and Brill (2001). In contrast to earlier studies (for example, Ratnaparkhi 1999 and Henderson 1999) Banko and Brill found that for statistical natural language processing it makes more sense to allocate resources to increase the size of the training corpus than it does to allocate Introduction In this paper we describe a set of processes for the acquisition of resources for quick rampup ma chine translation (MT) from any language lacking significant machine tractable resources into Eng lish. In previous work (Nirenburg et al. 1998) we developed an elicitation system that guides non expert language informa"
2006.amta-papers.1,C96-1017,0,0.027649,"Missing"
2006.amta-papers.1,J01-2001,0,0.0467324,"of the bridge language on Guarani vocabulary as well. In such cases, knowing the 7 If these precautions are taken, we believe that the bridgelanguage approach is likely to prove a fruitful method of language resource acquisition. 7 rules using a similar minimum edit distance ap proach. These rules, compatible with the Xerox Finite State Toolkit, can be refined by hand as needed. Finally, the system is evaluated by com paring it to a morphological rule set developed by unsupervised learning over a monolingual corpus. The unsupervised learning uses the Linguistica processor developed by John Goldsmith (2001). Using these evaluation results and proposed rules, the developer modifies the paradigm templates and rule sets. The application of this technique is the development of morphological analyzers for low density languages such as Amharic, Chechen, and Guarani. Doing morphology In previous sections we described the tools that are used to acquire monolingual text, parallel bilingual text, and lexical entries. In this section we describe the more complex task of constructing a morpho logical analyzer. In creating morphological analyz ers for machine translation, several development strategies ar"
2006.amta-papers.1,J01-1003,0,0.0446752,"Missing"
2006.amta-papers.1,W01-0711,0,0.0276769,"ical analyzer. In creating morphological analyz ers for machine translation, several development strategies are available. The analyzer can be devel oped by hand coding finite state rules. This ap proach was used, for example, by Pretorius and Bosch for Zulu (2002), Beesley for Arabic (1996), and Maxwell (2003). Alternatively, the analyzer can be developed by eliciting paradigm templates from language experts and using supervised learn ing techniques on these templates creating finite state morphographemic rewrite rules. This ap proach was used by Kemal, Nirenburg, and McShane (2001) and Zajac (2001). (Both these approaches were used in the Boas system.) In our current project, we have been using a hybrid ap proach that combines these two methods along with unsupervised learning. The approach follows an iterative elicitbuildtest methodology. Initially, paradigm templates are developed using informa tion from any reference grammars that may exist, and information from our language experts. Next, an initial system is built using supervised learning techniques applied to paradigm templates. This supervised approach works as follows. First, using the citation form and paradigm, we determi"
2013.iwslt-evaluation.8,P07-2045,0,0.0254044,"ibe a specialized normalization scheme for evaluating Arabic output, which was adopted for the IWSLT’2013 evaluation campaign. 1. Introduction We describe the Arabic-English and English-Arabic statistical machine translation (SMT) systems developed by the Qatar Computing Research Institute (QCRI) for the 2013 open evaluation campaign on spoken language translation organized in conjunction with the International Workshop on Spoken Language Translation (IWSLT). Below we give an overview of the settings we experimented with: • Decoders: We used a phrase-based SMT (PBSMT), as implemented in Moses [1], and two hierarchical decoders: Jane [2] and cdec [3]. See Section 6 for details. • Decoder settings: There are a variety of settings available for the above decoders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by"
2013.iwslt-evaluation.8,W10-1738,0,0.130794,"r evaluating Arabic output, which was adopted for the IWSLT’2013 evaluation campaign. 1. Introduction We describe the Arabic-English and English-Arabic statistical machine translation (SMT) systems developed by the Qatar Computing Research Institute (QCRI) for the 2013 open evaluation campaign on spoken language translation organized in conjunction with the International Workshop on Spoken Language Translation (IWSLT). Below we give an overview of the settings we experimented with: • Decoders: We used a phrase-based SMT (PBSMT), as implemented in Moses [1], and two hierarchical decoders: Jane [2] and cdec [3]. See Section 6 for details. • Decoder settings: There are a variety of settings available for the above decoders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, a"
2013.iwslt-evaluation.8,P10-4002,0,0.0806549,"Arabic output, which was adopted for the IWSLT’2013 evaluation campaign. 1. Introduction We describe the Arabic-English and English-Arabic statistical machine translation (SMT) systems developed by the Qatar Computing Research Institute (QCRI) for the 2013 open evaluation campaign on spoken language translation organized in conjunction with the International Workshop on Spoken Language Translation (IWSLT). Below we give an overview of the settings we experimented with: • Decoders: We used a phrase-based SMT (PBSMT), as implemented in Moses [1], and two hierarchical decoders: Jane [2] and cdec [3]. See Section 6 for details. • Decoder settings: There are a variety of settings available for the above decoders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, articles, etc."
2013.iwslt-evaluation.8,N06-2013,0,0.10965,"coders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, articles, etc. We experimented with standard segmentation schemes such as D0, D1, D2, D3, S2 and ATB, as defined in MADA [4, 5]. See Section 5 for details. • Domain adaptation: We experimented with three domain adaptation methods to make better use of the huge UN data, which is out-of-domain: (i) Modified Moore-Lewis filtering, (ii) phrase table merging, and (iii) phrase table backoff. See Section 7 for details. For our final submission, we synthesized a translation by combining the output of our best individual system with the output of other systems that are both relatively strong and can contribute to having more diversity, e.g., using a different decoder or a different segmentation scheme. We achieved the most not"
2013.iwslt-evaluation.8,P08-2039,0,0.073174,"coders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, articles, etc. We experimented with standard segmentation schemes such as D0, D1, D2, D3, S2 and ATB, as defined in MADA [4, 5]. See Section 5 for details. • Domain adaptation: We experimented with three domain adaptation methods to make better use of the huge UN data, which is out-of-domain: (i) Modified Moore-Lewis filtering, (ii) phrase table merging, and (iii) phrase table backoff. See Section 7 for details. For our final submission, we synthesized a translation by combining the output of our best individual system with the output of other systems that are both relatively strong and can contribute to having more diversity, e.g., using a different decoder or a different segmentation scheme. We achieved the most not"
2013.iwslt-evaluation.8,P12-1016,0,0.268259,"lish BLEU 1-TER System English IWSLT mono 109 English-French SETimes UN (Es-En + En-Fr) UN (Ar-En) News Crawl 2007-2009 News Crawl 2009-2012 Common Crawl Wiki Headlines Europarl v.7 News Commentary v.8 Gigaword v.5 2.7M 575M 4.2M 597M 115M 643M 745M 185M 1.1M 54M 5.3M 4,032M Arabic IWSLT mono UN News Commentary Arabic v.8 Gigaword Arabic v.5 2.7M 134M 4.8M 1,373M Table 1: Admissible training data for language modeling. Here English is tokenized, and Arabic is ATB-segmented. Preprocessing. We segmented the Arabic side of the bitext following the ATB scheme and using the Stanford word segmenter [6]. For the English side, we used the standard tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We"
2013.iwslt-evaluation.8,J93-2003,0,0.0246271,"abic IWSLT mono UN News Commentary Arabic v.8 Gigaword Arabic v.5 2.7M 134M 4.8M 1,373M Table 1: Admissible training data for language modeling. Here English is tokenized, and Arabic is ATB-segmented. Preprocessing. We segmented the Arabic side of the bitext following the ATB scheme and using the Stanford word segmenter [6]. For the English side, we used the standard tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard"
2013.iwslt-evaluation.8,N03-1017,0,0.0173015,"M 134M 4.8M 1,373M Table 1: Admissible training data for language modeling. Here English is tokenized, and Arabic is ATB-segmented. Preprocessing. We segmented the Arabic side of the bitext following the ATB scheme and using the Stanford word segmenter [6]. For the English side, we used the standard tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, th"
2013.iwslt-evaluation.8,2005.iwslt-1.8,0,0.0225691,"rd tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. De"
2013.iwslt-evaluation.8,W11-2123,0,0.408919,"sing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. Decoding. On tuning and testing, we used monotone-atpunctuation. On"
2013.iwslt-evaluation.8,P02-1040,0,0.0879883,", thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. Decoding. On tuning and testing, we used monotone-atpunctuation. On testing, we further used cube pruning. Table 2 shows the results3 for the baseline English-toArabic and Arabic-to-English SMT systems, compared to the baseline results reported on the WIT3 webpage. 3 For tst2010, we report MultEval BLEU and TER0.8: on tokenized and recased output for English, and on QCRI-normalized output for Arabic. For tst2011, tst2012, and tst2013, the organizers"
2013.iwslt-evaluation.8,D11-1125,0,0.032033,"each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. Decoding. On tuning and testing, we used monotone-atpunctuation. On testing, we further used cube pruning. Table 2 shows the results3 for the baseline English-toArabic and Arabic-to-English SMT systems, compared to the baseline results reported on the WIT3 webpage. 3 For tst2010, we report MultEval BLEU and TER0.8: on tokenized and recased output for English, and on QCRI-normalized output for Arabic. For tst2011, tst2012, and tst2013, the organizers used slightly different scorers. IWSL"
2013.iwslt-evaluation.8,C12-1121,1,0.915839,"ltEval BLEU and TER0.8: on tokenized and recased output for English, and on QCRI-normalized output for Arabic. For tst2011, tst2012, and tst2013, the organizers used slightly different scorers. IWSLT baseline Our baseline 23.6 24.7 English-Arabic BLEU 1-TER 43.0 45.6 11.9 12.6 28.6 29.1 Table 2: Our vs. IWSLT baseline results for English-toArabic and Arabic-to-English SMT, evaluated on tst2010. 4. System Settings Below we discuss the decoder settings and extensions we experimented with, focusing on Arabic-to-English. Table 3 shows the impact of each feature when added to the baseline. Tuning. [13] have shown that PRO tends to generate too short translations.4 They have suggested that the root of the problem was that PRO optimizes sentence-level BLEU+1, which smooths the precision component of BLEU, but leaves the brevity penalty intact, which destroys the balance between them. They have proposed a number of fixes, the simplest and most efficient among them being to smooth the brevity penalty as well.5 In our experiments, this yielded +0.2 BLEU for Arabic-to-English on tst2010. Operation sequence model. The operation sequence model (OSM) is an n-gram-based model, which represents the al"
2013.iwslt-evaluation.8,P13-2003,1,0.839319,"Missing"
2013.iwslt-evaluation.8,P13-2071,0,0.0200742,"of operations, e.g., generate a sequence of source and target words or perform reordering. The model memorizes Markov chains over such sequences, thus fusing lexical generation and reordering into a single generative model. OSM offers two advantages. First, it considers bilingual contextual information that goes beyond phrase boundaries. Second, it provides a better reordering mechanism that has richer conditioning than a lexicalized reordering model: the probability of an operation is conditioned on the n previous translation and reordering decisions. We used the Moses implementation of OSM [15], which has yielded improvements at WMT’13 [16]. In our experiments, it yielded +0.6 BLEU for Arabic-to-English on tst2010. Minimum Bayes risk decoding. We also experimented with minimum Bayes risk decoding (MBR)[17], which, instead of outputting the translation with the highest probability, prefers the one that is most similar to best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0"
2013.iwslt-evaluation.8,W13-2212,0,0.0142268,"urce and target words or perform reordering. The model memorizes Markov chains over such sequences, thus fusing lexical generation and reordering into a single generative model. OSM offers two advantages. First, it considers bilingual contextual information that goes beyond phrase boundaries. Second, it provides a better reordering mechanism that has richer conditioning than a lexicalized reordering model: the probability of an operation is conditioned on the n previous translation and reordering decisions. We used the Moses implementation of OSM [15], which has yielded improvements at WMT’13 [16]. In our experiments, it yielded +0.6 BLEU for Arabic-to-English on tst2010. Minimum Bayes risk decoding. We also experimented with minimum Bayes risk decoding (MBR)[17], which, instead of outputting the translation with the highest probability, prefers the one that is most similar to best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Trans"
2013.iwslt-evaluation.8,N04-1022,0,0.108744,"e model. OSM offers two advantages. First, it considers bilingual contextual information that goes beyond phrase boundaries. Second, it provides a better reordering mechanism that has richer conditioning than a lexicalized reordering model: the probability of an operation is conditioned on the n previous translation and reordering decisions. We used the Moses implementation of OSM [15], which has yielded improvements at WMT’13 [16]. In our experiments, it yielded +0.6 BLEU for Arabic-to-English on tst2010. Minimum Bayes risk decoding. We also experimented with minimum Bayes risk decoding (MBR)[17], which, instead of outputting the translation with the highest probability, prefers the one that is most similar to best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Transliterating OOVs. Out-of-vocabulary (OOV) words are problematic for languages with different scripts. Thus, we tried transliteration as post-processing: we extracted 1-1"
2013.iwslt-evaluation.8,P11-1044,1,0.871958,"o best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Transliterating OOVs. Out-of-vocabulary (OOV) words are problematic for languages with different scripts. Thus, we tried transliteration as post-processing: we extracted 1-1 word alignments from a subset of the UN bitext, and we used them to train a character-level transliteration system [18, 19] using Moses. As Table 3 shows this did not help, probably due to the small number of OOVs in tst2010. 4 See [14] for a discussion about more potential issues with PRO. --proargs=’--smooth-brevity-penalty’ 5 Available in Moses: System Baseline (B) OSM MBR Ttable 100 PRO-fix [13] TRANSLIT Drop UNK Arabic-English (tst2010) BLEU 1-TER 24.7 25.3 24.7 24.8 24.9 24.7 24.8 Arabic-English (tst2010) BLEU 1-TER System 45.6 46.1 45.7 45.6 44.7 45.6 45.7 SEG-D0 SEG-D1 SEG-D2 SEG-D3 SEG-S2 SEG-ATB 22.4 23.6 24.1 24.4 24.5 24.7 43.0 44.2 45.2 45.5 45.7 45.6 Table 5: Using different Arabic segmentation schem"
2013.iwslt-evaluation.8,P12-1049,1,0.860515,"o best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Transliterating OOVs. Out-of-vocabulary (OOV) words are problematic for languages with different scripts. Thus, we tried transliteration as post-processing: we extracted 1-1 word alignments from a subset of the UN bitext, and we used them to train a character-level transliteration system [18, 19] using Moses. As Table 3 shows this did not help, probably due to the small number of OOVs in tst2010. 4 See [14] for a discussion about more potential issues with PRO. --proargs=’--smooth-brevity-penalty’ 5 Available in Moses: System Baseline (B) OSM MBR Ttable 100 PRO-fix [13] TRANSLIT Drop UNK Arabic-English (tst2010) BLEU 1-TER 24.7 25.3 24.7 24.8 24.9 24.7 24.8 Arabic-English (tst2010) BLEU 1-TER System 45.6 46.1 45.7 45.6 44.7 45.6 45.7 SEG-D0 SEG-D1 SEG-D2 SEG-D3 SEG-S2 SEG-ATB 22.4 23.6 24.1 24.4 24.5 24.7 43.0 44.2 45.2 45.5 45.7 45.6 Table 5: Using different Arabic segmentation schem"
2013.iwslt-evaluation.8,P03-1021,0,0.0120716,"hierarchical cdec decoder [3]. We used its default features: forward and backward translation features, singleton features, a glue-rule probability, and a pass-through feature (to handle OOVs). We tuned the parameters using MIRA with IBM BLEU as the objective function and a k-best forest size of 250. Jane. We also used another hierarchical phrase-based decoder: Jane 2.2 [2]. We used the standard features: phrase translation probabilities and lexical smoothing in both directions, word and phrase penalties, a distance-based distortion model, and a 5-gram LM. We optimized the weights using MERT [21] on 100-best candidates with BLEU as objective. 5. Arabic Segmentation 7. Adaptation In Arabic, various clitics such as pronouns, conjunctions and articles appear concatenated to content words such as nouns and verbs. This can cause data sparseness issues, and thus clitics are typically segmented in a preprocessing step. There are various standard segmentation schemes defined in MADA [4, 5] such as D0, D1, D2, D3 and S2, for which we used the MADA+TOKAN toolkit [20], as well as ATB, which we performed using the Stanford segmenter [6]. Table 5 shows the results when training on the TED bitext o"
2013.iwslt-evaluation.8,P10-2041,0,0.0888063,"Missing"
2013.iwslt-evaluation.8,D11-1033,0,0.090854,"Missing"
2013.iwslt-evaluation.8,W08-0320,1,0.891463,"Missing"
2013.iwslt-evaluation.8,D09-1141,1,0.919233,"Missing"
2013.iwslt-evaluation.8,W09-0408,0,0.0608322,"Missing"
2013.iwslt-evaluation.8,W12-5611,0,0.0448251,"could build a strong LM through interpolation, similarly to our Arabic-to-English LM, that also used the Gigaword Arabic, UN, and News Commentary data (see Table 1). Desegmentation. Unlike the Arabic-to-English direction, where the segmentation was on the input side and thus the output was unaffected, here the segmentation had to be undone. For example, if we use an ATB-segmented target side, we end up with an ATB-segmented translation output, which we have to desegment in order to obtain proper Arabic. Desegmentation is not a trivial task since it involves some morphological adjustments, see [27] for a broader discussion. For desegmentation, we used the best approach described in [27]; in fact, we used their implementation. Normalization. Translating into Arabic is tricky because the Arabic spelling is often inconsistent in terms of punctuation (using both Arabic UTF8 and English punctuation symbols), digits (appearing as both Arabic and Indian characters), diacritics (can be used or omitted, and can often be wrong), spelling (there are many errors in the spelling of some Arabic characters, esp. Alef and Ta Marbuta; also, Waa appears sometimes separated). These problems are especially"
2013.iwslt-papers.2,2010.iwslt-evaluation.1,0,0.145712,"sed for this task, and also observe an absolute improvement of 1.6 BLEU when it is used in combination with TED data. Finally, we analyze some of the specific challenges when translating the educational content. 1. Introduction Lecture Translation has become an active field of research in the wider area of Speech Translation [1, 2]. This is demonstrated by large scale projects like the EU-funded translectures [3] and by evaluation campaigns like the one organized as part of the International Workshop on Spoken Language Translation (IWSLT), which introduced the challenge to translate TED talks [4] for the 2010 competition. However, the main limitation for the success of these projects continues to be the access to high quality training data. With the emergence of Massive Online Open Courses (MOOCs), thousands of video lectures have already been generated. Sites like Khan Academy1 , Coursera2 , Udacity3 , etc., continuously increase their repertoire of lectures, which range from basic math and science topics, to more advanced topics like machine learning, also covering history, economy, psychology, medicine, and more. Online education has bridged the geographical and financial gap, enab"
2013.iwslt-papers.2,federico-etal-2012-iwslt,0,0.0409166,"les are not available. It also can support volunteer translators, by providing an initial translation, which then can be post-edited [5]. Thus, SMT has the potential to increase the penetration of educational content, allowing it to reach a wider audience. To achieve this, an SMT system requires a large quantity of high-quality in-domain training data. Unfortunately, large data for machine translation has traditionally been constrained to domains such as legal documents, parliamentary proceedings and news. So far, the only openly accessible corpus for the lecture domain has been the TED talks [6]. In this paper, we introduce a new parallel corpus of subtitles of educational videos: the AMARA corpus for online educational content. We crawl a collection of multilingual community-generated subtitles6 . Furthermore, we explore the steps necessary to build corpora suitable for Machine Translation by processing the Arabic-English part of the multilingual collection. This yields a parallel corpus of about 2.6M Arabic and 3.9M English words. We explore different approaches to align the subtitles, and verify the quality of the generated parallel corpus by building translation models, and extri"
2013.iwslt-papers.2,2012.eamt-1.60,0,0.110653,"Loop (CHIL) [7], which consists of recordings and transcriptions of technical seminars and meetings in English. The content of the corpus includes a variety of topics: from audio and visual technologies to biology and finance. It is available through ELRA7 to its members. More recently, the IWSLT10 [4] evaluation campaign has turned its attention to the lecture and seminar domain by focusing on TED talks. To support this task, a collection of lecture translations has been automatically crawled from the TED website in a variety of languages and made publicly available through the WIT3 project [8]. In this paper, we used such data as a point of comparison. We crawl parallel subtitles of educational videos and use several measures to show the quality of the crawled corpus in comparison with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with r"
2013.iwslt-papers.2,tiedemann-2008-synchronizing,0,0.219263,"s attention to the lecture and seminar domain by focusing on TED talks. To support this task, a collection of lecture translations has been automatically crawled from the TED website in a variety of languages and made publicly available through the WIT3 project [8]. In this paper, we used such data as a point of comparison. We crawl parallel subtitles of educational videos and use several measures to show the quality of the crawled corpus in comparison with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with relative success in the past. For the NewsCommentary and OpenSrt corpora, [11] explore different ways to mix the phrase-table to adapt the Europarl corpus. For the Arabic-English IWSLT data, [12] achieve a relative improvement of 0.7 BLEU by mixing phrases from UN and IWSLT data using instance weighting with weights coming from the langu"
2013.iwslt-papers.2,P07-2045,0,0.0105846,"tional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on each available corpus (target side of a training bi-text or monolingual dataset) using KenLM [21]; we then interpolated these mod10 We els minimizing the perplexity on the target side of the tuning dataset (IWSLT dev-2010). Finally, we built a large joint log-linear model, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing of the system’s output. Tuning: We tuned the weights in the log-linear model by optimizing BLEU [22] on the tuning dataset, using PRO [23] with the fixed BLEU prosposed by [24]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000best lists for each iteration. Decoding: On tuning and testing, we used monotone-atpunctuation decoding (this had no impact on the translation length). On test"
2013.iwslt-papers.2,P12-1016,0,0.140562,"Missing"
2013.iwslt-papers.2,J93-2003,0,0.041392,"t on the training data 4.2. Experimental Setup Preprocessing: We tokenized the English side of all bi-texts as well as the monolingual data (GigaWord) for language modeling using the standard tokenizer of the Moses toolkit [17]. We further truecased this data by changing the casing of each sentence-initial word to its most frequent casing in the training corpus. For the Arabic side, we segmented the corpus following the ATB segmentation scheme with the Stanford word segmenter [18]. Training: We built separate directed word alignments for English→Arabic and for Arabic→English using IBM model 4 [19], and symmetrized them using grow-diag-final-and heuristic [20]. We extracted phrase pairs of maximum length seven. We scored these phrase pairs using maximum likelihood with Kneser-Ney smoothing,as implemented in the moses toolkit, thus obtaining a phrase table where each phrase-pair has the standard five translation model features. We also built a lexicalized reordering model : msdbidirectional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on each available corpus (target side of a training bi-text or monolingual dataset) using KenLM [21]; we then inter"
2013.iwslt-papers.2,tiedemann-2012-parallel,0,0.10406,"ite in a variety of languages and made publicly available through the WIT3 project [8]. In this paper, we used such data as a point of comparison. We crawl parallel subtitles of educational videos and use several measures to show the quality of the crawled corpus in comparison with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with relative success in the past. For the NewsCommentary and OpenSrt corpora, [11] explore different ways to mix the phrase-table to adapt the Europarl corpus. For the Arabic-English IWSLT data, [12] achieve a relative improvement of 0.7 BLEU by mixing phrases from UN and IWSLT data using instance weighting with weights coming from the language model perplexity. In this paper, we present the experimental results from data gathered from publicly available crowd-generated data, that has proved to be useful for the lectu"
2013.iwslt-papers.2,N03-1017,0,0.0523188,"e tokenized the English side of all bi-texts as well as the monolingual data (GigaWord) for language modeling using the standard tokenizer of the Moses toolkit [17]. We further truecased this data by changing the casing of each sentence-initial word to its most frequent casing in the training corpus. For the Arabic side, we segmented the corpus following the ATB segmentation scheme with the Stanford word segmenter [18]. Training: We built separate directed word alignments for English→Arabic and for Arabic→English using IBM model 4 [19], and symmetrized them using grow-diag-final-and heuristic [20]. We extracted phrase pairs of maximum length seven. We scored these phrase pairs using maximum likelihood with Kneser-Ney smoothing,as implemented in the moses toolkit, thus obtaining a phrase table where each phrase-pair has the standard five translation model features. We also built a lexicalized reordering model : msdbidirectional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on each available corpus (target side of a training bi-text or monolingual dataset) using KenLM [21]; we then interpolated these mod10 We els minimizing the perplexity on the tar"
2013.iwslt-papers.2,W12-3154,0,0.0338013,"allel subtitles of educational videos and use several measures to show the quality of the crawled corpus in comparison with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with relative success in the past. For the NewsCommentary and OpenSrt corpora, [11] explore different ways to mix the phrase-table to adapt the Europarl corpus. For the Arabic-English IWSLT data, [12] achieve a relative improvement of 0.7 BLEU by mixing phrases from UN and IWSLT data using instance weighting with weights coming from the language model perplexity. In this paper, we present the experimental results from data gathered from publicly available crowd-generated data, that has proved to be useful for the lecture domain, but that poses specific challenges, as it has a special focus on online education. 3. The AMARA Corpus Amara is a web-based platform for editing and"
2013.iwslt-papers.2,W11-2123,0,0.0748631,"ng IBM model 4 [19], and symmetrized them using grow-diag-final-and heuristic [20]. We extracted phrase pairs of maximum length seven. We scored these phrase pairs using maximum likelihood with Kneser-Ney smoothing,as implemented in the moses toolkit, thus obtaining a phrase table where each phrase-pair has the standard five translation model features. We also built a lexicalized reordering model : msdbidirectional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed LM model on each available corpus (target side of a training bi-text or monolingual dataset) using KenLM [21]; we then interpolated these mod10 We els minimizing the perplexity on the target side of the tuning dataset (IWSLT dev-2010). Finally, we built a large joint log-linear model, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing o"
2013.iwslt-papers.2,2012.iwslt-papers.7,0,0.0268125,"n with the closely related IWSLT data set. In the past, multilingual corpora creation from usercontributed movie subtitles has been addressed by [9]. Recently, a large collection of parallel movie subtitles from the Opensrt8 community along with tools for alignment of these has been made available through the Opus project [10]. Combination of corpora to improve the translation model has been explored with relative success in the past. For the NewsCommentary and OpenSrt corpora, [11] explore different ways to mix the phrase-table to adapt the Europarl corpus. For the Arabic-English IWSLT data, [12] achieve a relative improvement of 0.7 BLEU by mixing phrases from UN and IWSLT data using instance weighting with weights coming from the language model perplexity. In this paper, we present the experimental results from data gathered from publicly available crowd-generated data, that has proved to be useful for the lecture domain, but that poses specific challenges, as it has a special focus on online education. 3. The AMARA Corpus Amara is a web-based platform for editing and managing subtitles of online videos. It provides an easy-to-use interface, which allows users to collaboratively sub"
2013.iwslt-papers.2,P02-1040,0,0.091011,"uning dataset (IWSLT dev-2010). Finally, we built a large joint log-linear model, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing of the system’s output. Tuning: We tuned the weights in the log-linear model by optimizing BLEU [22] on the tuning dataset, using PRO [23] with the fixed BLEU prosposed by [24]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000best lists for each iteration. Decoding: On tuning and testing, we used monotone-atpunctuation decoding (this had no impact on the translation length). On testing, we further used cube pruning. did not use the second test set for the experiments in this paper. For the baseline system, we trained the phrase and the reordering models on the IWSLT training dataset. The language model was trained on the English side of the IWSLT training data. We"
2013.iwslt-papers.2,J93-1004,0,0.461534,"Missing"
2013.iwslt-papers.2,D11-1125,0,0.042418,"y, we built a large joint log-linear model, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing of the system’s output. Tuning: We tuned the weights in the log-linear model by optimizing BLEU [22] on the tuning dataset, using PRO [23] with the fixed BLEU prosposed by [24]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000best lists for each iteration. Decoding: On tuning and testing, we used monotone-atpunctuation decoding (this had no impact on the translation length). On testing, we further used cube pruning. did not use the second test set for the experiments in this paper. For the baseline system, we trained the phrase and the reordering models on the IWSLT training dataset. The language model was trained on the English side of the IWSLT training data. We tuned the weights on IWSLT-dev2010. B"
2013.iwslt-papers.2,C12-1121,1,0.888738,"odel, which used standard SMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. We used the phrase-based SMT model as implemented in the Moses toolkit [17] for translation, and reported evaluation results over two datasets. We reported BLEU calculated with respect of the original reference using NIST v13a, after detokenization and recasing of the system’s output. Tuning: We tuned the weights in the log-linear model by optimizing BLEU [22] on the tuning dataset, using PRO [23] with the fixed BLEU prosposed by [24]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000best lists for each iteration. Decoding: On tuning and testing, we used monotone-atpunctuation decoding (this had no impact on the translation length). On testing, we further used cube pruning. did not use the second test set for the experiments in this paper. For the baseline system, we trained the phrase and the reordering models on the IWSLT training dataset. The language model was trained on the English side of the IWSLT training data. We tuned the weights on IWSLT-dev2010. Below, we present the experimental resu"
2013.iwslt-papers.2,D09-1141,0,0.300772,"ARA only (T M1 ): Instead of using the IWSLT training data, we built the translation and reordering models using only the AMARA corpus. Concatenation (T M2 ): In this setting, we concatenated AMARA with IWSLT for training of the translation and reordering models. This generally improves word alignment, reduces OOV rate and improves translation quality if two corpora are from similar domain. However, if the added corpus is noisy or of out-of-domain, (e.g. UN data), we can observe a degradation in performance. Phrase table combination (T M3 ): We applied phrase table combination as described in [25]. We built two phrase tables and reordering models separately on the IWSLT and AMARA data. Then, we merged them by adding three additional indicator features to each entry to inform the decoder if the phrase was found in the first, second or both tables. This can be seen as a form of log-linear interpolation. SYS TM IW10 OOV AM13 OOV SYS LM IW10 AM13 B1 TM1 TM2 TM3 IWSLT AMARA IW+AM PT(IW,AM) 22.97 22.40 23.41 23.57 1.9 2.4 1.2 1.2 23.26 23.66 27.63 27.65 3.9 1.7 1.8 1.8 B1 LM1 LM2 LM3 LM4 IWSLT AMARA IWSLT+AMARA INTERPOL GW 22.97 22.83 23.69 23.59 24.24 23.26 24.05 25.90 25.62 24.79 Table 4:"
2015.mtsummit-papers.10,abdelali-etal-2014-amara,1,0.856126,"a standard task of translating German-to-English and Arabic-to-English IWSLT TED talks, we observed statistically significant improvements of up to +0.9 BLEU points. 1 Introduction Parallel data required to train Statistical Machine Translation (SMT) systems is often inadequate, and is typically collected opportunistically from wherever it is available. The conventional wisdom is that more data improves the translation quality. Additional data however, may not be best suited for tasks such as translating TED talks (Cettolo et al., 2014) or patents (Fujii et al., 2010) or educational content (Abdelali et al., 2014), and often come with the challenges of dealing with word-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution i"
2015.mtsummit-papers.10,D11-1033,0,0.702839,"ni et al., 2002) improvements of up to +0.6 for DE-EN and +0.9 for AR-EN. • Log-linear variant performed better in the case of NNJM giving an average improvements of +0.4 BLEU points for DE-EN and +0.5 for AR-EN. • Linear interpolation for NNJM models was slightly behind its log-linear variant. Data Selection: • OSM-based selection performed better for AR-EN task giving an average improvement of +0.7 • NNJM performed better at the DE-EN task giving an average improvement of +0.6 points. • Both OSM- and NNJM-based selection gave slightly better results than Modified-MooreLewis (MML) selection (Axelrod et al., 2011). The rest of the paper is organized as follows. Section 2 briefly describes the OSM and the NNJM models. Section 3 describes mixture model and data selection techniques that we apply using the OSM and the NNJM models to carry out adaptation. Section 4 presents the results. Section 5 discusses related work and Section 6 concludes the paper. 2 Joint Sequence Models In this section, we revisit Operation Sequence and Neural Network Joint models briefly. 2.1 Operation Sequence Model The Operation Sequence Model (OSM) is a bilingual model that couples translation and reordering by representing them"
2015.mtsummit-papers.10,2014.iwslt-evaluation.6,1,0.864662,"52K 24K 32K 28K Table 2: Statistics of the German-English and Arabic-English training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in Millions. ep = Europarl, cc = Common Crawl, un = United Nations and an output embedding layer of 750. Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry an"
2015.mtsummit-papers.10,2011.iwslt-evaluation.18,0,0.186872,"ata, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but n"
2015.mtsummit-papers.10,P13-1141,0,0.232794,"Missing"
2015.mtsummit-papers.10,N13-1114,0,0.591829,"EM-based weighting, (ii) using log-linear model inside the SMT pipeline. Secondly, we use cross-entropy difference (Moore and Lewis, 2010) between in- and out-domain models to perform data selection for domain adaptation. The bilingual property of the OSM and NNJM models gives them an edge over traditional LM-based methods, which do not capture source and target domain relevance jointly. The embedded reordering information modeled in OSM helps it to preserve reordering characteristic of the in-domain data. Capturing reordering variation across domains have been shown to be beneficial also by Chen et al. (2013a). NNJM adds a different dimension to it by semantically generalizing the data using distributed representation of words (Bengio et al., 2003). We evaluated our systems on a standard task of translating IWSLT TED talks for Germanto-English (DE-EN) and Arabic-to-English (AR-EN) language pairs. Below is a summary of our main findings: Model Weighting: • Linearly interpolating OSM models through EM-based weighting gave average BLEU (Papineni et al., 2002) improvements of up to +0.6 for DE-EN and +0.9 for AR-EN. • Log-linear variant performed better in the case of NNJM giving an average improveme"
2015.mtsummit-papers.10,P13-1126,0,0.590988,"EM-based weighting, (ii) using log-linear model inside the SMT pipeline. Secondly, we use cross-entropy difference (Moore and Lewis, 2010) between in- and out-domain models to perform data selection for domain adaptation. The bilingual property of the OSM and NNJM models gives them an edge over traditional LM-based methods, which do not capture source and target domain relevance jointly. The embedded reordering information modeled in OSM helps it to preserve reordering characteristic of the in-domain data. Capturing reordering variation across domains have been shown to be beneficial also by Chen et al. (2013a). NNJM adds a different dimension to it by semantically generalizing the data using distributed representation of words (Bengio et al., 2003). We evaluated our systems on a standard task of translating IWSLT TED talks for Germanto-English (DE-EN) and Arabic-to-English (AR-EN) language pairs. Below is a summary of our main findings: Model Weighting: • Linearly interpolating OSM models through EM-based weighting gave average BLEU (Papineni et al., 2002) improvements of up to +0.6 for DE-EN and +0.9 for AR-EN. • Log-linear variant performed better in the case of NNJM giving an average improveme"
2015.mtsummit-papers.10,N12-1047,0,0.0324171,"l. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in baseline MT systems were also trained on the concatenated data. Linear interpolation (OSMln ) based on EM-weighting shows significant improvements with average BLEU gains of +0.6 in DE-EN and +0.9 in AR-EN over the baseline system Bcat (see Table 3).6 One reason for better gains in AR-EN is the fact that the out-domain UN data 4 Training NNJM with backpropagation could be proh"
2015.mtsummit-papers.10,J81-4005,0,0.515198,"Missing"
2015.mtsummit-papers.10,P14-1129,0,0.128935,". Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (ii) the Neural Network Joint Model or NNJM (Devlin et al., 2014) — a continuous space model that learns neural network over augmented streams of source and target sequences. Both models are used as additional language model (LM) features insid"
2015.mtsummit-papers.10,P13-2119,0,0.300031,"based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was earlier done for language modeling using information retrieval techniques (Hildebrand et al., 2005) and using perplexity measure (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source side and target side language models. Duh et al. (2013) used recurrent neural network language model instead of an ngram-based language model to do the same. Translation model features were used recently by Liu et al. (2014); Hoang and Sima’an (2014) to do data selection. 5.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards th"
2015.mtsummit-papers.10,P13-2071,1,0.880717,"g a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (ii) the Neural Network Joint Model or NNJM (Devlin et al., 2014) — a continuous space model that learns neural network over augmented streams of source and target sequences. Both models are used as additional language mod"
2015.mtsummit-papers.10,C14-1041,1,0.833548,"epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in baseline MT systems were also trained on the concatenated data. Linear interpolation (OSMln ) based on EM-weighting shows significant improvements with average BLEU gains of +0.6 in DE-EN and +0.9 in AR-EN over the baseline system Bcat (see Table 3).6 One reason for better gains in AR-EN is the fact that the out-d"
2015.mtsummit-papers.10,P11-1105,1,0.931772,"y distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (ii) the Neural Network Joint Model or NNJM (Devlin et al., 2014) — a continuous space model that learns neural network over augmented streams of source and target sequences. Both models are used as additional language model (LM) features inside the SMT decoder. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 117 The diversity of the two models, i.e., OSM with embedded reordering information and NNJM with continuous space modeling, makes them interesting to be explored for domain adaptation."
2015.mtsummit-papers.10,N13-1073,0,0.0478691,"arget). Tokens are represented in Millions. ep = Europarl, cc = Common Crawl, un = United Nations and an output embedding layer of 750. Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT sy"
2015.mtsummit-papers.10,P12-2023,0,0.0681817,"adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation where no in-domain data is available (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion We targeted an unexplored area of using bilingual language models for domain adaptation. We applied model weighting and data selection techniques using OSM and NNJM models. Both methods were shown to be effective in the target translation tasks. Interpolating multi-domain models gave an average improvement of up to +0.9 BLEU points using OSM and +0.5 using NNJM. We also used NNJM and OSM models for data select"
2015.mtsummit-papers.10,W08-0334,0,0.165313,"l cut-off threshold is a time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network traini"
2015.mtsummit-papers.10,D10-1044,0,0.19196,"ion model features were used recently by Liu et al. (2014); Hoang and Sima’an (2014) to do data selection. 5.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixtu"
2015.mtsummit-papers.10,W07-0717,0,0.155727,"enges of dealing with word-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates r"
2015.mtsummit-papers.10,W09-0439,0,0.123872,"filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain ada"
2015.mtsummit-papers.10,D08-1089,0,0.0222621,"r training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in baseline MT systems were also trained on the concatenated data. Linear interpolation (OSMln ) ba"
2015.mtsummit-papers.10,P14-1066,0,0.0237562,"his allows the model to learn very rich translation and reordering patterns. Moreover, the model is based on minimal translation units (MTUs) and considers source and target contextual information across phrasal boundaries, thus addressing phrasal independence assumption and spurious segmentation problems in traditional phrase-based MT. 2.2 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to MT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013; Socher et al., 2013; Hinton et al., 2012). A bilingual Neural Network Joint model for MT was recently proposed by Devlin et al. (2014). It learns a feedforward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. Each input word in the context has a D dimensional ("
2015.mtsummit-papers.10,E14-1035,0,0.0822255,"Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation where no in-domain data is available (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion We targeted an unexplored area of using bilingual language models for domain adaptation. We applied model weighting and data selection techniques using OSM and NNJM models. Both methods were shown to be effective in the target translation tasks. Interpolating multi-domain models gave an average improvement of up to +0.9 BLEU points using OSM and +0.5 using NNJM. We also used NNJM and OSM models for data selection using differences"
2015.mtsummit-papers.10,W11-2123,0,0.0290447,"Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in baseline MT systems were also tr"
2015.mtsummit-papers.10,2005.eamt-1.19,1,0.837721,"be an effective way to discard poor quality or irrelevant training instances, which when included in the MT systems, hurts its performance. The idea is to score the out-domain data using model trained from the in-domain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was earlier done for language modeling using information retrieval techniques (Hildebrand et al., 2005) and using perplexity measure (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source side and target side language models. Duh et al. (2013) used recurrent neural network language model instead of an ngram-based language model to do the same. Translation model features were used recently by Liu et al. (2014); Hoang and Sima’an (2014) to do data selection. 5.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. Therefore rather th"
2015.mtsummit-papers.10,C14-1182,0,0.626827,"Missing"
2015.mtsummit-papers.10,D15-1147,1,0.427467,"e out-domain data that is unknown to the in-domain OSM, gets high probability7 and is ranked higher in the search space. On the contrary, the same gets down-weighted in a linearly interpolated global model. Both linear and log-linear interpolation of the NNJM models showed improvements over the baseline system Bcat (refer to Table 4). Log-linear interpolation (NNJMlg ) performed slightly better in both cases. Notice that NNJMlg does not face the same problem as OSMlg because all NNJM models are trained using the in-domain vocabulary with a low probability assigned to the out-domain UNKs.8 See Joty et al. (2015) for more details on our novel handling 7 Due to probability mass assigned to UNK sequences. order to reduce the training time and to learn better word representations, neural models are trained on most frequent vocabulary words only and low frequency words are represented under a class of unknown words, unk. This results in a large number of n-gram sequences containing at least one unk word and thereby, makes unk a highly probable word for the model. As a result of this discrepancy, sentences with more number of unk words will be selected. To solve this problem we created a separate class for"
2015.mtsummit-papers.10,D13-1176,0,0.0465297,"tion or reordering) decisions. This allows the model to learn very rich translation and reordering patterns. Moreover, the model is based on minimal translation units (MTUs) and considers source and target contextual information across phrasal boundaries, thus addressing phrasal independence assumption and spurious segmentation problems in traditional phrase-based MT. 2.2 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to MT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013; Socher et al., 2013; Hinton et al., 2012). A bilingual Neural Network Joint model for MT was recently proposed by Devlin et al. (2014). It learns a feedforward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. Each input word in the context has"
2015.mtsummit-papers.10,P07-2045,0,0.00623946,"erman-English and Arabic-English training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in Millions. ep = Europarl, cc = Common Crawl, un = United Nations and an output embedding layer of 750. Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Mo"
2015.mtsummit-papers.10,N12-1005,0,0.0261074,"t-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (ii) the Neural Network Joint Model or NNJM (Devlin et al., 2014) — a continuous space model that learns neural network over augmented streams of source and target sequences. Both models are used as a"
2015.mtsummit-papers.10,P14-2093,0,0.222498,"to reduce computational cost when training is expensive and also when memory is constrained. Data selection was earlier done for language modeling using information retrieval techniques (Hildebrand et al., 2005) and using perplexity measure (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source side and target side language models. Duh et al. (2013) used recurrent neural network language model instead of an ngram-based language model to do the same. Translation model features were used recently by Liu et al. (2014); Hoang and Sima’an (2014) to do data selection. 5.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases ra"
2015.mtsummit-papers.10,N13-1074,0,0.30113,"ain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et a"
2015.mtsummit-papers.10,J06-4004,0,0.0704009,"Missing"
2015.mtsummit-papers.10,C14-1105,0,0.264728,"pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation where no in-domain data is available (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion We targeted an unexplored area of using bilingual language models for domain adaptation. We applied model weighting and data selection techniques using OSM and NNJM models. Both methods were shown to be effective in the target translation tasks. Interpolating multi-domain models gave an average improvement of up to +0.9 BLEU points using OSM and +0.5 using NNJM. We also used NNJM and OSM models for data selection using differences in cross entropy and showed improvements of up to +0.6 BLEU points. The code will be contributed to Mo"
2015.mtsummit-papers.10,D09-1074,0,0.345709,"tolo et al., 2014) or patents (Fujii et al., 2010) or educational content (Abdelali et al., 2014), and often come with the challenges of dealing with word-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore"
2015.mtsummit-papers.10,P10-2041,0,0.680232,"tents (Fujii et al., 2010) or educational content (Abdelali et al., 2014), and often come with the challenges of dealing with word-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) th"
2015.mtsummit-papers.10,D09-1141,0,0.0657375,"time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NN"
2015.mtsummit-papers.10,P02-1040,0,0.102972,"Missing"
2015.mtsummit-papers.10,C12-2104,0,0.064795,"el to learn very rich translation and reordering patterns. Moreover, the model is based on minimal translation units (MTUs) and considers source and target contextual information across phrasal boundaries, thus addressing phrasal independence assumption and spurious segmentation problems in traditional phrase-based MT. 2.2 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to MT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013; Socher et al., 2013; Hinton et al., 2012). A bilingual Neural Network Joint model for MT was recently proposed by Devlin et al. (2014). It learns a feedforward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. Each input word in the context has a D dimensional (continuous-valu"
2015.mtsummit-papers.10,I08-2089,0,0.0130472,"nd an output embedding layer of 750. Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in base"
2015.mtsummit-papers.10,E12-1055,0,0.662401,"ord-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (i"
2015.mtsummit-papers.10,P13-1082,0,0.207199,"se level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation where no in-domain data is available (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion We targeted an unexplored area of using bilingual language models for domain adaptation. We applied model weighting and data selection techniques using OSM and NNJM models. Both methods were shown to be effective in the target translation tasks. Interpolating multi-domain models gave an average improvement of up to +0.9 BLEU points using OSM and +0.5 using NNJM. We also used NNJM and OSM models for data selection using differences in cross entropy and showed improvements of up to +0.6 BLEU points. The code wil"
2015.mtsummit-papers.10,P13-1045,0,0.034229,"reover, the model is based on minimal translation units (MTUs) and considers source and target contextual information across phrasal boundaries, thus addressing phrasal independence assumption and spurious segmentation problems in traditional phrase-based MT. 2.2 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to MT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013; Socher et al., 2013; Hinton et al., 2012). A bilingual Neural Network Joint model for MT was recently proposed by Devlin et al. (2014). It learns a feedforward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. Each input word in the context has a D dimensional (continuous-valued) Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami"
2015.mtsummit-papers.10,D13-1140,0,0.293,"models are trained by randomly selecting corpora of same size as that of the in-domain data. 4 Experiments Data: We used TED talks (Cettolo et al., 2014) as our in-domain corpus. For German-toEnglish (DE-EN), we used the data made available for WMT’14.2 This contains News, Europarl and Common Crawl as out-domain data. For Arabic-English (AR-EN), we used the UN corpus as out-domain data. We concatenated dev- and test-2010 for tuning and used test2011-2013 for evaluation. Table 2 shows the size of the training and test data used. NNJM Settings: The NNJM models were trained using NPLM3 toolkit (Vaswani et al., 2013) with the following settings. We used a target context of 5 words and an aligned source window of 9 words, forming a joint stream of 14-grams for training. We restricted source and target side vocabularies to 20K and 40K most frequent words. We used an input embedding layer of 150 2 http://www.statmt.org/wmt14/ 3 http://nlg.isi.edu/software/nplm/ Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 122 German-English Arabic-English Corpus Sent. TokDE TokEN Corpus Sent. TokAR TokEN iwslt news ep cc 177K 200K 1.9M 2.3M 3.3M 5.1M 48.7M 53.9M 3.5M 5.0M 51.0M 57"
2020.bucc-1.3,N16-3003,1,0.824794,"ng Statistical Machine Translation (SMT) pipeline. For evaluation purposes, Jehl et al. (2012) use crowdsourcing to create a parallel corpus of 1000 Arabic tweets and 3 manual English translations for each Arabic tweet and reports improvement for SMT pipeline. Abidi and Smaili (2017) used topics related to Syria to crawl Twitter and collect 58,000 Arabic tweets and 60,000 English tweets. The tweets are then preprocessed heavily, which requires knowledge of Arabic. Then, the tweets are aligned to produce a corpus of comparable Arabic-English tweets aimed at improving MT systems. Vicente et al. (2016) present a parallel corpus that covers 5 languages from the Iberian Peninsula, created by automatic collection and crowdsourcing. To align parallel content, Vicente et al. (2016) use measures such as publication date, string length similarity, hashtag and user mention overlap, and Longest Common Subsequence ratio (LCSR). LCSR exploits the similarity of the languages within the Iberian peninsula. The aim of the corpus is to aid in the development of microtext translation systems. Vicente et al. (2016) 3. Methodology and Corpus Construction Before diving further into the methodology, it’s import"
2020.bucc-1.3,W19-4622,1,0.839243,"t easily adaptable for less-resourced languages. Ling et al. (2013) collect parallel content of different languages from single tweets (compare Table 1 and Table 2 for difference). They reported a significant improvement in MT systems. In this work, we will not focus on extracting parallel content from single tweets. However, our methods can be adapted to do so in the future. Our work also augments existing work in Twitter account annotation. Specifically for Arabic Twitter users, there is a scarcity of resources. Inspired by Mubarak and Darwish (2014), who annotate tweets for their dialects, Bouamor et al. (2019) presented a dataset of 3000 Twitter accounts annotated with their countries of origin. Alhozaimi and Almishari (2018) categorize 80 Twitter accounts into 4 categories of topics the accounts are interested in. It suffices to say that there is a need for such resources and our annotation of Twitter accounts for country and topic, although not our primary goal, is a step forward. Related Work Although the amount of data on social media is growing at an incredible speed and can be a valuable resource for NLP tasks, the utilization of data on social media has been underwhelming. Efforts to use the"
2020.bucc-1.3,L16-1170,1,0.828424,"template for posting tweets and are likely to be bots. Table 4 shows an example of such accounts. These accounts are not very useful for the purpose of creating a corpus for machine translation. To identify these accounts, we plot number of words in all the tweets posted by the account against the number of unique words among them. If the ratio of unique words versus total words is below a threshold, we exclude the account. To increase the quality of the collected Arabic-English tweets, we can use complex Arabic word segmenter to split prefixes and suffixes, for example Farasa word segmenter (Darwish and Mubarak, 2016; Abdelali et al., 2016), or lemmatizer (Mubarak, 2018), and for English we can use Porter stemmer (Porter, 1980). We leave this for future work. 3.3. Arabic-English Parallel Tweets Corpus Using the method described in Section 3.2., we collect a corpus of 166K Arabic-English parallel tweets and 1,389 accounts who regularly post them. For our collection of Arabic-English parallel tweets, first, we collect 175M Arabic tweets in March 2014 using Twitter API with language Then, we calculate ratio of unique words and total number of words in tweets posted by each account. If this ratio falls below"
2020.bucc-1.3,W12-3153,0,0.00897186,"is a step forward. Related Work Although the amount of data on social media is growing at an incredible speed and can be a valuable resource for NLP tasks, the utilization of data on social media has been underwhelming. Efforts to use these platforms as a resource for translation are still relatively small. Sluyter Gäthje et al. (2018) built a parallel resource for English-German using 4000 English tweets that were manually translated into German with a special focus on the informal nature of the tweets. The objective was to provide a resource tailored for translating user generated-content. Jehl et al. (2012) and Abidi and Smaili (2017) extract parallel phrases by using CLIR techniques. The major difference is that these methods are extracting comparable data, whereas, we want to extract parallel tweets, which we can expect to be closer to true translation. Jehl et al. use a probabilistic translation-based retrieval (Xu et al., 2001) in the context of Twitter for the purpose of training Statistical Machine Translation (SMT) pipeline. For evaluation purposes, Jehl et al. (2012) use crowdsourcing to create a parallel corpus of 1000 Arabic tweets and 3 manual English translations for each Arabic twee"
2020.bucc-1.3,P13-1018,0,0.0360142,"out to as large of an audience as possible. Often the audience consists of individuals who use different languages. To build a connection with this diverse audience, organizations, celebrities, and public figures post tweets in multiple languages to ensure max reach out. Twitter, with traditionally 140 (Now, 280) character limit on the tweets, prompts the users to reach out to their audiences across multiple tweets containing the same message in different languages. In our paper, we propose a method to collect such tweets. These parallel tweets can be a great resource for machine translation. Ling et al., (2013) show that parallel texts from Twitter can significantly improve MT systems. As opposed to crowdsourcing translations that cost money or complex mechanisms of cross-language information retrieval, we provide a free and generic method of obtaining a large amount of translations that cover highly sought after new vocabulary and terminology. For example, in Table 1, we can see  QºË@ éÓY  that, éJ Kð g is translated to ""e-Service"" by the user. 14 In addition to collecting parallel tweets and Twitter accounts, we also annotate a subset of Twitter accounts for their countries and topics the acco"
2020.bucc-1.3,W14-3601,1,0.815595,"ernal resources. The generic and simple nature of our method makes it easily adaptable for less-resourced languages. Ling et al. (2013) collect parallel content of different languages from single tweets (compare Table 1 and Table 2 for difference). They reported a significant improvement in MT systems. In this work, we will not focus on extracting parallel content from single tweets. However, our methods can be adapted to do so in the future. Our work also augments existing work in Twitter account annotation. Specifically for Arabic Twitter users, there is a scarcity of resources. Inspired by Mubarak and Darwish (2014), who annotate tweets for their dialects, Bouamor et al. (2019) presented a dataset of 3000 Twitter accounts annotated with their countries of origin. Alhozaimi and Almishari (2018) categorize 80 Twitter accounts into 4 categories of topics the accounts are interested in. It suffices to say that there is a need for such resources and our annotation of Twitter accounts for country and topic, although not our primary goal, is a step forward. Related Work Although the amount of data on social media is growing at an incredible speed and can be a valuable resource for NLP tasks, the utilization of"
2020.bucc-1.3,L18-1181,1,0.785138,"ws an example of such accounts. These accounts are not very useful for the purpose of creating a corpus for machine translation. To identify these accounts, we plot number of words in all the tweets posted by the account against the number of unique words among them. If the ratio of unique words versus total words is below a threshold, we exclude the account. To increase the quality of the collected Arabic-English tweets, we can use complex Arabic word segmenter to split prefixes and suffixes, for example Farasa word segmenter (Darwish and Mubarak, 2016; Abdelali et al., 2016), or lemmatizer (Mubarak, 2018), and for English we can use Porter stemmer (Porter, 1980). We leave this for future work. 3.3. Arabic-English Parallel Tweets Corpus Using the method described in Section 3.2., we collect a corpus of 166K Arabic-English parallel tweets and 1,389 accounts who regularly post them. For our collection of Arabic-English parallel tweets, first, we collect 175M Arabic tweets in March 2014 using Twitter API with language Then, we calculate ratio of unique words and total number of words in tweets posted by each account. If this ratio falls below the threshold of 0.1, we exclude the account and all th"
2020.bucc-1.3,L18-1422,0,0.0373591,"Missing"
2020.bucc-1.3,L16-1469,0,0.0163787,"rpose of training Statistical Machine Translation (SMT) pipeline. For evaluation purposes, Jehl et al. (2012) use crowdsourcing to create a parallel corpus of 1000 Arabic tweets and 3 manual English translations for each Arabic tweet and reports improvement for SMT pipeline. Abidi and Smaili (2017) used topics related to Syria to crawl Twitter and collect 58,000 Arabic tweets and 60,000 English tweets. The tweets are then preprocessed heavily, which requires knowledge of Arabic. Then, the tweets are aligned to produce a corpus of comparable Arabic-English tweets aimed at improving MT systems. Vicente et al. (2016) present a parallel corpus that covers 5 languages from the Iberian Peninsula, created by automatic collection and crowdsourcing. To align parallel content, Vicente et al. (2016) use measures such as publication date, string length similarity, hashtag and user mention overlap, and Longest Common Subsequence ratio (LCSR). LCSR exploits the similarity of the languages within the Iberian peninsula. The aim of the corpus is to aid in the development of microtext translation systems. Vicente et al. (2016) 3. Methodology and Corpus Construction Before diving further into the methodology, it’s import"
2020.coling-demos.15,N16-3003,1,0.796987,"rade 6) 1 Buckwalter transliteration and translation are provided. We thank The World Organization for Renaissance of Arabic Language (WORAL) for data collection and preparation. 3 We use ISO 3166-1 alpha-2 for country codes. 2 81 4 System Description System Architecture: An overview of the system functionalities is illustrated in Figure 1, and the system can be publicly accessed using the following URL: curriculum.qcri.org. After the acquisition of the textbooks collection, we used the publicly available Farasa Arabic NLP toolkit to process the text. This includes morphological segmentation (Abdelali et al., 2016), diacritization (Darwish et al., 2017); and lemmatization (Mubarak, 2018). These steps are crucial to enhance the analysis given the complexities of Arabic. Next, language experts classified lemmas into 50 categories (ex: Function Words, Human, Animal, Food, History, Politics, Travel, Religious Acts, etc.) The system provides the following functions: Term Usage, Category, Statistics, Differences, and Text Grading. It also uses Text to Speech (TTS) , Machine Translation (MT), and Farasa Tools to pronounce, translate, and provide morphological analysis of lexical items respectively. Design: To"
2020.coling-demos.15,L18-1366,0,0.0587851,"orm that analyzes curricula can help identify shortcomings and whether they are tailored to desired outcomes. Natural Language Processing (NLP) can provide automated methods to perform such analysis and provide feedback to curricula developers. A wealth of research devoted to build, curate, and assess educational materials has been published for English and other Latin languages (Tyler, 1950; Oliva, 2005; Braun et al., 2006; Soto, 2015). Though some recent NLP work on Arabic has addressed language learning, readability and textbook assessments (Zaghouani et al., 2014; Zalmout et al., 2016; Al Khalil et al., 2018), the work is limited with rather scarce resources and tools. This paper aims to contribute to curricula assessment, and fill some of the gaps in the literature. We focus on analyzing Arabic curricula taught in Gulf countries at elementary school level. We built a tool that analyzes curricula by providing: statistics about word usage and morphological forms in different grades; words belonging to specific categories, such as food or animals; comparison with other curricula; and complexity levels of words in a text according to selected grades. The tool provides insights into the strengths and"
2020.coling-demos.15,W17-1302,1,0.819228,"translation are provided. We thank The World Organization for Renaissance of Arabic Language (WORAL) for data collection and preparation. 3 We use ISO 3166-1 alpha-2 for country codes. 2 81 4 System Description System Architecture: An overview of the system functionalities is illustrated in Figure 1, and the system can be publicly accessed using the following URL: curriculum.qcri.org. After the acquisition of the textbooks collection, we used the publicly available Farasa Arabic NLP toolkit to process the text. This includes morphological segmentation (Abdelali et al., 2016), diacritization (Darwish et al., 2017); and lemmatization (Mubarak, 2018). These steps are crucial to enhance the analysis given the complexities of Arabic. Next, language experts classified lemmas into 50 categories (ex: Function Words, Human, Animal, Food, History, Politics, Travel, Religious Acts, etc.) The system provides the following functions: Term Usage, Category, Statistics, Differences, and Text Grading. It also uses Text to Speech (TTS) , Machine Translation (MT), and Farasa Tools to pronounce, translate, and provide morphological analysis of lexical items respectively. Design: To implement our tool, we used Django4 , a"
2020.coling-demos.15,L18-1039,0,0.0127727,"predict the reading difficulty of texts. Al-Khalifa and Al-Ajlan (2010) proposed a tool for readability analysis and applied this to curricula in Saudi Arabia. Zalmout et al. (2016) described a process to analyze the textbooks of two different English teaching methods for English as a Second Language (ESL) by using readability scoring technique. Al Khalil et al. (2018) presented an Arabic reading corpus that was collected from textbooks from first to twelfth grade from United Arab Emirates and works of fiction to enhance the inadequate resources that effected educational applications. Garc´ıa Salido et al. (2018) proposed a lexical tool for academic writing in Spanish and described the data extraction from a corpus of academic texts. This tool basically provides insight into how to use typical vocabulary for academic genre in order to build an entire text. Arabic is a complex language with rich morphology. Stems are typically derived from a set of roots using predefined stem templates. Affixes can be attached to stems to generate words (surface forms). For . JºJ ð (“wsyktbwnhA” – “and they will write it”)1 has two prefixes (and and will) example, the word AîEñJ and two suffixes (they and it). Furthe"
2020.coling-demos.15,L18-1181,1,0.737646,"rld Organization for Renaissance of Arabic Language (WORAL) for data collection and preparation. 3 We use ISO 3166-1 alpha-2 for country codes. 2 81 4 System Description System Architecture: An overview of the system functionalities is illustrated in Figure 1, and the system can be publicly accessed using the following URL: curriculum.qcri.org. After the acquisition of the textbooks collection, we used the publicly available Farasa Arabic NLP toolkit to process the text. This includes morphological segmentation (Abdelali et al., 2016), diacritization (Darwish et al., 2017); and lemmatization (Mubarak, 2018). These steps are crucial to enhance the analysis given the complexities of Arabic. Next, language experts classified lemmas into 50 categories (ex: Function Words, Human, Animal, Food, History, Politics, Travel, Religious Acts, etc.) The system provides the following functions: Term Usage, Category, Statistics, Differences, and Text Grading. It also uses Text to Speech (TTS) , Machine Translation (MT), and Farasa Tools to pronounce, translate, and provide morphological analysis of lexical items respectively. Design: To implement our tool, we used Django4 , a Python web framework for the rapid"
2020.coling-demos.15,zaghouani-etal-2014-large,0,0.031376,"r example by vocabulary level. Developing a platform that analyzes curricula can help identify shortcomings and whether they are tailored to desired outcomes. Natural Language Processing (NLP) can provide automated methods to perform such analysis and provide feedback to curricula developers. A wealth of research devoted to build, curate, and assess educational materials has been published for English and other Latin languages (Tyler, 1950; Oliva, 2005; Braun et al., 2006; Soto, 2015). Though some recent NLP work on Arabic has addressed language learning, readability and textbook assessments (Zaghouani et al., 2014; Zalmout et al., 2016; Al Khalil et al., 2018), the work is limited with rather scarce resources and tools. This paper aims to contribute to curricula assessment, and fill some of the gaps in the literature. We focus on analyzing Arabic curricula taught in Gulf countries at elementary school level. We built a tool that analyzes curricula by providing: statistics about word usage and morphological forms in different grades; words belonging to specific categories, such as food or animals; comparison with other curricula; and complexity levels of words in a text according to selected grades. The"
2020.coling-demos.15,W16-4916,0,0.154605,"level. Developing a platform that analyzes curricula can help identify shortcomings and whether they are tailored to desired outcomes. Natural Language Processing (NLP) can provide automated methods to perform such analysis and provide feedback to curricula developers. A wealth of research devoted to build, curate, and assess educational materials has been published for English and other Latin languages (Tyler, 1950; Oliva, 2005; Braun et al., 2006; Soto, 2015). Though some recent NLP work on Arabic has addressed language learning, readability and textbook assessments (Zaghouani et al., 2014; Zalmout et al., 2016; Al Khalil et al., 2018), the work is limited with rather scarce resources and tools. This paper aims to contribute to curricula assessment, and fill some of the gaps in the literature. We focus on analyzing Arabic curricula taught in Gulf countries at elementary school level. We built a tool that analyzes curricula by providing: statistics about word usage and morphological forms in different grades; words belonging to specific categories, such as food or animals; comparison with other curricula; and complexity levels of words in a text according to selected grades. The tool provides insight"
2020.coling-main.447,D19-1632,0,0.061235,"Missing"
2020.coling-main.447,W14-3627,0,0.0295886,"ous fine-grained categories of dialects. Other efforts outside of MT arena to build resources on Arabic dialect include, but are not limited to (Diab et al., 2014; Khalifa et al., 2016; Jarrar et al., 2017; Suwon et al., 2020; Mubarak et al., 2020). Machine Translation Machine Translation of Arabic dialects got attention for a short while due to BOLT project. Subsequent efforts were carried to improve MSA-to-English systems by appending Dialectto-MSA module as pre-processing step (Salloum and Habash, 2011; Salloum and Habash, 2013; Zbib et al., 2012; Sajjad et al., 2013; Durrani et al., 2014; Jeblee et al., 2014) or adapting the MSA-to-English systems towards in-domain dialectal data (Sajjad et al., 2016). Salloum et al. (2014) studied the use of sentence level dialect identification in optimizing MT system selection in mixed dialectal scenario. More recently Baniata et al. (2018) used multi-task learning in neural MT with individual encoders for MSA and dialects and a shared decoder. Despite the number of efforts in translating Arabic dialects to MSA, they are limited to a few dialects and the results among various studies are not comparable due to the difference of evaluation sets. In this paper, we"
2020.coling-main.447,L16-1679,0,0.0262907,"o be frequent in the MADAR testset where average sentence length is only 7 words. 6 Related Work Data Resources Numerous efforts have been made to build content for dialectal Arabic. Zbib et al. (2012) released Egyptian- and Levantine-English data gathered from weblogs and online user groups, translated through Amazon Mechanical Turk. Bouamor et al. (2014) and Bouamor et al. (2018) created multi-parallel data resources covering various fine-grained categories of dialects. Other efforts outside of MT arena to build resources on Arabic dialect include, but are not limited to (Diab et al., 2014; Khalifa et al., 2016; Jarrar et al., 2017; Suwon et al., 2020; Mubarak et al., 2020). Machine Translation Machine Translation of Arabic dialects got attention for a short while due to BOLT project. Subsequent efforts were carried to improve MSA-to-English systems by appending Dialectto-MSA module as pre-processing step (Salloum and Habash, 2011; Salloum and Habash, 2013; Zbib et al., 2012; Sajjad et al., 2013; Durrani et al., 2014; Jeblee et al., 2014) or adapting the MSA-to-English systems towards in-domain dialectal data (Sajjad et al., 2016). Salloum et al. (2014) studied the use of sentence level dialect iden"
2020.coling-main.447,P17-4012,0,0.0119468,"omain of MSA to maximize the benefit of large available MSA-English parallel data and the small amount of available dialectal training data. Lastly, we use back-translation (Sennrich et al., 2016a) to increase the size of the dialectal Arabic-English training data. We train an English-MSA MT system, fine-tune it on dialects and translate English monolingual data to dialectal Arabic. Then, we use this noisy dialect-English data as an additional training data to improve dialectal Arabic to English translation system. Model Settings We used transformer-based seq2seq model implemented in OpenNMT (Klein et al., 2017). We used default training and decoding settings: 6 encoder and 6 decoder layers, layer size 512, attention heads 8, dropout 0.1, Adam β1 0.9, β2 0.998 and batch size 4096 subwords. For fine-tuning, we additionally use a warmup step size of 800 and label smoothing 0.1.9 We train for 20 epochs and select the best model using the provided development sets. For example, in the case of a dialect specific system say, Egyptian, we choose the model that performs the best on Egyptian development sets. For a system targeting multiple dialects, we choose the model with the best average performance acros"
2020.coling-main.447,L16-1147,0,0.0203505,"text of MT performance in Section 5. 4 Evaluation In this section, we describe experimental setup and present our results using the evaluation suite. 4.1 Training Data and Evaluation Data Table 2 summarizes the available Arabic-English training data. The only reasonable sized dialectal training data are of Levantine and Egyptian which consist of 136k and 37k parallel sentences respectively. Rest of the dialect data is very small. We additionally use a large MSA-English corpus to explore the usefulness of MSA in translating dialectal Arabic effectively. The MSA-English corpus consists of OPUS (Lison and Tiedemann, 2016), UN (Ziemski et al., 2016), TED (Cettolo, 2016), NEWS, and QED (Guzm´an et al., 2013) corpora. In addition to the dialect testsets mentioned in Section 3, we use five MSA-English testsets – one from News domain, news04, and four from TED talks, test11-14 for some selected experiments. 4.2 Training and Model Settings Training Settings We build models using various training settings. First, we train systems using the available dialect training data and the MSA data listed in Table 2. Second, we apply the fine-tuning strategy which has shown to be effective in domain adaptation (Sajjad et al., 2"
2020.lrec-1.761,W17-3008,1,0.905605,"Modeling techniques such as keyword-based search, traditional machine learning to deep learning have been explored. However, most of the previous studies are limited to IndoEuropean languages due to the availability of resources in these languages. Unlike these resource-rich languages, studies and resources for detecting offensive language in dialectal or Modern Standard Arabic (MSA) are still very limited. Similar to the Indo-European languages, most of the publicly available datasets for Arabic (Mubarak and Darwish, 2019; Mulki et al., 2019; Alakrot et al., 2018; Al-Ajlan and Ykhlef, 2018; Mubarak et al., 2017) originate mainly from one social media platform: either Twitter (TW) or YouTube (YT). However, the challenges of detecting offensive language are not constrained to one or two platforms but have a crossplatform nature (Salminen et al., 2020). Therefore, research efforts are needed to develop rich resources that can be used to design and evaluate cross-platform offensive language classifiers. In this study, we introduce one of the first Dialectal Arabic (DA) offensive language datasets, extracted from three different social media platforms: TW, YT, and Facebook (FB). Our annotated dataset comp"
2020.lrec-1.761,W19-3512,0,0.196572,"einmardi et al., 2015), aggression (Kumar et al., 2018) among others. Modeling techniques such as keyword-based search, traditional machine learning to deep learning have been explored. However, most of the previous studies are limited to IndoEuropean languages due to the availability of resources in these languages. Unlike these resource-rich languages, studies and resources for detecting offensive language in dialectal or Modern Standard Arabic (MSA) are still very limited. Similar to the Indo-European languages, most of the publicly available datasets for Arabic (Mubarak and Darwish, 2019; Mulki et al., 2019; Alakrot et al., 2018; Al-Ajlan and Ykhlef, 2018; Mubarak et al., 2017) originate mainly from one social media platform: either Twitter (TW) or YouTube (YT). However, the challenges of detecting offensive language are not constrained to one or two platforms but have a crossplatform nature (Salminen et al., 2020). Therefore, research efforts are needed to develop rich resources that can be used to design and evaluate cross-platform offensive language classifiers. In this study, we introduce one of the first Dialectal Arabic (DA) offensive language datasets, extracted from three different socia"
2020.lrec-1.761,D17-1117,0,0.0625589,"Missing"
2020.lrec-1.761,W18-5110,0,0.0233224,"Missing"
2020.wanlp-1.19,2020.osact-1.2,0,0.0195188,"tively dealing with the vanishing gradients problem. Bidirectional GRUs and LSTMs make better use of the training data as the data is traversed twice, typically leading to improved performance (SiamiNamini et al., 2019). Table 2 lists the model parameters. 4.2 Our BERT-based Method Recently, deep contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019b) resulted in major improvements for many NLP classification and language understanding tasks. For our proposed method, we fine-tuned AraBERT (v 0.1) for the classification task (Antoun et al., 2020). AraBERT is pre-trained on an identical architecture to BERT, namely an encoder with 12 Transformer blocks, hidden size of 768, and 12 self-attention heads. It is trained on a large Arabic news corpus containing 8.5M articles composed of roughly 2.5B tokens, and it uses SentencePiece (BP) word segmentation. AraBERT has been shown to yield better results than multilingual BERT from Google, which is trained on Arabic Wikipedia only (Antoun et al., 2020). We 211 Human generated Deepfake YªK ÐñJË@ IK  @P Qj.®Ë@ Q g AîE@ é<Ë@ ZA à@ YÒjÖß. AK ðP . úÎ« éJ ÊJK@ Qå B@ H@  úÎ« C£@    ñ®Ë@ ,Pñ"
2020.wanlp-1.19,N19-1423,0,0.179006,"/story/ai-generated-text-is-the-scariest-deepfake-of-all/ This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 207 Proceedings of the Fifth Arabic Natural Language Processing Workshop, pages 207–214 Barcelona, Spain (Online), December 12, 2020 Neural Netowrks (RNNs) are now applied to enhance the performance of auto-generated text detection models(Iqbal and Qureshi, 2020). Despite theirs success, they still suffer from the lack of annotated data for training. GPT-2 (Radford et al., 2019), and BERT (Devlin et al., 2019a) are among new approaches recently considered for overcoming this issue. A language understanding model learns contextual and task-independent representations of terms. A huge amount of texts obtained from large corpora is used to build generic (multilingual multi-application) models. Numerous deep learning text detection methods have been proposed to help readers determine if a piece of text has been authored by machine or human. The results of employing such language models are very effective in the detection of misinformation and propaganda by providing mechanisms to reveal the nature of"
2020.wanlp-1.21,L18-1577,0,0.0233125,", 2017), a FastText model 2 https://github.com/shammur/Arabic_news_text_classification_datasets https://data.mendeley.com/datasets/57zpx667y9/2 4 http://www.alkhaleej.ae/portal 5 https://www.alarabiya.net 6 https://www.akhbarona.com 7 https://data.mendeley.com/datasets/hhrb7phdyx/2 8 https://www.skynewsarabia.com 9 https://www.masrawy.com 10 https://github.com/antcorpus/antcorpus.data 11 https://data.mendeley.com/datasets/322pzsdxwy/1 3 227 that is trained on Wikipedia (Bojanowski et al., 2017), and dialectal word embeddings with small and noisy corpora (Erdmann et al., 2018) and with tweets (Abdul-Mageed et al., 2018; Farha and Magdy, 2019). As for contextual embeddings, a handful of models are available (ElJundi et al., 2019; Antoun et al., 2020; Talafha et al., 2020). The first available BERT model for Arabic was multilingual BERT (mBERT), which was pre-trained on the Wikipedia dumps of 104 languages including Arabic. However, previous studies have shown that monolingual BERT models perform significantly better than the mBERT (Polignano et al., 2019). A recent Arabic BERT model (AraBERT) (Antoun et al., 2020) was trained on Wikipedia and a large collection of Arabic news articles, with the base configur"
2020.wanlp-1.21,S18-1053,0,0.0244389,"orization datasets, where the first is composed of social media posts from a popular Arabic news channel that cover Twitter, Facebook, and YouTube, and the second is composed of tweets from popular Arabic accounts. The posts in the former are nearly exclusively authored in modern standard Arabic (MSA), while the tweets in the latter contain both MSA and dialectal Arabic. 1 Introduction Text classification, particularly of short texts, is an important problem in NLP and has been used in a variety of tasks in social media such as identifying people’s sentiment (Mohammad et al., 2013), emotions (Abdullah and Shaikh, 2018), interests (Keneshloo et al., 2016), stance (Mohammad et al., 2016), offensive languages (Chowdhury et al., 2020; Hassan et al., 2020) and communication styles (Mubarak et al., 2020). Text classification requires the availability of manually tagged text to train effective classification models. Due to annotation costs, adapting labeled texts from one domain to tag texts in other domains is desirable, as it would avail the need to tag in-domain data. With the recent success of pre-trained transformer-based models (e.g. BERT), various studies have adopted such models to generate contextualized"
2020.wanlp-1.21,2020.osact-1.2,0,0.302997,"7y9/2 4 http://www.alkhaleej.ae/portal 5 https://www.alarabiya.net 6 https://www.akhbarona.com 7 https://data.mendeley.com/datasets/hhrb7phdyx/2 8 https://www.skynewsarabia.com 9 https://www.masrawy.com 10 https://github.com/antcorpus/antcorpus.data 11 https://data.mendeley.com/datasets/322pzsdxwy/1 3 227 that is trained on Wikipedia (Bojanowski et al., 2017), and dialectal word embeddings with small and noisy corpora (Erdmann et al., 2018) and with tweets (Abdul-Mageed et al., 2018; Farha and Magdy, 2019). As for contextual embeddings, a handful of models are available (ElJundi et al., 2019; Antoun et al., 2020; Talafha et al., 2020). The first available BERT model for Arabic was multilingual BERT (mBERT), which was pre-trained on the Wikipedia dumps of 104 languages including Arabic. However, previous studies have shown that monolingual BERT models perform significantly better than the mBERT (Polignano et al., 2019). A recent Arabic BERT model (AraBERT) (Antoun et al., 2020) was trained on Wikipedia and a large collection of Arabic news articles, with the base configuration of the BERT model. The model showed success for many Arabic NLP downstream tasks. Recently, a Multidialect-Arabic-BERT (Talafh"
2020.wanlp-1.21,Q17-1010,0,0.01627,"l embeddings representation have been trained. Some popular Arabic static embeddings include: Arabic word2vec (Soliman et al., 2017), a FastText model 2 https://github.com/shammur/Arabic_news_text_classification_datasets https://data.mendeley.com/datasets/57zpx667y9/2 4 http://www.alkhaleej.ae/portal 5 https://www.alarabiya.net 6 https://www.akhbarona.com 7 https://data.mendeley.com/datasets/hhrb7phdyx/2 8 https://www.skynewsarabia.com 9 https://www.masrawy.com 10 https://github.com/antcorpus/antcorpus.data 11 https://data.mendeley.com/datasets/322pzsdxwy/1 3 227 that is trained on Wikipedia (Bojanowski et al., 2017), and dialectal word embeddings with small and noisy corpora (Erdmann et al., 2018) and with tweets (Abdul-Mageed et al., 2018; Farha and Magdy, 2019). As for contextual embeddings, a handful of models are available (ElJundi et al., 2019; Antoun et al., 2020; Talafha et al., 2020). The first available BERT model for Arabic was multilingual BERT (mBERT), which was pre-trained on the Wikipedia dumps of 104 languages including Arabic. However, previous studies have shown that monolingual BERT models perform significantly better than the mBERT (Polignano et al., 2019). A recent Arabic BERT model ("
2020.wanlp-1.21,2020.lrec-1.761,1,0.86977,"witter, Facebook, and YouTube, and the second is composed of tweets from popular Arabic accounts. The posts in the former are nearly exclusively authored in modern standard Arabic (MSA), while the tweets in the latter contain both MSA and dialectal Arabic. 1 Introduction Text classification, particularly of short texts, is an important problem in NLP and has been used in a variety of tasks in social media such as identifying people’s sentiment (Mohammad et al., 2013), emotions (Abdullah and Shaikh, 2018), interests (Keneshloo et al., 2016), stance (Mohammad et al., 2016), offensive languages (Chowdhury et al., 2020; Hassan et al., 2020) and communication styles (Mubarak et al., 2020). Text classification requires the availability of manually tagged text to train effective classification models. Due to annotation costs, adapting labeled texts from one domain to tag texts in other domains is desirable, as it would avail the need to tag in-domain data. With the recent success of pre-trained transformer-based models (e.g. BERT), various studies have adopted such models to generate contextualized embeddings for downstream tasks like text classification, using a small amount of in-domain data. To push the sta"
2020.wanlp-1.21,P18-2089,0,0.0192211,"nclude: Arabic word2vec (Soliman et al., 2017), a FastText model 2 https://github.com/shammur/Arabic_news_text_classification_datasets https://data.mendeley.com/datasets/57zpx667y9/2 4 http://www.alkhaleej.ae/portal 5 https://www.alarabiya.net 6 https://www.akhbarona.com 7 https://data.mendeley.com/datasets/hhrb7phdyx/2 8 https://www.skynewsarabia.com 9 https://www.masrawy.com 10 https://github.com/antcorpus/antcorpus.data 11 https://data.mendeley.com/datasets/322pzsdxwy/1 3 227 that is trained on Wikipedia (Bojanowski et al., 2017), and dialectal word embeddings with small and noisy corpora (Erdmann et al., 2018) and with tweets (Abdul-Mageed et al., 2018; Farha and Magdy, 2019). As for contextual embeddings, a handful of models are available (ElJundi et al., 2019; Antoun et al., 2020; Talafha et al., 2020). The first available BERT model for Arabic was multilingual BERT (mBERT), which was pre-trained on the Wikipedia dumps of 104 languages including Arabic. However, previous studies have shown that monolingual BERT models perform significantly better than the mBERT (Polignano et al., 2019). A recent Arabic BERT model (AraBERT) (Antoun et al., 2020) was trained on Wikipedia and a large collection of A"
2020.wanlp-1.21,W19-4621,0,0.0643966,"https://github.com/shammur/Arabic_news_text_classification_datasets https://data.mendeley.com/datasets/57zpx667y9/2 4 http://www.alkhaleej.ae/portal 5 https://www.alarabiya.net 6 https://www.akhbarona.com 7 https://data.mendeley.com/datasets/hhrb7phdyx/2 8 https://www.skynewsarabia.com 9 https://www.masrawy.com 10 https://github.com/antcorpus/antcorpus.data 11 https://data.mendeley.com/datasets/322pzsdxwy/1 3 227 that is trained on Wikipedia (Bojanowski et al., 2017), and dialectal word embeddings with small and noisy corpora (Erdmann et al., 2018) and with tweets (Abdul-Mageed et al., 2018; Farha and Magdy, 2019). As for contextual embeddings, a handful of models are available (ElJundi et al., 2019; Antoun et al., 2020; Talafha et al., 2020). The first available BERT model for Arabic was multilingual BERT (mBERT), which was pre-trained on the Wikipedia dumps of 104 languages including Arabic. However, previous studies have shown that monolingual BERT models perform significantly better than the mBERT (Polignano et al., 2019). A recent Arabic BERT model (AraBERT) (Antoun et al., 2020) was trained on Wikipedia and a large collection of Arabic news articles, with the base configuration of the BERT model."
2020.wanlp-1.21,2020.osact-1.9,1,0.697172,"uTube, and the second is composed of tweets from popular Arabic accounts. The posts in the former are nearly exclusively authored in modern standard Arabic (MSA), while the tweets in the latter contain both MSA and dialectal Arabic. 1 Introduction Text classification, particularly of short texts, is an important problem in NLP and has been used in a variety of tasks in social media such as identifying people’s sentiment (Mohammad et al., 2013), emotions (Abdullah and Shaikh, 2018), interests (Keneshloo et al., 2016), stance (Mohammad et al., 2016), offensive languages (Chowdhury et al., 2020; Hassan et al., 2020) and communication styles (Mubarak et al., 2020). Text classification requires the availability of manually tagged text to train effective classification models. Due to annotation costs, adapting labeled texts from one domain to tag texts in other domains is desirable, as it would avail the need to tag in-domain data. With the recent success of pre-trained transformer-based models (e.g. BERT), various studies have adopted such models to generate contextualized embeddings for downstream tasks like text classification, using a small amount of in-domain data. To push the state-of-the-art performa"
2020.wanlp-1.21,P18-1031,0,0.0314595,"elonging to 6 categories) (Abbas et al., 2011); and SL-RTANew11 (20k articles belonging to 40 categories). To capture syntactic and semantic information about words, pre-trained word static embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) were widely used in many NLP tasks. Recent research advancements led to pre-trained contextual embeddings that capture much information about words in context, leading to significant improvements for many NLP tasks such as text classification and sequence labeling (Mikolov et al., 2017; Peters et al., 2018; Devlin et al., 2018; Howard and Ruder, 2018; Lan et al., 2019; Liu et al., 2019; Yang et al., 2019). As for Arabic, various static and contextual embeddings representation have been trained. Some popular Arabic static embeddings include: Arabic word2vec (Soliman et al., 2017), a FastText model 2 https://github.com/shammur/Arabic_news_text_classification_datasets https://data.mendeley.com/datasets/57zpx667y9/2 4 http://www.alkhaleej.ae/portal 5 https://www.alarabiya.net 6 https://www.akhbarona.com 7 https://data.mendeley.com/datasets/hhrb7phdyx/2 8 https://www.skynewsarabia.com 9 https://www.masrawy.com 10 https://github.com/antcorpus/a"
2020.wanlp-1.21,L16-1147,0,0.0306364,"text): This model is pre-trained on a collection of publicly available corpora including Arabic Wikipedia, the 1.5B words Arabic Corpus (El-Khair, 2016), the OSIAN Corpus (Zeroual et al., 2019), Assafir news articles, and 4 other manually crawled news websites (AlAkhbar, Annahar, AL-Ahram, AL-Wafd) from the Wayback Machine. The final model is trained on approximately 70M sentences containing roughly 3B Arabic tokens (Antoun et al., 2020). Arabic BERT: QARiB (mixed style text): This model is trained on the Arabic GigaWord corpus,15 Abulkhair Arabic Corpus (El-Khair, 2016) , and OpenSubtitles (Lison and Tiedemann, 2016) in addition to 50 million tweets that were collected by issuing the query “lang:ar” against Twitter API. The final training corpus contains 120M sentences and tweets composed of 2.7B Arabic words. Downstream Task Design: For the downstream tasks, we fine-tuned the aforementioned BERT models for our classification task using a learning rate of 2e − 5 with a batch size of 64 and 3 epochs. For the training, we restricted the maximum input length to 128 tokens, with no extra preprocessing of the data. 4.3 Evaluation To asses the categorization effectiveness we used Macro F1, which is computed by"
2020.wanlp-1.21,S13-2053,0,0.0136787,"ntroduce two new Arabic text categorization datasets, where the first is composed of social media posts from a popular Arabic news channel that cover Twitter, Facebook, and YouTube, and the second is composed of tweets from popular Arabic accounts. The posts in the former are nearly exclusively authored in modern standard Arabic (MSA), while the tweets in the latter contain both MSA and dialectal Arabic. 1 Introduction Text classification, particularly of short texts, is an important problem in NLP and has been used in a variety of tasks in social media such as identifying people’s sentiment (Mohammad et al., 2013), emotions (Abdullah and Shaikh, 2018), interests (Keneshloo et al., 2016), stance (Mohammad et al., 2016), offensive languages (Chowdhury et al., 2020; Hassan et al., 2020) and communication styles (Mubarak et al., 2020). Text classification requires the availability of manually tagged text to train effective classification models. Due to annotation costs, adapting labeled texts from one domain to tag texts in other domains is desirable, as it would avail the need to tag in-domain data. With the recent success of pre-trained transformer-based models (e.g. BERT), various studies have adopted s"
2020.wanlp-1.21,S16-1003,0,0.0233564,"om a popular Arabic news channel that cover Twitter, Facebook, and YouTube, and the second is composed of tweets from popular Arabic accounts. The posts in the former are nearly exclusively authored in modern standard Arabic (MSA), while the tweets in the latter contain both MSA and dialectal Arabic. 1 Introduction Text classification, particularly of short texts, is an important problem in NLP and has been used in a variety of tasks in social media such as identifying people’s sentiment (Mohammad et al., 2013), emotions (Abdullah and Shaikh, 2018), interests (Keneshloo et al., 2016), stance (Mohammad et al., 2016), offensive languages (Chowdhury et al., 2020; Hassan et al., 2020) and communication styles (Mubarak et al., 2020). Text classification requires the availability of manually tagged text to train effective classification models. Due to annotation costs, adapting labeled texts from one domain to tag texts in other domains is desirable, as it would avail the need to tag in-domain data. With the recent success of pre-trained transformer-based models (e.g. BERT), various studies have adopted such models to generate contextualized embeddings for downstream tasks like text classification, using a sm"
2020.wanlp-1.21,D14-1162,0,0.0866959,"ataset include 486k articles with 52 categories. 3. Arabic News Text (ANT) Corpus10 (Chouigui et al., 2017): This dataset was collected from RSS feeds and contains approximately 6k articles belonging to 9 categories Other available datasets include: Khaleej-2004 (5k articles belonging to 4 categories) (Abbas and Smaili, 2005); Watan-2004 (20k articles belonging to 6 categories) (Abbas et al., 2011); and SL-RTANew11 (20k articles belonging to 40 categories). To capture syntactic and semantic information about words, pre-trained word static embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) were widely used in many NLP tasks. Recent research advancements led to pre-trained contextual embeddings that capture much information about words in context, leading to significant improvements for many NLP tasks such as text classification and sequence labeling (Mikolov et al., 2017; Peters et al., 2018; Devlin et al., 2018; Howard and Ruder, 2018; Lan et al., 2019; Liu et al., 2019; Yang et al., 2019). As for Arabic, various static and contextual embeddings representation have been trained. Some popular Arabic static embeddings include: Arabic word2vec (Soliman et al., 2017), a FastText m"
2020.wanlp-1.21,N18-1202,0,0.024731,"Smaili, 2005); Watan-2004 (20k articles belonging to 6 categories) (Abbas et al., 2011); and SL-RTANew11 (20k articles belonging to 40 categories). To capture syntactic and semantic information about words, pre-trained word static embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) were widely used in many NLP tasks. Recent research advancements led to pre-trained contextual embeddings that capture much information about words in context, leading to significant improvements for many NLP tasks such as text classification and sequence labeling (Mikolov et al., 2017; Peters et al., 2018; Devlin et al., 2018; Howard and Ruder, 2018; Lan et al., 2019; Liu et al., 2019; Yang et al., 2019). As for Arabic, various static and contextual embeddings representation have been trained. Some popular Arabic static embeddings include: Arabic word2vec (Soliman et al., 2017), a FastText model 2 https://github.com/shammur/Arabic_news_text_classification_datasets https://data.mendeley.com/datasets/57zpx667y9/2 4 http://www.alkhaleej.ae/portal 5 https://www.alarabiya.net 6 https://www.akhbarona.com 7 https://data.mendeley.com/datasets/hhrb7phdyx/2 8 https://www.skynewsarabia.com 9 https://www."
2020.wanlp-1.21,P10-1040,0,0.06095,"ia8 and Masrawy9 news sites. The released dataset include 486k articles with 52 categories. 3. Arabic News Text (ANT) Corpus10 (Chouigui et al., 2017): This dataset was collected from RSS feeds and contains approximately 6k articles belonging to 9 categories Other available datasets include: Khaleej-2004 (5k articles belonging to 4 categories) (Abbas and Smaili, 2005); Watan-2004 (20k articles belonging to 6 categories) (Abbas et al., 2011); and SL-RTANew11 (20k articles belonging to 40 categories). To capture syntactic and semantic information about words, pre-trained word static embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014) were widely used in many NLP tasks. Recent research advancements led to pre-trained contextual embeddings that capture much information about words in context, leading to significant improvements for many NLP tasks such as text classification and sequence labeling (Mikolov et al., 2017; Peters et al., 2018; Devlin et al., 2018; Howard and Ruder, 2018; Lan et al., 2019; Liu et al., 2019; Yang et al., 2019). As for Arabic, various static and contextual embeddings representation have been trained. Some popular Arabic static embeddings include: Arab"
2020.wanlp-1.21,W19-4619,0,0.0937594,"ween 1 and 5. 4.2.2 Pre-trained Bidirectional Encoder Representations from Transformers (BERT) Models We experimented with three different BERT models as follows: Multilingual BERT: mBERT (formal text): The model is pre-trained using a masked language modeling (MLM) objective using Wikipedia articles for 104 languages including Arabic. We used the case sensitive base model (Devlin et al., 2018). Arabic BERT: AraBERT (formal text): This model is pre-trained on a collection of publicly available corpora including Arabic Wikipedia, the 1.5B words Arabic Corpus (El-Khair, 2016), the OSIAN Corpus (Zeroual et al., 2019), Assafir news articles, and 4 other manually crawled news websites (AlAkhbar, Annahar, AL-Ahram, AL-Wafd) from the Wayback Machine. The final model is trained on approximately 70M sentences containing roughly 3B Arabic tokens (Antoun et al., 2020). Arabic BERT: QARiB (mixed style text): This model is trained on the Arabic GigaWord corpus,15 Abulkhair Arabic Corpus (El-Khair, 2016) , and OpenSubtitles (Lison and Tiedemann, 2016) in addition to 50 million tweets that were collected by issuing the query “lang:ar” against Twitter API. The final training corpus contains 120M sentences and tweets c"
2021.findings-emnlp.56,2021.nlp4if-1.9,1,0.705503,"tions. Some of the larger datasets include the Liar, Liar dataset of 12.8K claims from PolitiFact (Wang, 2017), the ClaimsKG dataset and system (Tchechmedjiev et al., 2019) of 28K claims from eight factchecking organizations, the MultiFC dataset of 38K claims from 26 fact-checking organizations (Augenstein et al., 2019), and the 10K claims Truth of Various Shades dataset (Rashkin et al., 2017). There have been also datasets for other languages, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino"
2021.findings-emnlp.56,N19-1216,1,0.795082,"tweets in Arabic, Bulgarian, Dutch, and English, and we are making it freely available to the research community. We further reported a number of evaluation results for all languages using various transformer architectures. Moreover, we performed advanced experiments, including multilingual training, modeling the Twitter context, the use of propagandistic language, and whether the user is likely to be a bot, as well as multitask learning. In future work, we plan to explore multimodality and explainability (Yu et al., 2021). We further want to model the task as a multitask ordinal regression (Baly et al., 2019), as Q2–Q5 are defined on an ordinal scale. Moreover, we would like to put the data and the system in some practical use; in fact, we have already used them to analyze disinformation about COVID-19 in Bulgaria (Nakov et al., 2021a) and Qatar (Nakov et al., 2021b). Finally, the data will be used in a shared task at the CLEF2022 CheckThat! lab; part of it was used for the NLP4IF-2021 shared task (Shaar et al., 2021a). Acknowledgments We thank Akter Fatema, Al-Awthan Ahmed, AlDobashi Hussein, El Messelmani Jana, Fayoumi 6.3 Multitask Learning Sereen, Mohamed Esraa, Ragab Saleh, and Shurafa For th"
2021.findings-emnlp.56,N18-2004,1,0.90491,"Missing"
2021.findings-emnlp.56,2020.acl-main.747,0,0.0346481,"a URL, and the factuality of the website it points to.4 Models Large-scale pretrained Transformer models have achieved state-of-the-art performance for several NLP tasks. We experimented with several such models to evaluate their efficacy under various training scenarios such as, binary vs. multiclass classification, multilingual setup, etc. We used BERT (Devlin et al., 2019) and RoBERTa for English, AraBERT (Antoun et al., 2020) for Arabic, and BERTje (de Vries et al., 2019) for Dutch. We further used multilingual transformers such as (Liu et al., 2019), multilingual BERT (mBERT) and XLM-r (Conneau et al., 2020). Finally, we used static embeddings from FastText (Joulin et al., 2017). 616 4 From http://mediabiasfactcheck.com English Q. Cls. Arabic Maj. FT BT RT Bulgarian Maj. FT ArBT XLM-r Dutch Maj. FT mBT XLM-r Maj. FT BTje XLM-r Binary (Coarse-grained) Q1 Q2 Q3 Q4 Q5 Q6 Q7 2 2 2 2 2 2 2 Avg. 48.7 91.6 96.3 66.7 67.7 86.7 78.3 77.7 89.0 69.3 96.3 83.8 92.1 80.6 76.5 92.1 96.4 85.6 80.6 88.9 85.5 78.6 92.7 96.9 89.0 84.4 90.5 86.1 76.6 84.1 86.5 88.3 83.8 84.0 96.0 90.3 65.9 88.9 77.4 84.2 83.1 96.3 89.0 66.7 89.8 77.4 58.3 95.0 96.5 86.8 70.5 83.2 80.1 84.0 94.7 96.0 87.7 80.5 84.5 81.6 87.6 95.0 96"
2021.findings-emnlp.56,2020.semeval-1.186,1,0.850189,"., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER"
2021.findings-emnlp.56,2020.acl-demos.32,1,0.926725,"., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER"
2021.findings-emnlp.56,S19-2147,0,0.0285024,"s, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar"
2021.findings-emnlp.56,2021.wanlp-1.9,0,0.0367297,"llected tweets by specifying a target language (English, Arabic, Bulgarian, or Dutch), a set of COVID-19 related keywords, as shown in Figure 2, and different time frames: from January 2020 till March 2021. We collected original tweets (no retweets or replies), we removed duplicates using a similarity-based approach (Alam et al., 2021b), and we filtered out tweets with less than five words. Finally, we selected the most frequently liked and retweeted tweets for annotation. COVID-19 Research There are a number of COVID-19 Twitter datasets: some unlabeled (Chen et al., 2020; Banda et al., 2021; Haouari et al., 2021), some automatically labeled with location information (Abdul-Mageed et al., 2021; Qazi et al., 2020), some labeled using distant supervision (Cinelli et al., 2020; Zhou et al., 2020), and some manually annotated (Song et al., 2020; Vidgen et al., 2020; Shahi and Nandini, 2020; Pulido et al., 2020; Dharawat et al., 2020). There is also work on credibility (Cinelli et al., 2020; Pulido et al., 2020; Zhou et al., 2020), racial prejudices and fear (Medford et al., 2020; Vidgen et al., 2020), as well as situational information, e.g., caution and advice (Li et al., 2020), as well as on detecting me"
2021.findings-emnlp.56,2020.nlpcovid19-2.11,0,0.043054,"Abdul-Mageed et al., 2021; Qazi et al., 2020), some labeled using distant supervision (Cinelli et al., 2020; Zhou et al., 2020), and some manually annotated (Song et al., 2020; Vidgen et al., 2020; Shahi and Nandini, 2020; Pulido et al., 2020; Dharawat et al., 2020). There is also work on credibility (Cinelli et al., 2020; Pulido et al., 2020; Zhou et al., 2020), racial prejudices and fear (Medford et al., 2020; Vidgen et al., 2020), as well as situational information, e.g., caution and advice (Li et al., 2020), as well as on detecting mentions and stance with respect to known misconceptions (Hossain et al., 2020). The closest work to ours is that of Song et al. (2020), who collected false and misleading claims about COVID-19 from IFCN Poynter, and annotated them as (1) Public authority, (2) Community spread and impact, (3) Medical advice, selftreatments, and virus effects, (4) Prominent actors, (5) Conspiracies, (6) Virus transmission, (7) Virus Figure 2: The keywords used to collect the tweets. origins and properties, (8) Public reaction, and (9) Vaccines, medical treatments, and tests. These categories partially overlap with ours, but account 3.2 Annotation Task for less perspectives. Moreover, we c"
2021.findings-emnlp.56,N18-5006,1,0.802211,"tweets (they used claims from news, speeches, political debates, community question answering fora, or were just made up by human annotators; RumourEval is a notable exception), targeted factuality only (we cover a number of other issues), were limited to a single language (typically English; except for CLEF), and did not focus on COVID-19. Check-Worthiness Estimation Another relevant research line is on detecting check-worthy claims in political debates using manual annotations (Hassan et al., 2015) or by observing the selection of fact-checkers (Gencheva et al., 2017; Patwari et al., 2017; Jaradat et al., 2018; Vasileva et al., 2019). 3 3.1 Dataset Data Collection We collected tweets by specifying a target language (English, Arabic, Bulgarian, or Dutch), a set of COVID-19 related keywords, as shown in Figure 2, and different time frames: from January 2020 till March 2021. We collected original tweets (no retweets or replies), we removed duplicates using a similarity-based approach (Alam et al., 2021b), and we filtered out tweets with less than five words. Finally, we selected the most frequently liked and retweeted tweets for annotation. COVID-19 Research There are a number of COVID-19 Twitter data"
2021.findings-emnlp.56,E17-2068,0,0.0257945,"le pretrained Transformer models have achieved state-of-the-art performance for several NLP tasks. We experimented with several such models to evaluate their efficacy under various training scenarios such as, binary vs. multiclass classification, multilingual setup, etc. We used BERT (Devlin et al., 2019) and RoBERTa for English, AraBERT (Antoun et al., 2020) for Arabic, and BERTje (de Vries et al., 2019) for Dutch. We further used multilingual transformers such as (Liu et al., 2019), multilingual BERT (mBERT) and XLM-r (Conneau et al., 2020). Finally, we used static embeddings from FastText (Joulin et al., 2017). 616 4 From http://mediabiasfactcheck.com English Q. Cls. Arabic Maj. FT BT RT Bulgarian Maj. FT ArBT XLM-r Dutch Maj. FT mBT XLM-r Maj. FT BTje XLM-r Binary (Coarse-grained) Q1 Q2 Q3 Q4 Q5 Q6 Q7 2 2 2 2 2 2 2 Avg. 48.7 91.6 96.3 66.7 67.7 86.7 78.3 77.7 89.0 69.3 96.3 83.8 92.1 80.6 76.5 92.1 96.4 85.6 80.6 88.9 85.5 78.6 92.7 96.9 89.0 84.4 90.5 86.1 76.6 84.1 86.5 88.3 83.8 84.0 96.0 90.3 65.9 88.9 77.4 84.2 83.1 96.3 89.0 66.7 89.8 77.4 58.3 95.0 96.5 86.8 70.5 83.2 80.1 84.0 94.7 96.0 87.7 80.5 84.5 81.6 87.6 95.0 96.5 88.4 82.9 85.1 81.7 36.5 64.9 62.3 63.9 44.4 84.7 65.6 75.4 75.1 76.9"
2021.findings-emnlp.56,2020.emnlp-demos.2,0,0.0153692,"80.2 69.2 68.3 Finally, we should note the strong performance Avg. 73.3 73.1 60.7 59.8 71.4 71.5 55.3 54.9 of context-free models such as FastText. We believe that it is suitable for the noisy text of Table 6: Multilingual experiments using mBERT. tweets due to its ability to model not only words Shown are results for monolingual vs. multilingual models (weighted F1 ). Mul is trained on the combined but also character n-grams. In future work, we English, Arabic, Bulgarian, and Dutch data. plan to try transformers specifically trained on tweets and/or on COVID-19 related data such as BERTweet (Nguyen et al., 2020) and COVID5 Twitter-BERT (Müller et al., 2020). We also tried XLM-r, but it performed worse. 618 6.2 Twitter/Propagandistic/Botometer We conducted experiments with Twitter, propaganda, and botness features alongside the posteriors from the BERT classifier, which we combined using XGBoost (Chen and Guestrin, 2016). The results are shown in Table 7. We can see that many of the combinations yielded improvements, with botness being the most useful, followed by propaganda, and finally by the Twitter object features. Binary (Coarse-grained) Q. Cls BERT B+TF B+Prop B+Bot B+All Q1 Q2 Q3 Q4 Q5 Q6 Q7 2"
2021.findings-emnlp.56,D17-1317,0,0.0286494,"onversations with a Ministry of Public Health. Our contributions can be summarized as follows: 2 Related Work Fact-Checking Research on fact-checking claims is largely based on datasets mined from major fact-checking organizations. Some of the larger datasets include the Liar, Liar dataset of 12.8K claims from PolitiFact (Wang, 2017), the ClaimsKG dataset and system (Tchechmedjiev et al., 2019) of 28K claims from eight factchecking organizations, the MultiFC dataset of 38K claims from 26 fact-checking organizations (Augenstein et al., 2019), and the 10K claims Truth of Various Shades dataset (Rashkin et al., 2017). There have been also datasets for other languages, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers"
2021.findings-emnlp.56,2021.nlp4if-1.12,1,0.887854,"We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER task on fact ex1 traction and verification, focusing on claims about https://github.com/firojalam/ COVID-19-disinformation Wikipedia content (Thorne et al., 2018, 2019). 612 Unlike our work, the above datasets did not focus on tweets (they used claims from news, speeches, political debates, community question answering fora, or were just made up by human annotators; RumourEval is a notable exception), targeted factuality only (we cover a number of other issues), were limited to a single language (t"
2021.wanlp-1.1,L18-1577,0,0.0352448,"Missing"
2021.wanlp-1.1,D14-1154,1,0.786389,"nd feminine forms with and without the definite article È@ (Al – the) such as ú ¯@ Q« (ErAqy - Iraqi (m.)), éJ ¯@ Q« (ErAqyp - Iraqi (f.)), and ú¯@ QªË@ (AlErAqy - the Iraqi (m.)). Arabic Variant Identification The second filter checks if the account mainly tweets in either dialectal Arabic or MSA. Since Arabic users commonly switch between MSA and dialectal Arabic, and we were interested in strictly dialectal tweets, we sought to filter out MSA tweets. There are multiple ways to distinguish between dialectal and MSA text. One such method involves using a list of strictly dialectal words (Darwish et al., 2014). However, constructing such lists across multiple dialects can be challenging. Thus, we opted to train a text classifier using a heuristically labeled tweets. Specifically, given 50 million tweets that we collected between March and September 2018, we assumed that tweets strictly containing the MSA ,úæË@  ,úæË@  YË@ relative pronouns áK ,ø YË@ ,ø YË@ (“Al*y, Al*Y, Alty, AltY, Al*yn” - who/that in masculine, feminine, and plural forms) were MSA, and those strictly containing the dialectal relative pronoun úÎË@ , ú ÎË@ (“Ally, AllY” – who/that) were dialectal. The major advantage of the diale"
2021.wanlp-1.1,2020.wanlp-1.9,0,0.0856536,"Missing"
2021.wanlp-1.1,R15-1015,0,0.0266832,"r tweets contained vulgar words. Removing the tweets of such users was motivated by the fact that their tweets contain strong genre specific signals, which may adversely affect the generalization of dialect identification. 99.5 100 Accuracy 92 90 99.5100 99 97 96.5 96 95 95 94 92 91.5 89.5 87.5 89 89 86.5 84.5 85 84.5 82.582.5 80 Normalization Tweets often contain tokens that are specific to the Twitter platform such as hashtags and user mentions. To improve generalization of the trained models (hopefully beyond tweets), we split hashtags into their semantic constituents (Bansal et al., 2015; Declerck and Lendvai, 2015) and replaced user mentions and URLs with “@USER” and “URL” respectively. 77 75 IQ BH KW SA AE OM QA YE SY JO PL LB EG SD LY TN DZ MA Country Figure 2: Annotation accuracy per country. Second annotators are colored in “Red”. The manually rejected tweets that the annotators classified as not from their dialects were mostly cases where the users interacted with or responded to users from different countries. In such cases, users tend to code-switch or adopt to other users’ dialects. For example, a user identified as Tunisian  ¢Ë@ Im&apos; AÓñÔ« AK@ (Ana EmwmA bHb tweeted ø ð@ éÒÊ . . AlZlmp Awy – I"
2021.wanlp-1.1,N19-1423,0,0.0195153,"SY ا ردن ا ك LB دا ة ز ّٰ ء ي ا وي ة ش ا را ل ا ي ء ا ا ا ا او ل وى د ادا ك اك ره ى ذو وة د ا ا ن ن خ PL م ا ول ن ت رض ف ي وي ا ازاي د ل إ ا د ك JO زول دم ا ّٰ ا ذ ر د ه ا SD زى دا را ن ن EG داك را ا TN ش را ا ا LY د ل ر أ ه MA ا رو د ة ا ن ا ى ا ر Also in other dialects Figure 3: Highest valence words for each country. contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), to name but a few, have achieved ground-breaking results in many NLP classification and language understanding tasks. Both mBERT and AraBERT are pre-trained on identical architectures, namely an encoder with 12 Transformer blocks, hidden size of 768, and 12 selfattention heads. However, they differ in one major way. While mBERT is pre-trained on Wikipedia text for 104 languages,AraBERT is trained on a large Arabic news corpus containing 8.5M articles composed of roughly 2.5B tokens. For consistency with mBERT, we used Ar"
2021.wanlp-1.1,W19-4621,0,0.0272532,". 5.1 Representations Surface Features: We used two different surfacelevel features, namely word and character n-grams. Specifically, we represented tweets using: i) character n-grams, where we used 2 to 6-grams (C{2-6}); ii) word n-grams, where we used unigrams (W{1}) and unigrams to 6-grams (W{1-6}); and iii) a combination of word and character n-grams. For our dataset and MADAR , we normalized URLs, numbers, and user mentions to URL, NUM, and MENTION respectively. We used tf-idf weighting for character and word n-grams. Static Embeddings: We used Mazajak wordlevel skip-gram embeddings (Abu Farha and Magdy, 2019) that were trained on 250M Arabic tweets with 300-dimensional vectors. Deep Contextualized Embeddings: We also experimented with two pre-trained contextualized embeddings with fine-tuning for down-stream tasks, namely BERTbase-multilingual (mBERT) and AraBERT (Antoun et al., 2020). Recently, deep 5.2 Classification Models For classification, we used an SVM classifier and fine-tuned mBERT and AraBERT. We utilized the SVM classifier when using surface features and static pre-trained Mazajak embeddings. We used the Scikit Learn libsvm implementations of the SVM classifier with a linear kernel. Wh"
2021.wanlp-1.1,L18-1579,0,0.0346953,"Missing"
2021.wanlp-1.1,L18-1573,0,0.0439378,"Missing"
2021.wanlp-1.1,2020.osact-1.2,0,0.0924101,"8). Multiple approaches have been used for dialect ID that exploit a variety of features, such as character or word n-grams (Darwish et al., 2014; Zaidan and Callison-Burch, 2014; Malmasi et al., 2016; Sadat et al., 2014), and techniques such as multiple kernel learning (Ionescu and Popescu, 2016) and distributed representation of dialects (Abdul-Mageed et al., 2018; Zhang and Abdul-Mageed, 2019) to name a few. Zhang and Abdul-Mageed (2019) used semi-supervised learning using multilingual BERT for user-level dialect identification on the MADAR Shared Task. Arabic Tranformers-based approaches (Antoun et al., 2020; Safaya et al., 2020) showed competitive results in NADI (Abdul-Mageed et al., 2020) Shared Task. 3 • All Arab country names written in either Arabic, English, or French,4 such as H. QªÖ Ï @ (Almgrb – Morocco), Morocco, and Maroc respectively. • The names of major cities in these countries in both Arabic and English as specified in  (Alqds – Jerusalem) Wikipedia,5 such as Y®Ë@ and à@ Qëð (whrAn – Oran, Algeria). • Arabic adjectives specifying all nationalities in both masculine and feminine forms with and without the definite article È@ (Al – the) such as ú ¯@ Q« (ErAqy - Iraqi (m.)), éJ"
2021.wanlp-1.1,P13-2081,0,0.0694626,"Missing"
2021.wanlp-1.1,P18-1031,0,0.0182863,"ة ز ّٰ ء ي ا وي ة ش ا را ل ا ي ء ا ا ا ا او ل وى د ادا ك اك ره ى ذو وة د ا ا ن ن خ PL م ا ول ن ت رض ف ي وي ا ازاي د ل إ ا د ك JO زول دم ا ّٰ ا ذ ر د ه ا SD زى دا را ن ن EG داك را ا TN ش را ا ا LY د ل ر أ ه MA ا رو د ة ا ن ا ى ا ر Also in other dialects Figure 3: Highest valence words for each country. contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), to name but a few, have achieved ground-breaking results in many NLP classification and language understanding tasks. Both mBERT and AraBERT are pre-trained on identical architectures, namely an encoder with 12 Transformer blocks, hidden size of 768, and 12 selfattention heads. However, they differ in one major way. While mBERT is pre-trained on Wikipedia text for 104 languages,AraBERT is trained on a large Arabic news corpus containing 8.5M articles composed of roughly 2.5B tokens. For consistency with mBERT, we used AraBERT with BP. Following Devlin e"
2021.wanlp-1.1,W16-4818,0,0.0376284,"Missing"
2021.wanlp-1.1,W19-4622,1,0.872945,"transcends geographical regions and borders. Automatically distinguishing between the different dialectal variations is valuable for many downstream applications such as machine translations (Diab et al., 2014), POS tagging (Darwish et al., 2020), geo-locating users, and author profiling (Sadat et al., 2014). Though there has been prior work on performing Arabic Dialect Identification (ADI), much of the work was conducted on datasets with significant limitations in terms of genre (Bouamor et al., 2018; Zaidan and Callison-Burch, 2011), number of dialects (Abdul-Mageed et al., 2018), or focus (Bouamor et al., 2019; Zaghouani and Charfi, 2018a), where often the focus was on geo-locating and profiling users as opposed to dialect identification. In this work, we expand beyond these efforts by utilizing tweets from across the MENA region to build a large, non-genre specific, fine-grained, and balanced country-level dialectal Arabic dataset that we use to build effective Arabic Dialect Identification. We rely on two main features to build the dataset. The first feature is the Twitter user profile description, where we identify users who self-declare themselves as belonging to a specific country in different"
2021.wanlp-1.1,W16-4801,0,0.0614661,"Missing"
2021.wanlp-1.1,J14-1006,0,0.0502226,"Missing"
2021.wanlp-1.1,W19-4637,0,0.0331781,"Missing"
2021.wanlp-1.1,W14-3601,1,0.826287,"Missing"
2021.wanlp-1.1,W17-3008,1,0.716641,"Missing"
2021.wanlp-1.1,W14-5904,0,0.0630664,"Missing"
2021.wanlp-1.1,2020.semeval-1.271,0,0.0745377,"Missing"
2021.wanlp-1.1,C18-1113,0,0.0161785,"gorithm treats each dialect as a singleton cluster at the outset and then successively merges (or agglomerates) clusters until all clusters have been merged into a single cluster that contains all dialects. Figure 4 shows the results of hierarchical clustering. The figure reflects the similarity and the geographical proximity of various dialects. At higher levels, dialects are grouped per region, where we can identify the major dialectal groups, namely Gulf, Maghrebi, Egyptian, and Levantine. This is aligned with geographical distribution of the dialects as well as the findings of prior work (Salameh et al., 2018). Corpus Statistics and Analysis Upon constructing the dataset, we attempted to explore its characteristics. First, we extracted features that are distinctive for each dialect. To do so, we computed the so-called valence score for each word in each dialect (Conover et al., 2011). The score helps determine the distinctiveness of a given word in a specific dialect in reference to other dialects. Given N (t, Di ), which is the frequency of the term t in Dialect Di , valence is computed as follows: V (t)i = N (t,Di ) N (Di ) 2 P N (t,D n) n N (Dn ) −1 (1) Where N (Di ) is the total number of occur"
2021.wanlp-1.1,K17-1043,1,0.906165,"Missing"
2021.wanlp-1.1,L18-1111,0,0.119407,"l regions and borders. Automatically distinguishing between the different dialectal variations is valuable for many downstream applications such as machine translations (Diab et al., 2014), POS tagging (Darwish et al., 2020), geo-locating users, and author profiling (Sadat et al., 2014). Though there has been prior work on performing Arabic Dialect Identification (ADI), much of the work was conducted on datasets with significant limitations in terms of genre (Bouamor et al., 2018; Zaidan and Callison-Burch, 2011), number of dialects (Abdul-Mageed et al., 2018), or focus (Bouamor et al., 2019; Zaghouani and Charfi, 2018a), where often the focus was on geo-locating and profiling users as opposed to dialect identification. In this work, we expand beyond these efforts by utilizing tweets from across the MENA region to build a large, non-genre specific, fine-grained, and balanced country-level dialectal Arabic dataset that we use to build effective Arabic Dialect Identification. We rely on two main features to build the dataset. The first feature is the Twitter user profile description, where we identify users who self-declare themselves as belonging to a specific country in different forms such as showing signs"
2021.wanlp-1.1,P11-2007,0,0.0826773,"Missing"
2021.wanlp-1.13,N16-3003,1,0.802265,"“prosti“doorman”) or tute”), and Q« (“ErS” – “pimp”). Figure 5 shows the top words with the highest valance scores for individual words in the offensive tweets. Larger fonts are used to highlight words with highest scores and align as well with the categories mentioned in the breakdown for the offensive languages. We slightly modified the valence score described by (Conover et al., 2011) to magnify its value by multiplying valence with frequency of occurrence. Data Pre-processing We performed several text pre-processing steps. First, we tokenized the text using the Farasa Arabic NLP toolkit (Abdelali et al., 2016). Second, we removed URLs, numbers, and all tweet specific tokens, namely mentions, retweets, and hashtags as they are not part of the language semantic structure, and therefore, not usable in pre-trained embeddings. Third, we performed basic Arabic letter normalization, namely variants of the letter alef to bare alef, ta marbouta to ha, and alef maqsoura to ya. We also separated words that are commonly incorrectly attached such as I . Ê¿ AK (“yAklb” – “O dog”), is split to I . Ê¿ AK (“yA klb”). Lastly, we normalized letter repetitions to allow for a maximum of 2 repeated letters. For example,"
2021.wanlp-1.13,W19-4621,0,0.0609864,"tive polarity and terms with negative polarity in tweets as features. 131 Static Embeddings We experimented with various static embeddings that were pre-trained on different corpora with different vector dimensionality. We compared pre-trained embeddings to embeddings that were trained on our dataset. For pre-trained embeddings, we used: fastText Egyptian Arabic pre-trained embeddings (Bojanowski et al., 2017) with vector dimensionality of 300; AraVec skip-gram embeddings (Mohammad et al., 2017), trained on 66.9M Arabic tweets with 100dimensional vectors; and Mazajak skip-gram embeddings (Abu Farha and Magdy, 2019), trained on 250M Arabic tweets with 300-dimensional vectors. Sentence embeddings were calculated by taking the mean of the embeddings of their tokens. The importance of testing a character level n-gram model like fastText lies in the agglutinative nature of the Arabic language. We trained a new fastText text classification model (Joulin et al., 2017) on our dataset with vectors of 40 dimensions, 0.5 learning rate, 2−10 character n-grams as features, for 30 epochs. These hyper-parameters were tuned using a 5-fold cross-validated grid-search. Deep Contextualized Embeddings We also experimented"
2021.wanlp-1.13,2020.osact-1.2,0,0.0282173,"to establish strong Arabic offensive language classification results. Though offensive tweets have finer-grained labels where offensive tweet could also be vulgar and/or hate speech, we conducted coarser-grained classification to determine if a tweet was offensive or not. For classification, we experimented with several tweet representation and classification models. For tweet representations, we used: the count of positive and negative terms, based on a polarity lexicon; static embeddings, namely fastText and SkipGram; and deep contextual embeddings, namely BERTbase-multilingual and AraBERT (Antoun et al., 2020). and 4.1 éJ ë@X ú ¯ hðP (“rwH fy dAhyp” – equivalent to “go to hell”). Name alteration: One common way to insult others is to change a letter or two in their names to produce new offensive words that rhyme with the original names. Some such examples include chang ing èQK Qm .Ì &apos;@ (“Aljzyrp” – “Aljazeera (channel)”) to èQK Q  m Ì &apos;@ (“Alxnzyrp” – “the pig”) and àA ®Ê g (“xl ¯Q k (“xrfAn” fAn” – “Khalfan (person name)”) to àA – “crazed”). Societal stratification: Some insults are associated with: certain jobs such as H . @ñK. (“bwAb” – ÐXAg (“xAdm” – “servant”); and specific societal compon"
2021.wanlp-1.13,Q17-1010,0,0.0340608,"used NileULex (El-Beltagy, 2016), which is an Arabic polarity lexicon containing 3,279 MSA and 2,674 Egyptian terms, out of which 4,256 are negative and 1,697 are positive. We used the counts of terms with positive polarity and terms with negative polarity in tweets as features. 131 Static Embeddings We experimented with various static embeddings that were pre-trained on different corpora with different vector dimensionality. We compared pre-trained embeddings to embeddings that were trained on our dataset. For pre-trained embeddings, we used: fastText Egyptian Arabic pre-trained embeddings (Bojanowski et al., 2017) with vector dimensionality of 300; AraVec skip-gram embeddings (Mohammad et al., 2017), trained on 66.9M Arabic tweets with 100dimensional vectors; and Mazajak skip-gram embeddings (Abu Farha and Magdy, 2019), trained on 250M Arabic tweets with 300-dimensional vectors. Sentence embeddings were calculated by taking the mean of the embeddings of their tokens. The importance of testing a character level n-gram model like fastText lies in the agglutinative nature of the Arabic language. We trained a new fastText text classification model (Joulin et al., 2017) on our dataset with vectors of 40 dim"
2021.wanlp-1.13,N19-1423,0,0.0375937,"m model like fastText lies in the agglutinative nature of the Arabic language. We trained a new fastText text classification model (Joulin et al., 2017) on our dataset with vectors of 40 dimensions, 0.5 learning rate, 2−10 character n-grams as features, for 30 epochs. These hyper-parameters were tuned using a 5-fold cross-validated grid-search. Deep Contextualized Embeddings We also experimented with pre-trained contextualized embeddings with fine-tuning for down-stream tasks. Recently, deep contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), have achieved ground-breaking results in many NLP classification and language understanding tasks. In this paper, we fine-tuned BERTbase-multilingual (or simply BERT) and AraBERT embeddings to classify Arabic offensive language on Twitter as it eliminates the need for feature engineering. Although Robustly Optimized BERT (RoBERTa) embeddings perform better than (BERTlarge ) on GLUE (Wang et al., 2018), RACE (Lai et al., 2017), and SQuAD (Rajpurkar et al., 2016) tasks, pre-trained multilingual RoBERTa models are not avail"
2021.wanlp-1.13,L16-1463,0,0.0294886,"ya. We also separated words that are commonly incorrectly attached such as I . Ê¿ AK (“yAklb” – “O dog”), is split to I . Ê¿ AK (“yA klb”). Lastly, we normalized letter repetitions to allow for a maximum of 2 repeated letters. For example, the token éêêêë (“hhhhh” – “hahahahaha”) is normalized to éë (“hh”). We also removed Arabic diacritics and word elongations (kashida). 4.2 Representations Lexical Features Since offensive words typically have a negative polarity, we wanted to test the effectiveness of using a polarity lexicon in detecting offensive tweets. For the lexicon, we used NileULex (El-Beltagy, 2016), which is an Arabic polarity lexicon containing 3,279 MSA and 2,674 Egyptian terms, out of which 4,256 are negative and 1,697 are positive. We used the counts of terms with positive polarity and terms with negative polarity in tweets as features. 131 Static Embeddings We experimented with various static embeddings that were pre-trained on different corpora with different vector dimensionality. We compared pre-trained embeddings to embeddings that were trained on our dataset. For pre-trained embeddings, we used: fastText Egyptian Arabic pre-trained embeddings (Bojanowski et al., 2017) with vec"
2021.wanlp-1.13,P18-1031,0,0.0286332,"the agglutinative nature of the Arabic language. We trained a new fastText text classification model (Joulin et al., 2017) on our dataset with vectors of 40 dimensions, 0.5 learning rate, 2−10 character n-grams as features, for 30 epochs. These hyper-parameters were tuned using a 5-fold cross-validated grid-search. Deep Contextualized Embeddings We also experimented with pre-trained contextualized embeddings with fine-tuning for down-stream tasks. Recently, deep contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), have achieved ground-breaking results in many NLP classification and language understanding tasks. In this paper, we fine-tuned BERTbase-multilingual (or simply BERT) and AraBERT embeddings to classify Arabic offensive language on Twitter as it eliminates the need for feature engineering. Although Robustly Optimized BERT (RoBERTa) embeddings perform better than (BERTlarge ) on GLUE (Wang et al., 2018), RACE (Lai et al., 2017), and SQuAD (Rajpurkar et al., 2016) tasks, pre-trained multilingual RoBERTa models are not available. BERT is pre-trained on Wiki"
2021.wanlp-1.13,E17-2068,0,0.0822972,"an Arabic pre-trained embeddings (Bojanowski et al., 2017) with vector dimensionality of 300; AraVec skip-gram embeddings (Mohammad et al., 2017), trained on 66.9M Arabic tweets with 100dimensional vectors; and Mazajak skip-gram embeddings (Abu Farha and Magdy, 2019), trained on 250M Arabic tweets with 300-dimensional vectors. Sentence embeddings were calculated by taking the mean of the embeddings of their tokens. The importance of testing a character level n-gram model like fastText lies in the agglutinative nature of the Arabic language. We trained a new fastText text classification model (Joulin et al., 2017) on our dataset with vectors of 40 dimensions, 0.5 learning rate, 2−10 character n-grams as features, for 30 epochs. These hyper-parameters were tuned using a 5-fold cross-validated grid-search. Deep Contextualized Embeddings We also experimented with pre-trained contextualized embeddings with fine-tuning for down-stream tasks. Recently, deep contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), have achieved ground-breaking results in many NLP classifi"
2021.wanlp-1.13,malmasi-zampieri-2017-detecting,0,0.0532717,"Missing"
2021.wanlp-1.13,W17-3008,1,0.85101,"Missing"
2021.wanlp-1.13,D16-1264,0,0.0502861,"BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), have achieved ground-breaking results in many NLP classification and language understanding tasks. In this paper, we fine-tuned BERTbase-multilingual (or simply BERT) and AraBERT embeddings to classify Arabic offensive language on Twitter as it eliminates the need for feature engineering. Although Robustly Optimized BERT (RoBERTa) embeddings perform better than (BERTlarge ) on GLUE (Wang et al., 2018), RACE (Lai et al., 2017), and SQuAD (Rajpurkar et al., 2016) tasks, pre-trained multilingual RoBERTa models are not available. BERT is pre-trained on Wikipedia text from 104 languages, and AraBERT is trained on a large Arabic news corpus containing 8.5M articles composed of roughly 2.5B tokens. Both use identical architectures and come with hundreds of millions of parameters. Both contain an encoder with 12 Transformer blocks, hidden size of 768, and 12 self-attention heads. These embedding use BP sub-word segments. Following Devlin et al. (2019), the classification consists of introducing a dense layer over the final hidden state h corresponding to fi"
2021.wanlp-1.13,K17-1043,1,0.897502,"Missing"
2021.wanlp-1.13,W18-5446,0,0.0297172,". Recently, deep contextualized language models such as BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019), UMLFIT (Howard and Ruder, 2018), and OpenAI GPT (Radford et al., 2018), have achieved ground-breaking results in many NLP classification and language understanding tasks. In this paper, we fine-tuned BERTbase-multilingual (or simply BERT) and AraBERT embeddings to classify Arabic offensive language on Twitter as it eliminates the need for feature engineering. Although Robustly Optimized BERT (RoBERTa) embeddings perform better than (BERTlarge ) on GLUE (Wang et al., 2018), RACE (Lai et al., 2017), and SQuAD (Rajpurkar et al., 2016) tasks, pre-trained multilingual RoBERTa models are not available. BERT is pre-trained on Wikipedia text from 104 languages, and AraBERT is trained on a large Arabic news corpus containing 8.5M articles composed of roughly 2.5B tokens. Both use identical architectures and come with hundreds of millions of parameters. Both contain an encoder with 12 Transformer blocks, hidden size of 768, and 12 self-attention heads. These embedding use BP sub-word segments. Following Devlin et al. (2019), the classification consists of introducing a"
2021.wanlp-1.13,N16-2013,0,0.0892398,"Missing"
2021.wanlp-1.13,S19-2010,0,0.10723,"Missing"
abdelali-etal-2014-amara,J93-1004,0,\N,Missing
abdelali-etal-2014-amara,tiedemann-2008-synchronizing,0,\N,Missing
abdelali-etal-2014-amara,J93-2003,0,\N,Missing
abdelali-etal-2014-amara,P02-1040,0,\N,Missing
abdelali-etal-2014-amara,P11-1105,0,\N,Missing
abdelali-etal-2014-amara,P13-2003,1,\N,Missing
abdelali-etal-2014-amara,P07-2045,0,\N,Missing
abdelali-etal-2014-amara,N04-1022,0,\N,Missing
abdelali-etal-2014-amara,N03-1017,0,\N,Missing
abdelali-etal-2014-amara,P12-1016,0,\N,Missing
abdelali-etal-2014-amara,2013.iwslt-papers.2,1,\N,Missing
abdelali-etal-2014-amara,tiedemann-2012-parallel,0,\N,Missing
abdelali-etal-2014-amara,W11-2123,0,\N,Missing
abdelali-etal-2014-amara,D11-1125,0,\N,Missing
abdelali-etal-2014-amara,2012.eamt-1.60,0,\N,Missing
abdelali-etal-2014-amara,C12-1121,1,\N,Missing
abdelali-etal-2014-amara,2010.iwslt-evaluation.1,0,\N,Missing
C08-1017,J92-4003,0,0.0306992,"gle’. Here, ‘co’ + ‘mingle’ are likely to be more significant to the overall meaning than ‘coming’ + ‘le’ – in fact, the presence of the n-gram ‘coming’ could be misleading in this case. One way to model this would be to change the weighting scheme. The problem with this is that the weighting for one token has to be contingent on the weighting for another in the same term. Otherwise, in this example, the n-gram ‘coming’ would presumably receive a high weighting based on its frequency elsewhere in the corpus. An alternative is to select the tokenization which maximizes mutual information (MI). Brown et al. (1992) describe one application of MI to identify word collocations; Kashioka et al. (1998) describe another, based on MI of character n-grams, for morphological analysis of Japanese. The pointwise MI of a pair s1 and s2 as adjacent symbols is MI = log P(s1 s2) – log P(s1) – log P(s2) (3) If s1 follows s2 less often than expected on the basis of their independent frequencies, then MI is negative; otherwise, it is positive. In our application, we want to consider all candidate tokenizations, sum MI for each candidate, and rule out all but one candidate. A to132 kenization is a candidate if it exhaust"
C08-1017,P07-1110,1,0.868678,"Missing"
C08-1017,I08-6001,1,0.392093,"c of the present paper, implementations of the Singular Value Decomposition (SVD) (which is at the heart of LSA), and related algorithms such as PARAFAC2 (Harshman 1972), have become both more widely available and more powerful. SVD, for example, is available in both commercial off-the-shelf packages and at least one opensource implementation designed to run on a parallel cluster (Heroux et al. 2005). Despite these advances, there are (as yet) not fully surmounted obstacles to working with certain language pairs, particularly when the languages are not closely related. This is demonstrated in Chew and Abdelali (2008). At least in part, this has to do with the lexical statistics of the languages concerned. For example, because Arabic has a much richer morphological structure than English and French (meaning is varied through the addition of prefixes and suffixes rather than separate terms such as particles), it has a considerably higher type-to-token 129 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 129–136 Manchester, August 2008 ratio. Exactly this type of language-specific statistical variation seems to lead to difficulties for statistics-based techni"
C08-1017,W02-0506,0,0.0258343,"ich is clearly available in the training corpus be more fully leveraged without sacrificing efficiency? 3 Possible solutions 3.1 Replacing terms with n-grams At first glance, one might think that stemming would be an answer. Stemming has been shown to improve IR, in particular for morphologically complex languages (recent examples, including with Arabic, are Lavie et al. 2004 and Abdou et al. 2005). We are not aware, however, of any previous results that show unequivocally that stemming is beneficial specifically in CLIR. Chew and Abdelali (2008) examine the use of a light stemmer for Arabic (Darwish 2002), and while this does result in a small overall increase in overall precision, there is paradoxically no increase for Arabic. The problem may be that the approach for Arabic needs to be matched by a similar approach for other languages in the parallel corpus. However, since stemmers are usually tailored to particular languages – and may even be unavailable for some languages – use of existing stemmers may not always be an option. Another more obviously languageindependent approach is to replace terms with character n-grams3. This is feasible for more or less any language, regardless of script."
C08-1017,J01-2001,0,0.0533915,"her than terms. Whatever LMSA variant is used, the underlying approach to morphological tokenization is completely languageindependent. Example output is shown in Table 5 for wordforms from the Russian lemma ]^P_`abNMR_c ‘to crawl’, where the common stem (or at least an approximation thereof) is correctly identified. Wordform ]^P_`abNOdP`e_c ]^P_`abNOdL`L_c ]^P_`abNOdL`_c ]^P_`abNOdLf_c Tokenization ]^P_`abNO ]^P_`abNO ]^P_`abNO ]^P_`abNO dP`e_c dL`L_c dL`_c dLf_c Table 5. Examples of MI-based tokenization 5 Note that (4) is closely related to the ‘weighted mutual information’ measure used in Goldsmith (2001: 172). We do not directly test the accuracy of these tokenizations. Rather, measures of CLIR precision (described in section 4) indirectly validate our morphological tokenizations. 4 Testing framework To assess our results on a basis comparable with previous work, we used the same training and test data as used in Chew et al. (2007) and Chew and Abdelali (2008). The training data consists of the text of the Bible in 31,226 parallel chunks, corresponding generally to verses, in Arabic, English, French, Russian and Spanish. The test data is the text of the Quran in the same 5 languages, in 114"
C08-1017,P98-1108,0,0.0335636,"ng than ‘coming’ + ‘le’ – in fact, the presence of the n-gram ‘coming’ could be misleading in this case. One way to model this would be to change the weighting scheme. The problem with this is that the weighting for one token has to be contingent on the weighting for another in the same term. Otherwise, in this example, the n-gram ‘coming’ would presumably receive a high weighting based on its frequency elsewhere in the corpus. An alternative is to select the tokenization which maximizes mutual information (MI). Brown et al. (1992) describe one application of MI to identify word collocations; Kashioka et al. (1998) describe another, based on MI of character n-grams, for morphological analysis of Japanese. The pointwise MI of a pair s1 and s2 as adjacent symbols is MI = log P(s1 s2) – log P(s1) – log P(s2) (3) If s1 follows s2 less often than expected on the basis of their independent frequencies, then MI is negative; otherwise, it is positive. In our application, we want to consider all candidate tokenizations, sum MI for each candidate, and rule out all but one candidate. A to132 kenization is a candidate if it exhaustively parses the entire string and has no overlapping tokens. Thus, for ‘comingle’, c"
C08-1017,N03-1017,0,0.0024792,"al but possibly unattainable case, an IR algorithm would produce equally reliable results for any language pair: for example, a query in English would retrieve equally good results in Arabic as in French. A number of developments in recent years have brought that goal more within reach. One of the factors that severely hampered early attempts at machine translation, for example, was the lack of available computing power. However, Moore’s Law, the driving force of change in computing since then, has opened the way for recent progress in the field, such as Statistical Machine Translation (SMT) (Koehn et al. 2003). Even more closely related to the topic of the present paper, implementations of the Singular Value Decomposition (SVD) (which is at the heart of LSA), and related algorithms such as PARAFAC2 (Harshman 1972), have become both more widely available and more powerful. SVD, for example, is available in both commercial off-the-shelf packages and at least one opensource implementation designed to run on a parallel cluster (Heroux et al. 2005). Despite these advances, there are (as yet) not fully surmounted obstacles to working with certain language pairs, particularly when the languages are not cl"
C08-1017,2004.tmi-1.1,0,0.0144485,"tic languages like English. But, as previous results such as Chew and Abdelali’s (2008) show, for a language like Arabic, the adverse consequences of a morphology-blind approach are more severe. The question then is: how can information which is clearly available in the training corpus be more fully leveraged without sacrificing efficiency? 3 Possible solutions 3.1 Replacing terms with n-grams At first glance, one might think that stemming would be an answer. Stemming has been shown to improve IR, in particular for morphologically complex languages (recent examples, including with Arabic, are Lavie et al. 2004 and Abdou et al. 2005). We are not aware, however, of any previous results that show unequivocally that stemming is beneficial specifically in CLIR. Chew and Abdelali (2008) examine the use of a light stemmer for Arabic (Darwish 2002), and while this does result in a small overall increase in overall precision, there is paradoxically no increase for Arabic. The problem may be that the approach for Arabic needs to be matched by a similar approach for other languages in the parallel corpus. However, since stemmers are usually tailored to particular languages – and may even be unavailable for so"
C08-1017,C98-1104,0,\N,Missing
C16-1299,N16-3003,1,0.814437,"IWSLT (Cettolo et al., 2014). We used TED talks as our in-domain (≈ 177K sentences) corpus. For Arabic-to-English, we used the multiUN (≈ 3.7M sentences) (Eisele and Chen, 2010) as our out-domain corpora. For English-to-German, we used data made available (≈ 4.4M sentences) for the 9th Workshop on Machine Translation3 as our out-domain data. Language models were trained on all the available monolingual data (English: ≈ 287.3M and German: ≈ 59.5M sentences). Machine translation systems were tuned on concatenation of the dev- and test2010 and evaluated on test2011-2013 datasets. We used Farasa (Abdelali et al., 2016) to tokenize Arabic and the default Moses tokenizer for English-and German. All data was truecased. See Table 1 for data sizes. NN Training: The NNJM models were trained using the NPLM4 toolkit (Vaswani et al., 2013) with the following settings: a target context of 5 words and an aligned source window of 9 words. We restricted source and target side vocabularies to the 20K and 40K most frequent words in the in-domain data.5 The word vector size D and the hidden layer size were set to 150 and 750, respectively. Training was done using SGD with NCE using 100 noise samples and a mini-batch size o"
C16-1299,D11-1033,0,0.316495,"ata. The fine-tuning method first trains the NNJM on the concatenated data, then runs a few additional epochs on the in-domain data to tune the model towards in-domain. We found our method to outperform both the NDAM and finetuning methods. We also carried experiments against phrase-table weighting methods such as instance weighting (Sennrich, 2012), and phrase-table fill-up combination (Bisazza et al., 2011) and found our approach to outperform these. Our approach is complementary and the gains obtained were found to be additive on top of phrase-table adaptation and MML-based data-selection (Axelrod et al., 2011). The remainder of this paper is organized as follows. Section 2 briefly describes neural network joint model. Section 3 describes our fusion model for domain adaptation. Section 4 presents results and analysis. Section 5 gives an account on the related work and Section 6 concludes the paper. 2 Neural Network Joint Model Neural models are quickly becoming the state-of-the-art in machine translation. The ability to generalize and better capture non-local dependencies gives them edge over traditional models. The two most prevalent approaches are to use NNs as a feature inside SMT decoder (Vaswan"
C16-1299,2014.iwslt-evaluation.6,1,0.849612,"ne test-11 test-12 test-13 2437 1433 1700 993 51K 4K 28K 18K 48K 23K 26K 17K tune test-11 test-12 test-13 2456 1199 1702 1169 48K 21K 30K 26K 52K 24K 32K 28K Table 1: Statistics of the English-German and Arabic-English training corpora in terms of Sentences and Tokens (represented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concat"
C16-1299,2011.iwslt-evaluation.18,0,0.32717,". (2015) and the fine-tuning method of Luong and Manning (2015). The NDAM model uses data dependent regularization in the NNJM model to weight training instances, while training the model on the concatenated data. The fine-tuning method first trains the NNJM on the concatenated data, then runs a few additional epochs on the in-domain data to tune the model towards in-domain. We found our method to outperform both the NDAM and finetuning methods. We also carried experiments against phrase-table weighting methods such as instance weighting (Sennrich, 2012), and phrase-table fill-up combination (Bisazza et al., 2011) and found our approach to outperform these. Our approach is complementary and the gains obtained were found to be additive on top of phrase-table adaptation and MML-based data-selection (Axelrod et al., 2011). The remainder of this paper is organized as follows. Section 2 briefly describes neural network joint model. Section 3 describes our fusion model for domain adaptation. Section 4 presents results and analysis. Section 5 gives an account on the related work and Section 6 concludes the paper. 2 Neural Network Joint Model Neural models are quickly becoming the state-of-the-art in machine t"
C16-1299,P13-1141,0,0.0183407,"r combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The model is learned by fusing in- and out-domain NNJM models into a composite model by adjusting their parameters in favor of the in-domain data. When used as a feature during decoding, our model obtains statistically significant improvements on top of a competition grade phrase-based baseline system. We also showed improvements compared to previous adaptation methods. Further gains were obtained when our models were combined with existing methods. Although t"
C16-1299,P13-1126,0,0.0143517,"complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The mod"
C16-1299,N12-1047,0,0.0163528,"ions training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we either replaced it with the NDAM models trained on weighted concatenation, or with our fusion models (NFM*), where models are trained independently and adjusted towards in-domain data or by interpolating them linearly (EM-weighting) or log-linearly. We also tried th"
C16-1299,P14-1129,0,0.551242,"n to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this paper, we propose a deep fusion approach to domain adaptation for SMT. We use the Neural Network Joint Model (NNJM) proposed by Devlin et al. (2014) as our base model. However, rather than training the model on a plain concatenation of in- and out-domain data or a weighted concatenation (Joty et al., 2015), we first train in- and out-domain NNJM models, and then learn a composite model by readjusting their parameters through backpropagating errors from the output layer to the word embedding layer of each model. The intuition behind learning the models separately, is to learn in-domain model parameters without contaminating them with the out-domain data. In a variant of our model, we restrict backpropagation to only the outermost hidden la"
C16-1299,P13-2119,0,0.55228,"set from the out-domain data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this paper, we propose a deep fusion approach to domain adaptation for SMT. We use the Neural Network Joint Model (NNJM) proposed by Devlin et al. (2014) as our base model. However, rather than training the model on a plain concatenation of in- and out-domain data or a weighted concatenation (Joty et al., 2015), we first train in- and out-domain NNJM models, and then learn a composite model by readju"
C16-1299,P13-2071,1,0.844807,"ented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we either replaced it with the NDAM models trained on weighted concatenation, or with our fusion models (NFM*), where models are trained independently and adjusted towards in-domain data or by inte"
C16-1299,E14-4029,1,0.842113,"tences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we either replaced it with the NDAM models trained on weighted concatenation, or with our fusion models (NFM*), where models are trained independently and adjusted towards in-domain data or by interpolating them linearly (EM-weighting) or log-linearly. We also tried the approach of Luong and Manning (2015) by Fine Tuning baseline model towards in-domain data (i.e., by trainin"
C16-1299,2015.mtsummit-papers.10,1,0.787326,"n data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this paper, we propose a deep fusion approach to domain adaptation for SMT. We use the Neural Network Joint Model (NNJM) proposed by Devlin et al. (2014) as our base model. However, rather than training the model on a plain concatenation of in- and out-domain data or a weighted concatenation (Joty et al., 2015), we first train in- and out-domain NNJM models, and then learn a composite model by readjusting their parameters thr"
C16-1299,N13-1073,0,0.0253079,"1 test-12 test-13 2456 1199 1702 1169 48K 21K 30K 26K 52K 24K 32K 28K Table 1: Statistics of the English-German and Arabic-English training corpora in terms of Sentences and Tokens (represented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we eit"
C16-1299,P12-2023,0,0.0239499,". Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The model is learned by fusing in- and out-domain NNJM models into a composite model by adjusting their parameters in favor of the in-domain data. When used as a feature during decoding, our model obtains statistically significant improvements on top of a competition grade phrase-based baseline system. We"
C16-1299,eisele-chen-2010-multiun,0,0.0113535,"fusion and linear interpolation have the same number of parameters, which is the sum of the size of the base models. In fusion, we readjust all the parameters of the base models (or just the output layer weights in fusion-II), where in linear interpolation, we only learn their mixing weight. 4 Experiments Data: We experimented with the data made available for the translation task of the International Workshop on Spoken Language Translation IWSLT (Cettolo et al., 2014). We used TED talks as our in-domain (≈ 177K sentences) corpus. For Arabic-to-English, we used the multiUN (≈ 3.7M sentences) (Eisele and Chen, 2010) as our out-domain corpora. For English-to-German, we used data made available (≈ 4.4M sentences) for the 9th Workshop on Machine Translation3 as our out-domain data. Language models were trained on all the available monolingual data (English: ≈ 287.3M and German: ≈ 59.5M sentences). Machine translation systems were tuned on concatenation of the dev- and test2010 and evaluated on test2011-2013 datasets. We used Farasa (Abdelali et al., 2016) to tokenize Arabic and the default Moses tokenizer for English-and German. All data was truecased. See Table 1 for data sizes. NN Training: The NNJM model"
C16-1299,W08-0334,0,0.0309179,"uence model and NNJM models. An alternative to completely filtering out less useful data is to minimize its effect by down-weighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al."
C16-1299,W07-0717,0,0.2591,"´an et al., 2013). This is because of the difference in stylistic variations, vocabulary choices and word sense ambiguities across genres. Domain adaptation aims at finding the optimal point, that maximizes on the useful information available in the out-domain data, in favor of the in-domain data, while preventing it from degrading the performance of the system. This is either done by selecting a subset from the out-domain data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this"
C16-1299,W09-0439,0,0.0335122,"less useful data is to minimize its effect by down-weighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion"
C16-1299,D10-1044,0,0.0854332,"ame. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015) performed data selection using operation sequence model and NNJM models. An alternative to completely filtering out less useful data is to minimize its effect by down-weighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation"
C16-1299,D08-1089,0,0.0282,"lish training corpora in terms of Sentences and Tokens (represented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we either replaced it with the NDAM models trained on weighted concatenation, or with our fusion models (NFM*), where models are trained in"
C16-1299,2013.iwslt-papers.2,1,0.90169,"Missing"
C16-1299,E14-1035,0,0.0175463,"e carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The model is learned by fusing in- and out-domain NNJM models into a composite model by adjusting their parameters in favor of the in-domain data. When used as a feature during decoding, our model obtains statistically significant improvements on top of a competition grade phrase-based baseline system. We also showed improveme"
C16-1299,W11-2123,0,0.00997717,"Statistics of the English-German and Arabic-English training corpora in terms of Sentences and Tokens (represented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we either replaced it with the NDAM models trained on weighted concatenation, or w"
C16-1299,2005.eamt-1.19,0,0.0559647,".3 in English-German and Arabic-English, respectively). 5 Related Work Previous work on domain adaptation in MT can be broken down broadly into two main categories namely data selection and model adaptation. Data selection has shown to be an effective way to discard poor quality or irrelevant training instances, which when included in an MT system, hurts its performance. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015) performed data selection using operation sequence model and NNJM models. An alternative to completely filtering out less useful data is to m"
C16-1299,C14-1182,0,0.336565,"Missing"
C16-1299,D15-1147,1,0.338038,"ukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this paper, we propose a deep fusion approach to domain adaptation for SMT. We use the Neural Network Joint Model (NNJM) proposed by Devlin et al. (2014) as our base model. However, rather than training the model on a plain concatenation of in- and out-domain data or a weighted concatenation (Joty et al., 2015), we first train in- and out-domain NNJM models, and then learn a composite model by readjusting their parameters through backpropagating errors from the output layer"
C16-1299,P07-2045,0,0.0037253,"est Set Sent. TokEN TokDE Corpus Sent. TokAR TokEN tune test-11 test-12 test-13 2437 1433 1700 993 51K 4K 28K 18K 48K 23K 26K 17K tune test-11 test-12 test-13 2456 1199 1702 1169 48K 21K 30K 26K 52K 24K 32K 28K Table 1: Statistics of the English-German and Arabic-English training corpora in terms of Sentences and Tokens (represented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the d"
C16-1299,W04-3250,0,0.0940932,"tion: In Table 4 we experiment with MML-based filtering and probe whether our model can also improve on top of data selection. Firstly, selecting no out-domain data degrades the English-to-German system. On the contrary, the Arabic-to-English system substantially improves. This shows that general domain data is useful for English-to-German and much of the outdomain data (UN corpus) used in these experiments is harmful in the case of Arabic-to-English. In comparison, data selection was found to be less useful in the case of English-to-German. But we found 9 p < 0.05 using bootstrap resampling (Koehn, 2004), with 1000 samples. 3183 English-to-German Arabic-to-English System tst11 tst12 tst13 Avg tst11 tst12 tst13 Avg Baselinecat BaselineID 27.3 26.7 22.9 22.5 24.5 23.6 24.9 24.3 26.1 27.2 29.4 30.0 30.5 30.2 28.7 29.1 MML +NFM-I 26.9 27.6 22.9 23.1 24.4 25.0 24.7 25.2 27.4 27.6 30.8 31.2 30.9 31.1 29.7 30.0 Table 4: Comparing with MML (Axelrod et al 2011) that using our fusion model instead of baseline NNJM in either system still gave improvements ( +0.5 and +0.3 in English-German and Arabic-English, respectively). 5 Related Work Previous work on domain adaptation in MT can be broken down broadl"
C16-1299,P14-2093,0,0.0440254,"Missing"
C16-1299,2015.iwslt-evaluation.11,0,0.400393,"ails: http://creativecommons.org/licenses/by/4.0/ 3177 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3177–3187, Osaka, Japan, December 11-17 2016. We evaluated our model on a standard task of translating TED talks for English-to-German and Arabicto-English language pairs. Compared to baseline NNJM models trained on a concatenation of in- and out-domain data, our model achieves an average improvement of up to 0.9 BLEU points. The most relevant to our work are the NDAM model of Joty et al. (2015) and the fine-tuning method of Luong and Manning (2015). The NDAM model uses data dependent regularization in the NNJM model to weight training instances, while training the model on the concatenated data. The fine-tuning method first trains the NNJM on the concatenated data, then runs a few additional epochs on the in-domain data to tune the model towards in-domain. We found our method to outperform both the NDAM and finetuning methods. We also carried experiments against phrase-table weighting methods such as instance weighting (Sennrich, 2012), and phrase-table fill-up combination (Bisazza et al., 2011) and found our approach to outperform thes"
C16-1299,D15-1166,0,0.0656008,"ibes neural network joint model. Section 3 describes our fusion model for domain adaptation. Section 4 presents results and analysis. Section 5 gives an account on the related work and Section 6 concludes the paper. 2 Neural Network Joint Model Neural models are quickly becoming the state-of-the-art in machine translation. The ability to generalize and better capture non-local dependencies gives them edge over traditional models. The two most prevalent approaches are to use NNs as a feature inside SMT decoder (Vaswani et al., 2013; Devlin et al., 2014), or as an end-to-end translation system (Luong et al., 2015; Bahdanau et al., 2015; Sennrich et al., 2016) designed as fully trainable model of which every component is tuned based on training corpora to maximize its translation performance. Our work falls in the former category and extends NNJM. The NNJM model learns a feed-forward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) (1) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T an"
C16-1299,N13-1074,0,0.0203016,"takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et"
C16-1299,C14-1105,0,0.0141523,"Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The model is learned by fusing in- and out-domain NNJM models into a composite model by adjusting their parameters in favor of the in-domain data. When used as a feature during decoding, our model obtains statistically significant improvements on top of a competition grade phrase-based baseline system. We also showed improvements compared to previous adaptation methods. Further gains were obtained when our models"
C16-1299,D09-1074,0,0.478744,"anslation tasks such as translating TED talks (Cettolo et al., 2014), patents (Fujii et al., 2010) and educational content (Guzm´an et al., 2013). This is because of the difference in stylistic variations, vocabulary choices and word sense ambiguities across genres. Domain adaptation aims at finding the optimal point, that maximizes on the useful information available in the out-domain data, in favor of the in-domain data, while preventing it from degrading the performance of the system. This is either done by selecting a subset from the out-domain data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015"
C16-1299,P10-2041,0,0.291581,"translating TED talks (Cettolo et al., 2014), patents (Fujii et al., 2010) and educational content (Guzm´an et al., 2013). This is because of the difference in stylistic variations, vocabulary choices and word sense ambiguities across genres. Domain adaptation aims at finding the optimal point, that maximizes on the useful information available in the out-domain data, in favor of the in-domain data, while preventing it from degrading the performance of the system. This is either done by selecting a subset from the out-domain data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based a"
C16-1299,D09-1141,0,0.0170088,"els. An alternative to completely filtering out less useful data is to minimize its effect by down-weighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense dis"
C16-1299,2013.iwslt-evaluation.8,1,0.909079,"Missing"
C16-1299,P13-1082,0,0.0755351,"nterpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The model is learned by fusing in- and out-domain NNJM models into a composite model by adjusting their parameters in favor of the in-domain data. When used as a feature during decoding, our model obtains statistically significant improvements on top of a competition grade phrase-based baseline system. We also showed improvements compared to previous adaptation methods. Further gains were ob"
C16-1299,P16-1009,0,0.0306581,"describes our fusion model for domain adaptation. Section 4 presents results and analysis. Section 5 gives an account on the related work and Section 6 concludes the paper. 2 Neural Network Joint Model Neural models are quickly becoming the state-of-the-art in machine translation. The ability to generalize and better capture non-local dependencies gives them edge over traditional models. The two most prevalent approaches are to use NNs as a feature inside SMT decoder (Vaswani et al., 2013; Devlin et al., 2014), or as an end-to-end translation system (Luong et al., 2015; Bahdanau et al., 2015; Sennrich et al., 2016) designed as fully trainable model of which every component is tuned based on training corpora to maximize its translation performance. Our work falls in the former category and extends NNJM. The NNJM model learns a feed-forward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) (1) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. This is essentially an (m+n)-gram bilingua"
C16-1299,E12-1055,0,0.546508,"is because of the difference in stylistic variations, vocabulary choices and word sense ambiguities across genres. Domain adaptation aims at finding the optimal point, that maximizes on the useful information available in the out-domain data, in favor of the in-domain data, while preventing it from degrading the performance of the system. This is either done by selecting a subset from the out-domain data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this paper, we propos"
C16-1299,D13-1140,0,0.208513,"2011). The remainder of this paper is organized as follows. Section 2 briefly describes neural network joint model. Section 3 describes our fusion model for domain adaptation. Section 4 presents results and analysis. Section 5 gives an account on the related work and Section 6 concludes the paper. 2 Neural Network Joint Model Neural models are quickly becoming the state-of-the-art in machine translation. The ability to generalize and better capture non-local dependencies gives them edge over traditional models. The two most prevalent approaches are to use NNs as a feature inside SMT decoder (Vaswani et al., 2013; Devlin et al., 2014), or as an end-to-end translation system (Luong et al., 2015; Bahdanau et al., 2015; Sennrich et al., 2016) designed as fully trainable model of which every component is tuned based on training corpora to maximize its translation performance. Our work falls in the former category and extends NNJM. The NNJM model learns a feed-forward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) (1) i=1 where, si is an m-word"
D15-1147,abdelali-etal-2014-amara,1,0.184811,"Missing"
D15-1147,D13-1106,0,0.0123399,"rge to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . ."
D15-1147,D11-1033,0,0.0505111,"nces, which when included in an MT system, hurts its performance. The idea is to score the outdomain data using a model trained from the indomain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. An al"
D15-1147,2014.iwslt-evaluation.6,1,0.906521,"Missing"
D15-1147,2011.iwslt-evaluation.18,0,0.0228131,"etely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasl"
D15-1147,N13-1114,0,0.33049,"domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural networ"
D15-1147,P13-1126,0,0.513472,"domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural networ"
D15-1147,N12-1047,0,0.0255873,"sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. 5.3 Intrinsic Evaluation In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010). We choose Arabic-English language pair for the development experiments and train domain-wise models to measure the relatedness of each domain with respect to the in-domain. We later replicated selective experiments for the English-German language pair. The first part of Table 2 summarizes the results for Arabic-English. The perplexity numbers in the second column (NNJMb ) show that NEWS is the 1265 Doma"
D15-1147,P14-1129,0,0.286949,"the SMT pipeline, starting from corpus preparation to word-alignment, and then training a wide range of models opens a wide horizon to carry out domain specific adaptations. This is typically done using either data selection (Matsoukas et al., 2009) or model adaptation (Foster and Kuhn, 2007). In this paper, we further research in model adaptation using the neural network framework. In recent years, there has been a growing interest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems. A notably successful attempt on the SMT frontier was recently made by Devlin et al. (2014). They proposed a neural network joint model (NNJM), which augments streams of source with target n-grams and learns a NN model over vector representation of such streams. The model is then integrated into the decoder and used as an additional language model feature. Our aim in this paper is to advance the state-ofthe-art in SMT by extending NNJM for domain adaptation to leverage the huge amount of out1259 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1259–1270, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguist"
D15-1147,P13-2119,0,0.178533,"a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advanta"
D15-1147,E14-4029,1,0.0587384,"del (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. 5.3 Intrinsic Evaluation In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010). We choose Arabic-English language pair for the development experiments and train domain-wise models to measure the relatedness of each domain with respect to the in-domain. We later replicated selective experiments for the English-German language pair. The first part of Table 2 summarizes the results for Arabic-English. The perplexity numbers in the s"
D15-1147,P13-1141,0,0.0984619,"ture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In r"
D15-1147,2015.mtsummit-papers.10,1,0.422186,"ensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting"
D15-1147,N13-1073,0,0.0372083,"9/125 IWSLT CC NEWS EP 177K 2.3M 200K 1.8M Tok. 3.5/3.3 57/53 2.8/3.4 51/48 Table 1: Statistics of the Arabic-English and English-German training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) f"
D15-1147,P12-2023,0,0.0241725,"2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss"
D15-1147,eisele-chen-2010-multiun,0,0.0290387,"Missing"
D15-1147,W08-0334,0,0.257478,"that finding an optimal cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptati"
D15-1147,W07-0717,0,0.0175294,"taken from out-domain data completely changes the meaning of the sentence. In this paper, we tackle this problem by proposing domain adaptation models that make use of all the data while preserving the in-domain preferences. A significant amount of research has been carried out recently in domain adaptation. The complexity of the SMT pipeline, starting from corpus preparation to word-alignment, and then training a wide range of models opens a wide horizon to carry out domain specific adaptations. This is typically done using either data selection (Matsoukas et al., 2009) or model adaptation (Foster and Kuhn, 2007). In this paper, we further research in model adaptation using the neural network framework. In recent years, there has been a growing interest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems. A notably successful attempt on the SMT frontier was recently made by Devlin et al. (2014). They proposed a neural network joint model (NNJM), which augments streams of source with target n-grams and learns a NN model over vector representation of such streams. The model is then integrated into the decoder and used as an additional language model feature. Our a"
D15-1147,W09-0439,0,0.181637,"An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidel"
D15-1147,D10-1044,0,0.0298816,"Missing"
D15-1147,D08-1089,0,0.134778,"training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used"
D15-1147,P14-1066,0,0.0175969,"s. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) defines the transfo"
D15-1147,P12-1016,0,0.0157674,"al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. 5.3 Intrinsic Evaluation In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010). We choose Arabic-English language pair for the development experiments and train domain-wise models to measure the"
D15-1147,2013.iwslt-papers.2,1,0.883493,"Missing"
D15-1147,E14-1035,0,0.111537,"2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptati"
D15-1147,W11-2123,0,0.0263896,"stics of the Arabic-English and English-German training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transl"
D15-1147,2005.eamt-1.19,1,0.289763,"o be an effective way to discard poor quality or irrelevant training instances, which when included in an MT system, hurts its performance. The idea is to score the outdomain data using a model trained from the indomain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that"
D15-1147,C14-1182,0,0.475615,"Missing"
D15-1147,D13-1176,0,0.0524999,"-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) d"
D15-1147,P07-2045,0,0.00828985,"are/nplm/ Corpus AR-EN Sent. IWSLT QED NEWS UN 150k 150k 203k 3.7M Tok. Corpus EN-DE Sent. 2.8/3.0 1.4/1.5 5.6/6.3 129/125 IWSLT CC NEWS EP 177K 2.3M 200K 1.8M Tok. 3.5/3.3 57/53 2.8/3.4 51/48 Table 1: Statistics of the Arabic-English and English-German training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and"
D15-1147,P14-2093,0,0.196766,"Missing"
D15-1147,C12-2104,0,0.0153129,"lized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) defines the transformations of xn"
D15-1147,N13-1074,0,0.197071,"It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Ma"
D15-1147,P13-1082,0,0.126125,"n (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data witho"
D15-1147,C14-1105,0,0.147949,"3). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability"
D15-1147,E12-1055,0,0.100976,"ss useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014),"
D15-1147,D09-1074,0,0.161736,"ice overload”. The sense of the Arabic phrase taken from out-domain data completely changes the meaning of the sentence. In this paper, we tackle this problem by proposing domain adaptation models that make use of all the data while preserving the in-domain preferences. A significant amount of research has been carried out recently in domain adaptation. The complexity of the SMT pipeline, starting from corpus preparation to word-alignment, and then training a wide range of models opens a wide horizon to carry out domain specific adaptations. This is typically done using either data selection (Matsoukas et al., 2009) or model adaptation (Foster and Kuhn, 2007). In this paper, we further research in model adaptation using the neural network framework. In recent years, there has been a growing interest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems. A notably successful attempt on the SMT frontier was recently made by Devlin et al. (2014). They proposed a neural network joint model (NNJM), which augments streams of source with target n-grams and learns a NN model over vector representation of such streams. The model is then integrated into the decoder and used as"
D15-1147,P13-1045,0,0.0308553,"s the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) defines the transformations of xn through the hidden layers, and wk are the weights from the last hidd"
D15-1147,N13-1090,0,0.072173,"al network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) defines the transformations of xn through the hidden layers, and wk are the weig"
D15-1147,D13-1140,0,0.283823,"(yn = k) is an indicator variable (i.e., ynk =1 when yn =k, otherwise 0). Optimization is performed using first-order online methods, such as stochastic gradient ascent (SGA) with standard backpropagation algorithm. Unfortunately, training NNLMs are impractically slow because for each training instance (xn , yn ), the softmax output layer (see Equation 2) needs to compute a summation over all words in the output vocabulary.2 Noise contrastive estimation or NCE (Gutmann and Hyv¨arinen, 2010) provides an efficient and stable way to avoid this repetitive computation as recently applied to NNLMs (Vaswani et al., 2013; Mnih and Teh, 2012). We can re-write Equation 2 as follows: P (yn = k|xn , θ) = σ(yn = k|xn , θ) Z(φ(xn ), W) (4) where σ(.) is the un-normalized score and Z(.) is the normalization factor. In NCE, we consider 2 This would take few weeks for a modern CPU machine to train a single NNJM model on the whole data. 1261 Look-up layer Hidden layer Output layer U Source token 1 W Source token 2 C π yn Source token 3 Target token 1 ψ ynm Target token 2 xn M φ(xn ) Figure 1: A simplified neural network joint model with noise contrastive loss, where we use 3-gram target words (i.e., 2-words history) an"
D15-1147,P10-2041,0,0.0346668,"rrelevant training instances, which when included in an MT system, hurts its performance. The idea is to score the outdomain data using a model trained from the indomain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time c"
D15-1147,D09-1141,0,0.149564,"cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not l"
D15-1147,P02-1040,0,0.103895,"he cross entropy by regularizing the loss function with respect to the in-domain model. The regularizer gives higher weight to the training instances that are similar to the in-domain data. Our second model takes a more conservative approach by additionally penalizing data instances similar to the out-domain data. We evaluate our models on the standard task of translating Arabic-English and English-German language pairs. Our adapted models achieve better perplexities (Chen and Goodman, 1999) than the models trained on in- and in+out-domain data. Improvements are also reflected in BLEU scores (Papineni et al., 2002) as we compare these models within the SMT pipeline. We obtain gains of up to 0.5 and 0.6 on Arabic-English and EnglishGerman pairs over a competitive baseline system. The remainder of this paper is organized as follows: Section 2 gives an account on related work. Section 3 revisits NNJM model and Section 4 discusses our models. Section 5 presents the experimental setup and the results. Section 6 concludes. 2 Related Work Previous work on domain adaptation in MT can be broken down broadly into two main categories namely data selection and model adaptation. 2.1 Data Selection Data selection has"
D15-1147,P13-2071,1,\N,Missing
D15-1147,P11-1105,1,\N,Missing
D15-1147,J15-2001,1,\N,Missing
D15-1147,W13-2212,1,\N,Missing
D15-1147,W14-3302,0,\N,Missing
D19-3037,W12-2301,0,0.0839234,"Missing"
D19-3037,N07-2014,0,0.106619,"Missing"
D19-3037,N16-3003,1,0.913237,"two varieties of Dialectal Arabic (DA), namely Moroccan (MA) and Tunisian (TN). Our system beats all previously reported SOTA results for the aforementioned varieties of Arabic. The underlying approach treats diacritic recovery as a translation problem, where a sequential encoder and a sequential decoder are employed with undiacritized characters as input and diacritized characters as output. The system is composed of four main componenets, namely: 1) a web application that efficiently handles concurrent user diacritization requests; 2) a text tokenization and cleaning module based on Farasa (Abdelali et al., 2016), a SOTA Arabic NLP toolkit; 3) Arabic variety identifier based on a fastText (Joulin et al., 2016), a deep learning classification toolkit, to properly ascertain the appropriate diacritization model; and 4) a Neural Machine Translation (NMT) based architecture, based on OpenNMT (Klein et al., 2017), to translate sequences of undiacritized characters to diacritized sequences. The contributions in this paper are: Short vowels, aka diacritics, are more often omitted when writing different varieties of Arabic including Modern Standard Arabic (MSA), Classical Arabic (CA), and Dialectal Arabic (DA)"
D19-3037,W17-1305,0,0.0467909,"Missing"
D19-3037,P17-4012,0,0.0277426,"er are employed with undiacritized characters as input and diacritized characters as output. The system is composed of four main componenets, namely: 1) a web application that efficiently handles concurrent user diacritization requests; 2) a text tokenization and cleaning module based on Farasa (Abdelali et al., 2016), a SOTA Arabic NLP toolkit; 3) Arabic variety identifier based on a fastText (Joulin et al., 2016), a deep learning classification toolkit, to properly ascertain the appropriate diacritization model; and 4) a Neural Machine Translation (NMT) based architecture, based on OpenNMT (Klein et al., 2017), to translate sequences of undiacritized characters to diacritized sequences. The contributions in this paper are: Short vowels, aka diacritics, are more often omitted when writing different varieties of Arabic including Modern Standard Arabic (MSA), Classical Arabic (CA), and Dialectal Arabic (DA). However, diacritics are required to properly pronounce words, which makes diacritic restoration (a.k.a. diacritization) essential for language learning and text-to-speech applications. In this paper, we present a system for diacritizing MSA, CA, and two varieties of DA, namely Moroccan and Tunisia"
D19-3037,W18-2507,0,0.0275433,"Missing"
D19-3037,D15-1274,0,0.0528553,"Missing"
D19-3037,D17-1151,0,0.0215157,"Missing"
D19-3037,N19-1248,1,0.599941,"ieties of Arabic Hamdy Mubarak Ahmed Abdelali Kareem Darwish Mohamed Eldesouki Younes Samih Hassan Sajjad {hmubarak,aabdelali}@qf.org.qa Qatar Computing Research Institute, HBKU Research Complex, Doha 5825, Qatar Abstract the diacritics, a prerequisite for Language Learning (Asadi, 2017) and Text to Speech (Sherif, 2018) among other applications. In this paper, we present a system that employs a character-based sequence-to-sequence model (seq2seq) (Britz et al., 2017; Cho et al., 2014; Kuchaiev et al., 2018) for diacritizing four different varieties of Arabic. We use the approach described by Mubarak et al. (2019), which they applied to MSA only, to build a system that effectively diacritizes MSA, CA, and and two varieties of Dialectal Arabic (DA), namely Moroccan (MA) and Tunisian (TN). Our system beats all previously reported SOTA results for the aforementioned varieties of Arabic. The underlying approach treats diacritic recovery as a translation problem, where a sequential encoder and a sequential decoder are employed with undiacritized characters as input and diacritized characters as output. The system is composed of four main componenets, namely: 1) a web application that efficiently handles con"
D19-3037,pasha-etal-2014-madamira,0,0.0626661,"Missing"
D19-3037,W17-1302,1,0.833503,"Missing"
D19-3037,W04-1612,0,0.189762,"Missing"
D19-3037,W02-0504,0,0.274384,"Missing"
darwish-etal-2014-using,N03-1033,0,\N,Missing
darwish-etal-2014-using,P03-1051,0,\N,Missing
darwish-etal-2014-using,E12-1069,0,\N,Missing
E17-3016,E14-4029,1,0.83174,"f the systems.3 The results shown in Figure 3 depict the significant time gain we achieved using the pruned phrase based system. However, with a 5 BLEU point difference in translation quality, we decided to compromise and use the slower NMTCPU in our final demo. We also allow the user to switch to the phrase-based system, if translation speed is more important. We did not use NMTGPU since it is very costly to put into production with its requirement for a dedicated GPU card. Finally, we added a customized dictionary and translated unknown words by transliterating them in a post-decoding step (Durrani et al., 2014). PB-Pruned: The PB-best system is not suitable for real time translation and has high memory requirements. To increase the efficiency, we dropped the OSM and NNJM features, heavily pruned the language model and used MML-filtering to select a subset of training data. The resulting system was trained on 1.2 M sentences, 10 times less the original data. 2.4 NMT-GPU: This is our best system2 that we submitted to the IWSLT’16 campaign (Durrani et al., 2016). The advantage of Neural models is that their size does not scale linearly with the data, and hence we were able to train using all available"
E17-3016,W16-2323,0,0.0666475,"Missing"
E17-3016,P14-1129,0,0.018282,"eamlessly switch between them. We had four systems to choose from for our demo, two of which were Phrase-based systems, and the two were Neural MT systems trained using Nematus (Sennrich et al., 2016). Figure 3: Performance and Translation speed of various MT systems PB-Best: This is a competition-grade phrasebased system, also used for our participation at the IWSLT’16 campaign (Durrani et al., 2016). It was trained using all the freely available ArabicEnglish data with state-of-the-art features such as a large language model, lexical reordering, OSM (Durrani et al., 2011) and NNJM features (Devlin et al., 2014). We also computed the translation speed of each of the systems.3 The results shown in Figure 3 depict the significant time gain we achieved using the pruned phrase based system. However, with a 5 BLEU point difference in translation quality, we decided to compromise and use the slower NMTCPU in our final demo. We also allow the user to switch to the phrase-based system, if translation speed is more important. We did not use NMTGPU since it is very costly to put into production with its requirement for a dedicated GPU card. Finally, we added a customized dictionary and translated unknown words"
E17-3016,P11-1105,1,\N,Missing
E17-3029,P09-1039,0,0.0167833,"Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in n"
E17-3029,N13-1008,1,0.77865,"Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in news https://github.com/andre-martins/ TurboParser 118 Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu Hoang. 2016. Is neural machine translation ready for deployment? A case study on 30 translation directions. CoRR, abs/1610.01108. documents are connected. 2.9 Storyline Construction and Summarization Storylines are co"
E17-3029,E17-3017,1,0.751559,"m a multilingual corpus of nearly 600k documents in 8 of the 9 SUMMA languages (all except Latvian), which were manually annotated by journalists at Deutsche Welle. The document model is a hierarchical attention network with attention at each level of the hierarchy, inspired by Yang et al. (2016), followed by a sigmoid classification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is"
E17-3029,P13-1020,0,0.025126,"Missing"
E17-3029,E17-1051,1,0.815701,"lassification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 K"
I08-6001,P07-1110,1,0.866207,"e ability to tokenize text at the boundaries between words, or more generally semantic units, it can be generalized to virtually all languages. 2.2 Training and test data For our experiments, the training and test data were taken from the Bible and Quran respectively. As training data, the Bible lends itself extremely well to multilingual LSA. It is highly available in multiple languages 1 (over 80 parallel translations in 50 languages, mostly public-domain, are available from a single website, www.unboundbible.org); and a very fine-grained alignment is possible (by verse) (Resnik et al 1999, Chew and Abdelali 2007). Many purpose-built parallel corpora are biased towards particular language groups (for example, the European Union funds work in CLIR, but it tends to be biased towards European languages – for example, see Peters 2001). This is not as true of the Bible, and the fact that it covers a wider range of languages is a reflection of the reasons it was translated in the first place. The question which is most commonly raised about use of the Bible in this way is whether its coverage of vocabulary from other domains is sufficient to allow it to be used as training data for most applications. Based o"
I08-6001,W06-1009,1,0.894388,"Missing"
I08-6001,W02-0506,0,0.108661,"ijsbergen 1979). obtain a vector of weighted frequencies of each However, since the aim of our experiments is to term in the document, then multiplying that vector assess whether we could identify the correct transby U × S-1, also as described above. The result was lation for a given document among a set of possia set of projected document vectors in the 300- bilities in another language (i.e., given the language of the query and the language of the results), dimensional LSA space. For some of our experiments, we used a light we selected ‘precision at 1 document’ as our prestemmer for Arabic (Darwish 2002) to replace in- ferred metric. This metric represents the proportion flected forms in the training data with citation of cases, on average, where the translation was reforms. It is commonly accepted that morphology trieved first. improves IR (Abdou et al. 2005, Lavie et al. 2004, Larkey et al. 2002, Oard and Gey 2002), and it will 3 Challenges of Semitic languages be seen that our results generally confirm this. The features which make Semitic languages chalFor Hebrew, we used the Westminster Lenin- lenging for information retrieval are generally grad Codex in the training data. Since this is"
I08-6001,2004.tmi-1.1,0,0.155244,"r a given document among a set of possia set of projected document vectors in the 300- bilities in another language (i.e., given the language of the query and the language of the results), dimensional LSA space. For some of our experiments, we used a light we selected ‘precision at 1 document’ as our prestemmer for Arabic (Darwish 2002) to replace in- ferred metric. This metric represents the proportion flected forms in the training data with citation of cases, on average, where the translation was reforms. It is commonly accepted that morphology trieved first. improves IR (Abdou et al. 2005, Lavie et al. 2004, Larkey et al. 2002, Oard and Gey 2002), and it will 3 Challenges of Semitic languages be seen that our results generally confirm this. The features which make Semitic languages chalFor Hebrew, we used the Westminster Lenin- lenging for information retrieval are generally grad Codex in the training data. Since this is avail- fairly well understood: it is probably fair to say able for download either with vowels or without that chief among them is their complex morpholvowels, no morphological pre-processing was re- ogy (for example, ambiguity resulting from diacriquired in this case; we simply"
K17-1043,N16-3003,1,0.913316,"suffixes for each dialect in comparison to MSA. As the tables show, MGR has the most number of prefixes, while GLF has the most number of suffixes. Further, there are certain prefixes and suffixes that are unique to dialects. While the prefix “Al” (the) leads the list of prefixes for all dialects, the prefix H . “b” in LEV and EGY, where it is either a progressive particle or a preposition, is used more frequently than in MSA, where it is used strictly as a preposition. Similarly, the suffix “kn” (your) is more frequent in LEV than any á» 5.1 We used the SVM-based ranking approach proposed by Abdelali et al. (2016), in which they used SVM based ranking to ascertain the best segmentation for Modern Standard Arabic (MSA), which they show to be fast and of high accuracy. The approach involves generating all possible segmentations of a word and then ranking them. The possible segmentations are generated based on possible prefixes and suffixes that are observed during training. For example, if hypothetically we only had the prefixes ð “w” (and) and È “l” (to) other dialect. The Negation suffix  “$” (not) and feminine suffix marker No. 8 11 11 14 19 Top 5 Al,w,l,b,f Al,b,w,m,h Al,b,w,l,E Al,w,b,l,mA Al,w,l,"
K17-1043,habash-etal-2012-conventional,0,0.0821494,"ained for each dialect and the number of words they contain. Dialect Egyptian Levantine Gulf Maghrebi No of Tokens 6,721 6,648 6,844 5,495 Table 1: Dataset size for the different dialects We manually segmented each word in the corpus while preserving the original characters. This decision was made to allow processing real dialectal words in their original form. Table 2 shows segmented examples from the different dialects. 3.1 Segmentation Convention In some research projects, segmentation of DA is done on a CODA’fied version of the text, where CODA is a standardized writing convention for DA (Habash et al., 2012). CODA guidelines provide directions on to how to normalize words, correct spelling and unify writing. Nonetheless, these guidelines are not available for all dialects. In the absence of such guidelines as well as the dynamic nature of the language, we choose to operate directly on the raw text. As in contrast to MSA, where guidelines for spelling are common and standardized, written DA seems to exhibit a lot of diversity, and hence, segmentation systems need to be robust enough to handle all the variants that might be encountered in such texts. Our segmentation convention is closer to stemmin"
K17-1043,W11-4417,1,0.839746,"Missing"
K17-1043,N13-1044,0,0.142459,"val. Though much work has focused on segmenting Modern Standard Arabic (MSA), recent work began to examine dialectal segmentation in some Arabic dialects. Dialectal segmentation is becoming increasingly important due to the ubiquity of social media, where users typically write in their own dialects as opposed to MSA. Dialectal text poses interesting challenges such as lack of spelling standards, pervasiveness of word merging, letter substitution or deletion, and foreign word borrowing. Existing work on dialectal segmentation focused on building resources and tools for each dialect separately (Habash et al., 2013; 2 Background Work on dialectal Arabic is fairly recent compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014; Eldesouki et al., 2016). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghrebi. Few resources for these dialects 432 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 432–441, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics are available such as the CALLHO"
K17-1043,W09-0807,0,0.0760766,"Missing"
K17-1043,bouamor-etal-2014-multidialectal,0,0.0679135,"Missing"
K17-1043,N16-1030,0,0.011125,"A+MSA). 5.2 Figure 2: Architecture of our proposed neural network Arabic segmentation model applied to the  word éJ.Ê¯ “qlbh” and output “qlb+h”. o and c are respectively the input gate, forget gate, output gate and cell activation vectors. More interpretation about this architecture can be found in (Graves and Schmidhuber, 2005) and(Lipton et al., 2015). Bi-LSTM-CRF Approach In this subsection we describe the different components of our Arabic segmentation bi-LSTMCRF based model, shown in Figure 2. It is a slight variant of the bi-LSTM-CRF architecture first proposed by Huang et al. (2015), Lample et al. (2016), and Ma and Hovy (2016) 5.2.1 Bi-LSTMs Another extension to the single LSTM networks are the bi-LSTMs (Schuster and Paliwal, 1997). They are also capable of learning long-term dependencies and maintain contextual features from both past and future states. As shown in Figure 2, they are comprised of two separate hidden layers that feed forwards to the same output layer. Recurrent Neural Networks A recurrent neural network (RNN) together with its variants, i.e. LSTM, bi-LSTM, GRU, belong to a family of powerful neural networks that are well suited for modeling sequential data. Over the last sev"
K17-1043,D14-1154,1,0.904598,"Missing"
K17-1043,W16-4828,1,0.82771,"social media, where users typically write in their own dialects as opposed to MSA. Dialectal text poses interesting challenges such as lack of spelling standards, pervasiveness of word merging, letter substitution or deletion, and foreign word borrowing. Existing work on dialectal segmentation focused on building resources and tools for each dialect separately (Habash et al., 2013; 2 Background Work on dialectal Arabic is fairly recent compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014; Eldesouki et al., 2016). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghrebi. Few resources for these dialects 432 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 432–441, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects"
K17-1043,P16-1101,0,0.0180724,"itecture of our proposed neural network Arabic segmentation model applied to the  word éJ.Ê¯ “qlbh” and output “qlb+h”. o and c are respectively the input gate, forget gate, output gate and cell activation vectors. More interpretation about this architecture can be found in (Graves and Schmidhuber, 2005) and(Lipton et al., 2015). Bi-LSTM-CRF Approach In this subsection we describe the different components of our Arabic segmentation bi-LSTMCRF based model, shown in Figure 2. It is a slight variant of the bi-LSTM-CRF architecture first proposed by Huang et al. (2015), Lample et al. (2016), and Ma and Hovy (2016) 5.2.1 Bi-LSTMs Another extension to the single LSTM networks are the bi-LSTMs (Schuster and Paliwal, 1997). They are also capable of learning long-term dependencies and maintain contextual features from both past and future states. As shown in Figure 2, they are comprised of two separate hidden layers that feed forwards to the same output layer. Recurrent Neural Networks A recurrent neural network (RNN) together with its variants, i.e. LSTM, bi-LSTM, GRU, belong to a family of powerful neural networks that are well suited for modeling sequential data. Over the last several years, they have ac"
K17-1043,maamouri-etal-2014-developing,0,0.0662827,"Missing"
K17-1043,mohamed-etal-2012-annotating,0,0.12055,"Missing"
K17-1043,P14-2034,0,0.0769493,"Missing"
K17-1043,W14-3601,1,0.936065,"Missing"
K17-1043,pasha-etal-2014-madamira,0,0.10814,"Missing"
K17-1043,W17-1306,1,0.586168,"Missing"
K17-1043,J14-1006,0,0.0520662,"important due to the ubiquity of social media, where users typically write in their own dialects as opposed to MSA. Dialectal text poses interesting challenges such as lack of spelling standards, pervasiveness of word merging, letter substitution or deletion, and foreign word borrowing. Existing work on dialectal segmentation focused on building resources and tools for each dialect separately (Habash et al., 2013; 2 Background Work on dialectal Arabic is fairly recent compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014; Eldesouki et al., 2016). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghrebi. Few resources for these dialects 432 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 432–441, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sente"
K17-1043,N12-1006,0,0.0492725,"oming increasingly important due to the ubiquity of social media, where users typically write in their own dialects as opposed to MSA. Dialectal text poses interesting challenges such as lack of spelling standards, pervasiveness of word merging, letter substitution or deletion, and foreign word borrowing. Existing work on dialectal segmentation focused on building resources and tools for each dialect separately (Habash et al., 2013; 2 Background Work on dialectal Arabic is fairly recent compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014; Eldesouki et al., 2016). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghrebi. Few resources for these dialects 432 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 432–441, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), wh"
L16-1054,W11-0705,0,0.156848,"Missing"
L16-1054,kirschenbaum-wintner-2010-general,0,0.0604887,"Missing"
L16-1054,P07-2045,0,0.00412632,"Missing"
L16-1054,W10-2404,0,0.0620325,"Missing"
L16-1054,W14-3601,1,0.875557,"Missing"
L16-1054,W15-3201,1,0.845594,"s available at http://alt. qcri.org/resources/TwitterAr2EnTranslit.tgz 9 Regions: Gulf (GLF), Egypt (EG), Levant (LEV), and Maghreb (MGR) Q»@ X (ZAkr)  @ (A$rf) ¬Qå ZAJ  (DyA’) ù®¢Ó (mSTfY) J ¯P (rfyq) ú G. QmÌ '@ (AlHrby) éJ Jë (hnyp) LEV Jamal Zaker Thaker Ashraf Achraf Diyaa Dhiyaa Mostafa Mostapha Rafik, Rafiq Rafig Rafic El Harby Al Harby Haniyya Haniyyeh Table 5: Samples of Arabic names that are transliterated differently according to regional dialectal variations. graphically, i.e. inferring a country or a region given only the full username written in Arabic on Latin characters (Mubarak and Darwish, 2015). 3.4. Transliteration Similarity Score Our hypothesis for name transliteration between N amearb and N ametrans needed a gauge to measure and quantify the similarity between them. Given a N amearb is transliterated using elaborate mapping scheme similar to Buckwalter transliteration. We took into consideration removing of name title, informal writings and dialectal variations, some characters are considered equivalent (ex: k=q, 353 gh=g, dh=d, sh= ch), vowels are removed from N amearb and N ametrans , and than similarity score is calculated using Levenshtein edit distance. For example, names"
L16-1054,P11-1044,0,0.0189049,"iNER), a large multilingual resource that is used for NE disambiguation, translation and transliteration. The resource contains lists of NEs with various sizes in 15 languages. They used triangulation cross languages to expand the initial lists. The size of the English list was 1.74 million entries. The numbers decrease sharply for nonWestern languages. Similarly, H´alek et, al. (2011) built a bilingual lexicons for English-Czech that was used to improve transliteration in a Statistical Machine Translation (SMT) task. Using the new mined resource improved the score with about 0.5 BLEU points. Sajjad et al. (2011; 2012) mined transliteration from parallel corpora to improve SMT system. Their unsupervised transliteration mining system uses a parallel corpus to generate a list of word pairs and filters transliteration pairs from that. The system will be retrained on the filtered dataset and this process is iterated several times until all transliteration word pairs were detected. The approach proved fruitful with a BLEU improvement of up to 0.4 points. Yoon et al. (2007) proposed a phonetic method for multilingual transliteration. The approach exploits the string alignment and linear classifiers that we"
L16-1054,P12-1049,0,0.0439081,"Missing"
L16-1054,W06-1630,0,0.032856,"o generate a list of word pairs and filters transliteration pairs from that. The system will be retrained on the filtered dataset and this process is iterated several times until all transliteration word pairs were detected. The approach proved fruitful with a BLEU improvement of up to 0.4 points. Yoon et al. (2007) proposed a phonetic method for multilingual transliteration. The approach exploits the string alignment and linear classifiers that were trained using the Winnow algorithm to learn transliteration characteristics. The results achieved were improved over earlier results reported by Tao et al. (2006). methods built using pure linguistic knowledge. Yoon et al. (2007) used Mean Reciprocal Rank (MRR) to measure the performance of the transliteration system tested on Arabic, Chinese, Hindi and Korean. The main challenges with former approaches is both unrobustness or dependability on scares resources that are not easy to find. Data collected from Twitter can expand rapidly and complement the resources in WK. 3. Collecting Names from Twitter When creating a new account on Twitter, user fills full name (in any characters; less than 20 characters), and an email. Twitter might suggest some user n"
L16-1054,wentland-etal-2008-building,0,0.0219146,"8 599.6 578.3 439.8 154.1 fr(k) de(k) es(k) ar(k) 907.2 469.6 397.1 133.1 857.4 340.6 120.9 699.1 136.8 233.2 his/her name. Hence, for our case-study, we proceed to collect full names written in Arabic with their transliterations using Twitter user ID (username field). Table 1: Statistics from WK using interwiki links for Named Entities translation/transliteration. 2. Related Work WK as a free multilingual encyclopedia, provides a valuable resource for parallel information that can be easily processed and deployed in cross-language Named Entity (NE) disambiguation, resolution and translation. Wentland et al. (2008) used WK to build Heidelberg NE Resource (HeiNER), a large multilingual resource that is used for NE disambiguation, translation and transliteration. The resource contains lists of NEs with various sizes in 15 languages. They used triangulation cross languages to expand the initial lists. The size of the English list was 1.74 million entries. The numbers decrease sharply for nonWestern languages. Similarly, H´alek et, al. (2011) built a bilingual lexicons for English-Czech that was used to improve transliteration in a Statistical Machine Translation (SMT) task. Using the new mined resource imp"
L16-1054,P07-1015,0,0.0283703,"teration in a Statistical Machine Translation (SMT) task. Using the new mined resource improved the score with about 0.5 BLEU points. Sajjad et al. (2011; 2012) mined transliteration from parallel corpora to improve SMT system. Their unsupervised transliteration mining system uses a parallel corpus to generate a list of word pairs and filters transliteration pairs from that. The system will be retrained on the filtered dataset and this process is iterated several times until all transliteration word pairs were detected. The approach proved fruitful with a BLEU improvement of up to 0.4 points. Yoon et al. (2007) proposed a phonetic method for multilingual transliteration. The approach exploits the string alignment and linear classifiers that were trained using the Winnow algorithm to learn transliteration characteristics. The results achieved were improved over earlier results reported by Tao et al. (2006). methods built using pure linguistic knowledge. Yoon et al. (2007) used Mean Reciprocal Rank (MRR) to measure the performance of the transliteration system tested on Arabic, Chinese, Hindi and Korean. The main challenges with former approaches is both unrobustness or dependability on scares resourc"
L18-1015,N16-3003,1,0.699404,"weet-specific POS tags Data Description Dialect Egyptian (EGY) Levantine (LEV) Gulf (GLF) Maghrebi (MGR) POS PROG PART https://catalog.ldc.upenn.edu/LDC2017T07 Buckwalter transliteration is used in the paper 94 tion is that MSA has more noun suffixes and grammatical case endings, while dialects have more progressive particles and negation suffixes. This variance is related more to the linguistic nature of the language rather than the genre. 4. 4.1. would be effective for dialects also, particularly given the overlap between MSA and dialectal Arabic. We used Farasa to determine stem templates (Abdelali et al., 2016). For all the experiments, we trained on the training and dev parts and tested on the test part. As mentioned earlier, we also randomly selected 350 MSA sentences from Arabic Penn Treebank (ATB) and treated MSA as a language variety. Doing so would allow us to observe the divergence of dialects from MSA and the relative effectiveness of using a small dataset compared to much more data. Experiments and Evaluation Experimental Setup For the experiments that we conducted, we used the CRF++ implementation of a CRF sequence labeler with L2 regularization and default value of 10 for the generalizati"
L18-1015,al-sabbagh-girju-2010-mining,0,0.0566271,"Missing"
L18-1015,bouamor-etal-2014-multidialectal,0,0.0463357,"Missing"
L18-1015,J92-4003,0,0.113623,"Missing"
L18-1015,cotterell-callison-burch-2014-multi,0,0.0466115,"Missing"
L18-1015,D14-1154,1,0.889503,"Missing"
L18-1015,W17-1316,1,0.769801,"ur justification for this noticeable disparity is that the POS distribution is affected by the genre. The MSA text is from the formal news domain with a special focus on facts and entities, while the dialects are informal expressions with a focus on events, attitudes, and conversations. Another observaThe words in the dataset were segmented in place without any modification or standardization attempts (ex. CODA (Habash et al., 2012)), and the segmentation guidelines aimed to generate a number of segments that match the correct number of POS tags for a word. We used the POS tagset described by Darwish et al. (2017) which has 18 tags for MSA POS tagging, and we added 2 dialect-specific tags (namely PROG PART, and NEG PART), and 4 tweet-specific tags (namely HASH, EMOT, MENTION, and URL). Table 1 contains description of the newly added tags5 . 4 Example I.JºJK . (bnktb) Segmentation and POS tagging were applied on the original raw text without any correction as suggested by Eldesouki et al. (2017) to overcome the need for standardization of different dialectal writings proposed in CODA by  ®J J.Óð Habash et al. (2012). For example the word ñËñ We used the dialectal Arabic dataset described by Eldesouk"
L18-1015,W05-0708,0,0.819563,"Missing"
L18-1015,P06-1086,0,0.314234,"Missing"
L18-1015,habash-etal-2012-conventional,0,0.027441,"prepositions, numbers, and definite articles appear more frequently in MSA than in dialects, while on the other hand dialects show higher frequency of verbs, pronouns and particles. Our justification for this noticeable disparity is that the POS distribution is affected by the genre. The MSA text is from the formal news domain with a special focus on facts and entities, while the dialects are informal expressions with a focus on events, attitudes, and conversations. Another observaThe words in the dataset were segmented in place without any modification or standardization attempts (ex. CODA (Habash et al., 2012)), and the segmentation guidelines aimed to generate a number of segments that match the correct number of POS tags for a word. We used the POS tagset described by Darwish et al. (2017) which has 18 tags for MSA POS tagging, and we added 2 dialect-specific tags (namely PROG PART, and NEG PART), and 4 tweet-specific tags (namely HASH, EMOT, MENTION, and URL). Table 1 contains description of the newly added tags5 . 4 Example I.JºJK . (bnktb) Segmentation and POS tagging were applied on the original raw text without any correction as suggested by Eldesouki et al. (2017) to overcome the need for"
L18-1015,N13-1044,0,0.508721,"Missing"
L18-1015,N13-1039,0,0.0374619,"Missing"
L18-1015,P08-2030,0,0.65644,"Missing"
L18-1015,W17-1306,1,0.871249,"or MSA POS tagging, and we added 2 dialect-specific tags (namely PROG PART, and NEG PART), and 4 tweet-specific tags (namely HASH, EMOT, MENTION, and URL). Table 1 contains description of the newly added tags5 . 4 Example I.JºJK . (bnktb) Segmentation and POS tagging were applied on the original raw text without any correction as suggested by Eldesouki et al. (2017) to overcome the need for standardization of different dialectal writings proposed in CODA by  ®J J.Óð Habash et al. (2012). For example the word ñËñ We used the dialectal Arabic dataset described by Eldesouki et al. (2017) and Samih et al. (2017b), which includes a set of 350 tweets for four major Arabic dialects that were manually segmented. The size of the dataset is as follows: No of Tweets 350 350 350 350 Description Progressive Part. Table 1: Dialect-specific and tweet-specific POS tags Data Description Dialect Egyptian (EGY) Levantine (LEV) Gulf (GLF) Maghrebi (MGR) POS PROG PART https://catalog.ldc.upenn.edu/LDC2017T07 Buckwalter transliteration is used in the paper 94 tion is that MSA has more noun suffixes and grammatical case endings, while dialects have more progressive particles and negation suffixes. This variance is rel"
L18-1015,K17-1043,1,0.86893,"or MSA POS tagging, and we added 2 dialect-specific tags (namely PROG PART, and NEG PART), and 4 tweet-specific tags (namely HASH, EMOT, MENTION, and URL). Table 1 contains description of the newly added tags5 . 4 Example I.JºJK . (bnktb) Segmentation and POS tagging were applied on the original raw text without any correction as suggested by Eldesouki et al. (2017) to overcome the need for standardization of different dialectal writings proposed in CODA by  ®J J.Óð Habash et al. (2012). For example the word ñËñ We used the dialectal Arabic dataset described by Eldesouki et al. (2017) and Samih et al. (2017b), which includes a set of 350 tweets for four major Arabic dialects that were manually segmented. The size of the dataset is as follows: No of Tweets 350 350 350 350 Description Progressive Part. Table 1: Dialect-specific and tweet-specific POS tags Data Description Dialect Egyptian (EGY) Levantine (LEV) Gulf (GLF) Maghrebi (MGR) POS PROG PART https://catalog.ldc.upenn.edu/LDC2017T07 Buckwalter transliteration is used in the paper 94 tion is that MSA has more noun suffixes and grammatical case endings, while dialects have more progressive particles and negation suffixes. This variance is rel"
L18-1015,W15-1511,0,0.189752,"Missing"
L18-1015,P11-2007,0,0.0736066,"Missing"
L18-1336,E17-3016,1,0.292645,"Missing"
L18-1336,2014.iwslt-papers.13,0,0.0457047,"rences, together with their interpretations done by professional interpreters. After giving a short survey of the related work, we present the WAW Corpus and details about the collection and curation process. Next, we provide a quantitative and qualitative assessment of the collected data. We also present a pilot/case study of the use of the corpus for extracting interpretation strategies used by interpreters. 2. Related Work In (Al-Khanji et al., 2000) interpreting strategies in Arabic have been studied, but no re-usable corpus was released. There are also Arabic speech corpora used for MT2 (Kumar et al., 2014; Zaidan and Callison-Burch, 2014), but they do not include human interpretation of the original speech (only translated speech transcripts are provided). We are not aware of any other publicly available interpreting corpora for Arabic, whereas they exist for Italian, Spanish, English, French, Dutch (Bendazzoli and Sandrelli, 2005; Falbo, 2012), Brazilian Portuguese and German (House et al., 2012), Japanese and Chinese (Tohyama and Matsubara, 2006; Hu and Qing, 2009). Differing from the existing Arabic speech corpora, the WAW corpus contains recordings of the original speakers, the recordings"
L18-1336,temnikova-etal-2017-interpreting,1,0.632503,"y annotating the interpreting strategies in the corpus. The aim of this study was to reveal which strategies interpreters from English into Arabic use, and how often. The hope is that this might eventually provide some indications if our speech-to-text automatic translation system (Dalvi et al., 2017) could benefit from implementing some of these human interpreting strategies. The study was also motivated by our previous observation on the differences in the word ratios between interpretations and translations and the discrepancy in number of named entities tags. This study is a follow-up of (Temnikova et al., 2017), where we conducted a preliminary annotation of the WAW corpus for interpreting strategies by analyzing a small sample of 7500 words (English+Arabic) from the transcripts of 4 sessions, with 2 female interpreters, including W2 (the most productive interpreter in our corpus), and 2 talks for each of these interpreters. For the study reported in the current paper we expanded the sample to 8 sessions, done by 4 interpreters, 2 women (W2, W4) and 2 men (M1, M7), adding up to 16,955 words in English and 9,477 words in Arabic. We selected these specific interpreters based on the transcript ratios a"
L18-1336,tohyama-matsubara-2006-collection,0,0.03329,"i et al., 2000) interpreting strategies in Arabic have been studied, but no re-usable corpus was released. There are also Arabic speech corpora used for MT2 (Kumar et al., 2014; Zaidan and Callison-Burch, 2014), but they do not include human interpretation of the original speech (only translated speech transcripts are provided). We are not aware of any other publicly available interpreting corpora for Arabic, whereas they exist for Italian, Spanish, English, French, Dutch (Bendazzoli and Sandrelli, 2005; Falbo, 2012), Brazilian Portuguese and German (House et al., 2012), Japanese and Chinese (Tohyama and Matsubara, 2006; Hu and Qing, 2009). Differing from the existing Arabic speech corpora, the WAW corpus contains recordings of the original speakers, the recordings of the interpreters, the transcripts of both recordings, and the translations of all transcripts. 3. WAW Corpus The WAW corpus comprises recordings from three international conferences, which took place in Qatar: WISE 2013 (World Innovation Summit for Education)3 , ARC’14 (Qatar Foundation’s Annual Research and Development Conference4 , and WISH 2013 (World Innovation Summit for Health)5 . The speeches and discussions were mostly in English, some"
L18-1336,J14-1006,0,0.0330133,"h their interpretations done by professional interpreters. After giving a short survey of the related work, we present the WAW Corpus and details about the collection and curation process. Next, we provide a quantitative and qualitative assessment of the collected data. We also present a pilot/case study of the use of the corpus for extracting interpretation strategies used by interpreters. 2. Related Work In (Al-Khanji et al., 2000) interpreting strategies in Arabic have been studied, but no re-usable corpus was released. There are also Arabic speech corpora used for MT2 (Kumar et al., 2014; Zaidan and Callison-Burch, 2014), but they do not include human interpretation of the original speech (only translated speech transcripts are provided). We are not aware of any other publicly available interpreting corpora for Arabic, whereas they exist for Italian, Spanish, English, French, Dutch (Bendazzoli and Sandrelli, 2005; Falbo, 2012), Brazilian Portuguese and German (House et al., 2012), Japanese and Chinese (Tohyama and Matsubara, 2006; Hu and Qing, 2009). Differing from the existing Arabic speech corpora, the WAW corpus contains recordings of the original speakers, the recordings of the interpreters, the transcrip"
L18-1620,N16-3003,1,0.859153,"was considered as a class, and a set of features mentioned at the end of the section were extracted for each clitic and used to train the SVM classifier. In this work we use a combination of features that includes probabilistic, binary, and Arabic-specific features. For probabilistic features we used a combination of bigrams, trigrams, and 4grams of tags and clitics. For binary features we used some features including meta-types of clitics, which indicate if a clitic is a number, a foreign word, a user mention or a URL. For Arabic specific features, we used stem template feature introduced by Abdelali et al. (2016). Where stem template represents the word pattern applied to the root mentioned in section 2.1 . The template for each clitic has been extracted and concatenated to word representation. The set of used features for SVM are: 1. Clitic features: each unique clitic in our training set acted as a feature, and an additional feature is added to represent out-of-vocabulary (OOV) clitics. We experimented with three different values for clitic features. The first value is binary (whether it exists or not). The second is the log of clitic counts in training data. The 3927 third is the Term Frequency-Inv"
L18-1620,W17-1316,1,0.896696,"Approach, Maximum Entropy Approach, Support Vector Machine(SVM) Approach and Neural Network Approach (Wilks, 1996). In this section we present our POS tagging approach; first we describe the set of features we extracted, then we discuss the two machine learning approaches we used, which are SVM and Bi-LSTM. It is worth mentioning that our taggers operate at clitic level instead of word level where a clitic is a word segment that has single POS tag. 3.1. SVM Based POS Tagger SVM is used in many NLP classification tasks including POS tagging and proves to achieve high accuracy results with MSA (Darwish et al., 2017; Gim´enez and M`arquez, 2003). For this work, we used an SVM multi-class, specifically the SVMmulticlass tool developed by Thorsten Joachims (Joachims, 2008). SVMmulticlass uses regularization parameter C to prevent overfitting (Manning et al., 2009). Each tag of POS tags was considered as a class, and a set of features mentioned at the end of the section were extracted for each clitic and used to train the SVM classifier. In this work we use a combination of features that includes probabilistic, binary, and Arabic-specific features. For probabilistic features we used a combination of bigrams"
L18-1620,N07-5003,0,0.0268657,"(Khalifa et al., 2016). • Morphologically: In most cases, there is no case inflection on GA words. Also, the prefix H . [ba] and • DA words are written as they are pronounced since there is no orthographic standards for dialects. This fact causes inconsistency in writing some words for  example the word Y [sQ Idq], which means ’truth’ 2 IPA is used to present Arabic words phonetically  [q] has different pronunciations e.g. ÈAg. [dZa:l] and ÈA¿ [ka:l] ] which means over, the sound • Word order: in dialects it is usually Subject-VerbObject (SVO) while it is Verb-Subject-Object (VSO) in MSA(Diab and Habash, 2007). is written as l .  [sQ IdZ] in some Gulf dialects variants. Another result of writing words as they are pronounced is that some letters are dropped when pro nounced. For example the word Y«A¯[qa:QId], which ©ËA£ [tQ a:lIQ], which means ’look’ is written as ¨A£ [tQ a:Q] in Kuwaiti Gulf dialect. Éªm.&apos; [najaQl] is the stem and each one of these segments is called clitic. For more explanation see (Darwish and Magdy, 2014; Habash, 2010). Researchers usually consider five main dialects for DA, namely: Egyptian, Iraqi, Levantine, Maghribi, and Gulf (Samih et al., 2017). Although Gulf Arabic is"
L18-1620,W05-0708,0,0.743529,"Missing"
L18-1620,P06-1086,0,0.326157,"Missing"
L18-1620,habash-etal-2012-conventional,0,0.01948,"eir grammars while dialects have no strict rules. In this paper, we focus our study on GA, which is one group of dialects that share many characteristics. It is the dialect of countries surrounding the Arab Gulf, such as Saudi Arabia, Kuwait, Qatar, Bahrain, Oman, United Arab Emirates and Iraq. GA has additional characteristics that distinguish it from other dialects, for example: • Phonologically: GA maintains the pronunciation of : X [D], H [T] and [DQ ] unlike other dialects. More• Vocabulary: Arabic dialects have richer vocabulary than MSA some of which are borrowed from other languages (Habash et al., 2012a). ÈA¯ [qa:l] , ’he said’(Khalifa et al., 2016). • Morphologically: In most cases, there is no case inflection on GA words. Also, the prefix H . [ba] and • DA words are written as they are pronounced since there is no orthographic standards for dialects. This fact causes inconsistency in writing some words for  example the word Y [sQ Idq], which means ’truth’ 2 IPA is used to present Arabic words phonetically  [q] has different pronunciations e.g. ÈAg. [dZa:l] and ÈA¿ [ka:l] ] which means over, the sound • Word order: in dialects it is usually Subject-VerbObject (SVO) while it is Verb-S"
L18-1620,W12-2301,0,0.345569,"eir grammars while dialects have no strict rules. In this paper, we focus our study on GA, which is one group of dialects that share many characteristics. It is the dialect of countries surrounding the Arab Gulf, such as Saudi Arabia, Kuwait, Qatar, Bahrain, Oman, United Arab Emirates and Iraq. GA has additional characteristics that distinguish it from other dialects, for example: • Phonologically: GA maintains the pronunciation of : X [D], H [T] and [DQ ] unlike other dialects. More• Vocabulary: Arabic dialects have richer vocabulary than MSA some of which are borrowed from other languages (Habash et al., 2012a). ÈA¯ [qa:l] , ’he said’(Khalifa et al., 2016). • Morphologically: In most cases, there is no case inflection on GA words. Also, the prefix H . [ba] and • DA words are written as they are pronounced since there is no orthographic standards for dialects. This fact causes inconsistency in writing some words for  example the word Y [sQ Idq], which means ’truth’ 2 IPA is used to present Arabic words phonetically  [q] has different pronunciations e.g. ÈAg. [dZa:l] and ÈA¿ [ka:l] ] which means over, the sound • Word order: in dialects it is usually Subject-VerbObject (SVO) while it is Verb-S"
L18-1620,L16-1679,0,0.217841,"s. In this paper, we focus our study on GA, which is one group of dialects that share many characteristics. It is the dialect of countries surrounding the Arab Gulf, such as Saudi Arabia, Kuwait, Qatar, Bahrain, Oman, United Arab Emirates and Iraq. GA has additional characteristics that distinguish it from other dialects, for example: • Phonologically: GA maintains the pronunciation of : X [D], H [T] and [DQ ] unlike other dialects. More• Vocabulary: Arabic dialects have richer vocabulary than MSA some of which are borrowed from other languages (Habash et al., 2012a). ÈA¯ [qa:l] , ’he said’(Khalifa et al., 2016). • Morphologically: In most cases, there is no case inflection on GA words. Also, the prefix H . [ba] and • DA words are written as they are pronounced since there is no orthographic standards for dialects. This fact causes inconsistency in writing some words for  example the word Y [sQ Idq], which means ’truth’ 2 IPA is used to present Arabic words phonetically  [q] has different pronunciations e.g. ÈAg. [dZa:l] and ÈA¿ [ka:l] ] which means over, the sound • Word order: in dialects it is usually Subject-VerbObject (SVO) while it is Verb-Subject-Object (VSO) in MSA(Diab and Habash, 2007)"
L18-1620,D15-1176,0,0.0960929,"Missing"
L18-1620,pasha-etal-2014-madamira,0,0.305784,"Missing"
L18-1620,P16-2067,0,0.0215024,". È@ [al] determiner. We also include meta-type feature which is an additional information added about the type of clitic i.e. to specify whether it is a number, an adjective number, a prefix, a suffix, a foreign, a punctuation, an Arabic letter and twitter specific types: hashtags, URLs and mentions. 4. 4.1. Bi-LSTM Based POS Tagger Bi-LSTM is a special type of Recurrent Neural Network (RNN). It has proved to be a good choice for sequence modeling tasks (Ling et al., 2015) such as speech processing, POS tagging, phrased based chunking ... etc. It is also less sensitive to training data size (Plank et al., 2016). Moreover, Bi-LSTM can capture the context around source words up to very long sequences in both directions (previous and upfront) (Wang et al., 2015). It also does not need hand crafted features to work well. These characteristics make it a suitable fit for POS tagging of DA. Since there is not much training data available for DA – GA in this case – and since DA lacks standards to design powerful features, a model is needed that auto-fits its features and characteristics. Bi-LSTM structure differs from the classic RNN in that it adds a memory cell to the neural network architecture that lear"
L18-1620,K17-1043,1,0.910025,"ct-Object (VSO) in MSA(Diab and Habash, 2007). is written as l .  [sQ IdZ] in some Gulf dialects variants. Another result of writing words as they are pronounced is that some letters are dropped when pro nounced. For example the word Y«A¯[qa:QId], which ©ËA£ [tQ a:lIQ], which means ’look’ is written as ¨A£ [tQ a:Q] in Kuwaiti Gulf dialect. Éªm.&apos; [najaQl] is the stem and each one of these segments is called clitic. For more explanation see (Darwish and Magdy, 2014; Habash, 2010). Researchers usually consider five main dialects for DA, namely: Egyptian, Iraqi, Levantine, Maghribi, and Gulf (Samih et al., 2017). Although Gulf Arabic is the largest existing dialect in social media, there is very limited attention towards building NLP tools for it. DA is derived from MSA; nevertheless, they differ at many linguistic levels. Some notable differences are in terms of: ¨A¯ [qa:Q], and the the verb h@P [raaè] are used to indicate future tense. In addition, the words I . Ó [mub], H. ñÓ [mob], AÓ [ma:], ñëAÓ [ma:hu] and H . ñëAÓ [ma:hu:b] are used for negation (Khalifa et al., 2016). These differences emphasize the need for specially designed NLP tools for dialects to prevent the performance drop when using"
N16-1125,N16-3004,1,0.864659,"Missing"
N16-1125,W07-0718,0,0.0456341,"res given by the evaluators. 1 Introduction Human evaluation has been the preferred method for tracking the progress of MT systems. In the past, the prevalent criterion was to judge the quality of a translation in terms of fluency and adequacy, on an absolute scale (White et al., 1994). However, different evaluators focused on different aspects of the translations, which increased the subjectivity of their judgments. As a result, evaluations suffered from low inter- and intra-annotator agreements (Turian et al., 2003; Snover et al., 2006). This caused a shift towards a ranking-based approach (Callison-Burch et al., 2007). Unfortunately, the disagreement between evaluators is still a challenge that cannot be easily resolved due to the non-transparent thought-process that evaluators follow to make a judgment. The eye-mind hypothesis (Just and Carpenter, 1980; Potter, 1983) states that when completing a task, people cognitively process objects that are in front of their eyes (i.e. where they fixate their gaze).1 Based on this assumption, it has been possible to study reading behavior and patterns (Rayner, 1998; Garrod, 2006; Hansen and Ji, 2010). The overall difficulty of a sentence and its syntactic complexity"
N16-1125,W12-3102,0,0.0857719,"Missing"
N16-1125,P11-1105,1,0.882545,"and distance between the start and end words. For subsequent words n, n + 1, this would mean a forward jump of distance equal to 1. All jumps with distance greater than 4 were sorted into a 5+ bucket. Additionally, we separate the features for reference and translation jumps. We also count the total number of jumps. Total jump distance We additionally aggregate jump distances2 to count the total distance covered while evaluating a sentence. We have reference distance and translation distance features. Again, the 2 Jump count and distance features have also shown to be useful in SMT decoders (Durrani et al., 2011). 1083 idea is that for a well-formed sentence, gaze distance should be less, compared to a poorly-formed one. Inter-region jumps While reading a translation, evaluators can jump between the translation and a reference to compare them. Intuitively, more jumps of this type could signify that the translation is harder to evaluate. Here we count the number of transitions between reference and translation. Dwell time The amount of time a person fixates on a region is a crucial marker for processing difficulty in sentence comprehension (Clifton et al., 2007) and moderately correlates with the quali"
N16-1125,W13-2305,0,0.0177286,"was performed by 6 different evaluators, resulting in 720 evaluations. The annotators were presented with a translationreference pair at a time. The two evaluation tasks corresponding to the same reference were presented at two different times with at least 40 other tasks in-between. This was done to prevent any possible spurious effects that may arise from remembering the content of a first translation, when evaluating the second translation of the same sentence. During each evaluation task, the evaluators were asked to assess the quality of a translation by providing a score between 0–100 (Graham et al., 2013). The observed inter-annotator agreement (Cohen’s kappa) among our annotators was 0.321. This is slightly higher than the overall inter-annotator agreement of 0.284 reported in WMT’12 for the Spanish-English.3 For reading patterns we use the EyeTribe eye-tracker at 3 For a rough comparison only. Note that these two numbers are not exactly comparable given that they are calculated on different subsets of the same data. Still, there is a fair agreement between the our evaluators and the expected wins from WMT’12 (avg. pairwise kappa of 0.381) 1084 Evaluation In our evaluation, we used eye-tracki"
N16-1125,W15-3059,1,0.875061,"Missing"
N16-1125,P02-1040,0,0.0970473,"n evaluation metric So far, we’ve shown that the individual sets of features based on reading patterns can help to predict translation quality, and that this goes beyond simple fluency. One question that remains to be answered is whether these features could be used as a whole to evaluate the quality of a translation semi-automatically. That is, whether we can use the gaze information, and other lexical information to anticipate the score that an evaluator will assign to a translation. Here, we present evaluation results combining several of these gaze features, and compare them against BLEU (Papineni et al., 2002), which uses lexical information and is designed to measure not only fluency but also adequacy. In Table 2, we present results in the following way: in (I) we present the best non-lexicalized feature combinations that improve the predictive power of the model. In (II) we re-introduce the results of lexicalized jumps feature. In (III) we present results of BLEU and the combination of eye-tracking features with it. Finally in (IV) we present the humanto-human agreement measured in average Kendall’s tau and in max human-to-human Kendall’s tau. Combinations of translation jumps In section I we pre"
N16-1125,2006.amta-papers.25,0,0.0363431,"patterns can be used to build semi-automatic metrics that anticipate the scores given by the evaluators. 1 Introduction Human evaluation has been the preferred method for tracking the progress of MT systems. In the past, the prevalent criterion was to judge the quality of a translation in terms of fluency and adequacy, on an absolute scale (White et al., 1994). However, different evaluators focused on different aspects of the translations, which increased the subjectivity of their judgments. As a result, evaluations suffered from low inter- and intra-annotator agreements (Turian et al., 2003; Snover et al., 2006). This caused a shift towards a ranking-based approach (Callison-Burch et al., 2007). Unfortunately, the disagreement between evaluators is still a challenge that cannot be easily resolved due to the non-transparent thought-process that evaluators follow to make a judgment. The eye-mind hypothesis (Just and Carpenter, 1980; Potter, 1983) states that when completing a task, people cognitively process objects that are in front of their eyes (i.e. where they fixate their gaze).1 Based on this assumption, it has been possible to study reading behavior and patterns (Rayner, 1998; Garrod, 2006; Hans"
N16-1125,stymne-etal-2012-eye,0,0.322545,"enomena remain to be explored in future work. Human performance On average, evaluators agreements with each other are fair (τ = 0.33) and below the best combination (CB3 ), while the maximum agreement of any two evaluators is relatively higher (τ = 0.53). This tells us that on average the semi-automatic approach to evaluation that we propose here is already competitive to predictions done by another (average) human. However, there is still room for improvement with respect to the mostagreeing pair of evaluators. 5 Related Work Eye-tracking devices have been used previously in the MT research. Stymne et al. (2012) used eye-tracking to identify and classify MT errors. 1086 SYS Feature Sets τ I. Combination of translation jumps EyeTrabj Backward jumps CTJ1 Backward jumps, total jumps CTJ2 Backward jumps, total jumps, distance 0.22 0.25 0.27 II. Eye-tracking: Best Lexicalized EyeLexall Lexicalized gaze jumps 0.22 III. Combinations with BLEU Bbleu BLEU CB1 Bbleu + EyeTrabj CB2 Bbleu + CTJ2 CB3 Bbleu + EyeLexall 0.34 0.38 0.39 0.42 IV. Human performance Avg Avg. human-to-human agreement Max Max. human-to-human agreement 0.33 0.53 Table 2: Result of combining several jump and lexicalized features with BLEU."
N16-1125,2003.mtsummit-papers.51,0,0.111887,"ts show that reading patterns can be used to build semi-automatic metrics that anticipate the scores given by the evaluators. 1 Introduction Human evaluation has been the preferred method for tracking the progress of MT systems. In the past, the prevalent criterion was to judge the quality of a translation in terms of fluency and adequacy, on an absolute scale (White et al., 1994). However, different evaluators focused on different aspects of the translations, which increased the subjectivity of their judgments. As a result, evaluations suffered from low inter- and intra-annotator agreements (Turian et al., 2003; Snover et al., 2006). This caused a shift towards a ranking-based approach (Callison-Burch et al., 2007). Unfortunately, the disagreement between evaluators is still a challenge that cannot be easily resolved due to the non-transparent thought-process that evaluators follow to make a judgment. The eye-mind hypothesis (Just and Carpenter, 1980; Potter, 1983) states that when completing a task, people cognitively process objects that are in front of their eyes (i.e. where they fixate their gaze).1 Based on this assumption, it has been possible to study reading behavior and patterns (Rayner, 19"
N16-1125,1994.amta-1.25,0,0.757488,"Missing"
N16-3003,W11-4417,0,0.0982575,"Missing"
N16-3003,C96-1017,0,0.288953,"Missing"
N16-3003,N12-1047,0,0.014351,"BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by the IWSLT shared task to evaluate the systems. In each experiment, we simply changed the segmentation pipeline to try different segmentation. We used ATB scheme for MADAMIRA which has shown to outperform its alternatives (S2 a"
N16-3003,darwish-etal-2014-using,1,0.880454,"Missing"
N16-3003,W02-0506,1,0.190376,"Missing"
N16-3003,P11-1105,1,0.385324,"olkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by the IWSLT shared task to evaluate the systems. In each experiment, we simply changed the segmentation pipeline to try different segmentation. We used ATB scheme for MADAMIRA which has shown to outperform its alternatives (S2 and D3) previously (Sajjad et al., 2013). Results: Table 2 compares the Arabic-to-English SMT systems using the three segmentation tools. Farasa p"
N16-3003,W14-3309,1,0.823799,"ord and MADAMIRA3 . The comparison was done in terms of BLEU (Papineni et al., 2002) and processing times. We used concatenation of IWSLT TED talks (Cettolo et al., 2014) (containing 183K Sentences) and NEWS corpus (containing 3 Release-01292014-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 201"
N16-3003,E14-4029,1,0.444802,"ord and MADAMIRA3 . The comparison was done in terms of BLEU (Papineni et al., 2002) and processing times. We used concatenation of IWSLT TED talks (Cettolo et al., 2014) (containing 183K Sentences) and NEWS corpus (containing 3 Release-01292014-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 201"
N16-3003,N13-1073,0,0.0288181,". We used concatenation of IWSLT TED talks (Cettolo et al., 2014) (containing 183K Sentences) and NEWS corpus (containing 3 Release-01292014-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c"
N16-3003,eisele-chen-2010-multiun,0,0.0434022,"Missing"
N16-3003,D08-1089,0,0.0253633,"ems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by the IWSLT shared task to evaluate the systems. In each experiment, we simply changed the segmentation pipeline to try different segmentation. We used ATB scheme for MADAMIRA which has shown to outperform its alternatives (S2 and D3) previously (Sajjad et al., 2013). Results: Table 2 compares the Arabic-to-Englis"
N16-3003,P12-1016,0,0.0135218,"e that each of the segmenters took to process the entire document collection. As can be seen from the results, Farasa outperformed using words, MADAMIRA, and Stanford significantly. Farasa was an order of magnitude faster than Stanford and two orders of magnitude faster than MADAMIRA. 5 Analysis The major advantage of using Farasa is speed, without loss in accuracy. This mainly results from optimization described earlier in the Section 2 which includes caching and limiting the context used for building the features vector. Stanford segmenter uses a third-order (i.e., 4-gram) Markov CRF model (Green and DeNero, 2012) to predict the correct segmentation. On the other hand, MADAMIRA bases its segmentation on the output of a morphological analyzer which provides a list of possible analyses (independent of context) for each word. Both text and analyses are passed to a feature modeling component, which applies SVM and language models to derive predictions for the word segmentation (Pasha et al., 2014). This hierarchy could explain the slowness of MADAMIRA versus other tokenizers. 6 Conclusion In this paper we introduced Farasa, a new Arabic segmenter, which uses SVM for ranking. We compared our segmenter with"
N16-3003,W11-2123,0,0.0161867,"es) and NEWS corpus (containing 3 Release-01292014-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by th"
N16-3003,P07-1019,0,0.0266176,"iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by the IWSLT shared task to evaluate the systems. In each experiment, we simply changed the segment"
N16-3003,P07-2045,0,0.0264015,"l Machine Translation (SMT) systems for Arabic↔English, to compare Farasa with Stanford and MADAMIRA3 . The comparison was done in terms of BLEU (Papineni et al., 2002) and processing times. We used concatenation of IWSLT TED talks (Cettolo et al., 2014) (containing 183K Sentences) and NEWS corpus (containing 3 Release-01292014-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Ope"
N16-3003,N04-1022,0,0.0119473,"14-1.0 was used in the experiments 13 iwslt12 iwslt13 Avg Time 30.4 30.0 30.2 30.8 30.5 30.8 30.6 30.3 30.5 4074 395 80 Table 2: Arabic-to-English Machine Translation, BLEU scores and Time (in seconds) 202K Sentences) to train phrase-based systems. Systems: We used Moses (Koehn et al., 2007), a state-of-the-art toolkit with the the settings described in (Durrani et al., 2014a): these include a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), used at runtime, MBR decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007) using a stack size of 1,000 during tuning and 5,000 during testing. We tuned with the k-best batch MIRA (Cherry and Foster, 2012). Among other features, we used lexicalized reordering model (Galley and Manning, 2008), a 5-gram Operation Sequence Model (Durrani et al., 2011), Class-based Models (Durrani et al., 2014b)4 and other default parameters. We used an unsupervised transliteration model (Durrani et al., 2014c) to transliterate the OOV words. We used the standard tune and test set provided by the IWSLT shared task to evaluate the systems. In each ex"
N16-3003,P14-2034,0,0.070743,"Missing"
N16-3003,P02-1040,0,0.119887,"Missing"
N16-3003,pasha-etal-2014-madamira,0,0.166971,"Missing"
N16-3003,2014.iwslt-evaluation.6,1,\N,Missing
N16-3003,2013.iwslt-evaluation.8,1,\N,Missing
N16-3004,P11-1105,1,0.850927,"ump distances7 to count the total distance covered while evaluating a sentence. We count reference and translation distance features separately. Such information is useful in analyzing the complexity and readability of the translation. Inter-region jumps While reading a translation, evaluators can jump between the translation and a reference to compare them. Intuitively, more jumps of this type could signify that the translation is harder to evaluate. Here we count the number of reference↔translation transitions. 7 Jump count and distance features have also shown to be useful in SMT decoders (Durrani et al., 2011). 20 Lexicalized features The features discussed above do not associate gaze movements with the words being read. We believe that this information can be critical to judge the overall difficulty of the reference sentence, and to evaluate which translation fragments are problematic to the reader. To compute the lexicalized features, we extract streams of reference and translation lexical sequences based on the gaze jumps, and score them using a tri-gram language model. Let Ri = r1 , r2 , . . . , rm be a sub-sequence of gaze movement over reference and there are R1 , R2 , . . . , Rn sequences, t"
N16-3004,W15-3059,1,0.502704,"Missing"
N16-3004,N16-1125,1,0.864864,"Missing"
N16-3004,stymne-etal-2012-eye,0,0.206082,"been adopted as the preferred tool in the WMT evaluation campaigns (Bojar et al., 2013), and thus, it is currently used by dozens of researchers. According to the eye-mind hypothesis (Just and Carpenter, 1980) people cognitively process objects that are in front of their eyes. This has enabled researchers to analyze and understand how people perform certain tasks like reading (Rayner, 1998; 1 It is subjective, expensive, time-consuming, boring, etc. Garrod, 2006; Harley, 2013). In recent times, eyetracking has also been used in Machine Translation to identify and classify translation errors (Stymne et al., 2012), to evaluate the usability of automatic translations (Doherty and O’Brien, 2014), and to improve the consistency of the human evaluation process (Guzm´an et al., 2015), etc. Furthermore, tracking how evaluators consume MT output, can help to reduce human evaluation subjectivity, as we could use evidence of what people do (i.e. unbiased reading patterns) and not only what they say they think (i.e. user-biased evaluation scores). However, the main limitation for the adoption of eye-tracking research has been the steep learning curve that is associated with eye-tracking analysis and the high-cos"
N16-3004,N13-1001,1,\N,Missing
N16-3004,J15-2001,1,\N,Missing
N19-1248,D15-1274,0,0.212375,"Missing"
N19-1248,D17-1151,0,0.0558737,"Missing"
N19-1248,W17-1302,1,0.866003,"Missing"
N19-1248,W02-0504,0,0.868168,"Missing"
N19-1248,P05-1071,0,0.215534,"Missing"
N19-1248,N07-2014,0,0.680665,"Missing"
N19-1248,P17-4012,0,0.0215192,"31 2.37 1.99 3.03 2.05 5.97 3.57 3.07 3.93 3.04 7.79 5.49 4.77 6.40 4.77 2.01 1.49 1.30 1.78 1.29 0.00 0.00 0.00 0.00 0.00 12 Combination ∗ 09 +† 11 1.89 2.89 4.49 1.21 0.00 Table 3: Diacritization results: *g represents ngram size e.g. 7g means 7-gram context. Experiment 09 and 11 are comparing NMT models – LSTM-based architecture with attention mechanism and Transformer model Setup and dropout rate = 0.3. The setting for the Transformer were: 6 encoder and 6 decoder layers each of size 512; number of attention heads = 8; feed forward dimension = 2048; and dropout = 0.1. We used the OpenNMT (Klein et al., 2017) implementation with tensorflow for all experiments. System Runs. We conducted a variety of experiments as follows, namely: Word-level experiments where the input is a sequence of words and the output is a sequence of diacritized words: – Baseline Word: uses the full sentences and shows the deficiency of using NMT directly. – Word 7g: uses non-overlapping windows of 7 words to compare to our best character-level model, which also uses a window of length 7. – Word 7g+overlap: uses a sliding window of 7 words. Character-level experiments where the input is represented as a sequence of character"
N19-1248,W18-2507,0,0.137075,"Missing"
N19-1248,W05-0711,0,0.22455,"Missing"
N19-1248,pasha-etal-2014-madamira,0,0.446003,"Missing"
N19-1248,W04-1612,0,0.362427,"Missing"
N19-1248,P06-1073,0,0.232166,"Missing"
N19-1248,E17-2060,0,0.0197172,"not fit in our scenario as it may create source and target segments of different lengths. In the Arabic diacritization problem, both source and target words and characters are strictly tied to each other and loosening it would result in sub-optimal performance and may generate unexpected errors. Context Window. The diacritization of Arabic words is highly sensitive to context. Character representations significantly increase the size of the source and target sequences. This leads to a well known limitation of character-based LSTM-based models, namely poor handling of long range dependencies (Sennrich, 2017). An easy fix is to split sentences greater than a certain length into multiple lines. However, boundary words may loose context in the newly created sequences. To handle this, we propose to keep a fixed size context window c for every word. Given a sentence, we use a sliding context window to split it into segments of overlapping windows of size c as in Table 1. This fixes the problems of both long range dependencies and context of neighboring words. We are further aided by the fact that local context can conclusively determine the correct diacritization in the vast majority of cases. Voting."
N19-1248,P16-1162,0,0.0460472,"split into a sequence of subword units each consisting of a letter and its diacritic(s). For example, source word “AlElm” would be represented as “A/l/E/l/m” and its diacritized target “AaloEalamu” as “Aa/lo/Ea/la/mu”. The character-level representation has several benefits, such as reducing the vocabulary size and avoiding OOV words. The splitting of diacritized 2391 words into subword units simplifies the problem as there will be identical number of source and target tokens in a parallel sentence. Later, we support our design decisions with results in the experiments section. Subwords (BPE (Sennrich et al., 2016)) have been used as a defacto standard in building NMT systems. They are a natural choice to handle unknown words. However, BPE does not fit in our scenario as it may create source and target segments of different lengths. In the Arabic diacritization problem, both source and target words and characters are strictly tied to each other and loosening it would result in sub-optimal performance and may generate unexpected errors. Context Window. The diacritization of Arabic words is highly sensitive to context. Character representations significantly increase the size of the source and target sequ"
P07-1110,W06-1009,1,0.776135,"take), this is perhaps understandable, as for each new language, a new translation algorithm must be included. The effort involved in extending query translation to multiple languages, therefore, is likely to be in proportion to the number of languages. With parallel corpora, the reason that research has been limited to only a few languages at a time – and usually just two at a time, as in the LSI work cited above – is more likely to be rooted in the widespread perception that good parallel corpora are difficult to obtain (see for example Asker 2004). However, recent work (Resnik et al. 1999, Chew et al. 2006) has challenged this idea. One advantage of a ‘massively parallel’ multilingual corpus is perhaps self-evident: within the LSI framework, the more languages are mapped into the single conceptual space, the fewer restrictions there are on which languages documents can be selected from for cross-language retrieval. However, several questions were raised for us as 872 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 872–879, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics we contemplated the use of a massively parall"
P07-1110,J05-4003,0,\N,Missing
P17-2095,P10-1048,1,0.856786,"ub-word segmentation based on BPE, and iii) two variants of character-based segmentation. We first map each source word to its corresponding segments (depending on the segmentation scheme), embed all segments of a word in vector space and feed them one-by-one to an encoder-decoder model. See Figure 1 for illustration. 2.1 Figure 1: Segmentation approaches for the word “b$rhm” “ ÑëQå.”; the blue vectors indicate the embedding(s) used before the encoding layer. 2.3 Morphological Segmentation Character-based models have been found to be effective in translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012) and OOV words (Durrani et al., 2014). Ling et al. (2016) used character embeddings to address the OOV word problem. We explored them as an alternative to morphological segmentation. Their advantage is that character embeddings do not require any complicated pre- and post-processing step other than segmenting words into characters. The fully character-level encoder treats the source sentence as a sequence of letters, encoding each letter (including white-space) in the LSTM encoder (see Figure 1). The decoding may follow identical settings. We restricted the characte"
P17-2095,D11-1033,0,0.0328528,"ng several segmentation strategies. 3 Experiments In the following, we describe the data and system settings and later present the results of machine translation and POS tagging. LSTM in the (bidirectional) encoder and the decoder, with a size of 500. We limit the sentence length to 100 for MORPH, UNSEG, BPE, cCNN, and 500 for CHAR experiments. The source and target vocabularies are limited to 50k each. 3.1 3.2 Settings Data The MT systems were trained on 1.2 Million sentences, a concatenation of TED corpus (Cettolo et al., 2012), LDC NEWS data, QED (Guzm´an et al., 2013) and an MML-filtered (Axelrod et al., 2011) UN corpus.1 We used dev+test10 for tuning and tst11-14 for testing. For EnglishArabic, outputs were detokenized using MADA detokenizer. Before scoring the output, we normalized them and reference translations using the QCRI normalizer (Sajjad et al., 2013). Machine Translation Results Table 1 presents MT results using various segmentation strategies. Compared to the UNSEG system, the MORPH system2 improved translation quality by 4.6 and 1.6 BLEU points in Ar-to-En and Ento-Ar systems, respectively. The results also improved by up to 3 BLEU points for cCNN and CHAR systems in the Ar-to-En dire"
P17-2095,E14-4029,1,0.887411,"acter-based segmentation. We first map each source word to its corresponding segments (depending on the segmentation scheme), embed all segments of a word in vector space and feed them one-by-one to an encoder-decoder model. See Figure 1 for illustration. 2.1 Figure 1: Segmentation approaches for the word “b$rhm” “ ÑëQå.”; the blue vectors indicate the embedding(s) used before the encoding layer. 2.3 Morphological Segmentation Character-based models have been found to be effective in translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012) and OOV words (Durrani et al., 2014). Ling et al. (2016) used character embeddings to address the OOV word problem. We explored them as an alternative to morphological segmentation. Their advantage is that character embeddings do not require any complicated pre- and post-processing step other than segmenting words into characters. The fully character-level encoder treats the source sentence as a sequence of letters, encoding each letter (including white-space) in the LSTM encoder (see Figure 1). The decoding may follow identical settings. We restricted the character-level representation to the Arabic side of the parallel corpus"
P17-2095,C96-1017,0,0.0492124,"ranslation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance. 1 Introduction Arabic word segmentation has shown to significantly improve output quality in NLP tasks such as machine translation (Habash and Sadat, 2006; Almahairi et al., 2016), part-of-speech tagging (Diab et al., 2004; Habash and Rambow, 2005), and information retrieval (M. Aljlayl and Grossman, 2002). A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al., 2014; Abdelali et al., 2016; Khalifa et al., 2016). Morphological segmentation splits words into morphemes. For example, ‘‘wktAbnA” “ AJK . AJ»ð” (gloss: and our book) is decomposed into its stem H AJ» + ð”. and affixes as: “w+ ktAb +nA” “ AK+ . 601 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 601–607 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2095 2 Segmentation Approaches We experimente"
P17-2095,P17-1080,1,0.841253,"ing a CNN over characters. The embedding are then provided to the encoder as input. The intuition is that the character-based word embedding should be able to learn the morphological phenomena a word inherits. Compared to fully characterlevel encoding, the encoder gets word-level embeddings as in the case of unsegmented words (see Figure 1). However, the word embedding is intuitively richer than the embedding learned over unsegmented words because of the convolution over characters. The method was previously shown to help neural MT (Belinkov and Glass, 2016; Costa-juss`a and Fonollosa, 2016). Belinkov et al. (2017) also showed character-based representations learned using a CNN to be superior, at learning word morphology, than their word-based counter-parts. However, they did not compare these against BPE-based segmentation. We use character-CNN to aid Arabic word segmentation. 602 # SEG tst11 Arabic-to-English tst12 tst13 tst14 AVG. tst11 English-to-Arabic tst12 tst13 tst14 UNSEG 25.7 28.2 27.3 23.9 26.3 15.8 17.1 18.1 15.5 16.6 MORPH cCNN CHAR BPE 29.2 29.0 28.8 29.7 33.0 32.0 31.8 32.5 32.9 32.5 32.5 33.6 28.3 28.0 27.8 28.4 30.9 30.3 30.2 31.1 16.5 14.3 15.3 17.5 18.8 12.8 17.1 18.0 20.4 13.6 18.0 2"
P17-2095,fishel-kirik-2010-linguistically,0,0.0293358,"ntext). The analyses are provided with the original text to a Feature Modeling component that applies an SVM and a language model to make predictions, which are scored by an Analysis Ranking component. Farasa on the other hand is a light weight segmenter, which ignores context and instead uses a variety of features and lexicons for segmentation. 2.2 Character-level Encoding Data Driven Sub-word Units A number of data-driven approaches have been proposed that learn to segment words into smaller units from data (Demberg, 2007; Sami Virpioja and Kurimo, 2013) and shown to improve phrasebased MT (Fishel and Kirik, 2010; Stallard et al., 2012). Recently, with the advent of neural MT, a few sub-word-based techniques have been proposed that segment words into smaller units to tackle the limited vocabulary and unknown word problems (Sennrich et al., 2016; Wu et al., 2016). In this work, we explore Byte-Pair Encoding (BPE), a data compression algorithm (Gage, 1994) as an alternative to morphological segmentation of Arabic. BPE splits words into symbols (a sequence of characters) and then iteratively replaces the most frequent symbols with their merged variants. In essence, frequent character n-gram sequences wil"
P17-2095,A00-1031,0,0.0614725,"this problem is; at test time, BPE is applied to those words only which were known to the full vocabulary of the training corpus. In this way, the sub-word units created by BPE for the word are already seen in a similar context during training and the model has learned to translate them correctly. The downside of this method is that it limits BPE’s power to segment unknown words to their correct sub-word units and outputs them as UNK in translation. 3.3 We also experimented with the aforementioned segmentation strategies for the task of Arabic POS tagging. Probabilistic taggers like HMMbased (Brants, 2000) and sequence learning models like CRF (Lafferty et al., 2001) consider previous words and/or tags to predict the tag of the current word. We mimic a similar setting but in a sequence-to-sequence learning framework. Figure 3 describes a step by step procedure to train a neural encoder-decoder tagger. Consider an Arabic phrase “klm >SdqA}k b$rhm” Discussion: Though BPE performed well for machine translation, there are a few reservations that we would like to discuss here. Since the main goal of the algorithm is to compress data and segmentation comes as a by-product, it often produces different"
P17-2095,2013.iwslt-papers.2,1,0.905117,"Missing"
P17-2095,2012.eamt-1.60,0,0.0196508,"3.6 18.0 20.0 17.2 12.6 15.3 16.6 18.2 13.3 16.4 18.0 AVG. Table 1: Results of comparing several segmentation strategies. 3 Experiments In the following, we describe the data and system settings and later present the results of machine translation and POS tagging. LSTM in the (bidirectional) encoder and the decoder, with a size of 500. We limit the sentence length to 100 for MORPH, UNSEG, BPE, cCNN, and 500 for CHAR experiments. The source and target vocabularies are limited to 50k each. 3.1 3.2 Settings Data The MT systems were trained on 1.2 Million sentences, a concatenation of TED corpus (Cettolo et al., 2012), LDC NEWS data, QED (Guzm´an et al., 2013) and an MML-filtered (Axelrod et al., 2011) UN corpus.1 We used dev+test10 for tuning and tst11-14 for testing. For EnglishArabic, outputs were detokenized using MADA detokenizer. Before scoring the output, we normalized them and reference translations using the QCRI normalizer (Sajjad et al., 2013). Machine Translation Results Table 1 presents MT results using various segmentation strategies. Compared to the UNSEG system, the MORPH system2 improved translation quality by 4.6 and 1.6 BLEU points in Ar-to-En and Ento-Ar systems, respectively. The resul"
P17-2095,P05-1071,0,0.072442,"racter CNN (Convolution Neural Network). On the tasks of Machine Translation and POS tagging, we found these methods to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance. 1 Introduction Arabic word segmentation has shown to significantly improve output quality in NLP tasks such as machine translation (Habash and Sadat, 2006; Almahairi et al., 2016), part-of-speech tagging (Diab et al., 2004; Habash and Rambow, 2005), and information retrieval (M. Aljlayl and Grossman, 2002). A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al., 2014; Abdelali et al., 2016; Khalifa et al., 2016). Morphological segmentation splits words into morphemes. For example, ‘‘wktAbnA” “ AJK . AJ»ð” (gloss: and our book) is decomposed into its stem H AJ» + ð”. and affixes as: “w+ ktAb +nA” “ AK+ . 601 Proceedings of the 55th Annual Meeting of the Association"
P17-2095,P16-2058,0,0.109475,"Missing"
P17-2095,N13-1044,0,0.0576645,"Missing"
P17-2095,P07-1116,0,0.100776,"al analyzer that generates a list of possible word-level analyses (independent of context). The analyses are provided with the original text to a Feature Modeling component that applies an SVM and a language model to make predictions, which are scored by an Analysis Ranking component. Farasa on the other hand is a light weight segmenter, which ignores context and instead uses a variety of features and lexicons for segmentation. 2.2 Character-level Encoding Data Driven Sub-word Units A number of data-driven approaches have been proposed that learn to segment words into smaller units from data (Demberg, 2007; Sami Virpioja and Kurimo, 2013) and shown to improve phrasebased MT (Fishel and Kirik, 2010; Stallard et al., 2012). Recently, with the advent of neural MT, a few sub-word-based techniques have been proposed that segment words into smaller units to tackle the limited vocabulary and unknown word problems (Sennrich et al., 2016; Wu et al., 2016). In this work, we explore Byte-Pair Encoding (BPE), a data compression algorithm (Gage, 1994) as an alternative to morphological segmentation of Arabic. BPE splits words into symbols (a sequence of characters) and then iteratively replaces the most fre"
P17-2095,N06-2013,0,0.0488146,"rd units, ii) characters as a unit of learning, and iii) word embeddings learned using a character CNN (Convolution Neural Network). On the tasks of Machine Translation and POS tagging, we found these methods to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance. 1 Introduction Arabic word segmentation has shown to significantly improve output quality in NLP tasks such as machine translation (Habash and Sadat, 2006; Almahairi et al., 2016), part-of-speech tagging (Diab et al., 2004; Habash and Rambow, 2005), and information retrieval (M. Aljlayl and Grossman, 2002). A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al., 2014; Abdelali et al., 2016; Khalifa et al., 2016). Morphological segmentation splits words into morphemes. For example, ‘‘wktAbnA” “ AJK . AJ»ð” (gloss: and our book) is decomposed into its stem H AJ» + ð”. and af"
P17-2095,C16-2047,0,0.0156449,"ater, gives optimal performance. 1 Introduction Arabic word segmentation has shown to significantly improve output quality in NLP tasks such as machine translation (Habash and Sadat, 2006; Almahairi et al., 2016), part-of-speech tagging (Diab et al., 2004; Habash and Rambow, 2005), and information retrieval (M. Aljlayl and Grossman, 2002). A considerable amount of research has therefore been spent on Arabic morphological segmentation in the past two decades, ranging from rule-based analyzers (Beesley, 1996) to state-of-the-art statistical segmenters (Pasha et al., 2014; Abdelali et al., 2016; Khalifa et al., 2016). Morphological segmentation splits words into morphemes. For example, ‘‘wktAbnA” “ AJK . AJ»ð” (gloss: and our book) is decomposed into its stem H AJ» + ð”. and affixes as: “w+ ktAb +nA” “ AK+ . 601 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 601–607 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2095 2 Segmentation Approaches We experimented with three data-driven segmentation schemes: i) morphological segmentation, ii) sub-word segmentation based"
P17-2095,P16-1162,0,0.504845,"bic translation (El Kholy and Habash, 2012)). More importantly, these tools are dialect- and domain-specific. A segmenter trained for modern standard Arabic (MSA) performs significantly worse on dialectal Arabic (Habash et al., 2013), or when it is applied to a new domain. In this work, we explore whether we can avoid the language-dependent pre/post-processing components and learn segmentation directly from the training data being used for a given task. We investigate data-driven alternatives to morphological segmentation using i) unsupervised sub-word units obtained using byte-pair encoding (Sennrich et al., 2016), ii) purely character-based segmentation (Ling et al., 2015), and iii) a convolutional neural network over characters (Kim et al., 2016). We evaluate these techniques on the tasks of machine translation (MT) and part-of-speech (POS) tagging and compare them against morphological segmenters MADAMIRA (Pasha et al., 2014) and Farasa (Abdelali et al., 2016). On the MT task, byte-pair encoding (BPE) performs the best among the three methods, achieving very similar performance to morphological segmentation in the Arabic-to-English direction and slightly worse in the other direction. Character-based"
P17-2095,P12-2063,0,0.0265535,"provided with the original text to a Feature Modeling component that applies an SVM and a language model to make predictions, which are scored by an Analysis Ranking component. Farasa on the other hand is a light weight segmenter, which ignores context and instead uses a variety of features and lexicons for segmentation. 2.2 Character-level Encoding Data Driven Sub-word Units A number of data-driven approaches have been proposed that learn to segment words into smaller units from data (Demberg, 2007; Sami Virpioja and Kurimo, 2013) and shown to improve phrasebased MT (Fishel and Kirik, 2010; Stallard et al., 2012). Recently, with the advent of neural MT, a few sub-word-based techniques have been proposed that segment words into smaller units to tackle the limited vocabulary and unknown word problems (Sennrich et al., 2016; Wu et al., 2016). In this work, we explore Byte-Pair Encoding (BPE), a data compression algorithm (Gage, 1994) as an alternative to morphological segmentation of Arabic. BPE splits words into symbols (a sequence of characters) and then iteratively replaces the most frequent symbols with their merged variants. In essence, frequent character n-gram sequences will be merged to form one"
P17-2095,P12-2059,0,0.0607745,"ased on BPE, and iii) two variants of character-based segmentation. We first map each source word to its corresponding segments (depending on the segmentation scheme), embed all segments of a word in vector space and feed them one-by-one to an encoder-decoder model. See Figure 1 for illustration. 2.1 Figure 1: Segmentation approaches for the word “b$rhm” “ ÑëQå.”; the blue vectors indicate the embedding(s) used before the encoding layer. 2.3 Morphological Segmentation Character-based models have been found to be effective in translating closely related language pairs (Durrani et al., 2010; Nakov and Tiedemann, 2012) and OOV words (Durrani et al., 2014). Ling et al. (2016) used character embeddings to address the OOV word problem. We explored them as an alternative to morphological segmentation. Their advantage is that character embeddings do not require any complicated pre- and post-processing step other than segmenting words into characters. The fully character-level encoder treats the source sentence as a sequence of letters, encoding each letter (including white-space) in the LSTM encoder (see Figure 1). The decoding may follow identical settings. We restricted the character-level representation to th"
P17-2095,pasha-etal-2014-madamira,0,0.112051,"Missing"
P17-2095,N04-4038,0,\N,Missing
P17-2095,N16-3003,1,\N,Missing
P17-2095,2013.iwslt-evaluation.8,1,\N,Missing
temnikova-etal-2017-interpreting,C10-2010,0,\N,Missing
temnikova-etal-2017-interpreting,N16-1111,0,\N,Missing
temnikova-etal-2017-interpreting,tohyama-matsubara-2006-collection,0,\N,Missing
temnikova-etal-2017-interpreting,ma-2006-champollion,0,\N,Missing
W15-3059,W11-2101,0,0.0448672,"Missing"
W15-3059,W07-0718,0,0.744642,"ls with only target language information. 1 Introduction Each year thousands of human judgments are used to evaluate the quality of Machine Translation (MT) systems to determine which algorithms and techniques are to be considered the new state-ofthe-art. In a typical scenario human judges evaluate a system’s output (or hypothesis) by comparing it to a source sentence and/or to a reference translation. Then, they score the hypothesis according to a set of defined criteria such as fluency and adequacy (White et al., 1994); or rank a set of hypotheses in order of preference (Vilar et al., 2007; Callison-Burch et al., 2007). Evaluating MT output can be a challenging task for a number of reasons: it is tedious and therefore evaluators can lose interest quickly; it is complex, especially if the guidelines are not well defined; and evaluators can have difficulty distinguishing between different aspects of the translations (Callison-Burch et al., 2007). • Given different scenarios, what source of information do evaluators use to evaluate a translation? Do they use the source text, the target text, or both? Does the availability of specific information changes the consistency of the evaluation? • Are there difference"
W15-3059,W12-3102,0,0.110664,"Missing"
W15-3059,P14-1065,1,0.885127,"Missing"
W15-3059,2013.mtsummit-wptp.5,0,0.0518021,"Missing"
W15-3059,2006.amta-papers.25,0,0.201332,"Missing"
W15-3059,stymne-etal-2012-eye,0,0.215144,"information do evaluators use to evaluate a translation? Do they use the source text, the target text, or both? Does the availability of specific information changes the consistency of the evaluation? • Are there differences of behavior between bilinguals (i.e. evaluators fluent in both source and target languages) and monolinguals (i.e. evaluators fluent only in the target language)? Which group is more consistent? 457 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 457–466, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. Stymne et al. (2012) applied eye-tracking to machine translation error analysis. They found that longer gaze time and and a higher number of fixations correlate with high number of errors in the MT output. Doherty and O’Brien (2014) used eyetracking to evaluate the quality of raw machine translation output in terms of its usability by an end user. They concluded that eye-tracking correlates well with the other measures which they used for their study. In this work, we use eye-tracking to observe which sources of information evaluators use while performing an MT evaluation task and how this impacts the task comple"
W15-3059,2003.mtsummit-papers.51,0,0.22915,"Missing"
W15-3059,W07-0713,0,0.0259127,"er to use monolinguals with only target language information. 1 Introduction Each year thousands of human judgments are used to evaluate the quality of Machine Translation (MT) systems to determine which algorithms and techniques are to be considered the new state-ofthe-art. In a typical scenario human judges evaluate a system’s output (or hypothesis) by comparing it to a source sentence and/or to a reference translation. Then, they score the hypothesis according to a set of defined criteria such as fluency and adequacy (White et al., 1994); or rank a set of hypotheses in order of preference (Vilar et al., 2007; Callison-Burch et al., 2007). Evaluating MT output can be a challenging task for a number of reasons: it is tedious and therefore evaluators can lose interest quickly; it is complex, especially if the guidelines are not well defined; and evaluators can have difficulty distinguishing between different aspects of the translations (Callison-Burch et al., 2007). • Given different scenarios, what source of information do evaluators use to evaluate a translation? Do they use the source text, the target text, or both? Does the availability of specific information changes the consistency of the eval"
W15-3059,H93-1040,0,0.574996,"Missing"
W15-3059,1994.amta-1.25,0,0.487622,"Missing"
W15-3059,1993.mtsummit-1.24,0,\N,Missing
W15-3059,W14-3352,1,\N,Missing
W15-3059,W12-4906,0,\N,Missing
W15-3218,W14-3605,0,0.0422065,"ion approach that handles specific error types such as dialectal word substitution and word splits and merges with the aid of a language model. We also applied corrections that are specific to second language learners that handle erroneous preposition selection, definiteness, and gender-number agreement. 1 Introduction 2 In This paper, we provide a system description for our submissions to the Arabic error correction shared task (QALB-2015 Shared Task on Automatic Correction of Arabic) as part of the Arabic NLP workshop. The QALB-2015 shared task is an extension of the first QALB shared task (Mohit et al., 2014) which addressed errors in comments written to Aljazeera articles by native Arabic speakers (Zaghouani et al., 2014). The current competition includes two tracks, and, in addition to errors produced by native speakers, also includes correction of texts written by learners of Arabic as a foreign language (L2) (Zaghouani et al., 2015). The native track includes Aljtrain-2014, Alj-dev-2014, Alj-test-2014 texts from QALB-2014. The L2 track includes L2-train-2015 and L2-dev-2015. This data was released for the development of the systems. The systems were scored on blind test sets Alj-test-2015 and"
W15-3218,W14-3617,1,0.791492,"Missing"
W15-3218,P12-1049,0,0.0411773,"Missing"
W15-3218,zaghouani-etal-2014-large,0,\N,Missing
W15-3218,W15-1614,0,\N,Missing
W17-1302,D15-1274,0,0.435692,"Missing"
W17-1302,W05-0711,0,0.852256,"Missing"
W17-1302,pasha-etal-2014-madamira,0,0.231088,"Missing"
W17-1302,L16-1170,1,0.907508,"ring with systems that were trained on the ATB, some preprocessing is required, as we show later, to make sure that we are not unfairly penalizing them. 3 Training and Test Corpora 3.2 Data Preparation Given a word in the diacritized corpus, we produce multiple representations of it. To illustrate the representations, we use the word “wakitAbihimo” (and their book) as our running example. 1. diacritized surface form (“wakitAbihimo”). 2. diacritized surface form without case ending. To remove case endings, we segment each word in the corpus to its underlying clitics using the Farasa segmenter (Darwish and Mubarak, 2016). For example, given the diacritized word “wakitAbihimo” (and their book), it would be segmented to the prefix “wa”, stem “kitAbi”, and suffix “himo”. The Our Diacritizer The diacritizer has two main components. The first component recovers the diacritics for the core word (i.e. word without case ending), and the second only recovers the case ending. In this section we describe: the training and test corpora we used and how we processed them; the training of our system that diacritizes core-words and guesses 11 plates. In our example, the template “wfEAlhm” would be mapped to “wafiEAlihimo” an"
W17-1302,W02-0504,0,0.652492,"Missing"
W17-1302,W04-1612,0,0.738422,"Missing"
W17-1302,N07-2014,0,0.12851,"Missing"
W17-1302,P06-1073,0,0.923617,"Missing"
W17-1302,D11-1128,1,\N,Missing
W17-1302,P07-2045,0,\N,Missing
W17-1306,N16-3003,1,0.794973,"or testing, 75 for development and the remaining 200 for training. The concept We followed in LSTM sequence labeling is that segmentation is one-to-one mapping at the character level where each character is annotated as either beginning a segment (B), continues a previous segment (M), ends a segment (E), or is a segment by itself (S). After the labeling is complete we merge the characters and labels  together, for example @ñËñ®J K. byqwlwA is labeled as “SBMMEBE”, which means that the word is segmented as b+yqwl+wA. We compar results of our two LSTM models (BiLSTM and BiLSTMCRF) with Farasa (Abdelali et al., 2016), an open source segementer for MSA3 , and MADAMIRA for Egyptian dialect. Table 3 shows accuracy for Farasa, MADAMIRA, and both of our models. • ËQK @ñË@ AlwAyrls “the  AlHA$tAj “the hashtag”. h. AJAêË@ AlgTY “the Spelling variation: e.g. ù¢ªË@ cover”, úÎë B l&gt;hly “to Ahly”. wireless”, • Morphological inflection (imperative): e.g.  ¯ fwqwA “wake up”. ø Y $dy “pull”, @ñ¯ñ • Segmentation ambiguity: e.g. éJ Ë lyh meaning mAlnA meaning “our “why” or “to him”, AJËAÓ money” or “what we have”. • Combinations not known to MADAMIRA:  ® ® JÓ mtqflwhA$ “don’t close it”, e.g. AëñÊ @ &gt;wSflkwA “I"
W17-1306,W09-0807,0,0.049808,"instead dual and feminine plural, dropping some articles and preposition in some syntactic constructs, and using only one form yn inof noun and verb suffixes such as áK wn and stead of àð respectively. • Many words do not overlap with MSA as result of language borrowing from other lan guages (Ibrahim, 2006), such as éJ ¯A¿ kAfiyh ñKAK  mi$ “not” Ó  . balA$ “do not”. Code switching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects and MSA as well as English translation. • Merging multiple words together by concatenating and dropping letters such as the word  . J J.Ó mbyjlhA$ (he"
W17-1306,bouamor-etal-2014-multidialectal,0,0.121902,"tching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects and MSA as well as English translation. • Merging multiple words together by concatenating and dropping letters such as the word  . J J.Ó mbyjlhA$ (he did not go to her), AêÊj which is a concatenation of “mA byjy lhA$”. • Some affixes are altered in form from their MSA counterparts, such as the feminine second person pronoun  k → ú» ky and the second person plural pronoun wn @ð wA instead of àð • In addition, there are the regular discourse features in informal texts, such as the use of emoticons and character repetition for emp"
W17-1306,D14-1154,1,0.858589,"cters embedding and stacks them to build a matrix. This latter is then used as the input to the Bi-directional LSTM. On the last layer, an affine transformation function followed by a CRF computes the probability distribution over all labels Early Stopping We also employ early stopping (Caruana et al., 2000; Graves et al., 2013b) to mitigate overfitting by monitoring the model’s performance on development set. 4 ary respectively. The architecture of our segmentation model, shown in Figure 2, is straightforward. It comprises the following three layers: Dataset We used the dataset described in (Darwish et al., 2014). The data was used in a dialect identification task to distinguish between dialectal Egyptian and MSA. It contains 350 tweets with more than 8,000 words including 3,000 unique words written in Egyptian dialect. The tweets have much dialectal content covering most of dialectal Egyptian phonological, morphological, and syntactic phenomena. It also includes Twitter-specific aspects of the text, such as #hashtags, @mentions, emoticons and URLs. We manually annotated each word in this corpus to provide: CODA-compliant writing (Habash et al., 2012), segmentation, stem, lemma, and POS, also the corr"
W17-1306,W15-3904,0,0.0407118,"Missing"
W17-1306,W16-4828,1,0.865893,"is paper, we show how a segmenter can be trained on only 350 annotated tweets using neural networks without any normalization or reliance on lexical features or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources. 1 The advent of the social networks and the spread of smart phones, yielded the need for dialectaware smart systems and motivated the research in Dialectal Arabic such as dialectal Arabic identification for both text (Eldesouki et al., 2016) and speech (Khurana et al., 2016), morphological analysis (Habash et al., 2013) and machine translation (Sennrich et al., 2016; Sajjad et al., 2013). Due to the rich morphology in Arabic and its dialects, word segmentation is one of the most important processing steps. Word segmentation is considered an integral part for many higher Arabic NLP tasks such as part-of-speech tagging, parsing and machine translation. For example, the Egyp JºÓð “wmktbhA$” meaning: “and tian word AîD . he didn’t write it”) includes four clitics surrounding the the verb (stem) “ktb”, and is rendered after segment"
W17-1306,N16-1030,0,0.164915,"− x + W← −← − h t−1 + b← −) ht = σ(Wx← h t h h h → − ← − → h + W← − h + by yt = W− hy t hy t set of labels. In our case S ={B, M, E, S, WB}, where B is the beginning of a token, M is the middle of a token, E is the end of a token, S is a single character token, and W B is the word boundary. w ~ is the weight vector for weighting the feature vec~ Training and decoding are performed by the tor Φ. Viterbi algorithm. Note that replacing the softmax with CRF at the output layer in neural networks has proved to be very fruitful in many sequence labeling tasks (Ma and Hovy, 2016; Huang et al., 2015; Lample et al., 2016; Samih et al., 2016) More interpretations about these formulas are found in Graves et al. (2013a). A very important element of the recent success of many NLP applications, is the use of characterlevel representations in deep neural networks. This has shown to be effective for numerous NLP tasks (Collobert et al., 2011; dos Santos et al., 2015) as it can capture word morphology and reduce out-of-vocabulary. This approach has also been especially useful for handling languages with rich morphology and large character sets (Kim et al., 2016). We use pre-trained character embeddings to initialize"
W17-1306,P16-1101,0,0.269252,"(ct ) where σ is the logistic sigmoid function, and i, f , o and c are respectively the input gate, forget gate, output gate and cell activation vectors. More interpretation about this architecture can be found in (Lipton et al., 2015). Figure 1 illustrates a single LSTM memory cell (Graves and Schmidhuber, 2005) Arabic Segmentation Model In this section, we will provide a brief description of LSTM, and introduce the different components of our Arabic segmentation model. For all our work, we used the Keras toolkit (Chollet, 2015). The architecture of our model, shown in Figure 2 is similar to Ma and Hovy (2016), Huang et al. (2015), and Collobert et al. (2011) 3.1 Long Short-term Memory A recurrent neural network (RNN) belongs to a family of neural networks suited for modeling sequential data. Given an input sequence x = (x1 , ..., xn ), an RNN computes the output vector yt of each word xt by iterating the following equations from t = 1 to n: 1 Figure 1: A Long Short-Term Memory Cell. 3.2 Bi-directional LSTM Bi-LSTM networks (Schuster and Paliwal, 1997) are extensions to the single LSTM networks. They MADAMIRA release 20160516 2.1 48 are capable of learning long-term dependencies and maintain contex"
W17-1306,maamouri-etal-2014-developing,0,0.0957441,"Missing"
W17-1306,habash-etal-2012-conventional,0,0.430983,"their MSA counterparts, such as the feminine second person pronoun  k → ú» ky and the second person plural pronoun wn @ð wA instead of àð • In addition, there are the regular discourse features in informal texts, such as the use of emoticons and character repetition for emphasis, e.g. úÍððððððñ«X@ AdEwwwwwwwliy “pray for me”. tAtuw “tattoo”, or coinage, such as the negative particles Ég. @P rAjil “man” Ég. P rajul, and vowel shortening, such as AÖß X dayomA “always” from AÖß @X dAyomA. from • Lack of standard orthography. Many of the words in DA do not follow a standard orthographic system (Habash et al., 2012). “cafe” and H t or  s as in Q J» kvyr Õç' tm → ñK tw. • Some morphological patterns that do not exist in MSA, such as the passive pattern AitofaEal, such as QåºK@ Aitokasar “it broke”. 47 For segmentation, Yao and Huang (2016) successfully used a bi-directional LSTM model for segmenting Chinese text. In this paper, we build on their work and extend it in two ways, namely combining bi-LSTM with CRF and applying on Arabic, which is an alphabetic language. Mohamed et al. (2012) built a segmenter based on memory-based learning. The segmenter has been trained on a small corpus of Egyptian"
W17-1306,mohamed-etal-2012-annotating,0,0.411081,"Missing"
W17-1306,N13-1044,0,0.282692,"neural networks without any normalization or reliance on lexical features or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources. 1 The advent of the social networks and the spread of smart phones, yielded the need for dialectaware smart systems and motivated the research in Dialectal Arabic such as dialectal Arabic identification for both text (Eldesouki et al., 2016) and speech (Khurana et al., 2016), morphological analysis (Habash et al., 2013) and machine translation (Sennrich et al., 2016; Sajjad et al., 2013). Due to the rich morphology in Arabic and its dialects, word segmentation is one of the most important processing steps. Word segmentation is considered an integral part for many higher Arabic NLP tasks such as part-of-speech tagging, parsing and machine translation. For example, the Egyp JºÓð “wmktbhA$” meaning: “and tian word AîD . he didn’t write it”) includes four clitics surrounding the the verb (stem) “ktb”, and is rendered after segmentation as “w+m+ktb+hA+$”. The clitics in this word are the coordinate conjunction"
W17-1306,P14-2034,0,0.310795,"Missing"
W17-1306,pasha-etal-2014-madamira,0,0.190181,"Missing"
W17-1306,P13-2001,1,0.858088,"tures or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources. 1 The advent of the social networks and the spread of smart phones, yielded the need for dialectaware smart systems and motivated the research in Dialectal Arabic such as dialectal Arabic identification for both text (Eldesouki et al., 2016) and speech (Khurana et al., 2016), morphological analysis (Habash et al., 2013) and machine translation (Sennrich et al., 2016; Sajjad et al., 2013). Due to the rich morphology in Arabic and its dialects, word segmentation is one of the most important processing steps. Word segmentation is considered an integral part for many higher Arabic NLP tasks such as part-of-speech tagging, parsing and machine translation. For example, the Egyp JºÓð “wmktbhA$” meaning: “and tian word AîD . he didn’t write it”) includes four clitics surrounding the the verb (stem) “ktb”, and is rendered after segmentation as “w+m+ktb+hA+$”. The clitics in this word are the coordinate conjunction “w”, the negation prefix “m”, the object pronoun “hA”, and the post"
W17-1306,W16-5806,1,0.909334,"under lenition, softening of a consonant, or fortition, hardening of a consonant. • Vowel elongation, such as • The use of masculine plural or singular noun forms instead dual and feminine plural, dropping some articles and preposition in some syntactic constructs, and using only one form yn inof noun and verb suffixes such as áK wn and stead of àð respectively. • Many words do not overlap with MSA as result of language borrowing from other lan guages (Ibrahim, 2006), such as éJ ¯A¿ kAfiyh ñKAK  mi$ “not” Ó  . balA$ “do not”. Code switching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple"
W17-1306,P16-1162,0,0.049716,"reliance on lexical features or linguistic resources. We deal with segmentation as a sequence labeling problem at the character level. We show experimentally that our model can rival state-of-the-art methods that heavily depend on additional resources. 1 The advent of the social networks and the spread of smart phones, yielded the need for dialectaware smart systems and motivated the research in Dialectal Arabic such as dialectal Arabic identification for both text (Eldesouki et al., 2016) and speech (Khurana et al., 2016), morphological analysis (Habash et al., 2013) and machine translation (Sennrich et al., 2016; Sajjad et al., 2013). Due to the rich morphology in Arabic and its dialects, word segmentation is one of the most important processing steps. Word segmentation is considered an integral part for many higher Arabic NLP tasks such as part-of-speech tagging, parsing and machine translation. For example, the Egyp JºÓð “wmktbhA$” meaning: “and tian word AîD . he didn’t write it”) includes four clitics surrounding the the verb (stem) “ktb”, and is rendered after segmentation as “w+m+ktb+hA+$”. The clitics in this word are the coordinate conjunction “w”, the negation prefix “m”, the object prono"
W17-1306,J14-1006,0,0.025968,"ing some articles and preposition in some syntactic constructs, and using only one form yn inof noun and verb suffixes such as áK wn and stead of àð respectively. • Many words do not overlap with MSA as result of language borrowing from other lan guages (Ibrahim, 2006), such as éJ ¯A¿ kAfiyh ñKAK  mi$ “not” Ó  . balA$ “do not”. Code switching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects and MSA as well as English translation. • Merging multiple words together by concatenating and dropping letters such as the word  . J J.Ó mbyjlhA$ (he did not go to her), AêÊj which is a concatenation of"
W17-1306,N12-1006,0,0.0419555,"inine plural, dropping some articles and preposition in some syntactic constructs, and using only one form yn inof noun and verb suffixes such as áK wn and stead of àð respectively. • Many words do not overlap with MSA as result of language borrowing from other lan guages (Ibrahim, 2006), such as éJ ¯A¿ kAfiyh ñKAK  mi$ “not” Ó  . balA$ “do not”. Code switching is and CK also very common in Arabic dialects (Samih et al., 2016). 2 Related Work Work on dialectal Arabic is fairly new compared to MSA. A number of research projects were devoted to dialect identification (Biadsy et al., 2009; Zbib et al., 2012; Zaidan and Callison-Burch, 2014). There are five major dialects including Egyptian, Gulf, Iraqi, Levantine and Maghribi. Few resources for these dialects are available such as the CALLHOME Egyptian Arabic Transcripts (LDC97T19), which was made available for research as early as 1997. Newly developed resources include the corpus developed by Bouamor et al. (2014), which contains 2,000 parallel sentences in multiple dialects and MSA as well as English translation. • Merging multiple words together by concatenating and dropping letters such as the word  . J J.Ó mbyjlhA$ (he did not go to her),"
W17-1316,darwish-etal-2014-using,1,0.942927,"engineering and word embeddigns in Arabic POS tagging. We show that feature engineering improves POS tagging significantly. • We explore the effectiveness of many features including morphological and contextual features for tagging each clitic or each word in-context. 130 Proceedings of The Third Arabic Natural Language Processing Workshop (WANLP), pages 130–137, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics tagsets such as that of the Penn Arabic Treebank (ATB), which has 70 tags (Maamouri et al., 2004). In our work, we elected to use the tagest proposed by Darwish et al. (2014) which is a simplified version of ATB tagset and uses 18 tags only. • We open-source both Arabic POS taggers, both of which are written entirely in Java. The SVMRank -based system has a load time of 5 seconds and can process about 2,000 word/second on an laptop with Intel i7 processor with 16 GB of RAM. 2 2.1 2.2 Background Arabic POS Tagging Most recent work on Arabic POS tagging has used statistical methods. Diab (2009) used an SVM classifier to ascertain the optimal POS tags. The classifier was trained on the ATB data. Essentially, they treated the problem as a sequence-labeling problem. An"
W17-1316,N12-1015,0,0.0186639,"Missing"
W17-1316,N13-1090,0,0.0296891,"tput i as follows: li = tanh(Lf Sif + Lb Sib + bl ) where Lf , Lb and bl denote the parameters for combining the forward and backward states. We experimented with a number of settings where the clitic sequence was augmented with a subset of features that includes character sequences, word meta type, stem template (Darwish et al., 2014), and also combined with 200 dimension word embeddings learned over the aforementioned collection of text containing 10 years of Al-Jazeera articles1 . To create the embeddings, we used word2vec with continuous skip-gram learning algorithm with an 8 gram window (Mikolov et al., 2013)2 . For the bi-LSTM experiments, we used the Java Neural Network Library3 , which is tuned for POS tagging(Ling et al., 2015). We extended the library to produce the additional aforementioned features. • Word context features: p(P OS|w−1 ), p(P OS|w1 ), p(P OS|w−2 , w−1 ), p(P OS|w−3 , w−2 , w−1 ), and p(P OS|w−4 , w−3 , w−2 , w−1 ) 3.1.3 OOVs and pre-Filtering For both clitic and word tagging, In case we could not compute a feature value during training (e.g., a clitic was never observed with a given POS tag), the feature value is assigned a small  value equal to 10−10 . If the clitic is a p"
W17-1316,pasha-etal-2014-madamira,0,0.226248,"Missing"
W17-1316,P16-2067,0,0.0573499,"Missing"
W17-1316,P09-2056,0,\N,Missing
W17-1316,L16-1170,1,\N,Missing
W19-4603,J92-4003,0,0.551827,"Missing"
W19-4603,W16-5801,0,0.0290695,"Missing"
W19-4603,P13-2037,0,0.0590321,"Missing"
W19-4603,W16-5812,0,0.0243184,"is relatively less challenging for computational analysis, as each sentence still follows a monolingual model, intrasentential CS poses a bottleneck challenge. It needs a special amount of attention, because it is only this type that involves the lexical and syntactic integration and activation of two language models at the same time. NLP systems trained on monolingual data suffer significantly when trying to process this kind bilingual text or utterance. CS has proved challenging for NLP technologies, not only because current tools are geared toward the processing of one language at a time (AlGhamdi et al., 2016), but also because codeswitched data is typically associated with additional challenges such as the non-conventional orthography, non-canonicity (nonstandard or incomplete) of syntactic structures, and the large number of OOV-words (Çetino˘glu et al., 2016), which suggest the need for larger training data than what is typically used in monolingual models. Unfortunately, shortage of training data has usually been cited as the reason for the under-performance of 19 diglossic code-switching, the shift is more likely to be lexical, morphological, and structural, rather than phonological, unlike th"
W19-4603,C12-2011,1,0.885038,"Missing"
W19-4603,Q16-1026,0,0.0173957,"nsure that we get representations of all the words and reduce the number of OOVs (out of vocabulary words). We find significant improvement using FastText embedding over the traditional word2vec representation (Mikolov et al., 2013). This is probably due to the utilization of sub-word (ex. prefixes or suffixes) information in the former. Character-level CNNs. Although originally designed for image recognition, CNNs have proven effective for various NLP tasks due to their ability to encode character-level representations of words as well as extract sub-word information (Collobert et al., 2011; Chiu and Nichols, 2016; dos Santos and Guimarães, 2015). Bi-LSTM Recurrent neural networks (RNN) are well suited for modeling sequential data, achieving ground-breaking results in many NLP tasks (e.g., machine translation). BiLSTMs (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997) are capable of learning long-term dependencies and maintaining contextual features from both past and future states while avoiding the vanishing/exploding gradients problem. They consist of two separate bidirectional hidden layers that feed forward to the same output layer. Figure 3: DNN Architecture. BCs on our crawled code-"
W19-4603,attia-etal-2010-automatically,1,0.834799,"Missing"
W19-4603,L18-1015,1,0.899707,"Missing"
W19-4603,W14-3901,0,0.0289644,"tagging of CS data and concluded that applying a machine learning framework as a voting mechanism on top of the output of two monolingual POS taggers achieves the best performance. Word-level CS identification for Arabic (along with Spanish–English) has been featured in a couple of shared tasks: the First Shared Task on Language Identification in CodeSwitched Data (Solorio et al., 2014) and the Second Shared Task on Language Identification in Code-Switched Data (Molina et al., 2016), of which Samih et al. (2016) was the winning system, and against which we compare our results in this project. Eskander et al. (2014) studied CS between EA written in Roman script (Arabizi) and English. Habash et al. (2008) created a standard annotation guidelines for CS between MSA and dialects. CS has also been studied in Arabic as a predictor of social influence in the collaborative writing in Wikipedia discussion pages in (Yoder et al., 2017) and it was found that CS is positively associated with the editor’s success in winning an argument. We notice from the literature that in some instances POS tagging has been used to aid with the identification of code-switching points, and in some other instances language identific"
W19-4603,Q17-1010,0,0.0325773,"Missing"
W19-4603,N13-1039,0,0.0935945,"Missing"
W19-4603,C82-1023,0,0.578981,"tactic rules of the two languages involved, sometime adding in or leaving out a determiner, or applying a system of affixation from one language and not the other. 1.2 Definition and Defining Perspectives The definition of CS has varied greatly depending on the different researchers’ attitude and perspectives of the operation involved. While some viewed it as a process where two languages are actively interacting with each other (ultimately creating a new code), other viewed the operation just as two separate languages sitting side-by-side as isolated islands. Following the first perspective, Joshi (1982) defined code-switching as the situation when two languages systematically interact with each other in the production of sentences in a framework which consists of two grammatical systems and a mechanism for switching between the two. Following the second perspective, Muysken (1995) defined CS as “the alternative use by bilinguals of two or more languages in the same conversation”, while other researchers (Auer, 1999; Nilep, 2006) defined it as the “juxtaposition” of elements from two different grammatical systems within the same speech. The juxtaposition definition has been widely cited in th"
W19-4603,N16-1030,0,0.01,"®K A£ É¯ BA« System Description Deep learning and neural nets have been used extensively in the past decade and were shown to significantly outperform traditional (linear) ML models. The proclaimed advantage of deep learning is that it eliminates the need for feature engineering. Yet, there has been a growing interest recently to augment neural nets with more and more linguistic features, which has been shown to boost performance for many tasks. We use a DNN (Deep Neural Network) model mainly suited for sequence tagging and is a variant of the bi-LSTM-CRF architecture (Ma and Hovy, 2016; Lample et al., 2016; Reimers and Gurevych, 2017; Huang et al., 2015). Our implementation is mostly inspired by the work of Reimers and Gurevych (2017). In its basic configuration, it combines a double representation of the input words by using word embeddings and a character-based representation with CNNs (convolutional Neural Networks). The input sequence is processed with bi-LSTMs, and the output layer is a linear chain CRF. We augment this model with various layers to accommodate the different features we want to incorporate. The features used in our model are explained below. 4.1 Translit. / Gloss byHbk Fine"
W19-4603,D17-1035,0,0.0140169,"tem Description Deep learning and neural nets have been used extensively in the past decade and were shown to significantly outperform traditional (linear) ML models. The proclaimed advantage of deep learning is that it eliminates the need for feature engineering. Yet, there has been a growing interest recently to augment neural nets with more and more linguistic features, which has been shown to boost performance for many tasks. We use a DNN (Deep Neural Network) model mainly suited for sequence tagging and is a variant of the bi-LSTM-CRF architecture (Ma and Hovy, 2016; Lample et al., 2016; Reimers and Gurevych, 2017; Huang et al., 2015). Our implementation is mostly inspired by the work of Reimers and Gurevych (2017). In its basic configuration, it combines a double representation of the input words by using word embeddings and a character-based representation with CNNs (convolutional Neural Networks). The input sequence is processed with bi-LSTMs, and the output layer is a linear chain CRF. We augment this model with various layers to accommodate the different features we want to incorporate. The features used in our model are explained below. 4.1 Translit. / Gloss byHbk Fine Tag prog_part Coarse Tag Ve"
W19-4603,P08-2030,0,0.104649,"Missing"
W19-4603,P16-1101,0,0.0119628,".j K. QÒªË@ð J.Ê¯ ®K A£ É¯ BA« System Description Deep learning and neural nets have been used extensively in the past decade and were shown to significantly outperform traditional (linear) ML models. The proclaimed advantage of deep learning is that it eliminates the need for feature engineering. Yet, there has been a growing interest recently to augment neural nets with more and more linguistic features, which has been shown to boost performance for many tasks. We use a DNN (Deep Neural Network) model mainly suited for sequence tagging and is a variant of the bi-LSTM-CRF architecture (Ma and Hovy, 2016; Lample et al., 2016; Reimers and Gurevych, 2017; Huang et al., 2015). Our implementation is mostly inspired by the work of Reimers and Gurevych (2017). In its basic configuration, it combines a double representation of the input words by using word embeddings and a character-based representation with CNNs (convolutional Neural Networks). The input sequence is processed with bi-LSTMs, and the output layer is a linear chain CRF. We augment this model with various layers to accommodate the different features we want to incorporate. The features used in our model are explained below. 4.1 Transli"
W19-4603,W16-5806,1,0.944761,"abels ambiguous unk lang1 lang2 mixed ne other et al. (2016) explored different technique for the POS tagging of CS data and concluded that applying a machine learning framework as a voting mechanism on top of the output of two monolingual POS taggers achieves the best performance. Word-level CS identification for Arabic (along with Spanish–English) has been featured in a couple of shared tasks: the First Shared Task on Language Identification in CodeSwitched Data (Solorio et al., 2014) and the Second Shared Task on Language Identification in Code-Switched Data (Molina et al., 2016), of which Samih et al. (2016) was the winning system, and against which we compare our results in this project. Eskander et al. (2014) studied CS between EA written in Roman script (Arabizi) and English. Habash et al. (2008) created a standard annotation guidelines for CS between MSA and dialects. CS has also been studied in Arabic as a predictor of social influence in the collaborative writing in Wikipedia discussion pages in (Yoder et al., 2017) and it was found that CS is positively associated with the editor’s success in winning an argument. We notice from the literature that in some instances POS tagging has been use"
W19-4603,W16-5805,0,0.0180421,"themselves, but our results show that words still give a stronger signal than POS tags alone. We also notice that Brown Clusters, named entity gazetteers and FastText pre-trained embeddings contribute to incrementally improve the performance of the system. Unfortunately adding information from the spelling word list did not show any improvement on the system, and this is why it is removed from the final system architecture. Now we compare our best model to the state-ofthe-art system of Samih et al. (2016), which won the 2016 Second Shared Task on Language Identification in Code-Switched Data (Molina et al., 2016) on the MSA–EA dataset. We compare the performance of the two systems in terms of f-score accuracy on both the development and test set, in Table 5 and Table 6 respectively. We also include the number of instances and the ratio percentage for each label. As the tables show, the category lang2 constitutes the majority class for both úÍ@ <ilY “to”, which can equally be used as either lang1 or lang2, depending on the context. 6 Conclusion We have presented a neural network system for conducting word-level code-switching identification. Our system outperforms the current stateof-the-art, and we sh"
W19-4603,W15-3904,0,0.0125713,"tions of all the words and reduce the number of OOVs (out of vocabulary words). We find significant improvement using FastText embedding over the traditional word2vec representation (Mikolov et al., 2013). This is probably due to the utilization of sub-word (ex. prefixes or suffixes) information in the former. Character-level CNNs. Although originally designed for image recognition, CNNs have proven effective for various NLP tasks due to their ability to encode character-level representations of words as well as extract sub-word information (Collobert et al., 2011; Chiu and Nichols, 2016; dos Santos and Guimarães, 2015). Bi-LSTM Recurrent neural networks (RNN) are well suited for modeling sequential data, achieving ground-breaking results in many NLP tasks (e.g., machine translation). BiLSTMs (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997) are capable of learning long-term dependencies and maintaining contextual features from both past and future states while avoiding the vanishing/exploding gradients problem. They consist of two separate bidirectional hidden layers that feed forward to the same output layer. Figure 3: DNN Architecture. BCs on our crawled code-switched corpus of 380 million wo"
W19-4603,N16-1159,0,0.025454,"Missing"
W19-4603,D08-1102,0,0.0258644,"Missing"
W19-4603,D08-1110,0,0.107844,"Missing"
W19-4603,W15-1511,0,0.061289,"Missing"
W19-4603,W15-2902,0,0.0329637,"Missing"
W19-4603,W17-2911,0,0.0157523,"n Language Identification in CodeSwitched Data (Solorio et al., 2014) and the Second Shared Task on Language Identification in Code-Switched Data (Molina et al., 2016), of which Samih et al. (2016) was the winning system, and against which we compare our results in this project. Eskander et al. (2014) studied CS between EA written in Roman script (Arabizi) and English. Habash et al. (2008) created a standard annotation guidelines for CS between MSA and dialects. CS has also been studied in Arabic as a predictor of social influence in the collaborative writing in Wikipedia discussion pages in (Yoder et al., 2017) and it was found that CS is positively associated with the editor’s success in winning an argument. We notice from the literature that in some instances POS tagging has been used to aid with the identification of code-switching points, and in some other instances language identification has been used as an indicator or a feature for POS tagging, showing what (Çetino˘glu et al., 2016) referred to as task inter-relatedness, or the cyclic nature of task dependencies. In our work, we use a POS tagger as a predictor of CS. The POS tagger used has been trained specifically on CS data. 3 Token Count"
W19-4605,S16-1086,0,0.0293136,"Missing"
W19-4605,P14-1006,0,0.125575,"Missing"
W19-4605,R13-2008,0,0.0565617,"Missing"
W19-4605,2005.mtsummit-papers.11,0,0.0274595,"Missing"
W19-4605,S17-2001,0,0.0814577,"Missing"
W19-4605,2012.eamt-1.60,0,0.0574591,"Missing"
W19-4605,W15-1521,0,0.120445,"Missing"
W19-4605,Q18-1039,0,0.0478041,"Missing"
W19-4605,L18-1218,0,0.0277428,"Missing"
W19-4605,N13-1090,0,0.743604,", August 1, 2019. 2019 Association for Computational Linguistics ing the same architecture in a Wikipedia corpora of 104 different languages, requiring not a single alignment signal and realising, if not outperforming, state-of-the-art score in many NLP tasks such as Part Of Speech Tagging and Named Entity Recognition. However, BERT demands significantly more machine effort (Wu and Dredze, 2019). Table 1 summarises the cross-language embedding models mentioned above according to the architecture and used corpus, the target languages and the evaluation methods. 2010), (Mnih and Hinton, 2009), (Mikolov et al., 2013c,b) and (Peters et al., 2018). In the cross-lingual context, several word embedding models are proposed. Blunsom and Hermann (2014) introduced a Bilingual Compositional Model (BiCVM). Leveraging from the fact that aligned sentences have the same meaning. BiCVM is based on a sentence-aligned corpus to learn the bilingual word embedding vectors. Vuli´c and Moens (2015b) introduced a Bilingual Word Embedding Skip-Gram (BWESG), this model is constructed through three main steps: i) prepare a Skip-Gram Negative Sampling (Mikolov et al., 2013b) architecture that deals with document aligned comparab"
W19-4605,D18-1027,0,0.0203385,"ing Model Raki Lachraf Echahid Hamma Lakhdar University, El Oued, Algeria El Moatez Billah Nagoudi Echahid Hamma Lakhdar University, El Oued, Algeria LIM laboratory, Laghouat raki.lachraf@univ-eloued.dz moatez-nagoudi@univ-eloued.dz Didier Schwab Ahmed Abdelali Youcef Ayachi LIG-GETALP Echahid Hamma Lakhdar Hamad Bin Khalifa University Qatar Computing Research Institute Univ. Grenoble Alpes, University France Doha, Qatar El Oued, Algeria youcef.ayachi@univ-eloued.dz aabdelali@qf.org.qa Abstract more challenging task because the knowledge is transferred between two or more different languages (Doval et al., 2018). Recently, crosslingual word embeddings was used to address several issues, e.g. machine translation (Zou et al., 2013), cross-language information retrieval (Vuli´c and Moens, 2015a; Zhou et al., 2012), crosslanguage semantic similarity (Ataman et al., 2016; Nagoudi et al., 2017b) and plagiarism detection across multiple languages (Ferrero et al., 2017; Barr´on-Cede˜no et al., 2013). Many cross-lingual word embedding models in natural language have been developed, particularly for English, but Arabic did not get that much of interest. In this paper, we propose six Arabic-English cross-lingua"
W19-4605,E17-2066,1,0.816012,"esearch Institute Univ. Grenoble Alpes, University France Doha, Qatar El Oued, Algeria youcef.ayachi@univ-eloued.dz aabdelali@qf.org.qa Abstract more challenging task because the knowledge is transferred between two or more different languages (Doval et al., 2018). Recently, crosslingual word embeddings was used to address several issues, e.g. machine translation (Zou et al., 2013), cross-language information retrieval (Vuli´c and Moens, 2015a; Zhou et al., 2012), crosslanguage semantic similarity (Ataman et al., 2016; Nagoudi et al., 2017b) and plagiarism detection across multiple languages (Ferrero et al., 2017; Barr´on-Cede˜no et al., 2013). Many cross-lingual word embedding models in natural language have been developed, particularly for English, but Arabic did not get that much of interest. In this paper, we propose six Arabic-English cross-lingual word embedding models1 . To train these models, we have used a large collection with more than 93 million pairs of parallel ArabicEnglish sentences. The rest of this paper is organised as follows: in section 2 we provide a quick overview of work related to the cross-lingual word embedding models. We describe our dataset collection and the preprocessing"
W19-4605,S18-1055,1,0.821111,"Textual Similarity (STS), while the intrinsic evaluation is based on the Word Translation (WT) task. 1 didier.schwab@imag.fr Introduction Distributed word representations in vector space (Word Embeddings) are one of the most successful applications in deep learning for capturing the semantic and syntactic properties of words. Lately, many NLP tasks have been enriched using tools based on Mono and Cross-Lingual word embedding models. For instance, Mono-Lingual Word Embeddings (MLWE) have been widely used in information retrieval (Vuli´c and Moens, 2015a), sentiment analysis (Tang et al., 2014; Nagoudi, 2018) text classification (Lai et al., 2015), semantic textual similarity (Kenter and De Rijke, 2015; Nagoudi and Schwab, 2017) and plagiarism detection (Nagoudi et al., 2018). Cross-Lingual Word Embeddings (CLWE) is a 2 Related works While we focus on the cross-lingual word embedding models, the interested reader may refer to a number of research studies on the subject of mono-lingual word embeddings in general (Collobert and Weston, 2008), (Turian et al., 1 All models can be downloaded from : https://github.com/Raki22/ArbEngVec.git 40 Proceedings of the Fourth Arabic Natural Language Processing W"
W19-4605,S17-2017,1,0.923432,"GETALP Echahid Hamma Lakhdar Hamad Bin Khalifa University Qatar Computing Research Institute Univ. Grenoble Alpes, University France Doha, Qatar El Oued, Algeria youcef.ayachi@univ-eloued.dz aabdelali@qf.org.qa Abstract more challenging task because the knowledge is transferred between two or more different languages (Doval et al., 2018). Recently, crosslingual word embeddings was used to address several issues, e.g. machine translation (Zou et al., 2013), cross-language information retrieval (Vuli´c and Moens, 2015a; Zhou et al., 2012), crosslanguage semantic similarity (Ataman et al., 2016; Nagoudi et al., 2017b) and plagiarism detection across multiple languages (Ferrero et al., 2017; Barr´on-Cede˜no et al., 2013). Many cross-lingual word embedding models in natural language have been developed, particularly for English, but Arabic did not get that much of interest. In this paper, we propose six Arabic-English cross-lingual word embedding models1 . To train these models, we have used a large collection with more than 93 million pairs of parallel ArabicEnglish sentences. The rest of this paper is organised as follows: in section 2 we provide a quick overview of work related to the cross-lingual word"
W19-4605,D19-1077,0,0.0588195,"Missing"
W19-4605,W11-2162,0,0.147859,"Missing"
W19-4605,W17-1303,1,0.855417,"chwab@imag.fr Introduction Distributed word representations in vector space (Word Embeddings) are one of the most successful applications in deep learning for capturing the semantic and syntactic properties of words. Lately, many NLP tasks have been enriched using tools based on Mono and Cross-Lingual word embedding models. For instance, Mono-Lingual Word Embeddings (MLWE) have been widely used in information retrieval (Vuli´c and Moens, 2015a), sentiment analysis (Tang et al., 2014; Nagoudi, 2018) text classification (Lai et al., 2015), semantic textual similarity (Kenter and De Rijke, 2015; Nagoudi and Schwab, 2017) and plagiarism detection (Nagoudi et al., 2018). Cross-Lingual Word Embeddings (CLWE) is a 2 Related works While we focus on the cross-lingual word embedding models, the interested reader may refer to a number of research studies on the subject of mono-lingual word embeddings in general (Collobert and Weston, 2008), (Turian et al., 1 All models can be downloaded from : https://github.com/Raki22/ArbEngVec.git 40 Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 40–48 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics ing the same archit"
W19-4605,C12-1164,0,0.0306475,"oatez-nagoudi@univ-eloued.dz Didier Schwab Ahmed Abdelali Youcef Ayachi LIG-GETALP Echahid Hamma Lakhdar Hamad Bin Khalifa University Qatar Computing Research Institute Univ. Grenoble Alpes, University France Doha, Qatar El Oued, Algeria youcef.ayachi@univ-eloued.dz aabdelali@qf.org.qa Abstract more challenging task because the knowledge is transferred between two or more different languages (Doval et al., 2018). Recently, crosslingual word embeddings was used to address several issues, e.g. machine translation (Zou et al., 2013), cross-language information retrieval (Vuli´c and Moens, 2015a; Zhou et al., 2012), crosslanguage semantic similarity (Ataman et al., 2016; Nagoudi et al., 2017b) and plagiarism detection across multiple languages (Ferrero et al., 2017; Barr´on-Cede˜no et al., 2013). Many cross-lingual word embedding models in natural language have been developed, particularly for English, but Arabic did not get that much of interest. In this paper, we propose six Arabic-English cross-lingual word embedding models1 . To train these models, we have used a large collection with more than 93 million pairs of parallel ArabicEnglish sentences. The rest of this paper is organised as follows: in s"
W19-4605,N18-1202,0,0.172265,"Missing"
W19-4605,D13-1141,0,0.0151394,"niversity, El Oued, Algeria LIM laboratory, Laghouat raki.lachraf@univ-eloued.dz moatez-nagoudi@univ-eloued.dz Didier Schwab Ahmed Abdelali Youcef Ayachi LIG-GETALP Echahid Hamma Lakhdar Hamad Bin Khalifa University Qatar Computing Research Institute Univ. Grenoble Alpes, University France Doha, Qatar El Oued, Algeria youcef.ayachi@univ-eloued.dz aabdelali@qf.org.qa Abstract more challenging task because the knowledge is transferred between two or more different languages (Doval et al., 2018). Recently, crosslingual word embeddings was used to address several issues, e.g. machine translation (Zou et al., 2013), cross-language information retrieval (Vuli´c and Moens, 2015a; Zhou et al., 2012), crosslanguage semantic similarity (Ataman et al., 2016; Nagoudi et al., 2017b) and plagiarism detection across multiple languages (Ferrero et al., 2017; Barr´on-Cede˜no et al., 2013). Many cross-lingual word embedding models in natural language have been developed, particularly for English, but Arabic did not get that much of interest. In this paper, we propose six Arabic-English cross-lingual word embedding models1 . To train these models, we have used a large collection with more than 93 million pairs of par"
W19-4605,P14-1146,0,0.0438114,"-language Semantic Textual Similarity (STS), while the intrinsic evaluation is based on the Word Translation (WT) task. 1 didier.schwab@imag.fr Introduction Distributed word representations in vector space (Word Embeddings) are one of the most successful applications in deep learning for capturing the semantic and syntactic properties of words. Lately, many NLP tasks have been enriched using tools based on Mono and Cross-Lingual word embedding models. For instance, Mono-Lingual Word Embeddings (MLWE) have been widely used in information retrieval (Vuli´c and Moens, 2015a), sentiment analysis (Tang et al., 2014; Nagoudi, 2018) text classification (Lai et al., 2015), semantic textual similarity (Kenter and De Rijke, 2015; Nagoudi and Schwab, 2017) and plagiarism detection (Nagoudi et al., 2018). Cross-Lingual Word Embeddings (CLWE) is a 2 Related works While we focus on the cross-lingual word embedding models, the interested reader may refer to a number of research studies on the subject of mono-lingual word embeddings in general (Collobert and Weston, 2008), (Turian et al., 1 All models can be downloaded from : https://github.com/Raki22/ArbEngVec.git 40 Proceedings of the Fourth Arabic Natural Langu"
W19-4605,tiedemann-2012-parallel,0,0.0459589,"Missing"
W19-4605,P10-1040,0,0.0737633,"al., 2013a), as illustrated in figure 1.  The letters @ , @ , @ are replaced with @ while  the letter è is replaced with è. Also, The letter ø followed by Z replaced with ø . We converted elongated words back to their  original form, example : èYë@@@@@@AªÓ, which means treaty in English, and QK@ Qm .Ì '@ , which means Algeria will be converted to èYëAªÓ, QK@ Qm Ì '@. . 3. In addition, we remove the stop-words from Arabic and English sentences. 4 4.1 Building ArbEngVec Models Used Architectures In Mikolov et al. (2013a) all the word embedding models (Collobert and Weston, 2008), (Turian et al., 2010), (Mnih and Hinton, 2009), (Mikolov et al., 2010), (Mikolov et al., 2013c) and (Mikolov et al., 2013b) have been compared and evaluated, and they show that CBOW (Mikolov et al., 2013c) and Skip-Gram (Mikolov et al., 2013b) models are significantly faster to train with better accuracy. Accordingly, we used the CBOW and SkipGram to build our Arabic-English cross-lingual word embedding models. Figure 1: Architecture of CBOW and Skip-gram as described in (Mikolov et al., 2013b) 4.2 Proposed Models In this section, we present our proposed ArbEngVec models. In order to learn our models, we have reli"
W19-4639,W18-3930,0,0.112038,"Missing"
W19-4639,W16-4818,0,0.0429729,"Missing"
W19-4639,E17-2068,0,0.0808269,"Missing"
W19-4639,W14-3601,1,0.945204,"Missing"
W19-4639,C18-1113,0,0.0919411,"Missing"
W19-4639,K17-1043,1,0.900152,"Missing"
W19-4639,L16-1658,1,0.898269,"Missing"
W19-4639,P11-2007,0,0.0851928,"Missing"
W19-4639,J14-1006,0,0.163523,"2016). The wide spread of dialectal use has increased the richness and diversity of the language, requiring greater complexity in dealing with it. Non-standard orthography, increased borrowing and coinage of new terms, and code switching are just a few among a long list of new challenges researchers have to deal with. Studying language varieties in particular is associated with important applications such as Dialect Identification (DID), Machine Translation (MT), and other text mining tasks. Performing DID can be achieved using a variety of features, such as character n-grams (Darwish, 2014; Zaidan and Callison-Burch, 2014; Malmasi et al., 2015), and a myriad of techniques, such as 2 System descriptions For both SubTask 1 and SubTask 2, we employed a hybrid system that incorporates different classifiers and components such DNNs and heuristics to perform sentence level dialectal Arabic identification. The classification strategy is built as a cascaded voting system that tags each sequence based on the decisions from two other underlying classifiers. DNNs: This model uses both Bidirectional Long Short Term Memory (Bi-LSTM) and Convolutional Neural Network (CNN) architectures to jointly learn both word-level and c"
W19-8713,L18-1336,1,0.84231,"slation quality (Waibel and Fuegen, 2012; Bangalore et al., 2012; Sridhar et al., 2013b; Schmid and Garside, 2005), few approaches are getting explicitly inspired by human interpreting, by learning from the strategies which interpreters employ in order to produce good quality translation (Niehues et al., 2016; He et al., 2015; Sridhar et al., 2013a). In line with this area of research, starting with an initial objective to boost a speech machine translation system working with English/Arabic language pair (Dalvi et al., 2017) we conduct experiments on a subset of sessions from the WAW corpus (Abdelali et al., 2018) - a corpus of simultaneously interpreted conference speeches, to get informed about interpreters’ behaviour and learn which strategies interpreters employ to maintain good output accuracy while in the same time not exceeding their delay from the speaker. Our task is complex, as we want to find a way in which human expertise in interpreting can boost the performance of speech machine translation systems. With this article, we are enriching our previous research (Temnikova et al., 2017; Abdelali et al., 2018) and run an extensive multilateral analysis on a subset of WAW corpus interpreted sessi"
W19-8713,E09-1011,0,0.0304885,"Missing"
W19-8713,N12-1048,0,0.0783737,"Missing"
W19-8713,N16-1111,0,0.0345043,"Missing"
W19-8713,D15-1006,0,0.0395242,"Missing"
W19-8713,E17-3016,1,0.830292,"Missing"
W19-8713,W17-1316,1,0.745554,"Missing"
W19-8713,ono-etal-2008-construction,0,0.10042,"Missing"
W19-8713,N03-1033,0,0.111272,"Missing"
W19-8713,2013.iwslt-papers.3,0,0.107904,"Missing"
W19-8713,N13-1023,0,0.0150121,"Institute, HBKU, Doha, Qatar 3 University of Alicante, Spain 4 Translation and Interpretation Institute, HBKU, Doha, Qatar . 1 irina.temnikova@gmail.com, 2 aabdelali@hbku.edu.qa, 3 sd89@alu.ua.es, 4 SHedaya@hbku.edu.qa Abstract time maintaining good automatic speech translation quality (Waibel and Fuegen, 2012; Bangalore et al., 2012; Sridhar et al., 2013b; Schmid and Garside, 2005), few approaches are getting explicitly inspired by human interpreting, by learning from the strategies which interpreters employ in order to produce good quality translation (Niehues et al., 2016; He et al., 2015; Sridhar et al., 2013a). In line with this area of research, starting with an initial objective to boost a speech machine translation system working with English/Arabic language pair (Dalvi et al., 2017) we conduct experiments on a subset of sessions from the WAW corpus (Abdelali et al., 2018) - a corpus of simultaneously interpreted conference speeches, to get informed about interpreters’ behaviour and learn which strategies interpreters employ to maintain good output accuracy while in the same time not exceeding their delay from the speaker. Our task is complex, as we want to find a way in which human expertise"
W19-8713,temnikova-etal-2017-interpreting,1,0.848926,"/Arabic language pair (Dalvi et al., 2017) we conduct experiments on a subset of sessions from the WAW corpus (Abdelali et al., 2018) - a corpus of simultaneously interpreted conference speeches, to get informed about interpreters’ behaviour and learn which strategies interpreters employ to maintain good output accuracy while in the same time not exceeding their delay from the speaker. Our task is complex, as we want to find a way in which human expertise in interpreting can boost the performance of speech machine translation systems. With this article, we are enriching our previous research (Temnikova et al., 2017; Abdelali et al., 2018) and run an extensive multilateral analysis on a subset of WAW corpus interpreted sessions, before extending to a large number of sessions. The aim of this article is to test how much and what information we can extract by a combined manual (expert) and automatic analysis and also to propose a new automatic method for d´ecalage calculation. We present the results of a manual evaluation run by two human experts on the points of reference generated by our d´ecalage method. Knowing that the strategies applied by interpreters and their d´ecalage (including d´ecalage as a si"
