2020.acl-main.130,P17-1152,0,0.0695968,"Missing"
2020.acl-main.130,W19-4828,0,0.0695299,"Missing"
2020.acl-main.130,N19-1423,0,0.142582,"similarity of the final hidden state from both LSTMs. Sequential Matching Network (Wu et al., 2017): To avoid losing information in the context, SMN constructs a word-word and a sequencesequence similarity matrix, instead of utilizing the last hidden state only, and then aggregates similarity matrix as a matching score. Deep Attention Matching Network: Zhou et al. (2018) adopt self attention module (Vaswani et al., 2017) to encode response and each utterance, respectively. To match utterance and response, DAM further applies cross-attention module and 3D matching to obtain final score. BERT (Devlin et al., 2019): Pre-training models have shown promising results on various multichoice and reasoning tasks (Whang et al., 2019; Xu et al., 2019). Following Devlin et al. (2019), we concatenate the context (sentence A), and a candidate response (sentence B) as BERT input. On the top of BERT, a fully-connected layer is used for transforming the [CLS] token representation to the matching score. RoBERTa: Liu et al. (2019) re-establish BERT’s masked language model training objective by using more data and different hyper-parameters. We fine-tune RoBERTa in the same way as BERT. GPT-2 (Radford et al., 2019): Giv"
2020.acl-main.130,N19-1246,0,0.0402022,"Missing"
2020.acl-main.130,D19-1243,0,0.0342445,"Missing"
2020.acl-main.130,P19-1356,0,0.0569446,"Missing"
2020.acl-main.130,N18-1023,0,0.0279937,"alogue datasets (Lowe et al., 2015; Wu et al., 2017). Because MuTual is modified from listening tests of English as a foreign language, the complexity of morphology and grammar is much simpler than other datasets. For human-annotated datasets, there is always a trade-off between the number of instances being annotated and the quality of annotations (Kryciski et al., 2019). Our dataset is smaller than the previous crawling-based dialogue dataset (Lowe et al., 2015; Wu et al., 2017) due to the collection method. But it is comparable with high-quality reasoning based dataset (Clark et al., 2018; Khashabi et al., 2018; Talmor et al., 2019) and human-designed dialogue dataset (Zhang et al., 2018a). Moreover, around 10k is sufficient to train a discriminative model (Nivre et al., 2019) or fine-tuning the pretraining model (Wang et al., 2019). To assess the distribution of different reasoning types, we annotate the specific types of reasoning that are involved for instance, sampled from the test set and categorize them into six groups. The definition and ratio of each group are shown as follows. Attitude Reasoning: This type of instance tests if a model knows the speaker’s attitude towards an object. Algebrai"
2020.acl-main.130,D19-1051,0,0.0296336,"instance when inspectors doubt the uniqueness or correctness of the answer. 3.2 Analysis The detailed statistics of MuTual are summarized in Table 2. MuTual has an average of 4.73 turns. The vocabulary size is 11,343, which is smaller than other dialogue datasets (Lowe et al., 2015; Wu et al., 2017). Because MuTual is modified from listening tests of English as a foreign language, the complexity of morphology and grammar is much simpler than other datasets. For human-annotated datasets, there is always a trade-off between the number of instances being annotated and the quality of annotations (Kryciski et al., 2019). Our dataset is smaller than the previous crawling-based dialogue dataset (Lowe et al., 2015; Wu et al., 2017) due to the collection method. But it is comparable with high-quality reasoning based dataset (Clark et al., 2018; Khashabi et al., 2018; Talmor et al., 2019) and human-designed dialogue dataset (Zhang et al., 2018a). Moreover, around 10k is sufficient to train a discriminative model (Nivre et al., 2019) or fine-tuning the pretraining model (Wang et al., 2019). To assess the distribution of different reasoning types, we annotate the specific types of reasoning that are involved for in"
2020.acl-main.130,D17-1082,0,0.0723066,"Missing"
2020.acl-main.130,W15-4640,0,0.493103,"we will be late for the dinner. Figure 1: B is incorrect because there is no reason to apologize. C and D can be excluded because the relationship between two speakers are waiter and customer based on the context. Introduction Building an intelligent conversational agent is one of the longest running goals in AI. Existing conversational agents can be categorized into taskoriented dialogue systems (Kannan et al., 2016) and non-task-oriented chatbot systems (Shum et al., 2018; Wu et al., 2019). Owing to the rise of deep learning techniques and the large amount of conversation data for training (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018b), we are now witnessing promising results of chatbots both in academia and industry (Pan et al., 2019; Tao et al., 2019). Neural dialogue systems are trained over a large dialogue corpus and used to predict responses given a context. There are two lines of methods. Retrievebased methods and generation based methods rely ∗ Contribution during internship at MSRA. on matching scores and perplexity scores, respectively. Due to the development of text matching and pre-training models (Devlin et al., 2019; Liu et al., 2019), a machine is able to achieve highly"
2020.acl-main.130,D19-1191,0,0.0373074,"Missing"
2020.acl-main.130,Q19-1016,0,0.113805,"sense knowledge are not captured sufficiently (Young et al., 2018). One important research question is how we can evaluate reasoning ability in chatbots, which can potentially allow us to bridge the gap between high performance on leader-board and unsatisfactory practical performance. To this end, we develop 1406 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1406–1416 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dataset Ubuntu (Lowe et al., 2015) PERSONA-CHAT (Zhang et al., 2018a) Dialogue NLI (Welleck et al., 2019) CoQA (Reddy et al., 2019) Douban (Wu et al., 2017) DREAM (Sun et al., 2019) WSC (Levesque et al., 2012) SWAG (Zellers et al., 2018) CommonsenseQA (Talmor et al., 2019) RACE (Lai et al., 2017) ARC (Clark et al., 2018) DROP (Dua et al., 2019) Cosmos (Huang et al., 2019) MuTual Task Next Utterances Prediction Next Utterances Prediction Next Utterances Prediction Conversational QA Next Utterances Prediction Reading Comprehension Coreference Resolution Plausible Inference Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Next Utterances Prediction Reasoning $ $ $"
2020.acl-main.130,P15-1152,0,0.0210348,"et al., 2018a) considers consistent personality in dialogue. Crowd workers are required to act the part of a given provided persona, and chat naturally. Dialogue NLI (Welleck et al., 2019) is a natural language inference dataset modified from PERSONA-CHAT. It demonstrates that NLI can be used to improve the consistency of dialogue models. CoQA (Reddy et al., 2019) is collected by pairing two annotators to chat about a passage in the form of questions and answers. Each question is dependent on the conversation history. There are also several large-scale datasets in Chinese, such as Sina Weibo (Shang et al., 2015), Douban Conversation Corpus (Wu et al., 2017) and E-commerce Dialogue Corpus (Zhang et al., 2018b). As shown in Table 1, most of the existing conversation benchmarks do not focus on testing reasoning ability. One exception is CoQA, which considers pragmatic reasoning. The difference is that CoQA is a machine comprehension dataset, in which conversations are based on a given passage. Another related reading comprehension dataset is DREAM (Sun et al., 2019), which is designed specifically for challenging dialogue-based reading 1407 Listening Comprehension M Ma'am, you forgot your phone. Dialogu"
2020.acl-main.130,Q19-1014,0,0.172647,"et al., 2018). One important research question is how we can evaluate reasoning ability in chatbots, which can potentially allow us to bridge the gap between high performance on leader-board and unsatisfactory practical performance. To this end, we develop 1406 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1406–1416 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dataset Ubuntu (Lowe et al., 2015) PERSONA-CHAT (Zhang et al., 2018a) Dialogue NLI (Welleck et al., 2019) CoQA (Reddy et al., 2019) Douban (Wu et al., 2017) DREAM (Sun et al., 2019) WSC (Levesque et al., 2012) SWAG (Zellers et al., 2018) CommonsenseQA (Talmor et al., 2019) RACE (Lai et al., 2017) ARC (Clark et al., 2018) DROP (Dua et al., 2019) Cosmos (Huang et al., 2019) MuTual Task Next Utterances Prediction Next Utterances Prediction Next Utterances Prediction Conversational QA Next Utterances Prediction Reading Comprehension Coreference Resolution Plausible Inference Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Next Utterances Prediction Reasoning $ $ $ "" $ "" "" "" "" "" "" "" "" "" Domain Technique Persona Per"
2020.acl-main.130,N19-1421,0,0.465732,"in chatbots, which can potentially allow us to bridge the gap between high performance on leader-board and unsatisfactory practical performance. To this end, we develop 1406 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1406–1416 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dataset Ubuntu (Lowe et al., 2015) PERSONA-CHAT (Zhang et al., 2018a) Dialogue NLI (Welleck et al., 2019) CoQA (Reddy et al., 2019) Douban (Wu et al., 2017) DREAM (Sun et al., 2019) WSC (Levesque et al., 2012) SWAG (Zellers et al., 2018) CommonsenseQA (Talmor et al., 2019) RACE (Lai et al., 2017) ARC (Clark et al., 2018) DROP (Dua et al., 2019) Cosmos (Huang et al., 2019) MuTual Task Next Utterances Prediction Next Utterances Prediction Next Utterances Prediction Conversational QA Next Utterances Prediction Reading Comprehension Coreference Resolution Plausible Inference Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Next Utterances Prediction Reasoning $ $ $ "" $ "" "" "" "" "" "" "" "" "" Domain Technique Persona Persona Diverse Open Open Open Movie Open Open Science Open Narrative Open Manually $ "" $ "" $ """
2020.acl-main.130,P19-1001,0,0.279625,"Missing"
2020.acl-main.130,voorhees-tice-2000-trec,0,0.41351,"Missing"
2020.acl-main.130,P19-1363,0,0.130911,"soning capability and commonsense knowledge are not captured sufficiently (Young et al., 2018). One important research question is how we can evaluate reasoning ability in chatbots, which can potentially allow us to bridge the gap between high performance on leader-board and unsatisfactory practical performance. To this end, we develop 1406 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1406–1416 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dataset Ubuntu (Lowe et al., 2015) PERSONA-CHAT (Zhang et al., 2018a) Dialogue NLI (Welleck et al., 2019) CoQA (Reddy et al., 2019) Douban (Wu et al., 2017) DREAM (Sun et al., 2019) WSC (Levesque et al., 2012) SWAG (Zellers et al., 2018) CommonsenseQA (Talmor et al., 2019) RACE (Lai et al., 2017) ARC (Clark et al., 2018) DROP (Dua et al., 2019) Cosmos (Huang et al., 2019) MuTual Task Next Utterances Prediction Next Utterances Prediction Next Utterances Prediction Conversational QA Next Utterances Prediction Reading Comprehension Coreference Resolution Plausible Inference Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Next Utterances P"
2020.acl-main.130,J19-1005,1,0.900814,"Missing"
2020.acl-main.130,P17-1046,1,0.758434,"the dinner. Figure 1: B is incorrect because there is no reason to apologize. C and D can be excluded because the relationship between two speakers are waiter and customer based on the context. Introduction Building an intelligent conversational agent is one of the longest running goals in AI. Existing conversational agents can be categorized into taskoriented dialogue systems (Kannan et al., 2016) and non-task-oriented chatbot systems (Shum et al., 2018; Wu et al., 2019). Owing to the rise of deep learning techniques and the large amount of conversation data for training (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018b), we are now witnessing promising results of chatbots both in academia and industry (Pan et al., 2019; Tao et al., 2019). Neural dialogue systems are trained over a large dialogue corpus and used to predict responses given a context. There are two lines of methods. Retrievebased methods and generation based methods rely ∗ Contribution during internship at MSRA. on matching scores and perplexity scores, respectively. Due to the development of text matching and pre-training models (Devlin et al., 2019; Liu et al., 2019), a machine is able to achieve highly competitive resul"
2020.acl-main.130,N19-1242,0,0.0248812,"e context, SMN constructs a word-word and a sequencesequence similarity matrix, instead of utilizing the last hidden state only, and then aggregates similarity matrix as a matching score. Deep Attention Matching Network: Zhou et al. (2018) adopt self attention module (Vaswani et al., 2017) to encode response and each utterance, respectively. To match utterance and response, DAM further applies cross-attention module and 3D matching to obtain final score. BERT (Devlin et al., 2019): Pre-training models have shown promising results on various multichoice and reasoning tasks (Whang et al., 2019; Xu et al., 2019). Following Devlin et al. (2019), we concatenate the context (sentence A), and a candidate response (sentence B) as BERT input. On the top of BERT, a fully-connected layer is used for transforming the [CLS] token representation to the matching score. RoBERTa: Liu et al. (2019) re-establish BERT’s masked language model training objective by using more data and different hyper-parameters. We fine-tune RoBERTa in the same way as BERT. GPT-2 (Radford et al., 2019): Given a context, the positive response has a higher probability compared with negative responses. Motivated by this, we concatenate co"
2020.acl-main.130,D18-1009,0,0.269438,"ow we can evaluate reasoning ability in chatbots, which can potentially allow us to bridge the gap between high performance on leader-board and unsatisfactory practical performance. To this end, we develop 1406 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1406–1416 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dataset Ubuntu (Lowe et al., 2015) PERSONA-CHAT (Zhang et al., 2018a) Dialogue NLI (Welleck et al., 2019) CoQA (Reddy et al., 2019) Douban (Wu et al., 2017) DREAM (Sun et al., 2019) WSC (Levesque et al., 2012) SWAG (Zellers et al., 2018) CommonsenseQA (Talmor et al., 2019) RACE (Lai et al., 2017) ARC (Clark et al., 2018) DROP (Dua et al., 2019) Cosmos (Huang et al., 2019) MuTual Task Next Utterances Prediction Next Utterances Prediction Next Utterances Prediction Conversational QA Next Utterances Prediction Reading Comprehension Coreference Resolution Plausible Inference Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Reading Comprehension Next Utterances Prediction Reasoning $ $ $ "" $ "" "" "" "" "" "" "" "" "" Domain Technique Persona Persona Diverse Open Open Open Movie Open Open Science Open"
2020.acl-main.130,P18-1205,0,0.49499,"re 1: B is incorrect because there is no reason to apologize. C and D can be excluded because the relationship between two speakers are waiter and customer based on the context. Introduction Building an intelligent conversational agent is one of the longest running goals in AI. Existing conversational agents can be categorized into taskoriented dialogue systems (Kannan et al., 2016) and non-task-oriented chatbot systems (Shum et al., 2018; Wu et al., 2019). Owing to the rise of deep learning techniques and the large amount of conversation data for training (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018b), we are now witnessing promising results of chatbots both in academia and industry (Pan et al., 2019; Tao et al., 2019). Neural dialogue systems are trained over a large dialogue corpus and used to predict responses given a context. There are two lines of methods. Retrievebased methods and generation based methods rely ∗ Contribution during internship at MSRA. on matching scores and perplexity scores, respectively. Due to the development of text matching and pre-training models (Devlin et al., 2019; Liu et al., 2019), a machine is able to achieve highly competitive results on these datasets"
2020.acl-main.130,C18-1317,0,0.432694,"re 1: B is incorrect because there is no reason to apologize. C and D can be excluded because the relationship between two speakers are waiter and customer based on the context. Introduction Building an intelligent conversational agent is one of the longest running goals in AI. Existing conversational agents can be categorized into taskoriented dialogue systems (Kannan et al., 2016) and non-task-oriented chatbot systems (Shum et al., 2018; Wu et al., 2019). Owing to the rise of deep learning techniques and the large amount of conversation data for training (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018b), we are now witnessing promising results of chatbots both in academia and industry (Pan et al., 2019; Tao et al., 2019). Neural dialogue systems are trained over a large dialogue corpus and used to predict responses given a context. There are two lines of methods. Retrievebased methods and generation based methods rely ∗ Contribution during internship at MSRA. on matching scores and perplexity scores, respectively. Due to the development of text matching and pre-training models (Devlin et al., 2019; Liu et al., 2019), a machine is able to achieve highly competitive results on these datasets"
2020.acl-main.130,P18-1103,0,0.0685413,"date response as the model output. The “IDF” is calculated only on the training set. Dual LSTM (Lowe et al., 2015): Two LSTMs are used to encode context and response, respectively. The relevance between context and response is calculated by the similarity of the final hidden state from both LSTMs. Sequential Matching Network (Wu et al., 2017): To avoid losing information in the context, SMN constructs a word-word and a sequencesequence similarity matrix, instead of utilizing the last hidden state only, and then aggregates similarity matrix as a matching score. Deep Attention Matching Network: Zhou et al. (2018) adopt self attention module (Vaswani et al., 2017) to encode response and each utterance, respectively. To match utterance and response, DAM further applies cross-attention module and 3D matching to obtain final score. BERT (Devlin et al., 2019): Pre-training models have shown promising results on various multichoice and reasoning tasks (Whang et al., 2019; Xu et al., 2019). Following Devlin et al. (2019), we concatenate the context (sentence A), and a candidate response (sentence B) as BERT input. On the top of BERT, a fully-connected layer is used for transforming the [CLS] token representa"
2020.acl-main.143,D16-1250,0,0.0197347,"methods, including word-by-word translation, unsupervised MT, and cross-lingual embedding transformation. On distant language pairs that unsupervised MT struggled to be effective, AT and Bi-view AT perform remarkably better. 2 Related Work The bilingual dictionaries used in previous works are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (U"
2020.acl-main.143,P18-1073,0,0.158206,"hard for unsupervised MT to perform well, AT performs remarkably better, achieving performances comparable to supervised SMT trained on more than 4M parallel sentences1 . 1 Introduction Motivated by a monolingual speaker acquiring translation ability by referring to a bilingual dictionary, we propose a novel MT task that no parallel sentences are available, while a ground-truth bilingual dictionary and large-scale monolingual corpora can be utilized. This task departs from unsupervised MT task that no parallel resources, including the ground-truth bilingual dictionary, are allowed to utilize (Artetxe et al., 2018c; Lample et al., 2018b). This task is also distinct to ∗ Corresponding Author. Code is available at https://github.com/ mttravel/Dictionary-based-MT 1 supervised/semi-supervised MT task that mainly depends on parallel sentences (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018; Sennrich et al., 2016a). The bilingual dictionary is often utilized as a seed in bilingual lexicon induction (BLI) that aims to induce more word pairs within the language pair (Mikolov et al., 2013). Another utilization of the bilingual dictionary is for translating lowfrequency word"
2020.acl-main.143,D18-1399,0,0.0376008,"Missing"
2020.acl-main.143,P19-1019,0,0.062444,"language pairs. 4.5 Experimental Results: with Cross-lingual Pretraining The bottom part of Table 2 reports performances of UNMT with XLM, which conducts the crosslingual pretraining on concatenated non-parallel corpora (Lample and Conneau, 2019), and performances of our AT/Bi-view AT with the anchored cross-lingual pretraining, i.e., ACP. The results show that our proposed AT approaches are still superior when equipped with the cross-lingual pretraining. UNMT obtains great improvement when combined with XLM, achieving state-of-the-art unsupervised MT performance better than Unsupervised SMT (Artetxe et al., 2019) and Unsupervised NMT (Lample et al., 2018b) across close and distant language pairs. 1575 Effect on Bilingual Word Embeddings As shown in Figure 2, we depict the word embeddings of some sampled words in English-Chinese after our Bi-view AT. The dimensions of the embedding vectors are reduced to two by using T-SNE and are visualized by the visualization tool in Tensorflow10 . We sample the English words that are not covered by the dictionary at first, then search their nearest Chinese neighbors in the embedding space. It shows that the words which constitute a new ground-truth translation pair"
2020.acl-main.143,J82-2005,0,0.709748,"Missing"
2020.acl-main.143,D16-1162,0,0.164289,"8b). This task is also distinct to ∗ Corresponding Author. Code is available at https://github.com/ mttravel/Dictionary-based-MT 1 supervised/semi-supervised MT task that mainly depends on parallel sentences (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018; Sennrich et al., 2016a). The bilingual dictionary is often utilized as a seed in bilingual lexicon induction (BLI) that aims to induce more word pairs within the language pair (Mikolov et al., 2013). Another utilization of the bilingual dictionary is for translating lowfrequency words in supervised NMT (Arthur et al., 2016; Zhang and Zong, 2016). We are the first to utilize the bilingual dictionary and the large scale monolingual corpora to see how much potential an MT system can achieve without using parallel sentences. This is different from using artificial bilingual dictionaries generated by unsupervised BLI for initializing an unsupervised MT system (Artetxe et al., 2018c,b; Lample et al., 2018a), we use the ground-truth bilingual dictionary and apply it throughout the training process. We propose Anchored Training (AT) to tackle this task. Since word representations are learned over monolingual corpora wi"
2020.acl-main.143,P18-1008,0,0.0261085,"that no parallel sentences are available, while a ground-truth bilingual dictionary and large-scale monolingual corpora can be utilized. This task departs from unsupervised MT task that no parallel resources, including the ground-truth bilingual dictionary, are allowed to utilize (Artetxe et al., 2018c; Lample et al., 2018b). This task is also distinct to ∗ Corresponding Author. Code is available at https://github.com/ mttravel/Dictionary-based-MT 1 supervised/semi-supervised MT task that mainly depends on parallel sentences (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018; Sennrich et al., 2016a). The bilingual dictionary is often utilized as a seed in bilingual lexicon induction (BLI) that aims to induce more word pairs within the language pair (Mikolov et al., 2013). Another utilization of the bilingual dictionary is for translating lowfrequency words in supervised NMT (Arthur et al., 2016; Zhang and Zong, 2016). We are the first to utilize the bilingual dictionary and the large scale monolingual corpora to see how much potential an MT system can achieve without using parallel sentences. This is different from using artificial bilingual dictionaries generate"
2020.acl-main.143,E17-1088,0,0.0146801,"of both side languages is quite challenging. We use the ground-truth dictionary to alleviate such problem, and experiments on distant language pairs show the necessity of using the bilingual dictionary. Other utilizations of the bilingual dictionary for tasks beyond MT include cross-lingual dependency parsing (Xiao and Guo, 2014), unsupervised crosslingual part-of-speech tagging and semi-supervised cross-lingual super sense tagging (Gouws and Søgaard, 2015), multilingual word embedding training (Ammar et al., 2016; Duong et al., 2016), and transfer learning for low-resource language modeling (Cohn et al., 2017). 3 Our Approach There are multiple freely available bilingual dictionaries such as Muse dictionary2 (Conneau et al., 2018), Wiktionary3 , and PanLex4 . We adopt Muse dictionary which contains 110 large-scale groundtruth bilingual dictionaries. We propose to inject the bilingual dictionary into the MT training by placing anchoring points on the large scale monolingual corpora to drive the semantic spaces of both languages becoming closer so that MT training without parallel sentences becomes easier. We present the proposed Anchored Training (AT) and Bi-view AT in the following. 3.1 Anchored Tr"
2020.acl-main.143,D16-1136,0,0.0209789,"orms remarkably bad on distant language pairs in which aligning the embeddings of both side languages is quite challenging. We use the ground-truth dictionary to alleviate such problem, and experiments on distant language pairs show the necessity of using the bilingual dictionary. Other utilizations of the bilingual dictionary for tasks beyond MT include cross-lingual dependency parsing (Xiao and Guo, 2014), unsupervised crosslingual part-of-speech tagging and semi-supervised cross-lingual super sense tagging (Gouws and Søgaard, 2015), multilingual word embedding training (Ammar et al., 2016; Duong et al., 2016), and transfer learning for low-resource language modeling (Cohn et al., 2017). 3 Our Approach There are multiple freely available bilingual dictionaries such as Muse dictionary2 (Conneau et al., 2018), Wiktionary3 , and PanLex4 . We adopt Muse dictionary which contains 110 large-scale groundtruth bilingual dictionaries. We propose to inject the bilingual dictionary into the MT training by placing anchoring points on the large scale monolingual corpora to drive the semantic spaces of both languages becoming closer so that MT training without parallel sentences becomes easier. We present the pr"
2020.acl-main.143,E14-1049,0,0.031871,"aries used in previous works are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the artificial dictionary generated by unsupervised BLI for initialization (Artetxe et al."
2020.acl-main.143,N15-1157,0,0.0293103,"g process. UNMT works well on close language pairs such as EnglishFrench, while performs remarkably bad on distant language pairs in which aligning the embeddings of both side languages is quite challenging. We use the ground-truth dictionary to alleviate such problem, and experiments on distant language pairs show the necessity of using the bilingual dictionary. Other utilizations of the bilingual dictionary for tasks beyond MT include cross-lingual dependency parsing (Xiao and Guo, 2014), unsupervised crosslingual part-of-speech tagging and semi-supervised cross-lingual super sense tagging (Gouws and Søgaard, 2015), multilingual word embedding training (Ammar et al., 2016; Duong et al., 2016), and transfer learning for low-resource language modeling (Cohn et al., 2017). 3 Our Approach There are multiple freely available bilingual dictionaries such as Muse dictionary2 (Conneau et al., 2018), Wiktionary3 , and PanLex4 . We adopt Muse dictionary which contains 110 large-scale groundtruth bilingual dictionaries. We propose to inject the bilingual dictionary into the MT training by placing anchoring points on the large scale monolingual corpora to drive the semantic spaces of both languages becoming closer s"
2020.acl-main.143,N16-1156,0,0.0134234,"erform remarkably better. 2 Related Work The bilingual dictionaries used in previous works are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the artificial dictionar"
2020.acl-main.143,D18-1549,0,0.0509903,"Missing"
2020.acl-main.143,P15-1027,0,0.0208843,"ive, AT and Bi-view AT perform remarkably better. 2 Related Work The bilingual dictionaries used in previous works are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the"
2020.acl-main.143,N15-1028,0,0.0210411,"orks are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the artificial dictionary generated by unsupervised BLI for initialization (Artetxe et al., 2018c; Lample et"
2020.acl-main.143,P16-1009,0,0.242082,"entences are available, while a ground-truth bilingual dictionary and large-scale monolingual corpora can be utilized. This task departs from unsupervised MT task that no parallel resources, including the ground-truth bilingual dictionary, are allowed to utilize (Artetxe et al., 2018c; Lample et al., 2018b). This task is also distinct to ∗ Corresponding Author. Code is available at https://github.com/ mttravel/Dictionary-based-MT 1 supervised/semi-supervised MT task that mainly depends on parallel sentences (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018; Sennrich et al., 2016a). The bilingual dictionary is often utilized as a seed in bilingual lexicon induction (BLI) that aims to induce more word pairs within the language pair (Mikolov et al., 2013). Another utilization of the bilingual dictionary is for translating lowfrequency words in supervised NMT (Arthur et al., 2016; Zhang and Zong, 2016). We are the first to utilize the bilingual dictionary and the large scale monolingual corpora to see how much potential an MT system can achieve without using parallel sentences. This is different from using artificial bilingual dictionaries generated by unsupervised BLI f"
2020.acl-main.143,P16-1162,0,0.424255,"entences are available, while a ground-truth bilingual dictionary and large-scale monolingual corpora can be utilized. This task departs from unsupervised MT task that no parallel resources, including the ground-truth bilingual dictionary, are allowed to utilize (Artetxe et al., 2018c; Lample et al., 2018b). This task is also distinct to ∗ Corresponding Author. Code is available at https://github.com/ mttravel/Dictionary-based-MT 1 supervised/semi-supervised MT task that mainly depends on parallel sentences (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017; Chen et al., 2018; Sennrich et al., 2016a). The bilingual dictionary is often utilized as a seed in bilingual lexicon induction (BLI) that aims to induce more word pairs within the language pair (Mikolov et al., 2013). Another utilization of the bilingual dictionary is for translating lowfrequency words in supervised NMT (Arthur et al., 2016; Zhang and Zong, 2016). We are the first to utilize the bilingual dictionary and the large scale monolingual corpora to see how much potential an MT system can achieve without using parallel sentences. This is different from using artificial bilingual dictionaries generated by unsupervised BLI f"
2020.acl-main.143,P19-1119,0,0.0252407,"through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the artificial dictionary generated by unsupervised BLI for initialization (Artetxe et al., 2018c; Lample et al., 2018a) or abandon the artificial dictionary by using joint BPE so that multiple BPE units can be shared by both languages (Lample et al., 2018b). We use the ground-truth dictionary instead and apply it throughout a novel training process. UNMT works well on close language pairs such as EnglishFrench, while performs remarkably bad on distant language pairs in which aligning the embeddings of both si"
2020.acl-main.143,W14-1613,0,0.0293924,"by both languages (Lample et al., 2018b). We use the ground-truth dictionary instead and apply it throughout a novel training process. UNMT works well on close language pairs such as EnglishFrench, while performs remarkably bad on distant language pairs in which aligning the embeddings of both side languages is quite challenging. We use the ground-truth dictionary to alleviate such problem, and experiments on distant language pairs show the necessity of using the bilingual dictionary. Other utilizations of the bilingual dictionary for tasks beyond MT include cross-lingual dependency parsing (Xiao and Guo, 2014), unsupervised crosslingual part-of-speech tagging and semi-supervised cross-lingual super sense tagging (Gouws and Søgaard, 2015), multilingual word embedding training (Ammar et al., 2016; Duong et al., 2016), and transfer learning for low-resource language modeling (Cohn et al., 2017). 3 Our Approach There are multiple freely available bilingual dictionaries such as Muse dictionary2 (Conneau et al., 2018), Wiktionary3 , and PanLex4 . We adopt Muse dictionary which contains 110 large-scale groundtruth bilingual dictionaries. We propose to inject the bilingual dictionary into the MT training b"
2020.acl-main.143,N15-1104,0,0.0287906,"uggled to be effective, AT and Bi-view AT perform remarkably better. 2 Related Work The bilingual dictionaries used in previous works are mainly for bilingual lexicon induction (BLI), which independently learns the embedding in each language using monolingual corpora, and then learns a transformation from one embedding space to another by minimizing squared euclidean distances between all word pairs in the dictionary (Mikolov et al., 2013; Artetxe et al., 2016). Later efforts for BLI include optimizing the transformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference i"
2020.acl-main.143,P18-1005,0,0.0184947,"sformation further through new training objectives, constraints, or normalizations (Xing et al., 2015; Lazaridou et al., 2015; Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017; Faruqui and Dyer, 2014; Lu et al., 2015). Besides, the bilingual dictionary is also used for supervised NMT which requires largescale parallel sentences (Arthur et al., 2016; Zhang and Zong, 2016). To our knowledge, we are the first to use the bilingual dictionary for MT without using any parallel sentences. Our work is closely related to unsupervised NMT (UNMT) (Artetxe et al., 2018c; Lample et al., 2018b; Yang et al., 2018; Sun et al., 2019), which does not use parallel sentences neither. The difference is that UNMT may use the artificial dictionary generated by unsupervised BLI for initialization (Artetxe et al., 2018c; Lample et al., 2018a) or abandon the artificial dictionary by using joint BPE so that multiple BPE units can be shared by both languages (Lample et al., 2018b). We use the ground-truth dictionary instead and apply it throughout a novel training process. UNMT works well on close language pairs such as EnglishFrench, while performs remarkably bad on distant language pairs in which aligning the em"
2020.acl-main.297,W06-1651,0,0.0926525,"k An opinion consists of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issue of data scarcity for ORL, considering SRL is highly related to ORL and has a considerable amount of training data. Inspired by the similarity between ORL and SRL in task definition, Kim and Hovy (2006) and Ruppenhofer et al. (2008) address ORL with a well-trained SRL model by treating"
2020.acl-main.297,P17-4017,0,0.0949806,"TL model effectively boosts the F1 score by 9.29 over the syntaxagnostic baseline. In addition, we find that the contributions from syntactic knowledge do not fully overlap with contextualized word representations (BERT). Our best model achieves 4.34 higher F1 score than the current state-ofthe-art. 1 $ Cardoso Opinion and sentiment analysis has a wide range of real-world applications like social media monitoring (Bollen et al., 2011), stock market prediction (Nguyen et al., 2015), box office prediction (Yu et al., 2010), and general e-commerce applications (Kim et al., 2013; Hu et al., 2017; Cui et al., 2017). In particular, fine-grained opinion analysis aims to identify users’ opinions in a text, including opinion expressions, holders of the opinions, targets of the opinions, target-dependent attitude, and intensity of opinions (Marasovi´c and Frank, 2018), which is very important for understanding political stance, Corresponding author challenge facing Chavez Target is ... Figure 1: An Example of ORL (bottom) and syntactic dependency tree (top) for “Cardoso says challenge facing Chavezis is reestablishing normalcy.” Introduction ∗ says Holder Expression customers’ reviews, marketing trends, and"
2020.acl-main.297,N19-1423,0,0.237545,"cy arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representation of syntax provides more information while alleviating parsing errors. For the second barrier, considering that the pipeline methods are notorious for the error propagation problem, we introduce multi-task learning (MTL) frameworks, which have been widely used in many NLP models when predictions at various processing levels are needed (Collobert and Weston, 2008; Ruder, 2017). Apart from the syntactic information, contextualized word representations like BERT (Devlin et al., 2019) are widely used to compensate for the sparsity of task-specific training data. They compress distributional semantics of words from large corpora, making the local context fluent and natural. However, the long-distance dependencies between words are often ignored, which is ideally able to be captured by syntactic analysis. In summary, based on previous studies in using syntax to improve various tasks, this work investigates whether syntax can enhance the neural ORL model. Particularly, we try to answer the following three questions. • How to effectively integrate various syntactic information"
2020.acl-main.297,P19-1024,0,0.0190934,"nd the ORL model. We first obtain the edge-weighted graph from the decoder of a well-trained biaffine parser as a data preprocessing step, and then feed the graph into our D EP GCN in the form of an adjacency matrix A 1 . Then we feed the outputs of the ORL BiLSTM-based encoder as the initial inputs h0 to the D EP GCN. Finally, we feed the output of the D EP GCN to the CRF-based decoder, and update the ORL results under the guidance of the syntactic information. Moreover, we introduce dense connections to the multi-layer D EP GCN for extracting more structural information (Huang et al., 2017; Guo et al., 2019). Instead of only adding connections between adjacent layers, we use dense connections from each layer to all the subsequent layers. Formally, the input of node i at the l-th layer is: (l) 4.1 (0) (1) (l−1) xi = hi ⊕ hi ⊕ · · · ⊕ hi Dependency Graph Convolutional Networks (D EP GCN) (3) (l) In this subsection, we propose dependency graph convolutional networks (D EP GCN) to better encode the syntactic information from the edgeweighted graphs. On the one hand, compared with explicit 1-best parse trees, edge-weighted graphs where hi is the output of node i at the l-th layer. We also make residua"
2020.acl-main.297,P18-1192,0,0.0735118,"human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without syntactic dependency 3249 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3249–3258 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relations, missing either “facing Chavez” or “challenge”. For the similar SRL task, many previous works have proposed to incorporate syntax into the neural models (Marcheggiani and Titov, 2017; He et al., 2018; Xia et al., 2019a). In contrast, few studies in the recent years explore this line of research for ORL. There are two barriers to apply syntactic dependency parsing to NLP tasks, i.e., 1) inaccuracy of the parsing results, and 2) error propagation of the processing pipeline. To overcome the first barrier, instead of employing the final discrete outputs (i.e., single 1-best dependency trees), we make use of the probability matrix of all dependency arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representation of syntax prov"
2020.acl-main.297,J13-3002,0,0.0940515,"dency relations, do not fully overlap with those from the contextualized word representations like BERT. Our overall model delivers a new stateof-the-art result on the benchmark MPQA corpus, with 4.34 absolute improvement over the previous best result. 2 Related work An opinion consists of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issue of data scarcity for ORL, conside"
2020.acl-main.297,P16-1087,0,0.537352,"s, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issue of data scarcity for ORL, considering SRL is highly related to ORL and has a considerable amount of training data. Inspired by the similarity between ORL and SRL in task definition, Kim and Hovy (2006) and Ruppenhofer et al. (2008) address ORL with a well-trained SRL model by treating opinion expressions as semantic predicates, and op"
2020.acl-main.297,W06-0301,0,0.113222,"from long-distance dependency relations, do not fully overlap with those from the contextualized word representations like BERT. Our overall model delivers a new stateof-the-art result on the benchmark MPQA corpus, with 4.34 absolute improvement over the previous best result. 2 Related work An opinion consists of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issu"
2020.acl-main.297,N18-1054,0,0.470419,"Missing"
2020.acl-main.297,D17-1159,0,0.406534,"ural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without syntactic dependency 3249 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3249–3258 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relations, missing either “facing Chavez” or “challenge”. For the similar SRL task, many previous works have proposed to incorporate syntax into the neural models (Marcheggiani and Titov, 2017; He et al., 2018; Xia et al., 2019a). In contrast, few studies in the recent years explore this line of research for ORL. There are two barriers to apply syntactic dependency parsing to NLP tasks, i.e., 1) inaccuracy of the parsing results, and 2) error propagation of the processing pipeline. To overcome the first barrier, instead of employing the final discrete outputs (i.e., single 1-best dependency trees), we make use of the probability matrix of all dependency arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representati"
2020.acl-main.297,P16-1105,0,0.0190351,"SRL as an auxiliary task, and employ different MTL frameworks to learn the common grounds between ORL and SRL and distinguish task-specific knowledge. Zhang et al. (2019b) extract neural features from a welltrained SRL model as SRL-aware word representations, and then feed them into the input layer of ORL, aiming to alleviate the error propagation problem. 3250 Figure 2: The overall architecture of our models. Many previous works have shown that syntactic information is of great value for SRL and other NLP tasks (He et al., 2018; Zhang et al., 2019c; Strubell et al., 2018; Xia et al., 2019a; Miwa and Bansal, 2016; Zhang et al., 2019a). Xia et al. (2019b) use the relative position between predicate words and other words in a dependency tree to represent syntactic information, while Roth and Lapata (2016) employ LSTM to obtain the embedding of a dependency path. Tai et al. (2015) and Kipf and Welling (2016) propose TreeLSTM and graph convolution network (GCN) to encode the tree/graphstructural data respectively. Both TreeLSTM and GCN are commonly used techniques to encode parse trees (Miwa and Bansal, 2016; Marcheggiani and Titov, 2017; Bastings et al., 2017). Zhang et al. (2019a) and Xia et al. (2019a)"
2020.acl-main.297,P16-1113,0,0.0491495,"res from a welltrained SRL model as SRL-aware word representations, and then feed them into the input layer of ORL, aiming to alleviate the error propagation problem. 3250 Figure 2: The overall architecture of our models. Many previous works have shown that syntactic information is of great value for SRL and other NLP tasks (He et al., 2018; Zhang et al., 2019c; Strubell et al., 2018; Xia et al., 2019a; Miwa and Bansal, 2016; Zhang et al., 2019a). Xia et al. (2019b) use the relative position between predicate words and other words in a dependency tree to represent syntactic information, while Roth and Lapata (2016) employ LSTM to obtain the embedding of a dependency path. Tai et al. (2015) and Kipf and Welling (2016) propose TreeLSTM and graph convolution network (GCN) to encode the tree/graphstructural data respectively. Both TreeLSTM and GCN are commonly used techniques to encode parse trees (Miwa and Bansal, 2016; Marcheggiani and Titov, 2017; Bastings et al., 2017). Zhang et al. (2019a) and Xia et al. (2019a) extract the hidden states from the LSTM encoder of the parser model as syntax-aware word representations, and feed them to downstream tasks as extra inputs. In contrast, few works have proved t"
2020.acl-main.297,ruppenhofer-etal-2008-finding,0,0.26963,"ion mining task, opinion role labeling (ORL) aims to identify different roles relevant to each opinion, i.e., who expressed what kind of sentiment towards what (Liu, 2012). Due to the lack of large-scale labeled data, ORL remains a challenging task to tackle. As a reference point, semantic role labeling (SRL) is very similar to ORL in the problem definition, but has 10 times more labeled data and thus achieves much higher performance than ORL (80∼90 vs. 60∼70 in F1 score). Motivated by the correlations between the two tasks, SRL has been utilized to help the ORL task by many previous studies (Ruppenhofer et al., 2008; Marasovi´c and Frank, 2018; Zhang et al., 2019b). However, when opinion expressions and arguments compose complicated syntactic structures, it is difficult to correctly recognize the opinion arguments even with shallow semantic representation like SRL (Marasovi´c and Frank, 2018). To compensate for the limited scale of labeled data for data-driven approaches, linguistic knowledge like syntax provides structural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Targ"
2020.acl-main.297,D18-1548,0,0.0256489,"ic roles. Marasovi´c and Frank (2018) take SRL as an auxiliary task, and employ different MTL frameworks to learn the common grounds between ORL and SRL and distinguish task-specific knowledge. Zhang et al. (2019b) extract neural features from a welltrained SRL model as SRL-aware word representations, and then feed them into the input layer of ORL, aiming to alleviate the error propagation problem. 3250 Figure 2: The overall architecture of our models. Many previous works have shown that syntactic information is of great value for SRL and other NLP tasks (He et al., 2018; Zhang et al., 2019c; Strubell et al., 2018; Xia et al., 2019a; Miwa and Bansal, 2016; Zhang et al., 2019a). Xia et al. (2019b) use the relative position between predicate words and other words in a dependency tree to represent syntactic information, while Roth and Lapata (2016) employ LSTM to obtain the embedding of a dependency path. Tai et al. (2015) and Kipf and Welling (2016) propose TreeLSTM and graph convolution network (GCN) to encode the tree/graphstructural data respectively. Both TreeLSTM and GCN are commonly used techniques to encode parse trees (Miwa and Bansal, 2016; Marcheggiani and Titov, 2017; Bastings et al., 2017). Z"
2020.acl-main.297,P15-1150,0,0.0563738,"them into the input layer of ORL, aiming to alleviate the error propagation problem. 3250 Figure 2: The overall architecture of our models. Many previous works have shown that syntactic information is of great value for SRL and other NLP tasks (He et al., 2018; Zhang et al., 2019c; Strubell et al., 2018; Xia et al., 2019a; Miwa and Bansal, 2016; Zhang et al., 2019a). Xia et al. (2019b) use the relative position between predicate words and other words in a dependency tree to represent syntactic information, while Roth and Lapata (2016) employ LSTM to obtain the embedding of a dependency path. Tai et al. (2015) and Kipf and Welling (2016) propose TreeLSTM and graph convolution network (GCN) to encode the tree/graphstructural data respectively. Both TreeLSTM and GCN are commonly used techniques to encode parse trees (Miwa and Bansal, 2016; Marcheggiani and Titov, 2017; Bastings et al., 2017). Zhang et al. (2019a) and Xia et al. (2019a) extract the hidden states from the LSTM encoder of the parser model as syntax-aware word representations, and feed them to downstream tasks as extra inputs. In contrast, few works have proved that syntactic knowledge is useful in the neural ORL models. Yang and Cardie"
2020.acl-main.297,D19-1541,1,0.407543,"ng of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without syntactic dependency 3249 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3249–3258 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relations, missing either “facing Chavez” or “challenge”. For the similar SRL task, many previous works have proposed to incorporate syntax into the neural models (Marcheggiani and Titov, 2017; He et al., 2018; Xia et al., 2019a). In contrast, few studies in the recent years explore this line of research for ORL. There are two barriers to apply syntactic dependency parsing to NLP tasks, i.e., 1) inaccuracy of the parsing results, and 2) error propagation of the processing pipeline. To overcome the first barrier, instead of employing the final discrete outputs (i.e., single 1-best dependency trees), we make use of the probability matrix of all dependency arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representation of syntax provides more informat"
2020.acl-main.297,N19-1075,0,0.138213,"ng of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without syntactic dependency 3249 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3249–3258 c July 5 - 10, 2020. 2020 Association for Computational Linguistics relations, missing either “facing Chavez” or “challenge”. For the similar SRL task, many previous works have proposed to incorporate syntax into the neural models (Marcheggiani and Titov, 2017; He et al., 2018; Xia et al., 2019a). In contrast, few studies in the recent years explore this line of research for ORL. There are two barriers to apply syntactic dependency parsing to NLP tasks, i.e., 1) inaccuracy of the parsing results, and 2) error propagation of the processing pipeline. To overcome the first barrier, instead of employing the final discrete outputs (i.e., single 1-best dependency trees), we make use of the probability matrix of all dependency arcs (also can be viewed as an edge-weighted directed graph) before searching for the 1-best tree. Such probabilistic representation of syntax provides more informat"
2020.acl-main.297,P13-1161,0,0.632022,"ts of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targets for the given opinion expressions. Previous works make use of SRL resources to address the issue of data scarcity for ORL, considering SRL is highly related to ORL and has a considerable amount of training data. Inspired by the similarity between ORL and SRL in task definition, Kim and Hovy (2006) and Ruppenhofer et al. (2008) address ORL with a well-trained SRL model by treating opinion expressions as"
2020.acl-main.297,Q14-1039,0,0.218287,"alleviate the error propagation problem; and 3) contributions from syntactic information, especially from long-distance dependency relations, do not fully overlap with those from the contextualized word representations like BERT. Our overall model delivers a new stateof-the-art result on the benchmark MPQA corpus, with 4.34 absolute improvement over the previous best result. 2 Related work An opinion consists of several components, e.g., expressions, holders, and targets. Some previous works focus on recognizing some components, whereas others try to recognize all components at the same time. Yang and Cardie (2014) and Breck et al. (2007) work entirely on labeling of the opinion expressions. Kim and Hovy (2006) and Johansson and Moschitti (2013) apply pipeline models to firstly predicting opinion expressions and then labeling holders and targets for each expression. Joint models simultaneously identify all opinion components, predicting which role is related to which opinion (Choi et al., 2006; Yang and Cardie, 2013; Katiyar and Cardie, 2016). In this work, we follow the opinion role labeling (ORL) task setting of Marasovi´c and Frank (2018) and Zhang et al. (2019b), and try to predict holders and targe"
2020.acl-main.297,N19-1118,1,0.170002,"dentify different roles relevant to each opinion, i.e., who expressed what kind of sentiment towards what (Liu, 2012). Due to the lack of large-scale labeled data, ORL remains a challenging task to tackle. As a reference point, semantic role labeling (SRL) is very similar to ORL in the problem definition, but has 10 times more labeled data and thus achieves much higher performance than ORL (80∼90 vs. 60∼70 in F1 score). Motivated by the correlations between the two tasks, SRL has been utilized to help the ORL task by many previous studies (Ruppenhofer et al., 2008; Marasovi´c and Frank, 2018; Zhang et al., 2019b). However, when opinion expressions and arguments compose complicated syntactic structures, it is difficult to correctly recognize the opinion arguments even with shallow semantic representation like SRL (Marasovi´c and Frank, 2018). To compensate for the limited scale of labeled data for data-driven approaches, linguistic knowledge like syntax provides structural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without"
2020.acl-main.297,D14-1162,0,0.083367,"RT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) to obtain deep contextualized word representations as our extra inputs. In particular, we use BERT-base (uncased) model and extract representations from the top-1 hidden layer. Our experiments show that using the top-1 layer representations performs better than the more common use of aggregating top-4 hidden layers.2 Parameters. We follow the previous works of Zhang et al. (2019b) and Marasovi´c and Frank (2018) without much parameter tuning. Specifically, we use the pretrained 100-dimensional glove embeddings (Pennington et al., 2014). The BiLSTM layer number is set to 3, and the hidden output size is 200. We apply 0.33 dropout to word representation and the hidden states of the BiLSTM. We choose Adam (Kingma and Ba, 2014) to optimize model parameters with a learning rate 10−3 . The entire training instances are trained for 30 epochs with the batch size of 50, and the best-epoch model at the peak performance on the dev corpus is chosen. For the MTL, we train the batches of ORL and parsing in turn since this interleaving training can obtain better performance in our experiments. Besides, we use the corpus weighting trick to"
2020.acl-main.297,N19-1066,0,0.110957,"dentify different roles relevant to each opinion, i.e., who expressed what kind of sentiment towards what (Liu, 2012). Due to the lack of large-scale labeled data, ORL remains a challenging task to tackle. As a reference point, semantic role labeling (SRL) is very similar to ORL in the problem definition, but has 10 times more labeled data and thus achieves much higher performance than ORL (80∼90 vs. 60∼70 in F1 score). Motivated by the correlations between the two tasks, SRL has been utilized to help the ORL task by many previous studies (Ruppenhofer et al., 2008; Marasovi´c and Frank, 2018; Zhang et al., 2019b). However, when opinion expressions and arguments compose complicated syntactic structures, it is difficult to correctly recognize the opinion arguments even with shallow semantic representation like SRL (Marasovi´c and Frank, 2018). To compensate for the limited scale of labeled data for data-driven approaches, linguistic knowledge like syntax provides structural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without"
2020.acl-main.297,D19-1057,1,0.172248,"dentify different roles relevant to each opinion, i.e., who expressed what kind of sentiment towards what (Liu, 2012). Due to the lack of large-scale labeled data, ORL remains a challenging task to tackle. As a reference point, semantic role labeling (SRL) is very similar to ORL in the problem definition, but has 10 times more labeled data and thus achieves much higher performance than ORL (80∼90 vs. 60∼70 in F1 score). Motivated by the correlations between the two tasks, SRL has been utilized to help the ORL task by many previous studies (Ruppenhofer et al., 2008; Marasovi´c and Frank, 2018; Zhang et al., 2019b). However, when opinion expressions and arguments compose complicated syntactic structures, it is difficult to correctly recognize the opinion arguments even with shallow semantic representation like SRL (Marasovi´c and Frank, 2018). To compensate for the limited scale of labeled data for data-driven approaches, linguistic knowledge like syntax provides structural information representing human understanding of the text. Naturally, dependency relations between words ease the discovering of opinion roles. Taking the example in Figure 1, the Target span is often incompletely recognized without"
2020.acl-main.397,W13-2322,0,0.401941,"Missing"
2020.acl-main.397,P19-1284,0,0.383702,"in Figure 1, AMR prefers to select the coordination (i.e. “and”) as the root, which is different from syntactic dependencies (i.e. “came”). Given the above observations, we investigate the effectiveness of latent syntactic dependencies for 4306 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4306–4319 c July 5 - 10, 2020. 2020 Association for Computational Linguistics AMR parsing. Different from existing work (Wang et al., 2016), which uses a dependency parser to provide explicit syntactic structures, we make use of a two-parameter distribution (Bastings et al., 2019) to induce latent graphs, which is differentiable under reparameterization (Kingma and Welling, 2014). We thus build a end-to-end model for AMR parsing with induced latent dependency structures as a middle layer, which is tuned in AMR training and thus can be more aligned to the need of AMR structure. For better investigating the correlation between induced and gold syntax, and better combine the strengths, we additionally consider fusing gold and induced structural dependencies into an align-free AMR parser (Zhang et al., 2019a). Specifically, we first obtain the input sentence’s syntactic de"
2020.acl-main.397,E17-1051,0,0.182802,"sent semantic relations. AMR introduces re-entrance relation to depict the node reuse in the graphs. It has been adopted in downstream NLP tasks, including text summarization (Liu et al., 2015; Dohare and Karnick, 2017), question answering (Mitra and Baral, 2016) and machine translation (Jones et al., 2012; Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR graph with decoding algorithms such as greedy and maximum spanning tree (MST). Additionally, reinforcement learning (Naseem et al., 2019) and sequ"
2020.acl-main.397,P14-1134,0,0.0497626,"on (Jones et al., 2012; Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR graph with decoding algorithms such as greedy and maximum spanning tree (MST). Additionally, reinforcement learning (Naseem et al., 2019) and sequence-to-sequence (Konstas et al., 2017) have been exploited in AMR parsing as well. Previous works (Wang et al., 2016; Artzi et al., 2015) shows that structural information can bring benefit to AMR parsing. Illustrated by Figure 1, for example syntactic dependencies can convey the main"
2020.acl-main.397,D18-1198,0,0.169396,"to depict the node reuse in the graphs. It has been adopted in downstream NLP tasks, including text summarization (Liu et al., 2015; Dohare and Karnick, 2017), question answering (Mitra and Baral, 2016) and machine translation (Jones et al., 2012; Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR graph with decoding algorithms such as greedy and maximum spanning tree (MST). Additionally, reinforcement learning (Naseem et al., 2019) and sequence-to-sequence (Konstas et al., 2017) have been exploit"
2020.acl-main.397,P19-1024,0,0.0188529,"ng et al. (2019a) extend a pointer generator (See et al., 2017), which can generate a node multiple times without alignment through the copy mechanism. With regards to latent structure, Naradowsky et al. (2012) couples syntactically-oriented NLP tasks to combinatorially constrained hidden syntactic representations. Bowman et al. (2016); Yogatama et al. (2017) and Choi et al. (2018) generate unsupervised constituent tree for text classification. The latent constituent trees are shallower than human annotated, and it can boost the performance of downstream NLP tasks (e.g., text classification). Guo et al. (2019) and Ji et al. (2019) employ selfattention and bi-affine attention mechanism respectively to generate soft connected graphs, and then adopt GNNs to encode the soft structure to take advantage from the structural information to their works. GCN and its variants are increasingly applied in embedding syntactic and semantic structures in NLP tasks (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Damonte and Cohen, 2019). Syntactic-GCN tries to alleviate the error propagation in external parsers with gates mechanism, it encodes both relations and labels with the gates, and filters the output"
2020.acl-main.397,C12-1083,0,0.0603499,"Missing"
2020.acl-main.397,P17-1014,0,0.211142,"17; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR graph with decoding algorithms such as greedy and maximum spanning tree (MST). Additionally, reinforcement learning (Naseem et al., 2019) and sequence-to-sequence (Konstas et al., 2017) have been exploited in AMR parsing as well. Previous works (Wang et al., 2016; Artzi et al., 2015) shows that structural information can bring benefit to AMR parsing. Illustrated by Figure 1, for example syntactic dependencies can convey the main predicate-argument structure. However, dependency structural information may be noisy due to the error propagation of external parsers. Moreover, AMR concentrates on semantic relations, which can be different from syntactic dependencies. For instance, in Figure 1, AMR prefers to select the coordination (i.e. “and”) as the root, which is different fro"
2020.acl-main.397,P19-1232,0,0.0340383,"Missing"
2020.acl-main.397,P82-1020,0,0.807868,"Missing"
2020.acl-main.397,P19-1450,0,0.0798049,"Missing"
2020.acl-main.397,P19-1237,0,0.0164503,"nd a pointer generator (See et al., 2017), which can generate a node multiple times without alignment through the copy mechanism. With regards to latent structure, Naradowsky et al. (2012) couples syntactically-oriented NLP tasks to combinatorially constrained hidden syntactic representations. Bowman et al. (2016); Yogatama et al. (2017) and Choi et al. (2018) generate unsupervised constituent tree for text classification. The latent constituent trees are shallower than human annotated, and it can boost the performance of downstream NLP tasks (e.g., text classification). Guo et al. (2019) and Ji et al. (2019) employ selfattention and bi-affine attention mechanism respectively to generate soft connected graphs, and then adopt GNNs to encode the soft structure to take advantage from the structural information to their works. GCN and its variants are increasingly applied in embedding syntactic and semantic structures in NLP tasks (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Damonte and Cohen, 2019). Syntactic-GCN tries to alleviate the error propagation in external parsers with gates mechanism, it encodes both relations and labels with the gates, and filters the output of each GCN layer ove"
2020.acl-main.397,N15-1114,0,0.0536212,"sentence “The boy came and left”. The dashed lines denote the connected relations in the syntactic dependencies but not appear in the AMR graph. Introduction Abstract Meaning Representations (AMRs) (Banarescu et al., 2013) model sentence level semantics as rooted, directed, acyclic graphs. Nodes in the graph are concepts which represent the events, objects and features of the input sentence, and edges between nodes represent semantic relations. AMR introduces re-entrance relation to depict the node reuse in the graphs. It has been adopted in downstream NLP tasks, including text summarization (Liu et al., 2015; Dohare and Karnick, 2017), question answering (Mitra and Baral, 2016) and machine translation (Jones et al., 2012; Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake Univer"
2020.acl-main.397,D18-1264,0,0.479564,"-entrance relation to depict the node reuse in the graphs. It has been adopted in downstream NLP tasks, including text summarization (Liu et al., 2015; Dohare and Karnick, 2017), question answering (Mitra and Baral, 2016) and machine translation (Jones et al., 2012; Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR graph with decoding algorithms such as greedy and maximum spanning tree (MST). Additionally, reinforcement learning (Naseem et al., 2019) and sequence-to-sequence (Konstas et al., 2017"
2020.acl-main.397,P18-1037,0,0.710402,"Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR graph with decoding algorithms such as greedy and maximum spanning tree (MST). Additionally, reinforcement learning (Naseem et al., 2019) and sequence-to-sequence (Konstas et al., 2017) have been exploited in AMR parsing as well. Previous works (Wang et al., 2016; Artzi et al., 2015) shows that structural information can bring benefit to AMR parsing. Illustrated by Figure 1, for example syntactic dependencies can convey the main predicate-argument s"
2020.acl-main.397,P14-5010,0,0.0123999,"interpret the probabilistic relations between the input words in AMR parsing by generating the latent graph2 . 2 Baseline: Align-Free AMR Parsing We adopt the parser of Zhang et al. (2019a) as our baseline, which treats AMR parsing as sequenceto-graph transduction. 2.1 Task Formalization Our baseline splits AMR parsing into a two-stage procedure: concept identification and edge prediction. The first task aims to identify the concepts (nodes) in AMR graph from input tokens, and the second task is designed to predict semantic relations between identified concepts. 1 We employ Stanford CoreNLP (Manning et al., 2014) to get the dependencies. 2 Our code will be available at: https://github. com/zhouqiji/ACL2020_AMR_Parsing. Formally, for a given input sequence of words w = hw1 , ..., wn i, the goal of concept identification in our baseline is sequentially predicting the concept nodes u = hu1 , ..., um i in the output AMR graph, and deterministically assigning corresponding indices d = hd1 , ..., dm i. P (u) = m Y P (ui |u<i , d<i , w), i=1 After identifying the concept nodes c and their corresponding indices d, we predict the semantic relations in the searching space R(u). X Predict(u) = arg max score(ui ,"
2020.acl-main.397,D17-1159,0,0.244453,"d wi at the l − th layer is: n X (l) (l−1) hi = σ( A˜ij W (l) hj /di + b(l) ), Training Similar to our baseline (Zhang et al., 2019a), we linearize the AMR concepts nodes by a pre-order traversal over the training dataset. We obtain gradient estimates of E(φ, θ) through Monte Carlo sampling from: E(φ, θ) = EU (0,I) [log P (node|ut , gφ (u, w), θ) + log Pt (head|uk , gφ (u, w), θ) j=1 ˜ = A + I with the n × n identity matrix where A Pn ˜ I, di = j=1 Aij is the degree of word wi in the graph for normalizing the activation to avoid the word representation with significantly different magnitudes (Marcheggiani and Titov, 2017; Kipf and Welling, 2017), and σ is a nonlinear activation function. In order to take benefits from both explicit and latent structural information in AMR parsing, we extend the Syntactic-GCN (Marcheggiani and Titov, 2017; Zhang et al., 2018b) with a graph fusion layer and omit labels in the graph (i.e. we only consider the connected relation in GCN). Specifically, we propose to merge the parsed syntactic dependencies and sampled latent graph through a graph fusion layer: F = πL + (1 − π)D where π is trainable gate variables are calculated via the sigmoid function, D and L are the parsed synta"
2020.acl-main.397,J08-4003,0,0.0123942,"Nodes in the graph are concepts which represent the events, objects and features of the input sentence, and edges between nodes represent semantic relations. AMR introduces re-entrance relation to depict the node reuse in the graphs. It has been adopted in downstream NLP tasks, including text summarization (Liu et al., 2015; Dohare and Karnick, 2017), question answering (Mitra and Baral, 2016) and machine translation (Jones et al., 2012; Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR grap"
2020.acl-main.397,P17-1186,0,0.0948125,"Missing"
2020.acl-main.397,D14-1162,0,0.0858128,"ching space R(u). X Predict(u) = arg max score(ui , uj ), r∈R(u) (u ,u )∈r i j where r = {(ui , uj ) |1 ≤ i, j ≤ m} is a set of directed relations between concept nodes. 2.2 Align-Free Concept Identification Our baseline extends the pointer-generator network with self-copy mechanism for concept identification (See et al., 2017; Zhang et al., 2018a). The extended model can copy the nodes not only from the source text, but also from the previously generated list of nodes on the target side. The concept identifier firstly encodes the input sentence into concatenated vector embeddings with GloVe (Pennington et al., 2014), BERT (Devlin et al., 2019), POS (part-of-speech) and characterlevel (Kim et al., 2016) embeddings. Subsequently, we encode the embedded sentence by a two-layer bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997): → − ← −l l−1 l l hli = [ f l (hl−1 i , hi−1 ); f (hi , hi+1 )], where hli is the l-th layer encoded hidden state at the time step i and h0i is the embedded token wi . Different from the encoding stage, the decoder does not use pre-trained BERT embeddings, but employs a two-layer LSTM to generate the decoding hidden state slt at each time step: l slt = f"
2020.acl-main.397,D15-1136,0,0.0514325,"Missing"
2020.acl-main.397,P17-1099,0,0.648157,"cept nodes u = hu1 , ..., um i in the output AMR graph, and deterministically assigning corresponding indices d = hd1 , ..., dm i. P (u) = m Y P (ui |u<i , d<i , w), i=1 After identifying the concept nodes c and their corresponding indices d, we predict the semantic relations in the searching space R(u). X Predict(u) = arg max score(ui , uj ), r∈R(u) (u ,u )∈r i j where r = {(ui , uj ) |1 ≤ i, j ≤ m} is a set of directed relations between concept nodes. 2.2 Align-Free Concept Identification Our baseline extends the pointer-generator network with self-copy mechanism for concept identification (See et al., 2017; Zhang et al., 2018a). The extended model can copy the nodes not only from the source text, but also from the previously generated list of nodes on the target side. The concept identifier firstly encodes the input sentence into concatenated vector embeddings with GloVe (Pennington et al., 2014), BERT (Devlin et al., 2019), POS (part-of-speech) and characterlevel (Kim et al., 2016) embeddings. Subsequently, we encode the embedded sentence by a two-layer bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997): → − ← −l l−1 l l hli = [ f l (hl−1 i , hi−1 ); f (hi , hi+1"
2020.acl-main.397,Q19-1002,1,0.831473,"AMR graph. Introduction Abstract Meaning Representations (AMRs) (Banarescu et al., 2013) model sentence level semantics as rooted, directed, acyclic graphs. Nodes in the graph are concepts which represent the events, objects and features of the input sentence, and edges between nodes represent semantic relations. AMR introduces re-entrance relation to depict the node reuse in the graphs. It has been adopted in downstream NLP tasks, including text summarization (Liu et al., 2015; Dohare and Karnick, 2017), question answering (Mitra and Baral, 2016) and machine translation (Jones et al., 2012; Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018"
2020.acl-main.397,D12-1074,0,0.0294963,"ls. Lyu and Titov (2018) treat the alignments as an latent variable for their probabilistic model, which jointly obtains the concepts, relations and alignments variables. Sequence-to-sequence AMR parsers transform AMR graphs into serialized sequences by external traversal rules, and then restore the generated the AMR sequence to avoid aligning issue (Konstas et al., 2017; van Noord and Bos, 2017). Moreover, Zhang et al. (2019a) extend a pointer generator (See et al., 2017), which can generate a node multiple times without alignment through the copy mechanism. With regards to latent structure, Naradowsky et al. (2012) couples syntactically-oriented NLP tasks to combinatorially constrained hidden syntactic representations. Bowman et al. (2016); Yogatama et al. (2017) and Choi et al. (2018) generate unsupervised constituent tree for text classification. The latent constituent trees are shallower than human annotated, and it can boost the performance of downstream NLP tasks (e.g., text classification). Guo et al. (2019) and Ji et al. (2019) employ selfattention and bi-affine attention mechanism respectively to generate soft connected graphs, and then adopt GNNs to encode the soft structure to take advantage f"
2020.acl-main.397,P19-1451,0,0.414534,"., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR graph with decoding algorithms such as greedy and maximum spanning tree (MST). Additionally, reinforcement learning (Naseem et al., 2019) and sequence-to-sequence (Konstas et al., 2017) have been exploited in AMR parsing as well. Previous works (Wang et al., 2016; Artzi et al., 2015) shows that structural information can bring benefit to AMR parsing. Illustrated by Figure 1, for example syntactic dependencies can convey the main predicate-argument structure. However, dependency structural information may be noisy due to the error propagation of external parsers. Moreover, AMR concentrates on semantic relations, which can be different from syntactic dependencies. For instance, in Figure 1, AMR prefers to select the coordination"
2020.acl-main.397,D17-1129,0,0.197665,"s. AMR introduces re-entrance relation to depict the node reuse in the graphs. It has been adopted in downstream NLP tasks, including text summarization (Liu et al., 2015; Dohare and Karnick, 2017), question answering (Mitra and Baral, 2016) and machine translation (Jones et al., 2012; Song et al., 2019). AMR parsing aims to transform natural language sentences into AMR semantic graphs. Similar to constituent parsing and dependency parsing (Nivre, 2008; Dozat and Manning, 2017), AMR parsers mainly employ two parsing techniques: transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Wang and Xue, 2017; Liu et al., 2018; Guo and Lu, 2018) use a sequence of transition actions † cc nsubj Part of work was done when the author was visiting Westlake University * Corresponding author. to incrementally construct the graph, while graphbased parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a; Cai and Lam, 2019) divides the task into concept identification and relation extraction stages and then generate a full AMR graph with decoding algorithms such as greedy and maximum spanning tree (MST). Additionally, reinforcement learning (Naseem et al., 2019) and sequence-to-sequence (Ko"
2020.acl-main.397,P19-1009,0,0.367829,"Missing"
2020.acl-main.397,D19-1392,0,0.175275,"Missing"
2020.acl-main.397,D18-1194,0,0.0605107,"Missing"
2020.acl-main.397,D18-1244,0,0.0942822,", ..., um i in the output AMR graph, and deterministically assigning corresponding indices d = hd1 , ..., dm i. P (u) = m Y P (ui |u<i , d<i , w), i=1 After identifying the concept nodes c and their corresponding indices d, we predict the semantic relations in the searching space R(u). X Predict(u) = arg max score(ui , uj ), r∈R(u) (u ,u )∈r i j where r = {(ui , uj ) |1 ≤ i, j ≤ m} is a set of directed relations between concept nodes. 2.2 Align-Free Concept Identification Our baseline extends the pointer-generator network with self-copy mechanism for concept identification (See et al., 2017; Zhang et al., 2018a). The extended model can copy the nodes not only from the source text, but also from the previously generated list of nodes on the target side. The concept identifier firstly encodes the input sentence into concatenated vector embeddings with GloVe (Pennington et al., 2014), BERT (Devlin et al., 2019), POS (part-of-speech) and characterlevel (Kim et al., 2016) embeddings. Subsequently, we encode the embedded sentence by a two-layer bidirectional LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997): → − ← −l l−1 l l hli = [ f l (hl−1 i , hi−1 ); f (hi , hi+1 )], where hli is th"
2020.acl-main.40,N19-1409,0,0.0218598,"Missing"
2020.acl-main.40,D18-1443,0,0.0551687,"Missing"
2020.acl-main.40,P07-2045,0,0.0102101,"2011, tst2012). For En→Fr, there are 236k sentence pairs for training and 10263 for validation. The concatenated validation sets are used as the test set (dev2010, tst2010, tst2011, tst2012, tst2013, tst2014, tst2015). For all IWSLT translation tasks, we use a joint source and target vocabulary with 10k byte-pair-encoding (BPE) types (Sennrich et al., 2016). For the WMT14 En→De task, the training corpus is identical to previous work (Vaswani et al., 2017; Wang et al., 2019a), which consists of about 4.5 million sentence pairs. All the data are tokenized using the script tokenizer.pl of Moses (Koehn et al., 2007) and segmented into subword symbols using jointly BPE with 32k merge operations. The shared source-target vocabulary contains about 37k BPE tokens. We use newstest2013 as the development set and newstest2014 as the test set. Following previous work, we evaluate IWSLT tasks with tokenized case-insensitive BLEU and report tokenized case-sensitive BLEU (Papineni et al., 2002) for WMT14 En→De. Model Settings. For IWSLT, the model configuration is transformer iwslt, representing a small model with embedding size 256 and FFN layer dimension 512. We train all models using the Adam optimizer (β1 /β2 ="
2020.acl-main.40,W04-1013,0,0.0342604,"Missing"
2020.acl-main.40,D15-1166,0,0.0824642,"56 on WMT14 English→German task that significantly outperforms state-of-the-art deep NMT models. 1 Introduction Neural machine translation (NMT) directly models the entire translation process using a large neural network and has gained rapid progress in recent years (Sutskever et al., 2014; Sennrich et al., 2016). The structure of NMT models has evolved quickly, such as RNN-based (Wu et al., 2016), CNN-based (Gehring et al., 2017) and attentionbased (Vaswani et al., 2017) systems. All of these models follow the encoder-decoder framework with attention (Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) paradigm. ∗ † Work done at Alibaba Group. Corresponding Author. Deep neural networks have revolutionized the state-of-the-art in various communities, from computer vision to natural language processing. However, training deep neural networks has been always a challenging problem. To encourage gradient flow and error propagation, researchers in the field of computer vision have proposed various approaches, such as residual connections (He et al., 2016), densely connected networks (Huang et al., 2017) and deep layer aggregation (Yu et al., 2018). In natural language processing, constructing dee"
2020.acl-main.40,W18-6301,0,0.0290882,"g of 0.1. Sentence pairs containing 16K∼32K tokens are grouped into one batch. Unless otherwise stated, we train small models with 15K maximum steps, Depth dec. (N ) enc. (N ×M ) 36-layer 6 6×6 54-layer 6 6×9 72-layer 6 6×12 Table 1: Deep architectures of M SC on IWSLT tasks. We simply set M1 = · · · = MN = M . and decode sentences using beam search with a beam size of 5 and length penalty of 1.0. For WMT14 En→De, the model configuration is transformer base/big, with a embedding size of 512/1024 and a FFN layer dimension of 2048/4096. Experiments on WMT are conducted on 8 P100 GPUs. Following Ott et al. (2018), we accumulate the gradient 8 iterations and then update to simulate a 64-GPU environment with a batch-size of 65K tokens per step. The Adam optimizer (β1 /β2 = 0.9/0.98 for base, β1 /β2 = 0.9/0.998) for big) and the warm-up strategy (8K steps for base, 16K steps for big) are also adopted. We use relatively larger batch size and dropout rate for deeper and bigger models for better convergence. The transformer base/big is updated for 100K/300K steps. For evaluation, we average the last 5/20 checkpoints for base/big, each of which is saved at the end of an epoch. Beam search is adopted with a w"
2020.acl-main.40,P02-1040,0,0.106736,"14 En→De task, the training corpus is identical to previous work (Vaswani et al., 2017; Wang et al., 2019a), which consists of about 4.5 million sentence pairs. All the data are tokenized using the script tokenizer.pl of Moses (Koehn et al., 2007) and segmented into subword symbols using jointly BPE with 32k merge operations. The shared source-target vocabulary contains about 37k BPE tokens. We use newstest2013 as the development set and newstest2014 as the test set. Following previous work, we evaluate IWSLT tasks with tokenized case-insensitive BLEU and report tokenized case-sensitive BLEU (Papineni et al., 2002) for WMT14 En→De. Model Settings. For IWSLT, the model configuration is transformer iwslt, representing a small model with embedding size 256 and FFN layer dimension 512. We train all models using the Adam optimizer (β1 /β2 = 0.9/0.98) with adaptive learning rate schedule (warm-up step with 4K for shallow models, 8K for deep models) as in (Vaswani et al., 2017) and label smoothing of 0.1. Sentence pairs containing 16K∼32K tokens are grouped into one batch. Unless otherwise stated, we train small models with 15K maximum steps, Depth dec. (N ) enc. (N ×M ) 36-layer 6 6×6 54-layer 6 6×9 72-layer"
2020.acl-main.40,N18-1202,0,0.0341367,"ous communities, from computer vision to natural language processing. However, training deep neural networks has been always a challenging problem. To encourage gradient flow and error propagation, researchers in the field of computer vision have proposed various approaches, such as residual connections (He et al., 2016), densely connected networks (Huang et al., 2017) and deep layer aggregation (Yu et al., 2018). In natural language processing, constructing deep architectures has shown effectiveness in language modeling, question answering, text classification and natural language inference (Peters et al., 2018; Radford et al., 2018; Al-Rfou et al., 2019; Devlin et al., 2019). However, among existing NMT models, most of them are generally equipped with 4-8 encoder and decoder layers (Wu et al., 2016; Vaswani et al., 2017). Deep neural network has been explored relatively little in NMT. Recent evidence (Bapna et al., 2018; Wang et al., 2019a) shows that model depth is indeed of importance to NMT, but a degradation problem has been exposed: by simply stacking more layers, the translation quality gets saturated and then degrades rapidly. To address this problem, Bapna et al. (2018) proposed a transpare"
2020.acl-main.40,P17-1099,0,0.0538974,"Missing"
2020.acl-main.40,P16-1162,0,0.526873,"ze and can obtain improvements of translation quality from considerably increased depth. On IWSLT translation tasks with three translation directions, our extremely deep models (with 72-layer encoders) surpass strong baselines by +2.2∼+3.1 BLEU points. In addition, our deep M SC achieves a BLEU score of 30.56 on WMT14 English→German task that significantly outperforms state-of-the-art deep NMT models. 1 Introduction Neural machine translation (NMT) directly models the entire translation process using a large neural network and has gained rapid progress in recent years (Sutskever et al., 2014; Sennrich et al., 2016). The structure of NMT models has evolved quickly, such as RNN-based (Wu et al., 2016), CNN-based (Gehring et al., 2017) and attentionbased (Vaswani et al., 2017) systems. All of these models follow the encoder-decoder framework with attention (Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) paradigm. ∗ † Work done at Alibaba Group. Corresponding Author. Deep neural networks have revolutionized the state-of-the-art in various communities, from computer vision to natural language processing. However, training deep neural networks has been always a challenging problem. To encourage"
2020.acl-main.40,N18-1117,0,0.0319613,"Missing"
2020.acl-main.40,D19-1083,0,0.141608,"Missing"
2020.acl-main.40,P18-1166,0,0.087265,"collaboration helps in constructing extremely deep models. (3) Considering that the deep M SC introduces more parameters, we also train another two M SC models with about the same or double number of parameters: with 18/36 layers, embedding size 512 and F FN layer dimension 1024. These models underperform the deeper 72-layer model, which shows that the number of parameters is not the key to the improvement. 5 Related Work Researchers have constructed deep NMT models that use linear connections to reduce the gradient propagation length inside the topology (Zhou et al., 2016; Wang et al., 2017; Zhang et al., 2018b) or read-write operations on stacked layers of memories (Meng et al., 2015). Such work has been conducted on the basis of the conventional RNN architectures and may not be fully applicable to the advanced Transformer. Recently, Bapna et al. (2018) introduced a transparent network into NMT models to ease the optimization of models with deeper encoders. To improve gradient flow they let each decoder layer find an unique weighted combination of all encoder layer outputs, instead of just the top encoder layer. Wang et al. (2019a) found that adopting the proper use of layer normalization helps to"
2020.acl-main.40,P17-1013,0,0.0178269,"WSLT14 En→De task. collaboration helps in constructing extremely deep models. (3) Considering that the deep M SC introduces more parameters, we also train another two M SC models with about the same or double number of parameters: with 18/36 layers, embedding size 512 and F FN layer dimension 1024. These models underperform the deeper 72-layer model, which shows that the number of parameters is not the key to the improvement. 5 Related Work Researchers have constructed deep NMT models that use linear connections to reduce the gradient propagation length inside the topology (Zhou et al., 2016; Wang et al., 2017; Zhang et al., 2018b) or read-write operations on stacked layers of memories (Meng et al., 2015). Such work has been conducted on the basis of the conventional RNN architectures and may not be fully applicable to the advanced Transformer. Recently, Bapna et al. (2018) introduced a transparent network into NMT models to ease the optimization of models with deeper encoders. To improve gradient flow they let each decoder layer find an unique weighted combination of all encoder layer outputs, instead of just the top encoder layer. Wang et al. (2019a) found that adopting the proper use of layer no"
2020.acl-main.40,P19-1499,0,0.0772604,"we show that: • While models with traditional stacking architecture exhibit worse performance on both training and validation data when depth increases, our framework is easy to optimize. • The deep M SC nets (with 72-layer encoders) bring great improvements on translation quality from increased depth, producing results that substantially better than existing systems. On the WMT14 English→German task, we obtain improved results by deep M SC networks with a depth of 48 layers, outperforming strong baselines by +2.5 BLEU points, and also defeat state-of-theart deep NMT models (Wu et al., 2019; Zhang et al., 2019a) with identical or less parameters.2 2 Background decomposed into the product of the conditional probability of each target word: P (y|x; Θ) = QTy t=1 P (yt |y&lt;t , x; Θ), where Ty is the length of sentence y, y&lt;t is the partial translation that contains the target tokens before position t. An encoder-decoder framework is commonly adopted to model the conditional probability P (y|x; Θ), in which the encoder and decoder can be implemented as RNN (Wu et al., 2016), CNN (Gehring et al., 2017), or Self-Attention network (Vaswani et al., 2017). Despite variant types of NMT architectures, multiple-"
2020.acl-main.40,P19-1176,0,0.284488,"cted networks (Huang et al., 2017) and deep layer aggregation (Yu et al., 2018). In natural language processing, constructing deep architectures has shown effectiveness in language modeling, question answering, text classification and natural language inference (Peters et al., 2018; Radford et al., 2018; Al-Rfou et al., 2019; Devlin et al., 2019). However, among existing NMT models, most of them are generally equipped with 4-8 encoder and decoder layers (Wu et al., 2016; Vaswani et al., 2017). Deep neural network has been explored relatively little in NMT. Recent evidence (Bapna et al., 2018; Wang et al., 2019a) shows that model depth is indeed of importance to NMT, but a degradation problem has been exposed: by simply stacking more layers, the translation quality gets saturated and then degrades rapidly. To address this problem, Bapna et al. (2018) proposed a transparent attention mechanism to ease the optimization of the models with deeper encoders. Wang et al. (2019a) continued this line of research but construct a much deeper encoder for Transformer by adopting the pre-norm method that establishes a direct way to propagate error gradients from the top layer to bottom levels, and passing the com"
2020.acl-main.40,P19-1624,0,0.283365,"cted networks (Huang et al., 2017) and deep layer aggregation (Yu et al., 2018). In natural language processing, constructing deep architectures has shown effectiveness in language modeling, question answering, text classification and natural language inference (Peters et al., 2018; Radford et al., 2018; Al-Rfou et al., 2019; Devlin et al., 2019). However, among existing NMT models, most of them are generally equipped with 4-8 encoder and decoder layers (Wu et al., 2016; Vaswani et al., 2017). Deep neural network has been explored relatively little in NMT. Recent evidence (Bapna et al., 2018; Wang et al., 2019a) shows that model depth is indeed of importance to NMT, but a degradation problem has been exposed: by simply stacking more layers, the translation quality gets saturated and then degrades rapidly. To address this problem, Bapna et al. (2018) proposed a transparent attention mechanism to ease the optimization of the models with deeper encoders. Wang et al. (2019a) continued this line of research but construct a much deeper encoder for Transformer by adopting the pre-norm method that establishes a direct way to propagate error gradients from the top layer to bottom levels, and passing the com"
2020.acl-main.40,P19-1558,0,0.280093,"ranslation tasks, we show that: • While models with traditional stacking architecture exhibit worse performance on both training and validation data when depth increases, our framework is easy to optimize. • The deep M SC nets (with 72-layer encoders) bring great improvements on translation quality from increased depth, producing results that substantially better than existing systems. On the WMT14 English→German task, we obtain improved results by deep M SC networks with a depth of 48 layers, outperforming strong baselines by +2.5 BLEU points, and also defeat state-of-theart deep NMT models (Wu et al., 2019; Zhang et al., 2019a) with identical or less parameters.2 2 Background decomposed into the product of the conditional probability of each target word: P (y|x; Θ) = QTy t=1 P (yt |y&lt;t , x; Θ), where Ty is the length of sentence y, y&lt;t is the partial translation that contains the target tokens before position t. An encoder-decoder framework is commonly adopted to model the conditional probability P (y|x; Θ), in which the encoder and decoder can be implemented as RNN (Wu et al., 2016), CNN (Gehring et al., 2017), or Self-Attention network (Vaswani et al., 2017). Despite variant types of NMT arch"
2020.acl-main.40,Q16-1027,0,0.0119839,"Ablation study on IWSLT14 En→De task. collaboration helps in constructing extremely deep models. (3) Considering that the deep M SC introduces more parameters, we also train another two M SC models with about the same or double number of parameters: with 18/36 layers, embedding size 512 and F FN layer dimension 1024. These models underperform the deeper 72-layer model, which shows that the number of parameters is not the key to the improvement. 5 Related Work Researchers have constructed deep NMT models that use linear connections to reduce the gradient propagation length inside the topology (Zhou et al., 2016; Wang et al., 2017; Zhang et al., 2018b) or read-write operations on stacked layers of memories (Meng et al., 2015). Such work has been conducted on the basis of the conventional RNN architectures and may not be fully applicable to the advanced Transformer. Recently, Bapna et al. (2018) introduced a transparent network into NMT models to ease the optimization of models with deeper encoders. To improve gradient flow they let each decoder layer find an unique weighted combination of all encoder layer outputs, instead of just the top encoder layer. Wang et al. (2019a) found that adopting the pro"
2020.acl-main.40,D18-1338,0,\N,Missing
2020.acl-main.40,D18-1457,0,\N,Missing
2020.acl-main.40,N19-1423,0,\N,Missing
2020.acl-main.482,D13-1135,0,0.318673,"Missing"
2020.acl-main.482,P16-1074,0,0.187763,"Missing"
2020.acl-main.482,K17-1023,0,0.0195992,"us stateof-the-art results on both tasks. the primary goal, thus we follow the first line of research using human-annotated data. Rao et al. (2015) studied zero pronoun resolution in multi-turn dialogues, claiming that their model does not rely on parsing trees to extract ZP positions and noun phrase as resolution candidates. However, they only consider the dropped pronouns that correspond to one of the dialogue participant. As a result, they only explore a small subset of the entire ZP resolution problem, and their task is closer to zero pronoun recovery. Most similar to our work, Liu et al. (2017) converted zero pronoun resolution as a machine reading comprehension task (Rajpurkar et al., 2016) in order to automatically construct a large-scale pseudo dataset for model pretraining. However, their model finetuning and evaluation with benchmark data still rely on human-annotated trees and gold zero pronoun positions. As a result, it is still uncertain what performance a model can achieve without such gold inputs. We address both issues in the joint task. Our work is inspired by the recent advances of heterogeneous multi-task learning using BERT (Devlin et al., 2019), which combines the su"
2020.acl-main.482,W16-3612,0,0.0368814,"Missing"
2020.acl-main.482,D10-1062,0,0.0193741,"o pronoun tasks. In addition, we find that it helps the robustness of multi-task learning to add a common sub-task (e.g. zero pronoun detection in our case) for additional supervision and alleviating annotation variances, if such a subtask is available. 2 3 Related work Previous work considers zero pronoun resolution and recovery separately. For zero pronoun recovery, existing methods can be classified according to the types of annotations they use. One line of work (Yang et al., 2015, 2019) simply relies on the human annotations, solving the task as sequence labeling. The other line of work (Chung and Gildea, 2010; Xiang et al., 2013; Wang et al., 2016) mines weak supervision signals from a large bilingual parallel corpus, where the other language is non-prodrop with fewer pronoun drops. The latter requires massive training data, and the MT performance is 1 https://catalog.ldc.upenn.edu/LDC2013T19 Model As shown in Figure 2, we model ZP recovery (frec ), ZP resolution (fres ), and the auxiliary ZP detection (fdet ) task with multi-task learning, where BERT (Devlin et al., 2019) is used to represent each input sentence s1 . . . sN of N words to provide shared features. 3.1 Zero pronoun recovery ZP recov"
2020.acl-main.482,N19-1423,0,0.547208,"as a dropped pronoun and what type the pronoun is. ZP resolution is solved as extractive reading comprehension (Rajpurkar et al., 2016), where each word space is taken as a query and its anaphoric mentions are treated as the answers. For non-ZP spaces where there is no corresponding anaphoric mentions, we assign the sentence beginning (span [0,0]) as the answer. Experiments on two benchmarks, OntoNotes 5.01 (ZP resolution) and BaiduZhdiao (Zhang et al., 2016) (ZP recovery), show that joint modeling gives us 1.5+ absolute F1-score gains for both tasks over our very strong baselines using BERT (Devlin et al., 2019). Our overall system gives an dramatic improvement of 3.5 F1 points over previous stateof-the-art results on both tasks. the primary goal, thus we follow the first line of research using human-annotated data. Rao et al. (2015) studied zero pronoun resolution in multi-turn dialogues, claiming that their model does not rely on parsing trees to extract ZP positions and noun phrase as resolution candidates. However, they only consider the dropped pronouns that correspond to one of the dialogue participant. As a result, they only explore a small subset of the entire ZP resolution problem, and their"
2020.acl-main.482,P11-1081,0,0.551431,"Missing"
2020.acl-main.482,D10-1086,0,0.525685,"Missing"
2020.acl-main.482,P17-1010,0,0.395405,"over previous stateof-the-art results on both tasks. the primary goal, thus we follow the first line of research using human-annotated data. Rao et al. (2015) studied zero pronoun resolution in multi-turn dialogues, claiming that their model does not rely on parsing trees to extract ZP positions and noun phrase as resolution candidates. However, they only consider the dropped pronouns that correspond to one of the dialogue participant. As a result, they only explore a small subset of the entire ZP resolution problem, and their task is closer to zero pronoun recovery. Most similar to our work, Liu et al. (2017) converted zero pronoun resolution as a machine reading comprehension task (Rajpurkar et al., 2016) in order to automatically construct a large-scale pseudo dataset for model pretraining. However, their model finetuning and evaluation with benchmark data still rely on human-annotated trees and gold zero pronoun positions. As a result, it is still uncertain what performance a model can achieve without such gold inputs. We address both issues in the joint task. Our work is inspired by the recent advances of heterogeneous multi-task learning using BERT (Devlin et al., 2019), which combines the su"
2020.acl-main.482,P19-1441,0,0.0226198,"t al., 2016) in order to automatically construct a large-scale pseudo dataset for model pretraining. However, their model finetuning and evaluation with benchmark data still rely on human-annotated trees and gold zero pronoun positions. As a result, it is still uncertain what performance a model can achieve without such gold inputs. We address both issues in the joint task. Our work is inspired by the recent advances of heterogeneous multi-task learning using BERT (Devlin et al., 2019), which combines the supervised data of several related tasks to achieve further improvements. In particular, Liu et al. (2019) utilize this framework to jointly solve GLUE tasks (Wang et al., 2019). But their experiments show that multitask learning does not help across all tasks. Our work takes a similar spirit, and our contribution is mainly on the zero pronoun tasks. In addition, we find that it helps the robustness of multi-task learning to add a common sub-task (e.g. zero pronoun detection in our case) for additional supervision and alleviating annotation variances, if such a subtask is available. 2 3 Related work Previous work considers zero pronoun resolution and recovery separately. For zero pronoun recovery,"
2020.acl-main.482,C96-2137,0,0.0827163,"Missing"
2020.acl-main.482,D16-1264,0,0.655517,"rom the supervised data of both tasks. As the result, we enjoy the benefit of more supervised training data. To improve the robustness of heterogeneous training and introduce more supervision, we introduce zero pronoun detection, a common sub-task for both ZP resolution and recovery. Zero pronoun detection is a binaryclassification task aiming to detect whether a word space has a dropped pronoun. We consider ZP recovery as a sequence labeling task, regarding whether each word space has a dropped pronoun and what type the pronoun is. ZP resolution is solved as extractive reading comprehension (Rajpurkar et al., 2016), where each word space is taken as a query and its anaphoric mentions are treated as the answers. For non-ZP spaces where there is no corresponding anaphoric mentions, we assign the sentence beginning (span [0,0]) as the answer. Experiments on two benchmarks, OntoNotes 5.01 (ZP resolution) and BaiduZhdiao (Zhang et al., 2016) (ZP recovery), show that joint modeling gives us 1.5+ absolute F1-score gains for both tasks over our very strong baselines using BERT (Devlin et al., 2019). Our overall system gives an dramatic improvement of 3.5 F1 points over previous stateof-the-art results on both t"
2020.acl-main.482,N15-1052,0,0.211097,"Missing"
2020.acl-main.482,Q19-1016,0,0.0292498,"Missing"
2020.acl-main.482,C08-1097,0,0.354952,"Missing"
2020.acl-main.482,Q19-1014,1,0.87841,"Missing"
2020.acl-main.482,W16-4615,0,0.0407466,"Missing"
2020.acl-main.482,C18-1002,0,0.192729,"Missing"
2020.acl-main.482,N16-1113,0,0.219648,"t helps the robustness of multi-task learning to add a common sub-task (e.g. zero pronoun detection in our case) for additional supervision and alleviating annotation variances, if such a subtask is available. 2 3 Related work Previous work considers zero pronoun resolution and recovery separately. For zero pronoun recovery, existing methods can be classified according to the types of annotations they use. One line of work (Yang et al., 2015, 2019) simply relies on the human annotations, solving the task as sequence labeling. The other line of work (Chung and Gildea, 2010; Xiang et al., 2013; Wang et al., 2016) mines weak supervision signals from a large bilingual parallel corpus, where the other language is non-prodrop with fewer pronoun drops. The latter requires massive training data, and the MT performance is 1 https://catalog.ldc.upenn.edu/LDC2013T19 Model As shown in Figure 2, we model ZP recovery (frec ), ZP resolution (fres ), and the auxiliary ZP detection (fdet ) task with multi-task learning, where BERT (Devlin et al., 2019) is used to represent each input sentence s1 . . . sN of N words to provide shared features. 3.1 Zero pronoun recovery ZP recovery is to restore any dropped pronouns f"
2020.acl-main.482,P13-1081,0,0.361222,"tion, we find that it helps the robustness of multi-task learning to add a common sub-task (e.g. zero pronoun detection in our case) for additional supervision and alleviating annotation variances, if such a subtask is available. 2 3 Related work Previous work considers zero pronoun resolution and recovery separately. For zero pronoun recovery, existing methods can be classified according to the types of annotations they use. One line of work (Yang et al., 2015, 2019) simply relies on the human annotations, solving the task as sequence labeling. The other line of work (Chung and Gildea, 2010; Xiang et al., 2013; Wang et al., 2016) mines weak supervision signals from a large bilingual parallel corpus, where the other language is non-prodrop with fewer pronoun drops. The latter requires massive training data, and the MT performance is 1 https://catalog.ldc.upenn.edu/LDC2013T19 Model As shown in Figure 2, we model ZP recovery (frec ), ZP resolution (fres ), and the auxiliary ZP detection (fdet ) task with multi-task learning, where BERT (Devlin et al., 2019) is used to represent each input sentence s1 . . . sN of N words to provide shared features. 3.1 Zero pronoun recovery ZP recovery is to restore an"
2020.acl-main.482,N19-1095,0,0.0207607,"Missing"
2020.acl-main.482,P15-2051,0,0.249019,"at multitask learning does not help across all tasks. Our work takes a similar spirit, and our contribution is mainly on the zero pronoun tasks. In addition, we find that it helps the robustness of multi-task learning to add a common sub-task (e.g. zero pronoun detection in our case) for additional supervision and alleviating annotation variances, if such a subtask is available. 2 3 Related work Previous work considers zero pronoun resolution and recovery separately. For zero pronoun recovery, existing methods can be classified according to the types of annotations they use. One line of work (Yang et al., 2015, 2019) simply relies on the human annotations, solving the task as sequence labeling. The other line of work (Chung and Gildea, 2010; Xiang et al., 2013; Wang et al., 2016) mines weak supervision signals from a large bilingual parallel corpus, where the other language is non-prodrop with fewer pronoun drops. The latter requires massive training data, and the MT performance is 1 https://catalog.ldc.upenn.edu/LDC2013T19 Model As shown in Figure 2, we model ZP recovery (frec ), ZP resolution (fres ), and the auxiliary ZP detection (fdet ) task with multi-task learning, where BERT (Devlin et al.,"
2020.acl-main.482,D17-1135,0,0.16016,"Missing"
2020.acl-main.524,D18-1017,0,0.0277997,"assification (Mooney and Bunescu, 2006), event detection (Popescu et al., 2011), sentiment classification (Mitchell et al., 2013), etc. NER is challenging because entity mentions are an open set and can be ambiguous in the context of a sentence. Due to relatively high cost in manual labeling, cross-domain NER has received increasing research attention. Recently, multi-task learning methods (Yang et al., 2017; Wang et al., 2018, 2019; Zhou et al., 2019; Jia et al., 2019) have achieved great success for cross-domain NER. Other methods such as fine-tuning (Rodriguez et al., 2018), share-private (Cao et al., 2018; Lin and Lu, 2018) and knowledge distill (Yang et al., 2019) also show effectivenesses for cross-domain NER. There are three main source of challenges in cross-domain NER. First, instances of the same type entities can be different across domains. For example, typical person names can include “Trump” and “Clinton” in the political news domain, but “James” and “Trout” in the sports domain. Second, different types of entities can exhibit different degrees of dissimilarities across domains. For example, a large number of location names are shared in the political news domain and the sports domai"
2020.acl-main.524,W16-2922,0,0.0143576,"iscellaneous). The Broad Twitter dataset consists of three types: P ER, L OC and O RG. BioNLP13CG mainly consists of five types, namely C HEM (simple chemical), C C (cellular component), G GP (gene and gene product), S PE (species) and C ELL (cell), BioNLP13PC mainly consists of three types: C HEM, C C and G GP. Hyperparameters. We choose NCRF++ (Yang and Zhang, 2018) for developing the models. The multi-task baselines are based on Jia et al. (2019). Our hyperparameter settings largely follow Yang et al. (2018); word embeddings for all models are initialized with PubMed 200-dimension vectors (Chiu et al., 2016) in BioNLP experiments and Accuracy Broad Twitter Size #Sentence #Entity #Sentence #Entity #Sentence #Entity #Sentence #Entity #Sentence #Entity #Sentence #Entity F-score CoNLL-2003 Entity Type P ER, L OC O RG, M ISC P ER, L OC O RG P ER, L OC O RG, M ISC C HEM, C C G GP, etc. C HEM, C C G GP, etc. P ER, L OC O RG, M ISC F-score Dataset 40 60 Iteration (b) Twitter. Figure 2: Performances of the main NER and auxiliary tasks against the total number of training iteratons. GloVe 100-dimension vectors (Pennington et al., 2014) in other experiments. All word embeddings are fine-tuned during trainin"
2020.acl-main.524,D18-1226,0,0.059571,"ey and Bunescu, 2006), event detection (Popescu et al., 2011), sentiment classification (Mitchell et al., 2013), etc. NER is challenging because entity mentions are an open set and can be ambiguous in the context of a sentence. Due to relatively high cost in manual labeling, cross-domain NER has received increasing research attention. Recently, multi-task learning methods (Yang et al., 2017; Wang et al., 2018, 2019; Zhou et al., 2019; Jia et al., 2019) have achieved great success for cross-domain NER. Other methods such as fine-tuning (Rodriguez et al., 2018), share-private (Cao et al., 2018; Lin and Lu, 2018) and knowledge distill (Yang et al., 2019) also show effectivenesses for cross-domain NER. There are three main source of challenges in cross-domain NER. First, instances of the same type entities can be different across domains. For example, typical person names can include “Trump” and “Clinton” in the political news domain, but “James” and “Trout” in the sports domain. Second, different types of entities can exhibit different degrees of dissimilarities across domains. For example, a large number of location names are shared in the political news domain and the sports domain, such as “Barcelo"
2020.acl-main.524,P18-1185,0,0.0373392,"Missing"
2020.acl-main.524,P16-1101,0,0.0313475,"= Pl exp(Ik ) (t) j=1 exp(Ij ) N m 1 XX n log(p(en t |xt )) |Dent |n=1 t=1 , (6) where [P; Q; v] are trainable parameters. The memory cell state of the C cell is updated as: ˆ(t) ) c ˆ(t) = ˆi(t) c ˆ(t) ˆ(t−1) c α + (1 − i (t) exp(Ik ) ˆ (t) = o ˆ (t) tanh(ˆ h c(t) ) (8) (t) exp(Ij ) (11) Similar to the loss of entity type prediction: Latten = − N m 1 XX n log(p(en t |xt )) |Dent |n=1 t=1 (12) While entity type prediction brings supervised information to guide the ET cells, attention scoring introduces supervision to guide the C cell. NER. This is the main task across domains. Standard CRFs (Ma and Hovy, 2016) are used. Given → − ← − → − ← − h = [ h 1 ⊕ h 1 , . . . , h m ⊕ h m ], the output probability p(y|x) over labels y=l1 , . . . , lm is: P (lt−1 ,lt ) lt exp{ t (wCRF · ht + bCRF )} , p(y|x) = P P (l0t−1 ,l0t ) l0t )} y 0 exp{ t (wCRF · ht + bCRF (7) ˆ (t) : Finally, we obtain the hidden state h (10) Attention scoring. Similar to the entity type prediction task, given the attention scores between the temporary C cell and ET cells in Equation 6: → − (t) ← −(t) → − (t) ← −(t) I(t) = [( I 1 + I 1 )/2, . . . , ( I l + I l )/2], we convert the attention scores to entity aligned distributions for xt"
2020.acl-main.524,C16-1111,0,0.102634,"(x)6=g 00 ◦ f (x)] ≈ Prx∼DS [g 0 ◦ f (x)6=g 00 ◦ f (x)] ≈ 0, which well meets the above optimization objecive. 4.1 80 0.98 0.96 0.94 0.92 0.90 0.88 0.86 100 NER Entity Prediction (right) Attention Scoring (right) 20 Table 1: Statistic of datasets. 4 80 0.98 0.96 0.94 0.92 0.90 0.88 0.86 100 Accuracy Train 15.0K 23.5K 6.3K 8.8K 4.3K 7.5K 2.5K 7.9K 3.0K 10.8K - Experiments Experimental Settings Datasets. We take six publicly available datasets for experiments, including BioNLP13PC and BioNLP13CG (N´edellec et al., 2013), CoNLL2003 English dataset (Sang and Meulder, 2003), Broad Twitter dataset (Derczynski et al., 2016), Twitter dataset (Lu et al., 2018) and CBS SciTech News dataset (Jia et al., 2019). Statistics of the datasets are shown in Table 1. In unsupervised domain adaptation experiments, 398,990 unlabeled sentences from CBS SciTech News collected by Jia et al. (2019) are used for target-domain LM training, a named entity dictionary from Web resource collected by Peng et al. (2019) is used for target-domain auxiliary tasks training. The CoNLL-2003, Twitter and CBS News have the same four types of entities, namely P ER (person), L OC (location), O RG (organization) and M ISC (miscellaneous). The Broad"
2020.acl-main.524,N19-1423,0,0.192581,"taset and the CoNLL-2003 as the source-domain dataset. These two datasets have a different entity type M ISC. In the Twitter dataset, Twitter is used as the target-domain dataset and the CoNLL-2003 as the source-domain dataset. These two datasets have the same entity types. The overall results are listed in Table 2. Target-domain only settings. In comparison with target-domain only models B I LSTM and M ULTI - 5911 Methods Crichton et al. (2017) Lu et al. (2018) Wang et al. (2019) Jia et al. (2019) B I L STM +E LMO (Peters et al., 2018) B I L STM +B IO E LMO (Peters et al., 2018) B ERT- BASE (Devlin et al., 2019) B IO B ERT- BASE (Lee et al., 2020) B I L STM M ULTI - CELL L STM M ULTI -TASK (L STM ) M ULTI -TASK (L STM )[R EPRO ]∗ M ULTI -TASK +P GN M ULTI -TASK +G RAD O URS O URS +E LMO /B IO E LMO O URS +B ERT- BASE /B IO B ERT- BASE BioNLP F1 #Params 78.90 82.48 79.86 85.61 94,605K 85.72 108M 79.24 304K 78.76 2,704K 81.06 309K 81.45 312K 81.17 4,533K 81.63 447K 83.12† 2,929K 86.65 105M 86.96†‡ 117M Datasets Broad Twitter F1 #Params 76.48 94,590K 77.28 108M 72.98 210K 72.54 641K 73.84 214K 73.82 214K 73.70 3,238K 74.12 342K 74.82† 827K 76.36 97,090K 78.43†‡ 111M Twitter F1 #Params 80.75 82.83 94,631"
2020.acl-main.524,D13-1171,0,0.0704148,"Missing"
2020.acl-main.524,W13-2001,0,0.0576615,"Missing"
2020.acl-main.524,P19-1231,0,0.0757697,"if 16: L ← L + λd Ldm + Lda 17: end for 18: Update paremeters of networks based on L. 19: end while A sentence-level negative log-likehood loss is used for training on Dner={(xn , y n )}N n=1 : Lner = − 3 1 |Dner | N X log(p(y n |xn )) (14) 3.2 Unsupervised Domain Adaptation To better leverage target-domain knowledge without target-domain NER labeled data, we conduct the auxiliary dictionary matching and language modeling tasks on target-domain raw data Tlm = {(xn )}N n=1 . Auxiliary tasks. To better extract entity knowledge from raw data, we use a pre-collected named entity dictionary De by Peng et al. (2019) to label + Tlm and obtain a set of entity words Dent , which are used to train entity prediction task and attention scoring task jointly. Language modeling. Follwing Jia et al. (2019), we use sampling softmax to compute forward LM probability pf (xt |x<t ) and backward LM probability pb (xt |x>t ), respectively:   − → 1 exp wx>t h t−1 +bxt Z   ← − 1 b p (xt |x>t ) = exp wx>t h t+1 +bxt , Z n=1 pf (xt |x<t ) = Transfer Learning The multi-cell LSTM structure above is domain agnostic, and can therefore be used for in-domain NER too. However, the main goal of the model is to transfer entity s"
2020.acl-main.524,P19-1236,1,0.848915,"1 Introduction Named entity recognition (NER) is a fundamental task in information extraction, providing necessary information for relation classification (Mooney and Bunescu, 2006), event detection (Popescu et al., 2011), sentiment classification (Mitchell et al., 2013), etc. NER is challenging because entity mentions are an open set and can be ambiguous in the context of a sentence. Due to relatively high cost in manual labeling, cross-domain NER has received increasing research attention. Recently, multi-task learning methods (Yang et al., 2017; Wang et al., 2018, 2019; Zhou et al., 2019; Jia et al., 2019) have achieved great success for cross-domain NER. Other methods such as fine-tuning (Rodriguez et al., 2018), share-private (Cao et al., 2018; Lin and Lu, 2018) and knowledge distill (Yang et al., 2019) also show effectivenesses for cross-domain NER. There are three main source of challenges in cross-domain NER. First, instances of the same type entities can be different across domains. For example, typical person names can include “Trump” and “Clinton” in the political news domain, but “James” and “Trout” in the sports domain. Second, different types of entities can exhibit different degrees"
2020.acl-main.524,D14-1162,0,0.0885975,"Missing"
2020.acl-main.524,N18-1202,0,0.269538,"s. In the Broad Twitter dataset, Broad Twitter is used as the target-domain dataset and the CoNLL-2003 as the source-domain dataset. These two datasets have a different entity type M ISC. In the Twitter dataset, Twitter is used as the target-domain dataset and the CoNLL-2003 as the source-domain dataset. These two datasets have the same entity types. The overall results are listed in Table 2. Target-domain only settings. In comparison with target-domain only models B I LSTM and M ULTI - 5911 Methods Crichton et al. (2017) Lu et al. (2018) Wang et al. (2019) Jia et al. (2019) B I L STM +E LMO (Peters et al., 2018) B I L STM +B IO E LMO (Peters et al., 2018) B ERT- BASE (Devlin et al., 2019) B IO B ERT- BASE (Lee et al., 2020) B I L STM M ULTI - CELL L STM M ULTI -TASK (L STM ) M ULTI -TASK (L STM )[R EPRO ]∗ M ULTI -TASK +P GN M ULTI -TASK +G RAD O URS O URS +E LMO /B IO E LMO O URS +B ERT- BASE /B IO B ERT- BASE BioNLP F1 #Params 78.90 82.48 79.86 85.61 94,605K 85.72 108M 79.24 304K 78.76 2,704K 81.06 309K 81.45 312K 81.17 4,533K 81.63 447K 83.12† 2,929K 86.65 105M 86.96†‡ 117M Datasets Broad Twitter F1 #Params 76.48 94,590K 77.28 108M 72.98 210K 72.54 641K 73.84 214K 73.82 214K 73.70 3,238K 74.12 342"
2020.acl-main.524,P18-4013,1,0.86656,"ry from Web resource collected by Peng et al. (2019) is used for target-domain auxiliary tasks training. The CoNLL-2003, Twitter and CBS News have the same four types of entities, namely P ER (person), L OC (location), O RG (organization) and M ISC (miscellaneous). The Broad Twitter dataset consists of three types: P ER, L OC and O RG. BioNLP13CG mainly consists of five types, namely C HEM (simple chemical), C C (cellular component), G GP (gene and gene product), S PE (species) and C ELL (cell), BioNLP13PC mainly consists of three types: C HEM, C C and G GP. Hyperparameters. We choose NCRF++ (Yang and Zhang, 2018) for developing the models. The multi-task baselines are based on Jia et al. (2019). Our hyperparameter settings largely follow Yang et al. (2018); word embeddings for all models are initialized with PubMed 200-dimension vectors (Chiu et al., 2016) in BioNLP experiments and Accuracy Broad Twitter Size #Sentence #Entity #Sentence #Entity #Sentence #Entity #Sentence #Entity #Sentence #Entity #Sentence #Entity F-score CoNLL-2003 Entity Type P ER, L OC O RG, M ISC P ER, L OC O RG P ER, L OC O RG, M ISC C HEM, C C G GP, etc. C HEM, C C G GP, etc. P ER, L OC O RG, M ISC F-score Dataset 40 60 Iterati"
2020.acl-main.524,C18-1168,0,0.0269171,"ng necessary information for relation classification (Mooney and Bunescu, 2006), event detection (Popescu et al., 2011), sentiment classification (Mitchell et al., 2013), etc. NER is challenging because entity mentions are an open set and can be ambiguous in the context of a sentence. Due to relatively high cost in manual labeling, cross-domain NER has received increasing research attention. Recently, multi-task learning methods (Yang et al., 2017; Wang et al., 2018, 2019; Zhou et al., 2019; Jia et al., 2019) have achieved great success for cross-domain NER. Other methods such as fine-tuning (Rodriguez et al., 2018), share-private (Cao et al., 2018; Lin and Lu, 2018) and knowledge distill (Yang et al., 2019) also show effectivenesses for cross-domain NER. There are three main source of challenges in cross-domain NER. First, instances of the same type entities can be different across domains. For example, typical person names can include “Trump” and “Clinton” in the political news domain, but “James” and “Trout” in the sports domain. Second, different types of entities can exhibit different degrees of dissimilarities across domains. For example, a large number of location names are shared in the political"
2020.acl-main.524,P19-1336,0,0.120262,"Missing"
2020.acl-main.524,W03-0419,0,0.257584,"Missing"
2020.acl-main.524,N18-1001,0,0.0758651,"rning methods and achieves the best results. 1 Introduction Named entity recognition (NER) is a fundamental task in information extraction, providing necessary information for relation classification (Mooney and Bunescu, 2006), event detection (Popescu et al., 2011), sentiment classification (Mitchell et al., 2013), etc. NER is challenging because entity mentions are an open set and can be ambiguous in the context of a sentence. Due to relatively high cost in manual labeling, cross-domain NER has received increasing research attention. Recently, multi-task learning methods (Yang et al., 2017; Wang et al., 2018, 2019; Zhou et al., 2019; Jia et al., 2019) have achieved great success for cross-domain NER. Other methods such as fine-tuning (Rodriguez et al., 2018), share-private (Cao et al., 2018; Lin and Lu, 2018) and knowledge distill (Yang et al., 2019) also show effectivenesses for cross-domain NER. There are three main source of challenges in cross-domain NER. First, instances of the same type entities can be different across domains. For example, typical person names can include “Trump” and “Clinton” in the political news domain, but “James” and “Trout” in the sports domain. Second, different typ"
2020.acl-main.524,D19-1429,0,0.0132801,"opescu et al., 2011), sentiment classification (Mitchell et al., 2013), etc. NER is challenging because entity mentions are an open set and can be ambiguous in the context of a sentence. Due to relatively high cost in manual labeling, cross-domain NER has received increasing research attention. Recently, multi-task learning methods (Yang et al., 2017; Wang et al., 2018, 2019; Zhou et al., 2019; Jia et al., 2019) have achieved great success for cross-domain NER. Other methods such as fine-tuning (Rodriguez et al., 2018), share-private (Cao et al., 2018; Lin and Lu, 2018) and knowledge distill (Yang et al., 2019) also show effectivenesses for cross-domain NER. There are three main source of challenges in cross-domain NER. First, instances of the same type entities can be different across domains. For example, typical person names can include “Trump” and “Clinton” in the political news domain, but “James” and “Trout” in the sports domain. Second, different types of entities can exhibit different degrees of dissimilarities across domains. For example, a large number of location names are shared in the political news domain and the sports domain, such as “Barcelona” and “Los Angeles”, but the case is ver"
2020.acl-main.524,C18-1327,1,0.94544,"orks that do not explicitly differentiate entity types (§3.4). Empirically, experiments on four fewshot and zero-shot datasets show that our method gives significantly better results compared to standard BiLSTM baselines with the same numbers of parameters. In addition, we obtain the best resutls on four cross-domain NER datasets. The code is released at https://github.com/ jiachenwestlake/Multi-Cell_LSTM. 2 Method Given a sentence x = [x1 , . . . , xm ], the vector representation wt for each word xt is the concatenation of its word embedding and the output of a character level CNN, following Yang et al. (2018). A bi-directional LSTM encoder is used to obtain sequence level features h = [h1 , . . . , hm ]. We use the forward LSTM component to explain the details in the following subsections. Finally, a CRF layer outputs the label sequence y = l1 , . . . , lm . 2.1 Baseline LSTM We adopt the standard LSTM (Graves and Schmidhuber, 2005) for the baseline. At each time step t (t ∈ [1, ..., m]), the baseline calculates a current hidden vector h(t) based on a memory cell c(t) . In particular, a set of input gate i(t) , output gate o(t) and forget gate f (t) are calculated as follows:    i(t) σ   o(t)"
2020.acl-main.565,D18-1547,0,0.188613,"Missing"
2020.acl-main.565,W17-5506,0,0.364581,"ringer street no traffic 200 Alester Ave 2 miles gas station Valero road block nearby 899 Ames Ct 5 miles hospital Stanford Childrens Health moderate traffic 481 Amaranta Ave 1 miles parking garage Palo Alto Garage R moderate traffic Driver Address to the gas station. Car Valero is located at 200 Alester Ave. Driver OK , please give me directions via a route that avoids all heavy traffic. Car Since there is a road block nearby, I found another route for you and I sent it on your screen. Figure 1: Example of a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver"
2020.acl-main.565,E17-2075,0,0.249266,"ere is a road block nearby, I found another route for you and I sent it on your screen. Figure 1: Example of a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver’s query about the “gas station”, the end-to-end dialogue system directly generates system response given the query and a corresponding knowledge base (KB). Email corresponding. Distance POI type 5672 barringer street Dialogue Introduction ∗ Address Though achieving promising performance, endto-end models rely on a considerable amount of labeled data, which limits their usefulness for new and extended dom"
2020.acl-main.565,N19-1375,0,0.421861,"igure 1: Example of a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver’s query about the “gas station”, the end-to-end dialogue system directly generates system response given the query and a corresponding knowledge base (KB). Email corresponding. Distance POI type 5672 barringer street Dialogue Introduction ∗ Address Though achieving promising performance, endto-end models rely on a considerable amount of labeled data, which limits their usefulness for new and extended domains. In practice, we cannot collect rich datasets for each new domain. Hence, it is"
2020.acl-main.565,D18-1498,0,0.0614214,"Missing"
2020.acl-main.565,P82-1020,0,0.80062,"Missing"
2020.acl-main.565,P18-1133,0,0.108761,"et al. (2019b), We hired human experts and asked them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5. Results are illustrated in Table 4. We can see that our framework outperforms GLMP on all metrics, which is consistent with the automatic evaluation. 4 Related Work Existing end-to-end task-oriented systems can be classified into two main classes. A series of work trains a single model on the mixed multi-domain dataset. Eric et al. (2017) augments the vocabulary distribution by concatenating KB attention to generatge entities. Lei et al. (2018) first integrates track dialogue believes in end-to-end task-oriented dialog. Madotto et al. (2018) combines end-toend memory network (Sukhbaatar et al., 2015) into sequence generation. Gangi Reddy et al. (2019) proposes a multi-level memory architecture which first addresses queries, followed by results and finally each key-value pair within a result. Wu et al. (2019a) proposes a global-to-locally pointer mechanism to query the knowledge base. Compared with their models, our framework can not only explicitly utilize domain-specific knowledge but also consider different relevance between each"
2020.acl-main.565,P17-1001,0,0.0269491,"red and final domain-specific feature: d f shprivate : (hsdec,t , hdec,t ) → hfdec,t . (17) Finally, we denote the dynamic fusion function |D| i as dynamic(hsdec,t , {hddec,t }i=1 ). Similar to Sec0 tion 2.2, we replace [hdec,t , hdec,t ] in Eq. 8 with 0 [hfdec,t , hfdec,t ]. The other components are kept the same as the shared-private encoder-decoder framework. Train 2,425 1,839 Dev 302 117 Test 304 141 Table 1: Statistics of datasets. Adversarial Training To encourage the model to learn domain-shared features, we apply adversarial learning on the shared encoder and decoder module. Following Liu et al. (2017), a gradient reversal layer (Ganin and Lempitsky, 2014) is introduced after the domain classifier layer. The adversarial training loss is denoted as Ladv . We follow Qin et al. (2019a) and the final loss function of our Dynamic fusion network is defined as: L = γb Lbasic + γm Lmoe + γa Ladv , (18) where Lbasic keep the same as GLMP (Wu et al., 2019a), γb , γm and γa are hyper-parameters. More details about Lbasic and Ladv can be found in appendix. 3 where θs represents the parameters of encoderm represents the parameters decoder model, θdec of the MoE module (Eq. 15) in the decoder and ei ∈ {0"
2020.acl-main.565,P18-1136,0,0.411253,"route for you and I sent it on your screen. Figure 1: Example of a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver’s query about the “gas station”, the end-to-end dialogue system directly generates system response given the query and a corresponding knowledge base (KB). Email corresponding. Distance POI type 5672 barringer street Dialogue Introduction ∗ Address Though achieving promising performance, endto-end models rely on a considerable amount of labeled data, which limits their usefulness for new and extended domains. In practice, we cannot collect rich"
2020.acl-main.565,D19-1214,1,0.822917,"a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver’s query about the “gas station”, the end-to-end dialogue system directly generates system response given the query and a corresponding knowledge base (KB). Email corresponding. Distance POI type 5672 barringer street Dialogue Introduction ∗ Address Though achieving promising performance, endto-end models rely on a considerable amount of labeled data, which limits their usefulness for new and extended domains. In practice, we cannot collect rich datasets for each new domain. Hence, it is important to consi"
2020.acl-main.565,D19-1013,1,0.882709,"Missing"
2020.acl-main.565,C18-1320,1,0.527232,"ent it on your screen. Figure 1: Example of a task-oriented dialogue that incorporates a knowledge base (KB) from the SMD dataset (Eric et al., 2017). Words with the same color refers queried entity from the KB. Better viewed in color. Task-oriented dialogue systems (Young et al., 2013) help users to achieve specific goals such as restaurant reservation or navigation inquiry. In recent years, end-to-end methods in the literature usually take the sequence-to-sequence (Seq2Seq) model to generate a response from a dialogue history (Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018; Wen et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2019b; Wu et al., 2019a). Taking the dialogue in Figure 1 as an example, to answer the driver’s query about the “gas station”, the end-to-end dialogue system directly generates system response given the query and a corresponding knowledge base (KB). Email corresponding. Distance POI type 5672 barringer street Dialogue Introduction ∗ Address Though achieving promising performance, endto-end models rely on a considerable amount of labeled data, which limits their usefulness for new and extended domains. In practice, we cannot collect rich datasets for each"
2020.acl-main.565,N18-1138,0,0.0548659,"Missing"
2020.acl-main.565,P18-1135,0,0.402986,"ynamic Domain-Specific Fusion Shared Module A Module B Module C Module Shared Module Mixed Data Domain A Domain B Domain C Domain A Mixed Data Domain B A Module B Module C Module Domain C Mixed Data (a) (c) (b) (d) Figure 2: Methods for multi-domain dialogue. Previous work either trains a general model on mixed multi-domain mixed datasets (a), or on each domain separately (b). The basic shared-private framework is shown (c). Our proposed extension with dynamic fusion mechanism is shown (d). to incorporate domain-shared and domain-private features is shared-private framework (Liu et al., 2017; Zhong et al., 2018; Wu et al., 2019b). Shown in Figure 2(c), it includes a shared module to capture domain-shared feature and a private module for each domain. The method explicitly differentiates shared and private knowledge. However, this framework still has two issues: (1) given a new domain with extremely little data, the private module can fail to effectively extract the corresponding domain knowledge. (2) the framework neglects the fine-grained relevance across certain subsets of domains. (e.g. schedule domain is more relevant to the navigation than to the weather domain.) To address the above issues, we"
2020.acl-main.591,P16-1231,0,0.0157421,"scenario, recent analysis also reveals that state-of-the-art sequential neural language models still fail to learn certain long-range syntactic dependencies (Kuncoro et al., 2018). Thus it is an interesting problem to explore the relation between language models and syntax and investigate whether syntax can be integrated to enhance neural language models. To this end, two main lines of work have been investigated, namely transition-based and distancebased methods, respectively. The former strand of work has sought to jointly train a transition-based parser (Nivre, 2008; Zhang and Nivre, 2011; Andor et al., 2016) with a language model using a linearized structured sentence. For example, recurrent neural network grammars (RNNGs) model the joint probability of both words and trees by training a generative, top-down parser (Dyer et al., 2016; Cheng et al., 2017). Subsequent work (Kim et al., 2019b) has developed an unsupervised variant of RNNGs based on an expectation maximization algorithm, which enables the system to be used as a language model without access to parser data. The second strand of work designs language models that are constrained using syntactic constituents induced using the notion of s"
2020.acl-main.591,D16-1053,0,0.0583519,"Missing"
2020.acl-main.591,P17-2019,0,0.118603,"uage models and syntax and investigate whether syntax can be integrated to enhance neural language models. To this end, two main lines of work have been investigated, namely transition-based and distancebased methods, respectively. The former strand of work has sought to jointly train a transition-based parser (Nivre, 2008; Zhang and Nivre, 2011; Andor et al., 2016) with a language model using a linearized structured sentence. For example, recurrent neural network grammars (RNNGs) model the joint probability of both words and trees by training a generative, top-down parser (Dyer et al., 2016; Cheng et al., 2017). Subsequent work (Kim et al., 2019b) has developed an unsupervised variant of RNNGs based on an expectation maximization algorithm, which enables the system to be used as a language model without access to parser data. The second strand of work designs language models that are constrained using syntactic constituents induced using the notion of syntactic distance (Shen et al., 2017, 2018). The distances are a sequence of scalars between consecutive words, which are higher when there is a higher level of constituent boundary between the corresponding pair of words. While aligning nicely with t"
2020.acl-main.591,P17-1076,0,0.0251763,"nce an ultrametric (Holly, 2001; Wu et al., 1999), a concept which is important in the theory of hierarchical agglomerative clustering (Johnson, 1967) and was first explored in a linguistic setting by Levelt (1974). 3.2 Converting Grammar Trees to Syntactic Distances To integrate treebank trees into ON-LSTM, we need to first convert syntactic trees into a representation based on syntactic distances. Since the original grammar trees are not necessarily binary, 6613 we first split non-binary nodes by adding sentinel intermediate nodes to form a right-branched binary tree, following the steps in Stern et al. (2017). Now for a binary tree with N leaf nodes, we have N − 1 non-leaf nodes that correspond to the N − 1 slots between each of the adjacent word pairs, each of which are assigned a syntactic distance (Figure 2). The binary tree can thus be represented as a sequence of distances d1 , d2 , . . . , dN −1 . The conversion from binary tree to syntactic distances thus translates to the assigning of a distance value for each of the N − 1 non-leaf nodes in the tree. This is achieved in a bottom-up process. We first initialize a distance value of 1 at all of the leaf nodes, and then compute the syntactic d"
2020.acl-main.591,D18-1489,0,0.0484441,"Missing"
2020.acl-main.591,N01-1029,0,0.34497,"Missing"
2020.acl-main.591,P11-2033,1,0.7346,"nformation. On another scenario, recent analysis also reveals that state-of-the-art sequential neural language models still fail to learn certain long-range syntactic dependencies (Kuncoro et al., 2018). Thus it is an interesting problem to explore the relation between language models and syntax and investigate whether syntax can be integrated to enhance neural language models. To this end, two main lines of work have been investigated, namely transition-based and distancebased methods, respectively. The former strand of work has sought to jointly train a transition-based parser (Nivre, 2008; Zhang and Nivre, 2011; Andor et al., 2016) with a language model using a linearized structured sentence. For example, recurrent neural network grammars (RNNGs) model the joint probability of both words and trees by training a generative, top-down parser (Dyer et al., 2016; Cheng et al., 2017). Subsequent work (Kim et al., 2019b) has developed an unsupervised variant of RNNGs based on an expectation maximization algorithm, which enables the system to be used as a language model without access to parser data. The second strand of work designs language models that are constrained using syntactic constituents induced"
2020.acl-main.591,Q18-1019,0,0.0303674,"ture prediction trees is 61.3 in unbiased algorithm. This indicates that our model brings more benefits on the LM side rather than the parsing side. 6 Ablation Study Layer used for supervision Table 4 (Top) shows the performances where the supervised signal is injected into different layers. Although injecting syntax into the last layer gives the best syntactic distance for grammar induction, it fails to achieve a similar improvement on perplexity. This suggests that a better syntactic structure may not always lead to a better language model. The observation is consistent with prior research (Williams et al., 2018). Figure 4: Accuracy breakdown w.r.t. constituent height in unbiased trees derived from the syntactic task distances in our model (top) and the language modeling distances (bottom). A constituent is considered as correct if its boundaries correspond to a true constituent. The constituents’ heights are those in the predicted tree. Since constituents that represent the whole sentence always have correct boundaries, they are excluded from the calculation. work. Parsing performance Our models give worse unlabeled parsing performance compared to transition-based methods. In particular, Kim et al. ("
2020.acl-main.591,P02-1025,0,0.209594,"e, 2011; Huddleston and Pullum, 2002; Adger, 2003; Bresnan, 2001; Chomsky, 1995; Sag et al., 2003). Using syntactic information for the language modeling task has been a popular research topic since the 1990s. Early efforts included various approaches that attempted to incorporate shallow syntactic information such as POS tags (Heeman and Allen, 1997; Srinivas, 1996) as well as a more complete structures (Wright et al., 1994; Jurafsky et al., 1995). Most of such work falls under the topic of structured language modeling (Chelba and ∗ Equal contribution. Jelinek, 2000; Van Uytsel et al., 2001; Xu et al., 2002). With the resurgence of neural network approaches, sequential, large-scale neural language models have been shown to significantly outperform traditional language models (Merity et al., 2017; Yang et al., 2018) without using syntactic structural information. On another scenario, recent analysis also reveals that state-of-the-art sequential neural language models still fail to learn certain long-range syntactic dependencies (Kuncoro et al., 2018). Thus it is an interesting problem to explore the relation between language models and syntax and investigate whether syntax can be integrated to enh"
2020.acl-main.591,W03-1021,0,0.199839,"Missing"
2020.acl-main.591,J93-2004,0,\N,Missing
2020.acl-main.591,J08-4003,0,\N,Missing
2020.acl-main.591,N16-1024,0,\N,Missing
2020.acl-main.591,P18-1132,0,\N,Missing
2020.acl-main.591,W18-5452,0,\N,Missing
2020.acl-main.591,D18-1544,0,\N,Missing
2020.acl-main.591,P19-1228,0,\N,Missing
2020.acl-main.591,P19-1337,0,\N,Missing
2020.acl-main.609,J08-1001,0,0.0779341,"ibly due to the word “if” in the second sentence, which is one indicator for the After relation. When the syntactic information is encoded by the GAT encoder, the GAT-enc+dec model can learn the fined-grained dependency reduced by the word “if”, and thus is able to obtain the accurate relation of the two sentences (i.e., Conti.) 6 Related work Discourse parsing is one important topic in the NLP. There are several main types of discourse parsing tasks in the literature, including rhetorical structure theory (RST; MANN and Thompson, 1988) based parsing, centering theory (CT; Grosz et al., 1995; Barzilay and Lapata, 2008) based parsing and DRT based parsing in this study. Discourse Representation Theory (DRT) based parsing is a relatively classic, yet not fully researched semantic analysis task because of its complexity. Le and Zuidema (2012) present the first work of a data-driven DRT parser, using a graphbased representation of DRT structures. Recently, van Noord et al. (2018b) apply the idea of neural machine translation for graph-based DRT parsing, achieving impressing performance. These studies only focus on sentence-level DRT representations, as the complexity would increase much at the paragraph level."
2020.acl-main.609,D17-1209,0,0.0567964,"Missing"
2020.acl-main.609,D14-1082,0,0.00847887,"hmarks, AVGsent and AVGword denote the average number of sentences and words per document, respectively. shared linear transformation and LeakyReLU is a non-linearity activation function. 4.2 GAT for the Encoder On the encoder side, we equip the inputs with dependency syntax structures, which have been demonstrated helpful for closely-related tasks such as RST discourse parsing. A GAT module is used to represent the encoder output as mentioned in Section 4.1. We transform the document into a dependency graph represented by a undirected adjacent matrix using an off-the shelf dependency parser (Chen and Manning, 2014). The hidden states of each node is updated with a multi-layer GAT network on the adjacent matrix A: H g-enc = G ATenc (H enc ⊕ E syn , A; W ), (9) where E syn is the embedding outputs of the syntactic labels in the dependency tree. The learned representation H g-enc is used to substitute the original H enc for predictions. 4.3 GAT for the Decoder We further enhance the baseline model by exploiting the partial output after skeleton prediction step is finished. On one hand, the skeleton structures can guide for DRU parsing. On the other hand, the joint skeleton and DRU parsing can further help"
2020.acl-main.609,P14-1048,0,0.0291782,"lifies graphs into trees. There are two existing papers in this line. Liu et al. (2018) are the first to work on DRTS parsing, who propose an end-to-end sequence-to-sequence model for the task. Further, Liu et al. (2019) improve the model by suggesting several effective strategies including supervised attention, copying from alignments, and constraint-based inference. In this work, we improve DRTS parsing instead of Liu et al. (2019) with two types of structure information. Syntax information has been widely exploited for NLP tasks. Seminal work exploits discrete features designed by experts (Feng and Hirst, 2014; Heilman and Sagae, 2015). Recently, a range of neural modules have been proposed to encode syntax, such as Tree-LSTM (Tai et al., 2015; Zhu et al.; Teng and Zhang, 2016), Tree-CNN (Roy et al., 2020) and the recently proposed implicit approaches (Yin et al., 2018; Zhang et al., 2019). Syntax has been demonstrated effective for RST based discourse parsing as well (Yu et al., 2018). Our work is to build a syntax tree-aware model and we are the first to use syntax for DRT based discourse parsing. GNN has received increasing interests for its strong capability of encoding structural information ("
2020.acl-main.609,J95-2003,0,0.539379,"sleading may be possibly due to the word “if” in the second sentence, which is one indicator for the After relation. When the syntactic information is encoded by the GAT encoder, the GAT-enc+dec model can learn the fined-grained dependency reduced by the word “if”, and thus is able to obtain the accurate relation of the two sentences (i.e., Conti.) 6 Related work Discourse parsing is one important topic in the NLP. There are several main types of discourse parsing tasks in the literature, including rhetorical structure theory (RST; MANN and Thompson, 1988) based parsing, centering theory (CT; Grosz et al., 1995; Barzilay and Lapata, 2008) based parsing and DRT based parsing in this study. Discourse Representation Theory (DRT) based parsing is a relatively classic, yet not fully researched semantic analysis task because of its complexity. Le and Zuidema (2012) present the first work of a data-driven DRT parser, using a graphbased representation of DRT structures. Recently, van Noord et al. (2018b) apply the idea of neural machine translation for graph-based DRT parsing, achieving impressing performance. These studies only focus on sentence-level DRT representations, as the complexity would increase m"
2020.acl-main.609,D19-1549,0,0.0816607,"2018; Li et al., 2015). For decoding, the skeleton structure of DRTS can be also beneficial for our task. As a two-phase decoding strategy is exploited, the skeleton tree from the first phase could be helpful for DRU parsing of the second phase. We propose to improve DRTS parsing by making use of the above structure information, modeling dependency-based syntax of the input sentences as well as the skeleton structure to enhance the baseline model of Liu et al. (2019) using Graph Attention Network (GAT) (Veliˇckovi´c et al., 2018), which has been demonstrated effective for tree/graph encoding (Huang and Carley, 2019; Linmei et al., 2019). In particular, we first derive dependency tree structures for each sentence in a paragraph from the Stanford Parser, and then encode them directly via one GAT module, which are fed as inputs for decoding. Second, after the first-state skeleton parsing is finished, we encode the skeleton structures by another GAT module, feeding the outputs for DRU parsing. Following Liu et al. (2019), we conduct experiments on the Groningen Meaning Bank (GMB) dataset. Results show that structural information is highly useful for our task, bring a significantly better performance over th"
2020.acl-main.609,N19-1075,0,0.0660953,"Missing"
2020.acl-main.609,C12-1094,0,0.240065,"rd “if”, and thus is able to obtain the accurate relation of the two sentences (i.e., Conti.) 6 Related work Discourse parsing is one important topic in the NLP. There are several main types of discourse parsing tasks in the literature, including rhetorical structure theory (RST; MANN and Thompson, 1988) based parsing, centering theory (CT; Grosz et al., 1995; Barzilay and Lapata, 2008) based parsing and DRT based parsing in this study. Discourse Representation Theory (DRT) based parsing is a relatively classic, yet not fully researched semantic analysis task because of its complexity. Le and Zuidema (2012) present the first work of a data-driven DRT parser, using a graphbased representation of DRT structures. Recently, van Noord et al. (2018b) apply the idea of neural machine translation for graph-based DRT parsing, achieving impressing performance. These studies only focus on sentence-level DRT representations, as the complexity would increase much at the paragraph level. In contrast, we investigate the paragraph level DRT parsing. DRTS parsing simplifies graphs into trees. There are two existing papers in this line. Liu et al. (2018) are the first to work on DRTS parsing, who propose an end-t"
2020.acl-main.609,P15-1107,0,0.0646558,"Missing"
2020.acl-main.609,D18-1262,0,0.052862,"Missing"
2020.acl-main.609,D19-1488,0,0.15692,"For decoding, the skeleton structure of DRTS can be also beneficial for our task. As a two-phase decoding strategy is exploited, the skeleton tree from the first phase could be helpful for DRU parsing of the second phase. We propose to improve DRTS parsing by making use of the above structure information, modeling dependency-based syntax of the input sentences as well as the skeleton structure to enhance the baseline model of Liu et al. (2019) using Graph Attention Network (GAT) (Veliˇckovi´c et al., 2018), which has been demonstrated effective for tree/graph encoding (Huang and Carley, 2019; Linmei et al., 2019). In particular, we first derive dependency tree structures for each sentence in a paragraph from the Stanford Parser, and then encode them directly via one GAT module, which are fed as inputs for decoding. Second, after the first-state skeleton parsing is finished, we encode the skeleton structures by another GAT module, feeding the outputs for DRU parsing. Following Liu et al. (2019), we conduct experiments on the Groningen Meaning Bank (GMB) dataset. Results show that structural information is highly useful for our task, bring a significantly better performance over the baseline. In particu"
2020.acl-main.609,P18-1040,1,0.887824,"TS for a clause in a document: “The letterx4 warns Jewish womenx16 that they will suffer if they date Arab men. ” p4 Introduction Discourse representation tree structure (DRTS) is a form of discourse structure based on Discourse Representation Theory of Kamp and Reyle (1993), a popular theory of meaning representation (Kamp, 1981; Asher, 1993; Asher and Lascarides, 2003). It is designed to account for a variety of linguistic phenomena, including the interpretation of pronouns and temporal expressions within and across sentences. Correspondingly, as one type of discourse parsing, DRTS parsing (Liu et al., 2018) can be helpful for paragraph or document-level text understanding by converting DRS to tree-style DRTS. (Liu et al., 2019). Figure 1 shows an example of DRTS, where the leaf nodes are discourse representation units (DRUs), upon which a discourse tree structure built. In particular, a DRU consists of several individual tuples, where each tuple denotes a relation inside the DRU. For example, there is a relationship “That” between the specific entity x16 and a proposition p4 . The relationships between the DRUs are organized by a tree skeleton, which includes three types of nodes: the S(DRS) nod"
2020.acl-main.609,P19-1629,1,0.139836,"uction Discourse representation tree structure (DRTS) is a form of discourse structure based on Discourse Representation Theory of Kamp and Reyle (1993), a popular theory of meaning representation (Kamp, 1981; Asher, 1993; Asher and Lascarides, 2003). It is designed to account for a variety of linguistic phenomena, including the interpretation of pronouns and temporal expressions within and across sentences. Correspondingly, as one type of discourse parsing, DRTS parsing (Liu et al., 2018) can be helpful for paragraph or document-level text understanding by converting DRS to tree-style DRTS. (Liu et al., 2019). Figure 1 shows an example of DRTS, where the leaf nodes are discourse representation units (DRUs), upon which a discourse tree structure built. In particular, a DRU consists of several individual tuples, where each tuple denotes a relation inside the DRU. For example, there is a relationship “That” between the specific entity x16 and a proposition p4 . The relationships between the DRUs are organized by a tree skeleton, which includes three types of nodes: the S(DRS) nodes to introduce DRU, the relation nodes for inter-DRU relationship, and the variable nodes, which are used to define S(DRS)"
2020.acl-main.609,L18-1267,0,0.0878823,"Missing"
2020.acl-main.609,Q18-1043,0,0.133286,"Missing"
2020.acl-main.609,P02-1040,0,0.112337,"o obtain the F1-scores of our results compared with the gold-standard clause form. Note that C OUNTER is computationally expensive, requiring more than 50 hours for the entire test dataset by using more than 100 threads. To facilitate development and analysis experiments, we suggest three alternatives for evaluation particularly for development experiments: (10) where E skt is the embedding outputs of the node labels in the generated skeleton tree, and the global skeleton-aware representation H g-skt is used instead of the original H skt for future predictions. 6822 (1) BLEU: a standard BLEU (Papineni et al., 2002) value is adopted as the metric to evaluate the resulting node sequence against the gold-standard output, since we model the task as a sequence-to-sequence task. (2) Skeleton: The bracket scoring method of constituent parsing is exploited to evaluate the skeleton performance, by regarding terminal DRU nodes as words in comparison with a constituent tree.3 3 https://nlp.cs.nyu.edu/evalb/ BLEU 50 Final -syntax -skelt 50.04 49.64 BLEU 50 Baseline +GAT-encoder +GAT-decoder +GAT-enc+dec 49 49.73 Model Head=1 Head=2 Head=3 Head=4 Head=5 50.04 49.64 48 49.11 49 (a) structure labels 47 46.83 Model lay"
2020.acl-main.609,P18-1150,1,0.789795,"de syntax, such as Tree-LSTM (Tai et al., 2015; Zhu et al.; Teng and Zhang, 2016), Tree-CNN (Roy et al., 2020) and the recently proposed implicit approaches (Yin et al., 2018; Zhang et al., 2019). Syntax has been demonstrated effective for RST based discourse parsing as well (Yu et al., 2018). Our work is to build a syntax tree-aware model and we are the first to use syntax for DRT based discourse parsing. GNN has received increasing interests for its strong capability of encoding structural information (Kipf and Welling, 2016; Bastings et al., 2017; Zhang et al., 2018; Zhang and Zhang, 2019; Song et al., 2018). GAT is one representative model, which demonstrates success in a number of NLP tasks (Huang and Carley, 2019; Linmei et al., 2019). In this work, we exploit GAT to represent treestructural information for DRTS parsing. 7 Conclusion We investigated the representation of structural information for discourse representation tree structure parsing, showing that a graph neural network can bring significant improvements. In particular, we use GAT for representing syntax in encoding, and representing a structural backbone for decoding. Experiments on the standard GMB dataset show that our method is"
2020.acl-main.609,P15-1150,0,0.150094,"Missing"
2020.acl-main.609,P19-1342,1,0.848305,"e been proposed to encode syntax, such as Tree-LSTM (Tai et al., 2015; Zhu et al.; Teng and Zhang, 2016), Tree-CNN (Roy et al., 2020) and the recently proposed implicit approaches (Yin et al., 2018; Zhang et al., 2019). Syntax has been demonstrated effective for RST based discourse parsing as well (Yu et al., 2018). Our work is to build a syntax tree-aware model and we are the first to use syntax for DRT based discourse parsing. GNN has received increasing interests for its strong capability of encoding structural information (Kipf and Welling, 2016; Bastings et al., 2017; Zhang et al., 2018; Zhang and Zhang, 2019; Song et al., 2018). GAT is one representative model, which demonstrates success in a number of NLP tasks (Huang and Carley, 2019; Linmei et al., 2019). In this work, we exploit GAT to represent treestructural information for DRTS parsing. 7 Conclusion We investigated the representation of structural information for discourse representation tree structure parsing, showing that a graph neural network can bring significant improvements. In particular, we use GAT for representing syntax in encoding, and representing a structural backbone for decoding. Experiments on the standard GMB dataset show"
2020.acl-main.609,P18-1030,1,0.854939,"f neural modules have been proposed to encode syntax, such as Tree-LSTM (Tai et al., 2015; Zhu et al.; Teng and Zhang, 2016), Tree-CNN (Roy et al., 2020) and the recently proposed implicit approaches (Yin et al., 2018; Zhang et al., 2019). Syntax has been demonstrated effective for RST based discourse parsing as well (Yu et al., 2018). Our work is to build a syntax tree-aware model and we are the first to use syntax for DRT based discourse parsing. GNN has received increasing interests for its strong capability of encoding structural information (Kipf and Welling, 2016; Bastings et al., 2017; Zhang et al., 2018; Zhang and Zhang, 2019; Song et al., 2018). GAT is one representative model, which demonstrates success in a number of NLP tasks (Huang and Carley, 2019; Linmei et al., 2019). In this work, we exploit GAT to represent treestructural information for DRTS parsing. 7 Conclusion We investigated the representation of structural information for discourse representation tree structure parsing, showing that a graph neural network can bring significant improvements. In particular, we use GAT for representing syntax in encoding, and representing a structural backbone for decoding. Experiments on the st"
2020.acl-main.609,P18-1070,0,0.0273566,"ve strategies including supervised attention, copying from alignments, and constraint-based inference. In this work, we improve DRTS parsing instead of Liu et al. (2019) with two types of structure information. Syntax information has been widely exploited for NLP tasks. Seminal work exploits discrete features designed by experts (Feng and Hirst, 2014; Heilman and Sagae, 2015). Recently, a range of neural modules have been proposed to encode syntax, such as Tree-LSTM (Tai et al., 2015; Zhu et al.; Teng and Zhang, 2016), Tree-CNN (Roy et al., 2020) and the recently proposed implicit approaches (Yin et al., 2018; Zhang et al., 2019). Syntax has been demonstrated effective for RST based discourse parsing as well (Yu et al., 2018). Our work is to build a syntax tree-aware model and we are the first to use syntax for DRT based discourse parsing. GNN has received increasing interests for its strong capability of encoding structural information (Kipf and Welling, 2016; Bastings et al., 2017; Zhang et al., 2018; Zhang and Zhang, 2019; Song et al., 2018). GAT is one representative model, which demonstrates success in a number of NLP tasks (Huang and Carley, 2019; Linmei et al., 2019). In this work, we explo"
2020.acl-main.609,C18-1047,1,0.935855,"oducing, as illustrated by Figure 1. Although highly effective, the above model ig6818 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6818–6828 c July 5 - 10, 2020. 2020 Association for Computational Linguistics nores some useful structure information in both the encoder and the decoder, which can be potentially useful for our task. Specifically, for encoding, syntax-based tree structure information has been demonstrated effective for a number of NLP tasks (Kasai et al., 2019; Li et al., 2018), including several other types of discourse parsing (Yu et al., 2018; Li et al., 2015). For decoding, the skeleton structure of DRTS can be also beneficial for our task. As a two-phase decoding strategy is exploited, the skeleton tree from the first phase could be helpful for DRU parsing of the second phase. We propose to improve DRTS parsing by making use of the above structure information, modeling dependency-based syntax of the input sentences as well as the skeleton structure to enhance the baseline model of Liu et al. (2019) using Graph Attention Network (GAT) (Veliˇckovi´c et al., 2018), which has been demonstrated effective for tree/graph encoding (Huan"
2020.acl-main.609,P19-1457,1,0.82088,"uding supervised attention, copying from alignments, and constraint-based inference. In this work, we improve DRTS parsing instead of Liu et al. (2019) with two types of structure information. Syntax information has been widely exploited for NLP tasks. Seminal work exploits discrete features designed by experts (Feng and Hirst, 2014; Heilman and Sagae, 2015). Recently, a range of neural modules have been proposed to encode syntax, such as Tree-LSTM (Tai et al., 2015; Zhu et al.; Teng and Zhang, 2016), Tree-CNN (Roy et al., 2020) and the recently proposed implicit approaches (Yin et al., 2018; Zhang et al., 2019). Syntax has been demonstrated effective for RST based discourse parsing as well (Yu et al., 2018). Our work is to build a syntax tree-aware model and we are the first to use syntax for DRT based discourse parsing. GNN has received increasing interests for its strong capability of encoding structural information (Kipf and Welling, 2016; Bastings et al., 2017; Zhang et al., 2018; Zhang and Zhang, 2019; Song et al., 2018). GAT is one representative model, which demonstrates success in a number of NLP tasks (Huang and Carley, 2019; Linmei et al., 2019). In this work, we exploit GAT to represent t"
2020.acl-main.712,P17-2021,0,0.0210526,"lignments, before training our model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for training, so no extra error propagation (for structure prediction) can be introduced. Conversely, their models generate trees together with target sentences, thus extra efforts (Wu et al., 2017) are introduced to alleviate error propagation. Finally, there exist tra"
2020.acl-main.712,P19-1080,0,0.0229762,"e girl wants the boy to go”, which conveys an opposite meaning to the AMR graph. In particular, this can be very likely if “the girl wants” appears much more frequent than “the boy wants” in the training corpus. This is a very important issue, because of its wide existence across many neural graph-to-text 7987 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7987–7998 c July 5 - 10, 2020. 2020 Association for Computational Linguistics generation models, hindering the usability of these models for real-world applications (Duˇsek et al., 2018, 2019; Balakrishnan et al., 2019). A potential solution for this issue is improving the training signal to enhance preserving of structural information. However, little work has been done to explore this direction so far, probably because designing such signals is non-trivial. As a first step towards this goal, we propose to enrich the training signal with additional autoencoding losses (Rei, 2017). Standard autoencoding for graph-to-sequence tasks requires reconstructing (parsing into) input graphs, while the parsing algorithm for one type of graphs (such as knowledge graphs) may not generalize to other graph types or may no"
2020.acl-main.712,W13-2322,0,0.0245618,"veness of our approach over a state-of-the-art baseline. Our code is available at http://github.com/ Soistesimmer/AMR-multiview. 1 boy ARG2 ARG1 eat-01 Above the Veil ARG0 followedBy lunch precededBy girl mod (a) beautiful Into Battle Aenir (b) Figure 1: (a) An AMR graph meaning “The boy wants the beautiful girl to eat lunch with him.”, and (b) A knowledge graph carrying the meaning “Above the Veil is an Australian novel and the sequel to Aenir. It was followed by Into the Battle.” Many text generation tasks take graph structures as their inputs, such as Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Knowledge Graph (KG) and database tables. For example, as shown in Figure 1(a), AMR-to-text generation is to generate a sentence that preserves the meaning of an input AMR graph, which is composed by a set of concepts (such as “boy” and “want-01”) and their relations (such as “:ARG0” and “:ARG1”). Similarly, as shown in Figure 1(b), KG-to-text generation is to produce a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks main"
2020.acl-main.712,P18-1026,0,0.0217616,"hown in Figure 1(b), KG-to-text generation is to produce a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to prod"
2020.acl-main.712,N19-1223,0,0.0333895,"ormation of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) exte"
2020.acl-main.712,N19-1366,0,0.0480077,"le relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) extended the reconstruction l"
2020.acl-main.712,W14-3348,0,0.0263306,"One major merit for Loss 2 is the generality, as node-to-word alignments may not be easily obtained, especially for multi-lingual tasks. 4.4 Training with Autoencoding Losses The final training signal with both proposed autoencoding losses is formalized as: 7991 lf inal = lbase + ↵lauto1 + lauto2 , (16) where ↵ and are coefficients for our proposed losses. Both coefficient values are selected by a development experiment. 5 Experiments We study the effectiveness of our autoencoding training framework on AMR-to-text generation and KG-to-text generation. BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014) scores are reported for comparison. Following previous work, we use the multi-bleu.perl from Moses2 for BLEU evaluation. 5.1 Data AMR datasets3 We take LDC2015E86 that contains 16,833, 1,368 and 1,371 instances for training, development and testing, respectively. Each instance contains a sentence and an AMR graph. Following previous work, we use a standard AMR simplifier (Konstas et al., 2017) to preprocess our AMR graphs, and take the PTB-based Stanford tokenizer4 to tokenize the sentences. The node-toword alignments are produced by the ISI aligner (Pourdamghani et al., 2014). We use this da"
2020.acl-main.712,W19-8652,0,0.0315152,"Missing"
2020.acl-main.712,W18-6539,0,0.033398,"Missing"
2020.acl-main.712,S16-1186,0,0.0222153,"to represent the whole-entity information in the sentence. Taking the edge “followedBy” in Figure 1(b) as an example, we first ground it onto the target sentence to connect words “Above” and “Into”. Next, we create edges with label “compound” from “Above” to words “the” and “Veil”, and from “Into” to words “the” and “Battle” to indicate the two associated entity mentions. For many tasks on graph-to-text generation, the node-to-word alignments can be easily generated from off-the-shell toolkits. For example, in AMRto-text generation, there have been several aligners (Pourdamghani et al., 2014; Flanigan et al., 2016; Wang and Xue, 2017; Liu et al., 2018c; Szubert et al., 2018) available for linking AMR nodes to words. For knowledge graphs, the alignments can be produced by simple rule-based matching or an entity-linking system. The resulting structure with labeled arcs connecting word pairs resembles a dependency tree, and thus we employ a deep biaffine model (Dozat and Manning, 2017) to predict this structure from the decoder states. More specifically, the model factorizes the probability for making each arc into two parts: an unlabeled factor and a labeled one. Given the decoder states s1 , . . . , sN"
2020.acl-main.712,W17-3518,0,0.413529,"y” in Figure 1(a)) contains a pair of entities and their relation. As the next step, the alignments between graph nodes and target words are generated to ground this view into the target sentence for reconstruction. Our second view is the linearization of each input graph produced by depth-first graph traversal, and this view is reconstructed token-by-token from the last decoder state. Overall the first view highlights the local information of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effective"
2020.acl-main.712,Q19-1019,0,0.023917,"tion is to produce a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucia"
2020.acl-main.712,N19-1235,0,0.0208537,"iew focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) extended the reconstruction loss of Tu et al. (201"
2020.acl-main.712,D19-6310,0,0.299588,"res, only the most frequent 20K are kept, while the rest are mapped into a special UNK feature.1 X i2[1..N ] p(yi |si ; ✓), (5) where ✓ represents all model parameters. R2 | (3) where W Q , W K and W R2 are model parameters, and dh denotes the encoder-state dimension. The encoder adopts L self-attention layers and H L = L (hL 1 . . . h|V |) represents the concatenated top-layer hidden states of the encoder, which will be used in attention-based decoding. 4 Multi-View Autoencoding Losses Figure 2 visualizes the training framework using our multi-view autoencoding losses, where the 1 Zhu et al. (2019) also mentions other (such as CNN-based or self-attention-based) alternatives to calculate ij . While the GPU memory consumption of these alternatives is a few times more than our baseline, ours actually shows a comparable performance. 7989 View 1: triple relations ARG2 ARG1 want-01 ARG1 ARG0 boy ARG2 ARG1 lunch ARG0 mod ARG0 ARG1 The boy wants the beautiful girl to eat lunch with him eat-01 ARG0 girl Encoder Attention Language modeling loss Decoder mod View 2: linearized graph beautiful want :ARG0 boy :ARG1 eat ( :ARG0 (girl :mod beautiful) :ARG1 lunch :ARG2 boy) Figure 2: The training framew"
2020.acl-main.712,P17-1089,0,0.0189482,". . . sN ) denotes the concatenated states for the target sentence (Equation 4), and the loss for reconstructing this view is defined as the negative log-likelihood for the linearized graph: X lauto2 = log p(xi |ti ; ✓), (15) i2[1..M ] (12) where [x] in the subscript represents choosing the x-th item from the corresponding vector. As the final step, the loss for reconstructing this view is defined as the negative log-likelihood of all target arcs E 0 (the grounded triples from E): X lauto1 = log p(yj , l|yi ) (13) (yj ,l,yi )2E 0 4.2 infer the original graph structure. Besides, previous work (Iyer et al., 2017; Konstas et al., 2017) has shown the effectiveness of generating linearized graphs as sequences for graph parsing, which also confirms our observation. Given a linearized graph represented as a sequence of tokens x1 , . . . , xM , where each token xi can be a graph node, a edge label or a inserted bracket, we adopt another standard Transformer decoder (SADecoderg ) to produce the sequence: Loss 2: Reconstructing Linearized Graphs with a Transformer Decoder As a supplement to our first loss for reconstructing the local information of each grounded triple, we introduce the second loss for predi"
2020.acl-main.712,P19-1236,1,0.815011,"or a table, and we propose two general and effective methods that reconstruct different complementary views of each input graph. Besides, we propose methods to breakdown the whole (graph, sentence) pair into smaller pieces of (edge, word) pairs with alignments, before training our model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for traini"
2020.acl-main.712,N19-1238,0,0.067091,"Missing"
2020.acl-main.712,P17-1014,0,0.415658,"and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucial input concepts and relations may be messed up or even dropped. Taking the AMR in Figure 1(a) as an example, a model may produce “t"
2020.acl-main.712,D18-1183,0,0.0602147,"er than a sentence or a table, and we propose two general and effective methods that reconstruct different complementary views of each input graph. Besides, we propose methods to breakdown the whole (graph, sentence) pair into smaller pieces of (edge, word) pairs with alignments, before training our model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decod"
2020.acl-main.712,D18-1264,0,0.0957235,"er than a sentence or a table, and we propose two general and effective methods that reconstruct different complementary views of each input graph. Besides, we propose methods to breakdown the whole (graph, sentence) pair into smaller pieces of (edge, word) pairs with alignments, before training our model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decod"
2020.acl-main.712,W18-6501,0,0.143887,"oder state. Overall the first view highlights the local information of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (recon"
2020.acl-main.712,W03-3017,0,0.220221,"al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for training, so no extra error propagation (for structure prediction) can be introduced. Conversely, their models generate trees together with target sentences, thus extra efforts (Wu et al., 2017) are introduced to alleviate error propagation. Finally, there exist transition-based algorithms (Nivre, 2003) to convert tree parsing into the prediction of transition actions, while we study reconstructing graphs, where there is no common parsing algorithm for all graph types. 7988 (Liu et al., 2018b) and sentiment analysis (Rei and Søgaard, 2019). Since input reconstruction is not intuitively related to these tasks, the autoencoding loss only serves as more training signals. Different from these efforts, we leverage autoencoding loss as a means to preserve input knowledge. Besides, we study reconstructing complex graphs, proposing a general multi-view approach for this goal. 3 Base: Structure-Aware"
2020.acl-main.712,P02-1040,0,0.106933,"ighly sensitive to the input order. One major merit for Loss 2 is the generality, as node-to-word alignments may not be easily obtained, especially for multi-lingual tasks. 4.4 Training with Autoencoding Losses The final training signal with both proposed autoencoding losses is formalized as: 7991 lf inal = lbase + ↵lauto1 + lauto2 , (16) where ↵ and are coefficients for our proposed losses. Both coefficient values are selected by a development experiment. 5 Experiments We study the effectiveness of our autoencoding training framework on AMR-to-text generation and KG-to-text generation. BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014) scores are reported for comparison. Following previous work, we use the multi-bleu.perl from Moses2 for BLEU evaluation. 5.1 Data AMR datasets3 We take LDC2015E86 that contains 16,833, 1,368 and 1,371 instances for training, development and testing, respectively. Each instance contains a sentence and an AMR graph. Following previous work, we use a standard AMR simplifier (Konstas et al., 2017) to preprocess our AMR graphs, and take the PTB-based Stanford tokenizer4 to tokenize the sentences. The node-toword alignments are produced by the ISI aligner (Pou"
2020.acl-main.712,D14-1048,0,0.178545,"rds of the entity in order to represent the whole-entity information in the sentence. Taking the edge “followedBy” in Figure 1(b) as an example, we first ground it onto the target sentence to connect words “Above” and “Into”. Next, we create edges with label “compound” from “Above” to words “the” and “Veil”, and from “Into” to words “the” and “Battle” to indicate the two associated entity mentions. For many tasks on graph-to-text generation, the node-to-word alignments can be easily generated from off-the-shell toolkits. For example, in AMRto-text generation, there have been several aligners (Pourdamghani et al., 2014; Flanigan et al., 2016; Wang and Xue, 2017; Liu et al., 2018c; Szubert et al., 2018) available for linking AMR nodes to words. For knowledge graphs, the alignments can be produced by simple rule-based matching or an entity-linking system. The resulting structure with labeled arcs connecting word pairs resembles a dependency tree, and thus we employ a deep biaffine model (Dozat and Manning, 2017) to predict this structure from the decoder states. More specifically, the model factorizes the probability for making each arc into two parts: an unlabeled factor and a labeled one. Given the decoder"
2020.acl-main.712,P17-1194,0,0.169993,"nal Linguistics, pages 7987–7998 c July 5 - 10, 2020. 2020 Association for Computational Linguistics generation models, hindering the usability of these models for real-world applications (Duˇsek et al., 2018, 2019; Balakrishnan et al., 2019). A potential solution for this issue is improving the training signal to enhance preserving of structural information. However, little work has been done to explore this direction so far, probably because designing such signals is non-trivial. As a first step towards this goal, we propose to enrich the training signal with additional autoencoding losses (Rei, 2017). Standard autoencoding for graph-to-sequence tasks requires reconstructing (parsing into) input graphs, while the parsing algorithm for one type of graphs (such as knowledge graphs) may not generalize to other graph types or may not even exist. To make our approach general across different types of graphs, we propose to reconstruct different views of each input graph (rather than the original graph), where each view highlights one aspect of the graph and is easy to produce. Then through multi-task learning, the autoencoding losses of all views are back-propagated to the whole model so that th"
2020.acl-main.712,D19-1314,0,0.0725088,"a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucial input concepts and re"
2020.acl-main.712,P16-1162,0,0.042332,"than LDC2015E86, we may conclude that the problem of dropping input information may not be effectively reduced by simply adding more supervised data, and as a result, our approach can still be effective on a larger dataset. This conclusion can also be confirmed by comparing the gains of our approach on both AMR datasets regarding BLEU score (2.3 vs 2.5). 5.9 Main Results on WebNLG Table 6 shows the comparison of our results with previous results on the WebNLG testset. ADAPT (Gardent et al., 2017) is based on the standard encoder-decoder architecture (Cho et al., 2014) with byte pair encoding (Sennrich et al., 2016), and it was the best system of the challenge. GCNEC (Marcheggiani and Perez-Beltrachini, 2018) is a recent model using a graph convolution network (Kipf and Welling, 2017) for encoding KGs. Our baseline shows a comparable performance with the previous state of the art. Based on this baseline, applying either loss leads to a significant improvement, and their combination brings a gain of more than 2 BLEU points. Although the baseline already achieves a very high BLEU score, yet the gains on this task are still comparable with those on AMR-to-text generation. This observation may imply that the"
2020.acl-main.712,Q19-1002,1,0.855603,"riments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) extended the reconstruction loss of Tu et al. (2017) on table-to-text generation, where a table contains multiple record"
2020.acl-main.712,P18-1150,1,0.905082,", KG-to-text generation is to produce a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentence"
2020.acl-main.712,N18-1106,0,0.0225874,"Missing"
2020.acl-main.712,P18-1151,0,0.0378542,"token from the last decoder state. Overall the first view highlights the local information of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target"
2020.acl-main.712,D17-1129,0,0.0198313,"entity information in the sentence. Taking the edge “followedBy” in Figure 1(b) as an example, we first ground it onto the target sentence to connect words “Above” and “Into”. Next, we create edges with label “compound” from “Above” to words “the” and “Veil”, and from “Into” to words “the” and “Battle” to indicate the two associated entity mentions. For many tasks on graph-to-text generation, the node-to-word alignments can be easily generated from off-the-shell toolkits. For example, in AMRto-text generation, there have been several aligners (Pourdamghani et al., 2014; Flanigan et al., 2016; Wang and Xue, 2017; Liu et al., 2018c; Szubert et al., 2018) available for linking AMR nodes to words. For knowledge graphs, the alignments can be produced by simple rule-based matching or an entity-linking system. The resulting structure with labeled arcs connecting word pairs resembles a dependency tree, and thus we employ a deep biaffine model (Dozat and Manning, 2017) to predict this structure from the decoder states. More specifically, the model factorizes the probability for making each arc into two parts: an unlabeled factor and a labeled one. Given the decoder states s1 , . . . , sN , the representation"
2020.acl-main.712,2020.tacl-1.2,0,0.0576841,"orts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucial input concepts and relations may be messed up or even dropped. Taking the AMR in Figure 1(a) as an example, a model may produce “the girl wants the boy to go”, which conveys an opposite meaning to the AM"
2020.acl-main.712,D18-1509,0,0.0228232,"struct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for training, so no extra error propagation (for structure prediction) can be introduced. Conversely, their models generate trees together with target sentences, thus extra efforts (Wu et al., 2017) are introduced to alleviate error propagation. Finally, there exist transition-based algorithms (Nivre, 2003"
2020.acl-main.712,D17-1239,0,0.0254359,"2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) extended the reconstruction loss of Tu et al. (2017) on table-to-text generation, where a table contains multiple records that fit into several fields.We study a more challenging topic on how to reconstruct a complex graph structure rather than a sentence or a table, and we propose two general and effective methods that reconstruct different complementary views of each input graph. Besides, we propose methods to breakdown the whole (graph, sentence) pair into smaller pieces of (edge, word) pairs with alignments, before training our model to reconstruct each edge given the corresponding word."
2020.acl-main.712,P17-1065,0,0.0213542,"ur model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for training, so no extra error propagation (for structure prediction) can be introduced. Conversely, their models generate trees together with target sentences, thus extra efforts (Wu et al., 2017) are introduced to alleviate error propagation. Finally, there exist transition-based alg"
2020.acl-main.712,D18-1112,1,0.768306,"hts the local information of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wisem"
2020.acl-main.712,D19-1548,0,0.667496,"ns (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucial input concepts and relations may be messed up or even dropped. Taking the AMR in Figure 1(a) as an example, a model may produce “the girl wants the boy to go”, whic"
2020.ccl-1.66,P04-3031,0,0.35081,"Missing"
2020.ccl-1.66,W18-5104,0,0.0526672,"Missing"
2020.cogalex-1.2,D17-1113,0,0.0155702,"2010) used WordNet (Miller, 1995) to compute the values of the features. Obviously, such feature based methods are constrained by corpora, and only focus on linguistic unimodal. Pereira et al. (2013) proposed a distributed semantics based method using features learnt form Wikipeida to predict neural activations for unseen concepts. Since then, various studies have shown 13 Figure 1: Compute multimodal embeddings. that distributed semantic representations have correlations with brain concept representation (Devereux et al., 2010; Murphy et al., 2012; Pereira et al., 2013; Pereira et al., 2018; Bulat et al., 2017). However, though these methods outperform the feature based methods, they still ignore the fact that the information in the real world comes as different modalities. In contrast to their work, we investigate the human conceptual representation mechanism via evaluating the effects of multimodal features rather than only unimodal linguistic feature. More closely related to our work, Bulat et al. (2017) presented a systematic evaluation and comparison of unimodal and multimodal semantic models in their ability to predict patterns of conceptual representation in the human brain. However, they onl"
2020.cogalex-1.2,W10-0609,0,0.339761,"ith both encoding and decoding the concepts of different semantic categories in brains. Mitchell et al. (2008) first introduced the task of predicting fMRI activation and proposed a featured-based model which takes a semantic representation of a single noun to predict the fMRI activation elicited by that noun. Subsequent studies (Pereira et al., 2018) introduced distributed based methods to build correlations between distributed semantic representations and patterns of neural activation. However, previous work mostly focuses on a single source of input features, e.g. count-based word vectors (Devereux et al., 2010; Murphy et al., 2012; Pereira et al., 2013; Pereira et al., 2018) to explore the in brain encoding process, which builds correlation between neural signals and distributed representation, and thus can be useful for better understanding both the brain and the word representation. But there has been little work systematically investigating the effect of different modalities on predicting fMRI activations. We address this limitation by empirically investigating two forms of rich source features: multimodal features and associative conceptual feature. First, we systematically compare input featur"
2020.cogalex-1.2,W10-0603,0,0.0304357,"conceptual representation mainly focus on correlation between words and corresponding fMRI activations, including feature based methods and distributed representation based methods. Seminal work of Mitchell et al. (2008) pioneered the use of corpus-derived word representations to predict brain activation data associated with the meaning of nouns. This feature based method selected 25 verbs (i.e., ‘see’, ‘say’, ‘taste’.), and calculated the co-occurrence frequency of the noun with each of 25 verbs. In this regard, a noun word is encoded into 25 sensor-motor features. Subsequent work including Jelodar et al. (2010) used WordNet (Miller, 1995) to compute the values of the features. Obviously, such feature based methods are constrained by corpora, and only focus on linguistic unimodal. Pereira et al. (2013) proposed a distributed semantics based method using features learnt form Wikipeida to predict neural activations for unseen concepts. Since then, various studies have shown 13 Figure 1: Compute multimodal embeddings. that distributed semantic representations have correlations with brain concept representation (Devereux et al., 2010; Murphy et al., 2012; Pereira et al., 2013; Pereira et al., 2018; Bulat"
2020.cogalex-1.2,D14-1005,0,0.0336611,"s the number of fMRI activation by the imaging dimension (amount of selected voxel, 500 for Mitchell et al. (2008) dataset and 5000 for Pereira et al. (2018) dataset). We investigate three types of multi-sense inputs, namely, linguistic, visual and auditory sources. And further we use associative conceptual input, namely, the associative conceptual words which is obtained from Small World of Word game. In the next two sections, we will introduce how to obtain multi-sense representations and associative conceptual representations. 4 Multi-Sense Representations Following Bruni et al. (2014) and Kiela and Bottou (2014), we construct multimodal semantic representation vector, Vm , by concatenating the linguistic, visual and auditory representations as shown in Figure 1: Vm = Vlinguistic kVvisual kVauditory , (2) where k is the concatenation operator. 14 4.1 Linguistic Representations The linguistic representation can be a dense vector that represents a word associated with a concept. Distributed word representations have been applied to statistical language modeling with considerable success (Bengio et al., 2003). This idea has enabled a substantial amount of progress in a wide range of NLP task, and was als"
2020.cogalex-1.2,D15-1293,0,0.0196399,"stages. We also notice that AUDITORY weakens model’s prediction ability except for P 6 and P 7. Together with the finding in unimodal experiments that auditory based model performs less significantly than the linguistic and visual based model, the result suggests that visual properties contribute the most in conceptual representation in conceptual representations of nouns in the human brain, while acoustic properties contribute less. The results from P 6 and P 7 also suggest there are individual differences in the effects of different modality data on conceptual representations in the brain. Kiela and Clark (2015) indicate that multimodal representations enriched by auditory information perform well on relatedness and similarity on words that have auditory associations such as instruments. We explore if the fMRI activation can be predicted by sound features, which is generated by using the objects which do not have obvious acoustic properties such as hand, foot, etc. Although the prediction accuracy is lower when using auditory features than using linguistic and visual features, it is significantly above the chance level. The results suggest that acoustic properties play a less important role but are u"
2020.cogalex-1.2,P16-4010,0,0.0185354,". For the auditory representations, we retrieve 3 to 100 audios from Freesound (Font et al., 2013) for each concept. To generate the auditory representation for each noun, we first obtain Mel-scale Frequency Cepstral Coefficients (MFCCs) (O’Shaughnessy, 1987) features of each audio and then quantize the features into a bag of audio words (BoAW) (Foote, 1997) representations. MFCCs are commonly used as features in speech recognition, information retrieval, and music analysis. After obtaining a BoAW set, we take the mean of each BoAW as the auditory representation. In this paper, we use MMFeat (Kiela, 2016) to generate 300-dimensional auditory representations. The code is available at https://github.com/douwekiela/mmfeat. 5 Associative Conceptual Representation Associative conceptual representation is a dense vector obtained from the associative conceptual words that are produced by humans in a game scene, and it is used to presented human’s associative thinking related a concept. To investigate that whether associative thinking can be reflected in the fMRI activation, we fuse the word vectors linearly and use it as our associative conceptual representations. The linear fusion is represented as:"
2020.cogalex-1.2,S12-1019,0,0.372321,"ecoding the concepts of different semantic categories in brains. Mitchell et al. (2008) first introduced the task of predicting fMRI activation and proposed a featured-based model which takes a semantic representation of a single noun to predict the fMRI activation elicited by that noun. Subsequent studies (Pereira et al., 2018) introduced distributed based methods to build correlations between distributed semantic representations and patterns of neural activation. However, previous work mostly focuses on a single source of input features, e.g. count-based word vectors (Devereux et al., 2010; Murphy et al., 2012; Pereira et al., 2013; Pereira et al., 2018) to explore the in brain encoding process, which builds correlation between neural signals and distributed representation, and thus can be useful for better understanding both the brain and the word representation. But there has been little work systematically investigating the effect of different modalities on predicting fMRI activations. We address this limitation by empirically investigating two forms of rich source features: multimodal features and associative conceptual feature. First, we systematically compare input features that come from lin"
2020.cogalex-1.2,D14-1162,0,0.0860808,"n applied to statistical language modeling with considerable success (Bengio et al., 2003). This idea has enabled a substantial amount of progress in a wide range of NLP task, and was also shown useful for brain conceptual representation (Devereux et al., 2010; Murphy et al., 2012). The approach is based on the distributional hypothesis (Firth, 1957; Harris, 1954) which assumes that words with similar contexts tend to have similar semantic meaning. The intuition underlying the model is ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning. GloVe (Pennington et al., 2014) provides multiple versions of pre-trained word embeddings. In this paper, we use a 300-dimensional version of GloVe, which trained on a corpus consisting of Wikipedia 2014 and Gigaword 5. 4.2 Visual Representations Visual representation is used to represent an image associated with a concept in a dense vector. Our approach to constructing the visual representations component is to utilize a collection of images associated with words representing a particular concept. For example, given a stimulus ‘carrot’, the associated images are a collection of ‘carrot’ images that we retrieve from the dat"
2020.coling-main.15,P19-1284,0,0.0202294,"multi-view attention model for joint summarization and sentiment classification. Wang and Ren (2018) improve the model of Ma et al. (2018) by using additional attention on the generated text. Different from their work, we are not directly concerned about making better summaries. Instead, we make a broader discussion on how to make the best use of both review and summary for sentiment classification. Our work is also related to rationalizing sentiment predictions. Zhang et al. (2016) regard gold-standard rationales as additional input and used rationale-level attention for text classification. Bastings et al. (2019) propose an unsupervised latent model that selects a rationale and subsequently uses it for sentiment analysis. Our work is similar in that we can visualize the most salient words in sentiment classification. Different from the existing methods, our rationalization is based on the interaction between a review and a summary, with the latter guiding the visualization. 175 (a) Architecture of a co-attention joint encoder (b) Architecture of a review-centric joint encoder Figure 3: Architectures of co-attention model and our review-centric model. 3 Task 3.1 Problem Formulation w w s = The input to"
2020.coling-main.15,P16-1223,0,0.014864,"we are the first to investigate the correlation between reviews and their summaries for expressing sentiment, and the first to empirically investigate different models making use of both reviews and summaries for better sentiment analysis. We release our code at https://github.com/RingoS/ sentiment-review-summary. 2 Related Work Our work is partly related to previous work building well-designed matching models to capture the relationship between two texts. In reading comprehension, a matching model is required to capture the similarity among a given passage, a question and a candidate answer. Chen et al. (2016) adopted two GRUs to encode the passage and question, respectively, and a bilinear function to compute the similarity on each passage token. Xiong et al. (2017) make use of co-attention, which shares one single attention matrix between the passage and the question, calculating both passage-to-question and question-to-passage attention scores. For retrieval-based dialogue systems, models are required to calculate the matching score between a candidate response and a conversation context. In particular, Sequential Matching Network (Wu et al., 2017) captures matching information by constructing w"
2020.coling-main.15,D17-1047,0,0.0205389,"ities by calculating the similarity between a word sequence and a set of label embeddings. Compared with these methods, which model matching between two pieces of text, our work is different in that we consider how to effectively make use of the complementary property between a review and a corresponding summary for better review sentiment analysis. Our work is related to previous work on sentiment analysis (Pang et al., 2002; Kim and Hovy, 2004; Liu, 2012), taking a whole review as input (Kim, 2014; Zhang et al., 2015; Yang et al., 2016; Johnson and Zhang, 2017) rather than specific aspects (Chen et al., 2017; Li et al., 2019). Different from previous work, we additionally consider user-generated or automatically-generated summaries as input. Our work is related to existing work on joint summarization and sentiment classification. Ma et al. (2018) propose a multi-view attention model for joint summarization and sentiment classification. Wang and Ren (2018) improve the model of Ma et al. (2018) by using additional attention on the generated text. Different from their work, we are not directly concerned about making better summaries. Instead, we make a broader discussion on how to make the best use"
2020.coling-main.15,D19-1422,1,0.839331,"o-passage attention scores. For retrieval-based dialogue systems, models are required to calculate the matching score between a candidate response and a conversation context. In particular, Sequential Matching Network (Wu et al., 2017) captures matching information by constructing word-to-word and a sequence-tosequence similarity matrices. Deep Attention Matching Network (Zhou et al., 2018) adopts self-attention and cross-attention modules to harvest intra-sentence relationship and inter-sentence relationship, respectively. To capture potential long-term label dependency in sequence labeling, Cui and Zhang (2019) use attention over label embeddings to refine the marginal label probabilities by calculating the similarity between a word sequence and a set of label embeddings. Compared with these methods, which model matching between two pieces of text, our work is different in that we consider how to effectively make use of the complementary property between a review and a corresponding summary for better review sentiment analysis. Our work is related to previous work on sentiment analysis (Pang et al., 2002; Kim and Hovy, 2004; Liu, 2012), taking a whole review as input (Kim, 2014; Zhang et al., 2015;"
2020.coling-main.15,P17-1052,0,0.0157648,"label embeddings to refine the marginal label probabilities by calculating the similarity between a word sequence and a set of label embeddings. Compared with these methods, which model matching between two pieces of text, our work is different in that we consider how to effectively make use of the complementary property between a review and a corresponding summary for better review sentiment analysis. Our work is related to previous work on sentiment analysis (Pang et al., 2002; Kim and Hovy, 2004; Liu, 2012), taking a whole review as input (Kim, 2014; Zhang et al., 2015; Yang et al., 2016; Johnson and Zhang, 2017) rather than specific aspects (Chen et al., 2017; Li et al., 2019). Different from previous work, we additionally consider user-generated or automatically-generated summaries as input. Our work is related to existing work on joint summarization and sentiment classification. Ma et al. (2018) propose a multi-view attention model for joint summarization and sentiment classification. Wang and Ren (2018) improve the model of Ma et al. (2018) by using additional attention on the generated text. Different from their work, we are not directly concerned about making better summaries. Instead, we make a"
2020.coling-main.15,C04-1200,0,0.828804,"gnal distribution of a review and that of its corresponding summary are in fact complementary to each other. We thus explore various architectures to better guide the interactions between the two and propose a hierarchically-refined review-centric attention model. Empirical results show that our review-centric model can make better use of user-written summaries for review sentiment analysis, and is also more effective compared to existing methods when the user summary is replaced with summary generated by an automatic summarization system. 1 Introduction Sentiment analysis (Pang et al., 2002; Kim and Hovy, 2004; Liu, 2012; Socher et al., 2013) is a fundamental task in natural language processing, which predicts the subjectivity and polarity of a given text. In practice, automatically extracting sentiment from user reviews has wide applications such as E-commerce and movie reviews (Manek et al., 2015; Guan et al., 2016; Kumari and Singh, 2016). In many review websites such as Amazon and IMDb, the user is allowed to give a summary in addition to the review, where summaries can contain more general information about the review. Figure 1 gives a few such examples. It is thus an interesting research ques"
2020.coling-main.15,D14-1181,0,0.00335143,"labeling, Cui and Zhang (2019) use attention over label embeddings to refine the marginal label probabilities by calculating the similarity between a word sequence and a set of label embeddings. Compared with these methods, which model matching between two pieces of text, our work is different in that we consider how to effectively make use of the complementary property between a review and a corresponding summary for better review sentiment analysis. Our work is related to previous work on sentiment analysis (Pang et al., 2002; Kim and Hovy, 2004; Liu, 2012), taking a whole review as input (Kim, 2014; Zhang et al., 2015; Yang et al., 2016; Johnson and Zhang, 2017) rather than specific aspects (Chen et al., 2017; Li et al., 2019). Different from previous work, we additionally consider user-generated or automatically-generated summaries as input. Our work is related to existing work on joint summarization and sentiment classification. Ma et al. (2018) propose a multi-view attention model for joint summarization and sentiment classification. Wang and Ren (2018) improve the model of Ma et al. (2018) by using additional attention on the generated text. Different from their work, we are not dir"
2020.coling-main.15,W02-1011,0,0.0400787,"the sentimental signal distribution of a review and that of its corresponding summary are in fact complementary to each other. We thus explore various architectures to better guide the interactions between the two and propose a hierarchically-refined review-centric attention model. Empirical results show that our review-centric model can make better use of user-written summaries for review sentiment analysis, and is also more effective compared to existing methods when the user summary is replaced with summary generated by an automatic summarization system. 1 Introduction Sentiment analysis (Pang et al., 2002; Kim and Hovy, 2004; Liu, 2012; Socher et al., 2013) is a fundamental task in natural language processing, which predicts the subjectivity and polarity of a given text. In practice, automatically extracting sentiment from user reviews has wide applications such as E-commerce and movie reviews (Manek et al., 2015; Guan et al., 2016; Kumari and Singh, 2016). In many review websites such as Amazon and IMDb, the user is allowed to give a summary in addition to the review, where summaries can contain more general information about the review. Figure 1 gives a few such examples. It is thus an inter"
2020.coling-main.15,D14-1162,0,0.0833351,"arison among different interacting schemes. Table 3: Results using gold summary as input. overall sentiment rating which ranges from 1 to 5. For fair comparison with previous work, we adopt the same domains and partition used by Ma et al. (2018) and Wang and Ren (2018), which includes three datasets (Toys & Games, Sports & Outdoors and Movies & TV). The statistics of our adopted dataset are shown in Table 2. For each dataset, the first 1000 samples are taken as the development set, the next 1000 samples as the test set, and the rest as the training set. 5.1 Experimental Settings We use GloVe (Pennington et al., 2014) 300-dimensional embeddings as pretrained word vectors. The LSTM hidden size is set to 256. We use Adam (Kingma and Ba, 2015) to optimize all models, with an learning rate of 3e − 4, momentum parameters β1 = 0.9, β2 = 0.999, and  = 1 × 10−8 . The dropout rate α and number of attention heads M are separately set depending on the size of each dataset and using development experiments, which are α = 0.5 and M = 1 for Toys & Games, α = 0.2 and M = 1 for Sports & Outdoors and α = 0.0 and M = 2 for Movies & TV. We adopt two layers for our review-centric model. In addition to conducting experiments"
2020.coling-main.15,P17-1099,0,0.0382617,"iments by replacing the user-written summary with a system-generated summary for two reasons. First, we want to know the extent to which our method can be generalized to settings of traditional sentiment classification, where the input consists of only one piece of text. This is the setting adopted by most previous research. Second, two of our baselines, namely HSSC (Ma et al., 2018) and SAHSSC (Wang and Ren, 2018), adopt this setting and use summary information via multi-task learning. For generating summaries, we separately adopt a pointer-generator network (PG-Net) with coverage mechanism (See et al., 2017) trained on the training set. 5.2 Results Our main results are shown in Tables 3 and 4. It can be seen from Table 3 that the baseline using review only outperforms that using summary only, which indicates that the review is more informative than the summary. In addition, the Separate Encoder models outperform both the Summary Only and the Review Only models, which indicates that additional summary input is beneficial to sentiment analysis. Finally, the Joint Encoder models generally outperform the Separate Encoder models, which suggests that modeling interactions between review and summary is"
2020.coling-main.15,D18-1065,0,0.0281224,": the hidden state matrices are concatenated and then average-pooled to form a final representation for later prediction. 2) self-attention baseline: the hidden state matrices are separately processed using self-attention mechanism. Subsequently the two matrices are concatenated and average-pooled to produce the final representation for later prediction. Our self-attention module follows the implementation of Lin et al. (2017). 4.2.2 Symmetric Joint Encoding On top of the sequence encoder, we separately adopt average pooling, self-attention (Lin et al., 2017), hard-attention (Xu et al., 2015; Shankar et al., 2018) and co-attention (Xiong et al., 2017) mechanisms as our joint baselines. In particular, co-attention can capture the interactions between review and summary by calculating the bidirectional symmetric attention flows with a shared attention weight matrix. 1) For joint encoder baselines using pooling and self-attention, only one BiLSTM is adopted, with concatenated review and summary texts as input. 2) The hard-attention baseline is trained using an additional extractive summarization objective. We implement our baseline following Xu et al. (2015) and Shankar et al. (2018). In particular, words"
2020.coling-main.15,D13-1170,0,0.00712604,"Missing"
2020.coling-main.15,P17-1046,0,0.0230322,"n passage, a question and a candidate answer. Chen et al. (2016) adopted two GRUs to encode the passage and question, respectively, and a bilinear function to compute the similarity on each passage token. Xiong et al. (2017) make use of co-attention, which shares one single attention matrix between the passage and the question, calculating both passage-to-question and question-to-passage attention scores. For retrieval-based dialogue systems, models are required to calculate the matching score between a candidate response and a conversation context. In particular, Sequential Matching Network (Wu et al., 2017) captures matching information by constructing word-to-word and a sequence-tosequence similarity matrices. Deep Attention Matching Network (Zhou et al., 2018) adopts self-attention and cross-attention modules to harvest intra-sentence relationship and inter-sentence relationship, respectively. To capture potential long-term label dependency in sequence labeling, Cui and Zhang (2019) use attention over label embeddings to refine the marginal label probabilities by calculating the similarity between a word sequence and a set of label embeddings. Compared with these methods, which model matching"
2020.coling-main.15,N16-1174,0,0.0541551,"use attention over label embeddings to refine the marginal label probabilities by calculating the similarity between a word sequence and a set of label embeddings. Compared with these methods, which model matching between two pieces of text, our work is different in that we consider how to effectively make use of the complementary property between a review and a corresponding summary for better review sentiment analysis. Our work is related to previous work on sentiment analysis (Pang et al., 2002; Kim and Hovy, 2004; Liu, 2012), taking a whole review as input (Kim, 2014; Zhang et al., 2015; Yang et al., 2016; Johnson and Zhang, 2017) rather than specific aspects (Chen et al., 2017; Li et al., 2019). Different from previous work, we additionally consider user-generated or automatically-generated summaries as input. Our work is related to existing work on joint summarization and sentiment classification. Ma et al. (2018) propose a multi-view attention model for joint summarization and sentiment classification. Wang and Ren (2018) improve the model of Ma et al. (2018) by using additional attention on the generated text. Different from their work, we are not directly concerned about making better sum"
2020.coling-main.15,D16-1076,0,0.0132771,"ries as input. Our work is related to existing work on joint summarization and sentiment classification. Ma et al. (2018) propose a multi-view attention model for joint summarization and sentiment classification. Wang and Ren (2018) improve the model of Ma et al. (2018) by using additional attention on the generated text. Different from their work, we are not directly concerned about making better summaries. Instead, we make a broader discussion on how to make the best use of both review and summary for sentiment classification. Our work is also related to rationalizing sentiment predictions. Zhang et al. (2016) regard gold-standard rationales as additional input and used rationale-level attention for text classification. Bastings et al. (2019) propose an unsupervised latent model that selects a rationale and subsequently uses it for sentiment analysis. Our work is similar in that we can visualize the most salient words in sentiment classification. Different from the existing methods, our rationalization is based on the interaction between a review and a summary, with the latter guiding the visualization. 175 (a) Architecture of a co-attention joint encoder (b) Architecture of a review-centric joint"
2020.coling-main.15,P18-1103,0,0.0538596,"Missing"
2020.coling-main.221,P18-1236,0,0.0859762,"al studies illustrate the importance of proposed sentiment forecasting task, and justify the effectiveness of our NSF model over several strong baselines. 1 Introduction Developing intelligent chatbots is of great appealing to both the industry and the academics. However it is challenging to build up such an intelligent chatbot which involves a series of high-level natural language processing techniques, such as sentiment analysis of utterances in dialog. Previous studies on sentiment classification focus on determining polarity (positive or negative) in a single document (Pang and Lee, 2008; Amplayo et al., 2018). In comparison, only few studies focus on determining polarity of utterances in dialog (Herzig et al., 2016; Majumder et al., 2018). However, all of these studies focus on determining the polarity of existing utterances. It may be more important to predict the polarity of next utterance yet to come. Given the example in Figure 1, although B expresses a positive sentiment in second utterance, A still shows a negative sentiment in his response. In this case, if B know that A would be very upset after his first utterance, he may revise his utterance to let A feel more comfortable. Hence, predict"
2020.coling-main.221,C18-1063,0,0.0192703,"Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sentimental text on the customer’s perception of the service agent. On the basic, Herzig et al. (2016) used SVM to classify sentiment in customer support dialogs by integrating both text based turn and dialog features. Recently, Cerisara et al. (2018) proposed a multi-task hierarchical recurrent network to classify sentiment and dialog act jointly. Majumder et al. (2018) proposed a recurrent neural networks to track of the individual states throughout the dialog and employed this information for sentiment classification. Different from previous studies which focus on detecting the polarity of existing utterances, we propose a novel and important task to forecast polarity of next utterance yet to come. 3 Neural Sentiment Forecasting Modeling As illustrated in Figure 1, given a existed utterance sequence {u1 , u2 , ..., un−1 } in a dialog d,"
2020.coling-main.221,D16-1171,0,0.0190581,"to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sentimental text on the customer’s perception of the service agent. On the basic, Herzig et al. (2016) used SVM to classify sentiment in customer support dialogs by integrating both text based turn and dialog features. Recently,"
2020.coling-main.221,D18-1280,0,0.0169534,"el to learn the representation of ui (1 ≤ i ≤ 3) (Section 3.1), and then employs the representation of ui to forecasting sentiment of next utterance un . • LSTMseq is a sequence based sentiment forecasting model, it employs a LSTM model to learn dialog representation d from the existed utterance sequence {u1 , u2 , u3 } (Eq. 2), and then employ the dialog representation d to forecast sentiment of un . • ICON takes one utterance with previous k utterances as input, and uses a GRU model for modeling inter-personal dependency in previous utterances and stores all history with one memory network (Hazarika et al., 2018). 1 There exist six categories of emotion in the dataset: joy, anger, disgust, fear, sadness, and surprise. Besides, many utterances do not express any emotion (i.e., neutral). 2453 Table 1: Comparison with baselines. Pos F1. Neg F1. Avg F1. LSTM1 0.545 0.285 0.415 2 LSTM 0.506 0.310 0.408 LSTM3 0.558 0.273 0.415 seq LSTM 0.563 0.342 0.453 ICON 0.529 0.293 0.411 DialogRNN 0.540 0.358 0.449 NSF 0.586 0.387 0.486 • DialogRNN employs recurrent neural networks to keep track of the individual states of utterances and uses this information for sentiment classification in dialog (Majumder et al., 201"
2020.coling-main.221,W16-3609,0,0.137295,"our NSF model over several strong baselines. 1 Introduction Developing intelligent chatbots is of great appealing to both the industry and the academics. However it is challenging to build up such an intelligent chatbot which involves a series of high-level natural language processing techniques, such as sentiment analysis of utterances in dialog. Previous studies on sentiment classification focus on determining polarity (positive or negative) in a single document (Pang and Lee, 2008; Amplayo et al., 2018). In comparison, only few studies focus on determining polarity of utterances in dialog (Herzig et al., 2016; Majumder et al., 2018). However, all of these studies focus on determining the polarity of existing utterances. It may be more important to predict the polarity of next utterance yet to come. Given the example in Figure 1, although B expresses a positive sentiment in second utterance, A still shows a negative sentiment in his response. In this case, if B know that A would be very upset after his first utterance, he may revise his utterance to let A feel more comfortable. Hence, predicting the polarity of the next utterance can help a speaker to improve their utterances, which is important in"
2020.coling-main.221,D18-1136,0,0.0213367,": Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sentimental text on the customer’s perception of the service agent. On the basic, Herzig et al. (2016) used SVM to classify sentiment in customer suppor"
2020.coling-main.221,D14-1181,0,0.00759751,"inguistics, pages 2448–2458 Barcelona, Spain (Online), December 8-13, 2020 A: John, I&apos;ve asked you not to smoke in here! I don&apos;t want to see you smoking in my office again. B: I&apos;m sorry, Ms. Fairbanks. I won&apos;t let it happen again. A: Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated tha"
2020.coling-main.221,I17-1099,0,0.0645772,"Missing"
2020.coling-main.221,D13-1170,0,0.0067668,"Spain (Online), December 8-13, 2020 A: John, I&apos;ve asked you not to smoke in here! I don&apos;t want to see you smoking in my office again. B: I&apos;m sorry, Ms. Fairbanks. I won&apos;t let it happen again. A: Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversation"
2020.coling-main.221,P15-1150,0,0.0123949,"oke in here! I don&apos;t want to see you smoking in my office again. B: I&apos;m sorry, Ms. Fairbanks. I won&apos;t let it happen again. A: Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sentimental text on th"
2020.coling-main.221,C16-1311,0,0.0168024,"banks. I won&apos;t let it happen again. A: Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sentimental text on the customer’s perception of the service agent. On the basic, Herzig et al. (2016) used S"
2020.coling-main.221,D16-1169,1,0.852207,"asked you not to smoke in here! I don&apos;t want to see you smoking in my office again. B: I&apos;m sorry, Ms. Fairbanks. I won&apos;t let it happen again. A: Negative (That&apos;s what you said the last time! If you want to smoke, you&apos;ll have to use your break time and go outside! ) Figure 1: Example of dialog for sentiment forecasting. 2 Related Work Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015). More recently, researches focus on aspect level sentiment analysis (Tang et al., 2016; Tay et al., 2018; Huang and Carley, 2018) and user-based or product-based sentiment classification (Chen et al., 2016; Song et al., 2017; Amplayo et al., 2018). Different from document-level sentiment classification, sentiment classification in dialog aims to detect polarity of utterances in dialog by considering the influence of the whole dialog. As a prime study, Ruusuvuori (2012) stated that sentiment plays a pivotal role in conversations. Zhang et al. (2011) studied the impact of sen"
2020.coling-main.266,C18-1233,0,0.0180067,"kinds of methods, syntax-agnostic methods, which focus on the SRL problem itself, and syntax-aware methods, which explore various ways to integrate syntactic knowledge into the SRL models. Zhou and Xu (2015) propose to use deep BiLSTMs for English spanbased SRL. He et al. (2017) further employ several deep learning advanced practices into the stacked BiLSTMs. With the rise of Transformer in machine translation, Tan et al. (2018) employ deep self-attention encoder for SRL, achieving strong performance. Marcheggiani et al. (2017) propose a simple and fast model with rich input representations. Cai et al. (2018) present a full end-to-end model which composed of BiLSTM encoder and BiAffine scorer. He et al. (2018a) first treat SRL as a predicate-argument-role tuple identification task. Following the trend, Li et al. (2019) extend this framework for both span-based and dependency-based SRL, with constraining the argument length to be 1 for dependency-based SRL. Syntactic knowledge has been explored in various ways to promote the performance of SRL models. Roth and Lapata (2016) propose to integrate the dependency path embeddings into the basic SRL model for dependency-based SRL. Strubell et al. (2018)"
2020.coling-main.266,W05-0620,0,0.133682,"Missing"
2020.coling-main.266,N19-1423,0,0.0299973,"cit to take advantage of heterogeneous syntactic knowledge, which we believe are highly complementary. Our baseline model follows the architecture of He et al. (2018a). Afterwards, we inject the heterogeneous syntactic knowledge into the base model using two proposed methods. For the explicit method, we try to encode the heterogeneous automatic dependency trees with the recent popular graph convolutional networks (GCN) (Kipf and Welling, 2016). For the implicit method, which is inspired by the powerful representations from pre-trained language models, like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), we introduce a method to extract implicit syntactic representations from the dependency parser trained with heterogeneous syntactic treebanks. It is well known that the main reason for the success of pre-trained language model representations is the use of large amounts of natural text. However, it is difficult to obtain and costly to annotate large amounts of syntactic data. Therefore, making full use of existing heterogeneous data is the most feasible and natural idea. Intuitively, the explicit method models the syntactic structure of a sentence, providing valuable syntactic position infor"
2020.coling-main.266,Q19-1019,0,0.0241109,"racting syntactic knowledge from heterogeneous dependency treebanks. First, we introduce the method for encoding singleton dependency trees and then detailedly describe the variations to extract heterogeneous syntactic knowledge. 3.1 Syntactic Representation We employ two different methods to fully encode the homogeneous syntactic trees, i.e., GCN that encodes the syntactic structures and implicit representations that encode the syntactic features. Explicit Method. Graph convolutional networks (GCN) are neural networks that work on graph structures, which have been explored in many NLP tasks (Guo et al., 2019; Zhang et al., 2020). Formally, we denote an undirected graph as G = (V, E), where V and E are the set of nodes and edges, respectively. The GCN computation flow of node v ∈ V at l-th layer is defined as ! X l (3) hlv = ρ Wl hl−1 u +b , u∈N (v) where Wl ∈ Rm×m is the weight matrix, bl ∈ Rm is the bias term, N (v) is the set of all one-hop neighbour nodes of v, and ρ is an activation function. Especially, h0u ∈ Rm is the initial input representation, and m is the representation dimension. In our work, we employ a 1-layer BiLSTM encoder over the input layer1 , and treat the BiLSTM outputs as th"
2020.coling-main.266,P17-1044,0,0.505726,"ct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syntactic knowledge into syntax-agnostic models. Roth and Lapata"
2020.coling-main.266,P18-2058,0,0.091085,"gnificant improvements over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syn"
2020.coling-main.266,P18-1192,0,0.575347,"gnificant improvements over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syn"
2020.coling-main.266,D19-1538,0,0.183322,"cate-argument-role tuple identification task. Following the trend, Li et al. (2019) extend this framework for both span-based and dependency-based SRL, with constraining the argument length to be 1 for dependency-based SRL. Syntactic knowledge has been explored in various ways to promote the performance of SRL models. Roth and Lapata (2016) propose to integrate the dependency path embeddings into the basic SRL model for dependency-based SRL. Strubell et al. (2018) propose a multi-task learning framework based on the self-attention encoder, which treats dependency parsing as an auxiliary task. He et al. (2019) propose an argument pruning method based on dependency tree positions for multilingual SRL. Recently, Xia et al. (2019a) propose a similar framework to extract syntactic representation for SRL, but they only focus on Chinese SRL. Xia et al. (2019b) compares four explicit methods to encode automatic dependency trees for SRL. Zhang et al. (2019) present different methods to encode dependency trees and compare various incorporation ways into a self-attention based SRL model. These previous works mainly focus on encoding single-sourced dependency treebank, which can only provide limited syntactic"
2020.coling-main.266,2020.acl-main.744,0,0.187854,"tperforming Ouchi et al. (2018) by +0.2 F1. In the end-to-end setting of the CoNLL-2005 dataset, our model achieves 86.95 and 80.53 F1 scores in the WSJ and Brown test data, respectively. Ablation study on heterogeneous treebanks. Even though integrating syntactic knowledge into SRL brings significant improvements to the CPB1.0 dataset, we also find that it is not obvious on the CoNLL2005 dataset. To know why the improvements on CoNLL-2005 are smaller than on the CPB1.0 dataset, 2986 Dev Models Predefined predicates. He et al. (2018a) He et al. (2018a) (w/ ELMo) Ouchi et al. (2018) (w/ ELMo)? Li et al. (2020) (w/ RoBERTa) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) End-to-end setting. He et al. (2018a) He et al. (2018a) (w/ ELMo) Baseline HybridHDP Baseline(w/ RoBERTa) HybridHDP (w/ RoBERTa) WSJ Brown P R F1 P R F1 P R F1 88.0 87.24 82.28 83.65 86.87 86.99 86.9 87.26 82.76 84.06 87.89 87.41 87.4 87.25 82.52 83.85 87.38 87.20 89.2 88.05 84.21 85.12 88.11 88.43 87.9 88.00 84.39 85.0 88.64 88.75 83.9 87.4 88.5 88.03 84.30 85.06 88.37 88.59 81.0 80.04 74.37 76.3 82.49 83.05 78.4 79.56 73.59 75.42 83.51 83.28 73.7 80.4 79.6 79.80 73.98 75.86 83.00 83.16 81.53 82.95 85.81 85.93 82.15"
2020.coling-main.266,K17-1041,0,0.102309,"s because of the development of deep learning. Previous works can mostly be divided into two kinds of methods, syntax-agnostic methods, which focus on the SRL problem itself, and syntax-aware methods, which explore various ways to integrate syntactic knowledge into the SRL models. Zhou and Xu (2015) propose to use deep BiLSTMs for English spanbased SRL. He et al. (2017) further employ several deep learning advanced practices into the stacked BiLSTMs. With the rise of Transformer in machine translation, Tan et al. (2018) employ deep self-attention encoder for SRL, achieving strong performance. Marcheggiani et al. (2017) propose a simple and fast model with rich input representations. Cai et al. (2018) present a full end-to-end model which composed of BiLSTM encoder and BiAffine scorer. He et al. (2018a) first treat SRL as a predicate-argument-role tuple identification task. Following the trend, Li et al. (2019) extend this framework for both span-based and dependency-based SRL, with constraining the argument length to be 1 for dependency-based SRL. Syntactic knowledge has been explored in various ways to promote the performance of SRL models. Roth and Lapata (2016) propose to integrate the dependency path em"
2020.coling-main.266,H94-1020,0,0.129263,"019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syntactic knowledge into syntax-agnostic models. Roth and Lapata (2016) propose to use dependency-based embeddings in a neural SRL model for dependency-based SRL. He et al. (2018b) introduce k-order pruning algorithm to prune arguments according to dependency trees. However, previous syntax-aware works mainly employ singleton/homogeneous automatic dependency trees, which are generated by a syntactic parser trained on a specific syntactic treebank, like Penn Treebank (PTB) (Marcus et al., 1994). Our work follows the syntax-aware approach and enhances SRL with heterogeneous syntactic knowledge. We define heterogeneous syntactic treebanks as treebanks that follow different annotation guidelines. All is well known, there exist many published dependency treebanks that follow different annotation guidelines, i.e., English PTB (Marcus et al., 1994), Universal Dependencies (UD) (Silveira et al., 2014), Penn Chinese Treebank (PCTB) (Xue et al., 2005), Chinese Dependency Treebank (CDT) (Che et ∗ Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 Internationa"
2020.coling-main.266,D18-1191,0,0.170603,"ate-argument structures. 4.3 Results and Analyses on English SRL Table 5 shows our results on English CoNLL-2005 development and test data, where WSJ is the indomain data and Brown is the out-of-domain data. Our implemented baseline model achieves slightly higher performance than the model (He et al., 2018a) we follow. The proposed methods can further improve our baseline model by +0.76 (p &lt; 1e-4) and +1.88 (p &lt; 1e-4) F1 scores on WSJ and Brown test data, respectively. With the help of RoBERTa representations, our full model achieves 88.59 F1 score on the test WSJ data, slightly outperforming Ouchi et al. (2018) by +0.2 F1. In the end-to-end setting of the CoNLL-2005 dataset, our model achieves 86.95 and 80.53 F1 scores in the WSJ and Brown test data, respectively. Ablation study on heterogeneous treebanks. Even though integrating syntactic knowledge into SRL brings significant improvements to the CPB1.0 dataset, we also find that it is not obvious on the CoNLL2005 dataset. To know why the improvements on CoNLL-2005 are smaller than on the CPB1.0 dataset, 2986 Dev Models Predefined predicates. He et al. (2018a) He et al. (2018a) (w/ ELMo) Ouchi et al. (2018) (w/ ELMo)? Li et al. (2020) (w/ RoBERTa) B"
2020.coling-main.266,D14-1162,0,0.0850612,"Missing"
2020.coling-main.266,N18-1202,0,0.0261584,"rspective of explicit and implicit to take advantage of heterogeneous syntactic knowledge, which we believe are highly complementary. Our baseline model follows the architecture of He et al. (2018a). Afterwards, we inject the heterogeneous syntactic knowledge into the base model using two proposed methods. For the explicit method, we try to encode the heterogeneous automatic dependency trees with the recent popular graph convolutional networks (GCN) (Kipf and Welling, 2016). For the implicit method, which is inspired by the powerful representations from pre-trained language models, like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), we introduce a method to extract implicit syntactic representations from the dependency parser trained with heterogeneous syntactic treebanks. It is well known that the main reason for the success of pre-trained language model representations is the use of large amounts of natural text. However, it is difficult to obtain and costly to annotate large amounts of syntactic data. Therefore, making full use of existing heterogeneous data is the most feasible and natural idea. Intuitively, the explicit method models the syntactic structure of a sentence, providing va"
2020.coling-main.266,P16-1113,0,0.444205,"tic knowledge brings significant improvements over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches"
2020.coling-main.266,D16-1212,0,0.0244291,"commonly used Chinese Proposition Bank 1.0 (CPB1.0) (Xue, 2008) and English CoNLL-2005 (Carreras and M`arquez, 2005) benchmarks. We implement our methods and baseline model with Pytorch, and our code, configurations, and models are released in https: //github.com/KiroSummer/HDP-SRL. Heterogeneous Dependency Treebanks. We employ PCTB7 and CDT as the heterogeneous dependency treebanks for Chinese, PTB and UD2 dependency treebanks for English. We employ BiAffine 2 We use the combination of EWT, GUM, LinES, and ParTUT of UD English corpus in our experiments. 2983 Dev Models Predefined predicates. Sha et al. (2016) Xia et al. (2017) Xia et al. (2019a) Xia et al. (2019a) (w/ BERT) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) End-to-end setting. Xia et al. (2019a) Xia et al. (2019a) (w/ BERT) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) Test P R F1 P R F1 81.62 84.92 87.17 86.93 82.36 85.59 86.65 87.02 83.39 81.99 85.25 86.91 86.97 81.94 84.86 87.48 87.93 80.59 84.45 87.11 87.57 77.69 79.67 83.91 87.54 81.26 84.65 87.29 87.75 80.35 83.4 85.47 86.73 80.89 84.23 86.31 86.41 82.39 85.92 80.62 83.81 85.89 86.57 79.82 83.67 86.30 87.04 78.22 82.89 86.14 86.23 81.73 85.57 7"
2020.coling-main.266,silveira-etal-2014-gold,0,0.054242,"Missing"
2020.coling-main.266,D18-1548,0,0.200322,"ents over strong baselines. We further conduct detailed analysis to gain insights on the usefulness of heterogeneous (vs. homogeneous) syntactic knowledge and the effectiveness of our proposed approaches for modeling such knowledge. 1 Introduction Semantic role labeling (SRL) is a fundamental task in natural language processing (NLP), which aims to find the predicate argument structures (Who did what to whom, when and where, etc.) in a sentence (see Figure 1 as an example). Recent SRL works can mostly be divided into two categories, i.e., syntax-aware (Roth and Lapata, 2016; He et al., 2018b; Strubell et al., 2018) and syntax-agnostic (He et al., 2017; He et al., 2018a) approaches according to whether incorporating syntactic knowledge or not. Most syntax-agnostic works employ deep BiLSTM or self-attention encoder to encode the contextual information of natural sentences, with various kinds of scorers to predict the probabilities of BIO-based semantic roles (He et al., 2017; Tan et al., 2018) or predicate-argument-role tuples (He et al., 2018a; Li et al., 2019). Motivated by the strong interplay between syntax and semantics, researchers explore various approaches to integrate syntactic knowledge into syn"
2020.coling-main.266,P17-1189,0,0.0185216,"ese Proposition Bank 1.0 (CPB1.0) (Xue, 2008) and English CoNLL-2005 (Carreras and M`arquez, 2005) benchmarks. We implement our methods and baseline model with Pytorch, and our code, configurations, and models are released in https: //github.com/KiroSummer/HDP-SRL. Heterogeneous Dependency Treebanks. We employ PCTB7 and CDT as the heterogeneous dependency treebanks for Chinese, PTB and UD2 dependency treebanks for English. We employ BiAffine 2 We use the combination of EWT, GUM, LinES, and ParTUT of UD English corpus in our experiments. 2983 Dev Models Predefined predicates. Sha et al. (2016) Xia et al. (2017) Xia et al. (2019a) Xia et al. (2019a) (w/ BERT) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) End-to-end setting. Xia et al. (2019a) Xia et al. (2019a) (w/ BERT) Baseline HybridHDP Baseline (w/ RoBERTa) HybridHDP (w/ RoBERTa) Test P R F1 P R F1 81.62 84.92 87.17 86.93 82.36 85.59 86.65 87.02 83.39 81.99 85.25 86.91 86.97 81.94 84.86 87.48 87.93 80.59 84.45 87.11 87.57 77.69 79.67 83.91 87.54 81.26 84.65 87.29 87.75 80.35 83.4 85.47 86.73 80.89 84.23 86.31 86.41 82.39 85.92 80.62 83.81 85.89 86.57 79.82 83.67 86.30 87.04 78.22 82.89 86.14 86.23 81.73 85.57 79.01 83.28 86.22 8"
2020.coling-main.266,D19-1541,1,0.823683,"fied as: ! hlv =ρ X Wl rlu l +b , (5) u∈N (v) l where the weight matrix Wl increases its column dimension by dhidden per layer, i.e., Wl ∈ Rdhidden ×d (dl = d + dhidden × (l − 1)). Implicit Method. Recently popular pre-trained language model embeddings (such as ELMo and BERT) have received much attention. These language models are trained on large amounts of natural text and can produce powerful implicit representations, whose effectiveness is shown in many NLP tasks. Inspired by these pre-trained language model representations and previous works on syntactic representations (Yu et al., 2018; Xia et al., 2019a), we make a trial to train a syntactic parser and extract similar implicit syntactic representations for SRL. We choose the state-of-the-art BiAffine parser (Dozat and Manning, 2017) as our basic dependency parser module. Concisely, BiAffine parser consists of an input layer, BiLSTMs encoder layer, and BiAffine scorers layer, as shown by the right component of Figure 3. We extract the hidden outputs from the 3-layer BiLSTMs encoder of the dependency parser module and make a softmax weighted summation on the outputs as the implicit syntactic representations. 1 The ExpHDP module shares the sam"
2020.coling-main.266,N19-1075,0,0.556792,"fied as: ! hlv =ρ X Wl rlu l +b , (5) u∈N (v) l where the weight matrix Wl increases its column dimension by dhidden per layer, i.e., Wl ∈ Rdhidden ×d (dl = d + dhidden × (l − 1)). Implicit Method. Recently popular pre-trained language model embeddings (such as ELMo and BERT) have received much attention. These language models are trained on large amounts of natural text and can produce powerful implicit representations, whose effectiveness is shown in many NLP tasks. Inspired by these pre-trained language model representations and previous works on syntactic representations (Yu et al., 2018; Xia et al., 2019a), we make a trial to train a syntactic parser and extract similar implicit syntactic representations for SRL. We choose the state-of-the-art BiAffine parser (Dozat and Manning, 2017) as our basic dependency parser module. Concisely, BiAffine parser consists of an input layer, BiLSTMs encoder layer, and BiAffine scorers layer, as shown by the right component of Figure 3. We extract the hidden outputs from the 3-layer BiLSTMs encoder of the dependency parser module and make a softmax weighted summation on the outputs as the implicit syntactic representations. 1 The ExpHDP module shares the sam"
2020.coling-main.266,J08-2004,0,0.0450443,"pendency data completes the forward process. 3.3 Hybrid HDP Our model combines the two representations together, according to our intuition that explicit and implicit syntactic representations are highly complementary, which is denoted as “HybridHDP” (Hybrid Heterogeneous Dependency Parsing) in later sections. In detail, we concatenate the two heterogeneous l isr syntactic representations with the SRL input, formulated as xi = embword ⊕ repchar wi wi ⊕ hv ⊕ hi . 4 Experiments and Analysis 4.1 Experimental Setup We conduct experiments on the commonly used Chinese Proposition Bank 1.0 (CPB1.0) (Xue, 2008) and English CoNLL-2005 (Carreras and M`arquez, 2005) benchmarks. We implement our methods and baseline model with Pytorch, and our code, configurations, and models are released in https: //github.com/KiroSummer/HDP-SRL. Heterogeneous Dependency Treebanks. We employ PCTB7 and CDT as the heterogeneous dependency treebanks for Chinese, PTB and UD2 dependency treebanks for English. We employ BiAffine 2 We use the combination of EWT, GUM, LinES, and ParTUT of UD English corpus in our experiments. 2983 Dev Models Predefined predicates. Sha et al. (2016) Xia et al. (2017) Xia et al. (2019a) Xia et a"
2020.coling-main.266,C18-1047,0,0.0252679,"yer would be modified as: ! hlv =ρ X Wl rlu l +b , (5) u∈N (v) l where the weight matrix Wl increases its column dimension by dhidden per layer, i.e., Wl ∈ Rdhidden ×d (dl = d + dhidden × (l − 1)). Implicit Method. Recently popular pre-trained language model embeddings (such as ELMo and BERT) have received much attention. These language models are trained on large amounts of natural text and can produce powerful implicit representations, whose effectiveness is shown in many NLP tasks. Inspired by these pre-trained language model representations and previous works on syntactic representations (Yu et al., 2018; Xia et al., 2019a), we make a trial to train a syntactic parser and extract similar implicit syntactic representations for SRL. We choose the state-of-the-art BiAffine parser (Dozat and Manning, 2017) as our basic dependency parser module. Concisely, BiAffine parser consists of an input layer, BiLSTMs encoder layer, and BiAffine scorers layer, as shown by the right component of Figure 3. We extract the hidden outputs from the 3-layer BiLSTMs encoder of the dependency parser module and make a softmax weighted summation on the outputs as the implicit syntactic representations. 1 The ExpHDP mod"
2020.coling-main.266,D19-1057,1,0.801133,"propose to integrate the dependency path embeddings into the basic SRL model for dependency-based SRL. Strubell et al. (2018) propose a multi-task learning framework based on the self-attention encoder, which treats dependency parsing as an auxiliary task. He et al. (2019) propose an argument pruning method based on dependency tree positions for multilingual SRL. Recently, Xia et al. (2019a) propose a similar framework to extract syntactic representation for SRL, but they only focus on Chinese SRL. Xia et al. (2019b) compares four explicit methods to encode automatic dependency trees for SRL. Zhang et al. (2019) present different methods to encode dependency trees and compare various incorporation ways into a self-attention based SRL model. These previous works mainly focus on encoding single-sourced dependency treebank, which can only provide limited syntactic knowledge. Our work focus on exploiting heterogeneous dependency benchmarks and the results verify our intuition that heterogeneous syntactic knowledge can provide more valid information. 6 Conclusion We propose to encode heterogeneous syntactic knowledge with explicit and implicit methods to help SRL. For the explicit aspect, we propose ExpHD"
2020.coling-main.266,2020.acl-main.297,1,0.824494,"knowledge from heterogeneous dependency treebanks. First, we introduce the method for encoding singleton dependency trees and then detailedly describe the variations to extract heterogeneous syntactic knowledge. 3.1 Syntactic Representation We employ two different methods to fully encode the homogeneous syntactic trees, i.e., GCN that encodes the syntactic structures and implicit representations that encode the syntactic features. Explicit Method. Graph convolutional networks (GCN) are neural networks that work on graph structures, which have been explored in many NLP tasks (Guo et al., 2019; Zhang et al., 2020). Formally, we denote an undirected graph as G = (V, E), where V and E are the set of nodes and edges, respectively. The GCN computation flow of node v ∈ V at l-th layer is defined as ! X l (3) hlv = ρ Wl hl−1 u +b , u∈N (v) where Wl ∈ Rm×m is the weight matrix, bl ∈ Rm is the bias term, N (v) is the set of all one-hop neighbour nodes of v, and ρ is an activation function. Especially, h0u ∈ Rm is the initial input representation, and m is the representation dimension. In our work, we employ a 1-layer BiLSTM encoder over the input layer1 , and treat the BiLSTM outputs as the input of the GCN mo"
2020.coling-main.266,P15-1109,0,0.0199839,"apparently no because integrating syntactic knowledge into pre-trained language models has attracted some attention (Wang et al., 2020). And unitizing heterogeneous syntactic knowledge would be a direct and natural idea, which we leave for future work. 5 Related Work Recently, SRL has achieved significant improvements because of the development of deep learning. Previous works can mostly be divided into two kinds of methods, syntax-agnostic methods, which focus on the SRL problem itself, and syntax-aware methods, which explore various ways to integrate syntactic knowledge into the SRL models. Zhou and Xu (2015) propose to use deep BiLSTMs for English spanbased SRL. He et al. (2017) further employ several deep learning advanced practices into the stacked BiLSTMs. With the rise of Transformer in machine translation, Tan et al. (2018) employ deep self-attention encoder for SRL, achieving strong performance. Marcheggiani et al. (2017) propose a simple and fast model with rich input representations. Cai et al. (2018) present a full end-to-end model which composed of BiLSTM encoder and BiAffine scorer. He et al. (2018a) first treat SRL as a predicate-argument-role tuple identification task. Following the"
2020.coling-main.340,D18-1017,0,0.62385,"character information into word representations based on sequence labeling. Different from English NER, East Asian languages including Chinese are written without explicit word boundary. One intuitive way to solve this problem is to segment the input sentences into words first, and then to apply word sequence labeling (Yang et al., 2016; He and Sun, 2017a). However, such methods suffer from error propagation between these two subtasks. To overcome this limitation, efforts have been devoted to incorporating word information by leveraging lexicon features and gazetteers (Peng and Dredze, 2015; Cao et al., 2018; Wu et al., 2019; Lin et al., 2019). As recent state-of-the-art (SOTA) lattice-based method, Zhang and Yang (2018) integrated matched lexical words information into character sequence with a directed acyclic graph (DAG) structure using lattice LSTM. While obtaining promising results, this model faces two challenges. First, as an extension to the non-parallelizable sequential LSTM to a DAG structured model, lattice LSTM is restricted to preprocess one character at a time, which can make it infeasibly to deploy. Second, due to the inherently unidirectional sequential nature, lattice LSTM fails"
2020.coling-main.340,N13-1006,0,0.0799335,"Missing"
2020.coling-main.340,N19-1423,0,0.671136,"m to enhance the local dependencies among neighboring tokens. The key insight is to modify the self-attention architecture by replacing the fully-connected topology with a pivot-shared structure. In this particular, every two non-neighboring tokens are connected by a shared pivot node to strengthen the dependency for two neighboring tokens. Experimental results on four datasets demonstrate that our model performs up to 11.4 times faster than baselines and achieves better performance. Furthermore, we show that our model can be easily integrated into the pre-trained language model such as BERT (Devlin et al., 2019), and combining them further improves the state of the art. In summary, this paper makes the following contributions: (1) We investigate lattice transformer encoder for Chinese NER, which is capable of handling lattices in batch mode and capturing dependencies between characters and matched lexical words. (2) We revise lattice-aware attention distribution via a porous mechanism, which enhances the ability of capturing useful local context. (3) Experimental results show that the proposed model is effective and efficient. The source code of this paper can be obtained from https://github.com/stra"
2020.coling-main.340,D19-1096,0,0.169166,"Missing"
2020.coling-main.340,E17-2113,0,0.263991,"al (Berger and Lafferty, 2017), relation extraction (Yu et al., 2019) and entity linking (Xue et al., 2019) require the NER as one of their preprocessing components. Recent studies show that English NER models have achieved improved performance by integrating character information into word representations based on sequence labeling. Different from English NER, East Asian languages including Chinese are written without explicit word boundary. One intuitive way to solve this problem is to segment the input sentences into words first, and then to apply word sequence labeling (Yang et al., 2016; He and Sun, 2017a). However, such methods suffer from error propagation between these two subtasks. To overcome this limitation, efforts have been devoted to incorporating word information by leveraging lexicon features and gazetteers (Peng and Dredze, 2015; Cao et al., 2018; Wu et al., 2019; Lin et al., 2019). As recent state-of-the-art (SOTA) lattice-based method, Zhang and Yang (2018) integrated matched lexical words information into character sequence with a directed acyclic graph (DAG) structure using lattice LSTM. While obtaining promising results, this model faces two challenges. First, as an extension"
2020.coling-main.340,N16-1030,0,0.111089,"-aware attention distribution via a porous mechanism, which enhances the ability of capturing useful local context. (3) Experimental results show that the proposed model is effective and efficient. The source code of this paper can be obtained from https://github.com/strawberryx/PLTE. 2 Related Work Our work is in line with NER models based on neural networks and lattice transformer models. Huang et al. (2015) proposed a BiLSTM-CRF model for NER and achieved strong performance. Santos and Guimaraes (2015) used word- and character-level representations based on the CharWNN deep neural network. Lample et al. (2016) designed a character LSTM and word LSTM for NER. Compared to our work, these word-based methods suffer from segmentation errors. 3832 To avoid segmentation errors, most recent NER models are built upon character sequence labeling. Peng and Dredze (2015) proposesd a joint training objective for three types of neural embeddings to better recognize entity boundary. Lu et al. (2016) presented a position-sensitive skip-gram model to learn multi-prototype Chinese character embeddings. He and Sun (2017a) took the positional character embeddings into account. Although these methods achieve promising"
2020.coling-main.340,W06-0115,0,0.44398,"set of manually labeled training data {(Si , yi )}|N i=1 , sentence-level log-likelihood loss with L2 regularization is used to train the model: L= N X log(P (yi |Si )) + i=1 λ k Θ k2 , 2 (7) where λ is the L2 regularization weight and Θ represents the parameter set. 5 Experiments We conduct experiments to investigate the effectiveness of our proposed PLTE method across different domains. Standard precision (P), recall (R) and F1-score (F1) are used as evaluation metrics. 5.1 Experimental Setup 5.1.1 Data We evaluate our model on four datasets, including OntoNotes (Ralph et al., 2011), MSRA (Levow, 2006), Weibo NER (Peng and Dredze, 2015; He and Sun, 2017b) and a Chinese Resume dataset (Zhang and Yang, 2018). We use the same training, valid and test split as (Zhang and Yang, 2018). For these datasets, both OntoNotes and MSRA are in news domain, while Weibo and Resume come from social media. 3836 OntoNotes Input Gold seg No seg Models Che et al. (2013) Wang et al. (2013) Yang et al. (2016) Lattice LSTM (2018) LR-CNN (2019a) CAN-NER(2019) PLTE BERT-Tagger Lattice LSTM[BERT] LR-CNN[BERT] PLTE[BERT] Resume Models Lattice LSTM (2018) CAN-NER(2019) LR-CNN (2019a) PLTE BERT-Tagger Lattice LSTM[BERT]"
2020.coling-main.340,D19-1646,0,0.0553706,"Missing"
2020.coling-main.340,N19-1247,0,0.163269,"ee types of neural embeddings to better recognize entity boundary. Lu et al. (2016) presented a position-sensitive skip-gram model to learn multi-prototype Chinese character embeddings. He and Sun (2017a) took the positional character embeddings into account. Although these methods achieve promising performance, they ignore word information lying in character sequence. Some work exploits rich word boundary and semantic information in character sequence. Cao et al. (2018) applied an adversarial transfer learning framework to integrate the task-shared word boundary information into Chinese NER. Liu et al. (2019) explored four different strategies for Word-Character LSTM. Gui et al. (2019a) proposed a CNN-based NER model that incorporates lexicons using a rethinking mechanism. Recent state-of-the-art methods exploit lattice-structured models to integrate latent word information into character sequence, which has been proven effective on various NLP tasks (Su et al., 2017; Tan et al., 2018) . Specifically, Zhang and Yang (2018) utilized the lattice LSTM to leverage explicit word information over character sequence labeling. Based on this method, Gui et al. (2019b) and Sui et al. (2019) formulated the l"
2020.coling-main.340,L16-1138,1,0.877329,"models. Huang et al. (2015) proposed a BiLSTM-CRF model for NER and achieved strong performance. Santos and Guimaraes (2015) used word- and character-level representations based on the CharWNN deep neural network. Lample et al. (2016) designed a character LSTM and word LSTM for NER. Compared to our work, these word-based methods suffer from segmentation errors. 3832 To avoid segmentation errors, most recent NER models are built upon character sequence labeling. Peng and Dredze (2015) proposesd a joint training objective for three types of neural embeddings to better recognize entity boundary. Lu et al. (2016) presented a position-sensitive skip-gram model to learn multi-prototype Chinese character embeddings. He and Sun (2017a) took the positional character embeddings into account. Although these methods achieve promising performance, they ignore word information lying in character sequence. Some work exploits rich word boundary and semantic information in character sequence. Cao et al. (2018) applied an adversarial transfer learning framework to integrate the task-shared word boundary information into Chinese NER. Liu et al. (2019) explored four different strategies for Word-Character LSTM. Gui e"
2020.coling-main.340,D15-1064,0,0.365373,"formance by integrating character information into word representations based on sequence labeling. Different from English NER, East Asian languages including Chinese are written without explicit word boundary. One intuitive way to solve this problem is to segment the input sentences into words first, and then to apply word sequence labeling (Yang et al., 2016; He and Sun, 2017a). However, such methods suffer from error propagation between these two subtasks. To overcome this limitation, efforts have been devoted to incorporating word information by leveraging lexicon features and gazetteers (Peng and Dredze, 2015; Cao et al., 2018; Wu et al., 2019; Lin et al., 2019). As recent state-of-the-art (SOTA) lattice-based method, Zhang and Yang (2018) integrated matched lexical words information into character sequence with a directed acyclic graph (DAG) structure using lattice LSTM. While obtaining promising results, this model faces two challenges. First, as an extension to the non-parallelizable sequential LSTM to a DAG structured model, lattice LSTM is restricted to preprocess one character at a time, which can make it infeasibly to deploy. Second, due to the inherently unidirectional sequential nature, l"
2020.coling-main.340,P16-2025,0,0.250544,"Missing"
2020.coling-main.340,W15-3904,0,0.0198522,"lattices in batch mode and capturing dependencies between characters and matched lexical words. (2) We revise lattice-aware attention distribution via a porous mechanism, which enhances the ability of capturing useful local context. (3) Experimental results show that the proposed model is effective and efficient. The source code of this paper can be obtained from https://github.com/strawberryx/PLTE. 2 Related Work Our work is in line with NER models based on neural networks and lattice transformer models. Huang et al. (2015) proposed a BiLSTM-CRF model for NER and achieved strong performance. Santos and Guimaraes (2015) used word- and character-level representations based on the CharWNN deep neural network. Lample et al. (2016) designed a character LSTM and word LSTM for NER. Compared to our work, these word-based methods suffer from segmentation errors. 3832 To avoid segmentation errors, most recent NER models are built upon character sequence labeling. Peng and Dredze (2015) proposesd a joint training objective for three types of neural embeddings to better recognize entity boundary. Lu et al. (2016) presented a position-sensitive skip-gram model to learn multi-prototype Chinese character embeddings. He an"
2020.coling-main.340,N18-2074,0,0.0232375,"and “京(Capital)”, although the semantics and boundary information of “南京市(NanJing City)” can be useful knowledge for predicting the tag of “南(South)” as “B-LOC”. In this paper, we address these issues by considering a novel Porous Lattice Transformer Encoder (PLTE). Inspired by previous research on machine translation (Xiao et al., 2019; Sperber et al., 2019), which integrated lattice-structured inputs into self-attention models, we propose a lattice transformer encoder for Chinese NER by introducing lattice-aware self-attention, which borrows the idea from the relative positional embedding (Shaw et al., 2018) to make self-attention aware of the relative position information in lattice structure. Considering that self-attention network calculates attention weights between each pair of tokens in a sequence regardless of their distance, we simply concatenate all the characters and lexical words as input to consume lattices without resorting to the DAG structure. In this way, characters coupled with lexical words can be processed in batches. A lexical word representation is allowed to build a direct relation with the included characters by lattice-aware self-attention, thus addressing the second issue"
2020.coling-main.340,P19-1115,0,0.114745,"onstituent characters ”南(South)” and ”京(Capital)”, resulting in the loss of crucial information for tagging. Besides, lattice LSTM cannot perform batched computation due to the directed acyclic graph input structure. the other two inside characters “南(South)” and “京(Capital)”, although the semantics and boundary information of “南京市(NanJing City)” can be useful knowledge for predicting the tag of “南(South)” as “B-LOC”. In this paper, we address these issues by considering a novel Porous Lattice Transformer Encoder (PLTE). Inspired by previous research on machine translation (Xiao et al., 2019; Sperber et al., 2019), which integrated lattice-structured inputs into self-attention models, we propose a lattice transformer encoder for Chinese NER by introducing lattice-aware self-attention, which borrows the idea from the relative positional embedding (Shaw et al., 2018) to make self-attention aware of the relative position information in lattice structure. Considering that self-attention network calculates attention weights between each pair of tokens in a sequence regardless of their distance, we simply concatenate all the characters and lexical words as input to consume lattices without resorting to the D"
2020.coling-main.340,D19-1396,0,0.0435755,"nto Chinese NER. Liu et al. (2019) explored four different strategies for Word-Character LSTM. Gui et al. (2019a) proposed a CNN-based NER model that incorporates lexicons using a rethinking mechanism. Recent state-of-the-art methods exploit lattice-structured models to integrate latent word information into character sequence, which has been proven effective on various NLP tasks (Su et al., 2017; Tan et al., 2018) . Specifically, Zhang and Yang (2018) utilized the lattice LSTM to leverage explicit word information over character sequence labeling. Based on this method, Gui et al. (2019b) and Sui et al. (2019) formulated the lattice structure as a graph and leveraged Graph Neural Networks (GNNs) to integrate lexical knowledge. However, for the NER task, coupling pre-trained language models such as BERT (Devlin et al., 2019) with GNNs and fine-tuning them can be non-trivial. Lattice transformer has been exploited in NMT (Xiao et al., 2019), as well as speech translation (Sperber et al., 2019; Zhang et al., 2019). Compared with existing work, our proposed porous lattice transformer encoder is different in both motivation and structure. We revise the fully-connected attention distribution with a pivot"
2020.coling-main.340,P19-1176,0,0.0245516,"ion mechanism has attracted increasing attention due to their flexibility in parallel computation and dependency modeling. Given an input sequence representation X = {x1 , · · · , xn } ∈ Rn×d , we can first transform it into queries Q = XWQ ∈ Rn×dk , keys K = XWK ∈ Rn×dk , and values V = XWV ∈ Rn×dv , where {WQ , WK , WV } are trainable parameters. The output sequence representation is calculated as: QKT Att(Q, K, V) = Softmax( √ )V, dk where 3.2 √ (1) dk is the scaling factor. Lattice Transformer Transformer has been used for many NLP tasks, notably machine translation and language modeling (Wang et al., 2019; Devlin et al., 2019). By invoking multi-layer self-attention for global context modeling, Transformer enables paralleled computation and addresses the inherent sequential computation shortcoming of RNNs. Lattice Transformer is a generalization of the standard transformer architecture to accept lattice-structured inputs, it linearizes the lattice structure and introduces a position relation score matrix to make self-attention aware of the topological structure of lattice: Att(Q, K, V) = Softmax( QKT + R √ )V, dk (2) where R ∈ Rn×n encodes the lattice-dependent relations between each pair of e"
2020.coling-main.340,P19-1298,0,0.175846,"ng City)” and its constituent characters ”南(South)” and ”京(Capital)”, resulting in the loss of crucial information for tagging. Besides, lattice LSTM cannot perform batched computation due to the directed acyclic graph input structure. the other two inside characters “南(South)” and “京(Capital)”, although the semantics and boundary information of “南京市(NanJing City)” can be useful knowledge for predicting the tag of “南(South)” as “B-LOC”. In this paper, we address these issues by considering a novel Porous Lattice Transformer Encoder (PLTE). Inspired by previous research on machine translation (Xiao et al., 2019; Sperber et al., 2019), which integrated lattice-structured inputs into self-attention models, we propose a lattice transformer encoder for Chinese NER by introducing lattice-aware self-attention, which borrows the idea from the relative positional embedding (Shaw et al., 2018) to make self-attention aware of the relative position information in lattice structure. Considering that self-attention network calculates attention weights between each pair of tokens in a sequence regardless of their distance, we simply concatenate all the characters and lexical words as input to consume lattices wit"
2020.coling-main.340,D18-1475,0,0.0282653,"-attention aware of the relative position information in lattice structure. Considering that self-attention network calculates attention weights between each pair of tokens in a sequence regardless of their distance, we simply concatenate all the characters and lexical words as input to consume lattices without resorting to the DAG structure. In this way, characters coupled with lexical words can be processed in batches. A lexical word representation is allowed to build a direct relation with the included characters by lattice-aware self-attention, thus addressing the second issue. Some work (Yang et al., 2018; Yang et al., 2019) demonstrates that self-attention benefits from locality modeling, especially for the NER task. As we can see from the example in Figure 1, the word “位 于(Locates In)” is the immediate and most obvious feature to guide the neighboring character “桥(Bridge)” to be identified as “E-LOC” instead of “E-PER”, while ”中国(China)” has no contribution to this decision. Given this observation, we further introduce a novel porous mechanism to enhance the local dependencies among neighboring tokens. The key insight is to modify the self-attention architecture by replacing the fully-connec"
2020.coling-main.340,N19-1407,0,0.0213322,"the relative position information in lattice structure. Considering that self-attention network calculates attention weights between each pair of tokens in a sequence regardless of their distance, we simply concatenate all the characters and lexical words as input to consume lattices without resorting to the DAG structure. In this way, characters coupled with lexical words can be processed in batches. A lexical word representation is allowed to build a direct relation with the included characters by lattice-aware self-attention, thus addressing the second issue. Some work (Yang et al., 2018; Yang et al., 2019) demonstrates that self-attention benefits from locality modeling, especially for the NER task. As we can see from the example in Figure 1, the word “位 于(Locates In)” is the immediate and most obvious feature to guide the neighboring character “桥(Bridge)” to be identified as “E-LOC” instead of “E-PER”, while ”中国(China)” has no contribution to this decision. Given this observation, we further introduce a novel porous mechanism to enhance the local dependencies among neighboring tokens. The key insight is to modify the self-attention architecture by replacing the fully-connected topology with a"
2020.coling-main.340,P18-1144,1,0.939307,"Asian languages including Chinese are written without explicit word boundary. One intuitive way to solve this problem is to segment the input sentences into words first, and then to apply word sequence labeling (Yang et al., 2016; He and Sun, 2017a). However, such methods suffer from error propagation between these two subtasks. To overcome this limitation, efforts have been devoted to incorporating word information by leveraging lexicon features and gazetteers (Peng and Dredze, 2015; Cao et al., 2018; Wu et al., 2019; Lin et al., 2019). As recent state-of-the-art (SOTA) lattice-based method, Zhang and Yang (2018) integrated matched lexical words information into character sequence with a directed acyclic graph (DAG) structure using lattice LSTM. While obtaining promising results, this model faces two challenges. First, as an extension to the non-parallelizable sequential LSTM to a DAG structured model, lattice LSTM is restricted to preprocess one character at a time, which can make it infeasibly to deploy. Second, due to the inherently unidirectional sequential nature, lattice LSTM fails to incorporate the word-level semantics into the representation of the characters except for the last character in"
2020.coling-main.340,P19-1649,0,0.0189236,"al., 2018) . Specifically, Zhang and Yang (2018) utilized the lattice LSTM to leverage explicit word information over character sequence labeling. Based on this method, Gui et al. (2019b) and Sui et al. (2019) formulated the lattice structure as a graph and leveraged Graph Neural Networks (GNNs) to integrate lexical knowledge. However, for the NER task, coupling pre-trained language models such as BERT (Devlin et al., 2019) with GNNs and fine-tuning them can be non-trivial. Lattice transformer has been exploited in NMT (Xiao et al., 2019), as well as speech translation (Sperber et al., 2019; Zhang et al., 2019). Compared with existing work, our proposed porous lattice transformer encoder is different in both motivation and structure. We revise the fully-connected attention distribution with a pivot-shared structure via the porous mechanism to enhance the local dependencies among neighboring tokens.1 To our knowledge, we are the first to design a lattice transformer for Chinese NER. 3 Background In this section, we first briefly review the self-attention mechanism, then move on to current lattice Transformer that our PLTE model is built upon. 3.1 Self-Attention Self-attention mechanism has attracted"
2020.coling-main.340,N19-1342,0,0.073775,"Missing"
2020.coling-main.377,D18-1338,0,0.288652,"ment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network (Wang et al., 2019b), or tree-like hierarchical merge (Dou et al., 2018). Moreover, the second makes each decoder layer explicitly align to a parallel encoder layer (He et al., 2018) or all encoder layers (Bapna et al., 2018). However, the above methods either complicate the original model (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b; Bapna et al., 2018) or limit the model’s flexibility, such as requiring the number of the encoder layers to be the same as the decoder layers (He et al., 2018). Instead, in this work, we propose layer-wise multi-view learning to address this problem from the perspective of model training, without changing the model structure. Our method’s highlight is that ∗ Work done during Ph.D. study at Northeastern University. Corresponding author. This work is licensed under a Creati"
2020.coling-main.377,P19-1425,0,0.0227968,"rm Transformer with a 12-layer encoder. The results were measured on the IWSLT’14 De→En validation set. i=0 indicates the embedding layer. to the primary view by feeding an auxiliary view. Figure 2 shows that the vector similarity between the i-th encoder layer and the topmost layer grows as the increase of i. Therefore, we can regard the middle layer’s auxiliary view as a noisy version of the primary view. Training with noises has been widely proven to effectively improve the model’s generalization ability, such as dropout (Srivastava et al., 2014), adversarial training (Miyato et al., 2017; Cheng et al., 2019) etc. We also experimentally confirm that our model is more robust than the single view model when injecting random noises into the encoding representation. Dark knowledge. Typically, the prediction target in Lnll is a one-hot distribution: Only the gold label is 1, while the others are 0. A better alternative is label smoothing (Szegedy et al., 2016), which reduces the probability of gold label by  and redistributes  to all non-gold labels on average. However, label smoothing ignores the relationship between non-gold labels. For example, if the current ground-truth is “improve”, then “promo"
2020.coling-main.377,D18-1217,0,0.12509,"e input data (Xu et al., 2013), where one of the keys is view construction. In our scenario, a view is the hidden representation of the input sentence (an array of hidden vectors for each token, e.g., H ∗ ). In this work, we further propose to take the off-the-shelf output of each encoder layer (i.e., H (l) 2 ) to construct the redundant views. In NLP, previous implementations of view construction generally require the model to recalculate on the reconstructed input, such as using different orders of n-grams in the bag-of-word model (Matsubara et al., 2005), randomly masking the input tokens (Clark et al., 2018). As opposed to them, our method is very cheap as the by-product of the standard layer-by-layer encoding process. According to the definition of a view, we can regard the vanilla Transformer as a single-view model since only the topmost encoder layer (also called primary view) is fed to the decoder. In contrast, MV-Transformer additionally contains an intermediate layer Ma (1 ≤ Ma < M ) as the auxiliary view 3 . The choice of Ma can be arbitrary, and we discuss its effect in § 4.2. Our goal is to learn a better single model with the help of the auxiliary view. Partially shared parameters. In t"
2020.coling-main.377,D18-1457,0,0.322233,"); (2) It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network (Wang et al., 2019b), or tree-like hierarchical merge (Dou et al., 2018). Moreover, the second makes each decoder layer explicitly align to a parallel encoder layer (He et al., 2018) or all encoder layers (Bapna et al., 2018). However, the above methods either complicate the original model (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b; Bapna et al., 2018) or limit the model’s flexibility, such as requiring the number of the"
2020.coling-main.377,D16-1139,0,0.0284317,"l ensemble MV-Transformer can be thought of as consisting of two models: A large model as the primary view, and a small model (with shallower encoder) as the auxiliary view. Here we compare with the other three methods of integrating multiple models: • Oneway-KD. Similar to Eq. 7 but detach the teacher’s prediction, i.e., gradients of the teacher’s prediction is not tracked, posing a one-way transfer from primary view to auxiliary view. • Seq-KD. Train the large model first and then translate the original training set by beam search to construct the distilled training set for the small model (Kim and Rush, 2016). • Ensemble. Independently train the two models and combine their predictions at inference time, e.g., by algorithmic average. Experiments are done on IWSLT’14 De→En, where the small model has a 3-layer encoder. As shown in Table 2, we can see that: (1) Oneway-KD suffers from severe degradation than MV when detaching the primary view, which indicates that making mutual learning between the primary view and auxiliary view is critical; (2) Seq-KD is almost useless or even badly hurts the performance (vs. Baseline (3L)), which is against the previous belief that Seq-KD helps the small model a lo"
2020.coling-main.377,W04-3250,0,0.0198279,"le 1: BLEU scores on five translation tasks. For (deep) Transformer, Aux./Pri. denotes the independently trained model with Ma /M -layer encoder respectively. For (deep) MV-Transformer, Aux./Pri. denotes the used view at inference time. ∆ denotes the improved BLEU score over the Transformer baseline when using multi-view learning at the same encoder depth. † denotes our implementation. Boldface and ∗ represent local and global best results, respectively. All the MV-Transformer results are significantly better (p<0.01) than the Transformer counterparts, measured by paired bootstrap resampling (Koehn, 2004). uses sacrebleu 10 , all other datasets are evaluated by multi-bleu.perl. Only De→En is reported by case insensitive BLEU. 3.2 Main results In addition to Transformer, we also re-implemented three previously proposed models that incorporate multiple encoder layers: multi-layer representation fusion (MLRF)(Wang et al., 2018), hierarchical aggregation (HieraAgg) (Dou et al., 2018), and transparent attention (TA) (Bapna et al., 2018). Table 1 shows the results of the five translation tasks on PostNorm and PreNorm. First, our MV-Transformer outperforms all baselines across the board. Specifically"
2020.coling-main.377,N18-1202,0,0.027903,"der finds a multi-layer representation of the source sentence, and the decoder queries the topmost encoding representation to produce the target sentence through a cross-attention mechanism (Wu et al., 2016; Vaswani et al., 2017). However, such overreliance on the topmost encoding layer is problematic in two aspects: (1) Prone to over-fitting, especially when the encoder is under-trained, such as in low-resource tasks (Wang et al., 2018); (2) It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network ("
2020.coling-main.377,W18-5431,0,0.0247007,"er representation of the source sentence, and the decoder queries the topmost encoding representation to produce the target sentence through a cross-attention mechanism (Wu et al., 2016; Vaswani et al., 2017). However, such overreliance on the topmost encoding layer is problematic in two aspects: (1) Prone to over-fitting, especially when the encoder is under-trained, such as in low-resource tasks (Wang et al., 2018); (2) It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network (Wang et al., 2019b), or tree-li"
2020.coling-main.377,P19-1021,0,0.0615377,"Missing"
2020.coling-main.377,C18-1255,1,0.872017,"odel. 1 Introduction Neural Machine Translation (NMT) adopts the encoder-decoder paradigm to model the entire translation process (Bahdanau et al., 2015). Specifically, the encoder finds a multi-layer representation of the source sentence, and the decoder queries the topmost encoding representation to produce the target sentence through a cross-attention mechanism (Wu et al., 2016; Vaswani et al., 2017). However, such overreliance on the topmost encoding layer is problematic in two aspects: (1) Prone to over-fitting, especially when the encoder is under-trained, such as in low-resource tasks (Wang et al., 2018); (2) It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; D"
2020.coling-main.377,P19-1176,1,0.913927,"ake full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network (Wang et al., 2019b), or tree-like hierarchical merge (Dou et al., 2018). Moreover, the second makes each decoder layer explicitly align to a parallel encoder layer (He et al., 2018) or all encoder layers (Bapna et al., 2018). However, the above methods either complicate the original model (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b; Bapna et al., 2018) or limit the model’s flexibility, such as requiring the number of the encoder layers to"
2020.coling-main.377,P19-1624,0,0.0591465,"ake full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network (Wang et al., 2019b), or tree-like hierarchical merge (Dou et al., 2018). Moreover, the second makes each decoder layer explicitly align to a parallel encoder layer (He et al., 2018) or all encoder layers (Bapna et al., 2018). However, the above methods either complicate the original model (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b; Bapna et al., 2018) or limit the model’s flexibility, such as requiring the number of the encoder layers to"
2020.coling-main.85,abbasi-etal-2014-benchmarking,0,0.0298167,"tive examples are correctly predicted by less than four models, and 147 negative examples are wrongly predicted by all eight models experimented. This error set contains some extremely difficult cases, such as “what goes up goes down and vice versa”, which flips its polarity for many times; and “the downtrend is not because [...] is a bad investment”, which express a positive sentiment with all negative words. Although not all the errors from the StockSen dataset are explainable, the majority are explainable and we found the six interesting error types as follows. Unlike the error analysis by Abbasi et al. (2014), which generally focused on machine learning features and is specific to Twitter data, our analysis emphasizes more on linguistic phenomena. One may ask whether these error types really afflict the sentiment analysis performance and are specific only to the finance domain. Therefore, we also look into 237 positive and 147 negative examples randomly sampled from the “overlapping errors”, and 384 random texts from the test population on the Yelp dataset. Indeed, we find more significant concentrations of these linguistic features in the errors from financial domain. Table 3 shows the estimated"
2020.coling-main.85,W19-5501,1,0.823644,"atterns, and (2) there are six types of linguistic features that are pervasive in the common errors. These findings provide important clues and practical considerations for improving sentiment analysis models for financial applications. 1 Introduction Natural language processing has been widely used for financial applications in recent years. These applications include stock market / foreign exchange market prediction, volatility modeling, asset allocation, business taxonomy construction, credit scoring, initial public offering valuation (IPO), and more (Ding et al., 2015; Xing et al., 2018a; Bai et al., 2019; Xing et al., 2019; Yang et al., 2020; Xing et al., 2021). Among them, there are mainly two threads of method to process textual inputs: the first is to directly encode financial texts by neural nets and to use the representations for learning downstream tasks (Xu and Cohen, 2018); the second is to analyze financial texts with critical linguistic features such as content semantics (Keith and Stent, 2019) or investors’ sentiment (Malandri et al., 2018), for the sake of interpretability. The objective of financial sentiment analysis (FSA) is to classify a piece of financial text as expressing b"
2020.coling-main.85,N19-1423,0,0.0128965,"i+1 i−1 i where ξt = [hi−k t−1 , ..., ht−1 , ht−1 , ht−1 , ..., ht−1 ], k is the window size for controlling information exchange between neighboring words. The value of gt is computed from hit−1 for all word index ¯ = avg(hi ) is the input. Finally, the i, i.e., gt is the hidden state for an LSTM cell, where h t−1 classification layer takes global g as the sentence representation, p(Si ) = softmax(W · g + b). 8. BERT: a powerful representation learning model that uses a transformer network to pre-train a language model with a masked tokens prediction task and a next sentence prediction task (Devlin et al., 2019). We fine-tune the 340M parameters model released by Google with the vocabularies of the training sets. Next, we use the cross entropy loss to train a binary classifier that takes the BERT context embedding for sentence Si . 3 Experiments In this section, we provide further details on the datasets, experimental settings that produce the classification results, and evaluation metrics. 3.1 Datasets We conduct sentiment analysis on two datasets to enable a comparison: (1) the Yelp dataset by Zhang et al. (2015) for the business review domain and, (2) the StockTwits Sentiment (StockSen) dataset1 f"
2020.coling-main.85,D17-1169,0,0.0325214,"il is equally important. In this paper, we explore the FSA behavior of some of the most common sentiment analysis models and the interpretability problem by using error visualization and linguistic analysis. Unlike many of This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 978 Proceedings of the 28th International Conference on Computational Linguistics, pages 978–987 Barcelona, Spain (Online), December 8-13, 2020 the domain adaptation-based FSA effort where only distant supervision is available (Felbo et al., 2017), we resort to self-labeling. Noticing that some financial social platforms allow users to simultaneously label their post as bullish (positive) or bearish (negative), such platforms make it possible for us to collect financial texts with high-quality sentiment labels in a “crowd-sourced” manner. We also compare performance of the same models on both the finance domain (StockSen) and the general business review domain (Yelp). We are specifically interested in the following research questions. RQ1: Do different sentiment analysis methods step into the same pitfalls (i.e., make the common errors"
2020.coling-main.85,E17-2068,0,0.0197802,"cast for analyzing financial texts by Loughran and McDonald (2011). The L&M dictionary is used the same way as per OpinionLex. 4. SVM: a robust and classical model for binary text classification. We only use term frequencyinverse document frequency (TF-IDF) features, i.e., the input is a TF-IDF vector of the token-size length. p(Si ) = SVM(TF-IDF||w ||). 5. fastText: a CBOW-like model that also considers sub-word information (n-gram features), hence the token number increases to predict a categorical output. The model is claimed to be on par with deep learning models for text classification (Joulin et al., 2017). 979 6. bi-LSTM: a model for representation learning that concatenates LSTM hidden states from both directions of a sentence to mitigate the problem of memory decay (Schuster and Paliwal, 1997). bi-LSTM has a forward and a backward component. For the forward component, → − it = σ(Wi · [ h t−1 , xt ] + bi ) → − ft = σ(Wf · [ h t−1 , xt ] + bf ) → − ot = σ(Wo · [ h t−1 , xt ] + bo ) → − ct = ft ct−1 + it (Wc · [ h t−1 , xt ] + bc ) → − h t−1 = ot tanh(ct−1 ), (1) where it , ot , ft , ct are values of the input gate, output gate, forget gate, and cell state respectively; → − σ denotes the sigmoi"
2020.coling-main.85,P19-1047,0,0.0187113,"e market prediction, volatility modeling, asset allocation, business taxonomy construction, credit scoring, initial public offering valuation (IPO), and more (Ding et al., 2015; Xing et al., 2018a; Bai et al., 2019; Xing et al., 2019; Yang et al., 2020; Xing et al., 2021). Among them, there are mainly two threads of method to process textual inputs: the first is to directly encode financial texts by neural nets and to use the representations for learning downstream tasks (Xu and Cohen, 2018); the second is to analyze financial texts with critical linguistic features such as content semantics (Keith and Stent, 2019) or investors’ sentiment (Malandri et al., 2018), for the sake of interpretability. The objective of financial sentiment analysis (FSA) is to classify a piece of financial text as expressing bullish or bearish opinions toward certain arguments. Although sentiment analysis in the general domain is extensively studied in the past decades (Cambria et al., 2013), FSA is a challenging task because of the lack of large-scale training data and the difficulty in labeling after acquiring the texts, which involves expert knowledge. As a result, the model performance for FSA are usually significantly wor"
2020.coling-main.85,D09-1019,0,0.0683084,"Missing"
2020.coling-main.85,W15-0115,0,0.0607124,"Missing"
2020.coling-main.85,D14-1162,0,0.0883174,"ones. In contrast, the Yelp dataset is balanced. We keep these prior distributions to see whether it affects the evaluation. 3.2 Experimental Setup The lexicon-based models (OpinionLex, SenticNet, and L&M), as this name manifests, do not make use of the sentiment labels, hence they are deterministic and training-free. For them, we follow (Taboada et al., 2011) for handling (double) negations. The SVM implementation uses the default regularization and kernel of Scikit-learn (Pedregosa et al., 2011). Our implementation of both bi-LSTM and S-LSTM use the 6B-tokens-uncased-300d GloVe embeddings (Pennington et al., 2014). All the three deep models are trained with an Adam optimizer (Kingma and Ba, 2015) with the initial learning rate equal to 2e−5 . For BERT, the training and testing batch sizes are set to 24 and 8 for efficiency. Among the eight models, OpinionLex, L&M, SVM, and fastText are bag-of-words type models while the rest consider syntactic or sequential features of a text. To facilitate fair comparisons between different methods in a real environment, we apply minimum pre-processing for the tweets. The texts are uncased and URLs are represented as single tokens. Stock tickers, emojis, microtext, an"
2020.coling-main.85,P19-1034,0,0.0168976,"general domain is extensively studied in the past decades (Cambria et al., 2013), FSA is a challenging task because of the lack of large-scale training data and the difficulty in labeling after acquiring the texts, which involves expert knowledge. As a result, the model performance for FSA are usually significantly worse than using the same sentiment analysis model for the general domain (which is referred to as a problem of domain adaptation). In addition to the above-mentioned challenges, FSA requires more interpretability comparing to sentiment analysis in other domains (Luo et al., 2018; Sedinkina et al., 2019). Early sentiment analysis studies leveraged on textual data from movie reviews, product reviews, and social media posts. In these applications, the purpose being roughly understanding customer feedbacks, statistically aggregating straightforward opinions suffices and a single mistake does not make much difference. Whereas for financial applications, a fraction of bad sentiment analysis results may cause extreme loss, hence have to be carefully treated with exception. Therefore, the goal of FSA is more than obtaining a high accuracy number: understanding when and why the method would fail is e"
2020.coling-main.85,J11-2001,0,0.0122193,"s barely possible. The quality ensures the high confidence level of metrics even though the size of the StockSen dataset is much smaller than the Yelp dataset. The StockSen dataset is imbalanced by nature: positive texts posted triples the number of negative ones. In contrast, the Yelp dataset is balanced. We keep these prior distributions to see whether it affects the evaluation. 3.2 Experimental Setup The lexicon-based models (OpinionLex, SenticNet, and L&M), as this name manifests, do not make use of the sentiment labels, hence they are deterministic and training-free. For them, we follow (Taboada et al., 2011) for handling (double) negations. The SVM implementation uses the default regularization and kernel of Scikit-learn (Pedregosa et al., 2011). Our implementation of both bi-LSTM and S-LSTM use the 6B-tokens-uncased-300d GloVe embeddings (Pennington et al., 2014). All the three deep models are trained with an Adam optimizer (Kingma and Ba, 2015) with the initial learning rate equal to 2e−5 . For BERT, the training and testing batch sizes are set to 24 and 8 for efficiency. Among the eight models, OpinionLex, L&M, SVM, and fastText are bag-of-words type models while the rest consider syntactic or"
2020.coling-main.85,P18-1183,0,0.0139743,"has been widely used for financial applications in recent years. These applications include stock market / foreign exchange market prediction, volatility modeling, asset allocation, business taxonomy construction, credit scoring, initial public offering valuation (IPO), and more (Ding et al., 2015; Xing et al., 2018a; Bai et al., 2019; Xing et al., 2019; Yang et al., 2020; Xing et al., 2021). Among them, there are mainly two threads of method to process textual inputs: the first is to directly encode financial texts by neural nets and to use the representations for learning downstream tasks (Xu and Cohen, 2018); the second is to analyze financial texts with critical linguistic features such as content semantics (Keith and Stent, 2019) or investors’ sentiment (Malandri et al., 2018), for the sake of interpretability. The objective of financial sentiment analysis (FSA) is to classify a piece of financial text as expressing bullish or bearish opinions toward certain arguments. Although sentiment analysis in the general domain is extensively studied in the past decades (Cambria et al., 2013), FSA is a challenging task because of the lack of large-scale training data and the difficulty in labeling after"
2020.coling-main.85,C18-1085,0,0.0245096,"and five machine learning-based models on the sentiment classification task on two datasets, we show that the machine learning-based models, e.g., SVM and BERT, usually make more false positive errors than false negatives; (3) we describe six error types which contribute to the understanding of linguistic features of tweets from the finance domain; (4) we introduce a new corpus (StockSen) for the FSA task. 2 Compared Models for the FSA Task Sentiment analysis models may be trained from label supervisions, leverage external knowledge about word polarities, or been a hybrid of those techniques (Ye et al., 2018). To have a good coverage of different types, we choose to investigate eight representative models from three clusters, i.e., lexicon-based (OpinionLex, SenticNet, and L&M), machine learning-based (SVM and fastText), and deep learning NLP models (bi-LSTM, S-LSTM, and BERT) as elaborated below. Formally, each financial text T consists of k sentences S1 , S2 , ..., Sk , where Si is a word sequence wi1 , wi2 , ..., wit . The FSA model outputs an average of binary sentence polarities: p(T ) = avg(p(Si )). 1. OpinionLex: a list of around 6,800 sentiment-carrying words collected by Hu and Liu (2004)"
2020.coling-main.85,P18-1030,1,0.789024,"ot , ft , ct are values of the input gate, output gate, forget gate, and cell state respectively; → − σ denotes the sigmoid function; xt is the word embedding for word wit ; h t−1 is the hidden state of the previous time step t − 1; W() denotes the state transfer matrices and b() is the bias. A different set of parameters are used for the backward component, which reads the sentence from wit to wi0 . → − ← − Finally, we calculate p(Si ) = softmax(W · [ h t+1 ; h 0 ] + b). 7. S-LSTM: a recent LSTM variant for encoding text with a parallel “sentence-level sub state gt ” for → − each time step (Zhang et al., 2018). Formally, for the S-LSTM model, [ h t−1 , xt ] in Equation (1) is replaced by [ ξt , xt , gt−1 ] and ct takes information flow from the left context cell clt−1 , the right context cell crt−1 , and the sentence context cell cgt−1 , i.e., ct = ft ct−1 + lt clt−1 + rt crt−1 + st cgt−1 + it (Wc · [ ξt , xt , gt−1 ] + bc ), (2) i+k i+1 i−1 i where ξt = [hi−k t−1 , ..., ht−1 , ht−1 , ht−1 , ..., ht−1 ], k is the window size for controlling information exchange between neighboring words. The value of gt is computed from hit−1 for all word index ¯ = avg(hi ) is the input. Finally, the i, i.e., gt is"
2020.emnlp-main.33,D19-1307,0,0.0450857,"Missing"
2020.emnlp-main.33,N18-1150,0,0.0228236,"ce-to-sequence architecture for abstractive summarization. Subsequently, Transformer was used and outperformed traditional abstractive summarizer by ROUGE scores (Duan et al., 2019). Techniques such as AMR parsing (Liu et al., 2015), copy (Gu et al., 2016), coverage (Tu et al., 2016; See et al., 2017), smoothing (M¨uller et al., 2019) and pre-training (Lewis et al., 2019; Liu and Lapata, 2019) were also examined to enhance summarization. Hybrid abstractive and extractive methods adopt a two-step approach including content selection and text generation (Gehrmann et al., 2018; Hsu et al., 2018; Celikyilmaz et al., 2018), achieving higher performance than end-to-end models in ROUGE. 1. Preneural vs Neural: Traditional rule-based methods are still strong baselines given powerful neural architectures. 2. Extractive vs Abstractive: Under similar settings, extractive approaches outperform abstractive models in general. The main shortcoming is unnecessity for extractive models, and omission and intrinsic hallucination for abstractive models. 3. Milestone Techniques: Copy works effectively in reproducing details. It also reduces duplication on the word level but tends to cause redundancy to a certain degree. Covera"
2020.emnlp-main.33,D18-1443,0,0.12087,"cs on the Accuracy and Fluency aspects. Models are analyzed by the overall error counts on a test set according to each metric, and therefore our evaluation can be more informative and objective compared with existing manual evaluation reports. We call this set of metrics PolyTope. Using PolyTope, we manually evaluate 10 text summarizers including Lead-3, TextRank (Mihalcea and Tarau, 2004), Sequenceto-sequence with Attention (Rush et al., 2015), SummaRuNNer (Nallapati et al., 2017), PointGenerator (See et al., 2017), Point-Generator-withCoverage (Tu et al., 2016; See et al., 2017), BottomUp (Gehrmann et al., 2018), BertSumExt (Liu and Lapata, 2019), BertSumExtAbs (Liu and Lapata, 2019) and BART (Lewis et al., 2019), through 446 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 446–469, c November 16–20, 2020. 2020 Association for Computational Linguistics which we compare neural structures with traditional preneural ones, and abstractive models with their extractive counterparts, discussing the effectiveness of frequently-used techniques in summarization systems. Empirically, we find that: et al., 2017; Xu and Durrett, 2019). Most recently, Zhong et al. (2020"
2020.emnlp-main.33,N18-1065,0,0.404629,"2019; Balachandran et al., 2020) summarization systems. Although improved ROUGE scores have been reported on standard benchmarks such as Gigaword (Graff et al., 2003), NYT (Grusky et al., 2018) and CNN/DM (Hermann et al., 2015) over the years, it is commonly accepted that the quality of machine-generated summaries still falls far behind human written ones. As a part of the reason, ROUGE has been shown insufficient as a precise indicator on summarization quality evaluation (Liu and Liu, 2008; B¨ohm et al., 2019). In the research literature, human evaluation has been conducted as a complement (Narayan et al., 2018). However, human evaluation reports that accompany ROUGE scores are limited in scope and coverage. On a fine-grained level, it still remains uncertain what we have achieved overall and what fundamental changes each milestone technique has brought. We aim to address the above issues by quantifying the primary sources of errors over representative models. In particular, following MQM (Mariana, 2014), we design 8 metrics on the Accuracy and Fluency aspects. Models are analyzed by the overall error counts on a test set according to each metric, and therefore our evaluation can be more informative"
2020.emnlp-main.33,N04-1019,0,0.216123,"Nallapati et al. (2017) and Zhou et al. (2018), respectively. ically consist of 2 to 3 aspects such as informativeness, fluency and succinctness. Recently, Maynez et al. (2020) conducted a human evaluation of 5 neural abstractive models on 500 articles. Their main goal is to verify the faithfulness and factuality in abstractive models. In contrast, we evaluate both rule-based baselines and extractive/abstractive summarizers on 8 error metrics, among which faithfulness and factuality are included. Our work is also related to research on human evaluation for summarization. To this end, Pyramid (Nenkova and Passonneau, 2004) scores a summarizer based on its system output and multiple references. Annotators are requested to identify the smallest content units of semantic meaning, and then associate each unit with a weight by counting how many reference summaries contain this unit. The score of a summary is computed according to the number and weight of units. In addition to Pyramid, there are human evaluation metrics based on ranking (Narayan et al., 2018), best-worst scaling (Kiritchenko and Mohammad, 2017) and question answering (Clarke and Lapata, 2010). The above methods assign one score to each summarization"
2020.emnlp-main.33,D17-1238,0,0.0242221,"xtractive summarization. Kryscinski et al. (2019) evaluated the overall quality of summarization in terms of redundancy, relevance and informativeness. All the above rely on automatic evaluation metrics. Our work is in line with these efforts in that we conduct a fine-grained evaluation on various aspects. Different from the above work, we use human evaluation instead of automatic evaluation. In fact, while yielding rich conclusions, the above analytical work has also exposed deficiencies of automatic toolkits. The quality of automatic evaluation is often criticized by the research community (Novikova et al., 2017; Zopf, 2018) for its insufficiency in neither permeating into the overall quality of generation-based texts (Liu and Liu, 2008) nor correlating with human judgements (Kryscinski et al., 2019). There has also been analysis work augmenting ROUGE with human evaluation (Narayan et al., 2018; Liu and Lapata, 2019). Such work reports coarse-grained human evaluation scores which typ447 Methods ROUGE ROUGE-1 ROUGE-2 ROUGE-L Lead-3 39.20 15.70 35.50 Extractive Methods TextRank Summa 40.20 39.60 17.56 16.20 36.44 35.30 BertExt 43.25 20.24 39.63 S2S 31.33 11.81 28.80 PG 36.44 15.66 33.42 Abstractive Met"
2020.emnlp-main.451,P19-1284,0,0.349471,"spect-level task, a different structure should be ideally learned for each aspect. As shown in Figure 1(b) and 1(c), when given the sentence “the portions are small but being that the food was so good makes up for that.”, ideal structures for the aspects “portions” and “food” can consist of links relevant to the terms and their opinion words only, without introducing additional information. We empirically investigate three different methods for inducing semantic dependencies, including attention (Vaswani et al., 2017), sparse attention (Correia et al., 2019) and hard Kuma discrete structures (Bastings et al., 2019). In particular, attention has been used as a soft alignment structure for tasks such as machine translation (Bahdanau et al., 2014), and sparse attention has been used for text generation (Martins et al., 2020). The Hard Kumaraswamy distribution has been used to induce discrete structures with full differentiability (Bastings et al., 2019). We build a unified self-attentivenetwork (SAN) framework (Vaswani et al., 2017) for investigating the three structure induction methods, using a graph convolutional network on top of the induced aspect-specific structure for aspect level sentiment classifi"
2020.emnlp-main.451,D19-1223,0,0.0407061,"Missing"
2020.emnlp-main.451,P19-1551,0,0.0192435,"plained → not apologetic, though semantically they are directly related. One intuitive solution to the aforementioned problems is to automatically induce semantic structures during the optimization process for sentiment 5596 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5596–5607, c November 16–20, 2020. 2020 Association for Computational Linguistics classification. To this end, existing work has investigated latent structures sentence-level sentiment classification (Yogatama et al., 2016; Kim et al., 2017; Choi et al., 2017; Zhang et al., 2018; Corro and Titov, 2019), but no existing work has considered aspect-level sentiment classification. For the aspect-level task, a different structure should be ideally learned for each aspect. As shown in Figure 1(b) and 1(c), when given the sentence “the portions are small but being that the food was so good makes up for that.”, ideal structures for the aspects “portions” and “food” can consist of links relevant to the terms and their opinion words only, without introducing additional information. We empirically investigate three different methods for inducing semantic dependencies, including attention (Vaswani et a"
2020.emnlp-main.451,N19-1423,0,0.121746,"ulti-hop attention and memory networks to correlate an aspect with its opinion words. Zhang et al. (2016) design gating mechanisms to select useful contextual information for each target. Attention networks are further explored by sequent work (Ma et al., 2017; Liu and Zhang, 2017). Li et al. (2018a) used targetspecific transformation networks to learn targetspecific word representations. Liang et al. (2019) used aspect-guided recurrent transition networks to generate aspect-specific sentence representations. Sun et al. (2019a) constructed aspect related auxiliary sentences as inputs to BERT (Devlin et al., 2019) for strong contextual encoders. Xu et al. (2019) proposed BERT-based post training for enhancing domain-specific contextual representations for aspect sentiment analysis. Recently, there is a line of work considering dependency tree information for ATSC. Lin et al. (2019) proposed deep mask memory network based on dependency trees. Zhang et al. (2019) and Sun et al. (2019b) encoded dependency tree using GCNs for aspect-level sentiment analysis. Zhao et al. (2019) used GCNs to model fully connected graphs between aspect terms, so that all targets can be classified using a shared representation"
2020.emnlp-main.451,P14-2009,0,0.521383,"tly better results than models without using latent graphs. 1 i complained to the manager , but he was not even apologetic (a) An example dependency tree from Stanford CoreNLP parser2 . the portions are small but being that the food was so good makes up for that . (b) A latent graph for the aspect term “portion”. the portions are small but being that the f ood was so good makes up for that . (c) A latent graph for the aspect term “food”. Introduction Aspect-level sentiment analysis aims to classify the sentiment polarities towards specific aspect terms in a given sentence (Jiang et al., 2011; Dong et al., 2014; Vo and Zhang, 2015). Aspects are also called opinion targets, which can typically be product or service features in customer reviews. For example, in the user comment “The environment is romantic, but the food is horrible”, the sentiments of the two aspects “environment” and “food” are positive and negative, respectively. The main challenge of aspect-level sentiment analysis is to effectively model the interaction between the aspect and its surrounding contexts. For example, identifying that “romantic” instead of “horrible” as the opinion word is the key to correctly classifying the sentimen"
2020.emnlp-main.451,N19-1259,0,0.0570794,"es than models without using latent graphs. To our knowledge, we are the first to investigate automatically inducing tree structures for targeted sentiment classification. We release our code at https://github.com/CCSoleil/latent graph atsc. 2 Related Work Aspect-level sentiment analysis Aspect-level sentiment analysis includes three main subtasks, namely aspect term sentiment classification (ATSC) (Jiang et al., 2011; Dong et al., 2014), aspect category sentiment classification (ACSC) (Jo and Oh, 2011; Pontiki et al., 2015, 2016) and aspect-term or opinion word extractions (Li et al., 2018b; Fan et al., 2019; Wan et al., 2020). In this paper, we focus on ATSC. To model relationships between the aspect terms and the context words, Vo and Zhang (2015) designed target-aware pooling functions to extract discriminative contexts. Tang et al. (2016a) modeled the interaction of targets and context words by using target-dependent LSTMs. Tang et al. (2016b) used multi-hop attention and memory networks to correlate an aspect with its opinion words. Zhang et al. (2016) design gating mechanisms to select useful contextual information for each target. Attention networks are further explored by sequent work (Ma"
2020.emnlp-main.451,D19-1549,0,0.119788,"or strong contextual encoders. Xu et al. (2019) proposed BERT-based post training for enhancing domain-specific contextual representations for aspect sentiment analysis. Recently, there is a line of work considering dependency tree information for ATSC. Lin et al. (2019) proposed deep mask memory network based on dependency trees. Zhang et al. (2019) and Sun et al. (2019b) encoded dependency tree using GCNs for aspect-level sentiment analysis. Zhao et al. (2019) used GCNs to model fully connected graphs between aspect terms, so that all targets can be classified using a shared representation. Huang and Carley (2019) proposed graph attention networks based on dependency trees for modeling structural relations. Wang et al. (2020) used relational graph attention networks to incorporate dependency edge type information, and construct aspect-specific graph structures by heuristically reshaping dependency trees. Latent graph induction Latent graphs can be induced to learn task-specific structures by end-toend models jointly with downstream tasks. Kim et al. (2017) proposed structural attention networks to introduce latent dependency graphs as intermediate layers for neural encoders. Niculae et al. (2018) used"
2020.emnlp-main.451,P11-1016,0,0.734892,"s, giving significantly better results than models without using latent graphs. 1 i complained to the manager , but he was not even apologetic (a) An example dependency tree from Stanford CoreNLP parser2 . the portions are small but being that the food was so good makes up for that . (b) A latent graph for the aspect term “portion”. the portions are small but being that the f ood was so good makes up for that . (c) A latent graph for the aspect term “food”. Introduction Aspect-level sentiment analysis aims to classify the sentiment polarities towards specific aspect terms in a given sentence (Jiang et al., 2011; Dong et al., 2014; Vo and Zhang, 2015). Aspects are also called opinion targets, which can typically be product or service features in customer reviews. For example, in the user comment “The environment is romantic, but the food is horrible”, the sentiments of the two aspects “environment” and “food” are positive and negative, respectively. The main challenge of aspect-level sentiment analysis is to effectively model the interaction between the aspect and its surrounding contexts. For example, identifying that “romantic” instead of “horrible” as the opinion word is the key to correctly class"
2020.emnlp-main.451,P18-1087,0,0.561657,"d its surrounding contexts. For example, identifying that “romantic” instead of “horrible” as the opinion word is the key to correctly classifying the sentiment of “environment”. Recently, graph convolutional networks (GCNs; Kipf and Welling (2017)) over dependency trees (Marcheggiani and Titov, 2017; Zhang et al., 2019; Sun et al., 2019b; Wang et al., 2020) have received much research attention. It has been shown to be more effective for learning aspect-specific representations than traditional sentence encoders without considering graph structures (Tang et al., 2016a,b; Liu and Zhang, 2017; Li et al., 2018a). Intuitively, dependency trees allow a model to better represent the correlation between aspect terms and their relevant opinion words. However, the existing methods suffer from two potential limitations. First, dependency parsing accuracies can be relatively low on noisy texts such as tweets, blogs and review comments, which are the main sources of aspect-level sentiment data. Second, dependency syntax according to a treebank may not be the most effective structure for capturing interaction between aspect terms and opinion words. Take Figure 1(a) for example. The aspect term “manager” is s"
2020.emnlp-main.451,D19-1559,0,0.0260644,"designed target-aware pooling functions to extract discriminative contexts. Tang et al. (2016a) modeled the interaction of targets and context words by using target-dependent LSTMs. Tang et al. (2016b) used multi-hop attention and memory networks to correlate an aspect with its opinion words. Zhang et al. (2016) design gating mechanisms to select useful contextual information for each target. Attention networks are further explored by sequent work (Ma et al., 2017; Liu and Zhang, 2017). Li et al. (2018a) used targetspecific transformation networks to learn targetspecific word representations. Liang et al. (2019) used aspect-guided recurrent transition networks to generate aspect-specific sentence representations. Sun et al. (2019a) constructed aspect related auxiliary sentences as inputs to BERT (Devlin et al., 2019) for strong contextual encoders. Xu et al. (2019) proposed BERT-based post training for enhancing domain-specific contextual representations for aspect sentiment analysis. Recently, there is a line of work considering dependency tree information for ATSC. Lin et al. (2019) proposed deep mask memory network based on dependency trees. Zhang et al. (2019) and Sun et al. (2019b) encoded depen"
2020.emnlp-main.451,E17-2091,1,0.913317,"between the aspect and its surrounding contexts. For example, identifying that “romantic” instead of “horrible” as the opinion word is the key to correctly classifying the sentiment of “environment”. Recently, graph convolutional networks (GCNs; Kipf and Welling (2017)) over dependency trees (Marcheggiani and Titov, 2017; Zhang et al., 2019; Sun et al., 2019b; Wang et al., 2020) have received much research attention. It has been shown to be more effective for learning aspect-specific representations than traditional sentence encoders without considering graph structures (Tang et al., 2016a,b; Liu and Zhang, 2017; Li et al., 2018a). Intuitively, dependency trees allow a model to better represent the correlation between aspect terms and their relevant opinion words. However, the existing methods suffer from two potential limitations. First, dependency parsing accuracies can be relatively low on noisy texts such as tweets, blogs and review comments, which are the main sources of aspect-level sentiment data. Second, dependency syntax according to a treebank may not be the most effective structure for capturing interaction between aspect terms and opinion words. Take Figure 1(a) for example. The aspect te"
2020.emnlp-main.451,D17-1159,0,0.0483564,"in customer reviews. For example, in the user comment “The environment is romantic, but the food is horrible”, the sentiments of the two aspects “environment” and “food” are positive and negative, respectively. The main challenge of aspect-level sentiment analysis is to effectively model the interaction between the aspect and its surrounding contexts. For example, identifying that “romantic” instead of “horrible” as the opinion word is the key to correctly classifying the sentiment of “environment”. Recently, graph convolutional networks (GCNs; Kipf and Welling (2017)) over dependency trees (Marcheggiani and Titov, 2017; Zhang et al., 2019; Sun et al., 2019b; Wang et al., 2020) have received much research attention. It has been shown to be more effective for learning aspect-specific representations than traditional sentence encoders without considering graph structures (Tang et al., 2016a,b; Liu and Zhang, 2017; Li et al., 2018a). Intuitively, dependency trees allow a model to better represent the correlation between aspect terms and their relevant opinion words. However, the existing methods suffer from two potential limitations. First, dependency parsing accuracies can be relatively low on noisy texts such"
2020.emnlp-main.451,2020.emnlp-main.348,0,0.0474571,"Missing"
2020.emnlp-main.451,D18-1108,0,0.371559,"on. Huang and Carley (2019) proposed graph attention networks based on dependency trees for modeling structural relations. Wang et al. (2020) used relational graph attention networks to incorporate dependency edge type information, and construct aspect-specific graph structures by heuristically reshaping dependency trees. Latent graph induction Latent graphs can be induced to learn task-specific structures by end-toend models jointly with downstream tasks. Kim et al. (2017) proposed structural attention networks to introduce latent dependency graphs as intermediate layers for neural encoders. Niculae et al. (2018) used SparseMAP to obtain a sparse distribution over latent dependency trees. Peng et al. (2018) implemented a differentiable proxy to the argmax 5597 3.1 Figure 1: Model architecture. operator over latent dependency trees, which can be regarded as a special case of introducing sparsity constraints into the softmax function (Niculae et al., 2018; Peters et al., 2019; Correia et al., 2019). Bastings et al. (2019) used HardKuma to sample stochastic interpretable discrete graphs for interpreting the classification results. Corro and Titov (2018) induced dependency structure for unsupervised parsi"
2020.emnlp-main.451,P18-1173,0,0.0340403,"Missing"
2020.emnlp-main.451,D14-1162,0,0.084821,"Missing"
2020.emnlp-main.451,P19-1146,0,0.0662416,"duced to learn task-specific structures by end-toend models jointly with downstream tasks. Kim et al. (2017) proposed structural attention networks to introduce latent dependency graphs as intermediate layers for neural encoders. Niculae et al. (2018) used SparseMAP to obtain a sparse distribution over latent dependency trees. Peng et al. (2018) implemented a differentiable proxy to the argmax 5597 3.1 Figure 1: Model architecture. operator over latent dependency trees, which can be regarded as a special case of introducing sparsity constraints into the softmax function (Niculae et al., 2018; Peters et al., 2019; Correia et al., 2019). Bastings et al. (2019) used HardKuma to sample stochastic interpretable discrete graphs for interpreting the classification results. Corro and Titov (2018) induced dependency structure for unsupervised parsing with a differentiable perturband-parsing method. While previous work obtains different structures using different methods, we investigate multiple methods for ATSC. More in line with our work, Yogatama et al. (2016) and Zhang et al. (2018) considered reinforcement learning for inducing latent structures for text classification. Our work is in line but differs in"
2020.emnlp-main.451,S15-2082,0,0.268948,"Missing"
2020.emnlp-main.451,S14-2004,0,0.228949,", where each xi contains a set of aspects ci,j . Formally, the loss function is given by L(θ) = − N X X i=1 ci,j log pyi,j + λ0 ||θ||2 , 2 where N is the number of training instances, θ is the set of model parameters, λ0 is a regularization hyperparameter, yi,j is the training label of the j-th aspect ci,j in xi and pyi,j is the aspect classification probability for ci,j , which is given by Eq 10. 4 Experiments We conduct experiments on five benchmark datasets for aspect-level sentiment analysis, including twitter posts (T WITTER) from Dong et al. (2014), laptop comments (L AP 14) provided by Pontiki et al. (2014), restaurant reviews of SemEval 2014 task 4 (R EST 14; Pontiki et al. 2014), SemEval 2015 task 12 (R EST 15; Pontiki et al. 2015) and SemEval 2016 task 5 (R EST 16; Pontiki et al. 2016). We pre-process these dataset in the same way as Tang et al. (2016b) and Zhang et al. (2019). Table 1 shows the statistics. Settings. We initialize word embeddings with 300-dimensional pretrained GloVe (Pennington et al., 2014) embeddings5 . The number of gated GCN blocks is 2. The head number is 8. The hidden dimension is 300. We parse the data using 5 http://nlp.stanford.edu/data/glove.840B.300d.zip Dataset #"
2020.emnlp-main.451,2020.acl-demos.14,0,0.0161368,".edu/data/glove.840B.300d.zip Dataset #Neu. #Neg. Train/Test 1,561/173 3,127/346 1,560/173 L AP 14 Train/Test 994/341 464/169 870/128 R EST 14 Train/Test 2,164/728 637/196 807/196 R EST 15 Train/Test 912/326 36/34 256/182 R EST 16 Train/Test 1,240/469 69/30 439/117 Table 1: Dataset statistics. Model depGCN sanGCN sparseGCN kumaGCN full -latent -dep Acc. F1 88.99 67.48 88.64 69.37 89.29 72.14 89.39 89.12 89.23 73.19 70.89 72.04 Table 2: Model performances on R EST 16. (10) where Wo and bo are model parameters and p is the predicted sentiment probability distribution. 3.7 #Pos. T WITTER Stanza (Qi et al., 2020). No dependency labels are used. For the other settings, we follow Zhang et al. (2019). Following previous conventions, we repeat each model three times and average the results, reporting accuracy (Acc.) and macro-f1 (F1). 4.1 Development Results Effect of latent graphs Table 2 shows the performances on R EST 16. We enhance dependency tree based graphs with self-attention based latent graph models (sanGCN), sparse self-attention based latent graph models (sparseGCN) and hard kuma based latent graph models (kumaGCN). sparseGCN significantly outperforms depGCN. sanGCN is also better than depGCN"
2020.emnlp-main.451,N19-1035,0,0.433471,"ment “The environment is romantic, but the food is horrible”, the sentiments of the two aspects “environment” and “food” are positive and negative, respectively. The main challenge of aspect-level sentiment analysis is to effectively model the interaction between the aspect and its surrounding contexts. For example, identifying that “romantic” instead of “horrible” as the opinion word is the key to correctly classifying the sentiment of “environment”. Recently, graph convolutional networks (GCNs; Kipf and Welling (2017)) over dependency trees (Marcheggiani and Titov, 2017; Zhang et al., 2019; Sun et al., 2019b; Wang et al., 2020) have received much research attention. It has been shown to be more effective for learning aspect-specific representations than traditional sentence encoders without considering graph structures (Tang et al., 2016a,b; Liu and Zhang, 2017; Li et al., 2018a). Intuitively, dependency trees allow a model to better represent the correlation between aspect terms and their relevant opinion words. However, the existing methods suffer from two potential limitations. First, dependency parsing accuracies can be relatively low on noisy texts such as tweets, blogs and review comments,"
2020.emnlp-main.451,D19-1569,0,0.41179,"ment “The environment is romantic, but the food is horrible”, the sentiments of the two aspects “environment” and “food” are positive and negative, respectively. The main challenge of aspect-level sentiment analysis is to effectively model the interaction between the aspect and its surrounding contexts. For example, identifying that “romantic” instead of “horrible” as the opinion word is the key to correctly classifying the sentiment of “environment”. Recently, graph convolutional networks (GCNs; Kipf and Welling (2017)) over dependency trees (Marcheggiani and Titov, 2017; Zhang et al., 2019; Sun et al., 2019b; Wang et al., 2020) have received much research attention. It has been shown to be more effective for learning aspect-specific representations than traditional sentence encoders without considering graph structures (Tang et al., 2016a,b; Liu and Zhang, 2017; Li et al., 2018a). Intuitively, dependency trees allow a model to better represent the correlation between aspect terms and their relevant opinion words. However, the existing methods suffer from two potential limitations. First, dependency parsing accuracies can be relatively low on noisy texts such as tweets, blogs and review comments,"
2020.emnlp-main.451,C16-1311,0,0.345118,"model the interaction between the aspect and its surrounding contexts. For example, identifying that “romantic” instead of “horrible” as the opinion word is the key to correctly classifying the sentiment of “environment”. Recently, graph convolutional networks (GCNs; Kipf and Welling (2017)) over dependency trees (Marcheggiani and Titov, 2017; Zhang et al., 2019; Sun et al., 2019b; Wang et al., 2020) have received much research attention. It has been shown to be more effective for learning aspect-specific representations than traditional sentence encoders without considering graph structures (Tang et al., 2016a,b; Liu and Zhang, 2017; Li et al., 2018a). Intuitively, dependency trees allow a model to better represent the correlation between aspect terms and their relevant opinion words. However, the existing methods suffer from two potential limitations. First, dependency parsing accuracies can be relatively low on noisy texts such as tweets, blogs and review comments, which are the main sources of aspect-level sentiment data. Second, dependency syntax according to a treebank may not be the most effective structure for capturing interaction between aspect terms and opinion words. Take Figure 1(a) fo"
2020.emnlp-main.451,D16-1021,0,0.182835,"model the interaction between the aspect and its surrounding contexts. For example, identifying that “romantic” instead of “horrible” as the opinion word is the key to correctly classifying the sentiment of “environment”. Recently, graph convolutional networks (GCNs; Kipf and Welling (2017)) over dependency trees (Marcheggiani and Titov, 2017; Zhang et al., 2019; Sun et al., 2019b; Wang et al., 2020) have received much research attention. It has been shown to be more effective for learning aspect-specific representations than traditional sentence encoders without considering graph structures (Tang et al., 2016a,b; Liu and Zhang, 2017; Li et al., 2018a). Intuitively, dependency trees allow a model to better represent the correlation between aspect terms and their relevant opinion words. However, the existing methods suffer from two potential limitations. First, dependency parsing accuracies can be relatively low on noisy texts such as tweets, blogs and review comments, which are the main sources of aspect-level sentiment data. Second, dependency syntax according to a treebank may not be the most effective structure for capturing interaction between aspect terms and opinion words. Take Figure 1(a) fo"
2020.emnlp-main.451,2020.acl-main.295,0,0.69785,"nt is romantic, but the food is horrible”, the sentiments of the two aspects “environment” and “food” are positive and negative, respectively. The main challenge of aspect-level sentiment analysis is to effectively model the interaction between the aspect and its surrounding contexts. For example, identifying that “romantic” instead of “horrible” as the opinion word is the key to correctly classifying the sentiment of “environment”. Recently, graph convolutional networks (GCNs; Kipf and Welling (2017)) over dependency trees (Marcheggiani and Titov, 2017; Zhang et al., 2019; Sun et al., 2019b; Wang et al., 2020) have received much research attention. It has been shown to be more effective for learning aspect-specific representations than traditional sentence encoders without considering graph structures (Tang et al., 2016a,b; Liu and Zhang, 2017; Li et al., 2018a). Intuitively, dependency trees allow a model to better represent the correlation between aspect terms and their relevant opinion words. However, the existing methods suffer from two potential limitations. First, dependency parsing accuracies can be relatively low on noisy texts such as tweets, blogs and review comments, which are the main s"
2020.emnlp-main.451,N19-1242,0,0.057501,"n aspect with its opinion words. Zhang et al. (2016) design gating mechanisms to select useful contextual information for each target. Attention networks are further explored by sequent work (Ma et al., 2017; Liu and Zhang, 2017). Li et al. (2018a) used targetspecific transformation networks to learn targetspecific word representations. Liang et al. (2019) used aspect-guided recurrent transition networks to generate aspect-specific sentence representations. Sun et al. (2019a) constructed aspect related auxiliary sentences as inputs to BERT (Devlin et al., 2019) for strong contextual encoders. Xu et al. (2019) proposed BERT-based post training for enhancing domain-specific contextual representations for aspect sentiment analysis. Recently, there is a line of work considering dependency tree information for ATSC. Lin et al. (2019) proposed deep mask memory network based on dependency trees. Zhang et al. (2019) and Sun et al. (2019b) encoded dependency tree using GCNs for aspect-level sentiment analysis. Zhao et al. (2019) used GCNs to model fully connected graphs between aspect terms, so that all targets can be classified using a shared representation. Huang and Carley (2019) proposed graph attentio"
2020.emnlp-main.451,D19-1464,0,0.458608,"ple, in the user comment “The environment is romantic, but the food is horrible”, the sentiments of the two aspects “environment” and “food” are positive and negative, respectively. The main challenge of aspect-level sentiment analysis is to effectively model the interaction between the aspect and its surrounding contexts. For example, identifying that “romantic” instead of “horrible” as the opinion word is the key to correctly classifying the sentiment of “environment”. Recently, graph convolutional networks (GCNs; Kipf and Welling (2017)) over dependency trees (Marcheggiani and Titov, 2017; Zhang et al., 2019; Sun et al., 2019b; Wang et al., 2020) have received much research attention. It has been shown to be more effective for learning aspect-specific representations than traditional sentence encoders without considering graph structures (Tang et al., 2016a,b; Liu and Zhang, 2017; Li et al., 2018a). Intuitively, dependency trees allow a model to better represent the correlation between aspect terms and their relevant opinion words. However, the existing methods suffer from two potential limitations. First, dependency parsing accuracies can be relatively low on noisy texts such as tweets, blogs an"
2020.emnlp-main.514,D11-1141,0,0.0975347,"ncluding lexical words, gazetteers and anchors in Wikipedia have been proved to be useful for a wide range of sentiment analysis tasks. For supervised NER task, some researchers utilize lattice structure to incorporate the lexical information into character-based NER and avoid the segmentation error propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant s"
2020.emnlp-main.514,W03-2201,0,0.0163477,"Missing"
2020.emnlp-main.514,D19-1025,0,0.244338,"named entity information which can provide rich knowledge for NER. Due to little knowledge connection between NER and general language modeling, how to adapt public pre-trained models to be NER-specific remains an open problem. To this end, injecting named entity knowledge during pre-training is a possible solution. However, this process of knowledge acquisition may be inefficient and expensive. In fact, there are extensive weakly labeled annotations that naturally exist on the web yet to be explored for NER model pre-training, which are relatively easier to obtain compared with labeled data (Cao et al., 2019). One can collect them from online resources, such as the Wikipedia anchors and gazetteers (named entity dictionaries). Although automatically derived corpora usually contain massive noisy data, it still contains some extend the valuable semantic information required for NER (Peng et al., 2019). In this paper, we propose a Coarse-to-Fine Entity knowledge Enhanced (CoFEE) pre-training framework for NER task, aiming to gather and utilize knowledge related to named entities. In particular, we first extract anchors from Wikipedia and use them as training corpora for entity span identification. Whi"
2020.emnlp-main.514,N19-1423,0,0.478224,"the task of discovering information entities and identifying their corresponding categories, such as mentions of people, organizations, locations, temporal and numeric expressions (Freitag, 2004). It is an essential component in many applications including machine translation (Babych and Hartley, 2003), relation extraction (Yu et al., 2019), entity linking (Xue et al., 2019a), and so on. ∗ Corresponding Author The source code can be https://github.com/strawberryx/CoFEE 1 obtained from Recently, NER has seen remarkable advances with the help of pre-trained representation models, such as BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019). Providing contextual representation, these pre-trained models could be easily applied to NER applications as an encoder by just fine-tuning it. Despite refreshing the state-of-theart performance of NER, the current pre-training techniques are not directly optimized for NER. Typically, these models build unsupervised training objectives to capture dependency between words and learn a general language representation (Tian et al., 2020), while rarely considering incorporating named entity information which can provide rich knowledge for NER. Due to little knowledge"
2020.emnlp-main.514,P19-1141,0,0.251407,"o be useful for a wide range of sentiment analysis tasks. For supervised NER task, some researchers utilize lattice structure to incorporate the lexical information into character-based NER and avoid the segmentation error propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al."
2020.emnlp-main.514,P18-2039,0,0.0588523,"rds, gazetteers and anchors in Wikipedia have been proved to be useful for a wide range of sentiment analysis tasks. For supervised NER task, some researchers utilize lattice structure to incorporate the lexical information into character-based NER and avoid the segmentation error propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al.,"
2020.emnlp-main.514,W04-3234,0,0.136435,"Missing"
2020.emnlp-main.514,D19-1355,0,0.0222868,"a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019). Task Specific Pre-training. Unsupervised language model pre-training and task-specific finetuning achieve SOTA results on many NLP tasks, including NER (Peters et al., 2018; Devlin et al., 2019; Li et al., 2020). Recently, with the help of automatically minded knowledge lying in the web, researchers devoted them to the pre-training models for specific tasks, including word sense disambiguation (Huang et al., 2019), word-in-context tasks (Levine et al., 2020), entity-linking and relation classification (Zhang et al., 2019), sentiment classification (Tian et al., 2020). 3 Background In this section, we give a brief introduction to MRC-NER (Li et al., 2020), which achieves satisfying performance in NER and thus is chosen as the foundation of our work. Given an input paragraph X = {x1 , x2 , · · · , xn } where xi denotes the i-th character, NER aims at discovering each entity xstart,end in X and identify its corresponding type y ∈ Y , where Y is the set of predefined tags(e.g., PER, LOC). xstart,end = {xst"
2020.emnlp-main.514,P19-1236,1,0.84352,"propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019). Task Specific Pre-training. Unsupervised language model pre-training and task-specific finetuning achieve SOTA results on many NLP tasks, including NER (Peters et al., 2018; Devlin et al., 2019; Li et al., 2020)."
2020.emnlp-main.514,2020.acl-main.519,0,0.636007,"Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019). Task Specific Pre-training. Unsupervised language model pre-training and task-specific finetuning achieve SOTA results on many NLP tasks, including NER (Peters et al., 2018; Devlin et al., 2019; Li et al., 2020). Recently, with the help of automatically minded knowledge lying in the web, researchers devoted them to the pre-training models for specific tasks, including word sense disambiguation (Huang et al., 2019), word-in-context tasks (Levine et al., 2020), entity-linking and relation classification (Zhang et al., 2019), sentiment classification (Tian et al., 2020). 3 Background In this section, we give a brief introduction to MRC-NER (Li et al., 2020), which achieves satisfying performance in NER and thus is chosen as the foundation of our work. Given an input paragraph X = {x1 , x2 , · · · , xn }"
2020.emnlp-main.514,D18-1226,0,0.02176,"segmentation error propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019). Task Specific Pre-training. Unsupervised language model pre-training and task-specific finetuning achieve SOTA results on many NLP tasks, including NER (Peters et al., 2018; Devlin et al., 2019;"
2020.emnlp-main.514,P19-1524,0,0.0812722,"ide range of sentiment analysis tasks. For supervised NER task, some researchers utilize lattice structure to incorporate the lexical information into character-based NER and avoid the segmentation error propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019). Task Speci"
2020.emnlp-main.514,P16-1101,0,0.182032,"performs other competitive baselines, often by large margins. We also demonstrate that CoFEE pre-training can work well in more challenging, label-free and low-resource scenarios. Further ablation studies show the impact of each pre-training task in achieving these strong performance. To the best of our knowledge, this is the first work that has tackled NER-specific representation during pre-training. 2 Related Work Entity Knowledge for NER. Recently, neural networks have been used for NER and achieved great success (Collobert et al., 2011; dos Santos and Guimar˜aes, 2015; Huang et al., 2015; Ma and Hovy, 2016). Specifically, various types of entity knowledge, including lexical words, gazetteers and anchors in Wikipedia have been proved to be useful for a wide range of sentiment analysis tasks. For supervised NER task, some researchers utilize lattice structure to incorporate the lexical information into character-based NER and avoid the segmentation error propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gaz"
2020.emnlp-main.514,P17-1135,0,0.0644342,"Missing"
2020.emnlp-main.514,P19-1231,0,0.40576,"is a possible solution. However, this process of knowledge acquisition may be inefficient and expensive. In fact, there are extensive weakly labeled annotations that naturally exist on the web yet to be explored for NER model pre-training, which are relatively easier to obtain compared with labeled data (Cao et al., 2019). One can collect them from online resources, such as the Wikipedia anchors and gazetteers (named entity dictionaries). Although automatically derived corpora usually contain massive noisy data, it still contains some extend the valuable semantic information required for NER (Peng et al., 2019). In this paper, we propose a Coarse-to-Fine Entity knowledge Enhanced (CoFEE) pre-training framework for NER task, aiming to gather and utilize knowledge related to named entities. In particular, we first extract anchors from Wikipedia and use them as training corpora for entity span identification. While anchors have no entity type information, the model could get general-typed entity 6345 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6345–6354, c November 16–20, 2020. 2020 Association for Computational Linguistics knowledge from them and learn"
2020.emnlp-main.514,N18-1202,0,0.0560047,"main (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019). Task Specific Pre-training. Unsupervised language model pre-training and task-specific finetuning achieve SOTA results on many NLP tasks, including NER (Peters et al., 2018; Devlin et al., 2019; Li et al., 2020). Recently, with the help of automatically minded knowledge lying in the web, researchers devoted them to the pre-training models for specific tasks, including word sense disambiguation (Huang et al., 2019), word-in-context tasks (Levine et al., 2020), entity-linking and relation classification (Zhang et al., 2019), sentiment classification (Tian et al., 2020). 3 Background In this section, we give a brief introduction to MRC-NER (Li et al., 2020), which achieves satisfying performance in NER and thus is chosen as the foundation of our work. Given an inpu"
2020.emnlp-main.514,P18-1144,1,0.852076,"re-training. 2 Related Work Entity Knowledge for NER. Recently, neural networks have been used for NER and achieved great success (Collobert et al., 2011; dos Santos and Guimar˜aes, 2015; Huang et al., 2015; Ma and Hovy, 2016). Specifically, various types of entity knowledge, including lexical words, gazetteers and anchors in Wikipedia have been proved to be useful for a wide range of sentiment analysis tasks. For supervised NER task, some researchers utilize lattice structure to incorporate the lexical information into character-based NER and avoid the segmentation error propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et"
2020.emnlp-main.514,P19-1139,0,0.023365,"ant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019). Task Specific Pre-training. Unsupervised language model pre-training and task-specific finetuning achieve SOTA results on many NLP tasks, including NER (Peters et al., 2018; Devlin et al., 2019; Li et al., 2020). Recently, with the help of automatically minded knowledge lying in the web, researchers devoted them to the pre-training models for specific tasks, including word sense disambiguation (Huang et al., 2019), word-in-context tasks (Levine et al., 2020), entity-linking and relation classification (Zhang et al., 2019), sentiment classification (Tian et al., 2020). 3 Background In this section, we give a brief introduction to MRC-NER (Li et al., 2020), which achieves satisfying performance in NER and thus is chosen as the foundation of our work. Given an input paragraph X = {x1 , x2 , · · · , xn } where xi denotes the i-th character, NER aims at discovering each entity xstart,end in X and identify its corresponding type y ∈ Y , where Y is the set of predefined tags(e.g., PER, LOC). xstart,end = {xstart , xstart+1 , · · · , xend−1 , xend } is a substring of X satisfying start ≤ end. Specifically, MRCNER form"
2020.emnlp-main.514,P19-1336,0,0.0401103,"Missing"
2020.emnlp-main.514,W15-3904,0,0.077935,"Missing"
2020.emnlp-main.514,D19-1396,0,0.0553228,"have been used for NER and achieved great success (Collobert et al., 2011; dos Santos and Guimar˜aes, 2015; Huang et al., 2015; Ma and Hovy, 2016). Specifically, various types of entity knowledge, including lexical words, gazetteers and anchors in Wikipedia have been proved to be useful for a wide range of sentiment analysis tasks. For supervised NER task, some researchers utilize lattice structure to incorporate the lexical information into character-based NER and avoid the segmentation error propagation of word (Zhang and Yang, 2018; Gui et al., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of wea"
2020.emnlp-main.514,2020.acl-main.374,0,0.0760166,"FEE 1 obtained from Recently, NER has seen remarkable advances with the help of pre-trained representation models, such as BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019). Providing contextual representation, these pre-trained models could be easily applied to NER applications as an encoder by just fine-tuning it. Despite refreshing the state-of-theart performance of NER, the current pre-training techniques are not directly optimized for NER. Typically, these models build unsupervised training objectives to capture dependency between words and learn a general language representation (Tian et al., 2020), while rarely considering incorporating named entity information which can provide rich knowledge for NER. Due to little knowledge connection between NER and general language modeling, how to adapt public pre-trained models to be NER-specific remains an open problem. To this end, injecting named entity knowledge during pre-training is a possible solution. However, this process of knowledge acquisition may be inefficient and expensive. In fact, there are extensive weakly labeled annotations that naturally exist on the web yet to be explored for NER model pre-training, which are relatively easi"
2020.emnlp-main.514,D18-1034,0,0.0182069,"., 2019a; Xue et al., 2019b; Gui et al., 2019b; Sui et al., 2019). Additionally, gazetteers have long been regarded as a piece useful knowledge for NER, previous methods commonly incorporated gazetteers by either using them as handcraft features (Alan et al., 2011; Dominic et al., 2018) or auxiliary structural information (Ding et al., 2019; Liu et al., 2019). For weakly supervised NER, a typical line of methods centres around transfer learning to extract source knowledge for target, such as crossdomain (Yang et al., 2017; Lin and Lu, 2018; Jia et al., 2019) or cross-lingual (Ni et al., 2017; Xie et al., 2018; Zhou et al., 2019). There are also a lot of weak labels lying on the web or gazetteers, which have not been explored. Consequently, a number of works focus on distantly supervised methods, using anchors or gazetteers to generate data by distant supervision (Liu et al., 2015; Yang et al., 2018; Cao et al., 2019; Peng et al., 2019). Task Specific Pre-training. Unsupervised language model pre-training and task-specific finetuning achieve SOTA results on many NLP tasks, including NER (Peters et al., 2018; Devlin et al., 2019; Li et al., 2020). Recently, with the help of automatically minded know"
2020.emnlp-main.518,W09-2905,0,0.0834352,"Missing"
2020.emnlp-main.518,P15-1001,0,0.0148035,"computed as: I(e; c) = H(e) − H(e|c) t=1 > l qli (hl−1 Wh,k )> t (5)   = H(e) + Ec∼p(c) Ee∼p(e|c) [log p(e|c)] ! ; (7) = H(e) + Ec∼p(c),e∼p(e|c) [log p(e|c)] , where H(e) indicates the entropy of e ∼ p(e), represented as H(e) = −Ee∼p(e) [log p(e)], which 6387 is a constant corresponding to the frequency of entities in a document. Thus the maximization of the mutual information I(e; c) is equivalent to the maximization of the expectation of log p(e|c). Considering the computational complexity due to the excessive number of candidate entities, we employ sampling softmax for output prediction (Jean et al., 2015). Formally, given the hidden outL puts of last layer {hL 1 , . . . , hT } and its corresponding entity labeled sequence e = {e1 , . . . , eT }, we compute the probability of each character ct (s.t. et 6= 0) aligning with its corresponding entity et as: Algorithm 2 Pre-training and fine-tuning. (8) Input: Raw text Dlm , entity dict Eent , NER dataset Dner Parameters: Entity embeddings Eent , Transformer layers o WT , MLM output layer WM LM , entity classification output o o layer WEN , NER output layer WN C ER . Output: Target NER model 1: while LM pre-training stopping condition is not met do"
2020.emnlp-main.518,N18-1202,0,0.021735,"and Wang, 2008; Liu et al., 2010; Li et al., 2014). Lexicon features have been applied so that the external word-level information enhances NER training (Luo et al., 2015; Zhang and Yang, 2018; Gui et al., 2019a,b; Xue et al., 2019). However, these methods are supervised models, which cannot deal with a dataset with relatively little labeled data. We address this problem by using a semisupervised method by using a pre-trained LM. Pre-trained Language Models. Pre-trained language models have been applied as an integral component in modern NLP systems for effectively improving downstream tasks (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019b). Recently, there is an increasing interest to augment such contextualized representation with external knowledge (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019). These methods focus on augmenting BERT by integrating KG embeddings such as TransE (Bordes et al., 2013). Different from the line of work, our model dynamically integrates document-specific entities without using any pre-trained entity embeddings. A more similar method is ERNIE (Sun et al., 2019a,b), which enhances BERT through knowledge in"
2020.emnlp-main.518,D19-1005,0,0.020946,"s are supervised models, which cannot deal with a dataset with relatively little labeled data. We address this problem by using a semisupervised method by using a pre-trained LM. Pre-trained Language Models. Pre-trained language models have been applied as an integral component in modern NLP systems for effectively improving downstream tasks (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019b). Recently, there is an increasing interest to augment such contextualized representation with external knowledge (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019). These methods focus on augmenting BERT by integrating KG embeddings such as TransE (Bordes et al., 2013). Different from the line of work, our model dynamically integrates document-specific entities without using any pre-trained entity embeddings. A more similar method is ERNIE (Sun et al., 2019a,b), which enhances BERT through knowledge integration. In particular, instead of masking individual subword tokens as BERT does, ERNIE is trained by masking full entities. The entity-level masking trick for ERNIE pre-training can be seen as an implicit way to integrate entity information through err"
2020.emnlp-main.518,D15-1058,0,0.0284444,"l., 2019). NER has been a challenging task due to the flexibility of named entities. There can be a large number of OOV named entities in the open domain, which poses challenges to supervised learning algorithms. In addition, named entities can be ambiguous. Take Figure 1 for example. The term “老 妇人(the old lady)” literally means “older woman”. Equal contribution. Organization 人 Char-Entity-Transformer Introduction ∗ 妇 However, in the context of football news, it means the nickname of a football club Juventus F.C.. Thus entity lexicons that contain domain knowledge can be useful for the task (Radford et al., 2015; Xu et al., 2019). Intuitively, such lexicons can be collected automatically from a set of documents that are relevant to the input text. For example, in the news domain, a set of news articles in the same domain and concurrent with the input text can contain highly relevant entities. In the finance domain, the financial report of a company over the years can serve as a context for collecting named entities when conducting NER for a current-year report. In the science domain, relevant articles can mention the same technological terms, which can facilitate recognition of the terms. In the lite"
2020.emnlp-main.518,P19-3007,0,0.0588586,"Missing"
2020.emnlp-main.518,P18-1144,1,0.85208,"ture. After pretraining, the embedding of “老妇人(The old lady)” has the global information and correctly classifies itself as an O RG, which also helps recognize “意甲(Serie A)” as an O RG. As a fundamental task in information extraction, named entity recognition (NER) is useful for NLP tasks such as relation extraction (Zelenko et al., 2003), event detection (Kumaran and Allan, 2004) and machine translation (Babych and Hartley, 2003). We investigate Chinese NER (Gao et al., 2005), for which the state-of-the-art methods use a character-based neural encoder augmented with lexicon word information (Zhang and Yang, 2018; Gui et al., 2019a,b; Xue et al., 2019). NER has been a challenging task due to the flexibility of named entities. There can be a large number of OOV named entities in the open domain, which poses challenges to supervised learning algorithms. In addition, named entities can be ambiguous. Take Figure 1 for example. The term “老 妇人(the old lady)” literally means “older woman”. Equal contribution. Organization 人 Char-Entity-Transformer Introduction ∗ 妇 However, in the context of football news, it means the nickname of a football club Juventus F.C.. Thus entity lexicons that contain domain knowled"
2020.emnlp-main.518,P19-1139,0,0.0189779,"ue et al., 2019). However, these methods are supervised models, which cannot deal with a dataset with relatively little labeled data. We address this problem by using a semisupervised method by using a pre-trained LM. Pre-trained Language Models. Pre-trained language models have been applied as an integral component in modern NLP systems for effectively improving downstream tasks (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019b). Recently, there is an increasing interest to augment such contextualized representation with external knowledge (Zhang et al., 2019; Liu et al., 2019a; Peters et al., 2019). These methods focus on augmenting BERT by integrating KG embeddings such as TransE (Bordes et al., 2013). Different from the line of work, our model dynamically integrates document-specific entities without using any pre-trained entity embeddings. A more similar method is ERNIE (Sun et al., 2019a,b), which enhances BERT through knowledge integration. In particular, instead of masking individual subword tokens as BERT does, ERNIE is trained by masking full entities. The entity-level masking trick for ERNIE pre-training can be seen as an implicit way to"
2020.emnlp-main.92,2020.acl-main.640,0,0.352246,"raph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performances by directly modeling the intercorrelations between distant node pairs with relation-aware global communication. Despite the progress on the encoder side, the current stateof-the-art models use a rather standard decoder: it functions as a language model, where each word is generated given only the previous words. As a result, one limitation of such decoders is that they tend to produce fluent sentences that may not retain the meaning of input AMRs. We investigate enhancing AMR-to-text decoding by integrating online back-parsing, simultaneously"
2020.emnlp-main.92,N19-1223,0,0.0333348,"Missing"
2020.emnlp-main.92,N19-1366,0,0.0197127,"any applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performances by directly modeling the intercorrelations between distant node pairs with relation-aware global communication. Despite the progress on the encoder side, the current stateof-the-art models use a rather standard decoder: it functions as a language model, where each word is generated given only the previous words"
2020.emnlp-main.92,W14-3348,0,0.0128671,". We tune these hyperparameters on the LDC2015E86 development 4 https://nlp.stanford.edu/software/tokenizer.shtml https://github.com/rsennrich/subword-nmt 6 We do not choose their best model (G-Trans-SA) due to its large GPU memory consumption, and its performance is actually comparable with G-Trans-F in our experiments. 1210 5 Model Figure 3: BLEU scores on the LDC2015E86 devset against different hyperparameter values. set and use the selected values for testing7 . Model Evaluation. We set the decoding beam size as 5 and take BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014) as automatic evaluation metrics. We also employ human evaluation to assess the semantic faithfulness and generation fluency of compared methods by randomly selecting 50 AMR graphs for comparison. Three people familiar with AMR are asked to score the generation quality with regard to three aspects — concept preservation rate, relation preservation rate and fluency (on a scale of [0, 5]). Details about the criteria are: • Concept preservation rate assesses to what extent the concepts in input AMR graphs are involved in generated sentences. • Relation preservation rate measures to what extent th"
2020.emnlp-main.92,N16-1087,0,0.0665669,"es little attention to the AMR node “local” and “problem” during text generation. In contrast, our system gives a more accurate alignment to the relevant AMR nodes in decoding. In the second example, the baseline model incorrectly positions the terms “doctor”, “see” and “worse cases” while our approach generates a more natural sentence. This can be attributed to the edge prediction task, which can inform the decoder to preserve the relation that “doctor” is the subject of “see” and “worse cases” is the object. 5 Related Work Early studies on AMR-to-text generation rely on statistical methods. Flanigan et al. (2016) convert input AMR graphs to trees by splitting re-entrances, before translating these trees into target sentences with a tree-to-string transducer; Pourdamghani et al. (2016) apply a phrase-based MT system on linearized AMRs; Song et al. (2017) design a synchronous node replacement grammar to parse input AMRs while generating target sentences. These approaches show comparable or better results than early neural models (Konstas et al., 2017). However, recent neural approaches (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020; Mager et al., 2020) have demonstrated the s"
2020.emnlp-main.92,W17-3518,0,0.0865946,"he task of AMRto-text generation (Konstas et al., 2017) aims to produce fluent sentences that convey consistent meaning with input AMR graphs. For example, taking the AMR in Figure 1 as input, a model can produce the sentence “The police could help the victim”. AMR-to-text generation has been shown useful for many applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performance"
2020.emnlp-main.92,Q19-1019,0,0.143331,". AMR-to-text generation has been shown useful for many applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performances by directly modeling the intercorrelations between distant node pairs with relation-aware global communication. Despite the progress on the encoder side, the current stateof-the-art models use a rather standard decoder: it functions as a language model,"
2020.emnlp-main.92,N19-1235,0,0.0198829,"machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performances by directly modeling the intercorrelations between distant node pairs with relation-aware global communication. Despite the progress on the encoder side, the current stateof-the-art models use a rather standard decoder: it functions as a language model, where each word is generated given only the previous words. As a result, one li"
2020.emnlp-main.92,D18-1086,0,0.016656,"possible-01” and “police”, represent concepts, and the edges, such as “ARG0” and “ARG1”, indicate relations between the concepts they connect. The task of AMRto-text generation (Konstas et al., 2017) aims to produce fluent sentences that convey consistent meaning with input AMR graphs. For example, taking the AMR in Figure 1 as input, a model can produce the sentence “The police could help the victim”. AMR-to-text generation has been shown useful for many applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on lineariz"
2020.emnlp-main.92,N19-1238,0,0.0324426,"Missing"
2020.emnlp-main.92,P17-1014,0,0.505826,"riments on two AMR benchmarks show the superiority of our model over the previous state-of-the-art system based on graph Transformer. 1 Figure 1: An example AMR graph meaning “The police could help the victim.” Introduction Abstract meaning representation (AMR) (Banarescu et al., 2013) is a semantic graph representation that abstracts meaning away from a sentence. Figure 1 shows an AMR graph, where the nodes, such as “possible-01” and “police”, represent concepts, and the edges, such as “ARG0” and “ARG1”, indicate relations between the concepts they connect. The task of AMRto-text generation (Konstas et al., 2017) aims to produce fluent sentences that convey consistent meaning with input AMR graphs. For example, taking the AMR in Figure 1 as input, a model can produce the sentence “The police could help the victim”. AMR-to-text generation has been shown useful for many applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attra"
2020.emnlp-main.92,C18-1101,0,0.0607051,"he nodes, such as “possible-01” and “police”, represent concepts, and the edges, such as “ARG0” and “ARG1”, indicate relations between the concepts they connect. The task of AMRto-text generation (Konstas et al., 2017) aims to produce fluent sentences that convey consistent meaning with input AMR graphs. For example, taking the AMR in Figure 1 as input, a model can produce the sentence “The police could help the victim”. AMR-to-text generation has been shown useful for many applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas"
2020.emnlp-main.92,C16-1291,0,0.0221542,"At each decoding time step, the proposed decoder takes the encoder states as inputs and generates a new word (as in Section 2.2), together with its corresponding AMR node (Section 3.1) and its outgoing edges (Section 3.2), These predictions are then used inputs to calculate the next state (Section 3.3). 3.1 Node Prediction We first equip a standard decoder with the ability to make word-to-node alignments while generating target words. Making alignments can be formalized as a matching problem, which aims to find the most relevant AMR graph node for each target word. Inspired by previous work (Liu et al., 2016; Mi et al., 2016), we solve the matching problem by supervising the word-to-node attention scores given by the Transformer decoder. In order to deal with words without alignments, we introduce a NULL node v∅ into the input AMR graph (as shown in 1208 Figure 2) and align such words to it.3 More specifically, at each decoding step t, our Transformer decoder first calculates the top decoder layer word-to-node attention distribution βt0 = 0 , β 0 , ..., β 0 ] (Eq 3 and Eq 4) after taking [βt0 t1 tN L L L the encoder states H L = [hL 0 , h1 , h2 , . . . , hN ] together with the previously generate"
2020.emnlp-main.92,2020.acl-main.167,0,0.0321795,"Missing"
2020.emnlp-main.92,D16-1249,0,0.167105,"time step, the proposed decoder takes the encoder states as inputs and generates a new word (as in Section 2.2), together with its corresponding AMR node (Section 3.1) and its outgoing edges (Section 3.2), These predictions are then used inputs to calculate the next state (Section 3.3). 3.1 Node Prediction We first equip a standard decoder with the ability to make word-to-node alignments while generating target words. Making alignments can be formalized as a matching problem, which aims to find the most relevant AMR graph node for each target word. Inspired by previous work (Liu et al., 2016; Mi et al., 2016), we solve the matching problem by supervising the word-to-node attention scores given by the Transformer decoder. In order to deal with words without alignments, we introduce a NULL node v∅ into the input AMR graph (as shown in 1208 Figure 2) and align such words to it.3 More specifically, at each decoding step t, our Transformer decoder first calculates the top decoder layer word-to-node attention distribution βt0 = 0 , β 0 , ..., β 0 ] (Eq 3 and Eq 4) after taking [βt0 t1 tN L L L the encoder states H L = [hL 0 , h1 , h2 , . . . , hN ] together with the previously generated sequence 0 and h"
2020.emnlp-main.92,P02-1040,0,0.107302,"Our models are trained for 500K steps on a single 2080Ti GPU. We tune these hyperparameters on the LDC2015E86 development 4 https://nlp.stanford.edu/software/tokenizer.shtml https://github.com/rsennrich/subword-nmt 6 We do not choose their best model (G-Trans-SA) due to its large GPU memory consumption, and its performance is actually comparable with G-Trans-F in our experiments. 1210 5 Model Figure 3: BLEU scores on the LDC2015E86 devset against different hyperparameter values. set and use the selected values for testing7 . Model Evaluation. We set the decoding beam size as 5 and take BLEU (Papineni et al., 2002) and Meteor (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014) as automatic evaluation metrics. We also employ human evaluation to assess the semantic faithfulness and generation fluency of compared methods by randomly selecting 50 AMR graphs for comparison. Three people familiar with AMR are asked to score the generation quality with regard to three aspects — concept preservation rate, relation preservation rate and fluency (on a scale of [0, 5]). Details about the criteria are: • Concept preservation rate assesses to what extent the concepts in input AMR graphs are involved in generated s"
2020.emnlp-main.92,W17-4770,0,0.0385168,"Missing"
2020.emnlp-main.92,Q19-1002,1,0.855378,"at abstracts meaning away from a sentence. Figure 1 shows an AMR graph, where the nodes, such as “possible-01” and “police”, represent concepts, and the edges, such as “ARG0” and “ARG1”, indicate relations between the concepts they connect. The task of AMRto-text generation (Konstas et al., 2017) aims to produce fluent sentences that convey consistent meaning with input AMR graphs. For example, taking the AMR in Figure 1 as input, a model can produce the sentence “The police could help the victim”. AMR-to-text generation has been shown useful for many applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 20"
2020.emnlp-main.92,P17-2002,1,0.826348,"s “doctor”, “see” and “worse cases” while our approach generates a more natural sentence. This can be attributed to the edge prediction task, which can inform the decoder to preserve the relation that “doctor” is the subject of “see” and “worse cases” is the object. 5 Related Work Early studies on AMR-to-text generation rely on statistical methods. Flanigan et al. (2016) convert input AMR graphs to trees by splitting re-entrances, before translating these trees into target sentences with a tree-to-string transducer; Pourdamghani et al. (2016) apply a phrase-based MT system on linearized AMRs; Song et al. (2017) design a synchronous node replacement grammar to parse input AMRs while generating target sentences. These approaches show comparable or better results than early neural models (Konstas et al., 2017). However, recent neural approaches (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020; Mager et al., 2020) have demonstrated the state-of-the-art performances thanks to the use of contextualized embeddings. Related work on NMT studies back-translation loss (Sennrich et al., 2016; Tu et al., 2017) by translating the target reference back into the source text (reconstruction"
2020.emnlp-main.92,P18-1150,1,0.920296,"ld help the victim”. AMR-to-text generation has been shown useful for many applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performances by directly modeling the intercorrelations between distant node pairs with relation-aware global communication. Despite the progress on the encoder side, the current stateof-the-art models use a rather standard decoder: it functions as"
2020.emnlp-main.92,D14-1048,0,0.0282584,"periments We conduct experiments on two benchmark AMRto-text generation datasets, including LDC2015E86 and LDC2017T10. These two datasets contain 16,833 and 36,521 training examples, respectively, and share a common set of 1,368 development and 1,371 test instances. 4.1 Experimental Settings Data preprocessing. Following previous work (Song et al., 2018; Zhu et al., 2019), we take a standard simplifier (Konstas et al., 2017) to preprocess AMR graphs, adopting the Stanford tokenizer4 and Subword Tool5 to segment text into subword units. The node-to-word alignments are generated by ISI aligner (Pourdamghani et al., 2014). We then project the source AMR graph onto the target sentence according to such alignments. For node prediction, the attention distributions are normalized, but the alignment scores generated by the ISI aligner are unnormalized hard 0/1 values. To enable cross entropy loss, we follow previous work (Mi et al., 2016) to normalize the goldstandard alignment scores. Hyperparameters. We choose the feature-based model6 of Zhu et al. (2019) as our baseline (GTrans-F-Ours). Also following their settings, both the encoder and decoder have 6 layers, with each layer having 8 attention heads. The sizes"
2020.emnlp-main.92,2020.tacl-1.2,0,0.529734,"oblems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performances by directly modeling the intercorrelations between distant node pairs with relation-aware global communication. Despite the progress on the encoder side, the current stateof-the-art models use a rather standard decoder: it functions as a language model, where each word is generated given only the previous words. As a result, one limitation of such decoders is that they tend to produce fluent sentences that may not retain the meaning of input AMRs. We investigate enhancing AMR-to-text decoding by integrating online back-parsing, simultaneously predicting a projec"
2020.emnlp-main.92,W16-6603,0,0.188465,"ding. In the second example, the baseline model incorrectly positions the terms “doctor”, “see” and “worse cases” while our approach generates a more natural sentence. This can be attributed to the edge prediction task, which can inform the decoder to preserve the relation that “doctor” is the subject of “see” and “worse cases” is the object. 5 Related Work Early studies on AMR-to-text generation rely on statistical methods. Flanigan et al. (2016) convert input AMR graphs to trees by splitting re-entrances, before translating these trees into target sentences with a tree-to-string transducer; Pourdamghani et al. (2016) apply a phrase-based MT system on linearized AMRs; Song et al. (2017) design a synchronous node replacement grammar to parse input AMRs while generating target sentences. These approaches show comparable or better results than early neural models (Konstas et al., 2017). However, recent neural approaches (Song et al., 2018; Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020; Mager et al., 2020) have demonstrated the state-of-the-art performances thanks to the use of contextualized embeddings. Related work on NMT studies back-translation loss (Sennrich et al., 2016; Tu et al., 2017) by tran"
2020.emnlp-main.92,D18-1509,0,0.15256,"of previous decoder hidden states, respectively. In contrast to the baseline in Eq 3, at time t+1, the hidden state of the first decoder layer is calculated as: sˆ1t+1 = SAN(s01 , ..., s0t , ~yt , ~vt , ~et ), c1t+1 = AN(ˆ s1t+1 , H L ), s1t+1 = (16) FF(c1t+1 , sˆ1t+1 ), where the definition of H L , SAN, AN, FF and [s01 , . . . , s0t ] are the same as Eq 3. ~v0 and ~e0 (as shown in Figure 2) are defined as zero vectors. The hidden states of upper decoder layers ([s2t+1 , ..., sL t+1 ]) are updated in the same way as Eq 3. Following previous work on syntactic text generation (Wu et al., 2017; Wang et al., 2018), we use gold AMR nodes and outgoing edges as inputs for training, while we take automatic predictions for decoding. 3.4 Training Objective The overall training objective is: `total = `std + λ1 `node + λ2 `label , (17) where λ1 and λ2 are weighting hyper-parameters for `node and `label , respectively. Model BLEU Meteor G-Trans-F-Ours 30.20 35.23 Node Prediction MSE Node Prediction CE 30.66 30.85 35.60 35.71 Edge Prediction share Edge Prediction independent 31.19 31.13 35.75 35.69 Table 1: BLEU and Meteor scores on the LDC2015E86 devset under different model settings. 4 Experiments We conduct e"
2020.emnlp-main.92,prasad-etal-2008-penn,0,0.00704984,"the back-parsing mechanism. With regard to the generation fluency, our model also gives better results than baseline. The main reason is that the relations between concepts such as subjectpredicate relation and modified relation are helpful Model BLEU Meteor Baseline 30.15 35.36 + Node Prediction + Node Prediction (Int.) 30.49 30.72 35.66 35.94 + Edge Prediction + Edge Prediction (Int.) 30.80 31.07 35.71 35.87 + Both Prediction + Both Prediction (Int.) 30.96 31.48 35.92 36.15 Table 5: Ablation study on LDC2015E86 test set. for generating fluency sentences. Apart from that, we study discourse (Prasad et al., 2008) relations, which are essential for generating a good sentence with correct meaning. Specifically, we consider 4 common discourse relations (“Cause”, “Contrast”, “Condition”, “Coordinating”). For each type of discourse, we randomly select 50 examples from the test set and ask 3 linguistic experts to calculate the discourse preservation accuracy by checking if the generated sentence preserves such information. Table 4 gives discourse preservation accuracy results of the baseline and our model, respectively. The baseline already performs well, which is likely because discourse information can so"
2020.emnlp-main.92,D17-1239,0,0.0603656,"Missing"
2020.emnlp-main.92,D19-1314,0,0.457003,"Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performances by directly modeling the intercorrelations between distant node pairs with relation-aware global communication. Despite the progress on the encoder side, the current stateof-the-art models use a rather standard decoder: it functions as a language model, where each word is generated given only the previous words. As a result, one limitation of such decode"
2020.emnlp-main.92,K17-1045,0,0.0260422,"s an AMR graph, where the nodes, such as “possible-01” and “police”, represent concepts, and the edges, such as “ARG0” and “ARG1”, indicate relations between the concepts they connect. The task of AMRto-text generation (Konstas et al., 2017) aims to produce fluent sentences that convey consistent meaning with input AMR graphs. For example, taking the AMR in Figure 1 as input, a model can produce the sentence “The police could help the victim”. AMR-to-text generation has been shown useful for many applications such as machine translation (Song et al., 2019) and summarization (Liu et al., 2015; Yasunaga et al., 2017; Liao et al., 2018; Hardy and Vlachos, 2018). In addition, AMR-to-text generation can be a good test bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-ba"
2020.emnlp-main.92,D19-1548,0,0.399423,"bed for general graph-to-sequence problems (Belz et al., 2011; Gardent et al., 2017). AMR-to-text generation has attracted increasing research attention recently. Previous work has focused on developing effective encoders for representing graphs. In particular, graph neural networks (Beck et al., 2018; Song et al., 2018; Guo et al., 2019) and richer graph representations (Damonte and Cohen, 2019; Hajdik et al., 2019; Ribeiro et al., 2019) have been shown to give better performances than RNN-based models (Konstas et al., 2017) on linearized graphs. Subsequent work exploited graph Transformer (Zhu et al., 2019; Cai and Lam, 2020; Wang et al., 2020), achieving better performances by directly modeling the intercorrelations between distant node pairs with relation-aware global communication. Despite the progress on the encoder side, the current stateof-the-art models use a rather standard decoder: it functions as a language model, where each word is generated given only the previous words. As a result, one limitation of such decoders is that they tend to produce fluent sentences that may not retain the meaning of input AMRs. We investigate enhancing AMR-to-text decoding by integrating online back-pars"
2020.findings-emnlp.447,D15-1075,0,0.0351953,"hypothesis are drawn from the premise but do not form a subsentence of premise, and thus the syntactic structures in hypothesis and premise are quite different. Both the BERT-CLS baseline and our GCN-based BERT model with co-attention links (BERT+CAGCN) are finetuned on MNLI dataset (Williams et al., 2018), and the neutral or contradiction labels are translated into non-entailment when evaluation (McCoy et al., 2019b). Note that E stands for entailment, and N stands for non-entailment. and the hypothesis into embedding vectors, which are fed to a multi-layer neural network for classification (Bowman et al., 2015). It has been shown that alignment between local words in the premise and hypothesis benefits the aggregation of information (Chen et al., 2016; Parikh et al., 2016), and encoding the sentence pair simultaneously can capture more interaction and thus further improve the performance (Devlin et al., 2018). We thus adopt this model as our baseline. Syntax has been proven beneficial for semantic tasks such as NLI (Bowman et al., 2016; Pang et al., 2019; Lei et al., 2019). Tree-based SPINN methods encode sentences by combining constituency phrases (Bowman et al., 2016). Recently, Pang et al. (2019)"
2020.findings-emnlp.447,P16-1139,0,0.159039,"stands for entailment, and N stands for non-entailment. and the hypothesis into embedding vectors, which are fed to a multi-layer neural network for classification (Bowman et al., 2015). It has been shown that alignment between local words in the premise and hypothesis benefits the aggregation of information (Chen et al., 2016; Parikh et al., 2016), and encoding the sentence pair simultaneously can capture more interaction and thus further improve the performance (Devlin et al., 2018). We thus adopt this model as our baseline. Syntax has been proven beneficial for semantic tasks such as NLI (Bowman et al., 2016; Pang et al., 2019; Lei et al., 2019). Tree-based SPINN methods encode sentences by combining constituency phrases (Bowman et al., 2016). Recently, Pang et al. (2019) proposed to enhance the token representation by using contextual vector representations from a pretrained parser. The GCN method has also been used to represent syntax for sentence matching (Lei et al., 2019), where the syntax of each sentence is encoded separately. In this paper, we use a GCN to encode a whole matching graph with syntactic information, showing that integrating syntax by our method benefits the generalization of"
2020.findings-emnlp.447,W03-0906,0,0.143785,"Missing"
2020.findings-emnlp.447,W19-3646,0,0.0152162,"However, there are still limitations on the generalization of such models to examples under a different distribution. In particular, it has been shown that seemingly simple types of examples in a carefully designed evaluation set (i.e. HANS) can lead to significant degeneration and large variability in performance (McCoy et al., 2019b,a). Table 1 shows a set of test cases from HANS, where premise and hypothesis have high lexical overlap but different syntactic structures. The BERT model gives incorrect results on most cases. This issue can negatively affect NLI applications such as dialogue (Dziri et al., 2019; Welleck et al., 2019). It has been shown that syntactic structures are useful for cross-domain generalization of NLP models (Wang et al., 2017; Strubell and McCallum, 2018). Intuitively, a more robust NLI model can be obtained by making use of structural information. We empirically investigate the effectiveness of syntactic features for enhancing the generalization of BERT-based matching models. In particular, given a pair of sentences, the dependency syntax of each sentence is obtained using a neural parser (Qi et al., 2018). The parse trees are then extended using four types of edge patter"
2020.findings-emnlp.447,D17-1159,0,0.0307879,"between each word wi in sentence A and each word wj in sentence B at the kth layer is cal(k) culated by the co-attention operation as Ci,j = (k)T (k) (k) σ(hi Wco hj ), where σ denotes the sigmoid function, h the feature vector, and Wco the affinity weight. The feature of node i is updated at the kth P (k+1) (k) (k) (k) layer by hi =f j∈N (i) gi,j (WE (i,j) hj +  (k) bE (i,j) , where f (·) is ReLU activation function, (k) N (·) is the neighbor set, and gi,j is a gate function that is described below. Note that we only take unlabelled dependencies into account to avoid over-parameterization (Marcheggiani and Titov, 2017), as shown in Equation 1. By bringing in sparse and unlabeled dependency relations, the embedding of each word is influenced by its immediately semantically or syntactically related words, which leads to a potentially more robust word representation. We apply a (k) gate gi,j to each edge to calculate the importance 4974 of information exchange (Marcheggiani and Titov, 2017). ( (k) Ci,j , if E (i, j) is co-attention; (k) gi,j = (k)T (k) (k) σ(hi vE (i,j) + dE (i,j) ), otherwise. (2) In addition, highway units are adopted in each layer to preserve information in multiple stacked GCN layers (Sriv"
2020.findings-emnlp.447,P19-1334,0,0.312622,"(Devlin et al., 2018) and XLNet (Yang et al., 2019) have given the state-of-the-art accuracy for this task. It has been shown that pre-trained models help to better capture heuristic patterns in a set of training data and therefore enhance indomain performance (Wang et al., 2018). However, there are still limitations on the generalization of such models to examples under a different distribution. In particular, it has been shown that seemingly simple types of examples in a carefully designed evaluation set (i.e. HANS) can lead to significant degeneration and large variability in performance (McCoy et al., 2019b,a). Table 1 shows a set of test cases from HANS, where premise and hypothesis have high lexical overlap but different syntactic structures. The BERT model gives incorrect results on most cases. This issue can negatively affect NLI applications such as dialogue (Dziri et al., 2019; Welleck et al., 2019). It has been shown that syntactic structures are useful for cross-domain generalization of NLP models (Wang et al., 2017; Strubell and McCallum, 2018). Intuitively, a more robust NLI model can be obtained by making use of structural information. We empirically investigate the effectiveness of"
2020.findings-emnlp.447,D16-1244,0,0.0827037,"Missing"
2020.findings-emnlp.447,K18-2016,0,0.021684,"s issue can negatively affect NLI applications such as dialogue (Dziri et al., 2019; Welleck et al., 2019). It has been shown that syntactic structures are useful for cross-domain generalization of NLP models (Wang et al., 2017; Strubell and McCallum, 2018). Intuitively, a more robust NLI model can be obtained by making use of structural information. We empirically investigate the effectiveness of syntactic features for enhancing the generalization of BERT-based matching models. In particular, given a pair of sentences, the dependency syntax of each sentence is obtained using a neural parser (Qi et al., 2018). The parse trees are then extended using four types of edge patterns, including a soft co-attention matching pattern that links the sentence pair into an integrated graph. A graph convolutional network (GCN) (Kipf and Welling, 2016) is used to represent the whole matching graph structure. Experiments show that the performance of the proposed model is much better than BERT and other syntax-based baselines on the category in HANS where the premise and non-entailment hypothesis have high lexical overlap but different syntactic structures, when both models are trained on MNLI dataset. It proves t"
2020.findings-emnlp.447,W18-2904,0,0.0216373,"ly simple types of examples in a carefully designed evaluation set (i.e. HANS) can lead to significant degeneration and large variability in performance (McCoy et al., 2019b,a). Table 1 shows a set of test cases from HANS, where premise and hypothesis have high lexical overlap but different syntactic structures. The BERT model gives incorrect results on most cases. This issue can negatively affect NLI applications such as dialogue (Dziri et al., 2019; Welleck et al., 2019). It has been shown that syntactic structures are useful for cross-domain generalization of NLP models (Wang et al., 2017; Strubell and McCallum, 2018). Intuitively, a more robust NLI model can be obtained by making use of structural information. We empirically investigate the effectiveness of syntactic features for enhancing the generalization of BERT-based matching models. In particular, given a pair of sentences, the dependency syntax of each sentence is obtained using a neural parser (Qi et al., 2018). The parse trees are then extended using four types of edge patterns, including a soft co-attention matching pattern that links the sentence pair into an integrated graph. A graph convolutional network (GCN) (Kipf and Welling, 2016) is used"
2020.findings-emnlp.447,W18-5446,0,0.0606201,"Missing"
2020.findings-emnlp.447,P17-1127,0,0.03452,"Missing"
2020.findings-emnlp.447,P19-1363,0,0.0131956,"still limitations on the generalization of such models to examples under a different distribution. In particular, it has been shown that seemingly simple types of examples in a carefully designed evaluation set (i.e. HANS) can lead to significant degeneration and large variability in performance (McCoy et al., 2019b,a). Table 1 shows a set of test cases from HANS, where premise and hypothesis have high lexical overlap but different syntactic structures. The BERT model gives incorrect results on most cases. This issue can negatively affect NLI applications such as dialogue (Dziri et al., 2019; Welleck et al., 2019). It has been shown that syntactic structures are useful for cross-domain generalization of NLP models (Wang et al., 2017; Strubell and McCallum, 2018). Intuitively, a more robust NLI model can be obtained by making use of structural information. We empirically investigate the effectiveness of syntactic features for enhancing the generalization of BERT-based matching models. In particular, given a pair of sentences, the dependency syntax of each sentence is obtained using a neural parser (Qi et al., 2018). The parse trees are then extended using four types of edge patterns, including a soft co"
2020.findings-emnlp.447,N18-1101,0,0.137296,"N The lawyer admired the students. N E N The secretary saw the managers. N E N The manager introduced the professor. N E E Hypothesis The managers saw the student. Table 1: Examples drawn from the “non-entailed lexical overlap” category in HANS (McCoy et al., 2019b) for the NLI task. In each example, the words in hypothesis are drawn from the premise but do not form a subsentence of premise, and thus the syntactic structures in hypothesis and premise are quite different. Both the BERT-CLS baseline and our GCN-based BERT model with co-attention links (BERT+CAGCN) are finetuned on MNLI dataset (Williams et al., 2018), and the neutral or contradiction labels are translated into non-entailment when evaluation (McCoy et al., 2019b). Note that E stands for entailment, and N stands for non-entailment. and the hypothesis into embedding vectors, which are fed to a multi-layer neural network for classification (Bowman et al., 2015). It has been shown that alignment between local words in the premise and hypothesis benefits the aggregation of information (Chen et al., 2016; Parikh et al., 2016), and encoding the sentence pair simultaneously can capture more interaction and thus further improve the performance (Dev"
2020.sdp-1.5,W19-5034,0,0.0333516,"ovidex.r5.d2q.1s (= expando + monoT5) r5.fusion2 r5.fusion1 Table 1: Selected TREC-COVID results. Our submissions are under teams “covidex” and “anserini”. All runs notated with † incorporate our infrastructure components in some way. Note that the metrics used in the first three rounds are different from those used in the final two rounds. 37 tion technique proved to be effective: when constructing the keyword query for a given topic, we take the non-stopwords from the query field and further expand them with terms belonging to named entities extracted from the question field using ScispaCy (Neumann et al., 2019). demonstrated our impact not only in developing effective ranking models, but also our service to the community in providing infrastructure. As another point of comparison, UIowaSRun3 (2c) represented a fusion of two traditional (i.e., term-based) relevance feedback runs, and did not use any neural networks. Interestingly, its effectiveness is not very far behind SparseDenseSciBert (2b), the best run in the feedback category (which does take advantage of BERT). It seems that BERTbased methods for exploiting relevance judgments yielded only modest improvements, likely due to the paucity of rel"
2020.sdp-1.5,2020.findings-emnlp.63,1,0.823597,"Missing"
2020.sdp-1.5,D19-1352,1,0.925726,"as follows: each full-text article was segmented into paragraphs and for each paragraph, we created a “document” comprising the title, abstract, and that paragraph. The title and abstract alone comprised an additional “document”. Thus, a fulltext article with n paragraphs yielded n + 1 separate retrieval units in the index. Keyword Search In our design, initial retrieval is performed by the Anserini IR toolkit (Yang et al., 2017, 2018),1 which we have been developing for several years and powers a number of our previous systems that incorporate various neural architectures (Yang et al., 2019; Yilmaz et al., 2019). Anserini represents an effort to better align real-world search applications with academic information retrieval research: under the covers, it builds on the popular and widely-deployed open-source Lucene search library, on top of which we provide a number of 1 To be consistent with standard IR parlance, we call each of these retrieval units a “document”, in a generic sense, despite their composite structure. In addition to the above indexing schemes, we considered three more based on our doc2query document expansion technique (Nogueira et al., 2019b; Nogueira and Lin, 2019). The idea behind"
2020.sdp-1.5,2020.nlpcovid19-acl.2,1,0.236165,"scientific articles (as of October, 2020), including most with full text, about COVID-19 and coronavirus-related research more broadly (for example, SARS and MERS). These articles are gathered from a variety of sources, including PubMed, a curated list of articles from the WHO, as well as preprints from arXiv, bioRxiv, and medRxiv. The goal of the effort is “to mobilize researchers to apply recent advances in natural language process3. Finally, we package the previous two components into Covidex, an end-to-end search engine and browsing interface deployed at covidex.ai, initially described in Zhang et al. (2020a). All three efforts have been successful. In the TRECCOVID challenge, our infrastructure and baselines have been adopted by many teams, which in some 31 Proceedings of the First Workshop on Scholarly Document Processing, pages 31–41 c Online, November 19, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 cases have submitted runs that scored higher than our own submissions. This illustrates the success of our infrastructure-building efforts (1). In round 3, we submitted the highest-scoring run that took advantage of previous training data and the secondhigh"
2020.sdp-1.5,N19-4013,1,0.878427,"Missing"
2020.semeval-1.39,2020.semeval-1.63,0,0.0364258,"atures from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. CUHK uses a Multitask Learning framework MTDNN (Liu et al., 2019b), adopting the “Explain, Reason and Predict” system. 313 Team Human BUT-FIT Solomon KaLM panaali* JUSTers cdjhz* JBNU ANA LMVE+ BLEU 22.4 19.3 18.5 17.2 16.1 16.0 15.9 15.7 12.9 Rank 1 2 3 4 5 6 7 8 9 Human 2.58 1.84 1.84 2.08 1.22 1.94 1.75 1.80 2.10 1.78 Rank 4 4 2 11 3 8 6 1 7 Team C"
2020.semeval-1.39,2020.acl-main.130,1,0.90634,"ts to generate natural language statements that obey or violate their commonsense knowledge about the world. 1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b; Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016; Talmor et al., 2018; Mihaylov et al., 2018). They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge. The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process. The SemEval-2020 Task 4 includes three subtasks on testing whether a system can distinguish natural language statements that make sense from those that do not, and probe the reasons. In the first subtask, a *"
2020.semeval-1.39,2020.semeval-1.77,0,0.0358153,"RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. CUHK uses a Multitask Learning framework MTDNN (Liu et al., 2019b), adopting the “Explain, Reason and Predict” system. 313 Team Human BUT-FIT Solomon KaLM panaali* JUSTers cdjhz* JBNU ANA LMVE+ BLEU 22.4 19.3 18.5 17.2 16.1 16.0 15.9 15.7 12.9 Rank 1 2 3 4 5 6 7 8 9 Human 2.58 1.84 1.84 2.08 1.22 1.94 1.75 1.80 2.10 1.78"
2020.semeval-1.39,2020.semeval-1.61,0,0.0336443,"75.8 10 11 12 13 14 15 16 17 18 Team Acc. Rank Masked Reasoner KDE SenseForce SSN-NLP TakeLab* UoR dania* CUHK bhu* praveenjoshi007* 73.5 72.8 68.3 66.8 65.9 55.5 51.2 36.4 32.6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT,"
2020.semeval-1.39,2020.semeval-1.78,0,0.130264,"LP TakeLab* UoR dania* CUHK bhu* praveenjoshi007* 73.5 72.8 68.3 66.8 65.9 55.5 51.2 36.4 32.6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li"
2020.semeval-1.39,2020.semeval-1.66,0,0.186821,"18 Team Acc. Rank Masked Reasoner KDE SenseForce SSN-NLP TakeLab* UoR dania* CUHK bhu* praveenjoshi007* 73.5 72.8 68.3 66.8 65.9 55.5 51.2 36.4 32.6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. •"
2020.semeval-1.39,P17-1025,0,0.0502593,"gue reasoning in the commonsense area (Cui et al., 2020). Those questions are not easy to answer without specializing certain domain knowledge, while our questions are based on daily common sense. Some datasets focus on non-sentential eventual plausibility (Wang et al., 2018; Porada et al., 2019), such as “gorilla-ride-camel”. In contrast, our dataset is based on statements which includes events, descriptions, assertion etc, not merely events, such as “China’s territory is larger than Japan’s”. And some datasets concentrate on limited attributes or actions of world knowledge, such as physics (Forbes and Choi, 2017). Our dataset concerns general commonsense knowledge beyond just physical common sense, the sentence in our task “Tom’s mom become (happy)/(upset) when Tom gets high grades in the exam” is about social and emotional common sense. For our first task, those statements that conforms to commonsense can also be phrased as being plausible. Thus our first task is similar to plausibility tests, despite that plausibility has a broader scope while our focus is on commonsense only. More importantly, compared with our work, the above tasks do not directly estimate general common sense or ask the logical r"
2020.semeval-1.39,2020.semeval-1.46,0,0.1313,"(Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. C"
2020.semeval-1.39,2020.semeval-1.45,0,0.0262627,"on these sequences with a next token prediction objective. At the test time, based on the statement, the model generates the reason tokens until the end-of-sentence token is generated. • KaLM (Wan and Huang, 2020) uses the sequence-to-sequence architecture BART. To enhance the source side statement, they extract keywords from the statement and search for evidence from Wiktionary.2 After that, they concatenate the evidence along with the original statement as the source sentence for the generation. This approach proves effective and makes their system second-best for human evaluations. • ANA (Konar et al., 2020) has the highest human evaluation score with a multitask learning framework. Specifically, they use a decoder-only transformer based on GPT-2 as the backbone model, and train 2 Wiktionary version: enwiktionary-20200220 314 the model with two self-attention heads: one for language models and another for classification. They then use data from both task B and task C to calculate language model loss and classification loss. Furthermore, they use OMCS at the pretraining stage and use CoS-E (Rajani et al., 2019) and OpenBook (Mihaylov et al., 2018) at the task-specific training stage. • Solomon (Sr"
2020.semeval-1.39,2020.semeval-1.69,0,0.0316577,"20), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. CUHK uses a Multitask Learning framework MTDNN (Liu e"
2020.semeval-1.39,P19-1441,0,0.13007,"his figure is mostly based on Team Solomon’s system. For Subtask B and C, the connector can be simply “No, ”, to help in constraining the model to learn a choice that explains the unreasonability of the statement. For Subtask A and B, the pretrained models are finetuned on the task-specific data with MLM-objective, and then trained as a binary classification task to score each input. For Subtask C, the cross-entropy loss of next-token-prediction is used to train the model, and beam search is used at inference. adopted the pretrained language models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019c), XLNET (Yang et al., 2019) and ALBERT (Lan et al., 2019) as the encoder of the model, and then finetune on the training set of the task. See Figure 1 for the most commonly-used model architectures for Subtask A and B. Also, the top-performing systems take advantage of external knowledge graphs such as ConceptNet (Speer et al., 2017), or unstructured text containing commonsense knowledge. Below we introduce in detail several top-performing systems and their main features. • CN-HIT-IT.NLP (Zhang et al., 2020) ranks top in Subtask A. They use a variant of K-BERT (Liu et al., 2019a) as the enco"
2020.semeval-1.39,2020.semeval-1.70,0,0.0390468,"(Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. CUHK uses a Multitask Lear"
2020.semeval-1.39,2020.semeval-1.50,0,0.0260087,"Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020),"
2020.semeval-1.39,2020.semeval-1.49,0,0.038915,".6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation f"
2020.semeval-1.39,2020.semeval-1.52,0,0.0366318,"3.5 72.8 68.3 66.8 65.9 55.5 51.2 36.4 32.6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT"
2020.semeval-1.39,D18-1260,0,0.367889,"ate their commonsense knowledge about the world. 1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b; Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016; Talmor et al., 2018; Mihaylov et al., 2018). They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge. The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process. The SemEval-2020 Task 4 includes three subtasks on testing whether a system can distinguish natural language statements that make sense from those that do not, and probe the reasons. In the first subtask, a * Equal Contribution This work is licensed under a Creative Commons"
2020.semeval-1.39,2020.semeval-1.75,0,0.0348917,"20), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT"
2020.semeval-1.39,N16-1098,0,0.222829,"e. In our task, we take an operational definition of making sense by asking human subjects to generate natural language statements that obey or violate their commonsense knowledge about the world. 1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b; Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016; Talmor et al., 2018; Mihaylov et al., 2018). They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge. The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process. The SemEval-2020 Task 4 includes three subtasks on testing whether a system can distinguish natural language statements that make"
2020.semeval-1.39,2020.semeval-1.65,0,0.193562,"iaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. CUHK uses a Multitask Learning framework MTDNN (Liu et al., 2019b), adopting the “Explain, Reason and Predict” system. 313 Team Human BUT-FIT Solomon KaLM panaali* JUSTers cdjhz* JBNU ANA LMVE+ BLEU 22.4 19.3 18.5 17.2 16.1 16.0 15.9 15.7 12.9 Rank 1 2 3 4 5 6 7 8 9 Human 2.58 1.84 1.84 2.08 1.22 1.94 1.75 1.80 2.10 1.78 Rank 4 4 2 11 3 8 6 1 7 Team CN-HIT-IT.NLP+ SWAGex UI TMLab* CUHK SSN-NLP UoR+ Masked Reasoner+ BLEU Rank Human Rank 9.7 7.1 5.5 5.4 4.3 2.2 0.9 0.6 1"
2020.semeval-1.39,P19-1459,0,0.0624979,"Missing"
2020.semeval-1.39,L18-1564,0,0.169229,"operational definition of making sense by asking human subjects to generate natural language statements that obey or violate their commonsense knowledge about the world. 1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b; Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016; Talmor et al., 2018; Mihaylov et al., 2018). They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge. The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process. The SemEval-2020 Task 4 includes three subtasks on testing whether a system can distinguish natural language statements that make sense from those that d"
2020.semeval-1.39,S18-1119,0,0.191276,"operational definition of making sense by asking human subjects to generate natural language statements that obey or violate their commonsense knowledge about the world. 1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b; Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016; Talmor et al., 2018; Mihaylov et al., 2018). They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge. The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process. The SemEval-2020 Task 4 includes three subtasks on testing whether a system can distinguish natural language statements that make sense from those that d"
2020.semeval-1.39,2020.semeval-1.80,0,0.0328145,"ration using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. CUHK uses a Multitask Learning framework MTDNN (Liu et al., 2019b), adopting the “Explain, Reason and Predict” system. 313 Team Human BUT-FIT Solomon KaLM panaali* JUSTers cdjhz* JBNU ANA LMVE+ BLEU 22.4 19.3 18.5 17.2 16.1 16.0 15.9 15.7 12.9 Rank 1 2 3 4 5 6 7 8 9 Human 2.58 1.84 1.84 2.08 1.22 1.94 1.75 1.80 2.10 1.78 Rank 4 4 2 11 3 8 6 1 7 Team CN-HIT-IT.NLP+ SWAGex UI TML"
2020.semeval-1.39,P02-1040,0,0.114832,"Missing"
2020.semeval-1.39,D19-6015,0,0.0352032,"t elementary level science that are related to the questions. The AI2 Reasoning Challenge (ARC) (Clark et al., 2018) gives thousands of questions with different knowledge types, as well as a relevant 14M-sentence corpus, mixed with science facts and other narrative sentences. MuTual provides a dataset for Multi-Turn dialogue reasoning in the commonsense area (Cui et al., 2020). Those questions are not easy to answer without specializing certain domain knowledge, while our questions are based on daily common sense. Some datasets focus on non-sentential eventual plausibility (Wang et al., 2018; Porada et al., 2019), such as “gorilla-ride-camel”. In contrast, our dataset is based on statements which includes events, descriptions, assertion etc, not merely events, such as “China’s territory is larger than Japan’s”. And some datasets concentrate on limited attributes or actions of world knowledge, such as physics (Forbes and Choi, 2017). Our dataset concerns general commonsense knowledge beyond just physical common sense, the sentence in our task “Tom’s mom become (happy)/(upset) when Tom gets high grades in the exam” is about social and emotional common sense. For our first task, those statements that con"
2020.semeval-1.39,P19-1487,0,0.0441511,"proach proves effective and makes their system second-best for human evaluations. • ANA (Konar et al., 2020) has the highest human evaluation score with a multitask learning framework. Specifically, they use a decoder-only transformer based on GPT-2 as the backbone model, and train 2 Wiktionary version: enwiktionary-20200220 314 the model with two self-attention heads: one for language models and another for classification. They then use data from both task B and task C to calculate language model loss and classification loss. Furthermore, they use OMCS at the pretraining stage and use CoS-E (Rajani et al., 2019) and OpenBook (Mihaylov et al., 2018) at the task-specific training stage. • Solomon (Srivastava et al., 2020), JUSTers (Fadel et al., 2020), SWAGex (Rim and Okazaki, 2020), UI (Doxolodeo and Mahendra, 2020) and CUHK (Wang et al., 2020) use GPT or GPT-2 finetuned on the task training data. JBNU (Na and Lee, 2020) uses UniLM, which incorporates three LM tasks: unidirectional LM, bidirectional LM and sequence-to-sequence prediction LM, and only use one of the reference correct reasons. UI does not use the training data and treats the generation as a Cloze task. SSN-NLP (S, 2020) uses the seq2seq"
2020.semeval-1.39,P18-1043,0,0.0303324,"sts, despite that plausibility has a broader scope while our focus is on commonsense only. More importantly, compared with our work, the above tasks do not directly estimate general common sense or ask the logical reasons behind the correct answers and questions. In recent years, some large-scale commonsense inference knowledge resources have been developed, which may be helpful in commonsense reasoning tasks. Atomic (Sap et al., 2018) presents a large-scale everyday commonsense knowledge graph, which has nine if-then relations with variables, including causes, effects, and so on. Event2Mind (Rashkin et al., 2018) proposes a new corpus and task, aiming to find out the mentioned/unmentioned people’s intents and reactions under various daily circumstances. These datasets are not directly useful for our benchmark since they focus only on a small domain. ConceptNet is a seminal knowledge graph that has been upgraded over time (Liu and Singh, 2004; Havasi et al., 2007; Speer and Havasi, 2013; Speer et al., 2017). ConceptNet constructs triples using labeled edges as relations and various words and/or phrases as entities. It also has the sentences describing the corresponding triples. In contrast to these dat"
2020.semeval-1.39,2020.semeval-1.51,0,0.208096,"T (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. CUHK uses a Multitask Learning framework MTDNN (Liu et al., 2019b), adopting the “Explain, Reason and Predict” system. 313 Team Human BUT-FIT Solomon KaLM panaali* JUSTers cdjhz* JBNU ANA LMVE+ BLEU 22.4 19.3 18.5 17.2 16.1 16.0 15.9 15.7 12.9 Rank 1 2 3 4 5 6 7 8 9 Human 2.58 1.84 1.84 2.08 1.22 1.94 1.75 1.80 2.10 1.78 Rank 4 4 2 11 3 8 6 1 7 Team CN-HIT-IT.NLP+ SWAGex UI TMLab* CUHK SSN-NLP UoR+ Masked Reasoner+ BLEU Rank Human Rank 9.7 7.1 5.5 5.4 4.3 2.2 0.9 0.6 10 11 12 13 14 15 16 17 1.74 1.75"
2020.semeval-1.39,2020.semeval-1.73,0,0.0561971,"Missing"
2020.semeval-1.39,2020.semeval-1.62,0,0.0343034,"soner KDE SenseForce SSN-NLP TakeLab* UoR dania* CUHK bhu* praveenjoshi007* 73.5 72.8 68.3 66.8 65.9 55.5 51.2 36.4 32.6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LM"
2020.semeval-1.39,K17-1004,0,0.0537086,"Missing"
2020.semeval-1.39,P18-2119,0,0.0175848,"uation. Situations with Adversarial Generations (SWAG) (Zellers et al., 2018) request a system to choose the most likely-to-happen alternative after a specific situation. Those datasets emphasize the pre-situations and/or the after-situations of certain situations, but not on the reasons why they occur or are caused. Besides, our dataset is not limited to events or situations. It concerns a broader commonsense setting, which includes events, descriptions, assertion etc. Some datasets are inspired by reading comprehension. The Story Cloze Test and ROCStories Corpora (Mostafazadeh et al., 2016; Sharma et al., 2018) aim to figure out the right ending from two candidate sentences after a four-sentence story. For a narrative text, MCScript (Ostermann et al., 2018a) gives various types of questions and pairs of answer candidates for each question. Most questions require knowledge beyond the facts mentioned in the text. Compared to those reading comprehension tasks, our benchmark encourages people to use any external resources they want. Some other datasets evolve from QA problems and care more about factual commonsense knowledge. SQUABU (Davis, 2016) provides a small hand-constructed test of commonsense and"
2020.semeval-1.39,2020.semeval-1.74,0,0.141097,"b* UI ehsantaher* uzh* 91.4 90.8 89.0 85.3 84.6 82.0 80.5 79.3 75.8 10 11 12 13 14 15 16 17 18 Team Acc. Rank Masked Reasoner KDE SenseForce SSN-NLP TakeLab* UoR dania* CUHK bhu* praveenjoshi007* 73.5 72.8 68.3 66.8 65.9 55.5 51.2 36.4 32.6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Moh"
2020.semeval-1.39,2020.semeval-1.76,0,0.021044,"aveenjoshi007* 73.5 72.8 68.3 66.8 65.9 55.5 51.2 36.4 32.6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) us"
2020.semeval-1.39,2020.semeval-1.67,0,0.0678589,"89.0 85.3 84.6 82.0 80.5 79.3 75.8 10 11 12 13 14 15 16 17 18 Team Acc. Rank Masked Reasoner KDE SenseForce SSN-NLP TakeLab* UoR dania* CUHK bhu* praveenjoshi007* 73.5 72.8 68.3 66.8 65.9 55.5 51.2 36.4 32.6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) us"
2020.semeval-1.39,N18-2049,0,0.283234,"rstand whether a given statement makes sense. In our task, we take an operational definition of making sense by asking human subjects to generate natural language statements that obey or violate their commonsense knowledge about the world. 1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b; Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016; Talmor et al., 2018; Mihaylov et al., 2018). They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge. The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process. The SemEval-2020 Task 4 includes three subtasks on testing whether a system can"
2020.semeval-1.39,P19-1393,1,0.559422,"nces. These datasets are not directly useful for our benchmark since they focus only on a small domain. ConceptNet is a seminal knowledge graph that has been upgraded over time (Liu and Singh, 2004; Havasi et al., 2007; Speer and Havasi, 2013; Speer et al., 2017). ConceptNet constructs triples using labeled edges as relations and various words and/or phrases as entities. It also has the sentences describing the corresponding triples. In contrast to these datasets, we investigate the evaluation of common sense, rather than building a resource. Before organizing this shared-task, a pilot study (Wang et al., 2019) has been performed, showing that there is still a significant gap between human and machine performance when no training data is provided, despite that the models have already been pretrained with over 100 million natural language sentences. In our task here, we also provide training data with human annotations. 6 Summary This paper summarizes SemEval-2020 Task 4: Commonsense Validation and Explanation. In this task, we construct a dataset that consists of 11,997 instances and 83,986 sentences. The task attracted around 40 participating teams, out of which 31 teams submit their system papers."
2020.semeval-1.39,2020.semeval-1.47,0,0.159388,"s in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. CUHK uses a Multitask Learning framework MTDNN (Liu et al., 2019b), adopting the “Explain, Reason and Predict” system. 313 Team Human BUT-FIT Solomon KaLM panaali* JUSTers cdjhz* JBNU ANA LMVE+ BLEU 22.4 19.3 18.5 17.2 16.1 16.0 15.9 15.7 12.9 Rank 1 2 3 4 5 6 7 8 9 Human 2.58 1.84 1.84 2.08 1.22 1.94 1.75 1.80 2.10 1.78 Rank 4 4 2 11 3 8 6 1 7 Team CN-HIT-IT.NLP+ SWAGex UI TMLab* CUHK SSN-NLP UoR+ Masked Reasoner+ BLEU Rank Human Rank 9.7 7.1 5"
2020.semeval-1.39,2020.semeval-1.42,0,0.0194753,"triples visible only to the corresponding entity. They use ConceptNet as the commonsense repository to extract the triples for the statements. • ECNU-SenseMaker (Zhao et al., 2020) ranks top in Subtask B. They use Knowledge-enhanced Graph Attention Network to leverage heterogeneous knowledge from both the structured knowledge base (i.e. ConceptNet) and the unstructured text to better improve the commonsense understanding. Like CN-HIT-IT.NLP, their model is also based on K-BERT. In addition, they use unstructured text from ConceptNet and Subtask C to pretrain the language model. • IIE-NLP-NUT (Xing et al., 2020) uses RoBERTa as the encoder, and conduct a second pretraining on the original RoBERTa model with the textual corpus from Open Mind Common Sense (Singh et 312 Team Human CN-HIT-IT.NLP ECNU-SenseMaker IIE-NLP-NUT nlpx* Solomon Qiaoning BUT-FIT olenet* KaLM CS-NET fkerem* JUSTers CS-NLP Acc. 99.1 97.0 96.7 96.4 96.4 96.0 95.9 95.8 95.5 95.3 94.8 94.4 92.9 92.7 Rank 1 2 3 3 5 6 7 8 9 10 11 12 13 Team Acc. Rank panaali* ZhengxianFan* LMVE Warren* TMLab* UAICS JUST eggy* UI Armins* DEEPYANG WUY* YNU-oxz 92.5 92.4 90.4 90.4 89.2 89.1 89.1 89.0 88.2 87.1 85.1 84.2 83.6 14 15 16 16 18 19 19 21 22 23 2"
2020.semeval-1.39,D18-1009,0,0.310879,"tial to gauge how well computers can understand whether a given statement makes sense. In our task, we take an operational definition of making sense by asking human subjects to generate natural language statements that obey or violate their commonsense knowledge about the world. 1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b; Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016; Talmor et al., 2018; Mihaylov et al., 2018). They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge. The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process. The SemEval-2020 Task 4 includes three su"
2020.semeval-1.39,2020.semeval-1.60,0,0.0190605,"ce. adopted the pretrained language models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019c), XLNET (Yang et al., 2019) and ALBERT (Lan et al., 2019) as the encoder of the model, and then finetune on the training set of the task. See Figure 1 for the most commonly-used model architectures for Subtask A and B. Also, the top-performing systems take advantage of external knowledge graphs such as ConceptNet (Speer et al., 2017), or unstructured text containing commonsense knowledge. Below we introduce in detail several top-performing systems and their main features. • CN-HIT-IT.NLP (Zhang et al., 2020) ranks top in Subtask A. They use a variant of K-BERT (Liu et al., 2019a) as the encoder to enhance language representations through knowledge graphs. K-BERT is a Transformer-based model, which enhances the language representations of the text by injecting relevant triples from a knowledge graph to form a knowledge-rich sentence tree, and then uses a mask-Transformer to make the triples visible only to the corresponding entity. They use ConceptNet as the commonsense repository to extract the triples for the statements. • ECNU-SenseMaker (Zhao et al., 2020) ranks top in Subtask B. They use Know"
2020.semeval-1.39,2020.semeval-1.48,0,0.0351203,"heir main features. • CN-HIT-IT.NLP (Zhang et al., 2020) ranks top in Subtask A. They use a variant of K-BERT (Liu et al., 2019a) as the encoder to enhance language representations through knowledge graphs. K-BERT is a Transformer-based model, which enhances the language representations of the text by injecting relevant triples from a knowledge graph to form a knowledge-rich sentence tree, and then uses a mask-Transformer to make the triples visible only to the corresponding entity. They use ConceptNet as the commonsense repository to extract the triples for the statements. • ECNU-SenseMaker (Zhao et al., 2020) ranks top in Subtask B. They use Knowledge-enhanced Graph Attention Network to leverage heterogeneous knowledge from both the structured knowledge base (i.e. ConceptNet) and the unstructured text to better improve the commonsense understanding. Like CN-HIT-IT.NLP, their model is also based on K-BERT. In addition, they use unstructured text from ConceptNet and Subtask C to pretrain the language model. • IIE-NLP-NUT (Xing et al., 2020) uses RoBERTa as the encoder, and conduct a second pretraining on the original RoBERTa model with the textual corpus from Open Mind Common Sense (Singh et 312 Tea"
2020.tacl-1.38,P18-1026,0,0.167178,"istical methods (Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2017). Recently, several neural graph-to-text models have exhibited success by leveraging encoder mechanisms based on LSTMs, GNNs, and Transformers. AMR-to-Text Generation. Various neural models have been proposed to generate sentences from Abstract Meaning Representation (AMR) graphs. Konstas et al. (2017) provide the first neural approach for this task, by linearizing the input graph as a sequence of nodes and edges. Song et al. (2018) propose the graph recurrent network to directly encode the AMR nodes, whereas Beck et al. (2018) develop a model based on gated GNNs. 590 However, both approaches only use local node aggregation strategies. Damonte and Cohen (2019) combine graph convolutional networks and LSTMs in order to learn complementary node contexts. However, differently from Transformers and GNNs, LSTMs generate node representations that are influenced by the node order. Ribeiro et al. (2019) develop a model based on different GNNs that learns node representations which simultaneously encode a top–down and a bottom–up views of the AMR graphs, whereas Guo et al. (2019) leverage dense connectivity in GNNs. Recently"
2020.tacl-1.38,D19-1052,0,0.0837704,"Missing"
2020.tacl-1.38,D14-1179,0,0.026513,"Missing"
2020.tacl-1.38,N16-1087,0,0.0487873,"ing distant connections between all nodes, we allow for these missing links to be captured, as KGs are known to be highly incomplete (Dong et al., 2014; Schlichtkrull et al., 2018). In contrast, the local strategy refines the node representation with richer neighborhood information, as nodes that share the same neighborhood exhibit a strong homophily: Two similar entities are much more likely to be connected than at random. Consequently, the local context enriches the node representation with local information 2 Related Work Early efforts for graph-to-text generation used statistical methods (Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2017). Recently, several neural graph-to-text models have exhibited success by leveraging encoder mechanisms based on LSTMs, GNNs, and Transformers. AMR-to-Text Generation. Various neural models have been proposed to generate sentences from Abstract Meaning Representation (AMR) graphs. Konstas et al. (2017) provide the first neural approach for this task, by linearizing the input graph as a sequence of nodes and edges. Song et al. (2018) propose the graph recurrent network to directly encode the AMR nodes, whereas Beck et al. (2018) develop a model bas"
2020.tacl-1.38,D18-1113,1,0.856445,"nerating text from KGs. In comparison to AMRs, which are rooted and connected graphs, KGs do not have a defined topology, which may vary widely among different datasets, making the generation process more demanding. KGs are sparse structures that potentially contain a large number of relations. Moreover, we are typically interested in generating multisentence texts from KGs, and this involves solving document planning issues (Konstas and Lapata, 2013). Recent neural approaches for KG-to-text generation simply linearize the KG triples, thereby loosing graph structure information. For instance, Colin and Gardent (2018), Moryossef et al. (2019), and Adapt (Gardent et al., 2017) utilize LSTM/ GRU to encode WebNLG graphs. Castro Ferreira et al. (2019) systematically compare pipeline and end-to-end models for text generation from WebNLG graphs. Trisedya et al. (2018) develop a graph encoder based on LSTMs that captures relationships within and between triples. Previous work has also studied how to explicitly encode the graph structure using GNNs or Transformers. Marcheggiani and Perez Beltrachini (2018) propose an encoder based on graph convolutional networks, that consider explicitly local node contexts, and s"
2020.tacl-1.38,W17-3518,1,0.256222,"F. R. Ribeiro† , Yue Zhang‡ , Claire Gardent§ and Iryna Gurevych† † Research Training Group AIPHES and UKP Lab, Technische Universit¨at Darmstadt ‡ School of Engineering, Westlake University, § CNRS/LORIA, Nancy, France ribeiro@aiphes.tu-darmstadt.de, yue.zhang@wias.org.cn claire.gardent@loria.fr, gurevych@ukp.informatik.tu-darmstadt.de Abstract be ordered and connected using appropriate discourse markers; and inter-sentential anaphora and ellipsis may need to be generated to avoid repetition. In this paper, we focus on generating texts rather than sentences where the output are short texts (Gardent et al., 2017) or paragraphs (KoncelKedziorski et al., 2019). A key issue in neural graph-to-text generation is how to encode the input graphs. The basic idea is to incrementally compute node representations by aggregating structural context information. To this end, two main approaches have been proposed: (i) models based on local node aggregation, usually built upon Graph Neural Networks (GNNs) (Kipf and Welling, 2017; Hamilton et al., 2017) and (ii) models that leverage global node aggregation. Systems that adopt global encoding strategies are typically based on Transformers (Vaswani et al. 2017), using"
2020.tacl-1.38,N19-1366,0,0.0564672,"Missing"
2020.tacl-1.38,W14-3348,0,0.0087213,"That is, we create a new relation node for each edge relation between two nodes. The new relation node is connected to the subject and object token entities by two binary relations, respectively. seeds, for the test sets, we report the averages over 4 training runs along with their standard deviation. We use byte pair encoding (Sennrich et al., 2016) to split entity words into smaller more frequent pieces. Therefore some nodes in the graph can be sub-words. We also obtain sub-words on the target side. Following previous works, we evaluate the results with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and CHRF++ (Popovi´c, 2015) automatic metrics and also perform a human evaluation (Section 5.6). For layer-wise models, the number of encoder layers are chosen from {2, 4, 6}, and for PGE and CGE, the global and local layers are chosen from and {2, 4, 6} and {1, 2, 3}, respectively. The hidden encoder dimensions are chosen from {256, 384, 448} (see Figure 3). Hyperparameters are tuned on the development set of both datasets. We report the test results when the BLEU score on dev set is optimal. 5 Experiments 5.1 Results on AGENDA We implemented all our models using PyTorch Geometric (PyG) (Fe"
2020.tacl-1.38,P17-4012,0,0.0669681,"Missing"
2020.tacl-1.38,P02-1040,0,0.109526,"n entities (Beck et al., 2018). That is, we create a new relation node for each edge relation between two nodes. The new relation node is connected to the subject and object token entities by two binary relations, respectively. seeds, for the test sets, we report the averages over 4 training runs along with their standard deviation. We use byte pair encoding (Sennrich et al., 2016) to split entity words into smaller more frequent pieces. Therefore some nodes in the graph can be sub-words. We also obtain sub-words on the target side. Following previous works, we evaluate the results with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and CHRF++ (Popovi´c, 2015) automatic metrics and also perform a human evaluation (Section 5.6). For layer-wise models, the number of encoder layers are chosen from {2, 4, 6}, and for PGE and CGE, the global and local layers are chosen from and {2, 4, 6} and {1, 2, 3}, respectively. The hidden encoder dimensions are chosen from {256, 384, 448} (see Figure 3). Hyperparameters are tuned on the development set of both datasets. We report the test results when the BLEU score on dev set is optimal. 5 Experiments 5.1 Results on AGENDA We implemented all our mode"
2020.tacl-1.38,N19-1238,0,0.214575,"Missing"
2020.tacl-1.38,W15-3049,0,0.0541798,"Missing"
2020.tacl-1.38,P17-1014,0,0.167206,"odels that encode an input graph combining both global and local node contexts, in order to learn better contextualized node embeddings. In our experiments, we demonstrate that our approaches lead to significant improvements on two graph-totext datasets achieving BLEU scores of 18.01 on the AGENDA dataset, and 63.69 on the WebNLG dataset for seen categories, outperforming state-of-the-art models by 3.7 and 3.1 points, respectively.1 1 Introduction Graph-to-text generation refers to the task of generating natural language text from input graph structures, which can be semantic representations (Konstas et al., 2017) or knowledge graphs (KGs) (Gardent et al., 2017; Koncel-Kedziorski et al., 2019). Whereas most recent work (Song et al., 2018; Ribeiro et al., 2019; Guo et al., 2019) focuses on generating sentences, a more challenging and interesting scenario emerges when the goal is to generate multisentence texts. In this context, in addition to sentence generation, document planning needs to be handled: The input needs to be mapped into several sentences; sentences need to 1 Code is available at https://github.com/UKPLab/ kg2text. 589 Transactions of the Association for Computational Linguistics, vol. 8,"
2020.tacl-1.38,W16-6603,0,0.0488296,"between all nodes, we allow for these missing links to be captured, as KGs are known to be highly incomplete (Dong et al., 2014; Schlichtkrull et al., 2018). In contrast, the local strategy refines the node representation with richer neighborhood information, as nodes that share the same neighborhood exhibit a strong homophily: Two similar entities are much more likely to be connected than at random. Consequently, the local context enriches the node representation with local information 2 Related Work Early efforts for graph-to-text generation used statistical methods (Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2017). Recently, several neural graph-to-text models have exhibited success by leveraging encoder mechanisms based on LSTMs, GNNs, and Transformers. AMR-to-Text Generation. Various neural models have been proposed to generate sentences from Abstract Meaning Representation (AMR) graphs. Konstas et al. (2017) provide the first neural approach for this task, by linearizing the input graph as a sequence of nodes and edges. Song et al. (2018) propose the graph recurrent network to directly encode the AMR nodes, whereas Beck et al. (2018) develop a model based on gated GNNs. 590 Howev"
2020.tacl-1.38,D13-1157,0,0.0618111,"ransformers, but learn globalized node representations, modeling graph paths in order to capture structural relations. KG-to-Text Generation. In this work, we focus on generating text from KGs. In comparison to AMRs, which are rooted and connected graphs, KGs do not have a defined topology, which may vary widely among different datasets, making the generation process more demanding. KGs are sparse structures that potentially contain a large number of relations. Moreover, we are typically interested in generating multisentence texts from KGs, and this involves solving document planning issues (Konstas and Lapata, 2013). Recent neural approaches for KG-to-text generation simply linearize the KG triples, thereby loosing graph structure information. For instance, Colin and Gardent (2018), Moryossef et al. (2019), and Adapt (Gardent et al., 2017) utilize LSTM/ GRU to encode WebNLG graphs. Castro Ferreira et al. (2019) systematically compare pipeline and end-to-end models for text generation from WebNLG graphs. Trisedya et al. (2018) develop a graph encoder based on LSTMs that captures relationships within and between triples. Previous work has also studied how to explicitly encode the graph structure using GNNs"
2020.tacl-1.38,D19-1314,1,0.905479,"iments, we demonstrate that our approaches lead to significant improvements on two graph-totext datasets achieving BLEU scores of 18.01 on the AGENDA dataset, and 63.69 on the WebNLG dataset for seen categories, outperforming state-of-the-art models by 3.7 and 3.1 points, respectively.1 1 Introduction Graph-to-text generation refers to the task of generating natural language text from input graph structures, which can be semantic representations (Konstas et al., 2017) or knowledge graphs (KGs) (Gardent et al., 2017; Koncel-Kedziorski et al., 2019). Whereas most recent work (Song et al., 2018; Ribeiro et al., 2019; Guo et al., 2019) focuses on generating sentences, a more challenging and interesting scenario emerges when the goal is to generate multisentence texts. In this context, in addition to sentence generation, document planning needs to be handled: The input needs to be mapped into several sentences; sentences need to 1 Code is available at https://github.com/UKPLab/ kg2text. 589 Transactions of the Association for Computational Linguistics, vol. 8, pp. 589–604, 2020. https://doi.org/10.1162/tacl a 00332 Action Editor: Alessandro Moschitti. Submission batch: 2/2019; Revision batch: 5/2020; Publi"
2020.tacl-1.38,W18-6501,0,0.122303,"Missing"
2020.tacl-1.38,P16-1162,0,0.0556469,"osion, we use regularization based on the basis function decomposition to define the model relation weights (Schlichtkrull et al., 2018). Also, as an alternative, we use the Levi Transformation to create nodes from relational edges between entities (Beck et al., 2018). That is, we create a new relation node for each edge relation between two nodes. The new relation node is connected to the subject and object token entities by two binary relations, respectively. seeds, for the test sets, we report the averages over 4 training runs along with their standard deviation. We use byte pair encoding (Sennrich et al., 2016) to split entity words into smaller more frequent pieces. Therefore some nodes in the graph can be sub-words. We also obtain sub-words on the target side. Following previous works, we evaluate the results with BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), and CHRF++ (Popovi´c, 2015) automatic metrics and also perform a human evaluation (Section 5.6). For layer-wise models, the number of encoder layers are chosen from {2, 4, 6}, and for PGE and CGE, the global and local layers are chosen from and {2, 4, 6} and {1, 2, 3}, respectively. The hidden encoder dimensions are chosen"
2020.tacl-1.38,P16-1000,0,0.208449,"Missing"
2020.tacl-1.38,P17-2002,1,0.873213,"w for these missing links to be captured, as KGs are known to be highly incomplete (Dong et al., 2014; Schlichtkrull et al., 2018). In contrast, the local strategy refines the node representation with richer neighborhood information, as nodes that share the same neighborhood exhibit a strong homophily: Two similar entities are much more likely to be connected than at random. Consequently, the local context enriches the node representation with local information 2 Related Work Early efforts for graph-to-text generation used statistical methods (Flanigan et al., 2016; Pourdamghani et al., 2016; Song et al., 2017). Recently, several neural graph-to-text models have exhibited success by leveraging encoder mechanisms based on LSTMs, GNNs, and Transformers. AMR-to-Text Generation. Various neural models have been proposed to generate sentences from Abstract Meaning Representation (AMR) graphs. Konstas et al. (2017) provide the first neural approach for this task, by linearizing the input graph as a sequence of nodes and edges. Song et al. (2018) propose the graph recurrent network to directly encode the AMR nodes, whereas Beck et al. (2018) develop a model based on gated GNNs. 590 However, both approaches"
2020.tacl-1.38,2020.tacl-1.2,0,0.0329849,"evelop a model based on gated GNNs. 590 However, both approaches only use local node aggregation strategies. Damonte and Cohen (2019) combine graph convolutional networks and LSTMs in order to learn complementary node contexts. However, differently from Transformers and GNNs, LSTMs generate node representations that are influenced by the node order. Ribeiro et al. (2019) develop a model based on different GNNs that learns node representations which simultaneously encode a top–down and a bottom–up views of the AMR graphs, whereas Guo et al. (2019) leverage dense connectivity in GNNs. Recently, Wang et al. (2020) propose a local graph encoder based on Transformers using separated attentions for incoming and outgoing neighbors. Recent methods (Zhu et al., 2019; Cai and Lam, 2020) also use Transformers, but learn globalized node representations, modeling graph paths in order to capture structural relations. KG-to-Text Generation. In this work, we focus on generating text from KGs. In comparison to AMRs, which are rooted and connected graphs, KGs do not have a defined topology, which may vary widely among different datasets, making the generation process more demanding. KGs are sparse structures that pot"
2020.tacl-1.38,P18-1150,1,0.930784,"dings. In our experiments, we demonstrate that our approaches lead to significant improvements on two graph-totext datasets achieving BLEU scores of 18.01 on the AGENDA dataset, and 63.69 on the WebNLG dataset for seen categories, outperforming state-of-the-art models by 3.7 and 3.1 points, respectively.1 1 Introduction Graph-to-text generation refers to the task of generating natural language text from input graph structures, which can be semantic representations (Konstas et al., 2017) or knowledge graphs (KGs) (Gardent et al., 2017; Koncel-Kedziorski et al., 2019). Whereas most recent work (Song et al., 2018; Ribeiro et al., 2019; Guo et al., 2019) focuses on generating sentences, a more challenging and interesting scenario emerges when the goal is to generate multisentence texts. In this context, in addition to sentence generation, document planning needs to be handled: The input needs to be mapped into several sentences; sentences need to 1 Code is available at https://github.com/UKPLab/ kg2text. 589 Transactions of the Association for Computational Linguistics, vol. 8, pp. 589–604, 2020. https://doi.org/10.1162/tacl a 00332 Action Editor: Alessandro Moschitti. Submission batch: 2/2019; Revisio"
2020.tacl-1.38,P18-1151,0,0.287663,"ntially contain a large number of relations. Moreover, we are typically interested in generating multisentence texts from KGs, and this involves solving document planning issues (Konstas and Lapata, 2013). Recent neural approaches for KG-to-text generation simply linearize the KG triples, thereby loosing graph structure information. For instance, Colin and Gardent (2018), Moryossef et al. (2019), and Adapt (Gardent et al., 2017) utilize LSTM/ GRU to encode WebNLG graphs. Castro Ferreira et al. (2019) systematically compare pipeline and end-to-end models for text generation from WebNLG graphs. Trisedya et al. (2018) develop a graph encoder based on LSTMs that captures relationships within and between triples. Previous work has also studied how to explicitly encode the graph structure using GNNs or Transformers. Marcheggiani and Perez Beltrachini (2018) propose an encoder based on graph convolutional networks, that consider explicitly local node contexts, and show superior performance compared with LSTMs. Recently, Koncel-Kedziorski et al. (2019) proposed a Transformer-based approach that computes the node representations by attending over node neighborhoods following a self-attention 591 strategy. In con"
2020.tacl-1.38,P18-1030,1,0.83059,"etter node representations in graph-to-text generation. To this end, existing methods use an artificial global node for message exchange with the other nodes. This strategy can be regarded as extending the graph structure but using similar message passing mechanisms. In particular, Koncel-Kedziorski et al. (2019) add a global node to the graph and use its representation to initialize the decoder. Recently, Guo et al. (2019) and Cai and Lam (2020) also utilized an artificial global node with direct edges to all other nodes to allow global message exchange for AMR-to-text generation. Similarly, Zhang et al. (2018) use a global node to a graph recurrent network model for sentence representation. Different from the above methods, we consider integrating global and local contexts at the node level, rather than the graph level, by investigating model alternatives rather than graph structure changes. In addition, we integrate GAT and Transformer architectures into a unified global-local model. 3 Graph-to-Text Model This section first describes (i) the graph transformation adopted to create a relational graph from the input (Section 3.1), and (ii) the graph encoders of our framework based on GAT (Veliˇckovi´"
2020.tacl-1.38,N19-1236,0,\N,Missing
2020.tacl-1.38,Q19-1019,0,\N,Missing
2020.tacl-1.38,D19-1548,0,\N,Missing
2020.tacl-1.38,2020.acl-main.640,0,\N,Missing
2021.acl-demo.41,D18-2029,0,0.0379272,"Missing"
2021.acl-demo.41,N19-1423,0,0.207492,"re rated on a 1-5 scale (5 denotes the best). in a general way, TextFlint supports generating massive and comprehensive transformed samples with just one command. By default, TextFlint performs all single transformations on the original dataset to form the corresponding transformed datasets, and the performance of the target models is tested on these datasets. The evaluation report provides a comparative view of model performance on datasets before and after certain types of transformations, which supports model weakness analyses and guides particular improvements. For example, take BERT base(Devlin et al., 2019) as the target model to verify its robustness on the CONLL2003 dataset(Tjong Kim Sang and De Meulder, 2003), whose robustness report is shown in Figure 5. The performance of BERT base decreases significantly in some morphology transformations, such as OCR, Keyboard, Typos, and Spelling Error. To combat these errors of input texts and improve the robustness of the model, we suggest that placing a word correction model(Pruthi et al., 2019) before BERT would be beneficial. 0.8 Case 2: Customized Evaluation For users who want to test model performance on specific aspects, they demand a customized"
2021.acl-demo.41,D18-1380,0,0.107351,"Missing"
2021.acl-demo.41,2021.naacl-demos.6,0,0.061782,"Missing"
2021.acl-demo.41,D14-1181,0,0.00437317,"Missing"
2021.acl-demo.41,2020.emnlp-main.500,1,0.864395,"like to teach kids in the kindergarten. The storm destroyed many houses in the village. ✘ Figure 1: Examples of three main generation functions. The transformation example is from ABSA (Aspectbased Sentiment Analysis) task, where the italic bold RevTgt (short for reverse target) denotes task-specific transformations, and the bold Typos denotes universal transformation. Introduction The detection of model robustness has been attracting increasing attention in recent years, given that deep neural networks (DNNs) of high accuracy can still be vulnerable to carefully crafted adversarial examples (Li et al., 2020), distribution shift (Miller et al., 2020), data transformation (Xing et al., 2020), and shortcut learning (Geirhos et al., 2020). Existing approaches to textual robustness evaluation focus on slightly modifying the input data, which maintains the original meaning and results in a different prediction. However, these methods often concentrate on either universal or task-specific generalization capabilities, which is difficult to comprehensively evaluate. In response to the shortcomings of recent works, we introduce TextFlint, a unified, multilingual, and analyzable robustness evaluation toolki"
2021.acl-demo.41,P18-1087,0,0.0272118,"Missing"
2021.acl-demo.41,P02-1040,0,0.116092,"are implemented based on TextAttack (Morris et al., 2020). Validator It is crucial to verify the quality of the samples generated by Transformation and AttackRecipe. TextFlint provides several metrics to evaluate the quality of the generated text, including (1) language model perplexity calculated based on the GPT2 model (Radford et al., 2019), (2) word replacement ratio in generated text compared with its original text, (3) edit distance between original text and generated text, (4) semantic 349 similarity calculated based on Universal Sentence Encoder (Cer et al., 2018), and (5) BLEU score (Papineni et al., 2002). 2.3 Reporter Layer Generation Layer yields three types of adversarial samples and verifies the robustness of the target model. Based on the evaluation results from Generation Layer, Report Layer aims to provide users with a standard analysis report from syntax, morphology, pragmatics, and paradigmatic relation aspects. The running process of Report Layer can be regarded as a pipeline from Analyzer to ReportGenerator. 3 Figure 4: Screenshot of TextFlint’s web interface running Ocr transformation for ABSA task. Usage Using TextFlint to verify the robustness of a specific model is as simple as"
2021.acl-demo.41,P19-1561,0,0.0287018,"Missing"
2021.acl-demo.41,2020.acl-main.442,0,0.114738,"methods equipped with pre-training LMs showed better performance than other models on the task-specific transformations, e.g., AddDiff , where the accuracy score of BERT-Aspect dropped from 90.32 to merely 81.58. All the evaluation results and comprehensive robustness analysis are available at textflint.io. 5 Related Work Our work is related to many existing open-source tools and works in different areas. Robustness Evaluation Many tools include evaluation methods for robustness, including NLPAug (Ma, 2019), Errudite (Wu et al., 2019), AllenNLP Interpret (Wallace et al., 2019), and Checklist (Ribeiro et al., 2020), which are only applicable to limited parts of robustness evaluations, while TextFlint supports comprehensive evaluation methods, e.g., subpopulation, adversarial attacks, transformations, and so on. Besides the common transformation methods like synonym substitution and typos, various task-specific transformations are tailored for each of the 12 NLP tasks, which is peculiar to TextFlint. Moreover, we are the first to provide linguistic support for the transformations, the designs for which were inspired by linguistics and have been proved plausible and readable by human annotators. Several t"
2021.acl-demo.41,C16-1311,0,0.0414402,"Missing"
2021.acl-demo.41,D16-1021,0,0.0913955,"Missing"
2021.acl-demo.41,D19-3002,0,0.0392234,"Missing"
2021.acl-demo.41,D16-1058,0,0.0373143,"Missing"
2021.acl-demo.41,P19-1073,0,0.133542,"underlying patterns about model robustness. As for the ABSA task (Table 2), methods equipped with pre-training LMs showed better performance than other models on the task-specific transformations, e.g., AddDiff , where the accuracy score of BERT-Aspect dropped from 90.32 to merely 81.58. All the evaluation results and comprehensive robustness analysis are available at textflint.io. 5 Related Work Our work is related to many existing open-source tools and works in different areas. Robustness Evaluation Many tools include evaluation methods for robustness, including NLPAug (Ma, 2019), Errudite (Wu et al., 2019), AllenNLP Interpret (Wallace et al., 2019), and Checklist (Ribeiro et al., 2020), which are only applicable to limited parts of robustness evaluations, while TextFlint supports comprehensive evaluation methods, e.g., subpopulation, adversarial attacks, transformations, and so on. Besides the common transformation methods like synonym substitution and typos, various task-specific transformations are tailored for each of the 12 NLP tasks, which is peculiar to TextFlint. Moreover, we are the first to provide linguistic support for the transformations, the designs for which were inspired by lingu"
2021.acl-demo.41,2020.emnlp-main.292,1,0.893206,"Missing"
2021.acl-long.251,D13-1160,0,0.194015,"s are used in this experiments. WB, TQ and NQ means WebQuestions, TriviaQA and NaturalQuestions, respectively. 2 Using SQuAD for Closed-book QA In the closed-book QA task (Roberts et al., 2020), a model needs to answer questions without external resources. Formally, the input is a question q, and the output is a sequence of tokens o. For evaluation, the correct golden answer g will be compared with o. Previous work (Roberts et al., 2020) uses the Exact Match (EM) metric to score o against g. We conduct closed-book QA by using the BART model (Lewis et al., 2020a) on four datasets-WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017), NaturalQuestions (Kwiatkowski et al., 2019) and SQuAD2 (Rajpurkar et al., 2018). BART is a transformer-based (Vaswani et al., 2017) sequence-to-sequence generative PLM, which we choose because it has achieved several state-of-the-art results on generative tasks. We use the publicly released checkpoint BART-Large in this work.2 To use a generative PLM on each dataset, the model is first finetuned using the training question-answer pairs. We call this process as QAfinetuning. While the other three datasets are used by following previous work (Roberts et al., 2020"
2021.acl-long.251,P19-1470,0,0.0177215,". (2020) set eight types of Cloze-style QA, such as ‘ALWAYS-NEVER’ and ‘AGE COMPARISON’, to test different types of knowledge in several discriminative PLMs, including BERT and RoBERTa (Liu et al., 2019). They also use the mask language modeling task to do QA without finetuning, and results show that the evaluated PLMs indeed contain those kinds of knowledge. Wang et al. (2019); Zhou et al. (2020) adopt some discriminative PLMs on commonsense reasoning QA tasks such as ComVE (Wang et al., 2020) and Swag (Zellers et al., 2018) without finetuning, indicating the PLMs have commonsense knowledge. Bosselut et al. (2019) show that pretrained transformer models can be used to help construct commonsense knowledge graphs, such as ConceptNet (Speer and Havasi, 2012). However, Poerner et al. (2019) argue that BERT uses some superficial cues such as stereotypical characters to solve factual questions. GPT-3 (Brown et al., 2020) seems to have ability to answer factual questions in zeroshot setting, but there exists some evidence that GPT-3 is limited in storing and using knowledge (Bergdahl, 2020). Roberts et al. (2020) firstly use closed-book QA to detect how much knowledge is in pre-trained language models’ parame"
2021.acl-long.251,P17-1171,0,0.0213733,"usions of Lewis et al. (2020b), and we further experiment with a more controlled SQuAD dataset, and discussed the weakness of BART in both memorization and knowledge retrieval. Because T5 (Raffel et al., 2020) is more resource demanding, considering the balance of effectiveness and experimental feasibility, we choose BART rather than the T5 model. Different from closed-book QA, where no additional resource is available when answering questions, open-domain QA requires models to generate a sequence of tokens as the answer to each question by looking up related text from unstructured documents (Chen et al., 2017). Chen et al. (2017) first try to retrieve related passages from Wikipedia for each question and encode both the question and passages into the model, then output the answer. Guu et al. (2020) integrate the retrieval process into pre-training process, helping the PLMs better retrieve information from external knowledge source when needed, and finding benefits on opendomain QA task. Retriever-based models have the advantage of relieving the burden of pre-trained language models to remember every factual detail. The retrieval QA setting is slightly reminiscent to our data augmentation setting in"
2021.acl-long.251,N19-1423,0,0.182141,"ll benchmarks with high test-train overlaps. We construct a new dataset of closed-book QA using SQuAD, and investigate the performance of BART. Experiments show that it is challenging for BART to remember training facts in high precision, and also challenging to answer closed-book questions even if relevant knowledge is retained. Some promising directions are found, including decoupling the knowledge memorizing process and the QA finetune process, forcing the model to recall relevant knowledge when question answering. 1 Introduction Large-scare pre-trained language models (PLMs) such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018) have significantly improved the performance of NLP tasks (Radford et al., 2019). There is increasing evidence showing that PLMs contain world knowledge (Petroni et al., 2019; Zhou et al., 2020; Talmor et al., 2020). As a result, recent research considers generative PLMs such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2020a) for Closed-book QA, which has only question-answer pairs without external knowledge source. For example, after being finetuned on a few QA pairs, a generative LM can directly output “Florence” after being given the question “Where was D"
2021.acl-long.251,P17-1147,0,0.100827,"WB, TQ and NQ means WebQuestions, TriviaQA and NaturalQuestions, respectively. 2 Using SQuAD for Closed-book QA In the closed-book QA task (Roberts et al., 2020), a model needs to answer questions without external resources. Formally, the input is a question q, and the output is a sequence of tokens o. For evaluation, the correct golden answer g will be compared with o. Previous work (Roberts et al., 2020) uses the Exact Match (EM) metric to score o against g. We conduct closed-book QA by using the BART model (Lewis et al., 2020a) on four datasets-WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017), NaturalQuestions (Kwiatkowski et al., 2019) and SQuAD2 (Rajpurkar et al., 2018). BART is a transformer-based (Vaswani et al., 2017) sequence-to-sequence generative PLM, which we choose because it has achieved several state-of-the-art results on generative tasks. We use the publicly released checkpoint BART-Large in this work.2 To use a generative PLM on each dataset, the model is first finetuned using the training question-answer pairs. We call this process as QAfinetuning. While the other three datasets are used by following previous work (Roberts et al., 2020), we make a novel adaptation o"
2021.acl-long.251,Q19-1026,0,0.0698227,"aQA and NaturalQuestions, respectively. 2 Using SQuAD for Closed-book QA In the closed-book QA task (Roberts et al., 2020), a model needs to answer questions without external resources. Formally, the input is a question q, and the output is a sequence of tokens o. For evaluation, the correct golden answer g will be compared with o. Previous work (Roberts et al., 2020) uses the Exact Match (EM) metric to score o against g. We conduct closed-book QA by using the BART model (Lewis et al., 2020a) on four datasets-WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017), NaturalQuestions (Kwiatkowski et al., 2019) and SQuAD2 (Rajpurkar et al., 2018). BART is a transformer-based (Vaswani et al., 2017) sequence-to-sequence generative PLM, which we choose because it has achieved several state-of-the-art results on generative tasks. We use the publicly released checkpoint BART-Large in this work.2 To use a generative PLM on each dataset, the model is first finetuned using the training question-answer pairs. We call this process as QAfinetuning. While the other three datasets are used by following previous work (Roberts et al., 2020), we make a novel adaptation of the SQuAD dataset for closed-book QA. SQuAD"
2021.acl-long.251,2020.acl-main.703,0,0.147919,"ound, including decoupling the knowledge memorizing process and the QA finetune process, forcing the model to recall relevant knowledge when question answering. 1 Introduction Large-scare pre-trained language models (PLMs) such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018) have significantly improved the performance of NLP tasks (Radford et al., 2019). There is increasing evidence showing that PLMs contain world knowledge (Petroni et al., 2019; Zhou et al., 2020; Talmor et al., 2020). As a result, recent research considers generative PLMs such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2020a) for Closed-book QA, which has only question-answer pairs without external knowledge source. For example, after being finetuned on a few QA pairs, a generative LM can directly output “Florence” after being given the question “Where was Dante born?”. Roberts et al. (2020) find that generative PLMs can store and use knowledge as they can achieve relatively high performance in closed-book QA task on three datasets. However, Lewis et al. (2020b) find that the excellent results are mainly ∗ † Equal contribution The corresponding author Figure 1: Process of generative PLMs for closed-book QA. (1)"
2021.acl-long.251,2021.ccl-1.108,0,0.0674469,"Missing"
2021.acl-long.251,N18-1202,0,0.0147319,"mely large documents from the internet. We also apply GPT-2 to LM-finetuning and QAfinetuning, which has similar architecture, pretraining and finetune process with GPT-3. Thus we believe that they can have the same fundamental problem. The results are shown in Table 12. LMfinetuned GPT-2 has worse performance compared to LM-finetuned BART. This confirms that the architecture and the training process of GPT3/GPT-2 do not solve the problems we find using BART. 6 Related Work There are two types of pre-trained language models (PLMs), discriminative PLMs such as BERT (Devlin et al., 2019), ELMo (Peters et al., 2018) and generative PLMs such as GPT (Radford et al., 3248 2018), BART (Lewis et al., 2020a). The key difference is that generative PLMs are of encoderdecoder architectures so they can generate text sequences of any length or token. An increasing number of works have shown that PLMs contains world knowledge. Petroni et al. (2019) first solves that discriminative PLMs such as BERT (Devlin et al., 2019) can be used for Cloze-style QA using a mask language modeling task without external resources, such as “Dante was born in [MASK].” → “Florence”. Their results show that PLMs have certain factual know"
2021.acl-long.251,D19-1250,0,0.0792844,"Missing"
2021.acl-long.251,P18-2124,0,0.0234707,". 2 Using SQuAD for Closed-book QA In the closed-book QA task (Roberts et al., 2020), a model needs to answer questions without external resources. Formally, the input is a question q, and the output is a sequence of tokens o. For evaluation, the correct golden answer g will be compared with o. Previous work (Roberts et al., 2020) uses the Exact Match (EM) metric to score o against g. We conduct closed-book QA by using the BART model (Lewis et al., 2020a) on four datasets-WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017), NaturalQuestions (Kwiatkowski et al., 2019) and SQuAD2 (Rajpurkar et al., 2018). BART is a transformer-based (Vaswani et al., 2017) sequence-to-sequence generative PLM, which we choose because it has achieved several state-of-the-art results on generative tasks. We use the publicly released checkpoint BART-Large in this work.2 To use a generative PLM on each dataset, the model is first finetuned using the training question-answer pairs. We call this process as QAfinetuning. While the other three datasets are used by following previous work (Roberts et al., 2020), we make a novel adaptation of the SQuAD dataset for closed-book QA. SQuAD (Rajpurkar et al., 2018) is a wildl"
2021.acl-long.251,D16-1264,0,0.0500108,"Guu et al. (2020) integrate the retrieval process into pre-training process, helping the PLMs better retrieve information from external knowledge source when needed, and finding benefits on opendomain QA task. Retriever-based models have the advantage of relieving the burden of pre-trained language models to remember every factual detail. The retrieval QA setting is slightly reminiscent to our data augmentation setting in Figure 4, but with the related passage being the input, rather than the output. In contrast, the settings we consider fully rely on a neural model for all knowledge. SQuAD (Rajpurkar et al., 2016, 2018) is a widely-used dataset for machine reading comprehension, which is also a type of QA task. It asks models to use a text span from a given referential passage to answer questions. It is also used in other type of QA task, for example, Chen et al. (2017) adopt it in the open-domain QA task. We first apply it on closed-book QA and analyze why it is superior than other three commonly used datasets. 7 Conclusion We investigated by using SQuAD, finding that closed-book QA is still challenging for generative pre-trained language models such as BART. The challenge lies both in remembering th"
2021.acl-long.251,2020.emnlp-main.437,0,0.229552,"Missing"
2021.acl-long.251,speer-havasi-2012-representing,0,0.0178849,"minative PLMs, including BERT and RoBERTa (Liu et al., 2019). They also use the mask language modeling task to do QA without finetuning, and results show that the evaluated PLMs indeed contain those kinds of knowledge. Wang et al. (2019); Zhou et al. (2020) adopt some discriminative PLMs on commonsense reasoning QA tasks such as ComVE (Wang et al., 2020) and Swag (Zellers et al., 2018) without finetuning, indicating the PLMs have commonsense knowledge. Bosselut et al. (2019) show that pretrained transformer models can be used to help construct commonsense knowledge graphs, such as ConceptNet (Speer and Havasi, 2012). However, Poerner et al. (2019) argue that BERT uses some superficial cues such as stereotypical characters to solve factual questions. GPT-3 (Brown et al., 2020) seems to have ability to answer factual questions in zeroshot setting, but there exists some evidence that GPT-3 is limited in storing and using knowledge (Bergdahl, 2020). Roberts et al. (2020) firstly use closed-book QA to detect how much knowledge is in pre-trained language models’ parameters. They perform experiments on three datasets WebQuestions (Berant et al., 2013), TriviaQA (Joshi et al., 2017) and NaturalQuestions (Kwiatko"
2021.acl-long.251,2020.tacl-1.48,0,0.639981,"also challenging to answer closed-book questions even if relevant knowledge is retained. Some promising directions are found, including decoupling the knowledge memorizing process and the QA finetune process, forcing the model to recall relevant knowledge when question answering. 1 Introduction Large-scare pre-trained language models (PLMs) such as BERT (Devlin et al., 2019), GPT (Radford et al., 2018) have significantly improved the performance of NLP tasks (Radford et al., 2019). There is increasing evidence showing that PLMs contain world knowledge (Petroni et al., 2019; Zhou et al., 2020; Talmor et al., 2020). As a result, recent research considers generative PLMs such as T5 (Raffel et al., 2020) and BART (Lewis et al., 2020a) for Closed-book QA, which has only question-answer pairs without external knowledge source. For example, after being finetuned on a few QA pairs, a generative LM can directly output “Florence” after being given the question “Where was Dante born?”. Roberts et al. (2020) find that generative PLMs can store and use knowledge as they can achieve relatively high performance in closed-book QA task on three datasets. However, Lewis et al. (2020b) find that the excellent results ar"
2021.acl-long.251,2020.semeval-1.39,1,0.754386,"as “Dante was born in [MASK].” → “Florence”. Their results show that PLMs have certain factual knowledge. Talmor et al. (2020) set eight types of Cloze-style QA, such as ‘ALWAYS-NEVER’ and ‘AGE COMPARISON’, to test different types of knowledge in several discriminative PLMs, including BERT and RoBERTa (Liu et al., 2019). They also use the mask language modeling task to do QA without finetuning, and results show that the evaluated PLMs indeed contain those kinds of knowledge. Wang et al. (2019); Zhou et al. (2020) adopt some discriminative PLMs on commonsense reasoning QA tasks such as ComVE (Wang et al., 2020) and Swag (Zellers et al., 2018) without finetuning, indicating the PLMs have commonsense knowledge. Bosselut et al. (2019) show that pretrained transformer models can be used to help construct commonsense knowledge graphs, such as ConceptNet (Speer and Havasi, 2012). However, Poerner et al. (2019) argue that BERT uses some superficial cues such as stereotypical characters to solve factual questions. GPT-3 (Brown et al., 2020) seems to have ability to answer factual questions in zeroshot setting, but there exists some evidence that GPT-3 is limited in storing and using knowledge (Bergdahl, 202"
2021.acl-long.251,P19-1393,1,0.851135,"evlin et al., 2019) can be used for Cloze-style QA using a mask language modeling task without external resources, such as “Dante was born in [MASK].” → “Florence”. Their results show that PLMs have certain factual knowledge. Talmor et al. (2020) set eight types of Cloze-style QA, such as ‘ALWAYS-NEVER’ and ‘AGE COMPARISON’, to test different types of knowledge in several discriminative PLMs, including BERT and RoBERTa (Liu et al., 2019). They also use the mask language modeling task to do QA without finetuning, and results show that the evaluated PLMs indeed contain those kinds of knowledge. Wang et al. (2019); Zhou et al. (2020) adopt some discriminative PLMs on commonsense reasoning QA tasks such as ComVE (Wang et al., 2020) and Swag (Zellers et al., 2018) without finetuning, indicating the PLMs have commonsense knowledge. Bosselut et al. (2019) show that pretrained transformer models can be used to help construct commonsense knowledge graphs, such as ConceptNet (Speer and Havasi, 2012). However, Poerner et al. (2019) argue that BERT uses some superficial cues such as stereotypical characters to solve factual questions. GPT-3 (Brown et al., 2020) seems to have ability to answer factual questions"
2021.acl-long.251,D18-1009,0,0.0253791,".” → “Florence”. Their results show that PLMs have certain factual knowledge. Talmor et al. (2020) set eight types of Cloze-style QA, such as ‘ALWAYS-NEVER’ and ‘AGE COMPARISON’, to test different types of knowledge in several discriminative PLMs, including BERT and RoBERTa (Liu et al., 2019). They also use the mask language modeling task to do QA without finetuning, and results show that the evaluated PLMs indeed contain those kinds of knowledge. Wang et al. (2019); Zhou et al. (2020) adopt some discriminative PLMs on commonsense reasoning QA tasks such as ComVE (Wang et al., 2020) and Swag (Zellers et al., 2018) without finetuning, indicating the PLMs have commonsense knowledge. Bosselut et al. (2019) show that pretrained transformer models can be used to help construct commonsense knowledge graphs, such as ConceptNet (Speer and Havasi, 2012). However, Poerner et al. (2019) argue that BERT uses some superficial cues such as stereotypical characters to solve factual questions. GPT-3 (Brown et al., 2020) seems to have ability to answer factual questions in zeroshot setting, but there exists some evidence that GPT-3 is limited in storing and using knowledge (Bergdahl, 2020). Roberts et al. (2020) firstl"
2021.acl-long.26,2020.findings-emnlp.117,0,0.0364422,"influential word for the prediction of a positive sentiment should be “superb” instead of “Nolan” or “film”. The issue of spurious patterns also partially affects the out-ofdomain (OOD) generalization of the models trained on independent, identical distribution (IID) data, leading to performance decay under distribution shift (Quionero-Candela et al., 2009; Sugiyama and Kawanabe, 2012; Ovadia et al., 2019). Researchers have recently found that such concerns about model performance decay and social bias in NLP come about out-of-domain because of a sensitivity to semantically spurious signals (Gardner et al., 2020), and recent studies have uncovered a problematic tendency for gender bias in sentiment analysis (Zmigrod et al., 2019; Maudslay et al., 2019; Lu et al., 2020). To this end, one of the possible solutions is data augmentation with counterfactual examples (Kaushik et al., 2020) to ensure that models learn real causal associations between the input text and labels. For example, a sentiment-flipped counterfactual of last example could be “Nolan’s movies always bore people, thanks to his poor directorial skills.”. When added to the original set of training data, such kinds of counterfactually augme"
2021.acl-long.26,2020.acl-main.463,0,0.0895054,"Missing"
2021.acl-long.26,2020.semeval-1.43,0,0.0344574,"in the field of CV (Goyal et al., 2019; Kenny and Keane, 2021), but investigated less in NLP. Recent work (Jacovi and Goldberg, 2020) highlight explanations of a given causal format, and Yang et al. (2020a) generate counterfactuals for explaining the prediction of financial text classification. We propose a similar but different research question, that is, whether the automatically generated counterfactual can be used for data augmentation to build more robust models, which has not been considered by the previous methods in XAI (Pedreschi et al., 2019; Slack et al., 2020b; Yang et al., 2020b; Ding et al., 2020). In the case of Sentiment Analysis, most of the previous works report experiments using a holdout test on the IID dataset (Liu, 2012; Yang et al., 2016; Johnson and Zhang, 2017). The current stateof-the-art methods make use of large pre-trained language models (e.g., BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) and SMART-RoBERTa (Jiang et al., 2020)) for calculating input represntations. It has been shown that these methods can suffer from spurious patterns (Kaushik et al., 2020; Wang and Culotta, 2021). Very recently, Wang and Culotta (2021) provide a starting point for exploring t"
2021.acl-long.26,2020.acl-main.197,0,0.0888116,"Missing"
2021.acl-long.26,P17-1052,0,0.011326,"al format, and Yang et al. (2020a) generate counterfactuals for explaining the prediction of financial text classification. We propose a similar but different research question, that is, whether the automatically generated counterfactual can be used for data augmentation to build more robust models, which has not been considered by the previous methods in XAI (Pedreschi et al., 2019; Slack et al., 2020b; Yang et al., 2020b; Ding et al., 2020). In the case of Sentiment Analysis, most of the previous works report experiments using a holdout test on the IID dataset (Liu, 2012; Yang et al., 2016; Johnson and Zhang, 2017). The current stateof-the-art methods make use of large pre-trained language models (e.g., BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) and SMART-RoBERTa (Jiang et al., 2020)) for calculating input represntations. It has been shown that these methods can suffer from spurious patterns (Kaushik et al., 2020; Wang and Culotta, 2021). Very recently, Wang and Culotta (2021) provide a starting point for exploring the efficacy of automatically generated CAD for sentiment analysis, but it is still based on IID hold-out tests only. However, spurious patterns in the training and test sets coul"
2021.acl-long.26,C04-1200,0,0.111303,"Missing"
2021.acl-long.26,2021.ccl-1.108,0,0.0642405,"Missing"
2021.acl-long.26,P11-1015,0,0.0661034,"related ideas. In conclusion, then, the final augmented dataset that is produced of three parts: (1) counterfactuals generated by RM-CT; (2) counterfactuals generated by REP-CT; (3) adversarial examples generated by synonym substitutions. 3.3 State-of-the-art Models SMART-RoBERTa (Jiang et al., 2020) RoBERTa-Large (Liu et al., 2019) RTC-attention (Zhang and Zhang, 2019) Bi-LSTM 4.1 4.2 Datasets Our evaluation uses three different kinds of datasets, in-domain data, challenge data, and outof-domain data. In-domain Data We first adopt two of the most popular benchmark datasets – SST-2 and IMDB (Maas et al., 2011) – to show the recent advances on sentiment analysis with the benefit of pre-trained models. However, we mainly focus on the robustness of various models for sentiment analysis in this work, rather than in-domain accuracy. Hence, following Wang and Culotta (2021) and Kaushik et al. (2020), we perform binary sentiment classification experiments on the IMDB dataset sampled from Maas et al. (2011) that contains 1707 training, 245 validation, and 488 testing examples with challenge dataset (paired counterfactuals). Challenge Data Based on the in-domain IMDB data, Kaushik et al. (2020) employ crowd"
2021.acl-long.26,2020.acl-main.169,0,0.0200124,"ally generated counterfactuals. C and AC contain the same size of training samples (3.4K). Original Samples Nolan’s film...superb directing skills (POS) It’s a poor film, but I must give it to the lead actress in this one (NEG) Original superb:0.213 film:0.446 Nolan:0.028 poor:-0.551 film:-0.257 actress:-0.02 Robust 0.627 0.019 0.029 -0.999 -7e-7 -1e-6 Table 3: Less sensitivity to spurious patterns has been shown in the robust BERT-base-uncased model. on an in-domain test. We further compare our method, human-label method, and two state-of-theart style-transfer methods (Sudhakar et al., 2019; Madaan et al., 2020) in terms of the model robustness on generalization test. Notably, we provide an ablation study lastly to discuss the influence of edit-distance for performance benefits. 5.1 State-of-the-art Models The SVM model for sentiment analysis is from scikit-learn and uses TF-IDF (Term FrequencyInverse Document Frequency) scores, while the Transformer-based models are built based on the Pytorch-Transformer package 4 . We keep the prediction models the same as Kaushik et al. (2020), except for Naive Bayes, which has been abandoned due to its high-variance performance shown in our experiments. In the fo"
2021.acl-long.26,D19-1530,0,0.0271626,"also partially affects the out-ofdomain (OOD) generalization of the models trained on independent, identical distribution (IID) data, leading to performance decay under distribution shift (Quionero-Candela et al., 2009; Sugiyama and Kawanabe, 2012; Ovadia et al., 2019). Researchers have recently found that such concerns about model performance decay and social bias in NLP come about out-of-domain because of a sensitivity to semantically spurious signals (Gardner et al., 2020), and recent studies have uncovered a problematic tendency for gender bias in sentiment analysis (Zmigrod et al., 2019; Maudslay et al., 2019; Lu et al., 2020). To this end, one of the possible solutions is data augmentation with counterfactual examples (Kaushik et al., 2020) to ensure that models learn real causal associations between the input text and labels. For example, a sentiment-flipped counterfactual of last example could be “Nolan’s movies always bore people, thanks to his poor directorial skills.”. When added to the original set of training data, such kinds of counterfactually augmented data (CAD) have 1 Introduction shown their benefits on learning real causal assoDeep neural models have recently made remark- ciations a"
2021.acl-long.26,D19-1018,0,0.0621528,"Missing"
2021.acl-long.26,N16-3020,0,0.0511416,"using a hold-out test methodology. For this reason, we designed an indirect method for evaluating the robustness of models, by comparing the performance of models trained on original and augmented data using out-of-domain data. The prediction benefit for out-of-domain data should provide some evidence about whether a model’s sensitivity to spurious patterns has been successfully mitigated. The resulting counterfactuals can be used for data augmentation and can also provide contrastive explanations for classifiers, and important and desirable consideration for the recent move towards more XAI (Ribeiro et al., 2016; Lundberg and Lee, 2017; Lipton, 2018; Pedreschi et al., 2019; Slack et al., 2020b). 3 Detailed Implementation We propose a new approach for automatically generating counterfactuals to enhance the robustness of sentiment analysis models by inverting the sentiment of causally important terms according to Algorithm 1 and based on the following stages: 308 1. The identification of genuine causal terms using self-supervised contextual decomposition (Section 3.1). 2. Generating counterfactual samples by (a) RMCT (removing causal terms) and (b) REP-CT (replacing the causal terms) (Section 3.2). 3."
2021.acl-long.26,2020.acl-main.442,0,0.0453598,"Missing"
2021.acl-long.26,S17-2088,0,0.052699,"Missing"
2021.acl-long.26,D19-1322,0,0.025104,"combined with automatically generated counterfactuals. C and AC contain the same size of training samples (3.4K). Original Samples Nolan’s film...superb directing skills (POS) It’s a poor film, but I must give it to the lead actress in this one (NEG) Original superb:0.213 film:0.446 Nolan:0.028 poor:-0.551 film:-0.257 actress:-0.02 Robust 0.627 0.019 0.029 -0.999 -7e-7 -1e-6 Table 3: Less sensitivity to spurious patterns has been shown in the robust BERT-base-uncased model. on an in-domain test. We further compare our method, human-label method, and two state-of-theart style-transfer methods (Sudhakar et al., 2019; Madaan et al., 2020) in terms of the model robustness on generalization test. Notably, we provide an ablation study lastly to discuss the influence of edit-distance for performance benefits. 5.1 State-of-the-art Models The SVM model for sentiment analysis is from scikit-learn and uses TF-IDF (Term FrequencyInverse Document Frequency) scores, while the Transformer-based models are built based on the Pytorch-Transformer package 4 . We keep the prediction models the same as Kaushik et al. (2020), except for Naive Bayes, which has been abandoned due to its high-variance performance shown in our"
2021.acl-long.26,2020.acl-main.540,0,0.0358166,"set of training data, such kinds of counterfactually augmented data (CAD) have 1 Introduction shown their benefits on learning real causal assoDeep neural models have recently made remark- ciations and improving the model robustness in able advances on sentiment analysis (Devlin et al., recent studies (Kaushik et al., 2020, 2021; Wang 2018; Liu et al., 2019; Yang et al., 2019; Xie et al., and Culotta, 2021). Unlike gradient-based adver2020). However, their implementation in practical sarial examples (Wang and Wan, 2019; Zhang et al., applications still encounters significant challenges. 2019; Zang et al., 2020), which cannot provide a Of particular concern, these models tend to learn in- clear boundary between positive and negative intended behavior that is often associated with spuri- stances to humans, counterfactuals could provide ous patterns (artifacts) (Jo and Bengio, 2017; Slack “human-like” logic to show a modification to the 306 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 306–316 August 1–6, 2021. ©2021 Association for Computational Linguistics input that makes a dif"
2021.acl-long.26,P19-1559,0,0.0531937,"Missing"
2021.acl-long.26,P19-1342,1,0.805254,"Missing"
2021.acl-long.26,P18-1030,1,0.784275,"Missing"
2021.acl-long.26,2020.tacl-1.25,0,0.0906273,"Missing"
2021.acl-long.26,2020.coling-main.541,1,0.75584,"itivity of contextual decomposition. Sentiment Dictionary refers to the opinion lexicon published by (Hu and Liu, 2004). also shares important conceptual features with our work. Since human counterfactual explanations are minimal in the sense that they select a few relevant causes (Byrne, 2019; Keane and Smyth, 2020) as is the requirement of minimal edits in our generation process. This has been explored more in the field of CV (Goyal et al., 2019; Kenny and Keane, 2021), but investigated less in NLP. Recent work (Jacovi and Goldberg, 2020) highlight explanations of a given causal format, and Yang et al. (2020a) generate counterfactuals for explaining the prediction of financial text classification. We propose a similar but different research question, that is, whether the automatically generated counterfactual can be used for data augmentation to build more robust models, which has not been considered by the previous methods in XAI (Pedreschi et al., 2019; Slack et al., 2020b; Yang et al., 2020b; Ding et al., 2020). In the case of Sentiment Analysis, most of the previous works report experiments using a holdout test on the IID dataset (Liu, 2012; Yang et al., 2016; Johnson and Zhang, 2017). The cu"
2021.acl-long.26,2020.semeval-1.40,0,0.0590774,"Missing"
2021.acl-long.26,N16-1174,0,0.0134868,"ons of a given causal format, and Yang et al. (2020a) generate counterfactuals for explaining the prediction of financial text classification. We propose a similar but different research question, that is, whether the automatically generated counterfactual can be used for data augmentation to build more robust models, which has not been considered by the previous methods in XAI (Pedreschi et al., 2019; Slack et al., 2020b; Yang et al., 2020b; Ding et al., 2020). In the case of Sentiment Analysis, most of the previous works report experiments using a holdout test on the IID dataset (Liu, 2012; Yang et al., 2016; Johnson and Zhang, 2017). The current stateof-the-art methods make use of large pre-trained language models (e.g., BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) and SMART-RoBERTa (Jiang et al., 2020)) for calculating input represntations. It has been shown that these methods can suffer from spurious patterns (Kaushik et al., 2020; Wang and Culotta, 2021). Very recently, Wang and Culotta (2021) provide a starting point for exploring the efficacy of automatically generated CAD for sentiment analysis, but it is still based on IID hold-out tests only. However, spurious patterns in the t"
2021.acl-long.26,D19-1053,0,0.128763,"19; Slack et al., 2020b). 3 Detailed Implementation We propose a new approach for automatically generating counterfactuals to enhance the robustness of sentiment analysis models by inverting the sentiment of causally important terms according to Algorithm 1 and based on the following stages: 308 1. The identification of genuine causal terms using self-supervised contextual decomposition (Section 3.1). 2. Generating counterfactual samples by (a) RMCT (removing causal terms) and (b) REP-CT (replacing the causal terms) (Section 3.2). 3. Selecting the human-like counterfactuals using MoverScore. (Zhao et al., 2019) (Section 3.3). The end result will be a set of counterfactuals that can be used to augment an existing dataset. 3.1 Identifying Causal Terms To identify causally important terms, we propose a hierarchical method, based on the sampling and sensitivity of contextual decomposition technique from Jin et al. (2019), by incrementally removing words from a sentence in order to evaluate the model’s sensitivity to these words. Significant changes in model outputs suggest the removal of important terms. For example, removing the word “best” from “The movie is the best that I have ever seen.”, is likely"
2021.acl-long.26,P19-1161,0,0.15419,"of spurious patterns also partially affects the out-ofdomain (OOD) generalization of the models trained on independent, identical distribution (IID) data, leading to performance decay under distribution shift (Quionero-Candela et al., 2009; Sugiyama and Kawanabe, 2012; Ovadia et al., 2019). Researchers have recently found that such concerns about model performance decay and social bias in NLP come about out-of-domain because of a sensitivity to semantically spurious signals (Gardner et al., 2020), and recent studies have uncovered a problematic tendency for gender bias in sentiment analysis (Zmigrod et al., 2019; Maudslay et al., 2019; Lu et al., 2020). To this end, one of the possible solutions is data augmentation with counterfactual examples (Kaushik et al., 2020) to ensure that models learn real causal associations between the input text and labels. For example, a sentiment-flipped counterfactual of last example could be “Nolan’s movies always bore people, thanks to his poor directorial skills.”. When added to the original set of training data, such kinds of counterfactually augmented data (CAD) have 1 Introduction shown their benefits on learning real causal assoDeep neural models have recently"
2021.acl-long.267,K16-1002,0,0.0407819,"Missing"
2021.acl-long.267,J93-2003,0,0.0782466,"Missing"
2021.acl-long.267,P05-1033,0,0.147096,"separately. As shown in Table 6, when we replace the combined attention on top 2 layers with group attention, the performance drops by 0.22, 0.09, and 0.75 d-BLEU on TED, News, and Europarl, respectively. When we replace the combined attention with global attention, the performance decrease is enlarged to 0.84, 0.69, and 1.00 d-BLEU, respectively. These results demonstrate the necessity of combined attention for integrating local and global context information. 6 Related Work The unit of translation has evolved from word (Brown et al., 1993; Vogel et al., 1996) to phrase (Koehn et al., 2003; Chiang, 2005, 2007) and further to sentence (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) in the MT literature. The trend shows that larger units of translation, when represented properly, can lead to improved translation quality. A line of document-level MT extends translation unit to multiple sentences (Tiedemann and Scherrer, 2017; Agrawal et al., 2018; Zhang et al., 2020; Ma et al., 2020). However, these approaches are limited within a short context of maximum four sentences. Recent studies extend the translation unit to whole document (Junczys-Dowmunt, 2019; Liu et a"
2021.acl-long.267,J07-2003,0,0.501204,"Missing"
2021.acl-long.267,P05-1066,0,0.291163,"Missing"
2021.acl-long.267,W15-4908,0,0.0442761,"Missing"
2021.acl-long.267,D11-1084,0,0.0365643,"w that G-Transformer converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets. 1 Context Source Target Decoder Translation1 Translation2 Source Encoder Source1 … Target Decoder Source2 … (b) Multi-sentence Translation Translation1 Translation2 …      … Source1 Source2  … … (c) G-Transformer (Doc-by-doc Translation) Figure 1: Overview of model structures for documentlevel machine translation. Document-level machine translation (MT) has received increasing research attention (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020). It is a more practically useful task compared to sentence-level MT because typical inputs in MT applications are text documents rather than individual sentences. A salient difference between document-level MT and sentence-level MT is that for the former, much larger inter-sentential context should be considered when translating each sentence, which include discourse structures such as anaphora, lexical cohesion, etc. Studies show that human translators consider such contexts when con"
2021.acl-long.267,P14-6007,0,0.188194,"ia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020). It is a more practically useful task compared to sentence-level MT because typical inputs in MT applications are text documents rather than individual sentences. A salient difference between document-level MT and sentence-level MT is that for the former, much larger inter-sentential context should be considered when translating each sentence, which include discourse structures such as anaphora, lexical cohesion, etc. Studies show that human translators consider such contexts when conducting document translation (Hardmeier, 2014; L¨aubli et al., 2018). Despite that neural models achieve competitive performances on sentence* Corresponding author. Source Encoder (a) Sentence-by-sentence Translation Introduction ∗ Context Encoder level MT, the performance of document-level MT is still far from satisfactory. Existing methods can be mainly classified into two categories. The first category translates a document sentence by sentence using a sequence-tosequence neural model (Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Zheng et al., 2020). Document-level context is integrated into sentence-translation b"
2021.acl-long.267,P13-4033,0,0.0299596,"r converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets. 1 Context Source Target Decoder Translation1 Translation2 Source Encoder Source1 … Target Decoder Source2 … (b) Multi-sentence Translation Translation1 Translation2 …      … Source1 Source2  … … (c) G-Transformer (Doc-by-doc Translation) Figure 1: Overview of model structures for documentlevel machine translation. Document-level machine translation (MT) has received increasing research attention (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020). It is a more practically useful task compared to sentence-level MT because typical inputs in MT applications are text documents rather than individual sentences. A salient difference between document-level MT and sentence-level MT is that for the former, much larger inter-sentential context should be considered when translating each sentence, which include discourse structures such as anaphora, lexical cohesion, etc. Studies show that human translators consider such contexts when conducting document transla"
2021.acl-long.267,P19-1356,0,0.139735,"e attention range may also result in unstable training and slow down the training process. 3.3 Conclusion The above experiments show that training failure on Transformer can be caused by local minima. Additionally, the oscillation of attention range may make it worse. During training process, the attention module needs to identify relevant tokens from whole sequence to attend to. Assuming that the sequence length is N , the complexity of the attention distribution increases when N grows from sentence-level to document-level. We propose to use locality properties (Rizzi, 2013; Hardmeier, 2014; Jawahar et al., 2019) of both the language itself and the translation task as a constraint in Transformer, regulating the hypothesis space of the self-attention and target-to-source attention, using a simple group tag method. 4 G-Transformer An example of G-Transformer is shown in Figure 6, where the input document contains more than 3 sentences. As can be seen from the figure, G-Transformer extends Transformer by augmenting the input and output with group tags (Bao and Zhang, 2021). In particular, each token is assigned a group tag, indicating its sentential index. While 3445 source group tags can be assigned det"
2021.acl-long.267,W19-5321,0,0.114867,"complexity. Second, more importantly, information exchange cannot be made between the current sentence and its document context in the same encoding module. The second category extends the translation unit from a single sentence to multiple sentences (Tiedemann and Scherrer, 2017; Agrawal et al., 3442 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3442–3455 August 1–6, 2021. ©2021 Association for Computational Linguistics 2018; Zhang et al., 2020) and the whole document (Junczys-Dowmunt, 2019; Liu et al., 2020). Recently, it has been shown that when the translation unit increases from one sentence to four sentences, the performance improves (Zhang et al., 2020; Scherrer et al., 2019). However, when the whole document is encoded as a single unit for sequence to sequence translation, direct supervised training has been shown to fail (Liu et al., 2020). As a solution, either large-scale pre-training (Liu et al., 2020) or data augmentation (Junczys-Dowmunt, 2019) has been used as a solution, leading to improved performance. These methods are shown in Figure 1(b). One limitation of suc"
2021.acl-long.267,D13-1176,0,0.0689615,"lace the combined attention on top 2 layers with group attention, the performance drops by 0.22, 0.09, and 0.75 d-BLEU on TED, News, and Europarl, respectively. When we replace the combined attention with global attention, the performance decrease is enlarged to 0.84, 0.69, and 1.00 d-BLEU, respectively. These results demonstrate the necessity of combined attention for integrating local and global context information. 6 Related Work The unit of translation has evolved from word (Brown et al., 1993; Vogel et al., 1996) to phrase (Koehn et al., 2003; Chiang, 2005, 2007) and further to sentence (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) in the MT literature. The trend shows that larger units of translation, when represented properly, can lead to improved translation quality. A line of document-level MT extends translation unit to multiple sentences (Tiedemann and Scherrer, 2017; Agrawal et al., 2018; Zhang et al., 2020; Ma et al., 2020). However, these approaches are limited within a short context of maximum four sentences. Recent studies extend the translation unit to whole document (Junczys-Dowmunt, 2019; Liu et al., 2020), using large augmented dataset or pretrained models."
2021.acl-long.267,P07-2045,0,0.0140967,"mentary v11 for training, which is document-delimited and sentence-aligned. newstest2015 is used for development, and newstest2016 for testing. Europarl. The corpus is extracted from Europarl v7, where sentences are segmented and aligned using additional information. The train, dev and test sets are randomly split from the corpus. The detailed statistics of these corpora are shown in Table 1. We pre-process the documents by splitting them into instances with up-to 512 tokens, taking a sentence as one instance if its length exceeds 512 tokens. We tokenize and truecase the sentences with MOSES (Koehn et al., 2007) tools, applying BPE (Sennrich et al., 2016) with 30000 merging operations. We consider three standard model configurations. Base Model. Following the standard Transformer base model (Vaswani et al., 2017), we use 6 layers, 8 heads, 512 dimension outputs, and 2048 3443 Language Dataset En-De TED News Europarl #Sentences train/dev/test 0.21M/9K/2.3K 0.24M/2K/3K 1.67M/3.6K/5.1K #Documents train/dev/test 1.7K/92/22 6K/80/154 118K/239/359 #Instances train/dev/test 11K/483/123 18.5K/172/263 162K/346/498 Avg #Sents/Inst train/dev/test 18.3/18.5/18.3 12.8/12.6/11.3 10.3/10.4/10.3 Avg #Tokens/Inst tra"
2021.acl-long.267,W17-3204,0,0.0197758,", leading to improved performance. These methods are shown in Figure 1(b). One limitation of such methods is that they require much more training time due to the necessity of data augmentation. Intuitively, encoding the whole input document as a single unit allows the best integration of context information when translating the current sentence. However, little work has been done investigating the underlying reason why it is difficult to train such a document-level NMT model. One remote clue is that as the input sequence grows larger, the input becomes more sparse (Pouget-Abadie et al., 2014; Koehn and Knowles, 2017). To gain more understanding, we make dedicated experiments on the influence of input length, data scale and model size for Transformer (Section 3), finding that a Transformer model can fail to converge when training with long sequences, small datasets, or big model size. We further find that for the failed cases, the model gets stuck at local minima during training. In such situation, the attention weights from the decoder to the encoder are flat, with large entropy values. This can be because that larger input sequences increase the challenge for focusing on a local span to translate when ge"
2021.acl-long.267,N03-1017,0,0.157576,"Missing"
2021.acl-long.267,D18-1512,0,0.0527802,"Missing"
2021.acl-long.267,2020.acl-main.703,0,0.0381462,"1.7K/92/22 6K/80/154 118K/239/359 #Instances train/dev/test 11K/483/123 18.5K/172/263 162K/346/498 Avg #Sents/Inst train/dev/test 18.3/18.5/18.3 12.8/12.6/11.3 10.3/10.4/10.3 Avg #Tokens/Inst train/dev/test 436/428/429 380/355/321 320/326/323 Table 1: En-De datasets for evaluation. 64 128 256 Tokens 512 1024 Loss 5K 10K 20K Instances 40K 80K 160K dimension hidden vectors. Big Model. We follow the standard Transformer big model (Vaswani et al., 2017), using 6 layers, 16 heads, 1024 dimension outputs, and 4096 dimension hidden vectors. Large Model. We use the same settings of BART large model (Lewis et al., 2020), which involves 12 layers, 16 heads, 1024 dimension outputs, and 4096 dimension hidden vectors. We use s-BLEU and d-BLEU (Liu et al., 2020) as the metrics. The detailed descriptions are in Appendix A. Transformer and Long Inputs We empirically study Transformer (see Appendix B) on the datasets. We run each experiment five times using different random seeds, reporting the average score for comparison. Failure Reproduction Input Length. We use the Base model and fixed dataset for this comparison. We split both the training and testing documents from Europarl dataset into instances with input le"
2021.acl-long.267,N19-4009,0,0.0609932,"Missing"
2021.acl-long.267,L18-1275,0,0.0196206,"d-BLEU. Here, rnd. denotes the model trained using randomly initialized parameters. Method TED News Europarl Drop G-Transformer (rnd.) Combined attention 25.84 25.23 33.87 25.62 25.14 33.12 -0.35 Only group attention Only global attention 25.00 24.54 32.87 -0.84 Table 6: Separate effect of group and global attention reporting in d-BLEU. Here, rnd. denotes the model trained using randomly initialized parameters. Russion (En-Ru) for deixis and ellipsis. We follow the Transformer concat baseline (Voita et al., 2019b) and use both 6M sentence pairs and 1.5M document pairs from OpenSubtitles2018 (Lison et al., 2018) to train our model. The results are shown in Table 4. G-Transformer outperforms Transformer baseline concat (Voita et al., 2019b) with a large margin on three discourse features, indicating a better leverage of the source-side context. When compared to previous model LSTM-T, G-Transformer achieves a better ellipsis on both infl. and VP. However, the score on deixis is still lower, which indicates a potential direction that we can investigate in further study. Word-dropout. As shown in Table 5, worddropout (Appendix C.1) contributes about 0.37 dBLEU on average. Its contribution to TED and News"
2021.acl-long.267,2020.tacl-1.47,0,0.234542,"ores for both nonpretraining and pre-training settings on three benchmark datasets. 1 Context Source Target Decoder Translation1 Translation2 Source Encoder Source1 … Target Decoder Source2 … (b) Multi-sentence Translation Translation1 Translation2 …      … Source1 Source2  … … (c) G-Transformer (Doc-by-doc Translation) Figure 1: Overview of model structures for documentlevel machine translation. Document-level machine translation (MT) has received increasing research attention (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020). It is a more practically useful task compared to sentence-level MT because typical inputs in MT applications are text documents rather than individual sentences. A salient difference between document-level MT and sentence-level MT is that for the former, much larger inter-sentential context should be considered when translating each sentence, which include discourse structures such as anaphora, lexical cohesion, etc. Studies show that human translators consider such contexts when conducting document translation (Hardmeier, 2014; L¨aubli et al., 2018). Despite that neural models achieve compe"
2021.acl-long.267,2020.acl-main.321,0,0.0889397,"K = GX . Decoder. We use one group multi-head attention module for self-attention and another group multihead attention module for cross-attention. Similar to the encoder, we assign the same group-tag sequence to the key and value of the self-attention, that GQ = GK = GY , but use different group-tag sequences for cross-attention that GQ = GY and GK = GX . 3446 Method TED News s-BLEU d-BLEU s-BLEU d-BLEU S ENT N MT (Vaswani et al., 2017) 23.10 22.40 HAN (Miculicich et al., 2018b) 24.58 25.03 SAN (Maruf et al., 2019) 24.42 24.84 25.10 24.91 Hybrid Context (Zheng et al., 2020) Flat-Transformer (Ma et al., 2020) 24.87 23.55 Transformer on sent (baseline) 24.82 25.19 Transformer on doc (baseline) 0.76 0.60 G-Transformer random initialized (ours) 23.53 25.84* 23.55 25.23* G-Transformer fine-tuned on sent Transformer (ours) 25.12 27.17* 25.52 27.11* Fine-tuning on Pre-trained Model Flat-Transformer+BERT (Ma et al., 2020) 26.61 24.52 G-Transformer+BERT (ours) 26.81 26.14 Transformer on sent fine-tuned on BART (baseline) 27.78 29.90 Transformer on doc fine-tuned on BART (baseline) 28.29 30.49 G-Transformer fine-tuned on BART (ours) 28.06 30.03* 30.34* 31.71* Europarl s-BLEU d-BLEU 29.40 28.60 29.75 30.40"
2021.acl-long.267,P18-1118,0,0.0153541,"mum four sentences. Recent studies extend the translation unit to whole document (Junczys-Dowmunt, 2019; Liu et al., 2020), using large augmented dataset or pretrained models. Liu et al. (2020) shows that Transformer trained directly on documentlevel dataset can fail, resulting in unreasonably low BLEU scores. Following these studies, we also model translation on the whole document. We solve the training challenge using a novel locality bias with group tags. Another line of work make document-level machine translation sentence by sentence, using additional components to represent the context (Maruf and Haffari, 2018; Zheng et al., 2020; Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Yang et al., 2019). Different from these approaches, G-Transformer uses a generic design for both source and context, translating whole document in one beam search instead of sentence-by-sentence. Some methods use a two-pass strategy, generating sentence translation first, integrating context information through a post-editing model (Voita et al., 2019a; Yu et al., 2020). In contrast, G-Transformer uses a single model, which reduces the complexity for both training and inference. The locality bias we introd"
2021.acl-long.267,N19-1313,0,0.240338,"e-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets. 1 Context Source Target Decoder Translation1 Translation2 Source Encoder Source1 … Target Decoder Source2 … (b) Multi-sentence Translation Translation1 Translation2 …      … Source1 Source2  … … (c) G-Transformer (Doc-by-doc Translation) Figure 1: Overview of model structures for documentlevel machine translation. Document-level machine translation (MT) has received increasing research attention (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020). It is a more practically useful task compared to sentence-level MT because typical inputs in MT applications are text documents rather than individual sentences. A salient difference between document-level MT and sentence-level MT is that for the former, much larger inter-sentential context should be considered when translating each sentence, which include discourse structures such as anaphora, lexical cohesion, etc. Studies show that human translators consider such contexts when conducting document translation (Hardmeier, 2014; L¨aubli et al., 2018). Despite that neural m"
2021.acl-long.267,D18-1325,0,0.104196,"former, achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets. 1 Context Source Target Decoder Translation1 Translation2 Source Encoder Source1 … Target Decoder Source2 … (b) Multi-sentence Translation Translation1 Translation2 …      … Source1 Source2  … … (c) G-Transformer (Doc-by-doc Translation) Figure 1: Overview of model structures for documentlevel machine translation. Document-level machine translation (MT) has received increasing research attention (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; Liu et al., 2020). It is a more practically useful task compared to sentence-level MT because typical inputs in MT applications are text documents rather than individual sentences. A salient difference between document-level MT and sentence-level MT is that for the former, much larger inter-sentential context should be considered when translating each sentence, which include discourse structures such as anaphora, lexical cohesion, etc. Studies show that human translators consider such contexts when conducting document translation (Hardmeier, 2014; L¨aubli et al., 2018)."
2021.acl-long.267,D19-6506,0,0.0407945,"Missing"
2021.acl-long.267,P16-1162,0,0.0636249,"nt-delimited and sentence-aligned. newstest2015 is used for development, and newstest2016 for testing. Europarl. The corpus is extracted from Europarl v7, where sentences are segmented and aligned using additional information. The train, dev and test sets are randomly split from the corpus. The detailed statistics of these corpora are shown in Table 1. We pre-process the documents by splitting them into instances with up-to 512 tokens, taking a sentence as one instance if its length exceeds 512 tokens. We tokenize and truecase the sentences with MOSES (Koehn et al., 2007) tools, applying BPE (Sennrich et al., 2016) with 30000 merging operations. We consider three standard model configurations. Base Model. Following the standard Transformer base model (Vaswani et al., 2017), we use 6 layers, 8 heads, 512 dimension outputs, and 2048 3443 Language Dataset En-De TED News Europarl #Sentences train/dev/test 0.21M/9K/2.3K 0.24M/2K/3K 1.67M/3.6K/5.1K #Documents train/dev/test 1.7K/92/22 6K/80/154 118K/239/359 #Instances train/dev/test 11K/483/123 18.5K/172/263 162K/346/498 Avg #Sents/Inst train/dev/test 18.3/18.5/18.3 12.8/12.6/11.3 10.3/10.4/10.3 Avg #Tokens/Inst train/dev/test 436/428/429 380/355/321 320/326/"
2021.acl-long.267,W17-4811,0,0.120714,"f et al., 2019; Zheng et al., 2020). Document-level context is integrated into sentence-translation by introducing additional context encoder. The structure of such a model is shown in Figure 1(a). These methods suffer from two limitations. First, the context needs to be encoded separately for translating each sentence, which adds to the runtime complexity. Second, more importantly, information exchange cannot be made between the current sentence and its document context in the same encoding module. The second category extends the translation unit from a single sentence to multiple sentences (Tiedemann and Scherrer, 2017; Agrawal et al., 3442 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3442–3455 August 1–6, 2021. ©2021 Association for Computational Linguistics 2018; Zhang et al., 2020) and the whole document (Junczys-Dowmunt, 2019; Liu et al., 2020). Recently, it has been shown that when the translation unit increases from one sentence to four sentences, the performance improves (Zhang et al., 2020; Scherrer et al., 2019). However, when the whole document is encoded as a single unit fo"
2021.acl-long.267,Q17-1007,0,0.0141943,"ces tends to be constant, the time and memory complexities of group attention are approximately O(N ), making training and inference on very long inputs feasible. 4.2 Combined Attention We use only group attention on lower layers for local sentence representation, and combined attention on top layers for integrating local and global context information. We use the standard multihead attention in Eq 5 for global context, naming it global multi-head attention (GlobalMHA). Group multi-head attention in Eq 8 and global multi-head attention are combined using a gate-sum module (Zhang et al., 2016; Tu et al., 2017) HL HG g H = GroupMHA(Q, K, V, GQ , GK ), = GlobalMHA(Q, K, V ), = sigmoid([HL , HG ]W + b), = HL g + HG (1 − g), (10) where W and b are linear projection parameters, and denotes element-wise multiplication. Previous study (Jawahar et al., 2019) shows that the lower layers of Transformer catch more local syntactic relations, while the higher layers represent longer distance relations. Based on these findings, we use combined attention only on the top layers for integrating local and global context. By this design, on lower layers, the sentences are isolated from each other, while on top layers"
2021.acl-long.267,C96-2141,0,0.609057,", we study the effect of group and global attention separately. As shown in Table 6, when we replace the combined attention on top 2 layers with group attention, the performance drops by 0.22, 0.09, and 0.75 d-BLEU on TED, News, and Europarl, respectively. When we replace the combined attention with global attention, the performance decrease is enlarged to 0.84, 0.69, and 1.00 d-BLEU, respectively. These results demonstrate the necessity of combined attention for integrating local and global context information. 6 Related Work The unit of translation has evolved from word (Brown et al., 1993; Vogel et al., 1996) to phrase (Koehn et al., 2003; Chiang, 2005, 2007) and further to sentence (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) in the MT literature. The trend shows that larger units of translation, when represented properly, can lead to improved translation quality. A line of document-level MT extends translation unit to multiple sentences (Tiedemann and Scherrer, 2017; Agrawal et al., 2018; Zhang et al., 2020; Ma et al., 2020). However, these approaches are limited within a short context of maximum four sentences. Recent studies extend the translation unit to who"
2021.acl-long.267,D19-1081,0,0.0225201,"Missing"
2021.acl-long.267,P19-1116,0,0.0264584,"Missing"
2021.acl-long.267,D19-1164,0,0.0121706,"al., 2020), using large augmented dataset or pretrained models. Liu et al. (2020) shows that Transformer trained directly on documentlevel dataset can fail, resulting in unreasonably low BLEU scores. Following these studies, we also model translation on the whole document. We solve the training challenge using a novel locality bias with group tags. Another line of work make document-level machine translation sentence by sentence, using additional components to represent the context (Maruf and Haffari, 2018; Zheng et al., 2020; Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Yang et al., 2019). Different from these approaches, G-Transformer uses a generic design for both source and context, translating whole document in one beam search instead of sentence-by-sentence. Some methods use a two-pass strategy, generating sentence translation first, integrating context information through a post-editing model (Voita et al., 2019a; Yu et al., 2020). In contrast, G-Transformer uses a single model, which reduces the complexity for both training and inference. The locality bias we introduce to G-Transformer is different from the ones in Longformer (Beltagy et al., 2020) and Reformer (Kitaev"
2021.acl-long.267,D18-1049,0,0.0297389,"Missing"
2021.acl-long.267,2020.emnlp-main.81,1,0.825661,"Missing"
2021.acl-long.324,2020.crac-1.4,0,0.157332,"nly named entities and date entities are considered, and they do not consider merging non-identical nodes (e.g., “Bill” and “he” in Figure 1) that are also frequent in reallife situation. Subsequent work considers more 4204 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4204–4214 August 1–6, 2021. ©2021 Association for Computational Linguistics co-reference cases by either manually annotating AMR coreference information (O’Gorman et al., 2018) or taking a pipeline system (Anikina et al., 2020) consisting of a textual coreference resolution model (Lee et al., 2018) and an AMR-to-text aligner (Flanigan et al., 2014). Yet there is little research on automatically resolving coreference ambiguities directly on AMR, making use of AMR graph-structural features. In this work, we formulate AMR coreference resolution as a missing-link prediction problem over AMR graphs, where the input consists of multiple sentence-level AMRs, and the goal is to recover the missing coreference links connecting the AMR nodes that represent to the same entity. There are two types of links. The first type corre"
2021.acl-long.324,W13-2322,0,0.119856,"Missing"
2021.acl-long.324,D17-1209,0,0.0606619,"Missing"
2021.acl-long.324,P18-1026,0,0.49436,"racter-level CNN. We concatenate both ek and echar embeddings for each k concept before using a linear projection to form the initial representation: xk = W node ([ek ; echar ]) + bnode , k (1) where W node and bnode are model parameters. To enable global information exchange across different sentence-level AMRs, we construct a draft document-level graph by connecting the root nodes of each AMR subgraph as shown in Figure 2. This is important because AMR coreference resolution involves cross-sentence reasoning. We then adopt Graph Recurrent Network (GRN, Song et al., 2018; Zhang et al., 2018; Beck et al., 2018) to obtain rich document-level node representations. GRN is one type of graph neural network that iteratively updates its node representations with the message passing framework (Scarselli et al., 2009). Compared with alternatives such as Graph Convolutional Network (GCN, Kipf and Welling 2017; 4205 :?????????? leave-11 ??1??? :name :???? : ???? :?????? :?? :op1 … … :CONNECT :arg1 ??2??2 ??(dummy ??, he) ??(leave-11, he) ??(Bill, he) he ??(arrive-01, he) :time ℒ???????? + ℒ???????????????????? = ?? … … date-entity ??3??? Input Representation GRN Encoder ∶dropped SOFTMAX ??3??? arrive-01 ??1??2"
2021.acl-long.324,2020.lrec-1.86,0,0.032401,"nts a sentence as a rooted, directed and acyclic graph, where nodes (e.g., “Bill” in Figure 1) represents concepts and edges (e.g., “:arg0”) are the semantic relations. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is important to"
2021.acl-long.324,2020.acl-main.119,0,0.240537,"tions. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is important to extend sentence-level AMRs into the multi-sentence level. To this end, a prerequisite step is AMR coreference resolution, which aims to find the AMR components"
2021.acl-long.324,2020.acl-main.640,0,0.264686,"tions. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is important to extend sentence-level AMRs into the multi-sentence level. To this end, a prerequisite step is AMR coreference resolution, which aims to find the AMR components"
2021.acl-long.324,N19-1366,0,0.0123276,"mising results over the years. Recent work (Lee et al., 2017, 2018; Kantor and Globerson, 2019) 4211 tackled the problem end-to-end by jointly detecting mentions and predicting coreference. Lee et al. (2018) build a complete end-to-end system with the span-ranking architecture and higher-order inference technique. While previous work considers only text-level coreference, we investigate AMR co-reference resolution. AMR Representation using GNN To encode AMR graphs, many variants of GNNs such as GRNs (Song et al., 2018; Beck et al., 2018), GCNs (Zhou et al., 2020; Zhang et al., 2020) and GATs (Damonte and Cohen, 2019; Cai and Lam, 2020b; Wang et al., 2020) have been introduced. We choose a classic GRN model following Song et al. (2018) to represent our document-level AMR graph and leave the exploiting on a more efficient GNN structure for future work. 5 Conclusion We investigated a novel end-to-end multi-sentence AMR coreference resolution model using a graph neural network. Compared with previous rulebased and pipeline methods, our model better captures multi-sentence semantic information. Results on MS-AMR (in-domain) and LP (out-of-domain) datasets show the superiority and robustness of our model. In a"
2021.acl-long.324,J12-4003,0,0.023857,"by our end-toend model is solid and can transfer to a downstream application. D2S-AMRcoref-bert achieves the best performance, which is consistent with the above experiments. 4 Related Work Multi-sentence AMR Although some previous work (Szubert et al., 2020; Van Noord and Bos, 2017) explore the coreference phenomena of AMR, they mainly focus on the situation within a sentence. On the other hand, previous work on multi-sentence AMR primarily focuses on data annotation. Song et al. (2019a) annotate dropped pronouns over Chinese AMR but only deals with implicit roles in specific constructions. Gerber and Chai (2012) provide implicit role annotations, but the resources were limited to a small inventory of 5-10 predicate types rather than all implicit arguments. O’Gorman et al. (2018) annotated the MS-AMR dataset by simultaneously considering coreference, implicit role coreference and bridging relations. We consider coreference resolution as the prerequisite for creating multi-sentence AMRs, proposing the first end-to-end model for this task. Coreference Resolution Coreference resolution is a fundamental problem in natural language processing. Neural network models have shown promising results over the yea"
2021.acl-long.324,D18-1086,0,0.121583,"dotted ellipse represents an implicit role coreference. Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism for natural language understanding. It represents a sentence as a rooted, directed and acyclic graph, where nodes (e.g., “Bill” in Figure 1) represents concepts and edges (e.g., “:arg0”) are the semantic relations. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have re"
2021.acl-long.324,S18-1150,0,0.0605553,"Missing"
2021.acl-long.324,2020.tacl-1.5,0,0.0135645,"hree measures include: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAFφ4 (Luo, 2005). Following previous studies (Lee et al., 2018), the primary metric AVG-F is the unweighted average of the above three F-scores. Baselines To study the effectiveness of end-toend AMR coreference resolution, we compare our model with the following baselines: • Rule-based (Liu et al., 2015): a heuristic method that builds a large document-level AMR graph by linking identical entities. • Pipeline (Anikina et al., 2020): it uses an off-theshelf coreference system (Lee et al., 2018) with SpanBERT (Joshi et al., 2020) embeddings, and an AMR-to-text aligner (Flanigan et al., 2014). The former generates coreference from text, and the later projects this information from text to AMRs. logQ(y) k=1 y∈Yk ∩GOLD(k) (16) where GOLD(k) =  if mention vk does not belong to any gold cluster. Q(y) is calculated using Eq. 10. 3 resolution information over the development and test data of the Little Prince (LP) AMR corpus4 and use it as an out-of-domain test set. For this dataset, we consider each chapter as a document. The data statistics are shown in Table 1. Experiments We conduct experiments on the MS-AMR dataset3 (O"
2021.acl-long.324,P19-1066,0,0.0118251,"he resources were limited to a small inventory of 5-10 predicate types rather than all implicit arguments. O’Gorman et al. (2018) annotated the MS-AMR dataset by simultaneously considering coreference, implicit role coreference and bridging relations. We consider coreference resolution as the prerequisite for creating multi-sentence AMRs, proposing the first end-to-end model for this task. Coreference Resolution Coreference resolution is a fundamental problem in natural language processing. Neural network models have shown promising results over the years. Recent work (Lee et al., 2017, 2018; Kantor and Globerson, 2019) 4211 tackled the problem end-to-end by jointly detecting mentions and predicting coreference. Lee et al. (2018) build a complete end-to-end system with the span-ranking architecture and higher-order inference technique. While previous work considers only text-level coreference, we investigate AMR co-reference resolution. AMR Representation using GNN To encode AMR graphs, many variants of GNNs such as GRNs (Song et al., 2018; Beck et al., 2018), GCNs (Zhou et al., 2020; Zhang et al., 2020) and GATs (Damonte and Cohen, 2019; Cai and Lam, 2020b; Wang et al., 2020) have been introduced. We choose"
2021.acl-long.324,P19-1340,0,0.0483421,"Missing"
2021.acl-long.324,D17-1018,0,0.197666,"level AMRs, and the goal is to recover the missing coreference links connecting the AMR nodes that represent to the same entity. There are two types of links. The first type corresponds to the standard situation, where the edge connects two entity nodes (e.g., “Bill” and “he” in Figure 1) that refer to the same entity. The second type is the implicit role coreference, where one node (e.g., “Paris” in Figure 1) is a dropped argument (“:arg3”) of other predicate node (“arrive-01”). We propose an AMR coreference resolution model by extending an end-to-end text-based coreference resolution model (Lee et al., 2017). In particular, we use a graph neural network to represent input AMRs for inducing expressive features. To enable cross-sentence information exchange, we make connections between sentence-level AMRs by linking their root nodes. Besides, we introduce a concept identification module to distinguish functional graph nodes (non-concept nodes, e.g., “person” in Figure 1), entity nodes (e.g., “Bill”), verbal nodes with implicit role (e.g., “arrive-01”) and other regular nodes (e.g., “leave-11”) to help improve the performance. The final antecedent prediction is conducted between the selected nodes a"
2021.acl-long.324,N18-2108,0,0.0505525,"Missing"
2021.acl-long.324,W15-4502,0,0.0166655,"cu et al., 2013) is a semantic formalism for natural language understanding. It represents a sentence as a rooted, directed and acyclic graph, where nodes (e.g., “Bill” in Figure 1) represents concepts and edges (e.g., “:arg0”) are the semantic relations. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015"
2021.acl-long.324,C18-1101,0,0.0542114,"s an implicit role coreference. Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism for natural language understanding. It represents a sentence as a rooted, directed and acyclic graph, where nodes (e.g., “Bill” in Figure 1) represents concepts and edges (e.g., “:arg0”) are the semantic relations. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing re"
2021.acl-long.324,N15-1114,0,0.343236,"sentation (AMR) has become very popular and AMR has been shown effective on many sentence-level tasks, little work has studied how to generate AMRs that can represent multi-sentence information. We introduce the first end-to-end AMR coreference resolution model in order to build multi-sentence AMRs. Compared with the previous pipeline and rule-based approaches, our model alleviates error propagation and it is more robust for both in-domain and out-domain situations. Besides, the document-level AMRs obtained by our model can significantly improve over the AMRs generated by a rule-based method (Liu et al., 2015) on text summarization. 1 :arg0 person :name name :op1 Bill Sentence2: He arrived at noon. arrive-01 leave-11 :arg1 :arg2 city he :name name Paris :arg3 date-entity :dayperiod :op1 noon Paris Figure 1: Multi-sentence AMR example, where nodes with the same non-black color are coreferential and the dotted ellipse represents an implicit role coreference. Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism for natural language understanding. It represents a sentence as a rooted, directed and acyclic graph, where nodes (e.g., “Bill” in Figure 1) repre"
2021.acl-long.324,H05-1004,0,0.0657307,"eference clusters GOLD(k)|N k=1 and antecedent candidates Yk = {, yπ , ..., yk−1 } for mention vk , Lantecedent measures whether mentions are linked to their correct antecedent. Since the antecedents are latent, the antecedent loss is a marginal log-likelihood of all correct antecedents implied by gold clustering: Lantecedent (θ) = N Y X 3.1 Setup Evaluation Metrics We use the standard evaluation metrics for coreference resolution evaluation, computed using the official CoNLL-2012 evaluation toolkit. Three measures include: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAFφ4 (Luo, 2005). Following previous studies (Lee et al., 2018), the primary metric AVG-F is the unweighted average of the above three F-scores. Baselines To study the effectiveness of end-toend AMR coreference resolution, we compare our model with the following baselines: • Rule-based (Liu et al., 2015): a heuristic method that builds a large document-level AMR graph by linking identical entities. • Pipeline (Anikina et al., 2020): it uses an off-theshelf coreference system (Lee et al., 2018) with SpanBERT (Joshi et al., 2020) embeddings, and an AMR-to-text aligner (Flanigan et al., 2014). The former generat"
2021.acl-long.324,P18-1037,0,0.0192462,"n Figure 1) represents concepts and edges (e.g., “:arg0”) are the semantic relations. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is important to extend sentence-level AMRs into the multi-sentence level. To this end, a prerequis"
2021.acl-long.324,P19-1451,0,0.021632,"s concepts and edges (e.g., “:arg0”) are the semantic relations. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is important to extend sentence-level AMRs into the multi-sentence level. To this end, a prerequisite step is AMR coref"
2021.acl-long.324,C18-1313,0,0.0377273,"Missing"
2021.acl-long.324,N15-1119,0,0.0284858,"Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is important to extend sentence-level AMRs into the multi-sentence level. To this end, a prerequisite step is AMR coreference resolution, which aims to find the AMR components referring to the same entity. Figure 1 shows the AMR graphs of two consecutive sentences in a document. An AMR coreference resolution model need to identify two coreference cases: “he” refers to “Bill” in the first graph, and “arrive-01” omits an argument “:arg3” that refers to “Paris”. Relatively little research has been done on AMR coreference r"
2021.acl-long.324,2020.findings-emnlp.163,0,0.0579032,"Missing"
2021.acl-long.324,W17-2315,0,0.0588787,"Missing"
2021.acl-long.324,Q19-1002,1,0.955433,"m for natural language understanding. It represents a sentence as a rooted, directed and acyclic graph, where nodes (e.g., “Bill” in Figure 1) represents concepts and edges (e.g., “:arg0”) are the semantic relations. Encompassing knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et"
2021.acl-long.324,2020.findings-emnlp.199,0,0.483163,"e with D2S-Rule-based on the downstream summerization task. This shows that the error propagation issue of Pipeline can introduce further negative effects to a downstream application. On the other hand, both D2S-AMRcoref-base and D2SAMRcoref-bert show much better results than the baselines across all Rouge metrics. This demonstrates that the improvements made by our end-toend model is solid and can transfer to a downstream application. D2S-AMRcoref-bert achieves the best performance, which is consistent with the above experiments. 4 Related Work Multi-sentence AMR Although some previous work (Szubert et al., 2020; Van Noord and Bos, 2017) explore the coreference phenomena of AMR, they mainly focus on the situation within a sentence. On the other hand, previous work on multi-sentence AMR primarily focuses on data annotation. Song et al. (2019a) annotate dropped pronouns over Chinese AMR but only deals with implicit roles in specific constructions. Gerber and Chai (2012) provide implicit role annotations, but the resources were limited to a small inventory of 5-10 predicate types rather than all implicit arguments. O’Gorman et al. (2018) annotated the MS-AMR dataset by simultaneously considering corefer"
2021.acl-long.324,W17-7306,0,0.0248713,"Missing"
2021.acl-long.324,M95-1005,0,0.167102,"ent Prediction Loss. Given a training AMR document with gold coreference clusters GOLD(k)|N k=1 and antecedent candidates Yk = {, yπ , ..., yk−1 } for mention vk , Lantecedent measures whether mentions are linked to their correct antecedent. Since the antecedents are latent, the antecedent loss is a marginal log-likelihood of all correct antecedents implied by gold clustering: Lantecedent (θ) = N Y X 3.1 Setup Evaluation Metrics We use the standard evaluation metrics for coreference resolution evaluation, computed using the official CoNLL-2012 evaluation toolkit. Three measures include: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998) and CEAFφ4 (Luo, 2005). Following previous studies (Lee et al., 2018), the primary metric AVG-F is the unweighted average of the above three F-scores. Baselines To study the effectiveness of end-toend AMR coreference resolution, we compare our model with the following baselines: • Rule-based (Liu et al., 2015): a heuristic method that builds a large document-level AMR graph by linking identical entities. • Pipeline (Anikina et al., 2020): it uses an off-theshelf coreference system (Lee et al., 2018) with SpanBERT (Joshi et al., 2020) embeddings, and an AMR-to-te"
2021.acl-long.324,2020.tacl-1.2,0,0.100941,"ee et al., 2017, 2018; Kantor and Globerson, 2019) 4211 tackled the problem end-to-end by jointly detecting mentions and predicting coreference. Lee et al. (2018) build a complete end-to-end system with the span-ranking architecture and higher-order inference technique. While previous work considers only text-level coreference, we investigate AMR co-reference resolution. AMR Representation using GNN To encode AMR graphs, many variants of GNNs such as GRNs (Song et al., 2018; Beck et al., 2018), GCNs (Zhou et al., 2020; Zhang et al., 2020) and GATs (Damonte and Cohen, 2019; Cai and Lam, 2020b; Wang et al., 2020) have been introduced. We choose a classic GRN model following Song et al. (2018) to represent our document-level AMR graph and leave the exploiting on a more efficient GNN structure for future work. 5 Conclusion We investigated a novel end-to-end multi-sentence AMR coreference resolution model using a graph neural network. Compared with previous rulebased and pipeline methods, our model better captures multi-sentence semantic information. Results on MS-AMR (in-domain) and LP (out-of-domain) datasets show the superiority and robustness of our model. In addition, experiments on the downstream t"
2021.acl-long.324,P19-1009,0,0.0479197,"Missing"
2021.acl-long.324,2020.emnlp-main.169,0,0.562041,"Missing"
2021.acl-long.324,P18-1030,1,0.902422,"puted by using a character-level CNN. We concatenate both ek and echar embeddings for each k concept before using a linear projection to form the initial representation: xk = W node ([ek ; echar ]) + bnode , k (1) where W node and bnode are model parameters. To enable global information exchange across different sentence-level AMRs, we construct a draft document-level graph by connecting the root nodes of each AMR subgraph as shown in Figure 2. This is important because AMR coreference resolution involves cross-sentence reasoning. We then adopt Graph Recurrent Network (GRN, Song et al., 2018; Zhang et al., 2018; Beck et al., 2018) to obtain rich document-level node representations. GRN is one type of graph neural network that iteratively updates its node representations with the message passing framework (Scarselli et al., 2009). Compared with alternatives such as Graph Convolutional Network (GCN, Kipf and Welling 2017; 4205 :?????????? leave-11 ??1??? :name :???? : ???? :?????? :?? :op1 … … :CONNECT :arg1 ??2??2 ??(dummy ??, he) ??(leave-11, he) ??(Bill, he) he ??(arrive-01, he) :time ℒ???????? + ℒ???????????????????? = ?? … … date-entity ??3??? Input Representation GRN Encoder ∶dropped SOFTMAX ??3"
2021.acl-long.324,2020.acl-main.397,1,0.901984,"knowledge of named entities, semantic roles and coreference structures, AMR has been proven effective for downstream tasks, including information extraction (Rao et al., 2017), text summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018), paraphrase detection (Issa Alaa Aldine et al., 2018), event detection (Li et al., 2015), machine translation (Song et al., 2019b) and dialogue understanding (Bonial et al., 2020). Existing work on AMR mainly focuses on individual sentences (Lyu and Titov, 2018; Naseem et al., 2019; Ge et al., 2019; Zhang et al., 2019; Cai and Lam, 2020a; Zhou et al., 2020). On the other hand, with the advance of neural networks in NLP, tasks involving multiple sentences with cross-sentence reasoning (e.g., text summarization, reading comprehension and dialogue response generation) have received increasing research attention. Given the effectiveness of AMR on sentencelevel tasks (Pan et al., 2015; Rao et al., 2017; Issa Alaa Aldine et al., 2018; Song et al., 2019b), it is important to extend sentence-level AMRs into the multi-sentence level. To this end, a prerequisite step is AMR coreference resolution, which aims to find the AMR components referring to the sam"
2021.acl-long.324,P18-1150,1,0.944722,"ngs echar k are computed by using a character-level CNN. We concatenate both ek and echar embeddings for each k concept before using a linear projection to form the initial representation: xk = W node ([ek ; echar ]) + bnode , k (1) where W node and bnode are model parameters. To enable global information exchange across different sentence-level AMRs, we construct a draft document-level graph by connecting the root nodes of each AMR subgraph as shown in Figure 2. This is important because AMR coreference resolution involves cross-sentence reasoning. We then adopt Graph Recurrent Network (GRN, Song et al., 2018; Zhang et al., 2018; Beck et al., 2018) to obtain rich document-level node representations. GRN is one type of graph neural network that iteratively updates its node representations with the message passing framework (Scarselli et al., 2009). Compared with alternatives such as Graph Convolutional Network (GCN, Kipf and Welling 2017; 4205 :?????????? leave-11 ??1??? :name :???? : ???? :?????? :?? :op1 … … :CONNECT :arg1 ??2??2 ??(dummy ??, he) ??(leave-11, he) ??(Bill, he) he ??(arrive-01, he) :time ℒ???????? + ℒ???????????????????? = ?? … … date-entity ??3??? Input Representation GRN Encoder"
2021.acl-long.342,W13-2322,0,0.0831056,"ared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g., “ARG0”1 ), while abstracting away function words. It can thus potentially offer core concepts and explicit structures needed for aggregating the main content in dialogue. In addition, AMR can also be useful for reducing the negative influence of variances in surf"
2021.acl-long.342,2020.acl-main.9,0,0.313415,"se generation tasks show the superiority of our model. To our knowledge, we are the first to leverage a formal semantic representation into neural dialogue modeling. 1 Ground-Truth: Maybe, but I would rather watch romance, science fiction, crime or even disaster movie instead of a horror picture… Transformer: Great. I’m looking forward to it. I just can’t keep away from the food that I saw. Figure 1: A conversation from DailyDialog. Some important contents are marked with squares. Introduction Dialogue systems have received increasing research attention (Wen et al., 2015; Serban et al., 2017; Bao et al., 2020), with much recent work focusing on social chats (Ritter et al., 2011; Li et al., 2017) and task-oriented dialogues (Wen et al., 2017; Dinan et al., 2019). There are two salient subtasks in dialogue modeling, namely dialogue understanding (Choi et al., 2018; Reddy et al., 2019; Yu et al., 2020) and response generation (Li et al., 2017; Budzianowski et al., 2018). The former refers to understanding of semantic and discourse details in a dialogue history, and the latter concerns making a fluent, novel and coherent utterance. The current state-of-the-art methods employ neural networks and end-to-"
2021.acl-long.342,2020.lrec-1.86,0,0.115932,"antic representation for the modeling of everyday conversations. Constructing AMRs beyond Sentence Level There are a few attempts to construct AMRs beyond the sentence level. Liu et al. (2015) construct document-level AMRs by merging identical concepts of sentence-level AMRs for abstractive summerization, and Liao et al. (2018) further extend this approach to multi-document summerization. O’Gorman et al. (2018) manually annotate co-reference information across sentence AMRs. We focus on creating conversation-level AMRs to facilitate information exchange more effectively for dialogue modeling. Bonial et al. (2020) adapt AMRs on dialogues by enriching the standard AMR schema with dialogue acts, tense and aspect, and they construct a dataset consisting of 340 dialogue AMRs. However, they propose theoretical changes in the schema for annotating AMRs, while we explore empirical solutions that leverage existing AMRs of the standard schema on dialogues. AMR Parsing and Encoding Our work is also related to AMR parsing (Flanigan et al., 2014; Konstas et al., 2017a; Lyu and Titov, 2018; Guo and Lu, 2018; Zhang et al., 2019; Cai and Lam, 2020) and AMR encoding (Konstas et al., 2017b; Song et al., 2018; Zhu et al"
2021.acl-long.342,D18-1547,0,0.04815,"Missing"
2021.acl-long.342,2020.acl-main.119,0,0.238764,"dialogue generation. One more advantage is that AMR is helpful to enhance the robustness and has a potential to improve the interpretability of neural models. To our knowledge, this is the first attempt to leverage the AMR semantic representation into neural networks for dialogue understanding and generation. Our code is available at https://github.com/muyeby/AMR-Dialogue. 2 Constructing Dialogue AMRs Figure 2 illustrates our method for constructing a dialogue-level AMR graph from multiple utterancelevel AMRs. Given a dialogue consisting multiple utterances, we adopt a pretrained AMR parser (Cai and Lam, 2020) to obtain an AMR graph for each utterance. For utterances containing multiple sentences, we parse them into multiple AMR graphs, and mark them belonging to the same utterance. We construct each dialogue AMR graph by making connections between utterance AMRs. In particular, we take three strategies according to speaker, identical concept and co-reference information. Speaker We add a dummy node and connect it to all root nodes of utterance AMRs. We add speaker tags (e.g., SPEAKER 1 and SPEAKER 2) to the edges to distinguish different speakers. The dummy node ensures that all utterance AMRs are"
2021.acl-long.342,P17-1175,0,0.0645532,"Missing"
2021.acl-long.342,2020.emnlp-main.651,0,0.0706939,"Missing"
2021.acl-long.342,D18-1241,0,0.0169852,"aster movie instead of a horror picture… Transformer: Great. I’m looking forward to it. I just can’t keep away from the food that I saw. Figure 1: A conversation from DailyDialog. Some important contents are marked with squares. Introduction Dialogue systems have received increasing research attention (Wen et al., 2015; Serban et al., 2017; Bao et al., 2020), with much recent work focusing on social chats (Ritter et al., 2011; Li et al., 2017) and task-oriented dialogues (Wen et al., 2017; Dinan et al., 2019). There are two salient subtasks in dialogue modeling, namely dialogue understanding (Choi et al., 2018; Reddy et al., 2019; Yu et al., 2020) and response generation (Li et al., 2017; Budzianowski et al., 2018). The former refers to understanding of semantic and discourse details in a dialogue history, and the latter concerns making a fluent, novel and coherent utterance. The current state-of-the-art methods employ neural networks and end-to-end training (Sutskever et al., 2014; Bahdanau et al., 2015) for dialogue modeling. For instance, sequence-to-sequence models have been used to encode a dialogue history, before directly synthesizing the next utterance (Vinyals and Le, 2015; Wen et al., 201"
2021.acl-long.342,N19-1423,0,0.0185884,"ttps://github.com/huggingface/neuralcoref For simplicity, we omit the coreference links between the second and third utterance for display. 4 (5) (1) L L {hL 1 , h2 , ..., hn }, αij = Attn(hi , hj ). (4) where W1 , W2 , b1 , b2 are model parameters. Prel = softmax(W3 [ha1 ; ha2 ] + b3 ), H = SeqEncoder(emb(S)), (3) where W3 and b3 are model parameters. The k-th value of Prel is the conditional probability of k-th relation in R. Given a training instance hS, a1 , a2 , ri, the local loss is: ` = −logP (r|S, a1 , a2 ; θ), (6) where θ denotes the set of model parameters. In practice, we use BERT (Devlin et al., 2019) for calculating ha1 and ha2 , which can be regarded as pre-trained initialization of the Transformer encoder. 4432 ℎ2?? ℎ1?? ℎ3?? ℎ4?? ℎ5?? Graph Transformer Projected AMR edges ℎ1?? Text ??1 ??1 ℎ2?? ℎ3?? ??2 Transformer ??2 (a) ??3 ℎ4?? ??3 ??4 ??1 ??1 ??2 ??2 ??3 ??3 Graph Encoder ??1 Sequence Encoder ??2 ℎ�1 ℎ� 3 ℎ� 2 ??3 ??3 ??1 ??2 ??3 ??4 ??5 ??4 Sequence Encoder Dual Attention ℎ� 4 (b) ??2 Graph Encoder Feature Fusion ℎ5?? ??5 ??1 ??2 ??3 ??4 ??5 ??4 ??1 ℎ� 5 … ???? ???? ???? ????+1 ????+1 … (c) Figure 3: AMR for dialogue modeling. (a) Using AMR to enrich text representation. (b,"
2021.acl-long.342,D19-1407,0,0.0519174,"Missing"
2021.acl-long.342,P14-1134,0,0.0788866,"(W Q hl−1 i ) (W hj √ d + W Rr ij ) , where W R is a transformation matrix, r ij is the embedding of relation rij , d is hidden state size, and {h01 , h02 , ..., h0M } = {n1 , n2 , ..., nM }. The hidden state of ni is then updated as: XM hli = αij (W V hl−1 + W R r ij ), (11) j j=1 where W V is a parameter matrix. Overall, given an input AMR graph G = hV, Ei, the graph Transformer encoder can be written as H = GraphEncoder(emb(V), emb(E)), (12) L , ..., hL } denotes top-layer where H = {hL , h 1 2 M graph encoder hidden states. 4.2 Enriching Text Representation We first use the JAMR aligner (Flanigan et al., 2014) to obtain a node-to-word alignment, then adopt the alignment to project the AMR edges onto text with following rules:    ri0 j 0 , if A(ni0 ) = wi , A(nj 0 ) = wj , if i = j, rˆij = Self,   None, otherwise, (13) where A is a one-to-K alignment (K ∈ [0, . . . , N ]). In this way, we obtain a projected graph G 0 = hV 0 , E 0 i, where V 0 represents the set of input words {w1 , w2 , ..., wN } and E 0 denotes a set of word-to-word semantic relations. Inspired by previous work on AMR graph modeling (Guo et al., 2019; Song et al., 2019b; Sun et al., 2019), we adopt a hierarchical encoder that"
2021.acl-long.342,kingsbury-palmer-2002-treebank,0,0.576566,"Missing"
2021.acl-long.342,P17-1014,0,0.272741,"rmation across sentence AMRs. We focus on creating conversation-level AMRs to facilitate information exchange more effectively for dialogue modeling. Bonial et al. (2020) adapt AMRs on dialogues by enriching the standard AMR schema with dialogue acts, tense and aspect, and they construct a dataset consisting of 340 dialogue AMRs. However, they propose theoretical changes in the schema for annotating AMRs, while we explore empirical solutions that leverage existing AMRs of the standard schema on dialogues. AMR Parsing and Encoding Our work is also related to AMR parsing (Flanigan et al., 2014; Konstas et al., 2017a; Lyu and Titov, 2018; Guo and Lu, 2018; Zhang et al., 2019; Cai and Lam, 2020) and AMR encoding (Konstas et al., 2017b; Song et al., 2018; Zhu et al., 2019; Song et al., 2020; Zhao et al., 2020; Bai et al., 2020). The former task makes it possible to use automatically-generated AMRs for downstream applications, while the latter helps to effectively exploit structural information in AMRs. In this work, we investigate AMRs for dialogue representation and combine AMRs with text for dialogue modeling. 9 Conclusion We investigated the feasibility of using AMRs for dialogue modeling, describing an"
2021.acl-long.342,N16-1014,0,0.0367911,"uation. Following Bao et al. (2020), we ask annotators who study linguistics to evaluate model outputs from four aspects, which are fluency, coherence, informativeness and overall performance. The scores are in a scale of {0, 1, 2}. The higher, the better. 6.1 6.2 6 Response Generation Experiments Settings We take Transformer as a baseline. Our hyperparameters are selected by word prediction accuracy on validation dataset. The detailed hyperparameters are given in Appendix (See Table 6). Metric We set the decoding beam size as 5 and adopt BLEU-1/2/3/4 (Papineni et al., 2002) and Distinct-1/2 (Li et al., 2016) as automatic evaluation metrics. The former measures the ngram overlap between generated response and Automatic Evaluation Results Table 2 reports the performances of the previous state-of-the-art methods and proposed models on the DailyDialog testset. For the previous methods, PLATO and PLATO w/o L are both Transformer models pre-trained on large-scale conversational data (8.3 million samples) and finetuned on DailyDialog. For completeness, we also report other systems including Seq2Seq (Vinyals and Le, 2015) and iVAEMI (Fang et al., 2019). 4436 Model Transformer Hier Dual Fluency Coherence"
2021.acl-long.342,I17-1099,0,0.199859,"t to leverage a formal semantic representation into neural dialogue modeling. 1 Ground-Truth: Maybe, but I would rather watch romance, science fiction, crime or even disaster movie instead of a horror picture… Transformer: Great. I’m looking forward to it. I just can’t keep away from the food that I saw. Figure 1: A conversation from DailyDialog. Some important contents are marked with squares. Introduction Dialogue systems have received increasing research attention (Wen et al., 2015; Serban et al., 2017; Bao et al., 2020), with much recent work focusing on social chats (Ritter et al., 2011; Li et al., 2017) and task-oriented dialogues (Wen et al., 2017; Dinan et al., 2019). There are two salient subtasks in dialogue modeling, namely dialogue understanding (Choi et al., 2018; Reddy et al., 2019; Yu et al., 2020) and response generation (Li et al., 2017; Budzianowski et al., 2018). The former refers to understanding of semantic and discourse details in a dialogue history, and the latter concerns making a fluent, novel and coherent utterance. The current state-of-the-art methods employ neural networks and end-to-end training (Sutskever et al., 2014; Bahdanau et al., 2015) for dialogue modeling. For"
2021.acl-long.342,C18-1101,0,0.0681589,"nodes, making it easy to find the most salient context. Explicit structures are also more interpretable compared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g., “ARG0”1 ), while abstracting away function words. It can thus potentially offer core concepts and explicit structures needed for aggregating the main"
2021.acl-long.342,N15-1114,0,0.174097,"es and builds structural relations between nodes, making it easy to find the most salient context. Explicit structures are also more interpretable compared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g., “ARG0”1 ), while abstracting away function words. It can thus potentially offer core concepts and explici"
2021.acl-long.342,P18-1037,0,0.017178,"AMRs. We focus on creating conversation-level AMRs to facilitate information exchange more effectively for dialogue modeling. Bonial et al. (2020) adapt AMRs on dialogues by enriching the standard AMR schema with dialogue acts, tense and aspect, and they construct a dataset consisting of 340 dialogue AMRs. However, they propose theoretical changes in the schema for annotating AMRs, while we explore empirical solutions that leverage existing AMRs of the standard schema on dialogues. AMR Parsing and Encoding Our work is also related to AMR parsing (Flanigan et al., 2014; Konstas et al., 2017a; Lyu and Titov, 2018; Guo and Lu, 2018; Zhang et al., 2019; Cai and Lam, 2020) and AMR encoding (Konstas et al., 2017b; Song et al., 2018; Zhu et al., 2019; Song et al., 2020; Zhao et al., 2020; Bai et al., 2020). The former task makes it possible to use automatically-generated AMRs for downstream applications, while the latter helps to effectively exploit structural information in AMRs. In this work, we investigate AMRs for dialogue representation and combine AMRs with text for dialogue modeling. 9 Conclusion We investigated the feasibility of using AMRs for dialogue modeling, describing an algorithm to construc"
2021.acl-long.342,N18-2078,0,0.0234461,"nt context. Explicit structures are also more interpretable compared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g., “ARG0”1 ), while abstracting away function words. It can thus potentially offer core concepts and explicit structures needed for aggregating the main content in dialogue. In addition, AMR can also be u"
2021.acl-long.342,2020.acl-main.173,0,0.022716,"al., 2015) for dialogue modeling. For instance, sequence-to-sequence models have been used to encode a dialogue history, before directly synthesizing the next utterance (Vinyals and Le, 2015; Wen et al., 2017; Bao et al., 2020). Despite giving strong empirical results, neural models can suffer from spurious feature associations in their neural semantic representation (Poliak et al., 2018; Kaushik et al., 2020), which can lead to weak robustness, inducing irrelevant dialogue states (Xu and Sarikaya, 2014; Sharma et al., 2019; Rastogi et al., 2019) and generating unfaithful or irrelevant text (Maynez et al., 2020; Niu and Bansal, 2020). As shown in Figure 1, the baseline Transformer model pays attention to the word “lamb” but ignores its surrounding context, which has important contents (marked with squares) that indicate its true meaning, thereby giving an irrelevant response that is related to food. Intuitively, such issues can be alleviated by having a structural representation of semantic information, which treats entities as nodes and builds structural relations between nodes, making it easy to find the most salient context. Explicit structures are also more interpretable compared to 4430 Proceed"
2021.acl-long.342,2020.acl-main.141,0,0.062879,"ious work on DialogRE, we report macro F1 score on relations in both the standard (F1) and conversational settings (F1c ; Yu et al., 2020). F1c is computed over the first few turns of a dialogue where two arguments are first mentioned. 7 Main Results Table 1 shows the results of different systems on DialogRE. We compare the proposed model with two BERT-based approches, BERT and BERTs . Based on BERT, BERTs (Yu et al., 2020) highlights speaker information by replacing speaker arguments with special tokens. For completeness, we also include recent methods, such as AGGCN (Guo et al., 2019), LSR (Nan et al., 2020) and DHGAT (Chen et al., 2020). BERTc and Hier, Dual represent our baseline and the proposed models, respectively. By incorporating speaker information, BERTs gives the best performance among the previous system. Our BERTc baseline outperforms BERTs by a large margin, as BERTc additionally considers argument representations for classification. Hier significantly (p < 0.01)8 outperforms BERTc in all settings, with 1.4 points of improvement in terms of F1 score on average. A similar trend is observed under F1c . This shows that semantic information in AMR is beneficial to dialogue relation extra"
2021.acl-long.342,C18-1313,0,0.0344113,"Missing"
2021.acl-long.342,J05-1004,0,0.303733,"Missing"
2021.acl-long.342,N19-2013,0,0.0204127,"etworks and end-to-end training (Sutskever et al., 2014; Bahdanau et al., 2015) for dialogue modeling. For instance, sequence-to-sequence models have been used to encode a dialogue history, before directly synthesizing the next utterance (Vinyals and Le, 2015; Wen et al., 2017; Bao et al., 2020). Despite giving strong empirical results, neural models can suffer from spurious feature associations in their neural semantic representation (Poliak et al., 2018; Kaushik et al., 2020), which can lead to weak robustness, inducing irrelevant dialogue states (Xu and Sarikaya, 2014; Sharma et al., 2019; Rastogi et al., 2019) and generating unfaithful or irrelevant text (Maynez et al., 2020; Niu and Bansal, 2020). As shown in Figure 1, the baseline Transformer model pays attention to the word “lamb” but ignores its surrounding context, which has important contents (marked with squares) that indicate its true meaning, thereby giving an irrelevant response that is related to food. Intuitively, such issues can be alleviated by having a structural representation of semantic information, which treats entities as nodes and builds structural relations between nodes, making it easy to find the most salient context. Explic"
2021.acl-long.342,Q19-1016,0,0.0267408,"of a horror picture… Transformer: Great. I’m looking forward to it. I just can’t keep away from the food that I saw. Figure 1: A conversation from DailyDialog. Some important contents are marked with squares. Introduction Dialogue systems have received increasing research attention (Wen et al., 2015; Serban et al., 2017; Bao et al., 2020), with much recent work focusing on social chats (Ritter et al., 2011; Li et al., 2017) and task-oriented dialogues (Wen et al., 2017; Dinan et al., 2019). There are two salient subtasks in dialogue modeling, namely dialogue understanding (Choi et al., 2018; Reddy et al., 2019; Yu et al., 2020) and response generation (Li et al., 2017; Budzianowski et al., 2018). The former refers to understanding of semantic and discourse details in a dialogue history, and the latter concerns making a fluent, novel and coherent utterance. The current state-of-the-art methods employ neural networks and end-to-end training (Sutskever et al., 2014; Bahdanau et al., 2015) for dialogue modeling. For instance, sequence-to-sequence models have been used to encode a dialogue history, before directly synthesizing the next utterance (Vinyals and Le, 2015; Wen et al., 2017; Bao et al., 2020)"
2021.acl-long.342,D11-1054,0,0.138267,"Missing"
2021.acl-long.342,2021.eacl-main.228,0,0.0155269,"rmation, which treats entities as nodes and builds structural relations between nodes, making it easy to find the most salient context. Explicit structures are also more interpretable compared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g., “ARG0”1 ), while abstracting away function words. It can thus potentiall"
2021.acl-long.342,P02-1040,0,0.114126,". In addition, we also conduct human evaluation. Following Bao et al. (2020), we ask annotators who study linguistics to evaluate model outputs from four aspects, which are fluency, coherence, informativeness and overall performance. The scores are in a scale of {0, 1, 2}. The higher, the better. 6.1 6.2 6 Response Generation Experiments Settings We take Transformer as a baseline. Our hyperparameters are selected by word prediction accuracy on validation dataset. The detailed hyperparameters are given in Appendix (See Table 6). Metric We set the decoding beam size as 5 and adopt BLEU-1/2/3/4 (Papineni et al., 2002) and Distinct-1/2 (Li et al., 2016) as automatic evaluation metrics. The former measures the ngram overlap between generated response and Automatic Evaluation Results Table 2 reports the performances of the previous state-of-the-art methods and proposed models on the DailyDialog testset. For the previous methods, PLATO and PLATO w/o L are both Transformer models pre-trained on large-scale conversational data (8.3 million samples) and finetuned on DailyDialog. For completeness, we also report other systems including Seq2Seq (Vinyals and Le, 2015) and iVAEMI (Fang et al., 2019). 4436 Model Trans"
2021.acl-long.342,S18-2023,0,0.0282355,"Missing"
2021.acl-long.342,N19-1057,0,0.0200161,"thods employ neural networks and end-to-end training (Sutskever et al., 2014; Bahdanau et al., 2015) for dialogue modeling. For instance, sequence-to-sequence models have been used to encode a dialogue history, before directly synthesizing the next utterance (Vinyals and Le, 2015; Wen et al., 2017; Bao et al., 2020). Despite giving strong empirical results, neural models can suffer from spurious feature associations in their neural semantic representation (Poliak et al., 2018; Kaushik et al., 2020), which can lead to weak robustness, inducing irrelevant dialogue states (Xu and Sarikaya, 2014; Sharma et al., 2019; Rastogi et al., 2019) and generating unfaithful or irrelevant text (Maynez et al., 2020; Niu and Bansal, 2020). As shown in Figure 1, the baseline Transformer model pays attention to the word “lamb” but ignores its surrounding context, which has important contents (marked with squares) that indicate its true meaning, thereby giving an irrelevant response that is related to food. Intuitively, such issues can be alleviated by having a structural representation of semantic information, which treats entities as nodes and builds structural relations between nodes, making it easy to find the most"
2021.acl-long.342,D19-1462,0,0.0203939,":poss :speaker1 I :mode bill :ARG1 I :ARG1 :ARG1 :ARG1 I :ARG2 say Dummy mistake I possible Graph Merge :ARG1 have :ARG0 :ARG0 sir I fear S1 :speaker2 certain :ARG1 :ARG0 AMR Parsing :mode interrogative I :ARG2 say S2 :speaker1 :poss bill I interrogative :ARG1 possible :ARG1 have :ARG0 it (b) Utterance AMR Graphs unknown (c) Dialogue AMR Graph Figure 2: Dialogue AMR graph construction process. Step 1: parse raw-text utterance into utterance AMR graphs; Step 2: connect utterance AMR graphs into a dialogue AMR graph. which are frequent in conversations (Grosz et al., 1995; Newman et al., 2008; Quan et al., 2019). We conduct co-reference resolution on dialogue text using an off-to-shelf model3 in order to identify concept nodes in utterance AMRs that refer to the same entity. For example, in Figure 2, “I” in the first utterance, and “sir” in the second utterance refer to the same entity, SPEAKR 1. We add edges labeled with COREF between them, starting from later nodes to earlier nodes (later and earlier here refer to the temporal order of ongoing conversation), to indicate their relation4 . 3 Baseline System We adopt a standard Transformer (Vaswani et al., 2017) for dialogue history encoding. Typicall"
2021.acl-long.342,Q19-1002,1,0.933316,"ures are also more interpretable compared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g., “ARG0”1 ), while abstracting away function words. It can thus potentially offer core concepts and explicit structures needed for aggregating the main content in dialogue. In addition, AMR can also be useful for reducing"
2021.acl-long.342,2020.acl-main.712,1,0.763791,"Rs on dialogues by enriching the standard AMR schema with dialogue acts, tense and aspect, and they construct a dataset consisting of 340 dialogue AMRs. However, they propose theoretical changes in the schema for annotating AMRs, while we explore empirical solutions that leverage existing AMRs of the standard schema on dialogues. AMR Parsing and Encoding Our work is also related to AMR parsing (Flanigan et al., 2014; Konstas et al., 2017a; Lyu and Titov, 2018; Guo and Lu, 2018; Zhang et al., 2019; Cai and Lam, 2020) and AMR encoding (Konstas et al., 2017b; Song et al., 2018; Zhu et al., 2019; Song et al., 2020; Zhao et al., 2020; Bai et al., 2020). The former task makes it possible to use automatically-generated AMRs for downstream applications, while the latter helps to effectively exploit structural information in AMRs. In this work, we investigate AMRs for dialogue representation and combine AMRs with text for dialogue modeling. 9 Conclusion We investigated the feasibility of using AMRs for dialogue modeling, describing an algorithm to construct dialogue-level AMRs automatically and exploiting two ways to incorporate AMRs into neural dialogue systems. Experiments on two benchmarks show advantage"
2021.acl-long.342,D19-1020,1,0.939365,"ures are also more interpretable compared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g., “ARG0”1 ), while abstracting away function words. It can thus potentially offer core concepts and explicit structures needed for aggregating the main content in dialogue. In addition, AMR can also be useful for reducing"
2021.acl-long.342,P18-1150,1,0.854873,"deling. Bonial et al. (2020) adapt AMRs on dialogues by enriching the standard AMR schema with dialogue acts, tense and aspect, and they construct a dataset consisting of 340 dialogue AMRs. However, they propose theoretical changes in the schema for annotating AMRs, while we explore empirical solutions that leverage existing AMRs of the standard schema on dialogues. AMR Parsing and Encoding Our work is also related to AMR parsing (Flanigan et al., 2014; Konstas et al., 2017a; Lyu and Titov, 2018; Guo and Lu, 2018; Zhang et al., 2019; Cai and Lam, 2020) and AMR encoding (Konstas et al., 2017b; Song et al., 2018; Zhu et al., 2019; Song et al., 2020; Zhao et al., 2020; Bai et al., 2020). The former task makes it possible to use automatically-generated AMRs for downstream applications, while the latter helps to effectively exploit structural information in AMRs. In this work, we investigate AMRs for dialogue representation and combine AMRs with text for dialogue modeling. 9 Conclusion We investigated the feasibility of using AMRs for dialogue modeling, describing an algorithm to construct dialogue-level AMRs automatically and exploiting two ways to incorporate AMRs into neural dialogue systems. Experim"
2021.acl-long.342,D18-1548,0,0.0189623,"ues can be alleviated by having a structural representation of semantic information, which treats entities as nodes and builds structural relations between nodes, making it easy to find the most salient context. Explicit structures are also more interpretable compared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g"
2021.acl-long.342,D19-1569,0,0.150277,"y having a structural representation of semantic information, which treats entities as nodes and builds structural relations between nodes, making it easy to find the most salient context. Explicit structures are also more interpretable compared to 4430 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4430–4445 August 1–6, 2021. ©2021 Association for Computational Linguistics neural representation and have been shown useful for information extraction (Strubell et al., 2018; Sun et al., 2019; Li et al., 2020; Bai et al., 2021; Sachan et al., 2021), summarization (Liu et al., 2015; Hardy and Vlachos, 2018; Liao et al., 2018) and machine translation (Marcheggiani et al., 2018; Song et al., 2019a). We explore AMR (Banarescu et al., 2013) as a semantic representation for dialogue histories in order to better represent conversations. As shown in the central block of Figure 2, AMR is one type of sentential semantic representations, which models a sentence using a rooted directed acyclic graph, highlighting its main concepts (e.g. “mistake”) and semantic relations (e.g., “ARG0”1 ), whil"
2021.acl-long.342,2020.acl-main.444,0,0.304169,"… Transformer: Great. I’m looking forward to it. I just can’t keep away from the food that I saw. Figure 1: A conversation from DailyDialog. Some important contents are marked with squares. Introduction Dialogue systems have received increasing research attention (Wen et al., 2015; Serban et al., 2017; Bao et al., 2020), with much recent work focusing on social chats (Ritter et al., 2011; Li et al., 2017) and task-oriented dialogues (Wen et al., 2017; Dinan et al., 2019). There are two salient subtasks in dialogue modeling, namely dialogue understanding (Choi et al., 2018; Reddy et al., 2019; Yu et al., 2020) and response generation (Li et al., 2017; Budzianowski et al., 2018). The former refers to understanding of semantic and discourse details in a dialogue history, and the latter concerns making a fluent, novel and coherent utterance. The current state-of-the-art methods employ neural networks and end-to-end training (Sutskever et al., 2014; Bahdanau et al., 2015) for dialogue modeling. For instance, sequence-to-sequence models have been used to encode a dialogue history, before directly synthesizing the next utterance (Vinyals and Le, 2015; Wen et al., 2017; Bao et al., 2020). Despite giving s"
2021.acl-long.342,P19-1009,0,0.0481586,"Missing"
2021.acl-long.342,2020.acl-main.67,0,0.0181622,"enriching the standard AMR schema with dialogue acts, tense and aspect, and they construct a dataset consisting of 340 dialogue AMRs. However, they propose theoretical changes in the schema for annotating AMRs, while we explore empirical solutions that leverage existing AMRs of the standard schema on dialogues. AMR Parsing and Encoding Our work is also related to AMR parsing (Flanigan et al., 2014; Konstas et al., 2017a; Lyu and Titov, 2018; Guo and Lu, 2018; Zhang et al., 2019; Cai and Lam, 2020) and AMR encoding (Konstas et al., 2017b; Song et al., 2018; Zhu et al., 2019; Song et al., 2020; Zhao et al., 2020; Bai et al., 2020). The former task makes it possible to use automatically-generated AMRs for downstream applications, while the latter helps to effectively exploit structural information in AMRs. In this work, we investigate AMRs for dialogue representation and combine AMRs with text for dialogue modeling. 9 Conclusion We investigated the feasibility of using AMRs for dialogue modeling, describing an algorithm to construct dialogue-level AMRs automatically and exploiting two ways to incorporate AMRs into neural dialogue systems. Experiments on two benchmarks show advantages of using AMR sema"
2021.acl-long.342,D19-1548,0,0.278152,"uares) and multiple speaker interactions. To this end, we propose an algorithm to automatically derive dialogue-level AMRs from utterance-level AMRs, by adding cross-utterance links that indicate speakers, identical mentions and co-reference links. One example is shown in the right block of Figure 2, where newly added edges are in color. We consider two main approaches of making use of such dialogue-level AMR structures. For the first method, we merge an AMR with tokens in its corresponding sentence via AMR-to-text alignments, before encoding the resulting structure using a graph Transformer (Zhu et al., 2019). For the second method, we separately encode an AMR and its corresponding sentence, before leveraging both representations via feature fusion (Mangai et al., 2010) or dual attention (Calixto et al., 2017). We verify the effectiveness of the proposed framework on a dialogue relation extraction task (Yu et al., 2020) and a response generation task (Li et al., 2017). Experimental results show that the proposed framework outperforms previous methods (Vaswani et al., 2017; Bao et al., 2020; Yu et al., 2020), achieving the new state-of-the-art results on both benchmarks. Deep analysis and human eva"
2021.acl-long.368,2020.acl-main.676,0,0.038994,"nal complexity. In this work, we focus on how the NMT model generalizes to complex compositions in a controllable setting and minimize the effects of the other factors. Compositional Generalization. Neural networks have been shown sample-inefficient, requiring large-scale training data, which suggests that they may lack compositionality (Lake and Baroni, 2018). Lake and Baroni (2018) introduce the SCAN dataset to help study compositional generalization of neural networks, which has received increasing interests (Russin et al., 2019b; 4768 Dess`ı and Baroni, 2019; Li et al., 2019c; Lake, 2019; Andreas, 2020; Gordon et al., 2020). Various benchmarks have been proposed including in the area of visual reasoning (Johnson et al., 2017b; Hudson and Manning, 2019), mathematics (Saxton et al., 2019), and semantic parsing (CFQ) (Keysers et al., 2020). However, no benchmark has been dedicated to machine translation in practice. We fill this gap by introducing a dataset with 216,000 instances and an average sentence length of 9.7 tokens. 3 Problem Definition Following Keysers et al. (2020), compositional generalization is defined as the capacity to systematically generalize to novel combinations of compone"
2021.acl-long.368,W18-5407,0,0.0266043,"arge parallel data, where new test sentences can be sparse. Disregarding out-of-vocabulary words, a main cause of sparsity is semantic composition: given a limited vocabulary, the number of possible compositions grows exponentially with respect to the composite length. The ability to understand and produce a potentially infinite number of novel combinations of known components, namely compositional generalization (Chomsky; Montague; Lake and Baroni, 2018; Keysers et al., 2020), has been demonstrated deficient in many machine learning (ML) methods (Johnson et al., 2017a; Lake and Baroni, 2018; Bastings et al., 2018; Loula et al., 2018; Russin et al., 2019a). In this paper, we study compositional generalization in the context of machine translation. For example, if “red cars” and “blue balls” are seen in training, a competent algorithm is expected to translate “red balls” correctly, even if the phrase has not been seen in training data. Intuitively, the challenge increases as the composite length grows. Recently, several studies have taken steps towards this specific problem. They either use a few dedicated samples (i.e., 8 test sentences) for evaluation (Lake and Baroni, 2018; Li et al., 2019b; Chen et"
2021.acl-long.368,I17-1001,0,0.0510864,"Missing"
2021.acl-long.368,D19-5617,0,0.0155627,"score in similarity-based metrics such as BLEU, demonstrating that fatal translation errors can be overlooked under traditional evaluation metrics. 4 Dataset Figure 1 gives an overview of our data construction process. We first source monolingual data (Section 4.1), and then build parallel data based by translation (Section 4.2). Then we synthesize a test set of novel compounds (Section 4.3), and offer an automatic evaluation method (Section 4.4). 4.1 Monolingual Data Source Our goal is to focus on compositional generalization and minimize the influence of additional factors such as polysemy (Berard et al., 2019), misalignment (Munteanu and Marcu, 2005), and stylistic problems (Hovy et al., 2020). The dataset should ideally have following characteristics. First, the vocabulary size should be small and contain only words of high-frequency in order to avoid problems caused by rare words. In other words, variety of composition should come from combining different frequent words instead of word diversity, as suggested in (Keysers et al., 2020). Metaphorical words, which can increase the translation difficulty, should be excluded. Second, source sentences should not be too long or have complex syntactic st"
2021.acl-long.368,D18-1313,0,0.0196074,"traditional evaluation metrics such as BLEU. In addition, we observe that various factors exert salient effects on model’s ability of compositional generalization, such as compound frequency, compound length, atom co-occurrence, linguistic patterns, and context complexity. The CoGnition dataset along with the automatic evaluation tool are realesed on https://github.com/yafuly/CoGnition. 2 Related Work Analysis of NMT. Our work is related to research analyzing NMT from various perspectives. There has been much linguistic analysis of NMT representations (Shi et al., 2016; Belinkov et al., 2017; Bisazza and Tump, 2018), interpretability (Ding et al., 2017; He et al., 2019; Voita et al., 2019a), and attention weights (Voita et al., 2019b; Michel et al., 2019). Robustness is also an important research direction. Work has shown that NMT models are prone to be negatively affected by both synthetic and natural noise (Belinkov and Bisk, 2018b; Cheng et al., 2018; Ebrahimi et al., 2018). For better exploration of robust NMT, Michel and Neubig (2018) propose an MTNT dataset containing several types of noise. Wang et al. (2020) provide in-depth analyses of inference miscalibration of NMT resulting from the discrepan"
2021.acl-long.368,P19-1425,0,0.0181937,"slation samples obtained from one popular web translation engine on January 19, 2021. Introduction Neural machine translation (NMT) has shown competitive performance on benchmark datasets such as IWSLT and WMT (Vaswani et al., 2017; Edunov et al., 2018a; Liu et al., 2020a), and even achieves parity with professional human translation under certain evaluation settings (Hassan et al., 2018). However, the performance can be relatively low in out-of-domain and low-resource conditions. In addition, NMT systems show poor robustness and vulnerability to input perturbations (Belinkov and Bisk, 2018a; Cheng et al., 2019). One example is shown in Table 1, where simple substitution of a word yields translation with completely different semantics. Many of these issues origin from the fact that NMT models are trained end-to-end over large parallel data, where new test sentences can be sparse. Disregarding out-of-vocabulary words, a main cause of sparsity is semantic composition: given a limited vocabulary, the number of possible compositions grows exponentially with respect to the composite length. The ability to understand and produce a potentially infinite number of novel combinations of known components, namel"
2021.acl-long.368,P18-1163,0,0.0159419,"on https://github.com/yafuly/CoGnition. 2 Related Work Analysis of NMT. Our work is related to research analyzing NMT from various perspectives. There has been much linguistic analysis of NMT representations (Shi et al., 2016; Belinkov et al., 2017; Bisazza and Tump, 2018), interpretability (Ding et al., 2017; He et al., 2019; Voita et al., 2019a), and attention weights (Voita et al., 2019b; Michel et al., 2019). Robustness is also an important research direction. Work has shown that NMT models are prone to be negatively affected by both synthetic and natural noise (Belinkov and Bisk, 2018b; Cheng et al., 2018; Ebrahimi et al., 2018). For better exploration of robust NMT, Michel and Neubig (2018) propose an MTNT dataset containing several types of noise. Wang et al. (2020) provide in-depth analyses of inference miscalibration of NMT resulting from the discrepancy between training and inference. Our work is in line but we discuss robustness from the perspective of compositional generalization. In this respect, Lake and Baroni (2018) propose a simple experiment to analyze compositionality in MT, followed by Chen et al. (2020) and Li et al. (2019b). Specifically, they introduce a novel word “dax”, and"
2021.acl-long.368,P19-1090,0,0.0162823,"ng compounds, sorted by frequency in the training set. Split Training set Validation set Random test set CG test set # Samples 196,246 10,000 10,000 10,800 5.2 Table 5: Statistics of CoGnition Dataset. jieba segmenter3 . We employ BPE with 3,000 merge operations, generating a vocabulary of 5,500 subwords. We focus on Transformer (Vaswani et al., 2017) because of its state-of-the-art performance on machine translation (Edunov et al., 2018b; Takase and Kiyono, 2021; Raffel et al., 2020; Zhu et al., 2020; Liu et al., 2020b) and better performance on existing compositional generalization dataset (Daniel et al., 2019). We implement our model using BASE configuration provided by Fairseq (Ott et al., 2019). The model consists of a 6-layer encoder and a 6-layer decoder with the hidden size 512. We tie input and output embeddings on the target side. The model parameters are optimized by Adam (Kingma and Ba, 2015), with 1 = 0.1, 9 2 = 0.98 and ✏ = 10 . The model is trained for 100,000 steps and we choose the best checkpoint on validation set for evaluation. We report character-level BLEU scores using SacreBLEU (Post, 2018) to measure the overall translation performance. In addition, we request expert translator"
2021.acl-long.368,P19-1381,0,0.046099,"Missing"
2021.acl-long.368,P17-1106,0,0.0122784,". In addition, we observe that various factors exert salient effects on model’s ability of compositional generalization, such as compound frequency, compound length, atom co-occurrence, linguistic patterns, and context complexity. The CoGnition dataset along with the automatic evaluation tool are realesed on https://github.com/yafuly/CoGnition. 2 Related Work Analysis of NMT. Our work is related to research analyzing NMT from various perspectives. There has been much linguistic analysis of NMT representations (Shi et al., 2016; Belinkov et al., 2017; Bisazza and Tump, 2018), interpretability (Ding et al., 2017; He et al., 2019; Voita et al., 2019a), and attention weights (Voita et al., 2019b; Michel et al., 2019). Robustness is also an important research direction. Work has shown that NMT models are prone to be negatively affected by both synthetic and natural noise (Belinkov and Bisk, 2018b; Cheng et al., 2018; Ebrahimi et al., 2018). For better exploration of robust NMT, Michel and Neubig (2018) propose an MTNT dataset containing several types of noise. Wang et al. (2020) provide in-depth analyses of inference miscalibration of NMT resulting from the discrepancy between training and inference. Ou"
2021.acl-long.368,C18-1055,0,0.0222158,"om/yafuly/CoGnition. 2 Related Work Analysis of NMT. Our work is related to research analyzing NMT from various perspectives. There has been much linguistic analysis of NMT representations (Shi et al., 2016; Belinkov et al., 2017; Bisazza and Tump, 2018), interpretability (Ding et al., 2017; He et al., 2019; Voita et al., 2019a), and attention weights (Voita et al., 2019b; Michel et al., 2019). Robustness is also an important research direction. Work has shown that NMT models are prone to be negatively affected by both synthetic and natural noise (Belinkov and Bisk, 2018b; Cheng et al., 2018; Ebrahimi et al., 2018). For better exploration of robust NMT, Michel and Neubig (2018) propose an MTNT dataset containing several types of noise. Wang et al. (2020) provide in-depth analyses of inference miscalibration of NMT resulting from the discrepancy between training and inference. Our work is in line but we discuss robustness from the perspective of compositional generalization. In this respect, Lake and Baroni (2018) propose a simple experiment to analyze compositionality in MT, followed by Chen et al. (2020) and Li et al. (2019b). Specifically, they introduce a novel word “dax”, and their training data con"
2021.acl-long.368,D18-1045,0,0.294252,"nce pairs. We quantitatively analyze effects of various factors using compound translation error rate, then demonstrate that the NMT model fails badly on compositional generalization, although it performs remarkably well under traditional metrics. 1 James breaks his promise Translation “·à˙ (Taylor keeps his promise) y∆Ø›Õ˙ (James breaks his promise) Table 1: Translation samples obtained from one popular web translation engine on January 19, 2021. Introduction Neural machine translation (NMT) has shown competitive performance on benchmark datasets such as IWSLT and WMT (Vaswani et al., 2017; Edunov et al., 2018a; Liu et al., 2020a), and even achieves parity with professional human translation under certain evaluation settings (Hassan et al., 2018). However, the performance can be relatively low in out-of-domain and low-resource conditions. In addition, NMT systems show poor robustness and vulnerability to input perturbations (Belinkov and Bisk, 2018a; Cheng et al., 2019). One example is shown in Table 1, where simple substitution of a word yields translation with completely different semantics. Many of these issues origin from the fact that NMT models are trained end-to-end over large parallel data,"
2021.acl-long.368,D19-1088,0,0.0180804,"bserve that various factors exert salient effects on model’s ability of compositional generalization, such as compound frequency, compound length, atom co-occurrence, linguistic patterns, and context complexity. The CoGnition dataset along with the automatic evaluation tool are realesed on https://github.com/yafuly/CoGnition. 2 Related Work Analysis of NMT. Our work is related to research analyzing NMT from various perspectives. There has been much linguistic analysis of NMT representations (Shi et al., 2016; Belinkov et al., 2017; Bisazza and Tump, 2018), interpretability (Ding et al., 2017; He et al., 2019; Voita et al., 2019a), and attention weights (Voita et al., 2019b; Michel et al., 2019). Robustness is also an important research direction. Work has shown that NMT models are prone to be negatively affected by both synthetic and natural noise (Belinkov and Bisk, 2018b; Cheng et al., 2018; Ebrahimi et al., 2018). For better exploration of robust NMT, Michel and Neubig (2018) propose an MTNT dataset containing several types of noise. Wang et al. (2020) provide in-depth analyses of inference miscalibration of NMT resulting from the discrepancy between training and inference. Our work is in line"
2021.acl-long.368,P18-1249,0,0.0315851,"Missing"
2021.acl-long.368,W19-5325,0,0.0489042,"Missing"
2021.acl-long.368,D19-1438,0,0.0888128,"018; Bastings et al., 2018; Loula et al., 2018; Russin et al., 2019a). In this paper, we study compositional generalization in the context of machine translation. For example, if “red cars” and “blue balls” are seen in training, a competent algorithm is expected to translate “red balls” correctly, even if the phrase has not been seen in training data. Intuitively, the challenge increases as the composite length grows. Recently, several studies have taken steps towards this specific problem. They either use a few dedicated samples (i.e., 8 test sentences) for evaluation (Lake and Baroni, 2018; Li et al., 2019b; Chen et al., 2020), or make simple modifications in sampled source sentences such as removing or adding adverbs, and concatenating two sentences (Raunak et al., 2019; Fadaee and Monz, 2020a). Such experimental data is limited in size, scope and specificity, and the forms of composition are coarse-grained and non-systematic. As a result, no qualitative conclusions have been drawn on the prevalence and characteristics of this problem in modern NMT. We make a first large-scale general domain investigation, constructing the CoGnition dataset (Compositional Generalization Machine Translation Dat"
2021.acl-long.368,W18-5413,0,0.0616385,"Missing"
2021.acl-long.368,D18-1050,0,0.018669,"is related to research analyzing NMT from various perspectives. There has been much linguistic analysis of NMT representations (Shi et al., 2016; Belinkov et al., 2017; Bisazza and Tump, 2018), interpretability (Ding et al., 2017; He et al., 2019; Voita et al., 2019a), and attention weights (Voita et al., 2019b; Michel et al., 2019). Robustness is also an important research direction. Work has shown that NMT models are prone to be negatively affected by both synthetic and natural noise (Belinkov and Bisk, 2018b; Cheng et al., 2018; Ebrahimi et al., 2018). For better exploration of robust NMT, Michel and Neubig (2018) propose an MTNT dataset containing several types of noise. Wang et al. (2020) provide in-depth analyses of inference miscalibration of NMT resulting from the discrepancy between training and inference. Our work is in line but we discuss robustness from the perspective of compositional generalization. In this respect, Lake and Baroni (2018) propose a simple experiment to analyze compositionality in MT, followed by Chen et al. (2020) and Li et al. (2019b). Specifically, they introduce a novel word “dax”, and their training data contains a single pattern of sentence pairs (e.g. “I am daxy”, “je"
2021.acl-long.368,N16-1098,0,0.205238,"forgot about the chair on the floor ! he jumped from the bench towards the large airplane on the floor . Table 3: Compound patterns in the CG test set. Compounds are in bold and shown in sentence context. translated literally, directly, and without rhetoric. Third, the corpus size should be large enough for training an NMT model sufficiently. Widely-adopted corpora such as parallel data released on WMT and IWSLT2 have large vocabularies and also contain noisy sentences and rich morphology (Li et al., 2019a), which do not fully meet our goal. We choose Story Cloze Test and ROCStories Corpora (Mostafazadeh et al., 2016, 2017) as our data source. The dataset is created for commonsense story understanding and generation, and consists of 101903 5-sentence stories. These stories are rather simple in items of vocabulary and syntax, but still contain rich phrases. In addition, the topic is constrained to daily life. Since the vocabulary size of 42, 458 is large, we select the top 2, 000 frequent words as our vocabulary and extract sentences where the words are exclusively from the restricted vocab. Moreover, sentences that are longer than 20 words are removed. In this way, we finally obtain 216, 246 2 https://wit"
2021.acl-long.368,W17-0906,0,0.0460684,"Missing"
2021.acl-long.368,J05-4003,0,0.201497,"ch as BLEU, demonstrating that fatal translation errors can be overlooked under traditional evaluation metrics. 4 Dataset Figure 1 gives an overview of our data construction process. We first source monolingual data (Section 4.1), and then build parallel data based by translation (Section 4.2). Then we synthesize a test set of novel compounds (Section 4.3), and offer an automatic evaluation method (Section 4.4). 4.1 Monolingual Data Source Our goal is to focus on compositional generalization and minimize the influence of additional factors such as polysemy (Berard et al., 2019), misalignment (Munteanu and Marcu, 2005), and stylistic problems (Hovy et al., 2020). The dataset should ideally have following characteristics. First, the vocabulary size should be small and contain only words of high-frequency in order to avoid problems caused by rare words. In other words, variety of composition should come from combining different frequent words instead of word diversity, as suggested in (Keysers et al., 2020). Metaphorical words, which can increase the translation difficulty, should be excluded. Second, source sentences should not be too long or have complex syntactic structures. As a result, a sentence can be"
2021.acl-long.368,N19-4009,0,0.0203698,"ndom test set CG test set # Samples 196,246 10,000 10,000 10,800 5.2 Table 5: Statistics of CoGnition Dataset. jieba segmenter3 . We employ BPE with 3,000 merge operations, generating a vocabulary of 5,500 subwords. We focus on Transformer (Vaswani et al., 2017) because of its state-of-the-art performance on machine translation (Edunov et al., 2018b; Takase and Kiyono, 2021; Raffel et al., 2020; Zhu et al., 2020; Liu et al., 2020b) and better performance on existing compositional generalization dataset (Daniel et al., 2019). We implement our model using BASE configuration provided by Fairseq (Ott et al., 2019). The model consists of a 6-layer encoder and a 6-layer decoder with the hidden size 512. We tie input and output embeddings on the target side. The model parameters are optimized by Adam (Kingma and Ba, 2015), with 1 = 0.1, 9 2 = 0.98 and ✏ = 10 . The model is trained for 100,000 steps and we choose the best checkpoint on validation set for evaluation. We report character-level BLEU scores using SacreBLEU (Post, 2018) to measure the overall translation performance. In addition, we request expert translators to annotate the correctness of compound translation. Translators are asked to only foc"
2021.acl-long.368,P02-1040,0,0.11022,"≈Ü Table 2: Examples of SCAN, CFQ, and our CoGnition datasets. lel dataset in English-Chinese, along with a synthetic test set to quantify and analyze the compositional generalization of NMT models. In particular, we define frequent syntactic constituents as compounds, and basic semantic components in constituents as atoms. In addition to the standard training, validation and test sets, the CoGnition dataset contains a compositional generalization test set, which contains novel compounds in each sentence, so that both the generalization error rate can be evaluated, and its influence on BLEU (Papineni et al., 2002) can be quantified. Our compositional generalization test set consists of 2,160 novel compounds, with up to 5 atoms and 7 words. In this way, generalization ability can be evaluated based on compound translation error rate. Empirical results show that the dominant Transformer (Vaswani et al., 2017) NMT model faces challenges in translating novel compounds, despite its competitive performance under traditional evaluation metrics such as BLEU. In addition, we observe that various factors exert salient effects on model’s ability of compositional generalization, such as compound frequency, compoun"
2021.acl-long.368,W18-6319,0,0.0161168,"2020b) and better performance on existing compositional generalization dataset (Daniel et al., 2019). We implement our model using BASE configuration provided by Fairseq (Ott et al., 2019). The model consists of a 6-layer encoder and a 6-layer decoder with the hidden size 512. We tie input and output embeddings on the target side. The model parameters are optimized by Adam (Kingma and Ba, 2015), with 1 = 0.1, 9 2 = 0.98 and ✏ = 10 . The model is trained for 100,000 steps and we choose the best checkpoint on validation set for evaluation. We report character-level BLEU scores using SacreBLEU (Post, 2018) to measure the overall translation performance. In addition, we request expert translators to annotate the correctness of compound translation. Translators are asked to only focus on examining whether the compound itself is translated correctly or not, disregarding errors in context. Specifically, a compound is correct only if its translation contains semantic meaning of all atoms and is fluent in human language. Since each of the 2,160 compounds is provided with 5 contexts, we can compute the translation error-rate for each compound. 3 https://github.com/fxsjy/jieba Main Results Table 6 show"
2021.acl-long.368,2020.acl-demos.14,0,0.0589628,"Missing"
2021.acl-long.368,P16-1162,0,0.0966581,"Missing"
2021.acl-long.368,D16-1159,0,0.0195351,"espite its competitive performance under traditional evaluation metrics such as BLEU. In addition, we observe that various factors exert salient effects on model’s ability of compositional generalization, such as compound frequency, compound length, atom co-occurrence, linguistic patterns, and context complexity. The CoGnition dataset along with the automatic evaluation tool are realesed on https://github.com/yafuly/CoGnition. 2 Related Work Analysis of NMT. Our work is related to research analyzing NMT from various perspectives. There has been much linguistic analysis of NMT representations (Shi et al., 2016; Belinkov et al., 2017; Bisazza and Tump, 2018), interpretability (Ding et al., 2017; He et al., 2019; Voita et al., 2019a), and attention weights (Voita et al., 2019b; Michel et al., 2019). Robustness is also an important research direction. Work has shown that NMT models are prone to be negatively affected by both synthetic and natural noise (Belinkov and Bisk, 2018b; Cheng et al., 2018; Ebrahimi et al., 2018). For better exploration of robust NMT, Michel and Neubig (2018) propose an MTNT dataset containing several types of noise. Wang et al. (2020) provide in-depth analyses of inference mi"
2021.acl-long.368,D19-1448,0,0.0113387,"us factors exert salient effects on model’s ability of compositional generalization, such as compound frequency, compound length, atom co-occurrence, linguistic patterns, and context complexity. The CoGnition dataset along with the automatic evaluation tool are realesed on https://github.com/yafuly/CoGnition. 2 Related Work Analysis of NMT. Our work is related to research analyzing NMT from various perspectives. There has been much linguistic analysis of NMT representations (Shi et al., 2016; Belinkov et al., 2017; Bisazza and Tump, 2018), interpretability (Ding et al., 2017; He et al., 2019; Voita et al., 2019a), and attention weights (Voita et al., 2019b; Michel et al., 2019). Robustness is also an important research direction. Work has shown that NMT models are prone to be negatively affected by both synthetic and natural noise (Belinkov and Bisk, 2018b; Cheng et al., 2018; Ebrahimi et al., 2018). For better exploration of robust NMT, Michel and Neubig (2018) propose an MTNT dataset containing several types of noise. Wang et al. (2020) provide in-depth analyses of inference miscalibration of NMT resulting from the discrepancy between training and inference. Our work is in line but we discuss robu"
2021.acl-long.368,P19-1580,0,0.0217457,"us factors exert salient effects on model’s ability of compositional generalization, such as compound frequency, compound length, atom co-occurrence, linguistic patterns, and context complexity. The CoGnition dataset along with the automatic evaluation tool are realesed on https://github.com/yafuly/CoGnition. 2 Related Work Analysis of NMT. Our work is related to research analyzing NMT from various perspectives. There has been much linguistic analysis of NMT representations (Shi et al., 2016; Belinkov et al., 2017; Bisazza and Tump, 2018), interpretability (Ding et al., 2017; He et al., 2019; Voita et al., 2019a), and attention weights (Voita et al., 2019b; Michel et al., 2019). Robustness is also an important research direction. Work has shown that NMT models are prone to be negatively affected by both synthetic and natural noise (Belinkov and Bisk, 2018b; Cheng et al., 2018; Ebrahimi et al., 2018). For better exploration of robust NMT, Michel and Neubig (2018) propose an MTNT dataset containing several types of noise. Wang et al. (2020) provide in-depth analyses of inference miscalibration of NMT resulting from the discrepancy between training and inference. Our work is in line but we discuss robu"
2021.acl-long.368,2020.acl-main.278,0,0.0197005,"linguistic analysis of NMT representations (Shi et al., 2016; Belinkov et al., 2017; Bisazza and Tump, 2018), interpretability (Ding et al., 2017; He et al., 2019; Voita et al., 2019a), and attention weights (Voita et al., 2019b; Michel et al., 2019). Robustness is also an important research direction. Work has shown that NMT models are prone to be negatively affected by both synthetic and natural noise (Belinkov and Bisk, 2018b; Cheng et al., 2018; Ebrahimi et al., 2018). For better exploration of robust NMT, Michel and Neubig (2018) propose an MTNT dataset containing several types of noise. Wang et al. (2020) provide in-depth analyses of inference miscalibration of NMT resulting from the discrepancy between training and inference. Our work is in line but we discuss robustness from the perspective of compositional generalization. In this respect, Lake and Baroni (2018) propose a simple experiment to analyze compositionality in MT, followed by Chen et al. (2020) and Li et al. (2019b). Specifically, they introduce a novel word “dax”, and their training data contains a single pattern of sentence pairs (e.g. “I am daxy”, “je suis daxiste”) while the test set contains different patterns. However, their"
2021.acl-long.454,D19-1165,0,0.12384,"ssociation for Computational Linguistics the combination of lexicon features and BERT for Chinese NER (Ma et al., 2020; Li et al., 2020), Chinese Word Segmentation (Gan and Zhang, 2020), and Chinese POS tagging (Tian et al., 2020b). The main idea is to integrate contextual representations from BERT and lexicon features into a neural sequence labeling model (shown in Figure 1 (a)). However, these approaches do not fully exploit the representation power of BERT, because the external features are not integrated into the bottom level. Inspired by the work about BERT Adapter (Houlsby et al., 2019; Bapna and Firat, 2019; Wang et al., 2020), we propose Lexicon Enhanced BERT (LEBERT) to integrate lexicon information between Transformer layers of BERT directly. Specifically, a Chinese sentence is converted into a charwords pair sequence by matching the sentence with an existing lexicon. A lexicon adapter is designed to dynamically extract the most relevant matched words for each character using a char-to-word bilinear attention mechanism. The lexicon adapter is applied between adjacent transformers in BERT (shown in Figure 1 (b)) so that lexicon features and BERT representation interact sufficiently through the"
2021.acl-long.454,D18-1017,0,0.0150718,"equence labeling have been achieved by neural network approaches (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Gui et al., 2017). Chinese sequence labeling is more challenging due to the lack of explicit word boundaries in Chinese sentences. One way of performing Chinese sequence labeling is to perform Chinese word segmentation (CWS) first, before applying word sequence labeling (Sun and Uszkoreit, 2012; Yang et al., 2016). However, it can suffer from the segmentation errors propagated from the CWS system (Zhang and Yang, 2018; Liu et al., 2019). Therefore, some approaches (Cao et al., 2018; Shen et al., 2016) perform Chinese sequence labeling directly at the character level, which has been empirically proven to be more effective (Ng and Low, 2004; Liu et al., 2010; Zhang and Yang, 2018). There are two lines of recent work enhancing character-based neural Chinese sequence labeling. The first considers integrating word information into a character-based sequence encoder, so that word features can be explicitly modeled (Zhang and Yang, 2018; Yang et al., 2019; Liu et al., 2019; Ding et al., 2019; Higashiyama et al., 2019). These methods can be treated as designing different varian"
2021.acl-long.454,Q16-1026,0,0.0336854,"d truncate the sentence to three characters. ci denotes the i-th Chinese character, wj denotes the j-th Chinese word. Introduction Sequence labeling is a classic task in natural language processing (NLP), which is to assign a label to each unit in a sequence (Jurafsky and Martin, 2009). Many important language processing tasks can be converted into this problem, such as partof-speech (POS) tagging, named entity recognition (NER), and text chunking. The current state-of-theart results for sequence labeling have been achieved by neural network approaches (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Gui et al., 2017). Chinese sequence labeling is more challenging due to the lack of explicit word boundaries in Chinese sentences. One way of performing Chinese sequence labeling is to perform Chinese word segmentation (CWS) first, before applying word sequence labeling (Sun and Uszkoreit, 2012; Yang et al., 2016). However, it can suffer from the segmentation errors propagated from the CWS system (Zhang and Yang, 2018; Liu et al., 2019). Therefore, some approaches (Cao et al., 2018; Shen et al., 2016) perform Chinese sequence labeling directly at the character level, which has been empirical"
2021.acl-long.454,N19-1423,0,0.497686,"Zhang and Yang, 2018). There are two lines of recent work enhancing character-based neural Chinese sequence labeling. The first considers integrating word information into a character-based sequence encoder, so that word features can be explicitly modeled (Zhang and Yang, 2018; Yang et al., 2019; Liu et al., 2019; Ding et al., 2019; Higashiyama et al., 2019). These methods can be treated as designing different variants to neural architectures for integrating discrete structured knowledge. The second considers the integration of large-scale pre-trained contextualized embeddings, such as BERT (Devlin et al., 2019), which has been shown to capture implicit wordlevel syntactic and semantic knowledge (Goldberg, 2019; Hewitt and Manning, 2019). The two lines of work are complementary to each other due to the different nature of discrete and neural representations. Recent work considers 5847 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5847–5858 August 1–6, 2021. ©2021 Association for Computational Linguistics the combination of lexicon features and BERT for Chinese NER (Ma et al., 20"
2021.acl-long.454,2020.findings-emnlp.425,0,0.137207,"et al., 2020b). Our method is in line with the above approaches trying to combine lexicon information and BERT. The difference is that we integrate lexicon into the bottom level, allowing in-depth knowledge interaction within BERT. There is also work employing lexicon to guide pre-training. ERNIE (Sun et al., 2019a,b) exploited entity-level and word-level masking to integrate knowledge into BERT in an implicit way. Jia et al. (2020) proposed Entity Enhanced BERT, further pre-training BERT using a domainspecific corpus and entity set with a carefully designed character-entity Transformer. ZEN (Diao et al., 2020) enhanced Chinese BERT with a multi5848 layered N-gram encoder but is limited by the small size of the N-gram vocabulary. Compared to the above pre-training methods, our model integrates lexicon information into BERT using an adapter, which is more efficient and requires no raw texts or entity set. BERT Adapter. BERT Adapter (Houlsby et al., 2019) aims to learn task-specific parameters for the downstream tasks. Specifically, they add adapters between layers of a pre-trained model and tune only the parameters in the added adapters for a certain task. Bapna and Firat (2019) injected taskspecific"
2021.acl-long.454,P19-1141,0,0.228502,"om the CWS system (Zhang and Yang, 2018; Liu et al., 2019). Therefore, some approaches (Cao et al., 2018; Shen et al., 2016) perform Chinese sequence labeling directly at the character level, which has been empirically proven to be more effective (Ng and Low, 2004; Liu et al., 2010; Zhang and Yang, 2018). There are two lines of recent work enhancing character-based neural Chinese sequence labeling. The first considers integrating word information into a character-based sequence encoder, so that word features can be explicitly modeled (Zhang and Yang, 2018; Yang et al., 2019; Liu et al., 2019; Ding et al., 2019; Higashiyama et al., 2019). These methods can be treated as designing different variants to neural architectures for integrating discrete structured knowledge. The second considers the integration of large-scale pre-trained contextualized embeddings, such as BERT (Devlin et al., 2019), which has been shown to capture implicit wordlevel syntactic and semantic knowledge (Goldberg, 2019; Hewitt and Manning, 2019). The two lines of work are complementary to each other due to the different nature of discrete and neural representations. Recent work considers 5847 Proceedings of the 59th Annual Meet"
2021.acl-long.454,I05-3017,0,0.0348223,"chmark datasets, including Weibo NER (Peng and Dredze, 2015, 2016), OntoNotes (Weischedel et al., 2011), Resume NER (Zhang and Yang, 2018), and MSRA (Levow, 2006). Weibo NER is a social media domain dataset, which is drawn from Sina Weibo; while OntoNotes and MSRA datasets are in the news domain. Resume NER 5851 dataset consists of resumes of senior executives, which is annotated by Zhang and Yang (2018). Chinese Word Segmentation. For Chinese word segmentation, we employ three benchmark datasets in our experiments, namely PKU, MSR, and CTB6, where the former two are from SIGHAN 2005 Bakeoff (Emerson, 2005) and the last one is from Xue et al. (2005). For MSR and PKU, we follow their official training/test data split. For CTB6, we use the same split as that stated in Yang and Xue (2012); Higashiyama et al. (2019). Chinese POS Tagging. For POS-tagging, three Chinese benchmark datasets are used, including CTB5 and CTB6 from the Penn Chinese TreeBank (Xue et al., 2005) and the Chinese GSD Treebank of Universal Dependencies(UD) (Nivre et al., 2016). The CTB datasets are in simplified Chinese while the UD dataset is in traditional Chinese. Following Shao et al. (2017), we first convert the UD dataset"
2021.acl-long.454,D17-1256,0,0.0256961,"to three characters. ci denotes the i-th Chinese character, wj denotes the j-th Chinese word. Introduction Sequence labeling is a classic task in natural language processing (NLP), which is to assign a label to each unit in a sequence (Jurafsky and Martin, 2009). Many important language processing tasks can be converted into this problem, such as partof-speech (POS) tagging, named entity recognition (NER), and text chunking. The current state-of-theart results for sequence labeling have been achieved by neural network approaches (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Gui et al., 2017). Chinese sequence labeling is more challenging due to the lack of explicit word boundaries in Chinese sentences. One way of performing Chinese sequence labeling is to perform Chinese word segmentation (CWS) first, before applying word sequence labeling (Sun and Uszkoreit, 2012; Yang et al., 2016). However, it can suffer from the segmentation errors propagated from the CWS system (Zhang and Yang, 2018; Liu et al., 2019). Therefore, some approaches (Cao et al., 2018; Shen et al., 2016) perform Chinese sequence labeling directly at the character level, which has been empirically proven to be mor"
2021.acl-long.454,D19-1096,0,0.0374854,"Missing"
2021.acl-long.454,N19-1419,0,0.0254488,"first considers integrating word information into a character-based sequence encoder, so that word features can be explicitly modeled (Zhang and Yang, 2018; Yang et al., 2019; Liu et al., 2019; Ding et al., 2019; Higashiyama et al., 2019). These methods can be treated as designing different variants to neural architectures for integrating discrete structured knowledge. The second considers the integration of large-scale pre-trained contextualized embeddings, such as BERT (Devlin et al., 2019), which has been shown to capture implicit wordlevel syntactic and semantic knowledge (Goldberg, 2019; Hewitt and Manning, 2019). The two lines of work are complementary to each other due to the different nature of discrete and neural representations. Recent work considers 5847 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5847–5858 August 1–6, 2021. ©2021 Association for Computational Linguistics the combination of lexicon features and BERT for Chinese NER (Ma et al., 2020; Li et al., 2020), Chinese Word Segmentation (Gan and Zhang, 2020), and Chinese POS tagging (Tian et al., 2020b). The main id"
2021.acl-long.454,N19-1276,0,0.0803328,"Zhang and Yang, 2018; Liu et al., 2019). Therefore, some approaches (Cao et al., 2018; Shen et al., 2016) perform Chinese sequence labeling directly at the character level, which has been empirically proven to be more effective (Ng and Low, 2004; Liu et al., 2010; Zhang and Yang, 2018). There are two lines of recent work enhancing character-based neural Chinese sequence labeling. The first considers integrating word information into a character-based sequence encoder, so that word features can be explicitly modeled (Zhang and Yang, 2018; Yang et al., 2019; Liu et al., 2019; Ding et al., 2019; Higashiyama et al., 2019). These methods can be treated as designing different variants to neural architectures for integrating discrete structured knowledge. The second considers the integration of large-scale pre-trained contextualized embeddings, such as BERT (Devlin et al., 2019), which has been shown to capture implicit wordlevel syntactic and semantic knowledge (Goldberg, 2019; Hewitt and Manning, 2019). The two lines of work are complementary to each other due to the different nature of discrete and neural representations. Recent work considers 5847 Proceedings of the 59th Annual Meeting of the Association for"
2021.acl-long.454,2020.coling-main.54,0,0.025016,"on (CWS) and Part-ofspeech (POS) tagging. Yang et al. (2019) applied a lattice LSTM for CWS, showing good performance. Zhao et al. (2020) improved the results of CWS with lexicon-enhanced adaptive attention. Tian et al. (2020b) enhanced the character-based Chinese POS tagging model with a multi-channel attention of N-grams. Pre-trained Model-based. Transformer-based pre-trained models, such as BERT (Devlin et al., 2019), have shown excellent performance for Chinese sequence labeling. Yang (2019) simply added a softmax on BERT, achieving state-of-the-art performance on CWS. Meng et al. (2019); Hu and Verberne (2020) showed that models using the character features from BERT outperform the static embedding-based approaches by a large margin for Chinese NER and Chinese POS tagging. Hybrid Model. Recent work tries to integrate the lexicon and pre-trained models by utilizing their respective strengths. Ma et al. (2020) concatenated separate features, BERT representation and lexicon information, and input them into a shallow fusion layer (LSTM) for Chinese NER. Li et al. (2020) proposed a shallow Flat-Lattice Transformer to handle the character-word graph, in which the fusion is still at model-level. Similarly"
2021.acl-long.454,2020.emnlp-main.518,1,0.619217,"aracter-word graph, in which the fusion is still at model-level. Similarly, character N-gram features and BERT vectors are concatenated for joint training CWS and POS tagging (Tian et al., 2020b). Our method is in line with the above approaches trying to combine lexicon information and BERT. The difference is that we integrate lexicon into the bottom level, allowing in-depth knowledge interaction within BERT. There is also work employing lexicon to guide pre-training. ERNIE (Sun et al., 2019a,b) exploited entity-level and word-level masking to integrate knowledge into BERT in an implicit way. Jia et al. (2020) proposed Entity Enhanced BERT, further pre-training BERT using a domainspecific corpus and entity set with a carefully designed character-entity Transformer. ZEN (Diao et al., 2020) enhanced Chinese BERT with a multi5848 layered N-gram encoder but is limited by the small size of the N-gram vocabulary. Compared to the above pre-training methods, our model integrates lexicon information into BERT using an adapter, which is more efficient and requires no raw texts or entity set. BERT Adapter. BERT Adapter (Houlsby et al., 2019) aims to learn task-specific parameters for the downstream tasks. Spe"
2021.acl-long.454,N16-1030,0,0.0371907,"y show two Transformer layers in BERT and truncate the sentence to three characters. ci denotes the i-th Chinese character, wj denotes the j-th Chinese word. Introduction Sequence labeling is a classic task in natural language processing (NLP), which is to assign a label to each unit in a sequence (Jurafsky and Martin, 2009). Many important language processing tasks can be converted into this problem, such as partof-speech (POS) tagging, named entity recognition (NER), and text chunking. The current state-of-theart results for sequence labeling have been achieved by neural network approaches (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Gui et al., 2017). Chinese sequence labeling is more challenging due to the lack of explicit word boundaries in Chinese sentences. One way of performing Chinese sequence labeling is to perform Chinese word segmentation (CWS) first, before applying word sequence labeling (Sun and Uszkoreit, 2012; Yang et al., 2016). However, it can suffer from the segmentation errors propagated from the CWS system (Zhang and Yang, 2018; Liu et al., 2019). Therefore, some approaches (Cao et al., 2018; Shen et al., 2016) perform Chinese sequence labeling directly at th"
2021.acl-long.454,W06-0115,0,0.030821,"e: P exp( i (Oi,yi + Tyi−1 ,yi )) P p(y|s) = P (9) yi + Ty˜i−1 ,˜ yi )) ˜ exp( i (Oi,˜ y where T is the transition score matrix and y ˜ denotes all possible tag sequences. Experiments Datasets We evaluate our method on ten datasets of three different sequence labeling tasks, including Chinese NER, Chinese Word Segmentation, and Chinese POS tagging. The statistics of the datasets is shown in Table 1. Chinese NER. We conduct experiments on four benchmark datasets, including Weibo NER (Peng and Dredze, 2015, 2016), OntoNotes (Weischedel et al., 2011), Resume NER (Zhang and Yang, 2018), and MSRA (Levow, 2006). Weibo NER is a social media domain dataset, which is drawn from Sina Weibo; while OntoNotes and MSRA datasets are in the news domain. Resume NER 5851 dataset consists of resumes of senior executives, which is annotated by Zhang and Yang (2018). Chinese Word Segmentation. For Chinese word segmentation, we employ three benchmark datasets in our experiments, namely PKU, MSR, and CTB6, where the former two are from SIGHAN 2005 Bakeoff (Emerson, 2005) and the last one is from Xue et al. (2005). For MSR and PKU, we follow their official training/test data split. For CTB6, we use the same split as"
2021.acl-long.454,2020.acl-main.611,0,0.126297,"ich has been shown to capture implicit wordlevel syntactic and semantic knowledge (Goldberg, 2019; Hewitt and Manning, 2019). The two lines of work are complementary to each other due to the different nature of discrete and neural representations. Recent work considers 5847 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5847–5858 August 1–6, 2021. ©2021 Association for Computational Linguistics the combination of lexicon features and BERT for Chinese NER (Ma et al., 2020; Li et al., 2020), Chinese Word Segmentation (Gan and Zhang, 2020), and Chinese POS tagging (Tian et al., 2020b). The main idea is to integrate contextual representations from BERT and lexicon features into a neural sequence labeling model (shown in Figure 1 (a)). However, these approaches do not fully exploit the representation power of BERT, because the external features are not integrated into the bottom level. Inspired by the work about BERT Adapter (Houlsby et al., 2019; Bapna and Firat, 2019; Wang et al., 2020), we propose Lexicon Enhanced BERT (LEBERT) to integrate lexicon information between Transforme"
2021.acl-long.454,N19-1247,1,0.909752,"king. The current state-of-theart results for sequence labeling have been achieved by neural network approaches (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Gui et al., 2017). Chinese sequence labeling is more challenging due to the lack of explicit word boundaries in Chinese sentences. One way of performing Chinese sequence labeling is to perform Chinese word segmentation (CWS) first, before applying word sequence labeling (Sun and Uszkoreit, 2012; Yang et al., 2016). However, it can suffer from the segmentation errors propagated from the CWS system (Zhang and Yang, 2018; Liu et al., 2019). Therefore, some approaches (Cao et al., 2018; Shen et al., 2016) perform Chinese sequence labeling directly at the character level, which has been empirically proven to be more effective (Ng and Low, 2004; Liu et al., 2010; Zhang and Yang, 2018). There are two lines of recent work enhancing character-based neural Chinese sequence labeling. The first considers integrating word information into a character-based sequence encoder, so that word features can be explicitly modeled (Zhang and Yang, 2018; Yang et al., 2019; Liu et al., 2019; Ding et al., 2019; Higashiyama et al., 2019). These method"
2021.acl-long.454,P16-2025,0,0.0437266,"Missing"
2021.acl-long.454,2020.emnlp-main.617,0,0.0418399,"Missing"
2021.acl-long.454,D18-1529,0,0.0586943,"Missing"
2021.acl-long.454,2020.findings-emnlp.260,0,0.15002,"state-of-the-art models using shallow fusion layer to integrate lexicon information and BERT. The hybrid models, including existing state-of-the-art models, BERT + Word, and 5 We also evaluated with other fusion layers, such as Transformer, but we found LSTM is consistently better. 6 For a fair comparison, in Table 2, we use * denotes training the model with the same pre-trained word embedding as ours; † means the model is also initialized using the Chinese BERT checkpoint from huggingface and evaluated using the seqeval tool. 5852 Model Yang et al. (2017) Ma et al. (2018) Yang et al. (2019) Qiu et al. (2020) Tian et al. (2020c)(with BERT) Tian et al. (2020c)(with ZEN) BERT BERT+Word ERINE ZEN LEBERT PKU 95.00 96.10 95.80 96.41 96.51 96.53 96.25 96.55 96.33 96.36 96.91 MSR 96.80 97.40 97.80 98.05 98.28 98.40 97.94 98.41 98.17 98.36 98.69 Model Shao et al. (2017) Zhang et al. (2018) Tian et al. (2020a)(BERT) Tian et al. (2020a)(ZEN) Tian et al. (2020b)(BERT) Tian et al. (2020b)(ZEN) BERT BERT+Word ERINE ZEN LEBERT CTB6 95.40 96.70 96.10 96.99 97.16 97.25 96.98 97.25 97.02 97.13 97.52 CTB5 94.38 94.95 96.77 96.86 96.60 96.82 96.25 96.77 96.51 96.60 97.14 CTB6 92.51 94.82 94.87 94.74 94.82 94.64 94.7"
2021.acl-long.454,2020.acl-main.528,0,0.43726,"et al., 2019), which has been shown to capture implicit wordlevel syntactic and semantic knowledge (Goldberg, 2019; Hewitt and Manning, 2019). The two lines of work are complementary to each other due to the different nature of discrete and neural representations. Recent work considers 5847 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5847–5858 August 1–6, 2021. ©2021 Association for Computational Linguistics the combination of lexicon features and BERT for Chinese NER (Ma et al., 2020; Li et al., 2020), Chinese Word Segmentation (Gan and Zhang, 2020), and Chinese POS tagging (Tian et al., 2020b). The main idea is to integrate contextual representations from BERT and lexicon features into a neural sequence labeling model (shown in Figure 1 (a)). However, these approaches do not fully exploit the representation power of BERT, because the external features are not integrated into the bottom level. Inspired by the work about BERT Adapter (Houlsby et al., 2019; Bapna and Firat, 2019; Wang et al., 2020), we propose Lexicon Enhanced BERT (LEBERT) to integrate lexicon information"
2021.acl-long.454,I17-1018,0,0.1154,"mer two are from SIGHAN 2005 Bakeoff (Emerson, 2005) and the last one is from Xue et al. (2005). For MSR and PKU, we follow their official training/test data split. For CTB6, we use the same split as that stated in Yang and Xue (2012); Higashiyama et al. (2019). Chinese POS Tagging. For POS-tagging, three Chinese benchmark datasets are used, including CTB5 and CTB6 from the Penn Chinese TreeBank (Xue et al., 2005) and the Chinese GSD Treebank of Universal Dependencies(UD) (Nivre et al., 2016). The CTB datasets are in simplified Chinese while the UD dataset is in traditional Chinese. Following Shao et al. (2017), we first convert the UD dataset into simplified Chinese before the POS-tagging experiments3 . Besides, UD has both universal and language-specific POS tags, we follow previous works (Shao et al., 2017; Tian et al., 2020a), referring to the corpus with two tagsets as UD1 and UD2, respectively. We use the official splits of train/dev/test in our experiments. 4.2 Experimental Settings Our model is constructed based on BERTBASE (Devlin et al., 2019), with 12 layers of transformer, and is initialized using the Chinese-BERT checkpoint from huggingface4 . We use the 200dimension pre-trained word em"
2021.acl-long.454,P16-1101,0,0.0404717,"r layers in BERT and truncate the sentence to three characters. ci denotes the i-th Chinese character, wj denotes the j-th Chinese word. Introduction Sequence labeling is a classic task in natural language processing (NLP), which is to assign a label to each unit in a sequence (Jurafsky and Martin, 2009). Many important language processing tasks can be converted into this problem, such as partof-speech (POS) tagging, named entity recognition (NER), and text chunking. The current state-of-theart results for sequence labeling have been achieved by neural network approaches (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Gui et al., 2017). Chinese sequence labeling is more challenging due to the lack of explicit word boundaries in Chinese sentences. One way of performing Chinese sequence labeling is to perform Chinese word segmentation (CWS) first, before applying word sequence labeling (Sun and Uszkoreit, 2012; Yang et al., 2016). However, it can suffer from the segmentation errors propagated from the CWS system (Zhang and Yang, 2018; Liu et al., 2019). Therefore, some approaches (Cao et al., 2018; Shen et al., 2016) perform Chinese sequence labeling directly at the character level,"
2021.acl-long.454,W04-3236,0,0.059753,"uence labeling is more challenging due to the lack of explicit word boundaries in Chinese sentences. One way of performing Chinese sequence labeling is to perform Chinese word segmentation (CWS) first, before applying word sequence labeling (Sun and Uszkoreit, 2012; Yang et al., 2016). However, it can suffer from the segmentation errors propagated from the CWS system (Zhang and Yang, 2018; Liu et al., 2019). Therefore, some approaches (Cao et al., 2018; Shen et al., 2016) perform Chinese sequence labeling directly at the character level, which has been empirically proven to be more effective (Ng and Low, 2004; Liu et al., 2010; Zhang and Yang, 2018). There are two lines of recent work enhancing character-based neural Chinese sequence labeling. The first considers integrating word information into a character-based sequence encoder, so that word features can be explicitly modeled (Zhang and Yang, 2018; Yang et al., 2019; Liu et al., 2019; Ding et al., 2019; Higashiyama et al., 2019). These methods can be treated as designing different variants to neural architectures for integrating discrete structured knowledge. The second considers the integration of large-scale pre-trained contextualized embeddi"
2021.acl-long.454,L16-1262,0,0.0174683,"Missing"
2021.acl-long.454,D15-1064,0,0.029317,"e scores P as: O = Wo H L + bo (8) For a label sequence y = {y1 , y2 , ..., yn }, we define its probability to be: P exp( i (Oi,yi + Tyi−1 ,yi )) P p(y|s) = P (9) yi + Ty˜i−1 ,˜ yi )) ˜ exp( i (Oi,˜ y where T is the transition score matrix and y ˜ denotes all possible tag sequences. Experiments Datasets We evaluate our method on ten datasets of three different sequence labeling tasks, including Chinese NER, Chinese Word Segmentation, and Chinese POS tagging. The statistics of the datasets is shown in Table 1. Chinese NER. We conduct experiments on four benchmark datasets, including Weibo NER (Peng and Dredze, 2015, 2016), OntoNotes (Weischedel et al., 2011), Resume NER (Zhang and Yang, 2018), and MSRA (Levow, 2006). Weibo NER is a social media domain dataset, which is drawn from Sina Weibo; while OntoNotes and MSRA datasets are in the news domain. Resume NER 5851 dataset consists of resumes of senior executives, which is annotated by Zhang and Yang (2018). Chinese Word Segmentation. For Chinese word segmentation, we employ three benchmark datasets in our experiments, namely PKU, MSR, and CTB6, where the former two are from SIGHAN 2005 Bakeoff (Emerson, 2005) and the last one is from Xue et al. (2005)."
2021.acl-long.454,C16-1029,0,0.0190968,"ave been achieved by neural network approaches (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Gui et al., 2017). Chinese sequence labeling is more challenging due to the lack of explicit word boundaries in Chinese sentences. One way of performing Chinese sequence labeling is to perform Chinese word segmentation (CWS) first, before applying word sequence labeling (Sun and Uszkoreit, 2012; Yang et al., 2016). However, it can suffer from the segmentation errors propagated from the CWS system (Zhang and Yang, 2018; Liu et al., 2019). Therefore, some approaches (Cao et al., 2018; Shen et al., 2016) perform Chinese sequence labeling directly at the character level, which has been empirically proven to be more effective (Ng and Low, 2004; Liu et al., 2010; Zhang and Yang, 2018). There are two lines of recent work enhancing character-based neural Chinese sequence labeling. The first considers integrating word information into a character-based sequence encoder, so that word features can be explicitly modeled (Zhang and Yang, 2018; Yang et al., 2019; Liu et al., 2019; Ding et al., 2019; Higashiyama et al., 2019). These methods can be treated as designing different variants to neural archite"
2021.acl-long.454,N18-2028,0,0.021016,"nvert the UD dataset into simplified Chinese before the POS-tagging experiments3 . Besides, UD has both universal and language-specific POS tags, we follow previous works (Shao et al., 2017; Tian et al., 2020a), referring to the corpus with two tagsets as UD1 and UD2, respectively. We use the official splits of train/dev/test in our experiments. 4.2 Experimental Settings Our model is constructed based on BERTBASE (Devlin et al., 2019), with 12 layers of transformer, and is initialized using the Chinese-BERT checkpoint from huggingface4 . We use the 200dimension pre-trained word embedding from Song et al. (2018), which is trained on texts of news and webpages using a directional skip-gram model. The lexicon D used in this paper is the vocab of the pre-trained word embedding. We apply the Lexicon Adapter between the 1-st and 2-nd Transformer in BERT and fine-tune both BERT and pre-trained word embedding during training. Hyperparameters. We use the Adam optimizer with an initial learning rate of 1e-5 for original parameters of BERT, and 1e-4 for other parameters introduced by LEBERT, and a maximum epoch number of 20 for training on all datasets. The max length of the sequence is set to 256, and the tra"
2021.acl-long.454,P12-1026,0,0.0360158,"ortant language processing tasks can be converted into this problem, such as partof-speech (POS) tagging, named entity recognition (NER), and text chunking. The current state-of-theart results for sequence labeling have been achieved by neural network approaches (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Gui et al., 2017). Chinese sequence labeling is more challenging due to the lack of explicit word boundaries in Chinese sentences. One way of performing Chinese sequence labeling is to perform Chinese word segmentation (CWS) first, before applying word sequence labeling (Sun and Uszkoreit, 2012; Yang et al., 2016). However, it can suffer from the segmentation errors propagated from the CWS system (Zhang and Yang, 2018; Liu et al., 2019). Therefore, some approaches (Cao et al., 2018; Shen et al., 2016) perform Chinese sequence labeling directly at the character level, which has been empirically proven to be more effective (Ng and Low, 2004; Liu et al., 2010; Zhang and Yang, 2018). There are two lines of recent work enhancing character-based neural Chinese sequence labeling. The first considers integrating word information into a character-based sequence encoder, so that word features"
2021.acl-long.454,2020.acl-main.735,0,0.349503,"2019; Hewitt and Manning, 2019). The two lines of work are complementary to each other due to the different nature of discrete and neural representations. Recent work considers 5847 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5847–5858 August 1–6, 2021. ©2021 Association for Computational Linguistics the combination of lexicon features and BERT for Chinese NER (Ma et al., 2020; Li et al., 2020), Chinese Word Segmentation (Gan and Zhang, 2020), and Chinese POS tagging (Tian et al., 2020b). The main idea is to integrate contextual representations from BERT and lexicon features into a neural sequence labeling model (shown in Figure 1 (a)). However, these approaches do not fully exploit the representation power of BERT, because the external features are not integrated into the bottom level. Inspired by the work about BERT Adapter (Houlsby et al., 2019; Bapna and Firat, 2019; Wang et al., 2020), we propose Lexicon Enhanced BERT (LEBERT) to integrate lexicon information between Transformer layers of BERT directly. Specifically, a Chinese sentence is converted into a charwords pai"
2021.acl-long.454,2020.coling-main.187,0,0.416022,"2019; Hewitt and Manning, 2019). The two lines of work are complementary to each other due to the different nature of discrete and neural representations. Recent work considers 5847 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5847–5858 August 1–6, 2021. ©2021 Association for Computational Linguistics the combination of lexicon features and BERT for Chinese NER (Ma et al., 2020; Li et al., 2020), Chinese Word Segmentation (Gan and Zhang, 2020), and Chinese POS tagging (Tian et al., 2020b). The main idea is to integrate contextual representations from BERT and lexicon features into a neural sequence labeling model (shown in Figure 1 (a)). However, these approaches do not fully exploit the representation power of BERT, because the external features are not integrated into the bottom level. Inspired by the work about BERT Adapter (Houlsby et al., 2019; Bapna and Firat, 2019; Wang et al., 2020), we propose Lexicon Enhanced BERT (LEBERT) to integrate lexicon information between Transformer layers of BERT directly. Specifically, a Chinese sentence is converted into a charwords pai"
2021.acl-long.454,2020.acl-main.734,0,0.325513,"2019; Hewitt and Manning, 2019). The two lines of work are complementary to each other due to the different nature of discrete and neural representations. Recent work considers 5847 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5847–5858 August 1–6, 2021. ©2021 Association for Computational Linguistics the combination of lexicon features and BERT for Chinese NER (Ma et al., 2020; Li et al., 2020), Chinese Word Segmentation (Gan and Zhang, 2020), and Chinese POS tagging (Tian et al., 2020b). The main idea is to integrate contextual representations from BERT and lexicon features into a neural sequence labeling model (shown in Figure 1 (a)). However, these approaches do not fully exploit the representation power of BERT, because the external features are not integrated into the bottom level. Inspired by the work about BERT Adapter (Houlsby et al., 2019; Bapna and Firat, 2019; Wang et al., 2020), we propose Lexicon Enhanced BERT (LEBERT) to integrate lexicon information between Transformer layers of BERT directly. Specifically, a Chinese sentence is converted into a charwords pai"
2021.acl-long.454,P12-1083,0,0.0374619,"a social media domain dataset, which is drawn from Sina Weibo; while OntoNotes and MSRA datasets are in the news domain. Resume NER 5851 dataset consists of resumes of senior executives, which is annotated by Zhang and Yang (2018). Chinese Word Segmentation. For Chinese word segmentation, we employ three benchmark datasets in our experiments, namely PKU, MSR, and CTB6, where the former two are from SIGHAN 2005 Bakeoff (Emerson, 2005) and the last one is from Xue et al. (2005). For MSR and PKU, we follow their official training/test data split. For CTB6, we use the same split as that stated in Yang and Xue (2012); Higashiyama et al. (2019). Chinese POS Tagging. For POS-tagging, three Chinese benchmark datasets are used, including CTB5 and CTB6 from the Penn Chinese TreeBank (Xue et al., 2005) and the Chinese GSD Treebank of Universal Dependencies(UD) (Nivre et al., 2016). The CTB datasets are in simplified Chinese while the UD dataset is in traditional Chinese. Following Shao et al. (2017), we first convert the UD dataset into simplified Chinese before the POS-tagging experiments3 . Besides, UD has both universal and language-specific POS tags, we follow previous works (Shao et al., 2017; Tian et al.,"
2021.acl-long.454,P18-1144,1,0.818005,"n (NER), and text chunking. The current state-of-theart results for sequence labeling have been achieved by neural network approaches (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Gui et al., 2017). Chinese sequence labeling is more challenging due to the lack of explicit word boundaries in Chinese sentences. One way of performing Chinese sequence labeling is to perform Chinese word segmentation (CWS) first, before applying word sequence labeling (Sun and Uszkoreit, 2012; Yang et al., 2016). However, it can suffer from the segmentation errors propagated from the CWS system (Zhang and Yang, 2018; Liu et al., 2019). Therefore, some approaches (Cao et al., 2018; Shen et al., 2016) perform Chinese sequence labeling directly at the character level, which has been empirically proven to be more effective (Ng and Low, 2004; Liu et al., 2010; Zhang and Yang, 2018). There are two lines of recent work enhancing character-based neural Chinese sequence labeling. The first considers integrating word information into a character-based sequence encoder, so that word features can be explicitly modeled (Zhang and Yang, 2018; Yang et al., 2019; Liu et al., 2019; Ding et al., 2019; Higashiyama et al.,"
2021.acl-long.454,N19-1342,0,0.0704961,"ng. Lexicon-based. Lexicon-based models aim to enhance character-based models with lexicon information. Zhang and Yang (2018) introduced a lat1 https://github.com/liuwei1206/LEBERT We follow the mainstream methods and regard Chinese Word Segmentation as a sequence labeling problem. 2 tice LSTM to encode both characters and words for Chinese NER. It is further improved by following efforts in terms of training efficiency (Gui et al., 2019a; Ma et al., 2020), model degradation (Liu et al., 2019), graph structure (Gui et al., 2019b; Ding et al., 2019), and removing the dependency of the lexicon (Zhu and Wang, 2019). Lexicon information has also been shown helpful for Chinese Word Segmentation (CWS) and Part-ofspeech (POS) tagging. Yang et al. (2019) applied a lattice LSTM for CWS, showing good performance. Zhao et al. (2020) improved the results of CWS with lexicon-enhanced adaptive attention. Tian et al. (2020b) enhanced the character-based Chinese POS tagging model with a multi-channel attention of N-grams. Pre-trained Model-based. Transformer-based pre-trained models, such as BERT (Devlin et al., 2019), have shown excellent performance for Chinese sequence labeling. Yang (2019) simply added a softmax"
2021.acl-long.454,P17-1078,1,0.896501,"Missing"
2021.acl-long.454,N19-1278,1,0.912777,"the segmentation errors propagated from the CWS system (Zhang and Yang, 2018; Liu et al., 2019). Therefore, some approaches (Cao et al., 2018; Shen et al., 2016) perform Chinese sequence labeling directly at the character level, which has been empirically proven to be more effective (Ng and Low, 2004; Liu et al., 2010; Zhang and Yang, 2018). There are two lines of recent work enhancing character-based neural Chinese sequence labeling. The first considers integrating word information into a character-based sequence encoder, so that word features can be explicitly modeled (Zhang and Yang, 2018; Yang et al., 2019; Liu et al., 2019; Ding et al., 2019; Higashiyama et al., 2019). These methods can be treated as designing different variants to neural architectures for integrating discrete structured knowledge. The second considers the integration of large-scale pre-trained contextualized embeddings, such as BERT (Devlin et al., 2019), which has been shown to capture implicit wordlevel syntactic and semantic knowledge (Goldberg, 2019; Hewitt and Manning, 2019). The two lines of work are complementary to each other due to the different nature of discrete and neural representations. Recent work considers 584"
2021.emnlp-main.179,2020.emnlp-main.275,0,0.472754,"ce a more consistent and engaging response. ternal knowledge base during training, guessing Dinan et al. (2019) propose a Transformer memory the meaning of the unseen entity with its context, network to retrieve knowledge from Wikipedia. Li so as to better understand the input utterance and et al. (2019) use two-step decoding, which first gen2329 erate a response based on context, and then take the generated response and relative knowledge as input to generate a new response. Kim et al. (2020) focus on knowledge selection in dialogue generation by utilizing a sequential latent variable model. Chen et al. (2020) further enhance the selection module with the posterior information. Zhao et al. (2020b) use reinforcement learning to optimize knowledge selection with unlabeled data. Different from their work, our KE-Blender does not take knowledge as input, because knowledge is only used to enhance our model during training. 3 3.1 Method Task Suppose that we have a training set DS = |L| {USi , KSi , RSi }|i=1 , where USi , KSi and RSi are the dialogue context, the external knowledge retrieved from the knowledge base and the response, respectively. In addition to DS , we have a test dataset DP = {UP , RP }"
2021.emnlp-main.179,W19-2310,0,0.0278137,"e dialogue pre-training model. We fine-tune the Blender on Wizard training set without utilizing external knowledge. KG-Blender. We fine-tune Blender on the Wizard training set by concatenating the context and the associated knowledge as the input. In the setting where external knowledge is unavailable, only context is used to generate response. 4.4 Metrics Automatic evaluation metrics: Following Dinan et al. (2019) and Kim et al. (2020), models are measured using the perplexity of the ground4.3 Baselines truth response (PPL) and unigram F1-score (F1). We compare KE-Blender with Blender and KGGhazarian et al. (2019) show that BERT can Blender, also drawing the following state-of-the-art be used to evaluate the generated response. We methods as reference: employ BERT-based evaluation metrics to evaluTransformer (Vaswani et al., 2017) is a stan- ate whether the generated response is knowledgedard transformer model for dialogue generation. It able as supplements to PPL and F1. As shown takes the concatenation of context utterances and in Table 3, the dialogue generation model is first ˆ based on the dithe associated knowledge as input. required to generate response R 2332 Test Seen Model PPL F1 Test Unseen"
2021.emnlp-main.179,2020.acl-main.423,0,0.0156439,"t 407 dialogues on the Reddit Trendings panel, demonstrating the effectiveness of the proposed method in practice. We release our code and dataset at https://github.com/Nealcly/ KE-Blender. 2 2.1 Related Work Knowledge Enhanced Pre-training BAIDU-ERNIE (Sun et al., 2019) uses entity-level masking and phrase-level masking strategy to enhance knowledge into language model. THUERNIE (Zhang et al., 2019) incorporates contextual representations with separate KG embeddings. LUKE (Yamada et al., 2020) proposes an entityaware self-attention to boost the performance of entity related tasks. SenseBERT (Levine et al., 2020) uses WordNet to infuse the lexical semantics knowledge into BERT. KnowBERT (Peters et al., 2019) incorporates knowledge base into BERT using the knowledge attention. TNF (Wu et al., 2021) accelerates pre-training by taking notes for the rare words. Compared with these methods, which enhances the pre-trained encoder by utilizing named entities or knowledge base, we inject knowledge to improve the generation ability of seq2seq models given the unseen word. To achieve this, we take Blender (Roller et al., 2020) as our backbone model, and propose two auxiliary training objectives (Figure 1(c)) in"
2021.emnlp-main.179,2020.acl-main.703,0,0.0189288,"he generation ability of seq2seq models given the unseen word. To achieve this, we take Blender (Roller et al., 2020) as our backbone model, and propose two auxiliary training objectives (Figure 1(c)) in fine2.2 Knowledge Grounded Dialogue tuning, dubbed as Knowledge Enhanced Blender Generation (KE-Blender). The first objective is Interpret Masked Word, which predicts the word’s defini- With advances in deep learning, pre-trained lantion based on the context, where the definition is guage models have shown promising results in diaobtained from a knowledge base. The second is logue generation (Lewis et al., 2020; Zhang et al., Hypernym Generation, which predicts the corre- 2020; Roller et al., 2020). To equip the models sponding hypernym of the word given by WordNet. with external knowledge, Zhang et al. (2018) first These two introduced training objectives force the show that adding user profile information is able to model to learn semantic information from the ex- produce a more consistent and engaging response. ternal knowledge base during training, guessing Dinan et al. (2019) propose a Transformer memory the meaning of the unseen entity with its context, network to retrieve knowledge from Wikip"
2021.emnlp-main.179,P19-1002,0,0.0638107,"Missing"
2021.emnlp-main.179,D19-1005,0,0.0514127,"Missing"
2021.emnlp-main.179,D11-1054,0,0.151504,"Missing"
2021.emnlp-main.179,2020.acl-main.183,0,0.0126138,"of our methods under both knowledge available and unavailable settings. 1 (a) Non-knowledge dialogue generation. (b) Knowledge grounded dialogue generation. Note that the knowledge of “COVID-19” can not be retrieved from the knowledge base, because it is a new term. Introduction (c) The proposed knowledge enhanced dialogue generation. Owing to large amounts of conversation data and pre-training models (Zhang et al., 2020; Roller et al., 2020), generation-based chatbots have achieved significant advances and even reach human parity on specific testsets (Zhang et al., 2018; Dinan et al., 2019; Smith et al., 2020). However, the robustness of the pre-trained model is still low with regard to unseen entities (Zhang et al., 2016; Dinan et al., 2019). In practice, users often talk with chatbots about latest news and the recently hot topics (Morris et al., 2016), which may not appear in the pre-training or fine-tuning corpus. For instance, “COVID-19” is a new term, which ∗ Figure 1: An illustration of how knowledge can help dialogue generation for different methods. does not appear in the training data of Blender 1 (Roller et al., 2020), leading to poor performance when a user mentions “COVID-19”. As shown"
2021.emnlp-main.179,2020.emnlp-main.523,0,0.0319752,"ender baselines (16.3 and 19.9 PPL). To further verify the effectiveness of our method in real-world scenarios, we collect 407 dialogues on the Reddit Trendings panel, demonstrating the effectiveness of the proposed method in practice. We release our code and dataset at https://github.com/Nealcly/ KE-Blender. 2 2.1 Related Work Knowledge Enhanced Pre-training BAIDU-ERNIE (Sun et al., 2019) uses entity-level masking and phrase-level masking strategy to enhance knowledge into language model. THUERNIE (Zhang et al., 2019) incorporates contextual representations with separate KG embeddings. LUKE (Yamada et al., 2020) proposes an entityaware self-attention to boost the performance of entity related tasks. SenseBERT (Levine et al., 2020) uses WordNet to infuse the lexical semantics knowledge into BERT. KnowBERT (Peters et al., 2019) incorporates knowledge base into BERT using the knowledge attention. TNF (Wu et al., 2021) accelerates pre-training by taking notes for the rare words. Compared with these methods, which enhances the pre-trained encoder by utilizing named entities or knowledge base, we inject knowledge to improve the generation ability of seq2seq models given the unseen word. To achieve this, we"
2021.emnlp-main.179,P18-1205,0,0.139303,"dialogue corpus verify the effectiveness of our methods under both knowledge available and unavailable settings. 1 (a) Non-knowledge dialogue generation. (b) Knowledge grounded dialogue generation. Note that the knowledge of “COVID-19” can not be retrieved from the knowledge base, because it is a new term. Introduction (c) The proposed knowledge enhanced dialogue generation. Owing to large amounts of conversation data and pre-training models (Zhang et al., 2020; Roller et al., 2020), generation-based chatbots have achieved significant advances and even reach human parity on specific testsets (Zhang et al., 2018; Dinan et al., 2019; Smith et al., 2020). However, the robustness of the pre-trained model is still low with regard to unseen entities (Zhang et al., 2016; Dinan et al., 2019). In practice, users often talk with chatbots about latest news and the recently hot topics (Morris et al., 2016), which may not appear in the pre-training or fine-tuning corpus. For instance, “COVID-19” is a new term, which ∗ Figure 1: An illustration of how knowledge can help dialogue generation for different methods. does not appear in the training data of Blender 1 (Roller et al., 2020), leading to poor performance w"
2021.emnlp-main.179,2020.acl-demos.30,0,0.0383602,"f the masked entity given the context; 2) Hypernym Generation, which predicts the hypernym of the entity based on the context. Experiment results on two dialogue corpus verify the effectiveness of our methods under both knowledge available and unavailable settings. 1 (a) Non-knowledge dialogue generation. (b) Knowledge grounded dialogue generation. Note that the knowledge of “COVID-19” can not be retrieved from the knowledge base, because it is a new term. Introduction (c) The proposed knowledge enhanced dialogue generation. Owing to large amounts of conversation data and pre-training models (Zhang et al., 2020; Roller et al., 2020), generation-based chatbots have achieved significant advances and even reach human parity on specific testsets (Zhang et al., 2018; Dinan et al., 2019; Smith et al., 2020). However, the robustness of the pre-trained model is still low with regard to unseen entities (Zhang et al., 2016; Dinan et al., 2019). In practice, users often talk with chatbots about latest news and the recently hot topics (Morris et al., 2016), which may not appear in the pre-training or fine-tuning corpus. For instance, “COVID-19” is a new term, which ∗ Figure 1: An illustration of how knowledge c"
2021.emnlp-main.179,P19-1139,0,0.0221808,"the knowledge available setting and unavailable setting, respectively, which outperforms the Blender baselines (16.3 and 19.9 PPL). To further verify the effectiveness of our method in real-world scenarios, we collect 407 dialogues on the Reddit Trendings panel, demonstrating the effectiveness of the proposed method in practice. We release our code and dataset at https://github.com/Nealcly/ KE-Blender. 2 2.1 Related Work Knowledge Enhanced Pre-training BAIDU-ERNIE (Sun et al., 2019) uses entity-level masking and phrase-level masking strategy to enhance knowledge into language model. THUERNIE (Zhang et al., 2019) incorporates contextual representations with separate KG embeddings. LUKE (Yamada et al., 2020) proposes an entityaware self-attention to boost the performance of entity related tasks. SenseBERT (Levine et al., 2020) uses WordNet to infuse the lexical semantics knowledge into BERT. KnowBERT (Peters et al., 2019) incorporates knowledge base into BERT using the knowledge attention. TNF (Wu et al., 2021) accelerates pre-training by taking notes for the rare words. Compared with these methods, which enhances the pre-trained encoder by utilizing named entities or knowledge base, we inject knowledg"
2021.emnlp-main.179,2020.emnlp-main.272,0,0.565196,"ing Dinan et al. (2019) propose a Transformer memory the meaning of the unseen entity with its context, network to retrieve knowledge from Wikipedia. Li so as to better understand the input utterance and et al. (2019) use two-step decoding, which first gen2329 erate a response based on context, and then take the generated response and relative knowledge as input to generate a new response. Kim et al. (2020) focus on knowledge selection in dialogue generation by utilizing a sequential latent variable model. Chen et al. (2020) further enhance the selection module with the posterior information. Zhao et al. (2020b) use reinforcement learning to optimize knowledge selection with unlabeled data. Different from their work, our KE-Blender does not take knowledge as input, because knowledge is only used to enhance our model during training. 3 3.1 Method Task Suppose that we have a training set DS = |L| {USi , KSi , RSi }|i=1 , where USi , KSi and RSi are the dialogue context, the external knowledge retrieved from the knowledge base and the response, respectively. In addition to DS , we have a test dataset DP = {UP , RP }. Unlike DS , DP does not contain external knowledge, because associated background kno"
2021.emnlp-main.218,N16-1030,0,0.0592786,"chanism used by Kiperwasser and Goldberg (2016). As the biaffine attention is widely used in other tasks like NER (Yu et al., 2020a) and semantic role labeling (Li et al., 2019b), we propose to use it for the entity relation extraction task in this work. 3 Entity Relation Extraction as Dependency Parsing While encoding VRDs, previous works take Both semantic entity relation extraction and depenentity labeling task as sequence labeling and re- dency parsing tasks aim to decide whether there implement the named entity recognition (NER) exists relation between two entities/words and asframework (Lample et al., 2016) but ignore layout sume that links always point from key/head unit information. Then, many works introduce a GCN- to value/modifier unit shown in Figure 1. Therebased module to encode layout information and fore, we can draw lessons from the dependency combine textual and visual information together parsing exploration as it has been studied for sev(Liu et al., 2019a; Yu et al., 2020b; Wei et al., 2020; eral decades and achieved great progress. BiCarbonell et al., 2021). In the GCN module, Liu affine parser, a strong model in dependency parsing, et al. (2019a); Yu et al. (2020b) take layout fe"
2021.emnlp-main.218,2021.acl-long.201,0,0.0654113,"Missing"
2021.emnlp-main.218,2020.coling-main.82,0,0.0990775,"Missing"
2021.emnlp-main.218,2020.acl-main.577,0,0.304298,"to the in-house customs data, achieving reliable performance in the production setting. 1 Introduction In real-life scenarios, there are many types of visually rich documents (VRDs), such as invoices, questionnaire forms, declaration materials and so on. These documents contain abundant layout information which helps us to understand the content while texts alone are not enough. In recent years, many works focus on how to extract key information from VRDs based on the results of OCR (Optical Character Recognition), which recognizes bounding boxes and texts within the boxes (Liu et al., 2019a; Yu et al., 2020b). Each bounding box contains 1) a group of words that belong together ∗ Corresponding author. The author’s contributions were carried out while at Alibaba Group. His current affiliation is Vipshop (China) Co., Ltd. from a semantic and spatial standpoint and 2) visual features such as layout, tabular structure and font size of the boxes in the document. We call such bounding boxes and texts within the boxes semantic entities1 , and each entity contains the word group and layout coordinates2 . Key information extraction (KIE) is such a task to analyze visually rich documents, which usually con"
2021.emnlp-main.218,N19-2005,0,0.347814,"l has been applied to the in-house customs data, achieving reliable performance in the production setting. 1 Introduction In real-life scenarios, there are many types of visually rich documents (VRDs), such as invoices, questionnaire forms, declaration materials and so on. These documents contain abundant layout information which helps us to understand the content while texts alone are not enough. In recent years, many works focus on how to extract key information from VRDs based on the results of OCR (Optical Character Recognition), which recognizes bounding boxes and texts within the boxes (Liu et al., 2019a; Yu et al., 2020b). Each bounding box contains 1) a group of words that belong together ∗ Corresponding author. The author’s contributions were carried out while at Alibaba Group. His current affiliation is Vipshop (China) Co., Ltd. from a semantic and spatial standpoint and 2) visual features such as layout, tabular structure and font size of the boxes in the document. We call such bounding boxes and texts within the boxes semantic entities1 , and each entity contains the word group and layout coordinates2 . Key information extraction (KIE) is such a task to analyze visually rich documents,"
2021.emnlp-main.218,2021.ccl-1.108,0,0.0217989,"Missing"
2021.emnlp-main.218,2020.acl-main.580,0,0.0138516,"u et al., 2019a; Yu et al., 2020b). • We apply our model to the real-world customs data with different layouts and achieve high performance in the production setting. 2 Related Work Visually rich document understanding includes many tasks, such as layout recognization (Zhong et al., 2019b; Li et al., 2020), table detection and recognition (Li et al., 2019a; Zhong et al., • At the relation scorer layer, we extract rela- 2019a) and key information extraction (Grali´nski tive position features between entities accord- et al., 2020; Guo et al., 2019; Huang et al., 2019; G. Jaume and Thiran, 2019; Majumder et al., 2020). ing to their coordinates. Our paper focuses on the key information extracApart from the above, inspired by the joint POS tion task which contains two subtasks, entity latagging and dependency parsing model (Nguyen beling and relation extraction. The former subtask 2760 Figure 2: The architecture of our proposed entity relation extraction model (left) and the biaffine parser model (right). tags entities with predefined labels, such as Task 3 on the SROIE data released by Huang et al. (2019), while the latter discovers relations between entities, such as Subtask C(3) on the FUNSD data (G. Jaum"
2021.emnlp-main.218,K18-2008,0,0.0418932,"Missing"
2021.emnlp-main.254,D17-1196,0,0.026588,"hen processing the data. Van Rijsbergen (2004) first proposed to unify information retrieval models into the mathematical 4 The Modeling Target framework of quantum mechanics in Hilbert space. Both quantum-inspired algorithms and quantum Sordoni et al. (2013) proposed a quantum language algorithms can model different features in the lanmodel, which models term dependencies using the guage. We divide them into word representation density matrix. This work indicates that the density (Section 4.1) and composition (Section 4.2). matrix may be a more general representation of texts. Based on this, Basile and Tamburini (2017) 4.1 Word Representation presented a language model using the evolution How to represent words is essential for most NLP of the state which can be implemented in speech tasks and can affect performance. Using quantum recognition. Li et al. (2019) encoded words as physics for word representation has the potential to quantum states and sentences as mixed systems. help including more features for words. Recently, in order to improve practicality, some quantum-inspired neural networks for natural lan4.1.1 Modeling Word Ambiguity guage problems have been proposed. Zhang et al. (2019) use a density"
2021.emnlp-main.254,2020.coling-tutorials.2,0,0.0872535,"Missing"
2021.emnlp-main.254,D14-1181,0,0.0062144,"Missing"
2021.emnlp-main.254,R19-1075,0,0.139466,"ic classification (Sordoni et al., 2013) Classical Composition Information retrieval (Xie et al., 2015) Classical Composition Information retrieval (Basile and Tamburini, 2017) Classical Composition Speech recognition (Li et al., 2018) Classical Composition Information retrieval (Zhang et al., 2018b) Classical Composition Question answering (Zhang et al., 2018a) Classical Composition Question answering (Zhang et al., 2018c) Classical Composition Text classification (Li et al., 2019) Classical Word + Composition Question answering (Zhang et al., 2019) Classical Composition Text classification (Lewis, 2019) Classical Word representation / (Meyer and Lewis, 2020) Classical Word representation / (Jiang et al., 2020) Classical Composition Information retrieval (Zhang et al., 2020) Classical Composition Text classification (Zhang et al., 2021) Classical+Quantum Word + Composition Text classification Table 1: Categorization of main surveyed papers according to three dimensions defined in the Introduction. Figure 1: Graphical framework proposed by (Coecke and Kissinger, 2018; Coecke et al., 2020) to demonstrate: (a) A ket |ψi, (b) A bra hψ|, (c) Bell state in Eq. 2, (d) (g(|ϕ1 i ⊗ |ϕ2 i)) ⊗ I, where m"
2021.emnlp-main.254,N19-1420,0,0.103829,"ring (Coecke et al., 2020) Quantum Word + Composition Question answering (Lorenz et al., 2021) Quantum Word + Composition Syntactic classification (Sordoni et al., 2013) Classical Composition Information retrieval (Xie et al., 2015) Classical Composition Information retrieval (Basile and Tamburini, 2017) Classical Composition Speech recognition (Li et al., 2018) Classical Composition Information retrieval (Zhang et al., 2018b) Classical Composition Question answering (Zhang et al., 2018a) Classical Composition Question answering (Zhang et al., 2018c) Classical Composition Text classification (Li et al., 2019) Classical Word + Composition Question answering (Zhang et al., 2019) Classical Composition Text classification (Lewis, 2019) Classical Word representation / (Meyer and Lewis, 2020) Classical Word representation / (Jiang et al., 2020) Classical Composition Information retrieval (Zhang et al., 2020) Classical Composition Text classification (Zhang et al., 2021) Classical+Quantum Word + Composition Text classification Table 1: Categorization of main surveyed papers according to three dimensions defined in the Introduction. Figure 1: Graphical framework proposed by (Coecke and Kissinger, 2018; Co"
2021.emnlp-main.254,2021.acl-long.368,1,0.716663,"e answer vectors pool. They exploited quantum advantage for finding the closest vector (Wiebe et al., 2015) and showed quantum speedup. Meichanetzidis et al. (2020b) showed the first-ever quantum NLP experiment on quantum hardware through a QA task. Although this is a proof-ofconcept experiment, it paved the way for the future use of quantum computers to deal with practical NLP problems. ity. For example, quantum theory is used to model interference phenomenon in information retrieval (Jiang et al., 2020) and term dependencies (Sordoni et al., 2013). It’s more consistent with human cognition. Li et al. (2021) demonstrate that neural machine translation models fail badly on compositional generalization. According to existing paper, we believe quantum NLP models have potential advantages in compositional generalization problem. Increasing storage capacity. Quantum computers have strong storage capabilities. As mentioned before, Coecke hold the view that NLP is quantumnative (Meichanetzidis et al., 2020b; Coecke et al., 2020) such that the exponentially large vector space required to represent sentences can only be naturally and feasibly realized in quantum computers. From this point of view, develop"
2021.emnlp-main.254,2021.ccl-1.108,0,0.0381524,"Missing"
2021.emnlp-main.254,2020.conll-1.21,0,0.287897,"um physics has ferent applications, e.g. information retrieval, also been exploited for cognition (Busemeyer and question answering (Section 5). Bruza, 2012), optimization (Soleimanpour et al., 2014) and other disciplines. In the field of natural Although quantum NLP is still an emerging field, language processing (NLP), quantum mechanics existing work shows exciting promise—not only has seen a surge of recent research interests, ad- better performance but also more efficient calculadressing problems ranging from lexical semantic tions are possible. In addition, noisy intermediateambiguities (Meyer and Lewis, 2020) to seman- scale quantum (NISQ) computers already exist and tic composition (Coecke et al., 2020), and from seem to have potential use in NLP tasks (Coecke information retrieval (Jiang et al., 2020) to text et al., 2020; Lorenz et al., 2021). It has been shown classification (Zhang et al., 2021), where different that quantum NLP can take effect in addressing characteristics of quantum physics have inspired the inherent ambiguities of words, representing novel algorithms. lexical semantic correlations, and calculating seDespite its growing research literature, no sur- mantic composition, which"
2021.emnlp-main.351,2020.emnlp-main.92,1,0.863934,"Missing"
2021.emnlp-main.351,W13-2322,0,0.119156,"StructAdapt More power to her for her achievements. fi Figure 1: (a) AMR for the sentence More power to her for her achievements. While in (b) the pretrained model gets as input the graph linearization, in (c) it additionally receives the graph connectivity information. Introduction Data-to-text tasks aim to generate meaningful and coherent natural language text that faithfully conveys structured data. Some examples of structured information include tables (Parikh et al., 2020), Knowledge Graphs (KGs) (Gardent et al., 2017; Vougiouklis et al., 2018) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In this work, we focus on AMR-to-text generation where the goal is to generate a fluent and grammatical sentence that is faithful to a given AMR graph (See Figure 1a). AMR is a semantic formalism that has received much research interest (Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019; Opitz et al., 2020, 2021; Fu et al., 2021) and has been shown to benefit downstream tasks 1 Our code and checkpoints are available at https://github.com/UKPLab/StructAdapt. power-01 :ARG1 such as text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Both statistical (Fla"
2021.emnlp-main.351,P18-1026,0,0.0789485,"nguage. As a result, knowledge from large-scale pretraining intuitively cannot be fully transferred, and finetuning a sentence representation using linearized graphs can lead to catastrophic forgetting of such distributional knowledge (Goodfellow et al., 2014; Kirkpatrick et al., 2017). Second, a linearized representation weakens structural information in the original graphs by diluting the explicit connectivity information (i.e., which nodes are connected to each other), and PLMs must infer how edge connections are specified in the sequence. This fact was also observed by Song et al. (2018), Beck et al. (2018) and Ribeiro et al. (2019), who show that GNN encoders outperform sequential encoders for AMR-to-text generation without pretraining. To mitigate the issues, we aim to explicitly encode the graph data into a PLM without contaminating its original distributional knowledge. To this end, we propose S TRUCTA DAPT, a novel structureaware adapter that effectively allows leveraging the input graph structure into PLMs (See Figure 1c). The main idea is to add layer-wise modules, which extract information from the pretrained layers and make use of it in a graph-structure encoding. As shown in Figure 2,"
2021.emnlp-main.351,2021.acl-long.324,1,0.759346,"ral language text that faithfully conveys structured data. Some examples of structured information include tables (Parikh et al., 2020), Knowledge Graphs (KGs) (Gardent et al., 2017; Vougiouklis et al., 2018) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In this work, we focus on AMR-to-text generation where the goal is to generate a fluent and grammatical sentence that is faithful to a given AMR graph (See Figure 1a). AMR is a semantic formalism that has received much research interest (Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019; Opitz et al., 2020, 2021; Fu et al., 2021) and has been shown to benefit downstream tasks 1 Our code and checkpoints are available at https://github.com/UKPLab/StructAdapt. power-01 :ARG1 such as text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Both statistical (Flanigan et al., 2016; Pourdamghani et al., 2016) and neural methods (Bai et al., 2020; Cai and Lam, 2020) have been investigated for AMRto-text generation, and dominant methods make use of Graph Neural Networks (GNNs) (Kipf and Welling, 2017) or Transformers (Vaswani et al., 2017) for representing the input graph. Pretrained language models"
2021.emnlp-main.351,W17-3518,0,0.0149925,"graph structure power :ARG1 she :mod more :purpose achieve :ARG0 she Pretrained Model with StructAdapt More power to her for her achievements. fi Figure 1: (a) AMR for the sentence More power to her for her achievements. While in (b) the pretrained model gets as input the graph linearization, in (c) it additionally receives the graph connectivity information. Introduction Data-to-text tasks aim to generate meaningful and coherent natural language text that faithfully conveys structured data. Some examples of structured information include tables (Parikh et al., 2020), Knowledge Graphs (KGs) (Gardent et al., 2017; Vougiouklis et al., 2018) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In this work, we focus on AMR-to-text generation where the goal is to generate a fluent and grammatical sentence that is faithful to a given AMR graph (See Figure 1a). AMR is a semantic formalism that has received much research interest (Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019; Opitz et al., 2020, 2021; Fu et al., 2021) and has been shown to benefit downstream tasks 1 Our code and checkpoints are available at https://github.com/UKPLab/StructAdapt. power-01 :ARG1 such as text summar"
2021.emnlp-main.351,Q19-1019,0,0.0190683,"on Data-to-text tasks aim to generate meaningful and coherent natural language text that faithfully conveys structured data. Some examples of structured information include tables (Parikh et al., 2020), Knowledge Graphs (KGs) (Gardent et al., 2017; Vougiouklis et al., 2018) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In this work, we focus on AMR-to-text generation where the goal is to generate a fluent and grammatical sentence that is faithful to a given AMR graph (See Figure 1a). AMR is a semantic formalism that has received much research interest (Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019; Opitz et al., 2020, 2021; Fu et al., 2021) and has been shown to benefit downstream tasks 1 Our code and checkpoints are available at https://github.com/UKPLab/StructAdapt. power-01 :ARG1 such as text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Both statistical (Flanigan et al., 2016; Pourdamghani et al., 2016) and neural methods (Bai et al., 2020; Cai and Lam, 2020) have been investigated for AMRto-text generation, and dominant methods make use of Graph Neural Networks (GNNs) (Kipf and Welling, 2017) or Transformers (Vaswani et al., 2"
2021.emnlp-main.351,2021.acl-long.381,0,0.0669303,"Missing"
2021.emnlp-main.351,2020.coling-main.218,0,0.0422375,"Missing"
2021.emnlp-main.351,N13-1132,0,0.0213716,"Missing"
2021.emnlp-main.351,P18-1031,0,0.0678754,"Missing"
2021.emnlp-main.351,2021.findings-acl.82,0,0.0507666,"Missing"
2021.emnlp-main.351,2020.inlg-1.14,0,0.0438655,"Missing"
2021.emnlp-main.351,P17-1014,0,0.0285109,"Missing"
2021.emnlp-main.351,2020.deelio-1.5,1,0.701379,"e PLM, yielding a high degree of parameter sharing for graph-to-text tasks. 4.2 Graph Representation We convert each G0 into a bipartite graph G1 = (V1 , E1 ), replacing each labeled edge (u, r, v) ∈ E0 with two unlabeled edges e1 = (u, r) and e2 = (r, v). Similar to Beck et al. (2018), this process converts the graph into its unlabeled version. Figure 3 shows an (a) AMR subgraph and (b) its unlabeled representation. Note that PLMs typically use a vocabulary with subword units (Sennrich et al., 2016). This presents a challenge in how to represent such a graph using subword tokens. Inspired by Ribeiro et al. (2020b), we transform each G1 into a new token graph G = (V, E), where each token of a node in V1 becomes a node v ∈ V. We convert each edge (u1 , v1 ) ∈ E1 into a set of edges and connect every token of u1 to every token of v1 . That is, an edge (u, v) will belong to E if and only if there exists an edge (u1 , v1 ) ∈ E1 such that u ∈ u1 and v ∈ v1 , where u1 and v1 are seen as sets of tokens. Figure 3c shows an example of the token graph. 4.3 Method employs a two-layer architecture in order to re-purpose the PLM for the graph-to-text task using a small number of new parameters. Formally, for each"
2021.emnlp-main.351,2020.acl-main.703,0,0.0594932,"oints are available at https://github.com/UKPLab/StructAdapt. power-01 :ARG1 such as text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Both statistical (Flanigan et al., 2016; Pourdamghani et al., 2016) and neural methods (Bai et al., 2020; Cai and Lam, 2020) have been investigated for AMRto-text generation, and dominant methods make use of Graph Neural Networks (GNNs) (Kipf and Welling, 2017) or Transformers (Vaswani et al., 2017) for representing the input graph. Pretrained language models (PLMs) (Devlin et al., 2019; Liu et al., 2020; Radford et al., 2019; Lewis et al., 2020) have been shown useful as a general text representation method, giving much improved results on a wide range of tasks (Wang et al., 2018, 2019). However, they cannot be directly leveraged to benefit AMR-to-text generation, and more generally graph-to-text generation, due to the structural nature of the input. One solution is to transform the structured input into a se4269 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4269–4282 c November 7–11, 2021. 2021 Association for Computational Linguistics quence, which can be directly fed into PLMs (See F"
2021.emnlp-main.351,2021.acl-long.353,0,0.0464137,"Missing"
2021.emnlp-main.351,C18-1101,0,0.288735,"Missing"
2021.emnlp-main.351,2021.eacl-main.129,0,0.0300749,"es the refined structural node representation zlv based on the local node context, using as input the global representation hlv generated by the current PLM encoder layer. In this way, the model is able to capture both the global context based on the PLM linguistic knowledge and the local context based on the graph knowledge. Finally, we employ A DAPT into the decoder in order to adapt the language model to the graph-to-text task. Evaluation. We evaluate the results with BLEU (Papineni et al., 2002) and chrF++ (Popovi´c, 2015) metrics. We also report the meaning (M) component of the MF-score (Opitz and Frank, 2021), which measures how well the source AMR graph can be reconstructed from the generated sentence. We use BERTScore (Zhang et al., 2020a) allowing a semantic evaluation that depends less on the surface forms. Finally, we also perform a human evaluation (§5.2). 5.1 Main Results We compare S TRUCTA DAPT with four methods: finetuning (F INE - TUNE), fine-tuning only the top or bottom 2 layers (FT- TOP 2, FT- BOTTOM 2) and A DAPT. All 5 Preliminary experiments with other architecture configurations led to worse or similar performance. 4273 6 Hyperparameter details are in the appendix A. Zhang et al."
2021.emnlp-main.351,2020.tacl-1.34,0,0.0136535,"aningful and coherent natural language text that faithfully conveys structured data. Some examples of structured information include tables (Parikh et al., 2020), Knowledge Graphs (KGs) (Gardent et al., 2017; Vougiouklis et al., 2018) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In this work, we focus on AMR-to-text generation where the goal is to generate a fluent and grammatical sentence that is faithful to a given AMR graph (See Figure 1a). AMR is a semantic formalism that has received much research interest (Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019; Opitz et al., 2020, 2021; Fu et al., 2021) and has been shown to benefit downstream tasks 1 Our code and checkpoints are available at https://github.com/UKPLab/StructAdapt. power-01 :ARG1 such as text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Both statistical (Flanigan et al., 2016; Pourdamghani et al., 2016) and neural methods (Bai et al., 2020; Cai and Lam, 2020) have been investigated for AMRto-text generation, and dominant methods make use of Graph Neural Networks (GNNs) (Kipf and Welling, 2017) or Transformers (Vaswani et al., 2017) for representing the input graph. Pre"
2021.emnlp-main.351,P02-1040,0,0.110337,"s under relation r ∈ R, and Wrl ∈ Rm×d encodes the edge type between the nodes u and v. Note that S TRUCTA DAPT computes the refined structural node representation zlv based on the local node context, using as input the global representation hlv generated by the current PLM encoder layer. In this way, the model is able to capture both the global context based on the PLM linguistic knowledge and the local context based on the graph knowledge. Finally, we employ A DAPT into the decoder in order to adapt the language model to the graph-to-text task. Evaluation. We evaluate the results with BLEU (Papineni et al., 2002) and chrF++ (Popovi´c, 2015) metrics. We also report the meaning (M) component of the MF-score (Opitz and Frank, 2021), which measures how well the source AMR graph can be reconstructed from the generated sentence. We use BERTScore (Zhang et al., 2020a) allowing a semantic evaluation that depends less on the surface forms. Finally, we also perform a human evaluation (§5.2). 5.1 Main Results We compare S TRUCTA DAPT with four methods: finetuning (F INE - TUNE), fine-tuning only the top or bottom 2 layers (FT- TOP 2, FT- BOTTOM 2) and A DAPT. All 5 Preliminary experiments with other architecture"
2021.emnlp-main.351,2020.emnlp-main.89,0,0.021665,"her to achieve. (c) Lightweight ne-tuning with graph structure power :ARG1 she :mod more :purpose achieve :ARG0 she Pretrained Model with StructAdapt More power to her for her achievements. fi Figure 1: (a) AMR for the sentence More power to her for her achievements. While in (b) the pretrained model gets as input the graph linearization, in (c) it additionally receives the graph connectivity information. Introduction Data-to-text tasks aim to generate meaningful and coherent natural language text that faithfully conveys structured data. Some examples of structured information include tables (Parikh et al., 2020), Knowledge Graphs (KGs) (Gardent et al., 2017; Vougiouklis et al., 2018) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In this work, we focus on AMR-to-text generation where the goal is to generate a fluent and grammatical sentence that is faithful to a given AMR graph (See Figure 1a). AMR is a semantic formalism that has received much research interest (Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019; Opitz et al., 2020, 2021; Fu et al., 2021) and has been shown to benefit downstream tasks 1 Our code and checkpoints are available at https://github.com/UKPLab/S"
2021.emnlp-main.351,2021.eacl-main.39,1,0.837333,"Missing"
2021.emnlp-main.351,2020.emnlp-demos.7,1,0.841136,"Missing"
2021.emnlp-main.351,2020.emnlp-main.617,1,0.83897,"Missing"
2021.emnlp-main.351,W15-3049,0,0.0193793,"Missing"
2021.emnlp-main.351,W16-6603,0,0.0583696,"Missing"
2021.emnlp-main.351,D19-1314,1,0.907719,"sks aim to generate meaningful and coherent natural language text that faithfully conveys structured data. Some examples of structured information include tables (Parikh et al., 2020), Knowledge Graphs (KGs) (Gardent et al., 2017; Vougiouklis et al., 2018) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In this work, we focus on AMR-to-text generation where the goal is to generate a fluent and grammatical sentence that is faithful to a given AMR graph (See Figure 1a). AMR is a semantic formalism that has received much research interest (Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019; Opitz et al., 2020, 2021; Fu et al., 2021) and has been shown to benefit downstream tasks 1 Our code and checkpoints are available at https://github.com/UKPLab/StructAdapt. power-01 :ARG1 such as text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Both statistical (Flanigan et al., 2016; Pourdamghani et al., 2016) and neural methods (Bai et al., 2020; Cai and Lam, 2020) have been investigated for AMRto-text generation, and dominant methods make use of Graph Neural Networks (GNNs) (Kipf and Welling, 2017) or Transformers (Vaswani et al., 2017) for representing"
2021.emnlp-main.351,2021.emnlp-main.57,1,0.829113,"Missing"
2021.emnlp-main.351,2020.tacl-1.38,1,0.845681,"e PLM, yielding a high degree of parameter sharing for graph-to-text tasks. 4.2 Graph Representation We convert each G0 into a bipartite graph G1 = (V1 , E1 ), replacing each labeled edge (u, r, v) ∈ E0 with two unlabeled edges e1 = (u, r) and e2 = (r, v). Similar to Beck et al. (2018), this process converts the graph into its unlabeled version. Figure 3 shows an (a) AMR subgraph and (b) its unlabeled representation. Note that PLMs typically use a vocabulary with subword units (Sennrich et al., 2016). This presents a challenge in how to represent such a graph using subword tokens. Inspired by Ribeiro et al. (2020b), we transform each G1 into a new token graph G = (V, E), where each token of a node in V1 becomes a node v ∈ V. We convert each edge (u1 , v1 ) ∈ E1 into a set of edges and connect every token of u1 to every token of v1 . That is, an edge (u, v) will belong to E if and only if there exists an edge (u1 , v1 ) ∈ E1 such that u ∈ u1 and v ∈ v1 , where u1 and v1 are seen as sets of tokens. Figure 3c shows an example of the token graph. 4.3 Method employs a two-layer architecture in order to re-purpose the PLM for the graph-to-text task using a small number of new parameters. Formally, for each"
2021.emnlp-main.351,2021.textgraphs-1.2,1,0.842111,"Missing"
2021.emnlp-main.351,P16-1162,0,0.0361553,"ration as we will show in §5 and §6. Moreover, different adapters for distinct graph domains can be used with the same PLM, yielding a high degree of parameter sharing for graph-to-text tasks. 4.2 Graph Representation We convert each G0 into a bipartite graph G1 = (V1 , E1 ), replacing each labeled edge (u, r, v) ∈ E0 with two unlabeled edges e1 = (u, r) and e2 = (r, v). Similar to Beck et al. (2018), this process converts the graph into its unlabeled version. Figure 3 shows an (a) AMR subgraph and (b) its unlabeled representation. Note that PLMs typically use a vocabulary with subword units (Sennrich et al., 2016). This presents a challenge in how to represent such a graph using subword tokens. Inspired by Ribeiro et al. (2020b), we transform each G1 into a new token graph G = (V, E), where each token of a node in V1 becomes a node v ∈ V. We convert each edge (u1 , v1 ) ∈ E1 into a set of edges and connect every token of u1 to every token of v1 . That is, an edge (u, v) will belong to E if and only if there exists an edge (u1 , v1 ) ∈ E1 such that u ∈ u1 and v ∈ v1 , where u1 and v1 are seen as sets of tokens. Figure 3c shows an example of the token graph. 4.3 Method employs a two-layer architecture in"
2021.emnlp-main.351,Q19-1002,1,0.914222,"Missing"
2021.emnlp-main.351,P18-1150,1,0.734082,"rmation. Introduction Data-to-text tasks aim to generate meaningful and coherent natural language text that faithfully conveys structured data. Some examples of structured information include tables (Parikh et al., 2020), Knowledge Graphs (KGs) (Gardent et al., 2017; Vougiouklis et al., 2018) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In this work, we focus on AMR-to-text generation where the goal is to generate a fluent and grammatical sentence that is faithful to a given AMR graph (See Figure 1a). AMR is a semantic formalism that has received much research interest (Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019; Opitz et al., 2020, 2021; Fu et al., 2021) and has been shown to benefit downstream tasks 1 Our code and checkpoints are available at https://github.com/UKPLab/StructAdapt. power-01 :ARG1 such as text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Both statistical (Flanigan et al., 2016; Pourdamghani et al., 2016) and neural methods (Bai et al., 2020; Cai and Lam, 2020) have been investigated for AMRto-text generation, and dominant methods make use of Graph Neural Networks (GNNs) (Kipf and Welling, 2017) or Transformers"
2021.emnlp-main.351,2020.findings-emnlp.199,0,0.0917268,"Missing"
2021.emnlp-main.351,2020.emnlp-main.169,0,0.307718,"s on the LDC2017T10 test set. Mean (±s.d.) over 4 seeds. 5 Experiments Our models are initialized with pre-trained T5 (Raffel et al., 2019), but our approach can be combined with other PLMs such as BART (Lewis et al., 2020). Our implementation is based on Hugging Face Transformer models (Wolf et al., 2019). We use T5base for all experiments and report results with T5large for the test sets.6 We use the Adam optimizer (Kingma and Ba, 2015) and employ a linearly decreasing learning rate schedule without warm-up. BLEU is used for the stopping criterion. Following recent work (Mager et al., 2020; Zhang et al., 2020b), we evaluate our proposed models on LDC2017T10 and LDC2020T02 corpora. r∈R u∈Nr (v) where R denotes the set of relations, i.e., the edge types default and reverse, Nr (v) denotes the set of neighbors under relation r ∈ R, and Wrl ∈ Rm×d encodes the edge type between the nodes u and v. Note that S TRUCTA DAPT computes the refined structural node representation zlv based on the local node context, using as input the global representation hlv generated by the current PLM encoder layer. In this way, the model is able to capture both the global context based on the PLM linguistic knowledge and t"
2021.emnlp-main.351,W18-5446,0,0.0603749,"Missing"
2021.emnlp-main.361,S14-2149,0,0.0225994,"e 1: ACSA results using different templates. ai indicates given category, pk indicates polarity type. ACD Template T+ /T− Dev F1 The ai category is discussed The ai category is not discussed 93.13 The sentence discusses the ai category The sentence discusses no ai category 92.67 It is about the ai category It is not about the ai category 92.44 Table 2: ACD results using different templates. ai indicates category type. tern) (Yin et al., 2017); (2) BERT (Devlin et al., 2019b) based model: BERT classification. For ACD, we compare our method with the following methods. (1) non-BERT models: XRCE (Brun et al., 2014), NRC-Canada (Kiritchenko et al., 2014); (2) BERT (Devlin et al., 2019b) based models: BERT classification, BERT-pair-NLI-B (Sun et al., 2019) , CNE-net (Dai et al., 2020). 4.2 Development Experiments Different templates can be used for expressing the same meaning. For instance, “The sen4.1 Baseline Methods timent polarity of hgiven_categoryi is posiWe compare our generation method with classifi- tive” can also be expressed by “The sentiment is positive for hgiven_categoryi”. For ACSA, cation and MLM baselines (Figure 3) using the we investigate the impact of manual templates same encoder. In"
2021.emnlp-main.361,2020.emnlp-main.565,0,0.0320047,"y is not discussed 93.13 The sentence discusses the ai category The sentence discusses no ai category 92.67 It is about the ai category It is not about the ai category 92.44 Table 2: ACD results using different templates. ai indicates category type. tern) (Yin et al., 2017); (2) BERT (Devlin et al., 2019b) based model: BERT classification. For ACD, we compare our method with the following methods. (1) non-BERT models: XRCE (Brun et al., 2014), NRC-Canada (Kiritchenko et al., 2014); (2) BERT (Devlin et al., 2019b) based models: BERT classification, BERT-pair-NLI-B (Sun et al., 2019) , CNE-net (Dai et al., 2020). 4.2 Development Experiments Different templates can be used for expressing the same meaning. For instance, “The sen4.1 Baseline Methods timent polarity of hgiven_categoryi is posiWe compare our generation method with classifi- tive” can also be expressed by “The sentiment is positive for hgiven_categoryi”. For ACSA, cation and MLM baselines (Figure 3) using the we investigate the impact of manual templates same encoder. In particular, BART generation (i.e., using the MAMS development set. Table 1 Figure 3(c)) is compared with BART classification shows the impact of different choice of tem(Fi"
2021.emnlp-main.361,W18-3027,0,0.0269142,"n between entities from BERT by constructing cloze-style templates. We are the first to apply such methods to ACSA, taking it as a baseline. Different from these template-based models, our final model uses BART for text generation, which better models the correlations between the input sentence and the output sentence compared with BERT. Generation Methods There has been work casting NLP problems as sequence generation tasks (Vinyals et al., 2015; Ma et al., 2017; Stanovsky and Dagan, 2018; Raffel et al., 2020), where the output is a sequence of tokens rather than a natural language sentence. Daza and Frank (2018) treat semantic role labelling as a sequence-to-sequence process. Li et al. (2019) solve the entity-relation extraction task as a multi-turn question answering generation method. Our work is similar in casting an NLP task as a generation task. Different from the above methods, our goal is to make the most of pre-trained knowledge in BART for ACSA. 3 Methods 3.1 Pre-trained language Models We take BERT (Devlin et al., 2019a) and BART (Lewis et al., 2020) as the pre-trained language models. Both are built on the Transformer (Vaswani et al., 2017) architecture. BERT (Devlin et al., 2019a) is an e"
2021.emnlp-main.361,N19-1423,0,0.593196,"reat ACSA and ACD as classification where the encoder takes the input sentence and tasks, learning aspect-specific sentence representa- the decoder generates a natural language sentence. tions (Wang et al., 2016; Ruder et al., 2016). Re- For ACD, the output follows a template stating cently, pre-trained language models (PLM) have whether the specific aspect is discussed (e.g., “The shown their effectiveness to this end (Jiang et al., hcategory_typei category is discussed”); for 2019). The main idea is to make use of pre-trained ACSA, the sentiment polarity of a specific asmodels such as BERT (Devlin et al., 2019a) for rep- pect is stated (e.g., “The sentiment polarity of resenting an aspect-specific form of the input (e.g., hgiven_categoryi is hpolarity_typei”). The by concatenating the aspect category to the end of setting corresponds closely to the denoising auto4406 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4406–4416 c November 7–11, 2021. 2021 Association for Computational Linguistics Aspect category detection The price category is discussed The price category is not discussed (scoring: 0.9) (scoring: 0.1) 2 Related Work Aspect Category Sentimen"
2021.emnlp-main.361,D19-1467,0,0.0351559,"Missing"
2021.emnlp-main.361,D19-1654,0,0.043291,"Missing"
2021.emnlp-main.361,S14-2076,0,0.0339887,"t templates. ai indicates given category, pk indicates polarity type. ACD Template T+ /T− Dev F1 The ai category is discussed The ai category is not discussed 93.13 The sentence discusses the ai category The sentence discusses no ai category 92.67 It is about the ai category It is not about the ai category 92.44 Table 2: ACD results using different templates. ai indicates category type. tern) (Yin et al., 2017); (2) BERT (Devlin et al., 2019b) based model: BERT classification. For ACD, we compare our method with the following methods. (1) non-BERT models: XRCE (Brun et al., 2014), NRC-Canada (Kiritchenko et al., 2014); (2) BERT (Devlin et al., 2019b) based models: BERT classification, BERT-pair-NLI-B (Sun et al., 2019) , CNE-net (Dai et al., 2020). 4.2 Development Experiments Different templates can be used for expressing the same meaning. For instance, “The sen4.1 Baseline Methods timent polarity of hgiven_categoryi is posiWe compare our generation method with classifi- tive” can also be expressed by “The sentiment is positive for hgiven_categoryi”. For ACSA, cation and MLM baselines (Figure 3) using the we investigate the impact of manual templates same encoder. In particular, BART generation (i.e., usin"
2021.emnlp-main.361,D16-1011,0,0.0312874,"s-entropy between the decoder’s output and the original template is used as the loss function: L=− m X log P (tc |t1,c−1 , X) (5) ACSA Template T Dev accuracy The sentiment polarity of ai is pk The sentiment is pk for ai The ai category has a pk label 83.78 83.44 82.31 c=1 4 Experiments We choose the SemEval-2014 restaurant review (Rest14) (Pontiki et al., 2014a), a variant of Rest14 (Rest14-hard) (Xue and Li, 2018) and the multiaspect multi-sentiment (MAMS) (Jiang et al., 2019) datasets for sentence-level sentiment , the TripAdvisor (Wang et al., 2010) and BeerAdvocate (McAuley et al., 2012; Lei et al., 2016) datasets for document-level sentiment. Standard splits of training/development/testing sets are adopted following previous work Tay et al. (2018), the details of which are shown in Appendix A. We use the pre-trained BERT-base1 and BARTbase2 models for task fine-tuning. We select the fine-tuning learning rate from {4e-5, 2e-5, and 1e5} and batch size from {8, 16, 24} for different models. The dropout probability is 0.1. The best model configuration is selected according to the highest performance on the development set. The details of settings are shown in Appendix A. Table 1: ACSA results usi"
2021.emnlp-main.361,2020.acl-main.703,0,0.111621,"weight alloclosely to BERT (Devlin et al., 2019a) pre-training. cation. Wang et al. (2019) propose the aspect-level In comparison to this MLM method, a generation sentiment capsules model (AS-Capsules), which method can better learn the correlation between the utilizes the correlation between aspect category input and output template as two related sequences, and sentiment through shared components. Li et al. which has been demonstrated by the strong perfor(2020a) propose a novel joint model which contains mance of BART for abstractive text summarization a shared sentiment prediction layer. (Lewis et al., 2020). All the models above are classification methods, Experimental results on three standard bench- which use a separate output network to give the output label. In contrast, we investigate natural marks datasets show that both generation and MLM language generation methods by directly following methods outperform classification methods using the same pre-trained language models. Finally, gen- the pre-training process of language models. Masked Language Model Methods There is eration methods give stronger performances than MLM methods, outperforming the previous state- a line of work using the ma"
2021.emnlp-main.361,P19-1129,0,0.2711,"model the interThe sentiment polarity of price is positive (scoring: 0.1) dependencies of sentences in a text with a hierarchiThe sentiment polarity of price is neutral (scoring: 0.2) cal bidirectional LSTM. Yin et al. (2017) model the The sentiment polarity of price is negative (scoring: 0.7) task as a machine comprehension problem by conAspect category sentiment analysis structing pseudo question-answer pairs. Xue and Figure 2: ACSA as a generation task. Li (2018) extract sentiment features with CNN and selectively output aspect category related features with gating mechanisms. Xing et al. (2019), Liang encoder training scheme of BART (Lewis et al., et al. (2019) and Zhu et al. (2019) incorporate as2020), which we use as the pre-trained model. pect category information into sentence encoders Compared with classification-based methods, our in the context modeling stage. Sun et al. (2019) method does not include more network parameters, construct auxiliary sentences from the aspect cateand thus can potentially generalize better to new do- gories and convert ACSA to a sentence-pair classimains (Brown et al., 2020; Gao et al., 2020). Given fication task. Li et al. (2020b) predict the sent"
2021.emnlp-main.361,2020.ccl-1.103,0,0.315628,"s by directly following the task setting during pre-training. Experiments on several benchmarks show that our method gives the best reported results, having large advantages in few-shot and zero-shot settings. ACD <price, food> The restaurant was expensive, but the menu was great ACSA < price: negative > < food: positive > Figure 1: Example of aspect category detection (ACD) and aspect category sentiment analysis (ACSA). the input sentence (Figure 3(a))), which provides useful semantic features for ACSA and ACD classifiers. Such methods have given highly competitive results (Sun et al., 2019; Li et al., 2020b). The above classification models benefit from contextualized representations, which contain knowledge learned by pre-training over large data (Lin et al., 2019). However, their use of pre-trained knowledge can be viewed as indirect due to at least two reasons. First, the classification task is performed by using a neural network on top of pre1 Introduction trained representation, with separate network paAspect-based sentiment analysis (ABSA) is a fine- rameters. Second, the integration of aspect categrained sentiment analysis task that includes a num- gory makes the aspect-specific input re"
2021.emnlp-main.361,2020.emnlp-main.287,0,0.220388,"s by directly following the task setting during pre-training. Experiments on several benchmarks show that our method gives the best reported results, having large advantages in few-shot and zero-shot settings. ACD <price, food> The restaurant was expensive, but the menu was great ACSA < price: negative > < food: positive > Figure 1: Example of aspect category detection (ACD) and aspect category sentiment analysis (ACSA). the input sentence (Figure 3(a))), which provides useful semantic features for ACSA and ACD classifiers. Such methods have given highly competitive results (Sun et al., 2019; Li et al., 2020b). The above classification models benefit from contextualized representations, which contain knowledge learned by pre-training over large data (Lin et al., 2019). However, their use of pre-trained knowledge can be viewed as indirect due to at least two reasons. First, the classification task is performed by using a neural network on top of pre1 Introduction trained representation, with separate network paAspect-based sentiment analysis (ABSA) is a fine- rameters. Second, the integration of aspect categrained sentiment analysis task that includes a num- gory makes the aspect-specific input re"
2021.emnlp-main.361,D19-1559,0,0.0629631,"Missing"
2021.emnlp-main.361,W19-4825,0,0.0220055,"Missing"
2021.emnlp-main.361,D19-1250,0,0.0134775,"nu was great </s> food </s> MLM head The menu was great. The sentiment polarity of food is [MASK]. <s> The menu was great </s> food </s> Input sentence (a) BART classification. Template (b) Masked language model(MLM). The sentiment polarity t1 t2 t3 Encoder of price is positive t4 t5 t6 t7 t4 t5 t6 of price is Decoder x1 x2 x3 x4 x5 t0 The restaurant was too expensive <s> t1 t2 t3 The sentiment polarity (c) BART generation. Figure 3: A comparison of aspect category sentiment analysis methods. extend Schick and Schütze (2020) by automatically generating label words and templates, respectively. Petroni et al. (2019) extract relation between entities from BERT by constructing cloze-style templates. We are the first to apply such methods to ACSA, taking it as a baseline. Different from these template-based models, our final model uses BART for text generation, which better models the correlations between the input sentence and the output sentence compared with BERT. Generation Methods There has been work casting NLP problems as sequence generation tasks (Vinyals et al., 2015; Ma et al., 2017; Stanovsky and Dagan, 2018; Raffel et al., 2020), where the output is a sequence of tokens rather than a natural lan"
2021.emnlp-main.361,S14-2004,0,0.072808,"Missing"
2021.emnlp-main.361,D16-1103,0,0.024732,"the sentiment polarities toward each aspect cate- ing the sentiment classification tasks into langory. In this work, we focus on these two tasks as guage modelling tasks. In particular, as shown well as the joint task that combines both. in Figure 2, both ACSA and ACD are transPrevious studies have investigated various meth- formed into sequence-to-sequence (seq2seq) tasks, ods that treat ACSA and ACD as classification where the encoder takes the input sentence and tasks, learning aspect-specific sentence representa- the decoder generates a natural language sentence. tions (Wang et al., 2016; Ruder et al., 2016). Re- For ACD, the output follows a template stating cently, pre-trained language models (PLM) have whether the specific aspect is discussed (e.g., “The shown their effectiveness to this end (Jiang et al., hcategory_typei category is discussed”); for 2019). The main idea is to make use of pre-trained ACSA, the sentiment polarity of a specific asmodels such as BERT (Devlin et al., 2019a) for rep- pect is stated (e.g., “The sentiment polarity of resenting an aspect-specific form of the input (e.g., hgiven_categoryi is hpolarity_typei”). The by concatenating the aspect category to the end of sett"
2021.emnlp-main.361,2020.coling-main.488,0,0.0372136,"ion method, we show that jointly The basic idea is to leverage information from performing ACSA and ACD leads to better results pre-trained models by defining specific sentence than the traditional pipeline. To our knowledge, prompt in a language modelling task. Brown et al. we are the first to employ a generative pre-trained (2020) use prompt for few-shot learning in text language model to address an ACSA/ACD prob- classification tasks. Schick and Schütze (2020) lem. We release our code at https://github. rephrase inputs as cloze questions for text classificom/lgw863/ACSA-generation. cation. Schick et al. (2020) and Gao et al. (2020) 4407 positive neutral negative label Pre-trained Encoder Pre-trained Decoder <s> The menu was great </s> food </s> MLM head The menu was great. The sentiment polarity of food is [MASK]. <s> The menu was great </s> food </s> Input sentence (a) BART classification. Template (b) Masked language model(MLM). The sentiment polarity t1 t2 t3 Encoder of price is positive t4 t5 t6 t7 t4 t5 t6 of price is Decoder x1 x2 x3 x4 x5 t0 The restaurant was too expensive <s> t1 t2 t3 The sentiment polarity (c) BART generation. Figure 3: A comparison of aspect category sentiment analysis m"
2021.emnlp-main.361,D18-1139,0,0.0179809,"ence-pair classimains (Brown et al., 2020; Gao et al., 2020). Given fication task. Li et al. (2020b) predict the sentiment a new domain with completely unseen aspect cat- of an aspect category mentioned in a sentence by egories and sentiment labels, our method can be aggregating the sentiments of the words indicating applied without changing output layer structure. the aspect category in the sentence. Several joint models were proposed to avoid erIn addition to classification-based methods, we ror propagation, which perform ACD and ACSA take masked language models (MLM) as a baseline jointly. Schmitt et al. (2018) propose two joint also, for which a natural counterpart of our method models: end-to-end LSTM and end-to-end CNN, is a mask-refilling task. As shown in Figure 3(b), which produce all the aspect categories and their different from our method, the output template is corresponding sentiment polarities at once. Hu concatenated to the input, with the keyword being et al. (2019) propose constrained attention netmasked for prediction. This MLM task corresponds works (CAN) to constrain the attention weight alloclosely to BERT (Devlin et al., 2019a) pre-training. cation. Wang et al. (2019) propose the"
2021.emnlp-main.361,D18-1263,0,0.0171339,"chick and Schütze (2020) by automatically generating label words and templates, respectively. Petroni et al. (2019) extract relation between entities from BERT by constructing cloze-style templates. We are the first to apply such methods to ACSA, taking it as a baseline. Different from these template-based models, our final model uses BART for text generation, which better models the correlations between the input sentence and the output sentence compared with BERT. Generation Methods There has been work casting NLP problems as sequence generation tasks (Vinyals et al., 2015; Ma et al., 2017; Stanovsky and Dagan, 2018; Raffel et al., 2020), where the output is a sequence of tokens rather than a natural language sentence. Daza and Frank (2018) treat semantic role labelling as a sequence-to-sequence process. Li et al. (2019) solve the entity-relation extraction task as a multi-turn question answering generation method. Our work is similar in casting an NLP task as a generation task. Different from the above methods, our goal is to make the most of pre-trained knowledge in BART for ACSA. 3 Methods 3.1 Pre-trained language Models We take BERT (Devlin et al., 2019a) and BART (Lewis et al., 2020) as the pre-trai"
2021.emnlp-main.361,N19-1035,0,0.214034,"seq language models by directly following the task setting during pre-training. Experiments on several benchmarks show that our method gives the best reported results, having large advantages in few-shot and zero-shot settings. ACD <price, food> The restaurant was expensive, but the menu was great ACSA < price: negative > < food: positive > Figure 1: Example of aspect category detection (ACD) and aspect category sentiment analysis (ACSA). the input sentence (Figure 3(a))), which provides useful semantic features for ACSA and ACD classifiers. Such methods have given highly competitive results (Sun et al., 2019; Li et al., 2020b). The above classification models benefit from contextualized representations, which contain knowledge learned by pre-training over large data (Lin et al., 2019). However, their use of pre-trained knowledge can be viewed as indirect due to at least two reasons. First, the classification task is performed by using a neural network on top of pre1 Introduction trained representation, with separate network paAspect-based sentiment analysis (ABSA) is a fine- rameters. Second, the integration of aspect categrained sentiment analysis task that includes a num- gory makes the aspect-"
2021.emnlp-main.361,D15-1167,0,0.132065,"Missing"
2021.emnlp-main.361,D16-1058,0,0.033905,"potentials by castthe sentiment polarities toward each aspect cate- ing the sentiment classification tasks into langory. In this work, we focus on these two tasks as guage modelling tasks. In particular, as shown well as the joint task that combines both. in Figure 2, both ACSA and ACD are transPrevious studies have investigated various meth- formed into sequence-to-sequence (seq2seq) tasks, ods that treat ACSA and ACD as classification where the encoder takes the input sentence and tasks, learning aspect-specific sentence representa- the decoder generates a natural language sentence. tions (Wang et al., 2016; Ruder et al., 2016). Re- For ACD, the output follows a template stating cently, pre-trained language models (PLM) have whether the specific aspect is discussed (e.g., “The shown their effectiveness to this end (Jiang et al., hcategory_typei category is discussed”); for 2019). The main idea is to make use of pre-trained ACSA, the sentiment polarity of a specific asmodels such as BERT (Devlin et al., 2019a) for rep- pect is stated (e.g., “The sentiment polarity of resenting an aspect-specific form of the input (e.g., hgiven_categoryi is hpolarity_typei”). The by concatenating the aspect catego"
2021.emnlp-main.361,P18-1234,0,0.0351786,"Missing"
2021.emnlp-main.361,N16-1174,0,0.0876476,"Missing"
2021.emnlp-main.361,D17-1217,0,0.0142548,"y detection The price category is discussed The price category is not discussed (scoring: 0.9) (scoring: 0.1) 2 Related Work Aspect Category Sentiment Analysis Wang et al. (2016) propose an attention-based LSTM network, which can concentrate on different parts of a senThe restaurant was too expensive tence when different aspect categories are taken as input. Ruder et al. (2016) model the interThe sentiment polarity of price is positive (scoring: 0.1) dependencies of sentences in a text with a hierarchiThe sentiment polarity of price is neutral (scoring: 0.2) cal bidirectional LSTM. Yin et al. (2017) model the The sentiment polarity of price is negative (scoring: 0.7) task as a machine comprehension problem by conAspect category sentiment analysis structing pseudo question-answer pairs. Xue and Figure 2: ACSA as a generation task. Li (2018) extract sentiment features with CNN and selectively output aspect category related features with gating mechanisms. Xing et al. (2019), Liang encoder training scheme of BART (Lewis et al., et al. (2019) and Zhu et al. (2019) incorporate as2020), which we use as the pre-trained model. pect category information into sentence encoders Compared with classi"
2021.emnlp-main.57,W13-2322,0,0.180259,"Missing"
2021.emnlp-main.57,P18-1026,0,0.0647295,"graphs. Overall, we find that a combination of both strategies further improves the performance, showing that they are complementary for this task. 2 Related Work Approaches for AMR-to-text generation predominantly focus on English, and typically employ an encoder-decoder architecture, employing a linearized representation of the graph (Konstas et al., 2017; Ribeiro et al., 2020a). Recently, models based on the graph-to-text paradigm (Ribeiro et al., 2020b; Schmitt et al., 2021) improve over linearized approaches, explicitly encoding the AMR structure with a graph encoder (Song et al., 2018; Beck et al., 2018; Ribeiro et al., 2019; Guo et al., 2019; Cai and Lam, 2020b; Ribeiro et al., 2021). Advances in multilingual AMR parsing have focused on a variety of different languages such as Brazilian Portuguese, Chinese, Czech and Spanish (Hajiˇc et al., 2014; Xue et al., 2014; MiguelesAbraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). In contrast, little work has focused on the reverse AMR-to-text setting (Fan and Gardent, 2020). We aim to close this gap by experimenting with different data augmentation methods for efficient multilingual AMR-to-text generation. 3 Multilingual AMR-to-Text Genera"
2021.emnlp-main.57,2020.emnlp-main.195,0,0.0306526,"owing Fan and generation of text in many different languages (Da- Gardent (2020), we parse English sentences into monte and Cohen, 2018; Zhu et al., 2019). silver AMRs from parallel multilingual corpora While previous work has predominantly focused (S ILVER AMR), resulting in a dataset consisting of on monolingual English settings (Cai and Lam, grammatically correct sentences with noisy AMR 2020b; Bevilacqua et al., 2021), recent work has structures. (2) We leverage machine translation also studied multilinguality in meaning represen- (MT) and translate the English sentences from the tations (Blloshmi et al., 2020; Sheth et al., 2021). gold AMR-to-text corpus to the respective target Whereas Damonte and Cohen (2018) demonstrate languages (S ILVER S ENT), resulting in a dataset with 1 correct AMR structures but potentially unfaithful Our code and checkpoints are available at https://github.com/UKPLab/m-AMR2Text. or non-grammatical sentences. (3) We experiment 742 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 742–750 c November 7–11, 2021. 2021 Association for Computational Linguistics with utilizing the AMR-to-text corpus with both gold English AMR and sen"
2021.emnlp-main.57,2020.acl-main.119,0,0.0561986,"gies further improves the performance, showing that they are complementary for this task. 2 Related Work Approaches for AMR-to-text generation predominantly focus on English, and typically employ an encoder-decoder architecture, employing a linearized representation of the graph (Konstas et al., 2017; Ribeiro et al., 2020a). Recently, models based on the graph-to-text paradigm (Ribeiro et al., 2020b; Schmitt et al., 2021) improve over linearized approaches, explicitly encoding the AMR structure with a graph encoder (Song et al., 2018; Beck et al., 2018; Ribeiro et al., 2019; Guo et al., 2019; Cai and Lam, 2020b; Ribeiro et al., 2021). Advances in multilingual AMR parsing have focused on a variety of different languages such as Brazilian Portuguese, Chinese, Czech and Spanish (Hajiˇc et al., 2014; Xue et al., 2014; MiguelesAbraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). In contrast, little work has focused on the reverse AMR-to-text setting (Fan and Gardent, 2020). We aim to close this gap by experimenting with different data augmentation methods for efficient multilingual AMR-to-text generation. 3 Multilingual AMR-to-Text Generation In AMR-to-text generation, we transduce an AMR graph G"
2021.emnlp-main.57,2020.acl-main.640,0,0.0416205,"gies further improves the performance, showing that they are complementary for this task. 2 Related Work Approaches for AMR-to-text generation predominantly focus on English, and typically employ an encoder-decoder architecture, employing a linearized representation of the graph (Konstas et al., 2017; Ribeiro et al., 2020a). Recently, models based on the graph-to-text paradigm (Ribeiro et al., 2020b; Schmitt et al., 2021) improve over linearized approaches, explicitly encoding the AMR structure with a graph encoder (Song et al., 2018; Beck et al., 2018; Ribeiro et al., 2019; Guo et al., 2019; Cai and Lam, 2020b; Ribeiro et al., 2021). Advances in multilingual AMR parsing have focused on a variety of different languages such as Brazilian Portuguese, Chinese, Czech and Spanish (Hajiˇc et al., 2014; Xue et al., 2014; MiguelesAbraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). In contrast, little work has focused on the reverse AMR-to-text setting (Fan and Gardent, 2020). We aim to close this gap by experimenting with different data augmentation methods for efficient multilingual AMR-to-text generation. 3 Multilingual AMR-to-Text Generation In AMR-to-text generation, we transduce an AMR graph G"
2021.emnlp-main.57,2020.acl-main.747,0,0.0374779,"approaches. Training Strategies. We propose different training strategies under the setting of §3.2 in order to investigate which combination leads to stronger multilingual AMR-to-text generation. Besides training models using S ILVER AMR or S ILVER S ENT, we investigate different combinations of multi-source training also using G OLDAMR. Main Results. Table 1 shows our main results.8 First, S ILVER AMR substantially outperforms Fan and Gardent (2020) despite being trained on the same amount of silver AMR data. We believe this is because we utilize mT5, whereas Fan and Gardent (2020) use XLM (Conneau et al., 2020), and our parallel data may contain different domain data. S ILVER S ENT considerably outperforms S ILVER AMR in all metrics, despite S ILVER AMR consisting of two orders of magnitude more data. We believe the reasons are twofold: Firstly, the correct semantic structure of gold AMR annotations is necessary to learn a faithful realization; Secondly, S ILVER S ENT provides examples of the same domain as the evaluation test set. We observe similar performance to S ILVER S ENT when training on both G OLDAMR and S ILVER AMR, indicating that the combination of target domain data and gold AMR graphs"
2021.emnlp-main.57,N18-1104,0,0.0843336,"ntences into monte and Cohen, 2018; Zhu et al., 2019). silver AMRs from parallel multilingual corpora While previous work has predominantly focused (S ILVER AMR), resulting in a dataset consisting of on monolingual English settings (Cai and Lam, grammatically correct sentences with noisy AMR 2020b; Bevilacqua et al., 2021), recent work has structures. (2) We leverage machine translation also studied multilinguality in meaning represen- (MT) and translate the English sentences from the tations (Blloshmi et al., 2020; Sheth et al., 2021). gold AMR-to-text corpus to the respective target Whereas Damonte and Cohen (2018) demonstrate languages (S ILVER S ENT), resulting in a dataset with 1 correct AMR structures but potentially unfaithful Our code and checkpoints are available at https://github.com/UKPLab/m-AMR2Text. or non-grammatical sentences. (3) We experiment 742 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 742–750 c November 7–11, 2021. 2021 Association for Computational Linguistics with utilizing the AMR-to-text corpus with both gold English AMR and sentences in multi-source scenarios to enhance multilingual training. Our contributions and the organizatio"
2021.emnlp-main.57,N19-2003,0,0.0151877,"evious work (Fan and et al., 2013), and has recently received much re- Gardent, 2020) while discussing the feasibility of search interest (Ribeiro et al., 2019; Wang et al., multilingual AMR-to-text generation, has inves2020; Mager et al., 2020; Harkous et al., 2020; Fu tigated synthetically generated AMR as the only et al., 2021). AMR has applications to a range of source of silver training data. NLP tasks, including summarization (Hardy and In this paper, we aim to close this gap by providVlachos, 2018) and spoken language understand- ing an extensive analysis of different augmentation ing (Damonte et al., 2019), and has the potential techniques to cheaply acquire silver-standard mulpower of acting as an interlingua that allows the tilingual AMR-to-text data: (1) Following Fan and generation of text in many different languages (Da- Gardent (2020), we parse English sentences into monte and Cohen, 2018; Zhu et al., 2019). silver AMRs from parallel multilingual corpora While previous work has predominantly focused (S ILVER AMR), resulting in a dataset consisting of on monolingual English settings (Cai and Lam, grammatically correct sentences with noisy AMR 2020b; Bevilacqua et al., 2021), recent work ha"
2021.emnlp-main.57,W14-3348,0,0.0248803,"models using mT5base from HuggingFace (Wolf et al., 2020). We use the Adafactor optimizer (Shazeer and Stern, 2018) and employ a linearly decreasing learning rate schedule without warm-up. The hyperparameters we tune include the batch size, number of epochs and learning 4 https://tatoeba.org/ https://github.com/UKPLab/sentencetransformers/tree/master/docs/datasets 6 The English sentences of the parallel corpus are parsed using a state-of-the-art AMR parser (Cai and Lam, 2020a). rate.7 The models are evaluated in the multilingual LDC2020T07 test set, using BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), chrF++ (Popovi´c, 2015) and BERTscore (Zhang et al., 2020) metrics. We compare with a MT baseline – we generate the test set with an AMR-toEnglish model trained with T5 (Ribeiro et al., 2021) and translate the generated English sentences to the target language using MT. For a fair comparison, our MT model is based on mT5 and trained with the same data as the other approaches. Training Strategies. We propose different training strategies under the setting of §3.2 in order to investigate which combination leads to stronger multilingual AMR-to-text generation. Besides training models using S IL"
2021.emnlp-main.57,2020.emnlp-main.231,0,0.461391,"t combining both complementary sources of information further improves multilingual AMR-to-text generation. Our models surpass the previous state of the art for German, Italian, Spanish, and Chinese by a large margin.1 live-01 :ARG1 glorious :ARG0 they AMR-to En Es It Their life looks glorious. Su vida parece gloriosa. La loro vita sembra gloriosa. De Ihr Leben sieht herrlich aus. Zh 他们的⽇⼦看起来很光鲜. Figure 1: A generation example from English AMR to multiple different languages. that parsers can be effectively trained to transform multilingual text into English AMR, Mille et al. (2018, 2019) and Fan and Gardent (2020) discuss 1 Introduction the reverse task, turning meaning representations AMR-to-text generation is the task of recover- into multilingual text, as shown in Figure 1. Howing a text with the same meaning as a given Ab- ever, gold-standard multilingual AMR training data stract Meaning Representation (AMR) (Banarescu is currently scarce, and previous work (Fan and et al., 2013), and has recently received much re- Gardent, 2020) while discussing the feasibility of search interest (Ribeiro et al., 2019; Wang et al., multilingual AMR-to-text generation, has inves2020; Mager et al., 2020; Harkous et"
2021.emnlp-main.57,2021.acl-long.324,1,0.633831,"Missing"
2021.emnlp-main.57,Q19-1019,0,0.110809,"ion of both strategies further improves the performance, showing that they are complementary for this task. 2 Related Work Approaches for AMR-to-text generation predominantly focus on English, and typically employ an encoder-decoder architecture, employing a linearized representation of the graph (Konstas et al., 2017; Ribeiro et al., 2020a). Recently, models based on the graph-to-text paradigm (Ribeiro et al., 2020b; Schmitt et al., 2021) improve over linearized approaches, explicitly encoding the AMR structure with a graph encoder (Song et al., 2018; Beck et al., 2018; Ribeiro et al., 2019; Guo et al., 2019; Cai and Lam, 2020b; Ribeiro et al., 2021). Advances in multilingual AMR parsing have focused on a variety of different languages such as Brazilian Portuguese, Chinese, Czech and Spanish (Hajiˇc et al., 2014; Xue et al., 2014; MiguelesAbraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). In contrast, little work has focused on the reverse AMR-to-text setting (Fan and Gardent, 2020). We aim to close this gap by experimenting with different data augmentation methods for efficient multilingual AMR-to-text generation. 3 Multilingual AMR-to-Text Generation In AMR-to-text generation, we trans"
2021.emnlp-main.57,W14-5808,0,0.0597511,"Missing"
2021.emnlp-main.57,D18-1086,0,0.0475649,"Missing"
2021.emnlp-main.57,2020.coling-main.218,0,0.183984,"Missing"
2021.emnlp-main.57,2005.mtsummit-papers.11,0,0.216957,"between English and target languages, we thus aim to identify the best augmentations strategies to achieve multilingual generation. As our monolingual AMR-to-text training dataset, we consider the LDC2017T10 dataset (G OLDAMR), containing English AMR graphs and sentences. We evaluate our different approaches on the multilingual LDC2020T07 test set by Damonte and Cohen (2018) consisting of gold annotations for Spanish (ES), Italian (IT), German (DE) and Chinese (ZH).3 For our multilingual parallel sentence corpus we consider data from different sources. For ES, IT and DE, we use: Europarl-v7 (Koehn, 2005), an aligned corpus of European Union parlia2 For example, for AMR-to-Spanish we use the prefix “translate AMR to Spanish:”. 3 This dataset was constructed by professional translators based on the LDC2017T10 test set. We employ mT5 (Xue et al., 2021), a Transformerbased encoder-decoder architecture (Vaswani et al., 743 BLEU BERTscore ES IT DE ZH All ES IT DE ZH All MT (Fan and Gardent, 2020) Multilingual model (Fan and Gardent, 2020) 21.6 21.7 19.6 19.8 15.7 15.3 - - - - - - - MT S ILVER AMR S ILVER S ENT S ILVER AMR + G OLDAMR S ILVER S ENT + G OLDAMR S ILVER AMR + S ILVER S ENT S ILVER AMR +"
2021.emnlp-main.57,2021.eacl-main.30,0,0.0591082,"Missing"
2021.emnlp-main.57,W19-4028,0,0.0228744,"of the graph (Konstas et al., 2017; Ribeiro et al., 2020a). Recently, models based on the graph-to-text paradigm (Ribeiro et al., 2020b; Schmitt et al., 2021) improve over linearized approaches, explicitly encoding the AMR structure with a graph encoder (Song et al., 2018; Beck et al., 2018; Ribeiro et al., 2019; Guo et al., 2019; Cai and Lam, 2020b; Ribeiro et al., 2021). Advances in multilingual AMR parsing have focused on a variety of different languages such as Brazilian Portuguese, Chinese, Czech and Spanish (Hajiˇc et al., 2014; Xue et al., 2014; MiguelesAbraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). In contrast, little work has focused on the reverse AMR-to-text setting (Fan and Gardent, 2020). We aim to close this gap by experimenting with different data augmentation methods for efficient multilingual AMR-to-text generation. 3 Multilingual AMR-to-Text Generation In AMR-to-text generation, we transduce an AMR graph G to a surface realization as a sequence of tokens y = hy1 , . . . , y|y |i. As input we use an English-centric AMR graph where the output y can be realized in different languages (see Figure 1). 3.1 Approach 2017), motivated by prior work (Ribeiro et al., 2020a, 2021) that l"
2021.emnlp-main.57,P18-1150,1,0.867436,"r relatively larger graphs. Overall, we find that a combination of both strategies further improves the performance, showing that they are complementary for this task. 2 Related Work Approaches for AMR-to-text generation predominantly focus on English, and typically employ an encoder-decoder architecture, employing a linearized representation of the graph (Konstas et al., 2017; Ribeiro et al., 2020a). Recently, models based on the graph-to-text paradigm (Ribeiro et al., 2020b; Schmitt et al., 2021) improve over linearized approaches, explicitly encoding the AMR structure with a graph encoder (Song et al., 2018; Beck et al., 2018; Ribeiro et al., 2019; Guo et al., 2019; Cai and Lam, 2020b; Ribeiro et al., 2021). Advances in multilingual AMR parsing have focused on a variety of different languages such as Brazilian Portuguese, Chinese, Czech and Spanish (Hajiˇc et al., 2014; Xue et al., 2014; MiguelesAbraira et al., 2018; Sobrevilla Cabezudo and Pardo, 2019). In contrast, little work has focused on the reverse AMR-to-text setting (Fan and Gardent, 2020). We aim to close this gap by experimenting with different data augmentation methods for efficient multilingual AMR-to-text generation. 3 Multilingual"
2021.emnlp-main.57,tian-etal-2014-um,0,0.0456627,"Missing"
2021.emnlp-main.57,2020.tacl-1.2,0,0.0291043,"Missing"
2021.emnlp-main.57,2020.emnlp-demos.6,0,0.290837,"complementary sources of information further improves multilingual AMR-to-text generation. Our models surpass the previous state of the art for German, Italian, Spanish, and Chinese by a large margin.1 live-01 :ARG1 glorious :ARG0 they AMR-to En Es It Their life looks glorious. Su vida parece gloriosa. La loro vita sembra gloriosa. De Ihr Leben sieht herrlich aus. Zh 他们的⽇⼦看起来很光鲜. Figure 1: A generation example from English AMR to multiple different languages. that parsers can be effectively trained to transform multilingual text into English AMR, Mille et al. (2018, 2019) and Fan and Gardent (2020) discuss 1 Introduction the reverse task, turning meaning representations AMR-to-text generation is the task of recover- into multilingual text, as shown in Figure 1. Howing a text with the same meaning as a given Ab- ever, gold-standard multilingual AMR training data stract Meaning Representation (AMR) (Banarescu is currently scarce, and previous work (Fan and et al., 2013), and has recently received much re- Gardent, 2020) while discussing the feasibility of search interest (Ribeiro et al., 2019; Wang et al., multilingual AMR-to-text generation, has inves2020; Mager et al., 2020; Harkous et"
2021.emnlp-main.57,2021.naacl-main.41,0,0.0342624,"sh AMR graphs and sentences. We evaluate our different approaches on the multilingual LDC2020T07 test set by Damonte and Cohen (2018) consisting of gold annotations for Spanish (ES), Italian (IT), German (DE) and Chinese (ZH).3 For our multilingual parallel sentence corpus we consider data from different sources. For ES, IT and DE, we use: Europarl-v7 (Koehn, 2005), an aligned corpus of European Union parlia2 For example, for AMR-to-Spanish we use the prefix “translate AMR to Spanish:”. 3 This dataset was constructed by professional translators based on the LDC2017T10 test set. We employ mT5 (Xue et al., 2021), a Transformerbased encoder-decoder architecture (Vaswani et al., 743 BLEU BERTscore ES IT DE ZH All ES IT DE ZH All MT (Fan and Gardent, 2020) Multilingual model (Fan and Gardent, 2020) 21.6 21.7 19.6 19.8 15.7 15.3 - - - - - - - MT S ILVER AMR S ILVER S ENT S ILVER AMR + G OLDAMR S ILVER S ENT + G OLDAMR S ILVER AMR + S ILVER S ENT S ILVER AMR + S ILVER S ENT + G OLDAMR 27.6 23.3 28.3 28.2 28.5 30.7 30.4 24.2 21.2 24.3 24.9 24.6 26.4 26.1 19.4 16.9 18.9 19.4 19.2 20.6 20.5 23.3 20.1 22.2 22.9 22.3 24.2 23.4 23.6 20.4 23.4 23.9 23.7 25.5 25.1 87.1 84.5 87.3 87.6 87.3 87.8 88.0 85.7 83.7 85.7"
2021.emnlp-main.57,xue-etal-2014-interlingua,0,0.0620617,"Missing"
2021.findings-acl.150,D18-1549,0,0.0218598,"o remarkable advantage compared with large-scale back-translation. 2. BERT has little effect on correcting smaller discrepancies in morphological and syntactic levels in NMT (Section 5.1& 5.2). 3. BERT features salient promotion for MT requiring heavy context understanding and intensive knowledge, but also brings concerns around bias and fairness (Section 5.3& 5.4). To our knowledge, we are the first to detect the effectiveness of pre-training in NMT by a comparison with back-translation in a fair setting. We also contribute to the analysis of BERT in a bilingual situation. 2 vised scenarios (Artetxe et al., 2018; Lample et al., 2018), tagged back-translated sources (Caswell et al., 2019) as well as systematic analysis (Burlot and Yvon, 2018; Poncelas et al., 2018; Edunov et al., 2020). In line with Edunov et al. (2018), we aim to broaden understanding of back-translation in a large-scale manner. While their focus is on different methods that generate synthetic source sentences, ours is to investigate how large-scale pretraining compares with large-scale back-translation in boosting translation performance. BERTology Much work has discussed BERT with respect to morphology (Edmiston, 2020; Haley, 2020)"
2021.findings-acl.150,W11-2138,0,0.0336311,"ly, they first encode the inputs by BERT and use the last layer’s output as an extra memory. The Transformer NMT network uses an extra self-attention module to weigh the memory in each layer of both the encoder and decoder. The model shows a noticeable improvement in both supervised, semisupervised and unsupervised tasks, leading to the new state-of-the-art results of using BERT in NMT. Given the significant improvements achieved by their work, we adopt this model in our experiments. Back-translation Back-translation is a widely used data augmentation technology originally introduced for SMT (Bojar and Tamchyna, 2011) and then flourished in NMT (Sennrich et al., 2016). It has been studied with dual-learning frameworks (He et al., 2016), large-scale extensions (Edunov et al., 2018; Wu et al., 2019), iterative versions (Hoang et al., 2018), unsuper3 Protocol for MT Evaluation We use BLEU (Papineni et al., 2002) and 8 more focused evaluation tasks to probe MT systems with pre-trained BERT and back-translation. Below we introduce the error analysis protocols in detail. 3.1 Morphological Competence We assess the morphological competence of MT systems translating from English into morphologically rich languages,"
2021.findings-acl.150,W18-6433,0,0.0914677,"Missing"
2021.findings-acl.150,W17-4705,0,0.0227994,"-scale extensions (Edunov et al., 2018; Wu et al., 2019), iterative versions (Hoang et al., 2018), unsuper3 Protocol for MT Evaluation We use BLEU (Papineni et al., 2002) and 8 more focused evaluation tasks to probe MT systems with pre-trained BERT and back-translation. Below we introduce the error analysis protocols in detail. 3.1 Morphological Competence We assess the morphological competence of MT systems translating from English into morphologically rich languages, which is a necessity for MT systems to overcome out-of-vocabulary source tokens and flexible word orders. We take Morpheval1 (Burlot and Yvon, 2017; Burlot et al., 2018) as one of the representative test suits, consisting of a set of contrast pairs that can be triggered in the source language and evaluated in the target language (Table 1). This dataset describes three types of contrasts: the first evaluates one single morphological derivational feature such as number, gender, tense; the second evaluates agreement; the third concerns lexical replacements of the same category, testing whether morphological consistency still holds if a word is replaced by a hyponym. 1 https://github.com/franckbrl/morpheval v2 1719 Morphology Syntax Homograp"
2021.findings-acl.150,W18-6315,0,0.0177672,"n morphological and syntactic levels in NMT (Section 5.1& 5.2). 3. BERT features salient promotion for MT requiring heavy context understanding and intensive knowledge, but also brings concerns around bias and fairness (Section 5.3& 5.4). To our knowledge, we are the first to detect the effectiveness of pre-training in NMT by a comparison with back-translation in a fair setting. We also contribute to the analysis of BERT in a bilingual situation. 2 vised scenarios (Artetxe et al., 2018; Lample et al., 2018), tagged back-translated sources (Caswell et al., 2019) as well as systematic analysis (Burlot and Yvon, 2018; Poncelas et al., 2018; Edunov et al., 2020). In line with Edunov et al. (2018), we aim to broaden understanding of back-translation in a large-scale manner. While their focus is on different methods that generate synthetic source sentences, ours is to investigate how large-scale pretraining compares with large-scale back-translation in boosting translation performance. BERTology Much work has discussed BERT with respect to morphology (Edmiston, 2020; Haley, 2020), syntax (Hewitt and Manning, 2019; Lin et al., 2019; Goldberg, 2019), semantics (Ettinger, 2020; Warstadt et al., 2019; Tenney et"
2021.findings-acl.150,W19-5206,0,0.0159002,"as little effect on correcting smaller discrepancies in morphological and syntactic levels in NMT (Section 5.1& 5.2). 3. BERT features salient promotion for MT requiring heavy context understanding and intensive knowledge, but also brings concerns around bias and fairness (Section 5.3& 5.4). To our knowledge, we are the first to detect the effectiveness of pre-training in NMT by a comparison with back-translation in a fair setting. We also contribute to the analysis of BERT in a bilingual situation. 2 vised scenarios (Artetxe et al., 2018; Lample et al., 2018), tagged back-translated sources (Caswell et al., 2019) as well as systematic analysis (Burlot and Yvon, 2018; Poncelas et al., 2018; Edunov et al., 2020). In line with Edunov et al. (2018), we aim to broaden understanding of back-translation in a large-scale manner. While their focus is on different methods that generate synthetic source sentences, ours is to investigate how large-scale pretraining compares with large-scale back-translation in boosting translation performance. BERTology Much work has discussed BERT with respect to morphology (Edmiston, 2020; Haley, 2020), syntax (Hewitt and Manning, 2019; Lin et al., 2019; Goldberg, 2019), semant"
2021.findings-acl.150,D18-1045,0,0.125636,"salient promotion for MT requiring heavy context understanding and intensive knowledge, but also brings concerns around bias and fairness (Section 5.3& 5.4). To our knowledge, we are the first to detect the effectiveness of pre-training in NMT by a comparison with back-translation in a fair setting. We also contribute to the analysis of BERT in a bilingual situation. 2 vised scenarios (Artetxe et al., 2018; Lample et al., 2018), tagged back-translated sources (Caswell et al., 2019) as well as systematic analysis (Burlot and Yvon, 2018; Poncelas et al., 2018; Edunov et al., 2020). In line with Edunov et al. (2018), we aim to broaden understanding of back-translation in a large-scale manner. While their focus is on different methods that generate synthetic source sentences, ours is to investigate how large-scale pretraining compares with large-scale back-translation in boosting translation performance. BERTology Much work has discussed BERT with respect to morphology (Edmiston, 2020; Haley, 2020), syntax (Hewitt and Manning, 2019; Lin et al., 2019; Goldberg, 2019), semantics (Ettinger, 2020; Warstadt et al., 2019; Tenney et al., 2019), and world knowledge (Poerner et al., 2019; Zhou et al., 2020). Both"
2021.findings-acl.150,W15-3014,0,0.0906789,"Bojar and ∗ † Equal contribution. Corresponding author. Tamchyna, 2011; Sennrich et al., 2016), in which an auxiliary target-to-source system is trained on genuine bitext, and then used to generate synthetic text from a large monolingual corpus on the target side. The synthetic and genuine pairs are then used together to train a source-to-target MT model. An alternative method of using monolingual data is the pre-trained language model (Devlin et al., 2019; Radford et al., 2019), a neural network trained over large texts and can be incorporated into standard NMT encoder-decoder architectures (Jean et al., 2015; Gulcehre et al., 2015; Zhu et al., 2020). Pre-trained language models have led to improvements in NMT results across low-resource scenarios (Song et al., 2019), crosslingual transfers (Conneau and Lample, 2019; Liu et al., 2020) and code-switching settings (Yang et al., 2020). Among these two dominant monolingual paradigms, there has been relatively more work investigating how back-translation helps NMT. For example, initial studies show that back-translation is beneficial to machine translation by producing more fluent outputs (Edunov et al., 2020). However, relatively little work has focus"
2021.findings-acl.150,D19-1588,0,0.0133191,"e pronoun translation(right part) and antecedent location (left part). System Standard Transformer + back translation (1:0.5) + back translation (1:1) + back translation (1:2) + back translation (1:4) BERT-fused model Zh→En Triggered 377 359 306 334 344 249 En→De BLEU 29.54 28.85 27.53 27.12 26.76 30.76 Table 8: Results on idiom translation. ditioned on both left and right context to capture intra-sentence dependency which is important for understanding coreferences. On the other hand, it also shows BERT’s limitation on long-range features in document-level contexts, which is also observed by Joshi et al. (2019). As mentioned earlier in Section 4.2, one training task of BERT is to predict the next sentence. We assume that BERT is better than the standard Transformer to capture relation between two sentences and thus can improve performance on translation involving long-range features. Based on our results, however, seemingly BERT’s potential in capturing sentence relations is not thoroughly exploited by NMT architectures. 5.4 Pragmatics Table 8 shows results for idiom translation. Among all translations, the baseline triggers 377 literal errors. Back-translation makes progress on the basis of the bas"
2021.findings-acl.150,J82-2005,0,0.690142,"Missing"
2021.findings-acl.150,W19-4825,0,0.0340209,"Missing"
2021.findings-acl.150,N19-1112,0,0.110168,"s on different methods that generate synthetic source sentences, ours is to investigate how large-scale pretraining compares with large-scale back-translation in boosting translation performance. BERTology Much work has discussed BERT with respect to morphology (Edmiston, 2020; Haley, 2020), syntax (Hewitt and Manning, 2019; Lin et al., 2019; Goldberg, 2019), semantics (Ettinger, 2020; Warstadt et al., 2019; Tenney et al., 2019), and world knowledge (Poerner et al., 2019; Zhou et al., 2020). Both internal attention weights (Clark et al., 2019; Htut et al., 2019) and external task performances(Liu et al., 2019a; Zhou et al., 2020) have been used as means of investigation. Our work aligns with external evaluation. However, existing work considers a monolingual setting while we discuss these issues under a bilingual task. Related Work Pre-training in NMT Gulcehre et al. (2015) and Jean et al. (2015) are among the first to integrate language models into the decoder part of NMT. Subsequent work extends the studies by adding pre-trained representations in the encoder part (Edunov et al., 2019) or the both sides (Ramachandran et al., 2017) of NMT networks. Recent research focused on leveraging the pretra"
2021.findings-acl.150,2020.tacl-1.47,0,0.019526,"lingual corpus on the target side. The synthetic and genuine pairs are then used together to train a source-to-target MT model. An alternative method of using monolingual data is the pre-trained language model (Devlin et al., 2019; Radford et al., 2019), a neural network trained over large texts and can be incorporated into standard NMT encoder-decoder architectures (Jean et al., 2015; Gulcehre et al., 2015; Zhu et al., 2020). Pre-trained language models have led to improvements in NMT results across low-resource scenarios (Song et al., 2019), crosslingual transfers (Conneau and Lample, 2019; Liu et al., 2020) and code-switching settings (Yang et al., 2020). Among these two dominant monolingual paradigms, there has been relatively more work investigating how back-translation helps NMT. For example, initial studies show that back-translation is beneficial to machine translation by producing more fluent outputs (Edunov et al., 2020). However, relatively little work has focused on how pretrained language models contribute to translation. We fill this gap by quantitatively comparing MT models trained with pre-trained language models and back-translation under a fair large-scale setting. Specifically, f"
2021.findings-acl.150,2021.ccl-1.108,0,0.0277459,"Missing"
2021.findings-acl.150,N19-1063,0,0.0236612,"wledge via injecting prior information on the encoder part of NMT. The results for gender translation are presented in Table 9. With BERT, gender bias in MT is not mitigated. The best performance is achieved by the model trained with back-translation data in a 1:2 setting, scoring 75.1, 0.1 and 5.2 in Accuracy, ∆G and ∆S, respectively. The scores of the BERT-fused model are 71.4, 3.2, 14.6, respectively, not competitive with the baseline on Accuracy and ∆G, and even much poor on ∆S. On the one hand, this further indicates that BERT may encode unintended social correlations during pretraining (May et al., 2019; Tan and Celis, 2019), and will propagate bias to downstream MT application. On the other hand, the poor ∆S score shows that the BERT-fused model is prone to translate based on gender stereotypes, and suffer deteriorated performance when translating antistereotypical assignments. This is in line with prior observations in QA and relation classification (Poerner et al., 2019) which shows that BERT’s knowledge can come from learning stereotypical associations. 6 Conclusion We presented a quantitative study of BERT in NMT as compared with large-scale back-translation. With 8 intrinsic evaluation"
2021.findings-acl.150,W18-6307,0,0.0444307,"Missing"
2021.findings-acl.150,P02-1040,0,0.113334,"and unsupervised tasks, leading to the new state-of-the-art results of using BERT in NMT. Given the significant improvements achieved by their work, we adopt this model in our experiments. Back-translation Back-translation is a widely used data augmentation technology originally introduced for SMT (Bojar and Tamchyna, 2011) and then flourished in NMT (Sennrich et al., 2016). It has been studied with dual-learning frameworks (He et al., 2016), large-scale extensions (Edunov et al., 2018; Wu et al., 2019), iterative versions (Hoang et al., 2018), unsuper3 Protocol for MT Evaluation We use BLEU (Papineni et al., 2002) and 8 more focused evaluation tasks to probe MT systems with pre-trained BERT and back-translation. Below we introduce the error analysis protocols in detail. 3.1 Morphological Competence We assess the morphological competence of MT systems translating from English into morphologically rich languages, which is a necessity for MT systems to overcome out-of-vocabulary source tokens and flexible word orders. We take Morpheval1 (Burlot and Yvon, 2017; Burlot et al., 2018) as one of the representative test suits, consisting of a set of contrast pairs that can be triggered in the source language an"
2021.findings-acl.150,W19-5353,0,0.0609227,"Missing"
2021.findings-acl.150,W19-5354,0,0.0136281,"ntactic agreement over long distances, discontiguous verbparticle constructions, transliteration of names and faithful translation of polarity (Table 1). 3.3 Semantic Competence Semantics helps MT enforce meaning preservation and handle data sparsity. We measure semantic competence from the ambiguity of content words, conjunctions and pronouns, corresponding to tasks of homograph translation, conjunction disambiguation, and pronoun coreference resolution, respectively. First, homograph translation requires models to determine the intended sense of polysemous words in context. We adopt MUCOW3 (Raganato et al., 2019), a lexical ambiguity benchmark in which a sentence containing an ambiguous word is paired with a correct reference and an incorrect modified translation with the ambiguous word being replaced by a word of a different sense. Second, NMT should theoretically be able to handle conjunctions with variant senses if the encoder cap3.4 We further evaluate systems on 3 challenging problems involving pragmatic inference: idiom translation, commonsense reasoning and gender bias. First, idiom translation still presents a difficulty because the meaning of idioms is non-compositional and non-literal, makin"
2021.findings-acl.150,D17-1039,0,0.0393221,"Missing"
2021.findings-acl.150,E17-2060,0,0.0256511,"ates the English conjunction but into two different German conjunctions aber or sondern. The former can be used after a positive or a negative clause, while the latter is only used after a negative clause when expressing a contradiction. Lastly, for coreference resolution, we adopt ContraPro5 (M¨uller et al., 2018) to evaluate the accuracy when models translate the English pronoun it to its German counterparts es (it), sie (she) and er (he), based on a correct understanding of antecedents. We evaluate whether MT models can generate coherent and grammatical sentences. We adopt the LingEval972 (Sennrich, 2017), a test set of contrastive translation pairs for analysis of a number of syntactic phenomena including syntactic agreement over long distances, discontiguous verbparticle constructions, transliteration of names and faithful translation of polarity (Table 1). 3.3 Semantic Competence Semantics helps MT enforce meaning preservation and handle data sparsity. We measure semantic competence from the ambiguity of content words, conjunctions and pronouns, corresponding to tasks of homograph translation, conjunction disambiguation, and pronoun coreference resolution, respectively. First, homograph tra"
2021.findings-acl.150,P16-1009,0,0.250457,"Bahdanau et al., 2014; Vaswani et al., 2017). One reason for its success is the availability of large amounts of training resources such as parallel corpora with high quality. For low-resource languages or domain-specific settings, monolingual data have also been effectively used by NMT systems (Zhang and Zong, 2016; Siddhant et al., 2020), providing rich linguistic features for translation. Two lines of work have been done on leveraging monolingual corpora to improve translation quality. One approach is back-translation (Bojar and ∗ † Equal contribution. Corresponding author. Tamchyna, 2011; Sennrich et al., 2016), in which an auxiliary target-to-source system is trained on genuine bitext, and then used to generate synthetic text from a large monolingual corpus on the target side. The synthetic and genuine pairs are then used together to train a source-to-target MT model. An alternative method of using monolingual data is the pre-trained language model (Devlin et al., 2019; Radford et al., 2019), a neural network trained over large texts and can be incorporated into standard NMT encoder-decoder architectures (Jean et al., 2015; Gulcehre et al., 2015; Zhu et al., 2020). Pre-trained language models have"
2021.findings-acl.150,L18-1005,0,0.0219319,"ambiguous word is paired with a correct reference and an incorrect modified translation with the ambiguous word being replaced by a word of a different sense. Second, NMT should theoretically be able to handle conjunctions with variant senses if the encoder cap3.4 We further evaluate systems on 3 challenging problems involving pragmatic inference: idiom translation, commonsense reasoning and gender bias. First, idiom translation still presents a difficulty because the meaning of idioms is non-compositional and non-literal, making word-by-word translation incorrect. We use the CIBB dataset 6 (Shao et al., 2018), in which a blacklist consisting literal translation of idiom characters is constructed and once translations from NMT trigger the blacklist, the literal translation errors can be counted to score the systems. Another demanding competence for NMT is commonsense reasoning. He et al. (2020) build 4 2 3 https: //github.com/rsennrich/lingeval97 https://github.com/Helsinki-NLP/MuCoW Pragmatic Competence 5 6 https://github.com/m-popovic https://github.com/ZurichNLP/ContraPro https://github.com/sythello/CIBB-dataset 1720 En→De Auth (M ) Synth (M ) 2.250 4.500 4.500 9.000 18.00 a bilingual test suite"
2021.findings-acl.150,2020.acl-main.252,0,0.0219666,"intensive understanding; 3) pre-training on huge datasets may introduce inductive social bias thus affects translation fairness. 1 Introduction Neural machine translation (NMT) has shown promising results as an end-to-end approach to automatic translation (Sutskever et al., 2014; Bahdanau et al., 2014; Vaswani et al., 2017). One reason for its success is the availability of large amounts of training resources such as parallel corpora with high quality. For low-resource languages or domain-specific settings, monolingual data have also been effectively used by NMT systems (Zhang and Zong, 2016; Siddhant et al., 2020), providing rich linguistic features for translation. Two lines of work have been done on leveraging monolingual corpora to improve translation quality. One approach is back-translation (Bojar and ∗ † Equal contribution. Corresponding author. Tamchyna, 2011; Sennrich et al., 2016), in which an auxiliary target-to-source system is trained on genuine bitext, and then used to generate synthetic text from a large monolingual corpus on the target side. The synthetic and genuine pairs are then used together to train a source-to-target MT model. An alternative method of using monolingual data is the"
2021.findings-acl.150,P19-1164,0,0.0138541,"nki-NLP/MuCoW Pragmatic Competence 5 6 https://github.com/m-popovic https://github.com/ZurichNLP/ContraPro https://github.com/sythello/CIBB-dataset 1720 En→De Auth (M ) Synth (M ) 2.250 4.500 4.500 9.000 18.00 a bilingual test suite which grounds commonsense knowledge into lexical ambiguity, contextual syntactic ambiguity and contextless syntactic ambiguity (Appendix A.3). Each source sentence has one ambiguity type and corresponds to two contrastive translations. We use this test suite 7 to measure commonsense knowledge and inference of NMT outputs. Lastly, we estimate gender bias. Following Stanovsky et al. (2019), we use the WinoMT8 dataset to extract gender features from translations and evaluate them against the gold annotations. 4 Table 2: Corpora statistics of sentence pairs. of the context-aware representation from BERT encoder: HB = BERT (x), Experimental Setup l HE = Data and Baseline l l−1 l−1 l−1 HDS = attnM S (HD , HD , HD ), 1 l L L l HD = attnB (HDS , HE , HE ) 2  l +attnE (HDS , HB , HB ) , 8 BERT-fused NMT 9 10 https://github.com/tjunlp-lab/CommonMT https://github.com/gabrielStanovsky/mt gender https://nlp.stanford.edu/projects/nmt/ https://github.com/pytorch/fairseq (2) (3) where attnM"
2021.findings-acl.150,D19-1286,0,0.01305,"c analysis (Burlot and Yvon, 2018; Poncelas et al., 2018; Edunov et al., 2020). In line with Edunov et al. (2018), we aim to broaden understanding of back-translation in a large-scale manner. While their focus is on different methods that generate synthetic source sentences, ours is to investigate how large-scale pretraining compares with large-scale back-translation in boosting translation performance. BERTology Much work has discussed BERT with respect to morphology (Edmiston, 2020; Haley, 2020), syntax (Hewitt and Manning, 2019; Lin et al., 2019; Goldberg, 2019), semantics (Ettinger, 2020; Warstadt et al., 2019; Tenney et al., 2019), and world knowledge (Poerner et al., 2019; Zhou et al., 2020). Both internal attention weights (Clark et al., 2019; Htut et al., 2019) and external task performances(Liu et al., 2019a; Zhou et al., 2020) have been used as means of investigation. Our work aligns with external evaluation. However, existing work considers a monolingual setting while we discuss these issues under a bilingual task. Related Work Pre-training in NMT Gulcehre et al. (2015) and Jean et al. (2015) are among the first to integrate language models into the decoder part of NMT. Subsequent work exten"
2021.findings-acl.150,D19-1430,0,0.0184667,"f both the encoder and decoder. The model shows a noticeable improvement in both supervised, semisupervised and unsupervised tasks, leading to the new state-of-the-art results of using BERT in NMT. Given the significant improvements achieved by their work, we adopt this model in our experiments. Back-translation Back-translation is a widely used data augmentation technology originally introduced for SMT (Bojar and Tamchyna, 2011) and then flourished in NMT (Sennrich et al., 2016). It has been studied with dual-learning frameworks (He et al., 2016), large-scale extensions (Edunov et al., 2018; Wu et al., 2019), iterative versions (Hoang et al., 2018), unsuper3 Protocol for MT Evaluation We use BLEU (Papineni et al., 2002) and 8 more focused evaluation tasks to probe MT systems with pre-trained BERT and back-translation. Below we introduce the error analysis protocols in detail. 3.1 Morphological Competence We assess the morphological competence of MT systems translating from English into morphologically rich languages, which is a necessity for MT systems to overcome out-of-vocabulary source tokens and flexible word orders. We take Morpheval1 (Burlot and Yvon, 2017; Burlot et al., 2018) as one of th"
2021.findings-acl.150,2020.emnlp-main.208,0,0.0341308,"c and genuine pairs are then used together to train a source-to-target MT model. An alternative method of using monolingual data is the pre-trained language model (Devlin et al., 2019; Radford et al., 2019), a neural network trained over large texts and can be incorporated into standard NMT encoder-decoder architectures (Jean et al., 2015; Gulcehre et al., 2015; Zhu et al., 2020). Pre-trained language models have led to improvements in NMT results across low-resource scenarios (Song et al., 2019), crosslingual transfers (Conneau and Lample, 2019; Liu et al., 2020) and code-switching settings (Yang et al., 2020). Among these two dominant monolingual paradigms, there has been relatively more work investigating how back-translation helps NMT. For example, initial studies show that back-translation is beneficial to machine translation by producing more fluent outputs (Edunov et al., 2020). However, relatively little work has focused on how pretrained language models contribute to translation. We fill this gap by quantitatively comparing MT models trained with pre-trained language models and back-translation under a fair large-scale setting. Specifically, for pre-trained language models, we reimplement B"
2021.findings-acl.150,D16-1160,0,0.0114145,"c tasks which involve intensive understanding; 3) pre-training on huge datasets may introduce inductive social bias thus affects translation fairness. 1 Introduction Neural machine translation (NMT) has shown promising results as an end-to-end approach to automatic translation (Sutskever et al., 2014; Bahdanau et al., 2014; Vaswani et al., 2017). One reason for its success is the availability of large amounts of training resources such as parallel corpora with high quality. For low-resource languages or domain-specific settings, monolingual data have also been effectively used by NMT systems (Zhang and Zong, 2016; Siddhant et al., 2020), providing rich linguistic features for translation. Two lines of work have been done on leveraging monolingual corpora to improve translation quality. One approach is back-translation (Bojar and ∗ † Equal contribution. Corresponding author. Tamchyna, 2011; Sennrich et al., 2016), in which an auxiliary target-to-source system is trained on genuine bitext, and then used to generate synthetic text from a large monolingual corpus on the target side. The synthetic and genuine pairs are then used together to train a source-to-target MT model. An alternative method of using"
2021.findings-acl.161,N19-1078,0,0.0166181,"the knowledge between different entity categories. Traditional Models Yang et al. (2018) Ma and Hovy (2016) Gui et al. (2020) Yamada et al. (2020)* Sequence Labeling BERT Sequence Labeling BART Few-shot Friendly Models Wiseman and Stratos (2019) Template BART multi-template BART P 91.93 89.60 P 90.51 91.72 R 91.54 91.63 R 93.34 93.40 F 90.77 91.21 92.02 94.30 91.73 90.60 F 89.94 91.90 92.55 Table 2: Model performance on the CoNLL03 .The original result of BERT (Devlin et al., 2019) was not achieved with the current version of the library as discussed and reported by Stanislawek et al. (2019), Akbik et al. (2019) and Gui et al. (2020). * indicates training on external data. Models BERT Ours PER 75.71 84.49 ORG 77.59 72.61 LOC* 60.72 71.98 MISC* 60.39 73.37 Overall 69.62 75.59 Table 3: In-domain Few-shot performance on the CoNLL03. * indicates it is a few-shot entity type. 5.3 Cross-domain Few-Shot NER Result We evaluate the model performance when the target entity types are different from the source-domain, and only a small amount of labeled data is available for training. We simulate the cross-domain low-resource data scenarios by random sampling training instances from a large training set as the tr"
2021.findings-acl.161,Q16-1026,0,0.220088,"Restaurant All I want is salmon character title Movie Who played jack in Nightmare Before Christmas Figure 1: Example of NER on different domains. Named entity recognition (NER) is a fundamental task in natural language processing, which identifies mention spans from text inputs according to pre-defined entity categories (Tjong Kim Sang and De Meulder, 2003), such as location, person, organization, etc. The current dominant methods use a sequential neural network such as BiLSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019) is used to represent the input text, and softmax (Chiu and Nichols, 2016; Strubell et al., 2017; Cui and Zhang, 2019) or CRF (Lample et al., 2016; Ma and Hovy, 2016; Luo et al., 2020) output layers to assign named entity tags (e.g. organization, person and location) or non-entity tags Corresponding Author News dish Introduction ∗ person location on each input token. Such a system is illustrated in Figure 2(a). Neural NER models require large labeled training data, which can be available for certain domains such as news, but scarce in most other domains. Ideally, it would be desirable to transfer knowledge from the resource-rich news domain so that a model can be u"
2021.findings-acl.161,D19-1422,1,0.920248,"e Movie Who played jack in Nightmare Before Christmas Figure 1: Example of NER on different domains. Named entity recognition (NER) is a fundamental task in natural language processing, which identifies mention spans from text inputs according to pre-defined entity categories (Tjong Kim Sang and De Meulder, 2003), such as location, person, organization, etc. The current dominant methods use a sequential neural network such as BiLSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019) is used to represent the input text, and softmax (Chiu and Nichols, 2016; Strubell et al., 2017; Cui and Zhang, 2019) or CRF (Lample et al., 2016; Ma and Hovy, 2016; Luo et al., 2020) output layers to assign named entity tags (e.g. organization, person and location) or non-entity tags Corresponding Author News dish Introduction ∗ person location on each input token. Such a system is illustrated in Figure 2(a). Neural NER models require large labeled training data, which can be available for certain domains such as news, but scarce in most other domains. Ideally, it would be desirable to transfer knowledge from the resource-rich news domain so that a model can be used in target domains based on a few labeled"
2021.findings-acl.161,N19-1423,0,0.528354,", the MIT Restaurant, and the ATIS (low-resource task), respectively. 1 Restaurant All I want is salmon character title Movie Who played jack in Nightmare Before Christmas Figure 1: Example of NER on different domains. Named entity recognition (NER) is a fundamental task in natural language processing, which identifies mention spans from text inputs according to pre-defined entity categories (Tjong Kim Sang and De Meulder, 2003), such as location, person, organization, etc. The current dominant methods use a sequential neural network such as BiLSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019) is used to represent the input text, and softmax (Chiu and Nichols, 2016; Strubell et al., 2017; Cui and Zhang, 2019) or CRF (Lample et al., 2016; Ma and Hovy, 2016; Luo et al., 2020) output layers to assign named entity tags (e.g. organization, person and location) or non-entity tags Corresponding Author News dish Introduction ∗ person location on each input token. Such a system is illustrated in Figure 2(a). Neural NER models require large labeled training data, which can be available for certain domains such as news, but scarce in most other domains. Ideally, it would be desirable to trans"
2021.findings-acl.161,2020.emnlp-main.181,0,0.033135,"Missing"
2021.findings-acl.161,N16-1030,0,0.0607911,"ghtmare Before Christmas Figure 1: Example of NER on different domains. Named entity recognition (NER) is a fundamental task in natural language processing, which identifies mention spans from text inputs according to pre-defined entity categories (Tjong Kim Sang and De Meulder, 2003), such as location, person, organization, etc. The current dominant methods use a sequential neural network such as BiLSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019) is used to represent the input text, and softmax (Chiu and Nichols, 2016; Strubell et al., 2017; Cui and Zhang, 2019) or CRF (Lample et al., 2016; Ma and Hovy, 2016; Luo et al., 2020) output layers to assign named entity tags (e.g. organization, person and location) or non-entity tags Corresponding Author News dish Introduction ∗ person location on each input token. Such a system is illustrated in Figure 2(a). Neural NER models require large labeled training data, which can be available for certain domains such as news, but scarce in most other domains. Ideally, it would be desirable to transfer knowledge from the resource-rich news domain so that a model can be used in target domains based on a few labeled instances. In practice, howe"
2021.findings-acl.161,2020.acl-main.703,0,0.108989,"the network parameters of the NER model. While being less costly, these methods cannot improve the neural representation for cross-domain instances. Second, these methods rely on similar textual patterns between the source domain and the target domain. This strong assumption may hinder the model performance when the target-domain writing style is different from the source domain. To address these issues, we investigate a template-based method for exploiting the few-shot learning potential of generative pre-trained language models to sequence labeling. Specifically, as shown in Figure 2, BART (Lewis et al., 2020) is fine-tuned with pre-defined templates filled by corresponding labeled entities. For example, we can define templates such as “hcandidate_spani is a hentity_typei entity”, where hentity_typei can be “person” and “location”, etc. Given the sentence “ACL will be held in Bangkok”, where “Bangkok” has a gold label “location”, we can train BART using a filled template “Bangkok is a location entity” as the decoder output for the input sentence. In terms of non-entity spans, we use a template “hcandidate_spani is not a named entity”, so that negative output sequences can also be sampled. During in"
2021.findings-acl.161,P19-1511,0,0.022439,"Missing"
2021.findings-acl.161,K19-1058,0,0.013176,"method can better transfer the knowledge between different entity categories. Traditional Models Yang et al. (2018) Ma and Hovy (2016) Gui et al. (2020) Yamada et al. (2020)* Sequence Labeling BERT Sequence Labeling BART Few-shot Friendly Models Wiseman and Stratos (2019) Template BART multi-template BART P 91.93 89.60 P 90.51 91.72 R 91.54 91.63 R 93.34 93.40 F 90.77 91.21 92.02 94.30 91.73 90.60 F 89.94 91.90 92.55 Table 2: Model performance on the CoNLL03 .The original result of BERT (Devlin et al., 2019) was not achieved with the current version of the library as discussed and reported by Stanislawek et al. (2019), Akbik et al. (2019) and Gui et al. (2020). * indicates training on external data. Models BERT Ours PER 75.71 84.49 ORG 77.59 72.61 LOC* 60.72 71.98 MISC* 60.39 73.37 Overall 69.62 75.59 Table 3: In-domain Few-shot performance on the CoNLL03. * indicates it is a few-shot entity type. 5.3 Cross-domain Few-Shot NER Result We evaluate the model performance when the target entity types are different from the source-domain, and only a small amount of labeled data is available for training. We simulate the cross-domain low-resource data scenarios by random sampling training instances from a large t"
2021.findings-acl.161,D17-1283,0,0.0657282,"s salmon character title Movie Who played jack in Nightmare Before Christmas Figure 1: Example of NER on different domains. Named entity recognition (NER) is a fundamental task in natural language processing, which identifies mention spans from text inputs according to pre-defined entity categories (Tjong Kim Sang and De Meulder, 2003), such as location, person, organization, etc. The current dominant methods use a sequential neural network such as BiLSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019) is used to represent the input text, and softmax (Chiu and Nichols, 2016; Strubell et al., 2017; Cui and Zhang, 2019) or CRF (Lample et al., 2016; Ma and Hovy, 2016; Luo et al., 2020) output layers to assign named entity tags (e.g. organization, person and location) or non-entity tags Corresponding Author News dish Introduction ∗ person location on each input token. Such a system is illustrated in Figure 2(a). Neural NER models require large labeled training data, which can be available for certain domains such as news, but scarce in most other domains. Ideally, it would be desirable to transfer knowledge from the resource-rich news domain so that a model can be used in target domains b"
2021.findings-acl.161,P19-1233,0,0.0176617,"19), Ziyadi et al. (2020) and Huang et al. (2020) when it comes to few-shot settings. To the best of our knowledge, we are the first to employ a generative pre-trained language model to address a few-shot sequence labeling problem. We release our code at https: //github.com/Nealcly/templateNER. 2 Related Work Neural methods have given competitive performance in NER. Some methods (Chiu and Nichols, 2016; Strubell et al., 2017) treat NER as a local classification problem at each input token, while other methods use CRF (Ma and Hovy, 2016) or a sequence-to-sequence framework (Zhang et al., 2018; Liu et al., 2019). Cui and Zhang (2019) and Gui et al. (2020) use a label attention network and Bayesian neural networks, respectively. Yamada et al. (2020) use entity-aware pre-training and obtain state-of-the-art results on NER. These approaches are similar to ours in the sense that parameters can be tuned in supervised learning, but unlike our method, they are designed for prescribed named entity types, which makes their domain adaptation costly with new few-shot entity types. Our work is motivated by distance-based fewshot NER, which aims to minimize domainadaptation cost. Wiseman and Stratos (2019) copy t"
2021.findings-acl.161,P16-1101,0,0.243796,"mas Figure 1: Example of NER on different domains. Named entity recognition (NER) is a fundamental task in natural language processing, which identifies mention spans from text inputs according to pre-defined entity categories (Tjong Kim Sang and De Meulder, 2003), such as location, person, organization, etc. The current dominant methods use a sequential neural network such as BiLSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019) is used to represent the input text, and softmax (Chiu and Nichols, 2016; Strubell et al., 2017; Cui and Zhang, 2019) or CRF (Lample et al., 2016; Ma and Hovy, 2016; Luo et al., 2020) output layers to assign named entity tags (e.g. organization, person and location) or non-entity tags Corresponding Author News dish Introduction ∗ person location on each input token. Such a system is illustrated in Figure 2(a). Neural NER models require large labeled training data, which can be available for certain domains such as news, but scarce in most other domains. Ideally, it would be desirable to transfer knowledge from the resource-rich news domain so that a model can be used in target domains based on a few labeled instances. In practice, however, a challenge is"
2021.findings-acl.161,D19-1250,0,0.0257147,"lf training on external data, yet yields better results. There is a line of work using templates to solve natural language understanding tasks. The basic idea is to leverage information from pre-trained models, by defining specific sentence templates in a language modeling task. Brown et al. (2020) first use prompt for few-shot learning in text classification tasks. Schick and Schütze (2020) rephrase inputs as cloze questions for text classification. Schick et al. (2020) and Gao et al. (2020) extend Schick and Schütze (2020) by automatically generating label words and templates, respectively. Petroni et al. (2019) extract relation between entities from BERT by constructing cloze-style templates. Sun et al. (2019) use templates to construct auxiliary sentences, and transform aspect sentiment task as a sentence-pair classification task. Our work is in line with exploiting pre-trained language model for templates-based NLP. While previous work considers sentence-level task as masked language modeling or uses language models to score a whole sentence, our method uses a language model to assign a score for each span given an input sentence. To our knowledge, we are the first to apply template-based method t"
2021.findings-acl.161,2020.coling-main.488,0,0.0189172,"ion (c) Training of template-based method. The template we use here is “hxi:j i is a hyk i entity&quot;. Figure 2: Overview of NER methods. rely on self training on external data, yet yields better results. There is a line of work using templates to solve natural language understanding tasks. The basic idea is to leverage information from pre-trained models, by defining specific sentence templates in a language modeling task. Brown et al. (2020) first use prompt for few-shot learning in text classification tasks. Schick and Schütze (2020) rephrase inputs as cloze questions for text classification. Schick et al. (2020) and Gao et al. (2020) extend Schick and Schütze (2020) by automatically generating label words and templates, respectively. Petroni et al. (2019) extract relation between entities from BERT by constructing cloze-style templates. Sun et al. (2019) use templates to construct auxiliary sentences, and transform aspect sentiment task as a sentence-pair classification task. Our work is in line with exploiting pre-trained language model for templates-based NLP. While previous work considers sentence-level task as masked language modeling or uses language models to score a whole sentence, our method"
2021.findings-acl.161,N19-1035,0,0.026598,"natural language understanding tasks. The basic idea is to leverage information from pre-trained models, by defining specific sentence templates in a language modeling task. Brown et al. (2020) first use prompt for few-shot learning in text classification tasks. Schick and Schütze (2020) rephrase inputs as cloze questions for text classification. Schick et al. (2020) and Gao et al. (2020) extend Schick and Schütze (2020) by automatically generating label words and templates, respectively. Petroni et al. (2019) extract relation between entities from BERT by constructing cloze-style templates. Sun et al. (2019) use templates to construct auxiliary sentences, and transform aspect sentiment task as a sentence-pair classification task. Our work is in line with exploiting pre-trained language model for templates-based NLP. While previous work considers sentence-level task as masked language modeling or uses language models to score a whole sentence, our method uses a language model to assign a score for each span given an input sentence. To our knowledge, we are the first to apply template-based method to sequence labeling. 3 Background We give the formal definition of few shot named entity recognition"
2021.findings-acl.161,P19-1533,0,0.292014,"In practice, however, a challenge is that entity categories can be different across different domains. As shown in Figure 1, the system is required to identify location and person in the news domain, but character and title in the movie domain. Both a softmax layer and CRF layer require a consistent label set between training and testing. As a result, given a new target domain, the output layer needs adjustment and training must be conducted again using both source and target domain, which can be costly. A recent line of work investigates the setting of few-shot NER by using distance metrics (Wiseman and Stratos, 2019; Yang and Katiyar, 2020; Ziyadi et al., 2020). The main idea is to train a similarity function based on instances in the source domain, and then make use of the similarity function in the target domain as a nearest neighbor criterion for few-shot NER. Compared with traditional methods, distancebased methods largely reduce the domain adapta1835 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1835–1845 August 1–6, 2021. ©2021 Association for Computational Linguistics tion cost, especially for scenarios where the number of target domains is large. Their performa"
2021.findings-acl.161,P17-1114,0,0.0395145,"Missing"
2021.findings-acl.161,2020.emnlp-main.523,0,0.210394,"employ a generative pre-trained language model to address a few-shot sequence labeling problem. We release our code at https: //github.com/Nealcly/templateNER. 2 Related Work Neural methods have given competitive performance in NER. Some methods (Chiu and Nichols, 2016; Strubell et al., 2017) treat NER as a local classification problem at each input token, while other methods use CRF (Ma and Hovy, 2016) or a sequence-to-sequence framework (Zhang et al., 2018; Liu et al., 2019). Cui and Zhang (2019) and Gui et al. (2020) use a label attention network and Bayesian neural networks, respectively. Yamada et al. (2020) use entity-aware pre-training and obtain state-of-the-art results on NER. These approaches are similar to ours in the sense that parameters can be tuned in supervised learning, but unlike our method, they are designed for prescribed named entity types, which makes their domain adaptation costly with new few-shot entity types. Our work is motivated by distance-based fewshot NER, which aims to minimize domainadaptation cost. Wiseman and Stratos (2019) copy the token-level label from nearest neighbors by retrieving a list of labeled sentences. Yang and Katiyar (2020) improve Wiseman and Stratos"
2021.findings-acl.161,C18-1327,1,0.903203,"Missing"
2021.findings-acl.161,2020.emnlp-main.516,0,0.142368,"llenge is that entity categories can be different across different domains. As shown in Figure 1, the system is required to identify location and person in the news domain, but character and title in the movie domain. Both a softmax layer and CRF layer require a consistent label set between training and testing. As a result, given a new target domain, the output layer needs adjustment and training must be conducted again using both source and target domain, which can be costly. A recent line of work investigates the setting of few-shot NER by using distance metrics (Wiseman and Stratos, 2019; Yang and Katiyar, 2020; Ziyadi et al., 2020). The main idea is to train a similarity function based on instances in the source domain, and then make use of the similarity function in the target domain as a nearest neighbor criterion for few-shot NER. Compared with traditional methods, distancebased methods largely reduce the domain adapta1835 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1835–1845 August 1–6, 2021. ©2021 Association for Computational Linguistics tion cost, especially for scenarios where the number of target domains is large. Their performance under standard in-do"
2021.findings-acl.442,N18-3011,0,0.046304,"Missing"
2021.findings-acl.442,W13-2322,0,0.0466126,"he original method without supervision on attention. As the decoder applies multi-head attention, we design the SA approach, in which the attention distributions of all heads are averaged to compute the attention loss. In this way, we consider the multi-head attention as a supervised attention channel. The SMA approach is designed as in Section 4.3, in which only the first head is a supervised attention channel. In SCE and CE approaches, we used SCE and CE loss function to supervise the attention, respectively. 5.2 AMR-to-text Generation Task and Model : Abstract meaning representation (AMR) (Banarescu et al., 2013) is a semantic graph representation that is independent of the syntactic realization of a sentence. In the graph, nodes represent concepts and edges represent semantic relations between the concepts. AMR-to-text generation is to generate sentences from AMR graphs. We use the AMR dataset LDC2015E86, which contains 16,833 training samples, 1368 development samples, and 1371 test samples. We use the model2 of Mager et al. (2020) on this task, which is a GPT-2 (Radford et al., 2019) model with fine-tuning. Aligner: We apply lemma matching to build the attention supervision as shown in Figure 1. Th"
2021.findings-acl.442,J93-2003,0,0.101517,"ning ideal alignments is infeasible or extremely costly, for most NLG tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment committee to assess.”, the ideal alignment of the last word “assess” is node (10). As the names of (8) and (10) are the same, it is not easy to pick (10) exactly. On the other hand, it is much easier to obtain a candidate set containing both (8) and (10) and be rather confident that the ideal alignment is in the set. For different tasks, both EM-based algorithms (Brown et al., 1993; Pourdamghani et al., 2014) or rule-based methods (Flanigan et al., 2014) can be used to obtain such ambiguous alignments. However, little work has discussed making use of ambiguous labels for supervised attention. We investigate the generalized supervised attention (GSA), where the supervision signal aligns a target word to multiple possible source items (named the quasi alignment), although only a subset of the items are the true alignment targets. The multiple source items are named candidate set of the quasi alignment. A generalized supervised attention framework is built for various text"
2021.findings-acl.442,P16-1184,0,0.0197889,"ion supervision and design the Summation Cross-Entropy to deal with the ambiguity in quasi alignments. Learning with ambiguous labels has been widely studied, in which the true label is not precisely annotated but in a candidate label set. In crosslingual Part-of-Speech, annotations are derived for low resource languages from cross-language projection, which results in partial or uncertain labels. To solve this problem, T¨ackstr¨om et al. (2013) proposed a partially observed conditional random field (CRF) (Lafferty et al., 2001) method, Wisniewski et al. (2014) made a history-based model, and Buys and Botha (2016) proposed an HMM-based model. SCE is designed for training attention weights using ambiguous labels. Xu et al. (2020) also study learning from ambiguous labels (called partial label learning) in classification tasks. Their method is based on constructing similar and dissimilar pairs of samples. However, supervised attention is not a traditional classification problem. The label spaces are various in different samples, making it difficult to construct similar pairs. Thus, the method is not suitable for GSA. 3 3.1 Basic Model Encoder-decoder Model with Attention Encoder-decoder models, including"
2021.findings-acl.442,D14-1179,0,0.0473474,"Missing"
2021.findings-acl.442,P19-1042,0,0.018163,"Comparison of the test attention of SCE structure. least two alignments. For data-to-text and AMR-to-text generation, SMA outperforms SA. On the other hand, SA performs better than SMA for text summarization. One possible reason is that the summarization dataset has much higher alignment coverage and multialignment coverage and the alignment accuracy may also be higher; consequently, supervised attention works so well that automatic attention becomes unnecessary or even distracting. 5.5 lated using the one-tailed sign test with bootstrap resampling on the test set of all three tasks following Chollampatt et al. (2019): • For data-to-text, we compare the Rouge-L score of SMA-SCE to the result of SA-CE. • For AMR-to-text, we compare the BLEU score of SMA-SCE to the result of SA-CE. • For summarization, we compare the Rouge-L score of SA-SCE to the result of SA-CE. Significance Test To assess the evidence of significance, we perform significance tests on GSA. The p-value is calcuThe p-value results are shown in Table 4, which show that the improvements are significant. 4997 Task P-value Data-to-Text 6.5489e-12 Amr-to-Text 5.5795e-10 Summarization 3.925e-5 Generated Matching 1 Matching 2 Matching 3 Matching 4"
2021.findings-acl.442,P14-1134,0,0.0254297,"tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment committee to assess.”, the ideal alignment of the last word “assess” is node (10). As the names of (8) and (10) are the same, it is not easy to pick (10) exactly. On the other hand, it is much easier to obtain a candidate set containing both (8) and (10) and be rather confident that the ideal alignment is in the set. For different tasks, both EM-based algorithms (Brown et al., 1993; Pourdamghani et al., 2014) or rule-based methods (Flanigan et al., 2014) can be used to obtain such ambiguous alignments. However, little work has discussed making use of ambiguous labels for supervised attention. We investigate the generalized supervised attention (GSA), where the supervision signal aligns a target word to multiple possible source items (named the quasi alignment), although only a subset of the items are the true alignment targets. The multiple source items are named candidate set of the quasi alignment. A generalized supervised attention framework is built for various text generation tasks with alignment relationships between target words and so"
2021.findings-acl.442,N19-1357,0,0.01962,"captioning (Anderson et al., 2018), dialogue systems (Liu et al., 2018), and so on. The attention mechanism (Bahdanau et al., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the corresponding author. automatic weights do not necessarily encode prior knowledge, such as the alignments between input and output (Jain and Wallace, 2019). To alleviate this problem, supervised attention was considered (Liu et al., 2016; Mi et al., 2016; Kamigaito et al., 2017; Nguyen et al., 2018; Nguyen and Nguyen, 2018), which shows that human knowledge is helpful for guiding the learning process of attention models. Previous work on supervised attention assumes access to ideal alignments. Unfortunately, obtaining ideal alignments is infeasible or extremely costly, for most NLG tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment co"
2021.findings-acl.442,I17-2002,0,0.0507966,"., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the corresponding author. automatic weights do not necessarily encode prior knowledge, such as the alignments between input and output (Jain and Wallace, 2019). To alleviate this problem, supervised attention was considered (Liu et al., 2016; Mi et al., 2016; Kamigaito et al., 2017; Nguyen et al., 2018; Nguyen and Nguyen, 2018), which shows that human knowledge is helpful for guiding the learning process of attention models. Previous work on supervised attention assumes access to ideal alignments. Unfortunately, obtaining ideal alignments is infeasible or extremely costly, for most NLG tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment committee to assess.”, the ideal alignment of the last word “assess” is node (10). As the names of (8) and (10) are the same,"
2021.findings-acl.442,N19-1238,0,0.015392,"est attention of SCE structure. least two alignments. For data-to-text and AMR-to-text generation, SMA outperforms SA. On the other hand, SA performs better than SMA for text summarization. One possible reason is that the summarization dataset has much higher alignment coverage and multialignment coverage and the alignment accuracy may also be higher; consequently, supervised attention works so well that automatic attention becomes unnecessary or even distracting. 5.5 lated using the one-tailed sign test with bootstrap resampling on the test set of all three tasks following Chollampatt et al. (2019): • For data-to-text, we compare the Rouge-L score of SMA-SCE to the result of SA-CE. • For AMR-to-text, we compare the BLEU score of SMA-SCE to the result of SA-CE. • For summarization, we compare the Rouge-L score of SA-SCE to the result of SA-CE. Significance Test To assess the evidence of significance, we perform significance tests on GSA. The p-value is calcuThe p-value results are shown in Table 4, which show that the improvements are significant. 4997 Task P-value Data-to-Text 6.5489e-12 Amr-to-Text 5.5795e-10 Summarization 3.925e-5 Generated Matching 1 Matching 2 Matching 3 Matching 4"
2021.findings-acl.442,P18-2027,0,0.0126983,"arget contain the same information. We report the details of model structures and hyper-parameters in the appendix. 5.1 Data-to-text Generation Task and Model : We consider the Abstract GENeration DAtaset (AGENDA) (Ammar et al., 2018), which contains pairs of a literature abstract and a knowledge graph extracted from the abstract. The nodes in the knowledge graphs are entity types, such as “Task” and “Method”. The edges are the relations between different entities, including “COMPARE”, “PART-OF”, and so on. We use the training, development, and test splits of 38,720/1000/1000, as Ammar et al. (2018) does. We use GraphWriter1 (Koncel-Kedziorski et al., 2019) on this task. The encoder of this model is a graph transformer and the decoder is an RNN decoder with attention and copying mechanism. More detail is introduced in the appendix. Aligner: The source items of this task include entities and relations, as shown in Figure 3. We use our string matching aligner to extract the alignments from target words to the source entities and extend our aligner for the alignments of relations, such as aligning target words “use” and “apply” to source relation “USED-FOR”. For the details of 1 https://git"
2021.findings-acl.442,C16-1291,0,0.276334,"lignments, which specify candidate sets of alignments and are much easier to obtain than ideal alignments. We design a Summation Cross-Entropy (SCE) loss and a Supervised Multiple Attention (SMA) structure to accommodate quasi alignments. Experiments on three text generation tasks demonstrate that GSA improves generation performance and is robust against errors in attention supervision. 1 Introduction The encoder-decoder framework has been applied to various natural language generation (NLG) tasks, such as neural machine translation (Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Liu et al., 2016), text generation (Wiseman et al., 2017; Puduppully et al., 2019), text summarization (Liu and Lapata, 2019; Lin et al., 2018), image captioning (Anderson et al., 2018), dialogue systems (Liu et al., 2018), and so on. The attention mechanism (Bahdanau et al., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the"
2021.findings-acl.442,P18-1138,0,0.0212933,"to accommodate quasi alignments. Experiments on three text generation tasks demonstrate that GSA improves generation performance and is robust against errors in attention supervision. 1 Introduction The encoder-decoder framework has been applied to various natural language generation (NLG) tasks, such as neural machine translation (Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Liu et al., 2016), text generation (Wiseman et al., 2017; Puduppully et al., 2019), text summarization (Liu and Lapata, 2019; Lin et al., 2018), image captioning (Anderson et al., 2018), dialogue systems (Liu et al., 2018), and so on. The attention mechanism (Bahdanau et al., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the corresponding author. automatic weights do not necessarily encode prior knowledge, such as the alignments between input and output (Jain and Wallace, 2019). To alleviate this problem, supervised attention"
2021.findings-acl.442,D19-1387,0,0.0234071,"s. We design a Summation Cross-Entropy (SCE) loss and a Supervised Multiple Attention (SMA) structure to accommodate quasi alignments. Experiments on three text generation tasks demonstrate that GSA improves generation performance and is robust against errors in attention supervision. 1 Introduction The encoder-decoder framework has been applied to various natural language generation (NLG) tasks, such as neural machine translation (Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Liu et al., 2016), text generation (Wiseman et al., 2017; Puduppully et al., 2019), text summarization (Liu and Lapata, 2019; Lin et al., 2018), image captioning (Anderson et al., 2018), dialogue systems (Liu et al., 2018), and so on. The attention mechanism (Bahdanau et al., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the corresponding author. automatic weights do not necessarily encode prior knowledge, such as the alignments"
2021.findings-acl.442,D15-1166,0,0.432645,"ed Attention method (GSA) based on quasi alignments, which specify candidate sets of alignments and are much easier to obtain than ideal alignments. We design a Summation Cross-Entropy (SCE) loss and a Supervised Multiple Attention (SMA) structure to accommodate quasi alignments. Experiments on three text generation tasks demonstrate that GSA improves generation performance and is robust against errors in attention supervision. 1 Introduction The encoder-decoder framework has been applied to various natural language generation (NLG) tasks, such as neural machine translation (Cho et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Liu et al., 2016), text generation (Wiseman et al., 2017; Puduppully et al., 2019), text summarization (Liu and Lapata, 2019; Lin et al., 2018), image captioning (Anderson et al., 2018), dialogue systems (Liu et al., 2018), and so on. The attention mechanism (Bahdanau et al., 2015) plays a significant role in the framework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of"
2021.findings-acl.442,2020.acl-main.167,0,0.0305511,"Missing"
2021.findings-acl.442,D16-1249,0,0.0571723,"and design the Summation Cross-Entropy to deal with the ambiguity in quasi alignments. Learning with ambiguous labels has been widely studied, in which the true label is not precisely annotated but in a candidate label set. In crosslingual Part-of-Speech, annotations are derived for low resource languages from cross-language projection, which results in partial or uncertain labels. To solve this problem, T¨ackstr¨om et al. (2013) proposed a partially observed conditional random field (CRF) (Lafferty et al., 2001) method, Wisniewski et al. (2014) made a history-based model, and Buys and Botha (2016) proposed an HMM-based model. SCE is designed for training attention weights using ambiguous labels. Xu et al. (2020) also study learning from ambiguous labels (called partial label learning) in classification tasks. Their method is based on constructing similar and dissimilar pairs of samples. However, supervised attention is not a traditional classification problem. The label spaces are various in different samples, making it difficult to construct similar pairs. Thus, the method is not suitable for GSA. 3 3.1 Basic Model Encoder-decoder Model with Attention Encoder-decoder models, including"
2021.findings-acl.442,K16-1028,0,0.203015,"and design the Summation Cross-Entropy to deal with the ambiguity in quasi alignments. Learning with ambiguous labels has been widely studied, in which the true label is not precisely annotated but in a candidate label set. In crosslingual Part-of-Speech, annotations are derived for low resource languages from cross-language projection, which results in partial or uncertain labels. To solve this problem, T¨ackstr¨om et al. (2013) proposed a partially observed conditional random field (CRF) (Lafferty et al., 2001) method, Wisniewski et al. (2014) made a history-based model, and Buys and Botha (2016) proposed an HMM-based model. SCE is designed for training attention weights using ambiguous labels. Xu et al. (2020) also study learning from ambiguous labels (called partial label learning) in classification tasks. Their method is based on constructing similar and dissimilar pairs of samples. However, supervised attention is not a traditional classification problem. The label spaces are various in different samples, making it difficult to construct similar pairs. Thus, the method is not suitable for GSA. 3 3.1 Basic Model Encoder-decoder Model with Attention Encoder-decoder models, including"
2021.findings-acl.442,C18-1193,0,0.0855731,"ework, which automatically extracts the alignments between the target and the source for predicting the next target output. One disadvantage of the vanilla attention mechanisms is that the ∗ The work was done when Yixian Liu and Xinyu Zhang were students of Shanghaitech University. Kewei Tu is the corresponding author. automatic weights do not necessarily encode prior knowledge, such as the alignments between input and output (Jain and Wallace, 2019). To alleviate this problem, supervised attention was considered (Liu et al., 2016; Mi et al., 2016; Kamigaito et al., 2017; Nguyen et al., 2018; Nguyen and Nguyen, 2018), which shows that human knowledge is helpful for guiding the learning process of attention models. Previous work on supervised attention assumes access to ideal alignments. Unfortunately, obtaining ideal alignments is infeasible or extremely costly, for most NLG tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment committee to assess.”, the ideal alignment of the last word “assess” is node (10). As the names of (8) and (10) are the same, it is not easy to pick (10) exactly. On the ot"
2021.findings-acl.442,D14-1048,0,0.0173769,"s is infeasible or extremely costly, for most NLG tasks. For example in Figure 1, for the AMR-to-text generation task, given the AMR graph for sentence “From among them, pick out 50 for submission to an assessment committee to assess.”, the ideal alignment of the last word “assess” is node (10). As the names of (8) and (10) are the same, it is not easy to pick (10) exactly. On the other hand, it is much easier to obtain a candidate set containing both (8) and (10) and be rather confident that the ideal alignment is in the set. For different tasks, both EM-based algorithms (Brown et al., 1993; Pourdamghani et al., 2014) or rule-based methods (Flanigan et al., 2014) can be used to obtain such ambiguous alignments. However, little work has discussed making use of ambiguous labels for supervised attention. We investigate the generalized supervised attention (GSA), where the supervision signal aligns a target word to multiple possible source items (named the quasi alignment), although only a subset of the items are the true alignment targets. The multiple source items are named candidate set of the quasi alignment. A generalized supervised attention framework is built for various text generation tasks with align"
2021.findings-acl.442,P19-1195,0,0.0153126,"est attention of SCE structure. least two alignments. For data-to-text and AMR-to-text generation, SMA outperforms SA. On the other hand, SA performs better than SMA for text summarization. One possible reason is that the summarization dataset has much higher alignment coverage and multialignment coverage and the alignment accuracy may also be higher; consequently, supervised attention works so well that automatic attention becomes unnecessary or even distracting. 5.5 lated using the one-tailed sign test with bootstrap resampling on the test set of all three tasks following Chollampatt et al. (2019): • For data-to-text, we compare the Rouge-L score of SMA-SCE to the result of SA-CE. • For AMR-to-text, we compare the BLEU score of SMA-SCE to the result of SA-CE. • For summarization, we compare the Rouge-L score of SA-SCE to the result of SA-CE. Significance Test To assess the evidence of significance, we perform significance tests on GSA. The p-value is calcuThe p-value results are shown in Table 4, which show that the improvements are significant. 4997 Task P-value Data-to-Text 6.5489e-12 Amr-to-Text 5.5795e-10 Summarization 3.925e-5 Generated Matching 1 Matching 2 Matching 3 Matching 4"
2021.findings-acl.442,Q13-1001,0,0.0722298,"Missing"
2021.findings-acl.442,D17-1239,0,0.0353923,"Missing"
2021.findings-acl.442,D14-1187,0,0.0468567,"Missing"
2021.findings-acl.449,2020.acl-main.463,0,0.0179136,"rization is the task of automatically generating a concise, salient, coherent and fluent summary of a given set of documents (Radev et al., 2002). Thanks to the advance in neural network models and the availability of large-scale labeled datasets, recent research has achieved promising progress on summarizing monologic texts such as news articles (Paulus et al., 2018; Gehrmann et al., 2018; Liu and Lapata, 2019; Liu et al., 2020), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents (Bender and Koller, 2020), has received significantly less attention from the summarization research community. One main reason is the paucity of a suitable summarization dataset built on dialogue texts. Most existing research uses the AMI meeting corpus (Carletta et al., 2005), which consists of 137 dialogues obtained from virtual multi-party meeting recordings. However, research on the corpus is limited to its Summary from SAMSum: Ryan is in Italy while Leo is working hard and wishing he could win the lottery. Figure 1: An example from D IALOG S UM dataset compared with an example from SAMSum dataset. small scale. S"
2021.findings-acl.449,D18-1547,0,0.0330492,"Missing"
2021.findings-acl.449,2020.emnlp-main.336,0,0.0905716,"ey important relations between main events, and identifying discourse relations and using proper phrases to express them in summaries can be challenging for summarization systems (Xu et al., 2020). Take Figure 1 (a) for example, the human annotated summary connects two main events (underlined) using “since” to express their causal relation explicitly. However, the causal relation between those two events are not explicitly expressed in the dialogue, and the distance between them is long. Multiple turns usually correspond to more complicated discourse structure and relation. Also, similar with Chen and Yang (2020), we find that model performance decreases when the number of dialogue turns grows (See Appendix B). To better evaluate model ability to disambiguate discourse relations in D IALOG S UM, we first collect discourse connectives from Penn Discourse Treebank (Miltsakaki et al., 2004), and check whether these connectives are included in summaries in the testset. If the three reference summaries of a dialogue all contain connectives, we assume that the dialogues have strong discourse signals. We choose 70 dialogues from D IALOG S UM in this way. We then ask linguists who specialize in discourse to e"
2021.findings-acl.449,2020.acl-main.130,1,0.893686,"Missing"
2021.findings-acl.449,D18-1443,0,0.0674089,"))))))) Leo: Yeah. They seem nice. (‘A`) Ryan: That&apos;s all???? I need more reactions!!!!!!!!!! Leo: I&apos;m tied to this office and working like a slave. AM I SUPPOSED TO SAY ""I AM SO JEALOUS!!!!!!!!""?! ! ! … Introduction Text summarization is the task of automatically generating a concise, salient, coherent and fluent summary of a given set of documents (Radev et al., 2002). Thanks to the advance in neural network models and the availability of large-scale labeled datasets, recent research has achieved promising progress on summarizing monologic texts such as news articles (Paulus et al., 2018; Gehrmann et al., 2018; Liu and Lapata, 2019; Liu et al., 2020), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents (Bender and Koller, 2020), has received significantly less attention from the summarization research community. One main reason is the paucity of a suitable summarization dataset built on dialogue texts. Most existing research uses the AMI meeting corpus (Carletta et al., 2005), which consists of 137 dialogues obtained from virtual multi-party meeting recordings. However, research on the co"
2021.findings-acl.449,D19-5409,0,0.264245,"eceived significantly less attention from the summarization research community. One main reason is the paucity of a suitable summarization dataset built on dialogue texts. Most existing research uses the AMI meeting corpus (Carletta et al., 2005), which consists of 137 dialogues obtained from virtual multi-party meeting recordings. However, research on the corpus is limited to its Summary from SAMSum: Ryan is in Italy while Leo is working hard and wishing he could win the lottery. Figure 1: An example from D IALOG S UM dataset compared with an example from SAMSum dataset. small scale. SAMSum (Gliwa et al., 2019) is a recently released written online dialogue summarization dataset, which contains 16k online chats with corresponding summaries. However, it focuses on conversations via messenger apps, which are rather short (around 94 tokens per conversation) and their language style and topics also differ from spoken daily dialogues. A comparison between the real-life scenario dialogue and online chat is shown in Figure 1. Online-chat messages contain unique tokens (e.g., “BTW”), emoticons (e.g., “:)”) and emojis (e.g., “ ”). In contrast, daily conversations have a different 5062 Findings of the Associa"
2021.findings-acl.449,J95-2003,0,0.661715,"Missing"
2021.findings-acl.449,N19-1238,0,0.081358,"Missing"
2021.findings-acl.449,D19-1051,0,0.0754933,"Missing"
2021.findings-acl.449,2020.emnlp-main.750,0,0.227527,"Missing"
2021.findings-acl.449,2020.acl-main.703,0,0.0336097,"rameters for news summarization3 , but changing the minimum length to 15. We train the 6-layer Transformer model with Adam (Kingma and Ba, 2014) for 100,000 steps. Copy attention mechanism is applied and the dropout rate is set to 0.1. U NI LM V 2 U NI LM V 2 (Bao et al., 2020) is a recently released pretrained language model for autoencoding and partially autoregressive language modeling. Here we use U NI LM V 2 BASE as a strong abstractive model. For dialogue summarization, we train the model with Adam for 100,000 steps with 2,000 warmup steps and learning rate is set to 1.5e−5 . BART BART (Lewis et al., 2020) is an encoderdecoder Transformer model pretrained on a large corpus using a denoising autoencoder task. We use the large version of BART and finetune it with 5,000 training steps/200 warmup steps for dialogue summarization. Learning rate is set to 3e−5 . 3.2 Results Table 5 presents the experimental results. In general, we find that non-pretrained abstractive models outperform LEAD (Table 4), and the best results are achieved by pretrained models, despite the fact that BART LARGE and U NI LM V 2 BASE are pretrained on monologic texts. Extractive Summary vs Abstractive Summary Transformer give"
2021.findings-acl.449,I17-1099,0,0.036943,"chats, but real-life dialogues contain business negotiation (Figure 1(a)). Intuitively, automatically summarizing such dialogues can help a business find common needs or complaints from customers. With the rise of personal assisting chatbots, summarizing dialogues from different aspects of daily life can also be useful for personal record management and other applications. We introduce Real-Life Scenario Dialogue Summarization (D IALOG S UM), a large-scale summarization dataset for dialogues. Dialogue data for D IALOG S UM are collected from three public dialogue corpora, namely Dailydialog (Li et al., 2017), DREAM (Sun et al., 2019) and MuTual (Cui et al., 2020), as well as an English speaking practice website. These datasets contain face-to-face spoken dialogues that cover a wide range of daily-life topics, including schooling, work, medication, shopping, leisure, travel. Most conversations take place between friends, colleagues, and between service providers and customers. We clean and preprocess the dialogue data into a unified format, and ask annotators to summarize them from an observer perspective. Topics are also manually labeled for each dialogue. An example of D IALOG S UM is shown in F"
2021.findings-acl.449,W04-1013,0,0.143348,"10.13 29.11 27.52 6.78 27.31 24.15 6.25 22.73 EXT- ORACLE R1 50.38 55.12 52.08 29.79 44.60 37.90 R2 28.55 30.55 31.5 8.81 17.37 13.88 RL 46.58 51.24 46.72 22.65 39.38 34.04 Table 4: Corpora statistics and extractive methods on CNN/DailyMail, NY Times, XSum, SAMSum and D IALOG S UM. Part of results is from Narayan et al. (2018). All results are computed on test sets. For D IALOG S UM, the results are the average of multi-reference results. test set, we provide three summaries written and checked by different annotators. For each test dialogue, we compare its and compute their pair-wise ROUGE (Lin, 2004) scores. Table 3 reports their averaged F1 scores of ROUGE-1 (R1), ROUGE2 (R2) and ROUGE-L (RL). We see R2 is relatively low while RL is high, which suggests that annotators’ usage of language is variable, but the main content and logical order are mostly the same. 2.4 Characteristics of D IALOG S UM We empirically compare D IALOG S UM with existing news summarization datasets and SAMSum. CNN/DailyMail (Hermann et al., 2015), NY Times (Sandhaus, 2008) and XSum (Narayan et al., 2018) are large-scale summarization datasets from the news domain, written in a monologic structure. XSum is a dataset"
2021.findings-acl.449,D19-1387,1,0.819956,"seem nice. (‘A`) Ryan: That&apos;s all???? I need more reactions!!!!!!!!!! Leo: I&apos;m tied to this office and working like a slave. AM I SUPPOSED TO SAY ""I AM SO JEALOUS!!!!!!!!""?! ! ! … Introduction Text summarization is the task of automatically generating a concise, salient, coherent and fluent summary of a given set of documents (Radev et al., 2002). Thanks to the advance in neural network models and the availability of large-scale labeled datasets, recent research has achieved promising progress on summarizing monologic texts such as news articles (Paulus et al., 2018; Gehrmann et al., 2018; Liu and Lapata, 2019; Liu et al., 2020), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents (Bender and Koller, 2020), has received significantly less attention from the summarization research community. One main reason is the paucity of a suitable summarization dataset built on dialogue texts. Most existing research uses the AMI meeting corpus (Carletta et al., 2005), which consists of 137 dialogues obtained from virtual multi-party meeting recordings. However, research on the corpus is limited to its"
2021.findings-acl.449,2021.naacl-main.56,1,0.838473,"Missing"
2021.findings-acl.449,miltsakaki-etal-2004-penn,0,0.138161,"vents (underlined) using “since” to express their causal relation explicitly. However, the causal relation between those two events are not explicitly expressed in the dialogue, and the distance between them is long. Multiple turns usually correspond to more complicated discourse structure and relation. Also, similar with Chen and Yang (2020), we find that model performance decreases when the number of dialogue turns grows (See Appendix B). To better evaluate model ability to disambiguate discourse relations in D IALOG S UM, we first collect discourse connectives from Penn Discourse Treebank (Miltsakaki et al., 2004), and check whether these connectives are included in summaries in the testset. If the three reference summaries of a dialogue all contain connectives, we assume that the dialogues have strong discourse signals. We choose 70 dialogues from D IALOG S UM in this way. We then ask linguists who specialize in discourse to evaluate model outputs and give scores from {−1, 0, 1}, where 1 means that the generated descriptions of main events are reasonable and contain correct discourse connectives, 0 means that the descriptions are good but contain no discourse connectives and −1 means that the descript"
2021.findings-acl.449,D18-1206,0,0.0638485,"Missing"
2021.findings-acl.449,D14-1162,0,0.0845504,"Missing"
2021.findings-acl.449,2020.emnlp-main.748,0,0.531908,"reactions!!!!!!!!!! Leo: I&apos;m tied to this office and working like a slave. AM I SUPPOSED TO SAY ""I AM SO JEALOUS!!!!!!!!""?! ! ! … Introduction Text summarization is the task of automatically generating a concise, salient, coherent and fluent summary of a given set of documents (Radev et al., 2002). Thanks to the advance in neural network models and the availability of large-scale labeled datasets, recent research has achieved promising progress on summarizing monologic texts such as news articles (Paulus et al., 2018; Gehrmann et al., 2018; Liu and Lapata, 2019; Liu et al., 2020), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents (Bender and Koller, 2020), has received significantly less attention from the summarization research community. One main reason is the paucity of a suitable summarization dataset built on dialogue texts. Most existing research uses the AMI meeting corpus (Carletta et al., 2005), which consists of 137 dialogues obtained from virtual multi-party meeting recordings. However, research on the corpus is limited to its Summary from SAMSum: Ryan is in Italy while Leo is"
2021.findings-acl.449,D19-1462,0,0.0692126,"Missing"
2021.findings-acl.449,J02-4001,0,0.397685,"l with. 1 Summary from DIALOGSUM: #Person_1# and #Person_2# agree to sign an agreement since #Person_1# could speed up the delivery as #Person_2# hopes. (b) Dialogue from SAMSum: … Leo: BTW what are those pics? Ryan: Pics from Italy!!! :):):):))))))))) Leo: Yeah. They seem nice. (‘A`) Ryan: That&apos;s all???? I need more reactions!!!!!!!!!! Leo: I&apos;m tied to this office and working like a slave. AM I SUPPOSED TO SAY ""I AM SO JEALOUS!!!!!!!!""?! ! ! … Introduction Text summarization is the task of automatically generating a concise, salient, coherent and fluent summary of a given set of documents (Radev et al., 2002). Thanks to the advance in neural network models and the availability of large-scale labeled datasets, recent research has achieved promising progress on summarizing monologic texts such as news articles (Paulus et al., 2018; Gehrmann et al., 2018; Liu and Lapata, 2019; Liu et al., 2020), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents (Bender and Koller, 2020), has received significantly less attention from the summarization research community. One main reason is the paucity of"
2021.findings-acl.449,Q19-1014,0,0.120783,"Missing"
2021.findings-acl.449,J05-2005,0,0.642475,"Missing"
2021.findings-acl.449,2020.acl-main.451,0,0.106567,"lower than human. Model-generated summaries have the Table 8: Human evaluation on discourse relations, with corresponding ROUGE scores on the sub-test set. Avg. stands for the averaged score here. highest scores on Fluency, while lowest on Consistency. It suggests that although model-generated summaries are grammatical and fluent, they still contain factual errors. Discourse Relation Reasonable summaries should convey important relations between main events, and identifying discourse relations and using proper phrases to express them in summaries can be challenging for summarization systems (Xu et al., 2020). Take Figure 1 (a) for example, the human annotated summary connects two main events (underlined) using “since” to express their causal relation explicitly. However, the causal relation between those two events are not explicitly expressed in the dialogue, and the distance between them is long. Multiple turns usually correspond to more complicated discourse structure and relation. Also, similar with Chen and Yang (2020), we find that model performance decreases when the number of dialogue turns grows (See Appendix B). To better evaluate model ability to disambiguate discourse relations in D I"
2021.findings-acl.449,P18-1205,0,0.022492,"be detected from the conversation. Annotators are also asked to write a short (around 3 tokens) topic for each dialogue. Appendix A shows the list of topics. Data Cleaning and Pre-Processing We delete non-English characters, correct typos and grammatical errors, and further filter out duplicated data based on text similarity. After deduplicating, proportions of the data sources are summarized in Table 2. Because of different data processing methods and annotation procedures, original dialogues in DailyDialog, DREAM and MuTual are in different formats. We follow previous work (Li et al., 2017; Zhang et al., 2018; Budzianowski et al., 2018; Dinan et al., 2019) and preprocess them into a biturn dialogue flow, merging continuous turns of the same speaker into one utterance. Also, we add tags (e.g. #Person 1# and #Person 2# in Figure 1(a)) before each dialogue turn, to distinguish speakers. The final D IALOG S UM dataset contains 13,460 dialogues, which are divided into training (12,460), validation (500) and test (500) sets. 2.2 Annotation We ask annotators to write dialogue summaries based on following criteria: the summary should (1) convey the most salient information of the dialogue and; (2) be brie"
2021.findings-acl.450,P19-1358,0,0.0275336,"these models all face challenges on the proposed dialogue understanding task. The DEQA dataset will release for research use. 1 Q2 A2 Co-reference Chain (OntoNotes style) Chain 1 (IDENT) Chain 3 (IDENT) 1.6-8 a clean house 1.12-12 grandma 2.5-5 it 2.8-8 she (b) Chain 2 (IDENT) 3.1-1 she 3.4-9 head janitor at St. 4.3-3 she Mary’s Hospital 5.4-4 she 6.3-3 she 4.5-6 that job Introduction Driven by the growth of interest in social chatbot, online customer service and virtual mobile assistant, social dialogue systems have received increasing research attention (Cui et al., 2017; Zhou et al., 2018; Hancock et al., 2019). The current dominant method has been sequence-to-sequence models, trained over large dialogue data end-to-end. Such models use neural network architectures such as Transformer (Vaswani et al., 2017) to encode a user utterance and a dialogue history before generating a system utterance (Adiwardana et al., 2020; Roller et al., 2020; Bao et al., 2020). A major advantage is the use of standard and general model architecture, which facilitates end-to-end training process over large scale dialogue text (Shang et al., 2015; Zhang et al., 2018, 2019, 2020). 1 Q1 A1 IDENT denotes the entities in a co"
2021.findings-acl.450,I17-1099,0,0.0241126,"gues include: • Ellipsis 1) Zero anaphora (Noun phrase ellipsis) U1 : “你喜欢邦乔维的音乐吗？” (“Do you like music of Bon Jovi?”) U2 : “是的(Yes)，我(I)喜欢(like)。” The noun phrase “邦乔维的音乐” (“music of Bon Jovi”) is omitted in the second utterance. 2) Verbal phrase ellipsis U1 : “I like the V6 engine of Audi S4.” U2 : “So do I.” Here, “do” is a trigger word which indicates the ellipsis of verbal phrase “like the V6 engine of Audi S4”. • Anaphor 1) Personal pronoun In this case, “Swifty” and “Taylor Swift” are coreference. 2.3 Data Annotation The English dialogue data are sourced from the DailyDialogue dataset (Li et al., 2017). The Chinese dialogue data are collected by ourself from Douban3 , a Chinese online forum. We randomly sample a subset of dialogues from the above two dataset respectively, and then annotate these dialogues in question answering. For annotation, the first step is to identify ellipsis, anaphor and co-reference phenomenons of utterances in dialogue data. For each utterance, annotators determine whether the meaning of the utterance is complete when ignoring the dialogue context. If the meaning of an utterance is determined as incomplete, we can further identify ellipsis. Both zero anaphora and v"
2021.findings-acl.450,P17-4017,0,0.0165461,"esentative dialogue models show that these models all face challenges on the proposed dialogue understanding task. The DEQA dataset will release for research use. 1 Q2 A2 Co-reference Chain (OntoNotes style) Chain 1 (IDENT) Chain 3 (IDENT) 1.6-8 a clean house 1.12-12 grandma 2.5-5 it 2.8-8 she (b) Chain 2 (IDENT) 3.1-1 she 3.4-9 head janitor at St. 4.3-3 she Mary’s Hospital 5.4-4 she 6.3-3 she 4.5-6 that job Introduction Driven by the growth of interest in social chatbot, online customer service and virtual mobile assistant, social dialogue systems have received increasing research attention (Cui et al., 2017; Zhou et al., 2018; Hancock et al., 2019). The current dominant method has been sequence-to-sequence models, trained over large dialogue data end-to-end. Such models use neural network architectures such as Transformer (Vaswani et al., 2017) to encode a user utterance and a dialogue history before generating a system utterance (Adiwardana et al., 2020; Roller et al., 2020; Bao et al., 2020). A major advantage is the use of standard and general model architecture, which facilitates end-to-end training process over large scale dialogue text (Shang et al., 2015; Zhang et al., 2018, 2019, 2020)."
2021.findings-acl.450,D18-1134,0,0.015898,"llenges, namely CoQA (Reddy et al., 2018) and QuAC (Choi et al., 2018). Rather 6 The average numbers of tokens in the answer types of entity, phrase, clause and fragment are 1.22, 2.15, 9.07, 5.48 in English dialogue data and 1.95, 3.28, 6.86 and 5.77 characters in Chinese dialogue data. than understanding the meaning of a given passage/document through the form of conversational question answering, the proposed task focuses on measuring the capability of understanding the dialogue itself. Besides the two challenges, several conversational machine reading/comprehension datasets were proposed (Elgohary et al., 2018; Dinan et al., 2018; Huang et al., 2018; Saeidi et al., 2018). The most common characteristic of these datasets are that their questions are open-domain and sequentially (or contextually) related, which shows a recent recognition in the research community that understanding the semantics of a complete conversation, including historical question and answer contexts, is crucial for these tasks. Our work is similar in spirit, but concentrating on clarification requests. Clarification Request in Dialogue Clarification requests (CR) in dialogue are mainly motivated by acoustic understanding and se"
2021.findings-acl.450,D19-1462,0,0.135062,"mplicit mentions by using zero pronouns. Take Table 1 (a) as an example, where the dialogue history consists of 7 utterances and the second utterance contains a pronoun “it”. At this point, we can measure system understanding of the dialogue state by checking whether the system can resolve the anaphora concerning “a clean house”. Our goal is to provide a large-scale benchmark and to evaluate the performance of social chatbot systems on dialogue understanding concerning entities. One way to define the task is to cast it as a co-reference resolution problem (Yin et al., 2017; Kong et al., 2019; Quan et al., 2019), where a benchmark can be constructed by manually labeling co-reference information on a dialogue dataset, as shown in Table 1 (b). However, such a benchmark does not fully meet our goal because a separate model is necessary for achieving co-reference resolution, and it may be challenging to seamlessly integrate such a co-reference module into a dialogue model being tested. We take a different method, checking dialogue understanding of dialogue systems by inserting clarification requests (Schlangen, 2004; Stoyanchev and Johnston, 2015) into dialogues, and evaluating the response of dialogue s"
2021.findings-acl.450,P05-1030,0,0.0693166,"ntextually) related, which shows a recent recognition in the research community that understanding the semantics of a complete conversation, including historical question and answer contexts, is crucial for these tasks. Our work is similar in spirit, but concentrating on clarification requests. Clarification Request in Dialogue Clarification requests (CR) in dialogue are mainly motivated by acoustic understanding and semantic understanding (Schlangen, 2004; Stoyanchev and Johnston, 2015). They are used mainly as a way to establish mutual knowledge or grounding in communication (Gabsdil, 2003; Rieser and Moore, 2005). Purver et al. (2003) proposed to classify the forms of clarification requests into 8 categories, including non-reprise clarifications, reprise sentences, reprise sluices, reprise fragments, gaps, gap fillers, conventional and other. Rodr´ıguez and Schlangen (2004) further summarized the surface forms, intonations and functions of clarification requests in spoken dialogue systems. Ginzburg (2016) detailed the semantics of dialogue and the fundamental problems to tackle for the semantic analysis in dialogue. In their work, a clarification request is defined to be a core function for dialogue s"
2021.findings-acl.450,2020.acl-main.626,0,0.0257303,"ted responses. This makes our benchmark directly useful for evaluating arbitrary social dialogue models. In contrast to open-ended responses in chit-chats, responses for the proposed clarification requests are factual thus facilitating automatic evaluation. Second, it allows easier crowdsourcing for dataset construction as compared with co-reference resolution, which requires strict training of manual labelers for understanding linguistic concepts. It is thus useful for acquiring large-scale datasets. Such observation is consistent with recent work on other NLP tasks (FitzGerald et al., 2018; Roit et al., 2020). Third, this method allows easy extension to dialogue understanding beyond the entity reference level, such as event co-references, semantic relations and discourse level understanding. No new labeling standards are necessary for adding a new task. According to the above observations, we create a large scale benchmark, open domain Dialogue Entity via Question Answering (DEQA), which consists of one English dataset and one Chinese dataset, of 8,415 and 6,203 dialogues, respectively. Each dialogue contains one or more questions similar to the one in Table 1. We choose to evaluate representative"
2021.findings-acl.450,D18-1233,0,0.0607599,"Missing"
2021.findings-acl.450,W04-2325,0,0.361542,"to cast it as a co-reference resolution problem (Yin et al., 2017; Kong et al., 2019; Quan et al., 2019), where a benchmark can be constructed by manually labeling co-reference information on a dialogue dataset, as shown in Table 1 (b). However, such a benchmark does not fully meet our goal because a separate model is necessary for achieving co-reference resolution, and it may be challenging to seamlessly integrate such a co-reference module into a dialogue model being tested. We take a different method, checking dialogue understanding of dialogue systems by inserting clarification requests (Schlangen, 2004; Stoyanchev and Johnston, 2015) into dialogues, and evaluating the response of dialogue systems on such requests. One example is shown in Table 1 (a), where we break a dialogue in the middle, adding clarification requests. For example, for the question “Who is just a neat freak?”, the correct system response should be “Grandma”, which reflects that the model has correct understanding of the dialogue context. The advantage is three fold. First, this method allows the evaluation of a dialogue system without using an external probe task, by directly evaluating system generated responses. This ma"
2021.findings-acl.450,P15-1152,0,0.0324938,"eived increasing research attention (Cui et al., 2017; Zhou et al., 2018; Hancock et al., 2019). The current dominant method has been sequence-to-sequence models, trained over large dialogue data end-to-end. Such models use neural network architectures such as Transformer (Vaswani et al., 2017) to encode a user utterance and a dialogue history before generating a system utterance (Adiwardana et al., 2020; Roller et al., 2020; Bao et al., 2020). A major advantage is the use of standard and general model architecture, which facilitates end-to-end training process over large scale dialogue text (Shang et al., 2015; Zhang et al., 2018, 2019, 2020). 1 Q1 A1 IDENT denotes the entities in a co-reference chain are identical. “1.6-8” indicates that “a clean house” is from the 6th to 8th tokens in the 1st utterance. Table 1: (a) Sample of English dialogue in the proposed dataset. U1 and U2 are two interlocutors in the dialogue. Qi and Ai (i=1,2) are clarification requests and the corresponding answers. (b) Co-reference chain annotation in OntoNotes 5.0 style.1 Despite showing effectiveness in empirical evaluation, existing work has a few important limitations. First, it is difficult to visualize or interpret"
2021.findings-acl.450,D17-1135,1,0.835148,"s can include explicit anaphora and implicit mentions by using zero pronouns. Take Table 1 (a) as an example, where the dialogue history consists of 7 utterances and the second utterance contains a pronoun “it”. At this point, we can measure system understanding of the dialogue state by checking whether the system can resolve the anaphora concerning “a clean house”. Our goal is to provide a large-scale benchmark and to evaluate the performance of social chatbot systems on dialogue understanding concerning entities. One way to define the task is to cast it as a co-reference resolution problem (Yin et al., 2017; Kong et al., 2019; Quan et al., 2019), where a benchmark can be constructed by manually labeling co-reference information on a dialogue dataset, as shown in Table 1 (b). However, such a benchmark does not fully meet our goal because a separate model is necessary for achieving co-reference resolution, and it may be challenging to seamlessly integrate such a co-reference module into a dialogue model being tested. We take a different method, checking dialogue understanding of dialogue systems by inserting clarification requests (Schlangen, 2004; Stoyanchev and Johnston, 2015) into dialogues, an"
2021.findings-acl.450,P19-1362,0,0.0178376,"proposed to alleviate the generation of vague and generic response, which is caused by the gradient vanishing of HRED model, by introducing a hidden variable z. Therefore, vHRED is a variational enhanced HRED model. CVAE: (Zhao et al., 2017) uses a prior network to model the gold response into a hidden variable z, which is as a condition in training step to improve the generation diversity. Static/Dynamic Attention: the mechanisms (Zhang et al., 2018) alternatively model the contextual representations of multi-turn dialogue history using two types of attentions rather than using RNN. ReCoSa: (Zhang et al., 2019) models the dialogue history in various granularity, e.g. context and response, using interactive attention and selfattention, respectively. Transformer: (Vaswani et al., 2017) is used as a representative pretrained encoder-decoder model for dialogue generation. DialoGPT: DialoGPT (Zhang et al., 2020) is a generative pretrained Transformer decoder for dialogue generation. To further conclude the characteristics of these models, Table 6 presents an overview of the characteristics of the chosen representative dialogue generation models in the proposed dialogue understanding task. 3.3 Implementat"
2021.findings-acl.450,C18-1206,1,0.905023,"Missing"
2021.findings-acl.450,2020.acl-demos.30,0,0.215735,"as event co-references, semantic relations and discourse level understanding. No new labeling standards are necessary for adding a new task. According to the above observations, we create a large scale benchmark, open domain Dialogue Entity via Question Answering (DEQA), which consists of one English dataset and one Chinese dataset, of 8,415 and 6,203 dialogues, respectively. Each dialogue contains one or more questions similar to the one in Table 1. We choose to evaluate representative multi-turn neural dialogue systems, including models using Transformer (Vaswani et al., 2017) and DialoGPT (Zhang et al., 2020). Results show that the prevalent models of multi-turn dialogue generation face challenges in the co-reference questions. We will release the dataset at Github 2 for research use. 2 Dataset We present the task (Section 2.1), the linguistic structures to evaluate (Section 2.2), the dataset construction (Section 2.3), the dataset characteristics (Section 2.4) and the evaluation metrics (Section 2.5) below. 2.1 Task Definition Given a multi-turn dialogue, the task is to answer questions concerning one or more turns of the dialogue history. In particular, the model needs to answer questions about"
2021.findings-acl.450,P17-1061,0,0.026974,"llowing 8 multi-turn dialogue generation models. HRED: (Serban et al., 2016) is a hierarchical RNNbased encoder-decoder framework to sequentially model multi-turn dialogue and generate responses. It consists of two directional RNNs. One RNN is modeling the tokens in an utterance. The other RNN is modeling the utterances in a dialogue context. vHRED: (Serban et al., 2017) is proposed to alleviate the generation of vague and generic response, which is caused by the gradient vanishing of HRED model, by introducing a hidden variable z. Therefore, vHRED is a variational enhanced HRED model. CVAE: (Zhao et al., 2017) uses a prior network to model the gold response into a hidden variable z, which is as a condition in training step to improve the generation diversity. Static/Dynamic Attention: the mechanisms (Zhang et al., 2018) alternatively model the contextual representations of multi-turn dialogue history using two types of attentions rather than using RNN. ReCoSa: (Zhang et al., 2019) models the dialogue history in various granularity, e.g. context and response, using interactive attention and selfattention, respectively. Transformer: (Vaswani et al., 2017) is used as a representative pretrained encode"
2021.findings-acl.61,P19-1470,0,0.0580417,"Missing"
2021.findings-acl.61,D15-1075,0,0.0673171,"Missing"
2021.findings-acl.61,W19-4828,0,0.240142,"Figure 2 shows one example, where the question concept is “bird”, and the correct answer is the answer concept connected through an AT L OCATION link in the C ONCEPT N ET knowledge graph. Such related concepts are not explicitly used in a BERT model for making prediction, and therefore its strength in the BERT representation is not necessarily optimized in task fine-tuning. We call such cues structured commonsense, which is a source of knowledge that we can explicitly measure. We take two methods for measuring structured commonsense in BERT, including directly measuring the attention weights (Clark et al., 2019) and measuring attribution scores by considering gradients (Mudrakarta et al., 2018). We conduct two sets of experiments to quantitatively measure commonsense links in different situation. In the first set, we examine the presence of commonsense links directly in the BERT representation both before and after fine-tuning (Section 5). In the second set of experiments, we investigate the correlation between commonsense links with model predictions (Section 6). While the former can serve as a probing task for understanding commonsense learned by pre-training, the latter can serve as a means for un"
2021.findings-acl.61,2020.acl-main.130,1,0.838958,"ind that BERT does use relevant knowledge for solving the task, and the presence of commonsense knowledge is positively correlated to the model accuracy. Figure 1: Two methods used to study structured commonsense knowledge in pre-trained Transformer. Commonsense link is drawn from the Target Concept (Answer Concept) to the Source Concept (Question Concept). Introduction Pre-trained language models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019b) give competitive results on a variety of NLP tasks (Zhou and Zhao, 2019; Joshi et al., 2019; Liu and Lapata, 2019; Cui et al., 2020). It has been shown that they can effectively capture syntactic features (Goldberg, 2019), semantic information (Liu et al., 2019a) and factual knowledge (Petroni et al., 2019), which provides support for the success in downstream tasks. Recently, there has been some debate about whether commonsense knowledge can be learned by a language model trained on large corpora. While Davison et al. (2019), Bosselut et al. (2019) and Rajani et al. (2019) argue that pre-trained language models can directly identify commonsense facts, Lin et al. (2019) and Klein and Nabi (2019) believe that structured com"
2021.findings-acl.61,D19-1109,0,0.0235311,"odels (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019b) give competitive results on a variety of NLP tasks (Zhou and Zhao, 2019; Joshi et al., 2019; Liu and Lapata, 2019; Cui et al., 2020). It has been shown that they can effectively capture syntactic features (Goldberg, 2019), semantic information (Liu et al., 2019a) and factual knowledge (Petroni et al., 2019), which provides support for the success in downstream tasks. Recently, there has been some debate about whether commonsense knowledge can be learned by a language model trained on large corpora. While Davison et al. (2019), Bosselut et al. (2019) and Rajani et al. (2019) argue that pre-trained language models can directly identify commonsense facts, Lin et al. (2019) and Klein and Nabi (2019) believe that structured commonsense knowledge is not captured well. Pre-trained language models have achieved empirical success when fine-tuned on specific com∗ monsense tasks such as C OSMOS QA (Huang et al., 2019), SWAG (Zellers et al., 2018), and CommonsenseQA (Talmor et al., 2019). One possible reason of the high performance is that there exist superficial cues or spurious associations in the dataset, which enables mod"
2021.findings-acl.61,N19-1423,0,0.0390967,"tural commonsense cues in BERT when solving commonsense tasks, and the importance of such cues for the model prediction. Using two different measures, we find that BERT does use relevant knowledge for solving the task, and the presence of commonsense knowledge is positively correlated to the model accuracy. Figure 1: Two methods used to study structured commonsense knowledge in pre-trained Transformer. Commonsense link is drawn from the Target Concept (Answer Concept) to the Source Concept (Question Concept). Introduction Pre-trained language models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019b) give competitive results on a variety of NLP tasks (Zhou and Zhao, 2019; Joshi et al., 2019; Liu and Lapata, 2019; Cui et al., 2020). It has been shown that they can effectively capture syntactic features (Goldberg, 2019), semantic information (Liu et al., 2019a) and factual knowledge (Petroni et al., 2019), which provides support for the success in downstream tasks. Recently, there has been some debate about whether commonsense knowledge can be learned by a language model trained on large corpora. While Davison et al. (2019), Bosselut et al. (2019) and Rajani et al. (2019"
2021.findings-acl.61,N18-2017,0,0.0584423,"Missing"
2021.findings-acl.61,P19-1356,0,0.0364696,"Missing"
2021.findings-acl.61,D19-1588,0,0.0167157,"iction. Using two different measures, we find that BERT does use relevant knowledge for solving the task, and the presence of commonsense knowledge is positively correlated to the model accuracy. Figure 1: Two methods used to study structured commonsense knowledge in pre-trained Transformer. Commonsense link is drawn from the Target Concept (Answer Concept) to the Source Concept (Question Concept). Introduction Pre-trained language models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019b) give competitive results on a variety of NLP tasks (Zhou and Zhao, 2019; Joshi et al., 2019; Liu and Lapata, 2019; Cui et al., 2020). It has been shown that they can effectively capture syntactic features (Goldberg, 2019), semantic information (Liu et al., 2019a) and factual knowledge (Petroni et al., 2019), which provides support for the success in downstream tasks. Recently, there has been some debate about whether commonsense knowledge can be learned by a language model trained on large corpora. While Davison et al. (2019), Bosselut et al. (2019) and Rajani et al. (2019) argue that pre-trained language models can directly identify commonsense facts, Lin et al. (2019) and Klein an"
2021.findings-acl.61,P19-1477,0,0.0201281,"l., 2019; Liu and Lapata, 2019; Cui et al., 2020). It has been shown that they can effectively capture syntactic features (Goldberg, 2019), semantic information (Liu et al., 2019a) and factual knowledge (Petroni et al., 2019), which provides support for the success in downstream tasks. Recently, there has been some debate about whether commonsense knowledge can be learned by a language model trained on large corpora. While Davison et al. (2019), Bosselut et al. (2019) and Rajani et al. (2019) argue that pre-trained language models can directly identify commonsense facts, Lin et al. (2019) and Klein and Nabi (2019) believe that structured commonsense knowledge is not captured well. Pre-trained language models have achieved empirical success when fine-tuned on specific com∗ monsense tasks such as C OSMOS QA (Huang et al., 2019), SWAG (Zellers et al., 2018), and CommonsenseQA (Talmor et al., 2019). One possible reason of the high performance is that there exist superficial cues or spurious associations in the dataset, which enables models to answer questions without understanding the task (Niven and Kao, 2019; Yu et al., 2020; Kaushik et al., 2020). For example, a model can choose the spurious cue word “m"
2021.findings-acl.61,2020.acl-main.671,0,0.0205871,"se 684 knowledge, in solving commonsense QA, rather than focusing on adversarial cases. Commonsense reasoning is a challenging task in natural language processing. Traditional methods rely heavily on hand-crafted features (Rahman and Ng, 2012; Bailey et al., 2015) and external knowledge bases (Sch¨uller, 2014). With recent advances in deep learning, pre-trained language models have been used as a powerful method for such tasks. Trinh and Le (2018) use a pre-trained language model to score candidate sentences for the Pronoun Disambiguation and Winograd Schema Challenge (Levesque et al., 2012). Klein and Nabi (2020) use a sentence-level loss to enhance commonsense knowledge in BERT. Mao et al. (2019) demonstrate that pre-trained language models fine-tuned on SWAG (Zellers et al., 2018) are able to provide commonsense grounding for story generation. For commonsense question answering, pre-trained language models with fine-tuning give the state-ofthe-art performance (Zellers et al., 2018; Huang et al., 2019; Talmor et al., 2019). Though the above work show usefulness of BERT on comonsense tasks, little work has been done investigating the mechansim for BERT solving the tasks. Our work thus complements exis"
2021.findings-acl.61,D19-1445,0,0.0178532,"x C. 4 Analysis Methods As mentioned earlier, we analyze commonsense links using the attention weight (Clark et al., 2019) and the corresponding attribution score (Sundararajan et al., 2017; Mudrakarta et al., 2018). We report results in one random execution for each experiment. We additionally tried five runs for each experiments, and found that the result variation is small (Appendix B). 4.1 Attention Weights Given a sentence, attention weights in Transformer can be viewed as the relative importance weight between each token and the other tokens when producing the next layer representation (Kovaleva et al., 2019; Vashishth et al., 2020). In particular, given a sequence of input vectors H = [h1 , h2 , . . . , h|H |], its self-attention representation uses each vector as a query to retrieve all context vectors in H, yielding a matrix of attention weights α ∈ R|H|×|H |. The value of α is computed using the scaled dot-product of the query vector of representation Q = WQ H and the key vector of representation K = WK H, followed by softmax normalization QKT α = sof tmax( √ ), dk (1) where dk is the dimension size of the key vector K. αi,j represents the attention strength from hi to hj . For multi-head att"
2021.findings-acl.61,D19-1282,0,0.0613078,"Zhao, 2019; Joshi et al., 2019; Liu and Lapata, 2019; Cui et al., 2020). It has been shown that they can effectively capture syntactic features (Goldberg, 2019), semantic information (Liu et al., 2019a) and factual knowledge (Petroni et al., 2019), which provides support for the success in downstream tasks. Recently, there has been some debate about whether commonsense knowledge can be learned by a language model trained on large corpora. While Davison et al. (2019), Bosselut et al. (2019) and Rajani et al. (2019) argue that pre-trained language models can directly identify commonsense facts, Lin et al. (2019) and Klein and Nabi (2019) believe that structured commonsense knowledge is not captured well. Pre-trained language models have achieved empirical success when fine-tuned on specific com∗ monsense tasks such as C OSMOS QA (Huang et al., 2019), SWAG (Zellers et al., 2018), and CommonsenseQA (Talmor et al., 2019). One possible reason of the high performance is that there exist superficial cues or spurious associations in the dataset, which enables models to answer questions without understanding the task (Niven and Kao, 2019; Yu et al., 2020; Kaushik et al., 2020). For example, a model can choos"
2021.findings-acl.61,N19-1112,0,0.0334766,"Missing"
2021.findings-acl.61,D19-1387,0,0.0190957,"fferent measures, we find that BERT does use relevant knowledge for solving the task, and the presence of commonsense knowledge is positively correlated to the model accuracy. Figure 1: Two methods used to study structured commonsense knowledge in pre-trained Transformer. Commonsense link is drawn from the Target Concept (Answer Concept) to the Source Concept (Question Concept). Introduction Pre-trained language models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019b) give competitive results on a variety of NLP tasks (Zhou and Zhao, 2019; Joshi et al., 2019; Liu and Lapata, 2019; Cui et al., 2020). It has been shown that they can effectively capture syntactic features (Goldberg, 2019), semantic information (Liu et al., 2019a) and factual knowledge (Petroni et al., 2019), which provides support for the success in downstream tasks. Recently, there has been some debate about whether commonsense knowledge can be learned by a language model trained on large corpora. While Davison et al. (2019), Bosselut et al. (2019) and Rajani et al. (2019) argue that pre-trained language models can directly identify commonsense facts, Lin et al. (2019) and Klein and Nabi (2019) believe"
2021.findings-acl.61,2021.ccl-1.108,0,0.0692971,"Missing"
2021.findings-acl.61,D19-1615,0,0.0133257,"mmonsense reasoning is a challenging task in natural language processing. Traditional methods rely heavily on hand-crafted features (Rahman and Ng, 2012; Bailey et al., 2015) and external knowledge bases (Sch¨uller, 2014). With recent advances in deep learning, pre-trained language models have been used as a powerful method for such tasks. Trinh and Le (2018) use a pre-trained language model to score candidate sentences for the Pronoun Disambiguation and Winograd Schema Challenge (Levesque et al., 2012). Klein and Nabi (2020) use a sentence-level loss to enhance commonsense knowledge in BERT. Mao et al. (2019) demonstrate that pre-trained language models fine-tuned on SWAG (Zellers et al., 2018) are able to provide commonsense grounding for story generation. For commonsense question answering, pre-trained language models with fine-tuning give the state-ofthe-art performance (Zellers et al., 2018; Huang et al., 2019; Talmor et al., 2019). Though the above work show usefulness of BERT on comonsense tasks, little work has been done investigating the mechansim for BERT solving the tasks. Our work thus complements existing research in this aspect. There is also a line of work leveraging C ON CEPT N ET t"
2021.findings-acl.61,P18-1176,0,0.388106,"ct answer is the answer concept connected through an AT L OCATION link in the C ONCEPT N ET knowledge graph. Such related concepts are not explicitly used in a BERT model for making prediction, and therefore its strength in the BERT representation is not necessarily optimized in task fine-tuning. We call such cues structured commonsense, which is a source of knowledge that we can explicitly measure. We take two methods for measuring structured commonsense in BERT, including directly measuring the attention weights (Clark et al., 2019) and measuring attribution scores by considering gradients (Mudrakarta et al., 2018). We conduct two sets of experiments to quantitatively measure commonsense links in different situation. In the first set, we examine the presence of commonsense links directly in the BERT representation both before and after fine-tuning (Section 5). In the second set of experiments, we investigate the correlation between commonsense links with model predictions (Section 6). While the former can serve as a probing task for understanding commonsense learned by pre-training, the latter can serve as a means for understanding whether a model learns to make better use of commonsense knowledge throu"
2021.findings-acl.61,P19-1459,0,0.0195212,"that pre-trained language models can directly identify commonsense facts, Lin et al. (2019) and Klein and Nabi (2019) believe that structured commonsense knowledge is not captured well. Pre-trained language models have achieved empirical success when fine-tuned on specific com∗ monsense tasks such as C OSMOS QA (Huang et al., 2019), SWAG (Zellers et al., 2018), and CommonsenseQA (Talmor et al., 2019). One possible reason of the high performance is that there exist superficial cues or spurious associations in the dataset, which enables models to answer questions without understanding the task (Niven and Kao, 2019; Yu et al., 2020; Kaushik et al., 2020). For example, a model can choose the spurious cue word “meadow” as a feature for positive reviews simply because “meadow” occurs frequently in positive documents. It remains an interesting research question whether commonsense knowledge plays a central role among statistical cues that BERT has when solving commonsense tasks. In other words, we are interested in investigating whether BERT solves commonsense tasks using commonsense knowledge. Corresponding Author We try to provide quantitative answers by mainly using the CommonsenseQA dataset, which asks"
2021.findings-acl.61,N18-1202,0,0.0152938,"itatively investigate the presence of structural commonsense cues in BERT when solving commonsense tasks, and the importance of such cues for the model prediction. Using two different measures, we find that BERT does use relevant knowledge for solving the task, and the presence of commonsense knowledge is positively correlated to the model accuracy. Figure 1: Two methods used to study structured commonsense knowledge in pre-trained Transformer. Commonsense link is drawn from the Target Concept (Answer Concept) to the Source Concept (Question Concept). Introduction Pre-trained language models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019b) give competitive results on a variety of NLP tasks (Zhou and Zhao, 2019; Joshi et al., 2019; Liu and Lapata, 2019; Cui et al., 2020). It has been shown that they can effectively capture syntactic features (Goldberg, 2019), semantic information (Liu et al., 2019a) and factual knowledge (Petroni et al., 2019), which provides support for the success in downstream tasks. Recently, there has been some debate about whether commonsense knowledge can be learned by a language model trained on large corpora. While Davison et al. (2019), Bos"
2021.findings-acl.61,D19-1250,0,0.0510436,"Missing"
2021.findings-acl.61,S18-2023,0,0.038552,"Missing"
2021.findings-acl.61,W18-5431,0,0.0663457,"Missing"
2021.findings-acl.61,D12-1071,0,0.0931777,"Missing"
2021.findings-acl.61,P19-1487,0,0.0121521,"Devlin et al., 2019; Liu et al., 2019b) give competitive results on a variety of NLP tasks (Zhou and Zhao, 2019; Joshi et al., 2019; Liu and Lapata, 2019; Cui et al., 2020). It has been shown that they can effectively capture syntactic features (Goldberg, 2019), semantic information (Liu et al., 2019a) and factual knowledge (Petroni et al., 2019), which provides support for the success in downstream tasks. Recently, there has been some debate about whether commonsense knowledge can be learned by a language model trained on large corpora. While Davison et al. (2019), Bosselut et al. (2019) and Rajani et al. (2019) argue that pre-trained language models can directly identify commonsense facts, Lin et al. (2019) and Klein and Nabi (2019) believe that structured commonsense knowledge is not captured well. Pre-trained language models have achieved empirical success when fine-tuned on specific com∗ monsense tasks such as C OSMOS QA (Huang et al., 2019), SWAG (Zellers et al., 2018), and CommonsenseQA (Talmor et al., 2019). One possible reason of the high performance is that there exist superficial cues or spurious associations in the dataset, which enables models to answer questions without understanding the"
2021.findings-acl.61,N19-1421,0,0.153136,"Recently, there has been some debate about whether commonsense knowledge can be learned by a language model trained on large corpora. While Davison et al. (2019), Bosselut et al. (2019) and Rajani et al. (2019) argue that pre-trained language models can directly identify commonsense facts, Lin et al. (2019) and Klein and Nabi (2019) believe that structured commonsense knowledge is not captured well. Pre-trained language models have achieved empirical success when fine-tuned on specific com∗ monsense tasks such as C OSMOS QA (Huang et al., 2019), SWAG (Zellers et al., 2018), and CommonsenseQA (Talmor et al., 2019). One possible reason of the high performance is that there exist superficial cues or spurious associations in the dataset, which enables models to answer questions without understanding the task (Niven and Kao, 2019; Yu et al., 2020; Kaushik et al., 2020). For example, a model can choose the spurious cue word “meadow” as a feature for positive reviews simply because “meadow” occurs frequently in positive documents. It remains an interesting research question whether commonsense knowledge plays a central role among statistical cues that BERT has when solving commonsense tasks. In other words,"
2021.findings-acl.61,D18-1009,0,0.106741,"port for the success in downstream tasks. Recently, there has been some debate about whether commonsense knowledge can be learned by a language model trained on large corpora. While Davison et al. (2019), Bosselut et al. (2019) and Rajani et al. (2019) argue that pre-trained language models can directly identify commonsense facts, Lin et al. (2019) and Klein and Nabi (2019) believe that structured commonsense knowledge is not captured well. Pre-trained language models have achieved empirical success when fine-tuned on specific com∗ monsense tasks such as C OSMOS QA (Huang et al., 2019), SWAG (Zellers et al., 2018), and CommonsenseQA (Talmor et al., 2019). One possible reason of the high performance is that there exist superficial cues or spurious associations in the dataset, which enables models to answer questions without understanding the task (Niven and Kao, 2019; Yu et al., 2020; Kaushik et al., 2020). For example, a model can choose the spurious cue word “meadow” as a feature for positive reviews simply because “meadow” occurs frequently in positive documents. It remains an interesting research question whether commonsense knowledge plays a central role among statistical cues that BERT has when so"
2021.findings-acl.61,P19-1230,0,0.01616,"es for the model prediction. Using two different measures, we find that BERT does use relevant knowledge for solving the task, and the presence of commonsense knowledge is positively correlated to the model accuracy. Figure 1: Two methods used to study structured commonsense knowledge in pre-trained Transformer. Commonsense link is drawn from the Target Concept (Answer Concept) to the Source Concept (Question Concept). Introduction Pre-trained language models (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019; Liu et al., 2019b) give competitive results on a variety of NLP tasks (Zhou and Zhao, 2019; Joshi et al., 2019; Liu and Lapata, 2019; Cui et al., 2020). It has been shown that they can effectively capture syntactic features (Goldberg, 2019), semantic information (Liu et al., 2019a) and factual knowledge (Petroni et al., 2019), which provides support for the success in downstream tasks. Recently, there has been some debate about whether commonsense knowledge can be learned by a language model trained on large corpora. While Davison et al. (2019), Bosselut et al. (2019) and Rajani et al. (2019) argue that pre-trained language models can directly identify commonsense facts, Lin et al."
2021.inlg-1.33,2021.findings-acl.449,1,0.400734,"ss on summarizing monologic texts, such as news articles (Liu and Lapata, 2019; Gehrmann et al., 2018), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents, differs from monologic texts in nature and has received significantly less attention from the summarization research community. A major reason is the paucity of suitable dialogue summarization datasets. To cope with this problem, we build a large scale labeled summarization dataset for real-life scenario dialogues, D IALOG S UM (Chen et al., 2021). An example from D IALOG S UM is shown in Figure 1. Compared with existing dialogue summariztaion datasets (Carletta et al., 2005; Gliwa et al., 2019; Zhong et al., 2021; Zhu et al., 2021), D IALOG S UM is useful for training neural models and is staying in the spoken domain as opposed to the written chat domain. Also, it contains diverse task-oriented dialogues that cover a wide range of daily-life topics. Summarizing those dialogues is useful for both business (e.g. help a business find common needs) and personal uses (e.g. track important events as 308 Proceedings of the 14th International"
2021.inlg-1.33,2020.acl-main.130,1,0.828823,"mmary2 to Summary3 Average to not only summarize what speakers are saying, but also what they are doing. 3 Task Description The task for participants is to provide a model that generates a summary given the input dialogue text. Both automatic and manual evaluation will be conducted to measure model performance. 3.1 Data The participant of DialogSum Challenge can start immediately, as the D IALOG S UM dataset has been already public 1 . We collect 13,460 dialogue data for D IALOG S UM from three public dialogue corpora, namely Dailydialog (Li et al., 2017), DREAM (Sun et al., 2019) and MuTual (Cui et al., 2020), as well as an English speaking practice website. In term of size, D IALOG S UM is comparable with SAMSum while its average dialogue length is much longer than SAMSum, which comforts the purpose of summarization and is thus more challenging. The dialogue data cover a wide range of daily-life topics, including diverse task-oriented scenarios. We ask annotators to summarize the dialogue from an observer’s perspective. To ensure the annotation quality, each summary has been checked twice by different people, where the reward and punishment mechanism is included. We also sample and check the data"
2021.inlg-1.33,D18-1443,0,0.0119614,"s unique challenges in dialogue summarization, we will manually evaluate system-generated summaries from multiple aspects designed for dialogue summarization, including coreference information, discourse relation, intent identification and objective description. An example is shown in Figure 1, where the summary describes main events in a business conversation. 2 Motivation Thanks to the advance in neural network models, and availability of large scale labeled datasets, recent research has achieved promising progress on summarizing monologic texts, such as news articles (Liu and Lapata, 2019; Gehrmann et al., 2018), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents, differs from monologic texts in nature and has received significantly less attention from the summarization research community. A major reason is the paucity of suitable dialogue summarization datasets. To cope with this problem, we build a large scale labeled summarization dataset for real-life scenario dialogues, D IALOG S UM (Chen et al., 2021). An example from D IALOG S UM is shown in Figure 1. Compared with existing dialogue"
2021.inlg-1.33,D19-5409,0,0.0174606,"pers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents, differs from monologic texts in nature and has received significantly less attention from the summarization research community. A major reason is the paucity of suitable dialogue summarization datasets. To cope with this problem, we build a large scale labeled summarization dataset for real-life scenario dialogues, D IALOG S UM (Chen et al., 2021). An example from D IALOG S UM is shown in Figure 1. Compared with existing dialogue summariztaion datasets (Carletta et al., 2005; Gliwa et al., 2019; Zhong et al., 2021; Zhu et al., 2021), D IALOG S UM is useful for training neural models and is staying in the spoken domain as opposed to the written chat domain. Also, it contains diverse task-oriented dialogues that cover a wide range of daily-life topics. Summarizing those dialogues is useful for both business (e.g. help a business find common needs) and personal uses (e.g. track important events as 308 Proceedings of the 14th International Conference on Natural Language Generation (INLG), pages 308–313, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Li"
2021.inlg-1.33,J95-2003,0,0.883796,"gue. Dialogue information flow is intuitively reflected in the dialogue discourse structures (Wolf and Gibson, 2005), where two utterances can be closely related even there is a large distance between them. Such a phenomenon is common in procedures and negotiations. For example, in Figure 1, the penultimate utterance “... we’ll draft the agreement... and sign it...” is actually replying to the third utterance “What’s the answer?”, between which the utterances can be viewed as negotiation 309 process and conditions. Also, frequent coreference and ellipsis make dialogue difficult to understand (Grosz et al., 1995; Quan et al., 2019). For example, to generate “wrong” in the summary in Figure 2, the model needs to understand “I think you have added someone else’s (laundry service on my bill)”, where “my bill” refers to “#Person 2#’s bill”. These linguistic phenomena make dialogues difficult to encode using ordinary representation learning techonologies (Chen et al., 2021). Second, compared with monologic summarization, dialogues are summarized from an observer’s perspective, which requires summary to be objective. For example, in Figure 3, #Person 1#’s statements are actually awaiting to be confirmed. H"
2021.inlg-1.33,N19-1238,0,0.0357933,"Missing"
2021.inlg-1.33,D19-1051,0,0.0518932,"Missing"
2021.inlg-1.33,2020.emnlp-main.750,0,0.0552237,"Missing"
2021.inlg-1.33,I17-1099,0,0.0233617,"ed Summary Summary1 to Summary2 Summary1 to Summary3 Summary2 to Summary3 Average to not only summarize what speakers are saying, but also what they are doing. 3 Task Description The task for participants is to provide a model that generates a summary given the input dialogue text. Both automatic and manual evaluation will be conducted to measure model performance. 3.1 Data The participant of DialogSum Challenge can start immediately, as the D IALOG S UM dataset has been already public 1 . We collect 13,460 dialogue data for D IALOG S UM from three public dialogue corpora, namely Dailydialog (Li et al., 2017), DREAM (Sun et al., 2019) and MuTual (Cui et al., 2020), as well as an English speaking practice website. In term of size, D IALOG S UM is comparable with SAMSum while its average dialogue length is much longer than SAMSum, which comforts the purpose of summarization and is thus more challenging. The dialogue data cover a wide range of daily-life topics, including diverse task-oriented scenarios. We ask annotators to summarize the dialogue from an observer’s perspective. To ensure the annotation quality, each summary has been checked twice by different people, where the reward and punishment"
2021.inlg-1.33,W04-1013,0,0.0804982,"#Person1# gives #Person_2# a wrong bill at first then corrects it. Figure 3: Selected case from D IALOG S UM dataset. Figure 2: Selected case from D IALOG S UM dataset. personal assistants). Empirical study and analysis demonstrate challenges in real-life scenario dialogue summarization (Chen et al., 2021). To highlight the challenges in dialogue summarization, we propose real-life scenario dialogue summarization challenge, DialogSum Challenge, to encourage researchers to investigate such problems. The evaluation for dialogue summarization contains both automatic evaluation, i.e. ROUGE score (Lin, 2004) and BERTScore (Zhang et al., 2019), and human evaluation from multiple aspects to address corresponding challenges (c.f. Section 2.1 and Section 3.3.2). For human evaluation, we anonymize the submitted models, and evaluate them on corresponding hidden sub-test sets to ensure the fairness. 2.1 Unique Challenges in D IALOG S UM Although dialogue summarization is in line with the philosophy of monologue summarization, we find some unique challenges in dialogue summarization. First, because of special linguistic phenomena, the dialogue on the source side differs from monologue. Dialogue informati"
2021.inlg-1.33,P19-1500,1,0.846327,"particular, to address unique challenges in dialogue summarization, we will manually evaluate system-generated summaries from multiple aspects designed for dialogue summarization, including coreference information, discourse relation, intent identification and objective description. An example is shown in Figure 1, where the summary describes main events in a business conversation. 2 Motivation Thanks to the advance in neural network models, and availability of large scale labeled datasets, recent research has achieved promising progress on summarizing monologic texts, such as news articles (Liu and Lapata, 2019; Gehrmann et al., 2018), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents, differs from monologic texts in nature and has received significantly less attention from the summarization research community. A major reason is the paucity of suitable dialogue summarization datasets. To cope with this problem, we build a large scale labeled summarization dataset for real-life scenario dialogues, D IALOG S UM (Chen et al., 2021). An example from D IALOG S UM is shown in Figure 1. Compare"
2021.inlg-1.33,2020.emnlp-main.748,0,0.0156833,"ummarization, we will manually evaluate system-generated summaries from multiple aspects designed for dialogue summarization, including coreference information, discourse relation, intent identification and objective description. An example is shown in Figure 1, where the summary describes main events in a business conversation. 2 Motivation Thanks to the advance in neural network models, and availability of large scale labeled datasets, recent research has achieved promising progress on summarizing monologic texts, such as news articles (Liu and Lapata, 2019; Gehrmann et al., 2018), patents (Pilault et al., 2020) and academic papers (Koncel-Kedziorski et al., 2019). However, dialogue, as an important channel for achieving communicative intents, differs from monologic texts in nature and has received significantly less attention from the summarization research community. A major reason is the paucity of suitable dialogue summarization datasets. To cope with this problem, we build a large scale labeled summarization dataset for real-life scenario dialogues, D IALOG S UM (Chen et al., 2021). An example from D IALOG S UM is shown in Figure 1. Compared with existing dialogue summariztaion datasets (Carlett"
2021.inlg-1.33,D19-1462,0,0.0201156,"ation flow is intuitively reflected in the dialogue discourse structures (Wolf and Gibson, 2005), where two utterances can be closely related even there is a large distance between them. Such a phenomenon is common in procedures and negotiations. For example, in Figure 1, the penultimate utterance “... we’ll draft the agreement... and sign it...” is actually replying to the third utterance “What’s the answer?”, between which the utterances can be viewed as negotiation 309 process and conditions. Also, frequent coreference and ellipsis make dialogue difficult to understand (Grosz et al., 1995; Quan et al., 2019). For example, to generate “wrong” in the summary in Figure 2, the model needs to understand “I think you have added someone else’s (laundry service on my bill)”, where “my bill” refers to “#Person 2#’s bill”. These linguistic phenomena make dialogues difficult to encode using ordinary representation learning techonologies (Chen et al., 2021). Second, compared with monologic summarization, dialogues are summarized from an observer’s perspective, which requires summary to be objective. For example, in Figure 3, #Person 1#’s statements are actually awaiting to be confirmed. Human annotators iden"
2021.inlg-1.33,Q19-1014,0,0.0280322,"mmary2 Summary1 to Summary3 Summary2 to Summary3 Average to not only summarize what speakers are saying, but also what they are doing. 3 Task Description The task for participants is to provide a model that generates a summary given the input dialogue text. Both automatic and manual evaluation will be conducted to measure model performance. 3.1 Data The participant of DialogSum Challenge can start immediately, as the D IALOG S UM dataset has been already public 1 . We collect 13,460 dialogue data for D IALOG S UM from three public dialogue corpora, namely Dailydialog (Li et al., 2017), DREAM (Sun et al., 2019) and MuTual (Cui et al., 2020), as well as an English speaking practice website. In term of size, D IALOG S UM is comparable with SAMSum while its average dialogue length is much longer than SAMSum, which comforts the purpose of summarization and is thus more challenging. The dialogue data cover a wide range of daily-life topics, including diverse task-oriented scenarios. We ask annotators to summarize the dialogue from an observer’s perspective. To ensure the annotation quality, each summary has been checked twice by different people, where the reward and punishment mechanism is included. We"
2021.inlg-1.33,2020.inlg-1.30,0,0.0959482,"Missing"
2021.inlg-1.33,W18-6538,0,0.0601786,"Missing"
2021.inlg-1.33,J05-2005,0,0.0696102,"aspects to address corresponding challenges (c.f. Section 2.1 and Section 3.3.2). For human evaluation, we anonymize the submitted models, and evaluate them on corresponding hidden sub-test sets to ensure the fairness. 2.1 Unique Challenges in D IALOG S UM Although dialogue summarization is in line with the philosophy of monologue summarization, we find some unique challenges in dialogue summarization. First, because of special linguistic phenomena, the dialogue on the source side differs from monologue. Dialogue information flow is intuitively reflected in the dialogue discourse structures (Wolf and Gibson, 2005), where two utterances can be closely related even there is a large distance between them. Such a phenomenon is common in procedures and negotiations. For example, in Figure 1, the penultimate utterance “... we’ll draft the agreement... and sign it...” is actually replying to the third utterance “What’s the answer?”, between which the utterances can be viewed as negotiation 309 process and conditions. Also, frequent coreference and ellipsis make dialogue difficult to understand (Grosz et al., 1995; Quan et al., 2019). For example, to generate “wrong” in the summary in Figure 2, the model needs"
2021.inlg-1.33,2020.acl-main.451,0,0.0413089,"n annotators to follow Chen et al. (2021)’s criteria and rate the summary on a 50 randomly selected sub-testset. However, system-generated summaries usually focus on the consequence of a dialogue, and fail to correctly identify interlocutors’ intents. Therefore, we will conduct human evaluation on intent identification on the 50 randomly selected sub-testset following Chen et al. (2021). Discourse Relation Coherent summaries convey important relations between main events, and identifying discourse relations and using proper phrases to express them can be challenging for summarization systems (Xu et al., 2020). However, causally related events are usually not explicitly expressed, and the distance between them is long due to the unique dialogue discourse structure (Grosz et al., 1995). To quantify such challenge, we will conduct human evaluation on discourse relation following (Chen et al., 2021) on the discourse sub-testset. Objective Description In addition to the above evaluation aspects, we also find that models tend to take all interlocutors’ contents as ground truth while failing to reason whether their statements are just subjective assumptions or even defended to be fake. Therefore, we will"
2021.inlg-1.33,2021.naacl-main.472,1,0.814203,"Missing"
2021.inlg-1.33,2021.naacl-main.474,1,0.653972,"owever, dialogue, as an important channel for achieving communicative intents, differs from monologic texts in nature and has received significantly less attention from the summarization research community. A major reason is the paucity of suitable dialogue summarization datasets. To cope with this problem, we build a large scale labeled summarization dataset for real-life scenario dialogues, D IALOG S UM (Chen et al., 2021). An example from D IALOG S UM is shown in Figure 1. Compared with existing dialogue summariztaion datasets (Carletta et al., 2005; Gliwa et al., 2019; Zhong et al., 2021; Zhu et al., 2021), D IALOG S UM is useful for training neural models and is staying in the spoken domain as opposed to the written chat domain. Also, it contains diverse task-oriented dialogues that cover a wide range of daily-life topics. Summarizing those dialogues is useful for both business (e.g. help a business find common needs) and personal uses (e.g. track important events as 308 Proceedings of the 14th International Conference on Natural Language Generation (INLG), pages 308–313, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics Dialogue from DIALOGSUM: #Per"
2021.naacl-main.144,P17-4017,0,0.0467976,"Missing"
2021.naacl-main.144,N19-1423,0,0.126883,"ng et al. (2019a) extract the semanthat our proposed unified model achieves superior tic representations from a pre-trained SRL model performance compared with previously proposed BMESO-based works. Our contributions are: (i) and feed them into the opinion mining model, achieving substantial improvements. Zhang et al. we propose a unified span-based model for opinion (2020) incorporate the powerful contextual repremining in the end-to-end fashion that also supports sentations of bi-directional encoder representations the given-expression setting, (ii) we successfully from Transformers (BERT) (Devlin et al., 2019) integrate syntactic constituents knowledge into our and external dependency syntactic knowledge. model with MTL and GCN, achieving promising improvements, (iii) detailed analyses demonstrate To solve or alleviate the weaknesses of the prethe effectiveness of our unified model and the use- viously proposed BMESO-based models, we profulness of integrating constituent syntactic knowl- pose a new method to unifiedly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classifica"
2021.naacl-main.144,Q19-1019,0,0.0464782,"Missing"
2021.naacl-main.144,P18-2058,0,0.0219271,"edly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classification as an MTL problem. Besides, to boost the opinion mining performance and motivated by the span-based task formalism, we explore to incorporate syntactic constituents into our model. Utilizing span-based representations have been investigated for many other NLP tasks, such as named entity recognition (NER) (Tan et al., 2020), constituency parsing (Kitaev and Klein, 2018), and semantic role labeling (SRL) (He et al., 2018). Generally, NER is a single span classification problem, constituency parsing is a span-based structure prediction problem, and SRL is a word-span classification problem. Different from them, in our methodology, OM is a span-span classification problem. 3 The S PAN OM Model 3.1 Task Definition. Given an input sentence s = w1 , w2 , ..., wn , our model aims to predict the gold-standard opinion structures Y ⊆ E × O × R, where E = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of expressions, O = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of opinion roles , and R is the set of opinion relations (holder"
2021.naacl-main.144,P14-1062,0,0.0186777,"char representation, and contextual word representation to compose the model input, denoted as: xi = embword ⊕ repchar ⊕ repcontext , (1) wi wi wi |s Encoder Layer. + M LPbexp (hb ) + M LPeexp (he ), srol = M LP rol (spanrol b,e ) (5) + M LPbrol (hb ) + M LPerol (he ). We can observe that for a sentence with n words, the numbers of candidate spans for expressions and roles are both n∗(n+1) , while the number of 2 gold expressions and roles are much fewer. To alleviate the unbalanced number of gold samples where ⊕ means the concatenate operation. We use the convolutional neural networks (CNN) (Kalchbrenner et al., 2014) to generate the character representations over the characters of words. 1797 1 We omit the process of span boundary module in Figure 2 for clarity. O Classification Layer MLP Holder OM Target MLP Encoder MLP Representation Layer OM Constituent MTL Input seriously needs equipment for detecting drugs GCN Constituent Encoder Input Layer GCN Input OM Encoder Layer Encoder Input MTL+GCN GCN Figure 2: The model architecture of our unified span-based opinion mining model (left) and syntactic constituent integration methods (right). and negative samples, we adapt the focal loss that is widely used in"
2021.naacl-main.144,P16-1087,0,0.342622,"rporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinio"
2021.naacl-main.144,D14-1162,0,0.0862699,"62.04 53.27 57.76 Proportional F1 Holder Target Overall 46.62 34.29 55.62 41.65 48.90 61.20 49.88 55.68 Table 1: Experimental results of our span-based opinion mining model and comparison with previous works on the MPQA2.0 dataset in the end-to-end setting. “-” means results are not reported in their paper. Exact P R F1 Zhang et al. (2019b) 60.21 48.52 53.04 S PAN OM 64.85 52.60 58.06 S PAN OM+BERT 67.15 60.63 63.71 Models Table 2: Results and comparison of the expression prediction on the exact metric in the end-to-end setting. 5.2 Hyper-parameters. We employ the 300-dimension GloVe vector (Pennington et al., 2014) as our pre-trained word embeddings. The character embeddings are randomly initialized and a CNN with kernel sizes of 3, 4, 5 is used to capture the character representations. For the contextual representations, we extract the representations from the base BERT by making a weighted summation over the last four layer outputs. The hidden size of the BiLSTM layer is set to 300 and we employ 2-layer BiLSTMs to encode the input representations. The dimension of opinion expression and role representations is 300 and the hidden size of expression, role, and relation classifiers is 150. We use 3-layer"
2021.naacl-main.144,P13-1161,0,0.210861,"thod. In addition, incorporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap"
2021.naacl-main.144,Q14-1039,0,0.0244727,"n Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1795–1804 June 6–11, 2021. ©2021 Association for Computational Linguistics belongs to an expression, 0 otherwise), thus one sample is expanded n times if one sentence has n expressions, which is inefficient (Marasovi´c and Frank, 2018; Zhang et al., 2020). 2) The BMESObased method is weak to capture long-range dependencies and prefers to predict shorter opinion role spans (Zhang et al., 2020). 2 Related Work There are several task settings for opinion mining in the community: 1) Breck et al. (2007); Yang and Cardie (2014) focus on labeling the expressions. 2) Katiyar and Cardie (2016); Zhang et al. (2019b); Quan et al. (2019) discover the opinion structures in the end-to-end setting, i.e, based on the systemMotivated by the span-based representations of atic expressions. 3) Marasovi´c and Frank (2018); opinion expressions and roles, we propose a unified Zhang et al. (2019a, 2020) identify the opinion span-based opinion mining model (S PAN OM) that roles based on the given expressions. Our work can solve or alleviate the aforementioned weak- follows the end-to-end setting and also supports nesses. First, we tre"
2021.naacl-main.144,P18-1249,0,0.388081,"ing constituent syntactic knowl- pose a new method to unifiedly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classification as an MTL problem. Besides, to boost the opinion mining performance and motivated by the span-based task formalism, we explore to incorporate syntactic constituents into our model. Utilizing span-based representations have been investigated for many other NLP tasks, such as named entity recognition (NER) (Tan et al., 2020), constituency parsing (Kitaev and Klein, 2018), and semantic role labeling (SRL) (He et al., 2018). Generally, NER is a single span classification problem, constituency parsing is a span-based structure prediction problem, and SRL is a word-span classification problem. Different from them, in our methodology, OM is a span-span classification problem. 3 The S PAN OM Model 3.1 Task Definition. Given an input sentence s = w1 , w2 , ..., wn , our model aims to predict the gold-standard opinion structures Y ⊆ E × O × R, where E = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of expressions, O = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of opinion ro"
2021.naacl-main.144,2020.acl-main.297,1,0.617524,"promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinion structures between different expresOpi"
2021.naacl-main.144,N19-1066,0,0.12942,"etting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinion structures between different expresOpinion mining (OM), which aims to find the opin- sions in one sentence. Figure 1 gives an example, ion structures of “Who expressed what opinions in which some overlapped opinion relations have towards what.” in one sentence, has achieved much been discarded by previous works (Katiyar and attention in recent years (Katiyar and Cardie, 2016; Cardie, 2016), such as [happy, he loves being EnMarasovi´c and Frank, 2018; Zhang et al., 2019b, derly Park, Target] and [loves, he, Holder]. There 2020). The opinion analysis has many NLP appli- are also other works which focus only on predicting cations, such as social media monitoring (Bollen opinions roles based on the gold-standard expreset al., 2011) and e-commerce applications (Cui sions, which also follow the BMESO-based method et al., 2017). The commonly used benchmark (Marasovi´c and Frank, 2018; Zhang et al., 2020). However, they also suffer from some weaknesses: ∗ Rui Wang’s contributions were carried out while at 1) the expressions are usually fed into the model inAlibaba"
2021.naacl-main.144,N18-1054,0,0.041995,"Missing"
2021.naacl-main.144,J93-2004,0,0.0741147,"F1 score of 67.66. Finally, we try to combine the two kinds of methods and the results are shown in the last major row. It is clear that combining the MTL method with OntoNotes and the GCN method with ParserPTB achieves better results than the reversed one. Therefore, our constituent-enhanced opinion mining model follows this combination. Besides, we can also see the relative lower results of “OntoNotes+PTB” in “+MTL” and “+GCN” settings, which is strange Which source of constituent knowledge is better? There are two main constituent syntax corpus in the community, i.e., Penn Treebank (PTB) (Marcus et al., 1993) and OntoNotes5.0 (Weischedel et al., 2013). The PTB corpus contains about 39k training data and mainly focuses on news data, while the OntoNotes5.0 corpus contains about 75k training data and focuses on multi-domain data (news, web, telephone conversation, and etc.). It is a worthy question to explore which is better for our span-based OM model, or what kind of combination is better. We compare them with various combinations on the BERT-based model, whose results are shown in Table 5. First, the sec7 We use the code of Kitaev and Klein (2018) to train the OntoNotes conond major row shows the"
2021.naacl-main.144,D18-1244,0,0.0229945,"information to expressions and roles. 4.2 The GCN Method. The MTL method enhances our OM model from the aspect of model representative ability by jointly modeling opinion mining and partial constituency parsing. We argue that modeling the syntactic constituent structure is also beneficial for OM because it provides valuable syntactic information for a sentence. Therefore, we try to employ the recently popular GCN (Kipf and Welling, 2016) to encode the constituent structure. However, the conventional GCN is not suitable for constituency trees, because it usually works on the dependency trees (Zhang et al., 2018, 2020) where the nodes are the surface words in a sentence. While, in constituent trees, there exists a certain number of non-terminal nodes3 , such as “NP”, “VP”, “SBAR” and so on. So it is hard to directly apply conventional GCN on the constituent trees. In the following, we first introduce the definition and workflow of typical GCN and then describe our modification. Formally, we denote an undirected graph as G = (V, E), where V and E are the set of nodes and edges, respectively. The GCN computation flow of node v ∈ V at l-th layer is defined as: ! X l hlv = ρ Wl hl−1 (11) u +b , u∈N (v) 3"
2021.splurobonlp-1.5,H89-2010,0,0.449165,"Missing"
2021.splurobonlp-1.5,D14-1162,0,0.0853973,"{I1 , I2 , . . . , In } to I = I1 , I2 , · · · , In by a 0 fully-connected layer Ij = FCimg (Ij ). Then, a 0 soft attention is applied to I with the previous context ht−1 , as shown in Eq. 3. 0 0 I¯ = SoftAttnimg (Q = ht−1 ; K = I ; V = I ) (3) Furthermore, we equip the agent with objectbased representation. Specifically, we get top-K object representations from each image with an object detection model3 . In this paper, we consider two kinds of object representation: object label representation and object visual representation. Specifically, the label representation uses the GloVe embedding (Pennington et al., 2014) of the type of the object, and visual representation uses the region-of-interest (ROI) pooling of the object detection model. We will compare the two representations and a hybrid representation of them in Appendix A.1. Formally, the object representations could be denoted as O = [O1 , O2 . . . On ], where for image Ij , there is Oj = [oj,1 , oj,2 , · · · , oj,K ]. oj,k is the k-th object representation in j-th image. 3.5 p ˆ 3.4 Spatial Configuration Grounding To guarantee the sequential execution, we design a state attention mechanism over the configurations. Visual Representation To execute"
2021.splurobonlp-1.5,2020.lrec-1.717,1,0.884065,"020) annotated fine-grained subinstructions and their corresponding trajectories and used the co-grounded features of a part of instruction and the image to predict the next action. From the image side, Hu et al. (2019) induced a high-level object-based visual representation to ground the language into the visual context. In the same direction, we propose a neural agent, namely Spatial-Configuration-Based-Navigation (SpC-NAV), and consider the structure of both modalities, that is, spatial semantics of the instructions and the objects in the images. We use the notion of Spatial Configuration (Dan et al., 2020) to model the instructions and design a state attention to ensure the execution order of spatial configurations. Then, we utilize the spatial semantics elements, namely motion indicator, spatial indicator and landmark in spatial configuration to establish the connection with the visual environment. Specifically, we use the similarity score between the landmark representation in the spatial configurations and the object representation in the panoramic images to control the transitions between configurations. Also, we align object representations with the configuration representations enriched w"
2021.splurobonlp-1.5,N19-1268,0,0.161398,"t at the previous step. St = C˜L · O · αt−1 3.8 Training and Inference We train our model with two state-of-the-art training strategies in this task. (1) T1: We follow SelfMonitor (Ma et al., 2019) optimizing the model with a cross-entropy loss to maximize the likelihood of the ground-truth navigable viewpoint given by the model, and a mean squared error loss to minimize the normalized distance in units of length from the current viewpoint to the goal destination. At each step, the next viewpoint is selected by sampling the predicted probability of each navigable viewpoint. (2) T2: We follow (Tan et al., 2019) training the model with the mixture of Imitation Learning and Reinforcement Learning, where Imitation Learning minimizes the cross-entropy loss of the prediction and always samples the ground-truth navigable viewpoint at each time step, and Reinforcement Learning uses policy gradient to update the parameters of the model. During inference, we conduct a greedy search with the highest probability of the next viewpoints to generate the trajectory. It should be noticed that beam search with a beam size greater than one is not practical because the agent needs to move forward and backward in the p"
2021.splurobonlp-1.5,2020.emnlp-main.714,1,0.733816,"the location/trans-location of an object with respect to a reference or a path that can be perceived in the environment. It contains finegrained spatial roles, such as motion indicator, landmark, spatial indicator, trajector. Essentially, each spatial configuration forms a sub-instruction in our setting. Figure 1 shows an example of splitting an instruction into its corresponding spatial configurations and the extracted spatial roles. Previous research argues representing the semantic structure of the language could improve the reasoning capabilities of deep learning models (Dan et al., 2020; Zheng and Kordjamshidi, 2020). There are relevant work modeling the meaning of spatial semantics in probabilistic models (Kollar et al., 2010; Tellex et al., 2011) and neural models (Regier, 1996; Ghanimifard and Dobnik, 2019). However, its impact on deep learning models for navigation remains an open research problem. The contribution of this paper is as follows: 1. We consider the spatial semantic structure of the instructions explicitly in terms of spatial configurations and their spatial semantic elements, i.e., spatial/motion indicators, and landmarks to enrich the configuration representations. 2. We introduce a sta"
2021.splurobonlp-1.5,2020.acl-main.229,0,0.0160586,"he ability to understand and follow natural language instructions is critical for intelligent agents to interact with humans and the physical world. One of the recently designed tasks in this direction is Vision-and-Language Navigation (VLN) (Anderson et al., 2018), which requires an agent to carry out a sequence of actions in a photo-realistic simulated environment in response to a sequence of natural language instructions. To accomplish this task, the agent should have three abilities: understanding linguistic semantics, perceiving the visual environment, and reasoning over both modalities (Zhu et al., 2020; Wang et al., 2019). While understanding vision and language are difficult problems by themselves, learning the connection between them without direct supervision makes this task even more challenging (Hong et al., 2020). To address this challenge, some neural agents establish the connection using attention mechanism 42 Proceedings of Second International Combined Workshop on Spatial Language Understanding andGrounded Communication for Robotics, pages 42–52 August 5–6, 2020. ©2021 Association for Computational Linguistics significantly in the seen environments and yields competitive results i"
2021.sustainlp-1.8,D19-1352,1,0.89724,"Missing"
2021.sustainlp-1.8,2020.findings-emnlp.412,0,0.0333139,"in the Age of Muppets: Effectiveness–Efficiency Tradeoffs in Multi-Stage Ranking Yue Zhang,1∗ Chengcheng Hu,2∗ Yuqi Liu,2∗ Hui Fang,1 Jimmy Lin2 1 Department of Electrical and Computer Engineering, University of Delaware 2 David R. Cheriton School of Computer Science, University of Waterloo {zhangyue, hfang}@udel.edu, {c45hu, y899liu, jimmylin}@uwaterloo.ca Abstract 2019), and this realization has compelled the field to explore other approaches, for example, simplified models (Hofstätter et al., 2020; Soldaini and Moschitti, 2020; Mitra et al., 2020; MacAvaney et al., 2020; Gao et al., 2020; Jiang et al., 2020) and learned dense representations (Xiong et al., 2020; Lin et al., 2020b). We are also motivated by the desire to reduce the computational costs of ranking with transformers, but from a different perspective. Based on the observation that neural networks in general (and transformers in particular) have largely supplanted feature-based learning to rank (LTR) in modern information retrieval, we ask the question: What, if anything, does “traditional” feature-based learning to rank have to offer in the age of muppets?1 The subtext of this question is that we, as a field, should not forget our own"
2021.sustainlp-1.8,N19-1423,0,0.0203672,"Applied to the MS MARCO passage and document ranking tasks, we are able to achieve the same level of effectiveness, but with up to 18× increase in efficiency. Furthermore, our techniques are orthogonal to other methods focused on accelerating transformer inference, and thus can be combined for even greater efficiency gains. A higher-level message from our work is that, even though pretrained transformers dominate the modern IR landscape, there are still important roles for “traditional” LTR techniques, and that we should not forget history. 1 Introduction Pretrained transformers such as BERT (Devlin et al., 2019) have dramatically increased retrieval effectiveness in many tasks across a multitude of domains (Lin et al., 2020a). Nevertheless, in a standard “retrieve-then-rerank” setup, the application of pretrained transformer-based rerankers incurs large computational costs and long query latencies, making those rerankers unrealistic for many real-world applications. For example, according to the ColBERT paper (Khattab and Zaharia, 2020), reranking 1000 hits from the MS MARCO passage dataset takes 32.9 seconds per query. Other researchers have noted the computational costs of transformer-based rankers"
2021.sustainlp-1.8,2020.sustainlp-1.11,1,0.821882,"n be reduced in various ways. We can accelerate inference using smaller or simpler models. Gao et al. (2020) use distillation to transfer knowledge captured in a larger model into a smaller model, achieving substantial speedups with minimal effectiveness loss. Hofstätter et al. (2020) propose a simpler transformer model to capture contextual information that trades effectiveness for much faster inference. Additional examples of this approach include Mitra et al. (2020) and MacAvaney et al. (2020). An alternative is to introduce early-exit optimizations, as in Soldaini and Moschitti (2020) and Xin et al. (2020). Further speedups can be gained by making modifications to the backbone transformer model, as in Sanh et al. (2020). The key point is that our 6 Conclusions The “retrieve-then-rerank” approach with transformers has been demonstrated to be effective in many IR tasks, but poor efficiency makes it less attractive for real-world applications. Our goal is to increase the efficiency of the entire pipeline but at the same time maintain the same level of effectiveness: this is achieved by a feature-based learning-to-rank module that filters candidates prior to neural reranking. On the MS MARCO passag"
2021.sustainlp-1.8,2020.findings-emnlp.63,1,0.826021,"0.382 0.382 0.443 0.445 0.444 5.60s 0.92s 0.46s (6×) (12×) BoWd2q (1k) + BERT BoWd2q (10k) + LTRd2q (50) + BERT 1000 50 0.389 0.389 0.454 0.454 9.63s 0.83s (12×) BoWd2q (1k) + T5 BoWd2q (10k) + LTRd2q (50) + T5 1000 50 0.386 0.388 0.453 0.454 5.60s 0.63s (9×) Table 2: The effectiveness and efficiency of different pipeline configurations on the MS MARCO passage ranking task. The effectiveness of the pipelines with additional LTR modules are statistically indistinguishable from the baselines without the LTR modules. on final-stage neural reranking. Previous evaluations (Nogueira and Cho, 2019; Nogueira et al., 2020; Pradeep et al., 2021) have already verified that these two models serve as competitive baselines. We pad all the token sequences in the batch to have the same length and truncate them if their lengths exceed 512 tokens. can reach MRR@10 parity with the baseline. We conduct two-tailed paired t-tests to confirm that there are no significant effectiveness differences between results before and after inserting LTR as the filtering stage. Depending on the pipeline setup, we only need to perform neural inference on 20 to 100 candidates—precisely because of our LTR filtering. We report the per-quer"
2021.sustainlp-1.8,2020.acl-main.504,0,0.0217153,"achieve a good balance between effectiveness and efficiency in end-to-end retrieval. Motivated by the observation, dating back more than a decade, that effective techniques are often computationally expensive, multi-stage retrieval architectures control latency by applying expensive techniques over only the most promising candidates (Wang et al., 2011). This is often operationalized as optimizing for recall in the earlier stages of the pipeline. Specifically in the context of transformers, multi-stage neural pipelines have been explored in the past by many researchers (Nogueira et al., 2019a; Soldaini and Moschitti, 2020; Matsubara et al., 2020; Pradeep et al., 2021). The key difference in our work is the (re-)introduction of “traditional” feature-based learning-to-rank approaches alongside neural models. This aligns with our broader goal of investigating how learning to rank might contribute to modern retrieval approaches dominated by neural models. The computational costs associated with ranking using pretrained transformers can be reduced in various ways. We can accelerate inference using smaller or simpler models. Gao et al. (2020) use distillation to transfer knowledge captured in a larger model into a s"
C10-2168,J99-2004,0,0.347837,"s on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations, e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al., 1998), and ﬁrst-order logical forms (Bos et al., 2004). One of the properties of the grammar formalism is that it is lexicalised, associating CCG lexical categories, or CCG supertags, with the words in a sentence (Steedman, 2000). Clark and Curran (2004) adapt the technique of supertagging (Bangalore and Joshi, 1999) to CCG, using a standard maximum entropy tagger to assign small sets of supertags to each word. The reduction in ambiguity resulting from the supertagging stage results in a surprisingly efﬁcient parser, given the rich structural output, operating at tens of newspaper sentences per second. In this paper we demonstrate that the CCG parser can be made more than twice as fast, with little or no loss in accuracy. A noteworthy feature of the CCG parser is that, after the supertagging 1471 Coling 2010: Poster Volume, pages 1471–1479, Beijing, August 2010 stage, the parser builds a complete packed c"
C10-2168,C04-1180,1,0.805962,"is a system that performs open information extraction on the web (Lin et al., 2009). However, the text processing that is performed by TextRunner, in particular the parsing, is rudimentary: ﬁnite-state shallow parsing technology that In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations, e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al., 1998), and ﬁrst-order logical forms (Bos et al., 2004). One of the properties of the grammar formalism is that it is lexicalised, associating CCG lexical categories, or CCG supertags, with the words in a sentence (Steedman, 2000). Clark and Curran (2004) adapt the technique of supertagging (Bangalore and Joshi, 1999) to CCG, using a standard maximum entropy tagger to assign small sets of supertags to each word. The reduction in ambiguity resulting from the supertagging stage results in a surprisingly efﬁcient parser, given the rich structural output, operating at tens of newspaper sentences per second. In this paper we demonstrate that the CCG pa"
C10-2168,P06-2006,0,0.0364055,"Missing"
C10-2168,P05-1022,0,0.187649,"ate that the CCG parser can be made more than twice as fast, with little or no loss in accuracy. A noteworthy feature of the CCG parser is that, after the supertagging 1471 Coling 2010: Poster Volume, pages 1471–1479, Beijing, August 2010 stage, the parser builds a complete packed chart, storing all sentences consistent with the assigned supertags and the parser’s CCG combinatory rules, with no chart pruning whatsoever. The use of chart pruning techniques, typically some form of beam search, is essential for practical parsing using Penn Treebank parsers (Collins, 1999; Petrov and Klein, 2007; Charniak and Johnson, 2005), as well as practical parsers based on linguistic formalisms, such as HPSG (Ninomiya et al., 2005) and LFG (Kaplan et al., 2004). However, in the CCG case, the use of the supertagger means that enough ambiguity has already been resolved to allow the complete chart to be represented. Despite the effectiveness of the supertagging stage, the number of derivations stored in a packed chart can still be enormous for typical newspaper sentences. Hence it is an obvious question whether chart pruning techniques can be profitably applied to the CCG parser. Some previous work (Djordjevic et al., 2007) h"
C10-2168,A00-2018,0,0.336784,"he supertagger output can be stored in a packed chart. However, wide-coverage CCG parsers still produce a very large number of derivations for typical newspaper or Wikipedia sentences. In this paper we investigate two forms of chart pruning, and develop a novel method for pruning complete cells in a parse chart. The result is a widecoverage CCG parser that can process almost 100 sentences per second, with little or no loss in accuracy over the baseline with no pruning. Many of the popular wide-coverage parsers available today operate at around one newspaper sentence per second (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007). There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). Introduction Many NLP tasks and applications require the processing of massive amounts of textual data. For example, knowledge acquisition efforts can involve processing billions of words of text (Curran, 2004). Also, the increasing need to process large amounts of web data places an efﬁciency"
C10-2168,C04-1041,1,0.967384,"e-state shallow parsing technology that In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations, e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al., 1998), and ﬁrst-order logical forms (Bos et al., 2004). One of the properties of the grammar formalism is that it is lexicalised, associating CCG lexical categories, or CCG supertags, with the words in a sentence (Steedman, 2000). Clark and Curran (2004) adapt the technique of supertagging (Bangalore and Joshi, 1999) to CCG, using a standard maximum entropy tagger to assign small sets of supertags to each word. The reduction in ambiguity resulting from the supertagging stage results in a surprisingly efﬁcient parser, given the rich structural output, operating at tens of newspaper sentences per second. In this paper we demonstrate that the CCG parser can be made more than twice as fast, with little or no loss in accuracy. A noteworthy feature of the CCG parser is that, after the supertagging 1471 Coling 2010: Poster Volume, pages 1471–1479, B"
C10-2168,P10-1036,1,0.767973,"Missing"
C10-2168,N06-1020,0,0.0609905,"taggers. 5.2.1 Final experiments using gold training and self training In this section we report our ﬁnal tests using Wikipedia data. We used two methods to derive training data for the taggers. The ﬁrst is the standard method, which is to transform gold-standard parse trees into begin and end tag sequences. This method is the method that we used for all previous experiments, and we call it “gold training”. In addition to gold training, we also investigate an alternative method, which is to obtain training data for the taggers from the output of the parser itself, in a form of self-training (McClosky et al., 2006). The intuition is that the tagger will learn what constituents a trained parser will eventually choose, and as long as the constituents favoured by the parsing model are not pruned, no reduction in accuracy can occur. There is the potential for an increase in speed, however, due to the pruning effect. For gold training, we used sections 02-21 of 1477 Model baseline binary gold binary 40K binary 200K binary 1M level gold level 40K level 200K level 1M Speed 47.6 80.8 75.5 77.4 78.6 93.7 92.8 92.5 96.6 CCGbank) did not improve the self-training results. We did see the usual speed improvements fr"
C10-2168,W05-1511,0,0.0222118,"orthy feature of the CCG parser is that, after the supertagging 1471 Coling 2010: Poster Volume, pages 1471–1479, Beijing, August 2010 stage, the parser builds a complete packed chart, storing all sentences consistent with the assigned supertags and the parser’s CCG combinatory rules, with no chart pruning whatsoever. The use of chart pruning techniques, typically some form of beam search, is essential for practical parsing using Penn Treebank parsers (Collins, 1999; Petrov and Klein, 2007; Charniak and Johnson, 2005), as well as practical parsers based on linguistic formalisms, such as HPSG (Ninomiya et al., 2005) and LFG (Kaplan et al., 2004). However, in the CCG case, the use of the supertagger means that enough ambiguity has already been resolved to allow the complete chart to be represented. Despite the effectiveness of the supertagging stage, the number of derivations stored in a packed chart can still be enormous for typical newspaper sentences. Hence it is an obvious question whether chart pruning techniques can be profitably applied to the CCG parser. Some previous work (Djordjevic et al., 2007) has investigated this question but with little success. In this paper we investigate two types of ch"
C10-2168,J07-4004,1,0.948597,"essing of massive amounts of textual data. For example, knowledge acquisition efforts can involve processing billions of words of text (Curran, 2004). Also, the increasing need to process large amounts of web data places an efﬁciency demand on existing NLP tools. TextRunner, for example, is a system that performs open information extraction on the web (Lin et al., 2009). However, the text processing that is performed by TextRunner, in particular the parsing, is rudimentary: ﬁnite-state shallow parsing technology that In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations, e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al., 1998), and ﬁrst-order logical forms (Bos et al., 2004). One of the properties of the grammar formalism is that it is lexicalised, associating CCG lexical categories, or CCG supertags, with the words in a sentence (Steedman, 2000). Clark and Curran (2004) adapt the technique of supertagging (Bangalore and Joshi, 1999) to CCG, using a standard maximum entropy tagger to a"
C10-2168,C04-1010,0,0.0481003,"in a parse chart. The result is a widecoverage CCG parser that can process almost 100 sentences per second, with little or no loss in accuracy over the baseline with no pruning. Many of the popular wide-coverage parsers available today operate at around one newspaper sentence per second (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007). There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). Introduction Many NLP tasks and applications require the processing of massive amounts of textual data. For example, knowledge acquisition efforts can involve processing billions of words of text (Curran, 2004). Also, the increasing need to process large amounts of web data places an efﬁciency demand on existing NLP tools. TextRunner, for example, is a system that performs open information extraction on the web (Lin et al., 2009). However, the text processing that is performed by TextRunner, in particular the parsing, is rudimentary: ﬁnite-state shallow parsing technology that In this paper"
C10-2168,N07-1051,0,0.469875,"utput can be stored in a packed chart. However, wide-coverage CCG parsers still produce a very large number of derivations for typical newspaper or Wikipedia sentences. In this paper we investigate two forms of chart pruning, and develop a novel method for pruning complete cells in a parse chart. The result is a widecoverage CCG parser that can process almost 100 sentences per second, with little or no loss in accuracy over the baseline with no pruning. Many of the popular wide-coverage parsers available today operate at around one newspaper sentence per second (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007). There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). Introduction Many NLP tasks and applications require the processing of massive amounts of textual data. For example, knowledge acquisition efforts can involve processing billions of words of text (Curran, 2004). Also, the increasing need to process large amounts of web data places an efﬁciency demand on existing NLP t"
C10-2168,E03-1071,1,0.80584,"Missing"
C10-2168,W96-0213,0,0.368336,"Missing"
C10-2168,N09-1073,0,0.236863,"epresented. Despite the effectiveness of the supertagging stage, the number of derivations stored in a packed chart can still be enormous for typical newspaper sentences. Hence it is an obvious question whether chart pruning techniques can be profitably applied to the CCG parser. Some previous work (Djordjevic et al., 2007) has investigated this question but with little success. In this paper we investigate two types of chart pruning: a standard beam search, similar to that used in the Collins parser (Collins, 1999), and a more aggressive strategy in which complete cells are pruned, following Roark and Hollingshead (2009). Roark and Hollingshead use a ﬁnite-state tagger to decide which words in a sentence can end or begin constituents, from which whole cells in the chart can be removed. We develop a novel extension to this approach, in which a tagger is trained to infer the maximum length constituent that can begin or end at a particular word. These lengths can then be used in a more agressive pruning strategy which we show to be signiﬁcantly more effective than the basic approach. Both beam search and cell pruning are highly effective, with the resulting CCG parser able to process almost 100 sentences per sec"
C10-2168,W07-2206,1,0.892852,"harniak and Johnson, 2005), as well as practical parsers based on linguistic formalisms, such as HPSG (Ninomiya et al., 2005) and LFG (Kaplan et al., 2004). However, in the CCG case, the use of the supertagger means that enough ambiguity has already been resolved to allow the complete chart to be represented. Despite the effectiveness of the supertagging stage, the number of derivations stored in a packed chart can still be enormous for typical newspaper sentences. Hence it is an obvious question whether chart pruning techniques can be profitably applied to the CCG parser. Some previous work (Djordjevic et al., 2007) has investigated this question but with little success. In this paper we investigate two types of chart pruning: a standard beam search, similar to that used in the Collins parser (Collins, 1999), and a more aggressive strategy in which complete cells are pruned, following Roark and Hollingshead (2009). Roark and Hollingshead use a ﬁnite-state tagger to decide which words in a sentence can end or begin constituents, from which whole cells in the chart can be removed. We develop a novel extension to this approach, in which a tagger is trained to infer the maximum length constituent that can be"
C10-2168,J07-3004,0,0.0352215,"with pointers to the children used in the creation. Equivalence is deﬁned in terms of the category and head of the constituent, to enable the Viterbi algorithm to efﬁciently ﬁnd the highest scoring derivation.1 A textbook treatment of CKY applied to statistical parsing is given in Jurafsky and Martin (2000). 2 We performed efﬁciency and accuracy tests on newspaper and Wikipedia data. For the newspaper data, we used the standard test sections from The CCG Parser The parser is described in detail in Clark and Curran (2007). It is based on CCGbank, a CCG version of the Penn Treebank developed by Hockenmaier and Steedman (2007). 3 Data and Evaluation Metrics 1 Use of the Viterbi algorithm in this way requires the features in the parser model to be local to a single rule application; Clark and Curran (2007) has more discussion. 1472 (ncmod num hundred 1 Seven 0) (conj and 2 sixty-one 3) (conj and 2 hundred 1) (dobj in 6 total 7) (ncmod made 5 in 6) (aux made 5 were 4) (ncsubj made 5 and 2 obj) (passive made 5) β Baseline 0.001 0.002 0.005 0.01 Seven hundred and sixty-one were made in total. Figure 1: Example Wikipedia test sentence annotated with grammatical relations. CCGbank. Following Clark and Curran (2007) we us"
C10-2168,J00-4006,0,0.00657188,"starting with two-word constituents (assuming the supertagging phase has been completed), incrementally increasing the span until the whole sentence is covered. The chart is packed in the standard sense that any two equivalent constituents created during the parsing process are placed in the same equivalence class, with pointers to the children used in the creation. Equivalence is deﬁned in terms of the category and head of the constituent, to enable the Viterbi algorithm to efﬁciently ﬁnd the highest scoring derivation.1 A textbook treatment of CKY applied to statistical parsing is given in Jurafsky and Martin (2000). 2 We performed efﬁciency and accuracy tests on newspaper and Wikipedia data. For the newspaper data, we used the standard test sections from The CCG Parser The parser is described in detail in Clark and Curran (2007). It is based on CCGbank, a CCG version of the Penn Treebank developed by Hockenmaier and Steedman (2007). 3 Data and Evaluation Metrics 1 Use of the Viterbi algorithm in this way requires the features in the parser model to be local to a single rule application; Clark and Curran (2007) has more discussion. 1472 (ncmod num hundred 1 Seven 0) (conj and 2 sixty-one 3) (conj and 2 h"
C10-2168,N04-1013,0,\N,Missing
C10-2168,J03-4003,0,\N,Missing
C12-2073,W06-1615,0,0.270605,"Missing"
C12-2073,W03-0407,0,0.0270842,"Missing"
C12-2073,P07-1034,0,0.281063,"orums, and internet literature, which are written in a different genre, and for which little manual annotation is available. In this paper, we choose internet literature as the target domain, and study domain adaptation for joint segmentation and POS-tagging. We consider the single model approach of Zhang and Clark (2010), trained using the CTB, as our baseline system, and apply self-training and character clustering to improve its performance on our test data from an internet novel. Much work has been done on domain adaptation for POS-tagging (Blitzer et al., 2006; Daumé III and Marcu, 2006; Jiang and Zhai, 2007). However, relatively little attention has been paid to the domain adaptation for joint segmentation and POS-tagging. Among the range of methods that have been developed for domain adaptation, self-training and character clustering are applicable to a comparatively large number of baseline supervised model types, including feature-based probability models and large-margin discriminative models, and are fairly straightforward to implement. We focus on unsupervised domain adaptation, using fully unannotated data in the target-domain. We evaluate our system on a set of manually annotated target-d"
C12-2073,P08-1102,0,0.0165596,"Missing"
C12-2073,C08-1049,0,0.0242137,"Missing"
C12-2073,P08-1068,0,0.0113675,"in Saerens et al. (2002), EM is applied to the target-domain only, and the source data is used for an initial estimation. In this paper, we apply the standard self-training process, but with target-domain data point selection (Rehbein, 2011; Søgaard, 2011). 3 Character clustering Word/character clustering is an unsupervised approach that groups similar words/characters according to their context. Clusters can be used as features instead of the original words/characters for the reduction of data sparsity. Word clustering has been applied to many NLP problems (Miller et al., 2004; Liang, 2005; Koo et al., 2008). For our domain adaptation problem, clusters are created from large unannotated targetdomain data, and applied as features in our joint segmentor and POS-tagger during both training and testing. The weights of the cluster features are estimated during training using source-domain data. During testing, they can help to alleviate the out-of-vocabulary (OOV) problem in the target-domain when a rare input has not been seen in the training data but belongs to a known cluster. We use Liang’s implementation (Liang, 2005) of the bottom-up agglomerative Brown algorithm (Brown et al., 1992) to generate"
C12-2073,P09-1058,0,0.0222645,"tion (Liang, 2005) of the bottom-up agglomerative Brown algorithm (Brown et al., 1992) to generate character clusters, choosing the numbers of clusters according to development experiments. 4 Experiments Software We use ZPar (Zhang and Clark, 2010, 2011) as the baseline system1 . The system uses a single discriminative model for joint segmentation and tagging, trained using the generalized perceptron algorithm. Standard beam search is applied to ensure efficient decoding. Source-domain data We use the CTB 5 for source-domain training, making the same training, development and test sections as Kruengkrai et al. (2009) (Table 1). Target-domain data We collect the target-domain data from a Chinese Internet novel “Jade dynasty”2 (also known as “Zhuxian”) by Ding Xiao. The first 18 chapters (927K words in 25413 sentences) have been collected. Section 1 of chapter 6 is used as the development data, and 1 2 www.sourceforge.net/project/zpar; version 0.4 An electronic version of the book is free for download from the Internet. 747 Data set chap. IDs # of sen. # of words Training 1-5, 8-18 6.1 7.2 25413 927405 159 226 5077 5173 Development Test Table 2: Target-domain training, development and test data. BE FA DC BD"
C12-2073,N06-1020,0,0.0202212,"Missing"
C12-2073,N04-1043,0,0.00851174,"ta is increased at each iteration; in Saerens et al. (2002), EM is applied to the target-domain only, and the source data is used for an initial estimation. In this paper, we apply the standard self-training process, but with target-domain data point selection (Rehbein, 2011; Søgaard, 2011). 3 Character clustering Word/character clustering is an unsupervised approach that groups similar words/characters according to their context. Clusters can be used as features instead of the original words/characters for the reduction of data sparsity. Word clustering has been applied to many NLP problems (Miller et al., 2004; Liang, 2005; Koo et al., 2008). For our domain adaptation problem, clusters are created from large unannotated targetdomain data, and applied as features in our joint segmentor and POS-tagger during both training and testing. The weights of the cluster features are estimated during training using source-domain data. During testing, they can help to alleviate the out-of-vocabulary (OOV) problem in the target-domain when a rare input has not been seen in the training data but belongs to a known cluster. We use Liang’s implementation (Liang, 2005) of the bottom-up agglomerative Brown algorithm"
C12-2073,W04-3236,0,0.0298088,"Missing"
C12-2073,W11-3808,0,0.0116869,"nd unlabeled data, the supervised training algorithm, and additional reranking and filtering of output predictions. Modifications can be made to the standard self-training process for domain adaptation to address the difference in source and target distributions (Margolis, 2011). In Tan et al. (2009), the weights on the target-domain data is increased at each iteration; in Saerens et al. (2002), EM is applied to the target-domain only, and the source data is used for an initial estimation. In this paper, we apply the standard self-training process, but with target-domain data point selection (Rehbein, 2011; Søgaard, 2011). 3 Character clustering Word/character clustering is an unsupervised approach that groups similar words/characters according to their context. Clusters can be used as features instead of the original words/characters for the reduction of data sparsity. Word clustering has been applied to many NLP problems (Miller et al., 2004; Liang, 2005; Koo et al., 2008). For our domain adaptation problem, clusters are created from large unannotated targetdomain data, and applied as features in our joint segmentor and POS-tagger during both training and testing. The weights of the cluster f"
C12-2073,P07-1078,0,0.0158414,"Missing"
C12-2073,N03-1027,0,0.0319633,"s reported. Clark et al. (2003) apply self-training to POS -tagging and achieve minor improvements. Steedman et al. (2003) report that self-training can either slightly improve or significantly harm the parsing accuracy. McClosky et al. (2006) achieves improved parsing accuracies using self-training, and Reichart and Rappoport (2007) has obtained significant improvement on small datasets with lexicalized parser. In this paper, we focus on the use of self-training for unsupervised domain adaptation. Selftraining has been applied to the domain adaptation of several NLP tasks, including parsing (Roark and Bacchiani, 2003; Sagae, 2010), POS-tagging (Jiang and Zhai, 2007) and crosslanguage text classification (Shi et al., 2010). It improves system performance on the target domain by simultaneously modelling annotated source-domain data and unannotated targetdomain data in the training process. Theoretically, self-training has a strong relationship with the EM algorithm, where tagging unlabeled data corresponds to the expectation step, and supervised parameter estimation corresponds to the maximization step. There are various factors that affects the effectiveness of self-training, such as the difference in the"
C12-2073,W10-2606,0,0.0144402,"003) apply self-training to POS -tagging and achieve minor improvements. Steedman et al. (2003) report that self-training can either slightly improve or significantly harm the parsing accuracy. McClosky et al. (2006) achieves improved parsing accuracies using self-training, and Reichart and Rappoport (2007) has obtained significant improvement on small datasets with lexicalized parser. In this paper, we focus on the use of self-training for unsupervised domain adaptation. Selftraining has been applied to the domain adaptation of several NLP tasks, including parsing (Roark and Bacchiani, 2003; Sagae, 2010), POS-tagging (Jiang and Zhai, 2007) and crosslanguage text classification (Shi et al., 2010). It improves system performance on the target domain by simultaneously modelling annotated source-domain data and unannotated targetdomain data in the training process. Theoretically, self-training has a strong relationship with the EM algorithm, where tagging unlabeled data corresponds to the expectation step, and supervised parameter estimation corresponds to the maximization step. There are various factors that affects the effectiveness of self-training, such as the difference in the distributions"
C12-2073,D10-1103,0,0.0195218,"(2003) report that self-training can either slightly improve or significantly harm the parsing accuracy. McClosky et al. (2006) achieves improved parsing accuracies using self-training, and Reichart and Rappoport (2007) has obtained significant improvement on small datasets with lexicalized parser. In this paper, we focus on the use of self-training for unsupervised domain adaptation. Selftraining has been applied to the domain adaptation of several NLP tasks, including parsing (Roark and Bacchiani, 2003; Sagae, 2010), POS-tagging (Jiang and Zhai, 2007) and crosslanguage text classification (Shi et al., 2010). It improves system performance on the target domain by simultaneously modelling annotated source-domain data and unannotated targetdomain data in the training process. Theoretically, self-training has a strong relationship with the EM algorithm, where tagging unlabeled data corresponds to the expectation step, and supervised parameter estimation corresponds to the maximization step. There are various factors that affects the effectiveness of self-training, such as the difference in the distributions of 746 Data set chap. IDs # of sen. # of words Training 1-270, 400-931, 1001-1151 301-325 271"
C12-2073,P11-2120,0,0.0172639,"ta, the supervised training algorithm, and additional reranking and filtering of output predictions. Modifications can be made to the standard self-training process for domain adaptation to address the difference in source and target distributions (Margolis, 2011). In Tan et al. (2009), the weights on the target-domain data is increased at each iteration; in Saerens et al. (2002), EM is applied to the target-domain only, and the source data is used for an initial estimation. In this paper, we apply the standard self-training process, but with target-domain data point selection (Rehbein, 2011; Søgaard, 2011). 3 Character clustering Word/character clustering is an unsupervised approach that groups similar words/characters according to their context. Clusters can be used as features instead of the original words/characters for the reduction of data sparsity. Word clustering has been applied to many NLP problems (Miller et al., 2004; Liang, 2005; Koo et al., 2008). For our domain adaptation problem, clusters are created from large unannotated targetdomain data, and applied as features in our joint segmentor and POS-tagger during both training and testing. The weights of the cluster features are esti"
C12-2073,E03-1008,0,0.0151779,"Missing"
C12-2073,P11-1139,0,0.0669048,"Missing"
C12-2073,C10-2168,1,0.876311,"Missing"
C12-2073,P08-1101,1,0.83525,"Missing"
C12-2073,D10-1082,1,0.730777,"督学习，聚类，自学习 Proceedings of COLING 2012: Posters, pages 745–754, COLING 2012, Mumbai, December 2012. 745 1 Introduction Joint segmentation and POS-tagging can improve upon a pipelined baseline by reducing error propagation and accommodating features that represent combined word and POS information. Three general approaches have been taken to perform joint inference, namely two-stage ensemble methods (Jiang et al., 2008a; Sun, 2011), reranking (Jiang et al., 2008b; Shi and Wang, 2007) and single joint models with heuristic search (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), leading to improved accuracies on the Chinese Treebank data. All these methods rely on supervised learning, and are expected to perform worse when the test domain shifts from CTB to blogs, computer forums, and internet literature, which are written in a different genre, and for which little manual annotation is available. In this paper, we choose internet literature as the target domain, and study domain adaptation for joint segmentation and POS-tagging. We consider the single model approach of Zhang and Clark (2010), trained using the CTB, as our baseline system, and apply self-training and"
C12-2073,J11-1005,1,0.851308,"Missing"
C12-2073,C04-1081,0,\N,Missing
C12-2073,P04-1015,0,\N,Missing
C12-2073,D10-1056,0,\N,Missing
C12-2073,P08-1085,0,\N,Missing
C12-2073,D12-1075,0,\N,Missing
C12-2073,J94-2001,0,\N,Missing
C12-2073,I11-1035,0,\N,Missing
C12-2073,J92-4003,0,\N,Missing
C12-2073,P13-1076,0,\N,Missing
C12-2073,P09-1057,0,\N,Missing
C12-2073,petrov-etal-2012-universal,0,\N,Missing
C12-2073,Q13-1001,0,\N,Missing
C12-2073,P13-1057,0,\N,Missing
C12-2073,N13-1014,0,\N,Missing
C12-2073,P12-1110,0,\N,Missing
C12-2073,I05-3025,0,\N,Missing
C12-2073,D10-1017,0,\N,Missing
C12-2073,P07-1094,0,\N,Missing
C12-2073,P07-1033,0,\N,Missing
C12-2136,E12-1009,0,0.0271337,"RDS IN CHINESE: 依存分析, 错误分析, ZPar, MaltParser, MSTParser Proceedings of COLING 2012: Posters, pages 1391–1400, COLING 2012, Mumbai, December 2012. 1391 1 Introduction Beam-search has been applied to transition-based dependency parsing in recent studies (Zhang and Clark, 2008; Huang and Sagae, 2010; Hatori et al., 2011). In addition to reducing search errors compared to greedy search, it also enables the use of global models that accommodate richer non-local features without overfitting, leading to recent state-of-the-art accuracies of transition-based dependency parsing (Zhang and Nivre, 2011; Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012) that are competitive with the best graph-based dependency parsers. It has been known that a transition-based parser using global learning, beam-search and rich features gives significantly higher accuracies than one with local learning and greedy search. However, the effects of global learning, beam-search and rich features have not been separately studied. Apart from the natural conclusion that beam-search reduces error propagation compared to greedy search, exactly how these techniques help to improve parsing has not been discussed, and many interesting questions re"
C12-2136,D12-1133,1,0.756664,"误分析, ZPar, MaltParser, MSTParser Proceedings of COLING 2012: Posters, pages 1391–1400, COLING 2012, Mumbai, December 2012. 1391 1 Introduction Beam-search has been applied to transition-based dependency parsing in recent studies (Zhang and Clark, 2008; Huang and Sagae, 2010; Hatori et al., 2011). In addition to reducing search errors compared to greedy search, it also enables the use of global models that accommodate richer non-local features without overfitting, leading to recent state-of-the-art accuracies of transition-based dependency parsing (Zhang and Nivre, 2011; Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012) that are competitive with the best graph-based dependency parsers. It has been known that a transition-based parser using global learning, beam-search and rich features gives significantly higher accuracies than one with local learning and greedy search. However, the effects of global learning, beam-search and rich features have not been separately studied. Apart from the natural conclusion that beam-search reduces error propagation compared to greedy search, exactly how these techniques help to improve parsing has not been discussed, and many interesting questions remain unanswered. For exam"
C12-2136,W06-2920,0,0.435748,"xhaustive parsing than local, greedy parsing in the use of global models and non-greedy search. On the other hand, beam-search does not affect the fundamental transition-based parsing process, which allows the use of rich non-local features, and is very different from graph-based parsing. An interesting question is how such differences in models and algorithms affect empirical errors. McDonald and Nivre (2007) make a comparative analysis of local greedy transition-based MaltParser and global near-exhaustive graph-based MSTParser (McDonald and Pereira, 2006) using the CoNLL-X Shared Task data (Buchholz and Marsi, 2006), showing that the parsers give near identical overall accuracies, but have very different error distributions according to various metrics. While MaltParser is more accurate on frequently occurring short sentences and dependencies, it performs worse on long sentences and dependencies due to search errors. We present empirical studies of the error distribution of global, beam-search transition-based dependency parsing, using ZPar (Zhang and Nivre, 2011) as a representative system. We follow McDonald and Nivre (2007) and perform a comparative error analysis of ZPar, MSTParser and MaltParser usi"
C12-2136,W02-1001,0,0.0497659,"Missing"
C12-2136,P04-1015,0,0.0603703,"Missing"
C12-2136,I11-1136,0,0.0151324,"以取得与最好的基于图的依存分析器 同一水平的精度。我们分析全局学习和柱搜索对基于转移的依存分析器的精度与错误分布 的影响。首先，全局学习和柱搜索需要同时使用才能达到显著优于局部学习和贪婪搜索的 效果。此外，全局学习和柱搜索的联合使用不仅可以减少错误蔓延，还可以支持更为复杂 的模型训练而不过拟合。最后，我们对应用了全局学习和柱搜索的基于转移的依存分析器 进行错误分析，且将此分析与对MaltParser与MSTParser的错误对比相比较。 KEYWORDS: Dependency parsing, error analysis, ZPar, MaltParser, MSTParser. KEYWORDS IN CHINESE: 依存分析, 错误分析, ZPar, MaltParser, MSTParser Proceedings of COLING 2012: Posters, pages 1391–1400, COLING 2012, Mumbai, December 2012. 1391 1 Introduction Beam-search has been applied to transition-based dependency parsing in recent studies (Zhang and Clark, 2008; Huang and Sagae, 2010; Hatori et al., 2011). In addition to reducing search errors compared to greedy search, it also enables the use of global models that accommodate richer non-local features without overfitting, leading to recent state-of-the-art accuracies of transition-based dependency parsing (Zhang and Nivre, 2011; Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012) that are competitive with the best graph-based dependency parsers. It has been known that a transition-based parser using global learning, beam-search and rich features gives significantly higher accuracies than one with local learning and greedy search. However, the effe"
C12-2136,P10-1110,0,0.113183,"柱搜索和全局模型被应用于基于转移的依存分析，可以取得与最好的基于图的依存分析器 同一水平的精度。我们分析全局学习和柱搜索对基于转移的依存分析器的精度与错误分布 的影响。首先，全局学习和柱搜索需要同时使用才能达到显著优于局部学习和贪婪搜索的 效果。此外，全局学习和柱搜索的联合使用不仅可以减少错误蔓延，还可以支持更为复杂 的模型训练而不过拟合。最后，我们对应用了全局学习和柱搜索的基于转移的依存分析器 进行错误分析，且将此分析与对MaltParser与MSTParser的错误对比相比较。 KEYWORDS: Dependency parsing, error analysis, ZPar, MaltParser, MSTParser. KEYWORDS IN CHINESE: 依存分析, 错误分析, ZPar, MaltParser, MSTParser Proceedings of COLING 2012: Posters, pages 1391–1400, COLING 2012, Mumbai, December 2012. 1391 1 Introduction Beam-search has been applied to transition-based dependency parsing in recent studies (Zhang and Clark, 2008; Huang and Sagae, 2010; Hatori et al., 2011). In addition to reducing search errors compared to greedy search, it also enables the use of global models that accommodate richer non-local features without overfitting, leading to recent state-of-the-art accuracies of transition-based dependency parsing (Zhang and Nivre, 2011; Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012) that are competitive with the best graph-based dependency parsers. It has been known that a transition-based parser using global learning, beam-search and rich features gives significantly higher accuracies than one with local learning and greedy sea"
C12-2136,D07-1013,1,0.950889,"d only when the two are jointly applied. Second, we show that the accuracies of a local, greedy transition-based parser cannot be improved by adding the rich features of Zhang and Nivre (2011). Our result suggests that global learning with beam-search accommodates more complex models with richer features than a local model with greedy search and therefore enables higher accuracies. One interesting aspect of using a global model with beam-search is that it narrows down the contrast between “local, greedy, transition-based parsing” and “global, exhaustive, graph-based parsing” as exemplified by McDonald and Nivre (2007). On the one hand, global beam-search parsing is more similar to global, exhaustive parsing than local, greedy parsing in the use of global models and non-greedy search. On the other hand, beam-search does not affect the fundamental transition-based parsing process, which allows the use of rich non-local features, and is very different from graph-based parsing. An interesting question is how such differences in models and algorithms affect empirical errors. McDonald and Nivre (2007) make a comparative analysis of local greedy transition-based MaltParser and global near-exhaustive graph-based M"
C12-2136,E06-1011,0,0.083071,"ne hand, global beam-search parsing is more similar to global, exhaustive parsing than local, greedy parsing in the use of global models and non-greedy search. On the other hand, beam-search does not affect the fundamental transition-based parsing process, which allows the use of rich non-local features, and is very different from graph-based parsing. An interesting question is how such differences in models and algorithms affect empirical errors. McDonald and Nivre (2007) make a comparative analysis of local greedy transition-based MaltParser and global near-exhaustive graph-based MSTParser (McDonald and Pereira, 2006) using the CoNLL-X Shared Task data (Buchholz and Marsi, 2006), showing that the parsers give near identical overall accuracies, but have very different error distributions according to various metrics. While MaltParser is more accurate on frequently occurring short sentences and dependencies, it performs worse on long sentences and dependencies due to search errors. We present empirical studies of the error distribution of global, beam-search transition-based dependency parsing, using ZPar (Zhang and Nivre, 2011) as a representative system. We follow McDonald and Nivre (2007) and perform a co"
C12-2136,P09-1040,1,0.894307,"Missing"
C12-2136,W06-2933,1,0.812759,"tion compared to greedy search, exactly how these techniques help to improve parsing has not been discussed, and many interesting questions remain unanswered. For example, the contribution of global learning in improving the accuracies has not been separately studied. It has not been shown how global learning affects the accuracies, or whether it is important at all. For another example, it would be interesting to know whether a local, greedy, transition-based parser can be equipped with the rich features of Zhang and Nivre (2011) to improve its accuracy, and in particular whether MaltParser (Nivre et al., 2006) can achieve the same level of accuracies as ZPar (Zhang and Nivre, 2011) by using the same range of rich feature definitions. In this paper, we answer the above questions empirically. First, we separate out global learning and beam-search, and study the effect of each technique by comparison with a local greedy baseline. Our results show that significant improvements are achieved only when the two are jointly applied. Second, we show that the accuracies of a local, greedy transition-based parser cannot be improved by adding the rich features of Zhang and Nivre (2011). Our result suggests that"
C12-2136,P05-1013,1,0.492204,"we evaluate the parsers on the CoNLL-X Shared Task data (Buchholz and Marsi, 2006), which include training and test sentences for 13 different languages. For each parser, we conjoin the outputs for all 13 languages in the same way as McDonald and Nivre (2007), and calculate error distributions over the aggregated output. Accuracies are measured using the labeled attached score (LAS) evaluation metric, which is defined as the percentage of words (excluding punctuation) that are assigned both the correct head word and the correct arc label. To handle non-projectivity, pseudo-projective parsing (Nivre and Nilsson, 2005) is applied to ZPar and MaltParser, transforming non-projective trees into pseudo-projective trees in the training data, and post-processing pseudo-projective outputs by the parser to transform them into non-projective trees. MSTParser produces non-projective trees from projective trees by scorebased rearrangements of arcs. 3.2 Error distributions We take a range of different perspectives to characterize the errors of ZPar, comparing them with those of MaltParser and MSTParser by measuring the accuracies against various types of metrics, including the size of the sentences and dependency arcs,"
C12-2136,D08-1059,1,0.893122,"析全局模型和柱搜索对基于转移依存分析器的影响 柱搜索和全局模型被应用于基于转移的依存分析，可以取得与最好的基于图的依存分析器 同一水平的精度。我们分析全局学习和柱搜索对基于转移的依存分析器的精度与错误分布 的影响。首先，全局学习和柱搜索需要同时使用才能达到显著优于局部学习和贪婪搜索的 效果。此外，全局学习和柱搜索的联合使用不仅可以减少错误蔓延，还可以支持更为复杂 的模型训练而不过拟合。最后，我们对应用了全局学习和柱搜索的基于转移的依存分析器 进行错误分析，且将此分析与对MaltParser与MSTParser的错误对比相比较。 KEYWORDS: Dependency parsing, error analysis, ZPar, MaltParser, MSTParser. KEYWORDS IN CHINESE: 依存分析, 错误分析, ZPar, MaltParser, MSTParser Proceedings of COLING 2012: Posters, pages 1391–1400, COLING 2012, Mumbai, December 2012. 1391 1 Introduction Beam-search has been applied to transition-based dependency parsing in recent studies (Zhang and Clark, 2008; Huang and Sagae, 2010; Hatori et al., 2011). In addition to reducing search errors compared to greedy search, it also enables the use of global models that accommodate richer non-local features without overfitting, leading to recent state-of-the-art accuracies of transition-based dependency parsing (Zhang and Nivre, 2011; Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012) that are competitive with the best graph-based dependency parsers. It has been known that a transition-based parser using global learning, beam-search and rich features gives significantly higher accuracies than one with local"
C12-2136,J11-1005,1,0.498712,"sing the shift-reduce process than a global tree search. 4 Conclusion We studied empirically the effect of global learning and beam-search on the overall accuracies and error distributions of transition-based dependency parsing. We first analyzed the ways in which global learning and beam-search improved parsing accuracies over local learning and greedy search, showing that they allow more complex parsing models without overfitting, including the use of rich non-local features and online reordering for non-projective parsing, which result in state-of-the-art accuracies (Zhang and Nivre, 2011; Zhang and Clark, 2011; Bohnet and Nivre, 2012). We also showed that the effects result from the interaction between global learning and beam-search, and that applying either of the techniques by itself does not lead to improvements over local learning and greedy search. We then performed a detailed error analysis of a global, beam-search transition-based dependency parser, relating it to the classic comparison of local greedy transition-based and global near-exhaustive graph-based parsing (McDonald and Nivre, 2007). Our results might serve to inspire further parser developments by providing more insights into thes"
C12-2136,P11-2033,1,0.525544,"arser, MSTParser. KEYWORDS IN CHINESE: 依存分析, 错误分析, ZPar, MaltParser, MSTParser Proceedings of COLING 2012: Posters, pages 1391–1400, COLING 2012, Mumbai, December 2012. 1391 1 Introduction Beam-search has been applied to transition-based dependency parsing in recent studies (Zhang and Clark, 2008; Huang and Sagae, 2010; Hatori et al., 2011). In addition to reducing search errors compared to greedy search, it also enables the use of global models that accommodate richer non-local features without overfitting, leading to recent state-of-the-art accuracies of transition-based dependency parsing (Zhang and Nivre, 2011; Bohnet and Kuhn, 2012; Bohnet and Nivre, 2012) that are competitive with the best graph-based dependency parsers. It has been known that a transition-based parser using global learning, beam-search and rich features gives significantly higher accuracies than one with local learning and greedy search. However, the effects of global learning, beam-search and rich features have not been separately studied. Apart from the natural conclusion that beam-search reduces error propagation compared to greedy search, exactly how these techniques help to improve parsing has not been discussed, and many i"
C14-1026,W09-3036,0,0.115541,"Missing"
C14-1026,I11-1138,0,0.0433831,"Missing"
C14-1026,W09-2307,0,0.0597728,"Parser / / ZPar-con / / Constituent Parsing len<=40 words Unlimited / / / / / / / / 85.25 84.22 85.02 84.12 Constituent Parsing(DS2PS) len<=40 words Unlimited 84.77 83.43 85.47 84.33 85.53 84.47 85.92 84.84 / / / / Table 5: Parsing results on our treebank using automatic POS-tags. 31K Chinese-English sentence pairs from the Xinhua Corpus (Liu et al., 2006), and we used NIST MT Evaluation 2006 test set as the development set, and the NIST 2003 (MT03), 2004 (MT04) and 2005 (MT05) test sets as the test sets. For Stanford dependency trees, we parsed the source sentences with the Stanford Parser (Chang et al., 2009) (version 3.3.1), which was trained on CTB 7.0. For the PMT treebank, we used the Ours-PS parser, trained with 14000 sentences (the last 463 sentences are used as development data for the parser). All the MT configurations are the same as Xie et al. (2011). The results are shown in Table 6. The Chinese-English translation outputs using our parser and treebank are slightly lower but comparable to those using the Stanford Parser. Note that our treebank contains 336K words on People’s Daily, while the CTB 7.0 contains about 1.19M words, most on Xinhua, the source of the MT training and test data."
C14-1026,A00-2018,0,0.188435,"s of the multi-view framework, we implemented an arc-standard transition-based dependency parser and added phrase structure features produced by the phrase structure view. Experimental results show the effectiveness of additional features for dependency parsing. Further, experiments on dependency-to-string machine translation show that our treebank and parser could achieve similar results compared to the Stanford Parser trained on CTB 7.0. 1 Introduction Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into th"
C14-1026,P99-1065,0,0.351502,"Missing"
C14-1026,J03-4003,0,0.0418252,"he effectiveness of the multi-view framework, we implemented an arc-standard transition-based dependency parser and added phrase structure features produced by the phrase structure view. Experimental results show the effectiveness of additional features for dependency parsing. Further, experiments on dependency-to-string machine translation show that our treebank and parser could achieve similar results compared to the Stanford Parser trained on CTB 7.0. 1 Introduction Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treeba"
C14-1026,D07-1098,0,0.013097,"hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on statistical dependency parsing has frequently used dependency treebanks converted from phrase structure treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1993) and Penn Chinese Treebank (CTB) (Xue et al., 2000). However, previous research shows that dependency categories in converted treebanks are simplified (Johansson and Nugues, 2007), and the widely used head-table PS to DS conversion approach encounters ambiguities and uncertainty, especially for complex coordination structures (Xue, 2007). The main reason is that the PS treebanks were designe"
C14-1026,C12-1052,0,0.0788052,"similar results compared to the Stanford Parser trained on CTB 7.0. 1 Introduction Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on statistical dependency parsing has frequently used dependency treebanks converted from phrase structure treebanks, such as the Penn Treebank (PT"
C14-1026,hajic-etal-2012-announcing,0,0.0651536,"Missing"
C14-1026,W00-1205,0,0.0798795,"ross-clause punctuations (PUS). 3.1.2 A Case Study: Generating the Hierarchy of Coordination Structure We take coordination structures as an example to illustrate the PS hierarchy generation process. Typically, researchers treat the rightmost conjunct as the head of a coordinate structure. However, doing so introduces modifier scope ambiguities when modifiers are also attached to the rightmost head. Vice versa, treating the leftmost conjunct as the head will lead to ambiguities when modifiers attached to the left head (Che et al., 2012). Another choice is treating the conjunction as the head (Huang et al., 2000; Xue, 2007). However, this is usually not preferred since it makes parsing more difficult and a choice still has to be made between the left and right elements when there is no conjunction in a coordinate structure (Xue, 2007). Our strategy is as follows: (1) Choose the rightmost conjunct as the head to eliminate the ambiguities when the modifiers are attached to the left; (2) Classify coordinate structures into common coordinate structures (COO) and sharing-right-child coordinate structures (COS). COO words are taken as common left nodes (as shown in Figure 2), while COS words are special le"
C14-1026,W07-2416,0,0.029084,"endents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on statistical dependency parsing has frequently used dependency treebanks converted from phrase structure treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1993) and Penn Chinese Treebank (CTB) (Xue et al., 2000). However, previous research shows that dependency categories in converted treebanks are simplified (Johansson and Nugues, 2007), and the widely used head-table PS to DS conversion approach encounters ambiguities and uncertainty, especially for complex coordination structures (Xue, 2007). The main reason is that the PS tre"
C14-1026,P06-1077,0,0.0134568,"rd) i (dawn) q (again) ˜ (one) g(time) ü (come) 3 (in) ô (the Pearl River) • (estuary)). “!” denotes the head constituent. Dependency Parsing Parsers UAS LAS Mate-tools 82.98 79.37 ZPar-dep 82.73 80.20 Ours-standard 82.81 80.04 Ours-PS 83.28 80.50 Berkeley Parser / / ZPar-con / / Constituent Parsing len<=40 words Unlimited / / / / / / / / 85.25 84.22 85.02 84.12 Constituent Parsing(DS2PS) len<=40 words Unlimited 84.77 83.43 85.47 84.33 85.53 84.47 85.92 84.84 / / / / Table 5: Parsing results on our treebank using automatic POS-tags. 31K Chinese-English sentence pairs from the Xinhua Corpus (Liu et al., 2006), and we used NIST MT Evaluation 2006 test set as the development set, and the NIST 2003 (MT03), 2004 (MT04) and 2005 (MT05) test sets as the test sets. For Stanford dependency trees, we parsed the source sentences with the Stanford Parser (Chang et al., 2009) (version 3.3.1), which was trained on CTB 7.0. For the PMT treebank, we used the Ours-PS parser, trained with 14000 sentences (the last 463 sentences are used as development data for the parser). All the MT configurations are the same as Xie et al. (2011). The results are shown in Table 6. The Chinese-English translation outputs using ou"
C14-1026,J93-2004,0,0.0495929,"n and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on statistical dependency parsing has frequently used dependency treebanks converted from phrase structure treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1993) and Penn Chinese Treebank (CTB) (Xue et al., 2000). However, previous research shows that dependency categories in converted treebanks are simplified (Johansson and Nugues, 2007), and the widely used head-table PS to DS conversion approach encounters ambiguities and uncertainty, especially for complex coordination structures (Xue, 2007). The main reason is that the PS treebanks were designed without consideration of DS conversion, leading to inherent ambiguities in the mapping, and loss of information in the resulting DS treebanks. To minimize information loss during treebank conversions, a t"
C14-1026,H05-1066,0,0.321181,"Missing"
C14-1026,D13-1108,0,0.0544104,"Missing"
C14-1026,J08-4003,0,0.025968,"treebanks, we perform empirical analysis to the treebank, by the statistical dependency parsing and dependency-tostring machine translation tasks. Several researchers explored joint DS and PS information to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013). Most tried to combine the outputs of constituent and dependency parsers by stacking or bagging. Since our treebank is multi-view, it is possible to combine DS features and PS features directly in the decoding process. We implemented an arc-standard transition-based dependency parser (Nivre, 2008) based on the arceager parser of Zhang and Nivre (2011), which is a state-of-the-art transition-based dependency parser (Zhang and Nivre, 2012). It is more reasonable to derive the phrasal category of a phrase after the complete subtree (phrase) rather than partial subtree headed by a word has been built. The arc-standard parser differs from the arc-eager parser in that it postpones the attachment of right-modifiers until the complete subtrees headed by the modifiers themselves have been built. Because of this, we add PS features into an arc-standard parser rather than an arc-eager one. The pa"
C14-1026,N07-1051,0,0.0127179,"-standard transition-based dependency parser and added phrase structure features produced by the phrase structure view. Experimental results show the effectiveness of additional features for dependency parsing. Further, experiments on dependency-to-string machine translation show that our treebank and parser could achieve similar results compared to the Stanford Parser trained on CTB 7.0. 1 Introduction Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to"
C14-1026,rambow-etal-2002-dependency,0,0.0525225,"d) v (verb) w (punctuation) Table 1: Mapping from PKU POS to our POS. the dependency categories between them (for terminal words, parts-of-speech can be used as phrasal categories). Consequently, in Chinese, the canonical PS, containing information of constituent hierarchies and phrasal categories, can be derived naturally from the canonical DS. As Xia et al. (2009) stated, a rich set of dependency categories should be designed to ensure lossless conversion from DS to PS. When the information of PS has been represented in DS explicitly or implicitly, we can convert DS to PS without ambiguity (Rambow et al., 2002). Given our framework, a multi-view Chinese treebank, containing 14,463 sentences and 336K words, is constructed. This main corpus is based on the Peking University People’s Daily Corpus. We name our treebank the Peking University Multi-view Chinese Treebank (PMT) release 1.0. To verify the usefulness of the treebank for statistical NLP, a transition-based dependency parser is implemented to include PS features produced in the derivation process of phrasal categories. We perform a set of empirical evaluations, with experimental results on both dependency parsing and dependency-to-string machin"
C14-1026,N10-1049,0,0.0173777,"in the mapping, and loss of information in the resulting DS treebanks. To minimize information loss during treebank conversions, a treebank could be designed by considering PS and DS information simultaneously; such treebanks have been proposed as multi-view treebanks (Xia et al., 2009). We develop a multi-view treebank for Chinese, which treats PS and DS as different views of the same internal structures of a sentence. We choose the DS view as the base view, from which PS would be derived. Our choice is based on the effectiveness of information transfer rather than convenience of annotation (Rambow, 2010; Bhatt and Xia, 2012). Research on Chinese syntax (Zhu, 1982; Chen, 1999; Chen, 2009) shows that the phrasal category of a constituent can be derived from the phrasal categories of its immediate subconstituents and This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 257 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 257–268, Dublin, Ireland, August 23-29 2014. PKU POS"
C14-1026,N04-1032,0,0.0394926,"on between heads and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on statistical dependency parsing has frequently used dependency treebanks converted from phrase structure treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1993) and Penn Chinese Treebank (CTB) (Xue et al., 2000). However, previous research shows that dependency categories in converted treebanks are simplified (Johansson and Nugues, 2007), and the widely used head-table PS to DS conversion approach encounters ambiguities and uncertainty, especially for complex coordination structures (Xue, 2007). The ma"
C14-1026,Q13-1025,0,0.0657543,"to the Stanford Parser trained on CTB 7.0. 1 Introduction Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on statistical dependency parsing has frequently used dependency treebanks converted from phrase structure treebanks, such as the Penn Treebank (PTB) (Marcus et al., 1"
C14-1026,N03-1033,0,0.0288741,"Missing"
C14-1026,C10-2148,0,0.088723,"parser could achieve similar results compared to the Stanford Parser trained on CTB 7.0. 1 Introduction Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to convert PS into DS (Magerman, 1994; Collins et al., 1999; Collins, 2003; Sun and Jurafsky, 2004; Johansson and Nugues, 2007; Duan et al., 2007; Zhang and Clark, 2008). Research on statistical dependency parsing has frequently used dependency treebanks converted from phrase structure treebanks, such"
C14-1026,H01-1014,0,0.0746166,"Missing"
C14-1026,D11-1020,0,0.124266,"r example. Figure 3(a) shows the correct PS while Figure 3(b) shows an incorrect parser output. In particular, “i (dawn)” is put under the incorrect constituent. When converted into DS, both lead to the correct link, with “i  (dawn)” being the SBV modifier of “ü (come)” (Figure 3(c)). As a result, the PS parser error is erased in the conversion into DS. The same can happen in DS to PS conversion. 6.2 Dependency-to-string Machine Translation We compare the effects of our treebank and the Stanford dependencies converted from CTB on machine translation, using the dependency-to-string system of Xie et al. (2011). Our training corpus consists of 2 https://code.google.com/p/mate-tools/ http://code.google.com/p/berkeley-parser-analyser/ 4 http://sourceforge.net/projects/zpar/ 3 263 Figure 3: An instance where PS parser error is erased in the PS to DS conversion (ðD (bright) (de, an auxiliary word) i (dawn) q (again) ˜ (one) g(time) ü (come) 3 (in) ô (the Pearl River) • (estuary)). “!” denotes the head constituent. Dependency Parsing Parsers UAS LAS Mate-tools 82.98 79.37 ZPar-dep 82.73 80.20 Ours-standard 82.81 80.04 Ours-PS 83.28 80.50 Berkeley Parser / / ZPar-con / / Constituent Parsing len<=40 word"
C14-1026,W03-3023,0,0.206075,"Missing"
C14-1026,D08-1059,1,0.809789,"ed dependency parser and added phrase structure features produced by the phrase structure view. Experimental results show the effectiveness of additional features for dependency parsing. Further, experiments on dependency-to-string machine translation show that our treebank and parser could achieve similar results compared to the Stanford Parser trained on CTB 7.0. 1 Introduction Phrase structures (PS) and dependency structures (DS) are two of the most popular grammar formalisms for statistical parsing (Collins, 2003; Charniak, 2000; McDonald et al., 2005; Nivre, 2006; Petrov and Klein, 2007; Zhang and Clark, 2008). While DS trees emphasize the grammatical relation between heads and dependents, PS trees stress the hierarchical constituent structures of sentences. Several researchers have explored DS and PS simultaneously to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013) and tree-to-string machine translation (Meng et al., 2013), showing that the two types of information complement each other for NLP tasks. Most existing Chinese and English treebanks fall into the phrase structure category, and much work has been done to convert PS into DS (Mage"
C14-1026,W09-3825,1,0.878569,"Missing"
C14-1026,J11-1005,1,0.896959,"Missing"
C14-1026,P11-2033,1,0.81137,"he treebank, by the statistical dependency parsing and dependency-tostring machine translation tasks. Several researchers explored joint DS and PS information to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013). Most tried to combine the outputs of constituent and dependency parsers by stacking or bagging. Since our treebank is multi-view, it is possible to combine DS features and PS features directly in the decoding process. We implemented an arc-standard transition-based dependency parser (Nivre, 2008) based on the arceager parser of Zhang and Nivre (2011), which is a state-of-the-art transition-based dependency parser (Zhang and Nivre, 2012). It is more reasonable to derive the phrasal category of a phrase after the complete subtree (phrase) rather than partial subtree headed by a word has been built. The arc-standard parser differs from the arc-eager parser in that it postpones the attachment of right-modifiers until the complete subtrees headed by the modifiers themselves have been built. Because of this, we add PS features into an arc-standard parser rather than an arc-eager one. The parser processes a sentence from left to right, using a s"
C14-1026,C12-2136,1,0.850113,"lation tasks. Several researchers explored joint DS and PS information to enhance the quality of syntactic parsing (Wang and Zong, 2010; Farkas and Bohnet, 2012; Sun and Wan, 2013). Most tried to combine the outputs of constituent and dependency parsers by stacking or bagging. Since our treebank is multi-view, it is possible to combine DS features and PS features directly in the decoding process. We implemented an arc-standard transition-based dependency parser (Nivre, 2008) based on the arceager parser of Zhang and Nivre (2011), which is a state-of-the-art transition-based dependency parser (Zhang and Nivre, 2012). It is more reasonable to derive the phrasal category of a phrase after the complete subtree (phrase) rather than partial subtree headed by a word has been built. The arc-standard parser differs from the arc-eager parser in that it postpones the attachment of right-modifiers until the complete subtrees headed by the modifiers themselves have been built. Because of this, we add PS features into an arc-standard parser rather than an arc-eager one. The parser processes a sentence from left to right, using a stack to maintain partially built derivations and a queue to hold next incoming words. Th"
C14-1026,P13-1043,1,0.901629,"Missing"
C14-1078,P05-1001,0,0.00996289,"cluster-based features for dependency parsing models. Suzuki et al. (2009) adapted a Semi-supervised Structured Conditional Model (SS-SCM) to dependency parsing. Suzuki et al. (2011) reported the best results so far on the standard test sets of PTB using a condensed feature representation combined with the word cluster-based features of Koo et al. (2008). Chen et al. (2013) mapped the base features into predefined types using the information of frequencies counted in large amounts of auto-parsed data. The work of Suzuki et al. (2011) and Chen et al. (2013) were to perform feature clustering. Ando and Zhang (2005) presented a semi-supervised learning algorithm named alternating structure optimization for text chunking. They used a large projection matrix to map sparse base features into a small number of high level features over a large number of auxiliary problems. One of the advantages of our approach is that it is simpler and more general than that of Ando and Zhang (2005). Our approach can easily be applied to other tasks by defining new feature contexts. 7 Conclusion In this paper, we have presented an approach to learning feature embeddings for dependency parsing from large amounts of raw data. B"
C14-1078,D12-1133,0,0.0372811,"ph-based parsing model proposed by McDonald et al. (2005). 2.1 Dependency parsing Given an input sentence x = (w0 , w1 , ..., wi , ..., wm ), where w0 is ROOT and wi (i = 0) refers to a word, the task of dependency parsing is to find y ∗ which has the highest score for x, y ∗ = arg max score(x, y) y∈Y (x) where Y (x) is the set of all the valid dependency trees for x. There are two major models (Nivre and McDonald, 2008): the transition-based model and graph-based model, which showed comparable accuracies for a wide range of languages (Nivre et al., 2007; Bohnet, 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). We apply feature embeddings to a graph-based model in this paper. 2.2 Graph-based parsing model We use an ordered pair (wi , wj ) ∈ y to define a dependency relation in tree y from word wi to word wj (wi is the head and wj is the dependent), and Gx to define a graph that consists of a set of nodes Vx = {w0 , w1 , ..., wi , ..., wm } and a set of arcs (edges) Ex = {(wi , wj )|i = j, wi ∈ Vx , wj ∈ (Vx − {w0 })}. The parsing model of McDonald et al. (2005) searches for the maximum spanning tree (MST) in Gx . We denote Y (Gx ) as the set of all the subgraphs of Gx that are valid spanning trees"
C14-1078,C10-1011,0,0.102072,"hidden-class representations of features. Based on feature embeddings, we present a set of new features for graph-based dependency parsing models. Experiments on the standard Chinese and English data sets show that the new parser achieves significant performance improvements over a strong baseline. 1 Introduction Discriminative models have become the dominant approach for dependency parsing (Nivre et al., 2007; Zhang and Clark, 2008; Hatori et al., 2011). State-of-the-art accuracies have been achieved by the use of rich features in discriminative models (Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010; Zhang and Nivre, 2011). While lexicalized features extracted from non-local contexts enhance the discriminative power of parsers, they are relatively sparse. Given a limited set of training data (typically less than 50k sentences for dependency parsing), the chance of a feature occurring in the training data but not in the test data can be high. Another limitation on features is that many are typically derived by (manual) combination of atomic features. For example, given the head word (wh ) and part-of-speech tag (ph ), dependent word (wd ) and part-of-speech tag (pd ), and the label (l) of"
C14-1078,D07-1101,0,0.120651,"ned features but also benefit from the hidden-class representations of features. Based on feature embeddings, we present a set of new features for graph-based dependency parsing models. Experiments on the standard Chinese and English data sets show that the new parser achieves significant performance improvements over a strong baseline. 1 Introduction Discriminative models have become the dominant approach for dependency parsing (Nivre et al., 2007; Zhang and Clark, 2008; Hatori et al., 2011). State-of-the-art accuracies have been achieved by the use of rich features in discriminative models (Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010; Zhang and Nivre, 2011). While lexicalized features extracted from non-local contexts enhance the discriminative power of parsers, they are relatively sparse. Given a limited set of training data (typically less than 50k sentences for dependency parsing), the chance of a feature occurring in the training data but not in the test data can be high. Another limitation on features is that many are typically derived by (manual) combination of atomic features. For example, given the head word (wh ) and part-of-speech tag (ph ), dependent word (wd ) and part-of-s"
C14-1078,D09-1060,1,0.725213,"Missing"
C14-1078,D13-1129,1,0.599637,"in the Gigaword corpus. We report the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens (excluding all punctuation tokens) with the correct HEAD. We also report the scores on complete dependency tree matches (COMP). 1 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We excluded the texts of PTB from the BLLIP WSJ Corpus. 3 We excluded the texts of CTB5 from the Gigaword data. 2 822 Baseline Baseline+BrownClu M2 Koo and Collins (2010) Zhang and Nivre (2011) Koo et al. (2008) Suzuki et al. (2009) Chen et al. (2009) Zhou et al. (2011) Suzuki et al. (2011) Chen et al. (2013) UAS 92.78 93.37 93.74 93.04 92.9 93.16 93.79 93.16 92.64 94.22 93.77 Table 5: Results on English data. N/A=Not Available. COMP 48.08 49.26 50.82 N/A 48.0 N/A N/A 47.15 46.61 N/A 51.36 Baseline M2 Li et al. (2011a) Hatori et al. (2011) Li et al. (2012) Chen et al. (2013) POS 93.61 93.61 93.08 93.94 94.51 N/A UAS 81.04 82.94 80.74 81.33 81.21 83.08 COMP 29.73 31.72 29.11 29.90 N/A 32.21 Table 6: Results on Chinese data. N/A=Not Available. 5.2 Development experiments In this section, we use the English development data to investigate the effects of different vector sizes of feature embeddings, a"
C14-1078,J81-4005,0,0.781478,"Missing"
C14-1078,P05-1004,0,0.174592,"d ; pd ], [wh ; ph ; wd ], and so on, in addition to the atomic features: [wh ], [ph ], etc. Such combination is necessary for high accuracies because the dominant approach uses linear models, which can not capture complex correlations between atomic features. We tackle the above issues by borrowing solutions from word representations, which have been intensely studied in the NLP community (Turian et al., 2010). In particular, distributed representations of words have been used for many NLP problems, which represent a word by information from the words it frequently co-occurs with (Lin, 1997; Curran, 2005; Collobert et al., 2011; Bengio, 2009; Mikolov et al., 2013b). The representation can be learned from large amounts of raw sentences, and hence used to reduce OOV rates in test data. In addition, since the representation of each word carries information about its context words, it can also be used to calculate word similarity (Mikolov et al., 2013a), or used as additional semantic features (Koo et al., 2008). In this paper, we show that a distributed representation can be learned for features also. Learned from large amount of automatically parsed data, the representation of each feature can"
C14-1078,I11-1136,0,0.0100531,"auto-parsed data. Our target is to learn feature embeddings that can not only make full use of well-established hand-designed features but also benefit from the hidden-class representations of features. Based on feature embeddings, we present a set of new features for graph-based dependency parsing models. Experiments on the standard Chinese and English data sets show that the new parser achieves significant performance improvements over a strong baseline. 1 Introduction Discriminative models have become the dominant approach for dependency parsing (Nivre et al., 2007; Zhang and Clark, 2008; Hatori et al., 2011). State-of-the-art accuracies have been achieved by the use of rich features in discriminative models (Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010; Zhang and Nivre, 2011). While lexicalized features extracted from non-local contexts enhance the discriminative power of parsers, they are relatively sparse. Given a limited set of training data (typically less than 50k sentences for dependency parsing), the chance of a feature occurring in the training data but not in the test data can be high. Another limitation on features is that many are typically derived by (manual) combination of ato"
C14-1078,P10-1001,0,0.0136798,"also benefit from the hidden-class representations of features. Based on feature embeddings, we present a set of new features for graph-based dependency parsing models. Experiments on the standard Chinese and English data sets show that the new parser achieves significant performance improvements over a strong baseline. 1 Introduction Discriminative models have become the dominant approach for dependency parsing (Nivre et al., 2007; Zhang and Clark, 2008; Hatori et al., 2011). State-of-the-art accuracies have been achieved by the use of rich features in discriminative models (Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010; Zhang and Nivre, 2011). While lexicalized features extracted from non-local contexts enhance the discriminative power of parsers, they are relatively sparse. Given a limited set of training data (typically less than 50k sentences for dependency parsing), the chance of a feature occurring in the training data but not in the test data can be high. Another limitation on features is that many are typically derived by (manual) combination of atomic features. For example, given the head word (wh ) and part-of-speech tag (ph ), dependent word (wd ) and part-of-speech tag (pd ), and th"
C14-1078,P08-1068,0,0.883169,", 2010). In particular, distributed representations of words have been used for many NLP problems, which represent a word by information from the words it frequently co-occurs with (Lin, 1997; Curran, 2005; Collobert et al., 2011; Bengio, 2009; Mikolov et al., 2013b). The representation can be learned from large amounts of raw sentences, and hence used to reduce OOV rates in test data. In addition, since the representation of each word carries information about its context words, it can also be used to calculate word similarity (Mikolov et al., 2013a), or used as additional semantic features (Koo et al., 2008). In this paper, we show that a distributed representation can be learned for features also. Learned from large amount of automatically parsed data, the representation of each feature can be defined on the ∗ Corresponding author This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 816 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 816–826, Dublin, Ireland, August 23-29"
C14-1078,P09-1058,0,0.0231128,"Missing"
C14-1078,I11-1171,0,0.0397526,"Missing"
C14-1078,D11-1109,1,0.812188,"Missing"
C14-1078,C12-1103,1,0.858885,"Missing"
C14-1078,P97-1009,0,0.39586,"wh ; ph ; wd ; pd ], [wh ; ph ; wd ], and so on, in addition to the atomic features: [wh ], [ph ], etc. Such combination is necessary for high accuracies because the dominant approach uses linear models, which can not capture complex correlations between atomic features. We tackle the above issues by borrowing solutions from word representations, which have been intensely studied in the NLP community (Turian et al., 2010). In particular, distributed representations of words have been used for many NLP problems, which represent a word by information from the words it frequently co-occurs with (Lin, 1997; Curran, 2005; Collobert et al., 2011; Bengio, 2009; Mikolov et al., 2013b). The representation can be learned from large amounts of raw sentences, and hence used to reduce OOV rates in test data. In addition, since the representation of each word carries information about its context words, it can also be used to calculate word similarity (Mikolov et al., 2013a), or used as additional semantic features (Koo et al., 2008). In this paper, we show that a distributed representation can be learned for features also. Learned from large amount of automatically parsed data, the representation of eac"
C14-1078,D07-1013,0,0.103419,"et al. (2013a) and Mikolov et al. (2013b) introduce efficient models to learn high-quality word embeddings from extremely large amounts of raw text, which offer a possible solution to the efficiency issue of learning feature embeddings. We adapt their approach for learning feature embeddings, showing how an unordered feature context can be used to learn the representation of a set of complex features. Using this method, a large number of embeddings are trained from automatically parsed texts, based on which a set of new features are designed and incorporated into a graph-based parsing model (McDonald and Nivre, 2007). We conduct experiments on the standard data sets of the Penn English Treebank and the Chinese Treebank V5.1. The results indicate that our proposed approach significantly improves parsing accuracies. 2 Background In this section, we introduce the background of dependency parsing and build a baseline parser based on the graph-based parsing model proposed by McDonald et al. (2005). 2.1 Dependency parsing Given an input sentence x = (w0 , w1 , ..., wi , ..., wm ), where w0 is ROOT and wi (i = 0) refers to a word, the task of dependency parsing is to find y ∗ which has the highest score for x,"
C14-1078,P05-1012,0,0.474326,"f complex features. Using this method, a large number of embeddings are trained from automatically parsed texts, based on which a set of new features are designed and incorporated into a graph-based parsing model (McDonald and Nivre, 2007). We conduct experiments on the standard data sets of the Penn English Treebank and the Chinese Treebank V5.1. The results indicate that our proposed approach significantly improves parsing accuracies. 2 Background In this section, we introduce the background of dependency parsing and build a baseline parser based on the graph-based parsing model proposed by McDonald et al. (2005). 2.1 Dependency parsing Given an input sentence x = (w0 , w1 , ..., wi , ..., wm ), where w0 is ROOT and wi (i = 0) refers to a word, the task of dependency parsing is to find y ∗ which has the highest score for x, y ∗ = arg max score(x, y) y∈Y (x) where Y (x) is the set of all the valid dependency trees for x. There are two major models (Nivre and McDonald, 2008): the transition-based model and graph-based model, which showed comparable accuracies for a wide range of languages (Nivre et al., 2007; Bohnet, 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). We apply feature embeddings to a"
C14-1078,P08-1108,0,0.0123186,"icate that our proposed approach significantly improves parsing accuracies. 2 Background In this section, we introduce the background of dependency parsing and build a baseline parser based on the graph-based parsing model proposed by McDonald et al. (2005). 2.1 Dependency parsing Given an input sentence x = (w0 , w1 , ..., wi , ..., wm ), where w0 is ROOT and wi (i = 0) refers to a word, the task of dependency parsing is to find y ∗ which has the highest score for x, y ∗ = arg max score(x, y) y∈Y (x) where Y (x) is the set of all the valid dependency trees for x. There are two major models (Nivre and McDonald, 2008): the transition-based model and graph-based model, which showed comparable accuracies for a wide range of languages (Nivre et al., 2007; Bohnet, 2010; Zhang and Nivre, 2011; Bohnet and Nivre, 2012). We apply feature embeddings to a graph-based model in this paper. 2.2 Graph-based parsing model We use an ordered pair (wi , wj ) ∈ y to define a dependency relation in tree y from word wi to word wj (wi is the head and wj is the dependent), and Gx to define a graph that consists of a set of nodes Vx = {w0 , w1 , ..., wi , ..., wm } and a set of arcs (edges) Ex = {(wi , wj )|i = j, wi ∈ Vx , wj ∈"
C14-1078,W04-2407,0,0.0218123,"Missing"
C14-1078,W96-0213,0,0.194199,"Missing"
C14-1078,P13-1045,0,0.057052,"heir tagger includes rich external resources. 823 6 Related work Learning feature embeddings are related to two lines of research: deep learning models for NLP, and semi-supervised dependency parsing. Recent studies used deep learning models in a variety of NLP tasks. Turian et al. (2010) applied word embeddings to chunking and Named Entity Recognition (NER). Collobert et al. (2011) designed a unified neural network to learn distributed representations that were useful for part-of-speech tagging, chunking, NER, and semantic role labeling. They tried to avoid task-specific feature engineering. Socher et al. (2013) proposed a Compositional Vector Grammar, which combined PCFGs with distributed word representations. Zheng et al. (2013) investigated Chinese character embeddings for Chinese word segmentation and part-of-speech tagging. Wu et al. (2013) directly applied word embeddings to Chinese dependency parsing. In most cases, words or characters were the inputs to the learning systems and word/character embeddings were used for the tasks. Our work is different in that we explore distributed representations at the feature level and we can make full use of well-established hand-designed features. We use l"
C14-1078,D09-1058,0,0.174023,"egmentation and tagging, and the Baseline parser was used to parse the sentences in the Gigaword corpus. We report the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens (excluding all punctuation tokens) with the correct HEAD. We also report the scores on complete dependency tree matches (COMP). 1 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We excluded the texts of PTB from the BLLIP WSJ Corpus. 3 We excluded the texts of CTB5 from the Gigaword data. 2 822 Baseline Baseline+BrownClu M2 Koo and Collins (2010) Zhang and Nivre (2011) Koo et al. (2008) Suzuki et al. (2009) Chen et al. (2009) Zhou et al. (2011) Suzuki et al. (2011) Chen et al. (2013) UAS 92.78 93.37 93.74 93.04 92.9 93.16 93.79 93.16 92.64 94.22 93.77 Table 5: Results on English data. N/A=Not Available. COMP 48.08 49.26 50.82 N/A 48.0 N/A N/A 47.15 46.61 N/A 51.36 Baseline M2 Li et al. (2011a) Hatori et al. (2011) Li et al. (2012) Chen et al. (2013) POS 93.61 93.61 93.08 93.94 94.51 N/A UAS 81.04 82.94 80.74 81.33 81.21 83.08 COMP 29.73 31.72 29.11 29.90 N/A 32.21 Table 6: Results on Chinese data. N/A=Not Available. 5.2 Development experiments In this section, we use the English development data"
C14-1078,P11-2112,0,0.217599,"o parse the sentences in the Gigaword corpus. We report the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens (excluding all punctuation tokens) with the correct HEAD. We also report the scores on complete dependency tree matches (COMP). 1 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We excluded the texts of PTB from the BLLIP WSJ Corpus. 3 We excluded the texts of CTB5 from the Gigaword data. 2 822 Baseline Baseline+BrownClu M2 Koo and Collins (2010) Zhang and Nivre (2011) Koo et al. (2008) Suzuki et al. (2009) Chen et al. (2009) Zhou et al. (2011) Suzuki et al. (2011) Chen et al. (2013) UAS 92.78 93.37 93.74 93.04 92.9 93.16 93.79 93.16 92.64 94.22 93.77 Table 5: Results on English data. N/A=Not Available. COMP 48.08 49.26 50.82 N/A 48.0 N/A N/A 47.15 46.61 N/A 51.36 Baseline M2 Li et al. (2011a) Hatori et al. (2011) Li et al. (2012) Chen et al. (2013) POS 93.61 93.61 93.08 93.94 94.51 N/A UAS 81.04 82.94 80.74 81.33 81.21 83.08 COMP 29.73 31.72 29.11 29.90 N/A 32.21 Table 6: Results on Chinese data. N/A=Not Available. 5.2 Development experiments In this section, we use the English development data to investigate the effects of different vector sizes of fe"
C14-1078,P10-1040,0,0.227454,"rt-of-speech tag (ph ), dependent word (wd ) and part-of-speech tag (pd ), and the label (l) of a dependency arc, state-of-the-art dependency parsers can have the combined features: [wh ; ph ], [wh ; ph ; wd ; pd ], [wh ; ph ; wd ], and so on, in addition to the atomic features: [wh ], [ph ], etc. Such combination is necessary for high accuracies because the dominant approach uses linear models, which can not capture complex correlations between atomic features. We tackle the above issues by borrowing solutions from word representations, which have been intensely studied in the NLP community (Turian et al., 2010). In particular, distributed representations of words have been used for many NLP problems, which represent a word by information from the words it frequently co-occurs with (Lin, 1997; Curran, 2005; Collobert et al., 2011; Bengio, 2009; Mikolov et al., 2013b). The representation can be learned from large amounts of raw sentences, and hence used to reduce OOV rates in test data. In addition, since the representation of each word carries information about its context words, it can also be used to calculate word similarity (Mikolov et al., 2013a), or used as additional semantic features (Koo et"
C14-1078,W13-5708,0,0.0202883,"Missing"
C14-1078,W03-3023,0,0.258589,"Missing"
C14-1078,D08-1059,1,0.690511,"d from large amounts of auto-parsed data. Our target is to learn feature embeddings that can not only make full use of well-established hand-designed features but also benefit from the hidden-class representations of features. Based on feature embeddings, we present a set of new features for graph-based dependency parsing models. Experiments on the standard Chinese and English data sets show that the new parser achieves significant performance improvements over a strong baseline. 1 Introduction Discriminative models have become the dominant approach for dependency parsing (Nivre et al., 2007; Zhang and Clark, 2008; Hatori et al., 2011). State-of-the-art accuracies have been achieved by the use of rich features in discriminative models (Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010; Zhang and Nivre, 2011). While lexicalized features extracted from non-local contexts enhance the discriminative power of parsers, they are relatively sparse. Given a limited set of training data (typically less than 50k sentences for dependency parsing), the chance of a feature occurring in the training data but not in the test data can be high. Another limitation on features is that many are typically derived by (manu"
C14-1078,P11-2033,1,0.861683,"epresentations of features. Based on feature embeddings, we present a set of new features for graph-based dependency parsing models. Experiments on the standard Chinese and English data sets show that the new parser achieves significant performance improvements over a strong baseline. 1 Introduction Discriminative models have become the dominant approach for dependency parsing (Nivre et al., 2007; Zhang and Clark, 2008; Hatori et al., 2011). State-of-the-art accuracies have been achieved by the use of rich features in discriminative models (Carreras, 2007; Koo and Collins, 2010; Bohnet, 2010; Zhang and Nivre, 2011). While lexicalized features extracted from non-local contexts enhance the discriminative power of parsers, they are relatively sparse. Given a limited set of training data (typically less than 50k sentences for dependency parsing), the chance of a feature occurring in the training data but not in the test data can be high. Another limitation on features is that many are typically derived by (manual) combination of atomic features. For example, given the head word (wh ) and part-of-speech tag (ph ), dependent word (wd ) and part-of-speech tag (pd ), and the label (l) of a dependency arc, state"
C14-1078,D13-1061,0,0.0137224,"esearch: deep learning models for NLP, and semi-supervised dependency parsing. Recent studies used deep learning models in a variety of NLP tasks. Turian et al. (2010) applied word embeddings to chunking and Named Entity Recognition (NER). Collobert et al. (2011) designed a unified neural network to learn distributed representations that were useful for part-of-speech tagging, chunking, NER, and semantic role labeling. They tried to avoid task-specific feature engineering. Socher et al. (2013) proposed a Compositional Vector Grammar, which combined PCFGs with distributed word representations. Zheng et al. (2013) investigated Chinese character embeddings for Chinese word segmentation and part-of-speech tagging. Wu et al. (2013) directly applied word embeddings to Chinese dependency parsing. In most cases, words or characters were the inputs to the learning systems and word/character embeddings were used for the tasks. Our work is different in that we explore distributed representations at the feature level and we can make full use of well-established hand-designed features. We use large amounts of raw data to infer feature embeddings. There are several previous studies relevant to using raw data for d"
C14-1078,P11-1156,0,0.0129294,"Missing"
C14-1078,J92-4003,0,\N,Missing
C14-1078,D07-1096,0,\N,Missing
C16-1014,I13-1039,0,0.00921476,"the customer review domain (Ott et al., 2011; Mukherjee et al., 2013; Li et al., 2014). Various types of indicator features have been investigated. For examples, Jindal and Liu (2008) trained models using features based on the review content, the reviewer, and the product itself. Yoo and Gretzel (2009) gathered 40 truthful and 42 deceptive hotel reviews and manually compared the linguistic differences between them. Ott et al. (2011) created a benchmark dataset by employing Turkers to write fake reviews. Their data were adopted by a line of subsequent work (Ott et al., 2012; Feng et al., 2012; Feng and Hirst, 2013). For example, Feng et al. (2012) looked into syntactic features from Context Free Grammar (CFG) parse trees to improve the performance. Feng and Hirst (2013) built profiles of hotels from collections of 141 reviews, measuring the compatibility of customer reviews to the hotel profile, and using it as a feature for opinion spam detection. Recently, Li et al. (2014) created a wider-coverage benchmark, which comprises of data from three domains (Hotel, Restaurant, and Doctor), and explored generalized approaches for identifying online deceptive opinion spam. We adopt this dataset for our experim"
C16-1014,P12-2034,0,0.348667,"y been extended to the customer review domain (Ott et al., 2011; Mukherjee et al., 2013; Li et al., 2014). Various types of indicator features have been investigated. For examples, Jindal and Liu (2008) trained models using features based on the review content, the reviewer, and the product itself. Yoo and Gretzel (2009) gathered 40 truthful and 42 deceptive hotel reviews and manually compared the linguistic differences between them. Ott et al. (2011) created a benchmark dataset by employing Turkers to write fake reviews. Their data were adopted by a line of subsequent work (Ott et al., 2012; Feng et al., 2012; Feng and Hirst, 2013). For example, Feng et al. (2012) looked into syntactic features from Context Free Grammar (CFG) parse trees to improve the performance. Feng and Hirst (2013) built profiles of hotels from collections of 141 reviews, measuring the compatibility of customer reviews to the hotel profile, and using it as a feature for opinion spam detection. Recently, Li et al. (2014) created a wider-coverage benchmark, which comprises of data from three domains (Hotel, Restaurant, and Doctor), and explored generalized approaches for identifying online deceptive opinion spam. We adopt this"
C16-1014,P14-1062,0,0.178007,"beled reviews, Ren et al. (2014) proposed a semi-supervised learning method, and built an accurate classifier to identify deceptive reviews. Kim et al. (2015) introduced a frame-based semantic feature based on FrameNet. Experimental results show that semantic frame features can improve the classification accuracy. We focus on the review content in this paper, but their features can be used to extend our model. 2.2 Neural Network Models for Representation Learning Neural network models have been exploited to learn dense feature representation for a variety of NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Ren et al., 2016b). Distributed word representations (Mikolov et al., 2013) have been used as the basic building block by most models for NLP. Numerous methods have been proposed to learn representations of phrases and larger text segments from distributed word representations. For example, Le and Mikolov (2014) introduced paragraph vector to learn document representations, extending to word embedding methods of Mikolov et al. (2013). Socher et al. (2013) introduced a family of recursive neural networks to represent sentence-level semantic composition. Follow-up research includes recursive n"
C16-1014,D14-1181,0,0.00821706,"ather than find a most accurate neural model variation for this task. 3.1 Sentence Model We represent words using embeddings (Bengio et al., 2003), which are low-dimensional dense realvalued vectors. For each word w, we use a look-up matrix E to obtain its embedding e(w) ∈ RD , where E ∈ RD×V is a model parameter, D is the word vector dimension size and V is the vocabulary size. E can be randomly initialized from a uniform distribution (Socher et al., 2013), or pre-trained from a large raw corpus (Mikolov et al., 2013). As shown in the bottom of Figure 1, a convolutional neural network (CNN) (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2014) is used to learn dense representations of a sentence. We use three convolutional filters to capture the local semantics of n-grams of various granularities. Formally, denote a sentence consisting of n words as {w1 , w2 , .., wi , ..wn }. Each word wi is mapped to the embedding representation e(wi ) ∈ RD . A convolutional filter is a list of linear layers with shared parameters. Let D1 , D2 , D3 be the width of the three convolutional filters, respectively. We set D1 = 1, D2 = 2 and D3 = 3 for representing unigrams, bigrams and trigrams, res"
C16-1014,P14-1147,0,0.777683,"ce, machine learning methods for automatically detecting deceptive opinion spam can be useful. The objective of the task is to identify whether a given document is a spam or not. The majority of existing approaches follow the seminal work of Jindal and Liu (2008), employing classifiers with supervised learning. Most studies focus on designing effective features to enhance the classification performance. Typical features represent linguistic and psychological cues, but fail to effectively represent a document from the viewpoint of global discourse structures. For example, Ott et al. (2011) and Li et al. (2014) represent documents with Unigram, POS and LIWC (Linguistic Inquiry and Word Count) (Newman et al., 2003) features. Although such features give the strong performance, their sparsity makes it difficult to capture non-local semantic information over a sentence or discourse. Recently, neural network models have been used to learn semantic representations for NLP tasks (Le and Mikolov, 2014; Tang et al., 2015), achieving highly competitive results. Potential advantages of using neural networks for spam detection are three-fold. First, neural models use dense hidden layers for automatic feature co"
C16-1014,D15-1278,0,0.0941374,"). Socher et al. (2013) introduced a family of recursive neural networks to represent sentence-level semantic composition. Follow-up research includes recursive neural network with global feed backward mechanisms (Paulus et al., 2014), deep recursive layers (Irsoy and Cardie, 2014), and adaptive composition functions (Dong et al., 2014). Convolutional neural networks have been widely used for semantic composition (Kalchbrenner et al., 2014; Johnson and Zhang, 2014), automatically capturing n-gram information. Sequential models such as recurrent neural network or long short-term memory (LSTM) (Li et al., 2015a; Tang et al., 2015) have also been used for recurrent semantic composition. The attention mechanism was first proposed in machine translation (Bahdanau et al., 2014). Further uses of the attention mechanism include parsing (Vinyals et al., 2014), natural language question answering (Sukhbaatar et al., 2015; Kumar et al., 2015; Hermann et al., 2015), and image question answering (Yang et al., 2015). We explore CNN and recurrent neural networks with attention mechanism to learn document representation for detecting deceptive opinion spam, comparing their effect with bag-of-word and paragraph v"
C16-1014,P15-1107,0,0.0991985,"). Socher et al. (2013) introduced a family of recursive neural networks to represent sentence-level semantic composition. Follow-up research includes recursive neural network with global feed backward mechanisms (Paulus et al., 2014), deep recursive layers (Irsoy and Cardie, 2014), and adaptive composition functions (Dong et al., 2014). Convolutional neural networks have been widely used for semantic composition (Kalchbrenner et al., 2014; Johnson and Zhang, 2014), automatically capturing n-gram information. Sequential models such as recurrent neural network or long short-term memory (LSTM) (Li et al., 2015a; Tang et al., 2015) have also been used for recurrent semantic composition. The attention mechanism was first proposed in machine translation (Bahdanau et al., 2014). Further uses of the attention mechanism include parsing (Vinyals et al., 2014), natural language question answering (Sukhbaatar et al., 2015; Kumar et al., 2015; Hermann et al., 2015), and image question answering (Yang et al., 2015). We explore CNN and recurrent neural networks with attention mechanism to learn document representation for detecting deceptive opinion spam, comparing their effect with bag-of-word and paragraph v"
C16-1014,P11-1032,0,0.433191,"show that the dense neural features significantly outperforms the previous state-of-the-art methods, demonstrating the advantage of neural models in capturing semantic characteristics. In addition, automatic neural features and manual discrete features are complementary sources of information, and a combination leads to further improvements. 2 2.1 Related Work Deceptive Opinion Spam Detection Spam detection has been extensively investigated in the Web-page and E-mail domains (Gy¨ongyi et al., 2004; Ntoulas et al., 2006), while research has recently been extended to the customer review domain (Ott et al., 2011; Mukherjee et al., 2013; Li et al., 2014). Various types of indicator features have been investigated. For examples, Jindal and Liu (2008) trained models using features based on the review content, the reviewer, and the product itself. Yoo and Gretzel (2009) gathered 40 truthful and 42 deceptive hotel reviews and manually compared the linguistic differences between them. Ott et al. (2011) created a benchmark dataset by employing Turkers to write fake reviews. Their data were adopted by a line of subsequent work (Ott et al., 2012; Feng et al., 2012; Feng and Hirst, 2013). For example, Feng et"
C16-1014,D14-1055,1,0.584684,"es, which can be sparse and fail to effectively encode the semantic information from the overall discourse. In this paper, we propose to learn document-level neural representation for better detecting deceptive opinion spam. To our knowledge, we are the first to investigate deep learning for deceptive opinion spam detection. There has been work that exploits features outside the review content itself. In addition to Jindal and Liu (2008), Mukherjee et al. (2013) explored the features from customer’s behavior to identify deception. Based on some truthful reviews and a lot of unlabeled reviews, Ren et al. (2014) proposed a semi-supervised learning method, and built an accurate classifier to identify deceptive reviews. Kim et al. (2015) introduced a frame-based semantic feature based on FrameNet. Experimental results show that semantic frame features can improve the classification accuracy. We focus on the review content in this paper, but their features can be used to extend our model. 2.2 Neural Network Models for Representation Learning Neural network models have been exploited to learn dense feature representation for a variety of NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Ren e"
C16-1014,D13-1170,0,0.0613233,"g Neural network models have been exploited to learn dense feature representation for a variety of NLP tasks (Collobert et al., 2011; Kalchbrenner et al., 2014; Ren et al., 2016b). Distributed word representations (Mikolov et al., 2013) have been used as the basic building block by most models for NLP. Numerous methods have been proposed to learn representations of phrases and larger text segments from distributed word representations. For example, Le and Mikolov (2014) introduced paragraph vector to learn document representations, extending to word embedding methods of Mikolov et al. (2013). Socher et al. (2013) introduced a family of recursive neural networks to represent sentence-level semantic composition. Follow-up research includes recursive neural network with global feed backward mechanisms (Paulus et al., 2014), deep recursive layers (Irsoy and Cardie, 2014), and adaptive composition functions (Dong et al., 2014). Convolutional neural networks have been widely used for semantic composition (Kalchbrenner et al., 2014; Johnson and Zhang, 2014), automatically capturing n-gram information. Sequential models such as recurrent neural network or long short-term memory (LSTM) (Li et al., 2015a; Tang"
C16-1014,D15-1167,0,0.421189,"ypical features represent linguistic and psychological cues, but fail to effectively represent a document from the viewpoint of global discourse structures. For example, Ott et al. (2011) and Li et al. (2014) represent documents with Unigram, POS and LIWC (Linguistic Inquiry and Word Count) (Newman et al., 2003) features. Although such features give the strong performance, their sparsity makes it difficult to capture non-local semantic information over a sentence or discourse. Recently, neural network models have been used to learn semantic representations for NLP tasks (Le and Mikolov, 2014; Tang et al., 2015), achieving highly competitive results. Potential advantages of using neural networks for spam detection are three-fold. First, neural models use dense hidden layers for automatic feature combinations, which can capture complex global semantic information that is difficult to express using traditional discrete manual features. This can be useful in addressing the limitation of discrete models mentioned above. Second, neural networks take distributed word embeddings as inputs, which can be trained from a large-scale raw text, thus alleviating the sparsity of annotated data to some This work is"
C16-1014,P15-2116,0,0.0372922,"Missing"
C16-1153,P13-2037,0,0.0144394,"or-based document representation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open domains. Most of the previous studies focused on monolingual text, while our proposed bilingual attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from monolingual and bilingual contexts with attention mechanisms. 2.2 Research on Code-switching and Bilingual Text Code-switched documents have received considerable attention in the NLP community (Adel et al., 2013; Garrette et al., 2015). Several studies have focused on code-switching identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting code-switched points (Solorio and Liu, 2008), identifying code-switched tokens (Lignos and Marcus, 2013), adding code-switched support to language models (Li and Fung, 2012), and POS tagging for code-switching text (Jamatia et al., 2015). There is relatively little work focus on predicting emotion in code-switching text. Wang et al. (2015) proposed a machine translation based approach to predict emotion in"
C16-1153,D08-1092,0,0.024572,"g monolingual and bilingual information in both lexical and document level with neural network model and attention mechanism, while previous research only focused on lexicallevel bilingual information. In addition, we do not use any external resource, such as bilingual and sentiment dictionary, to train our model. More remotely connected, multilingual natural language processing has attracted increasing attention in the computational linguistic community due to its broad real-world applications. Relevant studies have been reported in various natural language processing tasks, such as parsing (Burkett and Klein, 2008), information retrieval (Gao et al., 2009), text classification (Amini et al., 2010), and so on. There are a number of studies on predicting sentiment polarity through multilingual text. Wan (2009) incorporated unlabeled data in the target language into classifier with co-training to improve classification performance. Wei and Pal (2010) regarded cross-lingual sentiment classification as a domain adaptation task and applied structural correspondence learning (SCL) to tackle this problem. Their approach achieves a better performance than the co-training algorithm. More recently, Meng et al. (20"
C16-1153,C14-1008,0,0.00907215,"-specific lexicon. For emotion classification, Liu et al. (2013) used a co-training framework to infer the news from readers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and emotion analysis (Vo and Zhang, 2015; Zhang et al., 2015). dos Santos and Gatti (2014) proposed a character-based deep convolutional neural network to predict sentiment of short text. Tang et al. (2015) proposed a neural network model to learn vector-based document representation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open domains. Most of the previous studies focused on monolingual text, while our proposed bilingual attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from monolingual and bilingual context"
C16-1153,P09-1121,0,0.0223953,"exical and document level with neural network model and attention mechanism, while previous research only focused on lexicallevel bilingual information. In addition, we do not use any external resource, such as bilingual and sentiment dictionary, to train our model. More remotely connected, multilingual natural language processing has attracted increasing attention in the computational linguistic community due to its broad real-world applications. Relevant studies have been reported in various natural language processing tasks, such as parsing (Burkett and Klein, 2008), information retrieval (Gao et al., 2009), text classification (Amini et al., 2010), and so on. There are a number of studies on predicting sentiment polarity through multilingual text. Wan (2009) incorporated unlabeled data in the target language into classifier with co-training to improve classification performance. Wei and Pal (2010) regarded cross-lingual sentiment classification as a domain adaptation task and applied structural correspondence learning (SCL) to tackle this problem. Their approach achieves a better performance than the co-training algorithm. More recently, Meng et al. (2012) employed the parallel corpus for cross"
C16-1153,N15-1109,0,0.0140355,"epresentation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open domains. Most of the previous studies focused on monolingual text, while our proposed bilingual attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from monolingual and bilingual contexts with attention mechanisms. 2.2 Research on Code-switching and Bilingual Text Code-switched documents have received considerable attention in the NLP community (Adel et al., 2013; Garrette et al., 2015). Several studies have focused on code-switching identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting code-switched points (Solorio and Liu, 2008), identifying code-switched tokens (Lignos and Marcus, 2013), adding code-switched support to language models (Li and Fung, 2012), and POS tagging for code-switching text (Jamatia et al., 2015). There is relatively little work focus on predicting emotion in code-switching text. Wang et al. (2015) proposed a machine translation based approach to predict emotion in codeswitching text with"
C16-1153,R15-1033,0,0.0212779,"Missing"
C16-1153,lee-etal-2014-annotating,1,0.895524,"Missing"
C16-1153,C12-1102,0,0.00831582,"tecture to capture informative words from monolingual and bilingual contexts with attention mechanisms. 2.2 Research on Code-switching and Bilingual Text Code-switched documents have received considerable attention in the NLP community (Adel et al., 2013; Garrette et al., 2015). Several studies have focused on code-switching identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting code-switched points (Solorio and Liu, 2008), identifying code-switched tokens (Lignos and Marcus, 2013), adding code-switched support to language models (Li and Fung, 2012), and POS tagging for code-switching text (Jamatia et al., 2015). There is relatively little work focus on predicting emotion in code-switching text. Wang et al. (2015) proposed a machine translation based approach to predict emotion in codeswitching text with various external resources. Our approach departs from the previous work that we model the task by considering monolingual and bilingual information in both lexical and document level with neural network model and attention mechanism, while previous research only focused on lexicallevel bilingual information. In addition, we do not use an"
C16-1153,P15-1101,1,0.208181,"focused on analyzing emotions in monolingual text. Some of these studies emotion lexicon building, for example, Rao et al. (2012) automatically built a word-emotion mapping dictionary for social emotion detection, Yang et al. (2014) proposed a novel emotion-aware topic model to build a domain-specific lexicon. For emotion classification, Liu et al. (2013) used a co-training framework to infer the news from readers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and emotion analysis (Vo and Zhang, 2015; Zhang et al., 2015). dos Santos and Gatti (2014) proposed a character-based deep convolutional neural network to predict sentiment of short text. Tang et al. (2015) proposed a neural network model to learn vector-based document representation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open dom"
C16-1153,P13-1018,0,0.0560854,"proposed model. Visualization of the attention layers illustrates that the model selects informative words qualitatively. 1 Introduction Microblogs such as Twitter and Facebook have gained tremendous popularity in the past decade, they often contain extremely current, even breaking, information about world events. However, the writing style of microblogs tends to be quite colloquial and nonstandard, unlike the style found in more traditional, edited genres (Li et al., 2015; Vo and Zhang, 2015). In addition, authors from multi-lingual communities tend to write code-switching posts frequently (Ling et al., 2013; Wang et al., 2015). These pose challenges for automatic emotion prediction tasks. There has been some previous research focusing on both emotion analysis (Pang et al., 2002; Lee et al., 2014) and code-switching text analysis (Solorio and Liu, 2008; Ling et al., 2013; Jamatia et al., 2015). However, little research has focused on predicting emotion in code-switching text. Different from monolingual emotion prediction, the emotion in code-switching posts can be expressed in either monolingual or bilingual forms. In this study, we focus on Chinese and English mixed code-switching text from Chin"
C16-1153,P13-2091,1,0.280309,"ows the effectiveness of our proposed BAN model with both monolingual and bilingual information. 2 2.1 Related Works Emotion Analysis Over the last decade, there has been much work exploring various aspects of emotion analysis (Wiebe et al., 2005). While most focused on analyzing emotions in monolingual text. Some of these studies emotion lexicon building, for example, Rao et al. (2012) automatically built a word-emotion mapping dictionary for social emotion detection, Yang et al. (2014) proposed a novel emotion-aware topic model to build a domain-specific lexicon. For emotion classification, Liu et al. (2013) used a co-training framework to infer the news from readers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and emotion analysis (Vo and Zhang, 2015; Zhang et al., 2015). dos Santos and Gatti (2014) proposed a character-based deep convolut"
C16-1153,P12-1060,0,0.0429676,"nd Klein, 2008), information retrieval (Gao et al., 2009), text classification (Amini et al., 2010), and so on. There are a number of studies on predicting sentiment polarity through multilingual text. Wan (2009) incorporated unlabeled data in the target language into classifier with co-training to improve classification performance. Wei and Pal (2010) regarded cross-lingual sentiment classification as a domain adaptation task and applied structural correspondence learning (SCL) to tackle this problem. Their approach achieves a better performance than the co-training algorithm. More recently, Meng et al. (2012) employed the parallel corpus for cross-lingual sentiment classification. They explored the case when no labeled data is available in the parallel corpus. However, such multi-lingual models do not explicitly consider code-switching, since their data sets are always parallel corpus. As the two languages are mixed in the code-switching text without parallel, code-switching corpus is more difficult to process. 3 Bilingual Attention Network Given a post X with T words (X =< w1 , w2 , ..., wT >), where each word wt is represented with a Kdimensional embedding (Mikolov et al., 2013), our goal is to"
C16-1153,W02-1011,0,0.0389416,"Missing"
C16-1153,D08-1102,0,0.0135616,"attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from monolingual and bilingual contexts with attention mechanisms. 2.2 Research on Code-switching and Bilingual Text Code-switched documents have received considerable attention in the NLP community (Adel et al., 2013; Garrette et al., 2015). Several studies have focused on code-switching identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting code-switched points (Solorio and Liu, 2008), identifying code-switched tokens (Lignos and Marcus, 2013), adding code-switched support to language models (Li and Fung, 2012), and POS tagging for code-switching text (Jamatia et al., 2015). There is relatively little work focus on predicting emotion in code-switching text. Wang et al. (2015) proposed a machine translation based approach to predict emotion in codeswitching text with various external resources. Our approach departs from the previous work that we model the task by considering monolingual and bilingual information in both lexical and document level with neural network model a"
C16-1153,D15-1167,0,0.00344898,"ers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and emotion analysis (Vo and Zhang, 2015; Zhang et al., 2015). dos Santos and Gatti (2014) proposed a character-based deep convolutional neural network to predict sentiment of short text. Tang et al. (2015) proposed a neural network model to learn vector-based document representation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open domains. Most of the previous studies focused on monolingual text, while our proposed bilingual attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from monolingual and bilingual contexts with attention mechanisms. 2.2 Research on Code-switching and Bilingual Text Code-switched documents have received"
C16-1153,P09-1027,0,0.0105089,"on, we do not use any external resource, such as bilingual and sentiment dictionary, to train our model. More remotely connected, multilingual natural language processing has attracted increasing attention in the computational linguistic community due to its broad real-world applications. Relevant studies have been reported in various natural language processing tasks, such as parsing (Burkett and Klein, 2008), information retrieval (Gao et al., 2009), text classification (Amini et al., 2010), and so on. There are a number of studies on predicting sentiment polarity through multilingual text. Wan (2009) incorporated unlabeled data in the target language into classifier with co-training to improve classification performance. Wei and Pal (2010) regarded cross-lingual sentiment classification as a domain adaptation task and applied structural correspondence learning (SCL) to tackle this problem. Their approach achieves a better performance than the co-training algorithm. More recently, Meng et al. (2012) employed the parallel corpus for cross-lingual sentiment classification. They explored the case when no labeled data is available in the parallel corpus. However, such multi-lingual models do n"
C16-1153,P15-2125,1,0.914845,"documents have received considerable attention in the NLP community (Adel et al., 2013; Garrette et al., 2015). Several studies have focused on code-switching identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting code-switched points (Solorio and Liu, 2008), identifying code-switched tokens (Lignos and Marcus, 2013), adding code-switched support to language models (Li and Fung, 2012), and POS tagging for code-switching text (Jamatia et al., 2015). There is relatively little work focus on predicting emotion in code-switching text. Wang et al. (2015) proposed a machine translation based approach to predict emotion in codeswitching text with various external resources. Our approach departs from the previous work that we model the task by considering monolingual and bilingual information in both lexical and document level with neural network model and attention mechanism, while previous research only focused on lexicallevel bilingual information. In addition, we do not use any external resource, such as bilingual and sentiment dictionary, to train our model. More remotely connected, multilingual natural language processing has attracted inc"
C16-1153,P10-2048,0,0.013418,"ilingual natural language processing has attracted increasing attention in the computational linguistic community due to its broad real-world applications. Relevant studies have been reported in various natural language processing tasks, such as parsing (Burkett and Klein, 2008), information retrieval (Gao et al., 2009), text classification (Amini et al., 2010), and so on. There are a number of studies on predicting sentiment polarity through multilingual text. Wan (2009) incorporated unlabeled data in the target language into classifier with co-training to improve classification performance. Wei and Pal (2010) regarded cross-lingual sentiment classification as a domain adaptation task and applied structural correspondence learning (SCL) to tackle this problem. Their approach achieves a better performance than the co-training algorithm. More recently, Meng et al. (2012) employed the parallel corpus for cross-lingual sentiment classification. They explored the case when no labeled data is available in the parallel corpus. However, such multi-lingual models do not explicitly consider code-switching, since their data sets are always parallel corpus. As the two languages are mixed in the code-switching"
C16-1153,P14-2069,0,0.0236212,"hen a sequence of tokens is relevant rather than simply filtering for sequences of tokens, taken out of context. Evaluation shows the effectiveness of our proposed BAN model with both monolingual and bilingual information. 2 2.1 Related Works Emotion Analysis Over the last decade, there has been much work exploring various aspects of emotion analysis (Wiebe et al., 2005). While most focused on analyzing emotions in monolingual text. Some of these studies emotion lexicon building, for example, Rao et al. (2012) automatically built a word-emotion mapping dictionary for social emotion detection, Yang et al. (2014) proposed a novel emotion-aware topic model to build a domain-specific lexicon. For emotion classification, Liu et al. (2013) used a co-training framework to infer the news from readers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and em"
C16-1153,N16-1174,0,0.0377432,"iations to the LSTM model, and we choose one for which the hidden state ht for each time-step t is given by: it = σ(W (i) xt + U (i) hi−1 + b(i) ) (1) ft = σ(W (f ) xt + U (f ) ht−1 + b(f ) ) (2) ot = σ(W (o) xt + U (o) ht−1 + b(o) ) (3) ut = tanh(W (u) + U (u) ht−1 + b(u) ) (4) ct = it ut + ft ct−1 (5) ht = ot tanh(ct ) (6) where σ denotes the sigmoid function. After the LSTM process, we obtain an annotation ht for a given word wt . 3.2 Attention Mechanism Not all words contribute equally to the representation of the meaning. Hence, we introduce an attention mechanism (Bahdanau et al., 2014; Yang et al., 2016) to extract the words that are important to the meaning of the post, and aggregate the representation of those informative words to form a vector. Since emotion can be expressed in either one or two languages in code-switching text, we build the vectors from monolingual and bilingual contexts respectively. For the monolingual case, we build two vectors v (cn) and v (en) to capture informative information from the Chinese and English contexts separately. For the bilingual case, we construct a vector v (bi) to capture the salient words from the mixed text. Bilingual Attention. We use an attentio"
C16-1153,D15-1073,1,0.85283,"c model to build a domain-specific lexicon. For emotion classification, Liu et al. (2013) used a co-training framework to infer the news from readers’ comments and writers’ emotions collectively. Wen and Wan (2014) used class sequential rules for emotion classification of microblog texts by regarding each post as a data sequence. Li et al. (2015) proposed a factor graph based framework to incorporate both label and context dependency for emotion classification. Deep neural networks have been proved effectiveness for many NLP tasks, including sentiment and emotion analysis (Vo and Zhang, 2015; Zhang et al., 2015). dos Santos and Gatti (2014) proposed a character-based deep convolutional neural network to predict sentiment of short text. Tang et al. (2015) proposed a neural network model to learn vector-based document representation. Zhang et al. (2015) 1625 employed a neural network based CRFs for extracting opinion targets on open domains. Most of the previous studies focused on monolingual text, while our proposed bilingual attention network model focuses on exploring the monolingual and bilingual information collectively, and we also propose a new architecture to capture informative words from mono"
C16-1201,D13-1160,0,0.0183853,"btaining better event embeddings and making more accurate prediction on stock market volatilities. 1 Introduction Text mining techniques have been used to perform event-driven stock prediction (Ding et al., 2015). The main idea is to learn distributed representations of structured events (i.e. event embeddings) from text, and use them as the basis to generate textual features for predicting price movements in stock markets. Here the definition of events follows the open information extraction literature (Fader et al., 2011; Yates et al., 2007), which has seen applications in semantic parsing (Berant et al., 2013), information retrieval (Sun et al., 2015) and text mining (Ding et al., 2014). Formally, an event is defined as a tuple (A, P, O), where A represents the agent, P represents the predicate and O represents the object. For example, “Microsoft profit rises 11 percent” can be represented as the event tuple (A =“Microsoft profit”, P =“rises”, O =“11 percent”). In addition, the main advantages of event embeddings include (1) they can capture both the syntactic and the semantic information among events and (2) they can be used to alleviate the sparsity of discrete events compared with one-hot featur"
C16-1201,D13-1167,0,0.0251,"conventional features, as they can capture structured relations. However, one disadvantage of structured representations of events is that they lead to increased sparsity, which potentially limits the predictive power. Ding et al. (2015) propose to address this issue by representing structured events using event embeddings, which are dense vectors. This paper proposes to leverage ground truth from knowledge graph to enhance event embeddings. Knowledge Graph Embedding Recently, several methods have been explored to represent and encode knowledge graph (Bordes et al., 2013; Bordes et al., 2014; Chang et al., 2013; Ji et al., 2015; Lin et al., 2015) in distributed vectors. In this line of work, each entity is represented as a d-dimensional vector and each relation between two entities is modeled by using a matrix or a tensor. Most existing methods learn knowledge embeddings by minimizing a global loss function over all the entities and relations in 2134 C S2 S1 T3 A T1 PT T2 O Figure 2: Baseline event-embedding model. a knowledge graph. Entity vectors can encode global information over the knowledge graph, and hence are useful for knowledge graph completion (Socher et al., 2013). In this paper, we enco"
C16-1201,D14-1148,1,0.855022,"rket volatilities. 1 Introduction Text mining techniques have been used to perform event-driven stock prediction (Ding et al., 2015). The main idea is to learn distributed representations of structured events (i.e. event embeddings) from text, and use them as the basis to generate textual features for predicting price movements in stock markets. Here the definition of events follows the open information extraction literature (Fader et al., 2011; Yates et al., 2007), which has seen applications in semantic parsing (Berant et al., 2013), information retrieval (Sun et al., 2015) and text mining (Ding et al., 2014). Formally, an event is defined as a tuple (A, P, O), where A represents the agent, P represents the predicate and O represents the object. For example, “Microsoft profit rises 11 percent” can be represented as the event tuple (A =“Microsoft profit”, P =“rises”, O =“11 percent”). In addition, the main advantages of event embeddings include (1) they can capture both the syntactic and the semantic information among events and (2) they can be used to alleviate the sparsity of discrete events compared with one-hot feature vectors. The learning principle is that events are syntactically or semantic"
C16-1201,D11-1142,0,0.0820265,"s on event similarity and stock market prediction show that our model is more capable of obtaining better event embeddings and making more accurate prediction on stock market volatilities. 1 Introduction Text mining techniques have been used to perform event-driven stock prediction (Ding et al., 2015). The main idea is to learn distributed representations of structured events (i.e. event embeddings) from text, and use them as the basis to generate textual features for predicting price movements in stock markets. Here the definition of events follows the open information extraction literature (Fader et al., 2011; Yates et al., 2007), which has seen applications in semantic parsing (Berant et al., 2013), information retrieval (Sun et al., 2015) and text mining (Ding et al., 2014). Formally, an event is defined as a tuple (A, P, O), where A represents the agent, P represents the predicate and O represents the object. For example, “Microsoft profit rises 11 percent” can be represented as the event tuple (A =“Microsoft profit”, P =“rises”, O =“11 percent”). In addition, the main advantages of event embeddings include (1) they can capture both the syntactic and the semantic information among events and (2"
C16-1201,P15-1067,0,0.0163423,"s, as they can capture structured relations. However, one disadvantage of structured representations of events is that they lead to increased sparsity, which potentially limits the predictive power. Ding et al. (2015) propose to address this issue by representing structured events using event embeddings, which are dense vectors. This paper proposes to leverage ground truth from knowledge graph to enhance event embeddings. Knowledge Graph Embedding Recently, several methods have been explored to represent and encode knowledge graph (Bordes et al., 2013; Bordes et al., 2014; Chang et al., 2013; Ji et al., 2015; Lin et al., 2015) in distributed vectors. In this line of work, each entity is represented as a d-dimensional vector and each relation between two entities is modeled by using a matrix or a tensor. Most existing methods learn knowledge embeddings by minimizing a global loss function over all the entities and relations in 2134 C S2 S1 T3 A T1 PT T2 O Figure 2: Baseline event-embedding model. a knowledge graph. Entity vectors can encode global information over the knowledge graph, and hence are useful for knowledge graph completion (Socher et al., 2013). In this paper, we encode entity vectors"
C16-1201,P15-1045,1,0.793893,"ore accurate prediction on stock market volatilities. 1 Introduction Text mining techniques have been used to perform event-driven stock prediction (Ding et al., 2015). The main idea is to learn distributed representations of structured events (i.e. event embeddings) from text, and use them as the basis to generate textual features for predicting price movements in stock markets. Here the definition of events follows the open information extraction literature (Fader et al., 2011; Yates et al., 2007), which has seen applications in semantic parsing (Berant et al., 2013), information retrieval (Sun et al., 2015) and text mining (Ding et al., 2014). Formally, an event is defined as a tuple (A, P, O), where A represents the agent, P represents the predicate and O represents the object. For example, “Microsoft profit rises 11 percent” can be represented as the event tuple (A =“Microsoft profit”, P =“rises”, O =“11 percent”). In addition, the main advantages of event embeddings include (1) they can capture both the syntactic and the semantic information among events and (2) they can be used to alleviate the sparsity of discrete events compared with one-hot feature vectors. The learning principle is that"
C16-1201,P13-1086,0,0.245908,"ddings, and we define a corresponding loss function to incorporate knowledge graph information, by following recent work on multi-relation models (Socher et al., 2013). Large-scale experiments on a YAGO corpus show that incorporating knowledge graph brings promising improvements to event embeddings. With better embeddings, we achieve better performance on stock prediction compared to the state-of-the-art methods. 2 Related Work Stock Market Prediction There has been a line of work predicting stock markets using text information from daily news (Lavrenko et al., 2000; Schumaker and Chen, 2009; Xie et al., 2013; Peng and Jiang, 2015; Li et al., 2016). Pioneering work extracts different types of textual features from news documents, such as bags-of-words, noun phrases, named entities and structured events. Ding et al. (2014) show that structured events from open information extraction (Yates et al., 2007; Fader et al., 2011) can achieve better performance compared to conventional features, as they can capture structured relations. However, one disadvantage of structured representations of events is that they lead to increased sparsity, which potentially limits the predictive power. Ding et al. (2015)"
C16-1201,D14-1071,0,0.0215393,"ddings, so that information of knowledge graphs can be used for event-driven text mining and other tasks. Socher et al. (2013) has shown that previous work (Bordes et al., 2011; Jenatton et al., 2012; Bordes et al., 2012; Sutskever et al., 2009; Collobert and Weston, 2008) are special cases of their model, which is based on a neural tensor network. We follow Socher et al. (2013) and use tensors to represent relations in knowledge graph embeddings. Our work is also related to prior research on joint embedding of words and knowledge graphs (Xu et al., 2014; Wang et al., 2014; Tian et al., 2016; Yang et al., 2014). Such work focuses on injecting semantic knowledge into distributed word representations, thus enhancing their information content. The resulting embeddings of words and phrases have been shown useful for improving NLP tasks, such as question answering and topic prediction. In comparison, our work integrates knowledge into vector representations of events, which was shown more useful than words for certain text mining tasks. 3 Knowledge-Driven Event Representations We begin by introducing the baseline event embedding learning model, which serves as the basis of proposed framework. Then, we sh"
C16-1201,N07-4013,0,0.0251176,"y and stock market prediction show that our model is more capable of obtaining better event embeddings and making more accurate prediction on stock market volatilities. 1 Introduction Text mining techniques have been used to perform event-driven stock prediction (Ding et al., 2015). The main idea is to learn distributed representations of structured events (i.e. event embeddings) from text, and use them as the basis to generate textual features for predicting price movements in stock markets. Here the definition of events follows the open information extraction literature (Fader et al., 2011; Yates et al., 2007), which has seen applications in semantic parsing (Berant et al., 2013), information retrieval (Sun et al., 2015) and text mining (Ding et al., 2014). Formally, an event is defined as a tuple (A, P, O), where A represents the agent, P represents the predicate and O represents the object. For example, “Microsoft profit rises 11 percent” can be represented as the event tuple (A =“Microsoft profit”, P =“rises”, O =“11 percent”). In addition, the main advantages of event embeddings include (1) they can capture both the syntactic and the semantic information among events and (2) they can be used to"
C16-1201,J11-1005,1,0.214117,"ing the quality of learned event embeddings on two tasks: event similarity and stock market prediction. 4.1 Experimental Settings We use publicly available financial news from Reuters and Bloomberg over the period from October 2006 to November 2013, released by Ding et al. (2014). There are 106,521 documents in total from Reuters News, from which we extracted 83,468 structured events. From Bloomberg News, there are 447,145 documents, from which 282,794 structured events are extracted. The structured events are extracted from news text using Open IE (Fader et al., 2011) and dependency parsing (Zhang and Clark, 2011), by strictly following the method of Ding et al. (2015). The timestamps of the news are also extracted, for alignment with stock price information. We conduct stock market prediction experiments on predicting the Standard & Poor’s 500 stock (S&P 500) index and its individual stocks, obtaining indices and prices from Yahoo Finance. Detail statistics of the training, development (tuning) and test sets are shown in Table 1. For training knowledge-driven event embeddings, we use YAGO as the knowledge graph. The full knowledge graph consists of 10 million entities and 120 million facts, in more th"
C16-1231,K16-1017,0,0.588274,"of tweet content features, author features, audience features and environment features, finding that contextual features are very useful for tweet sarcasm detection. So far, most existing sarcasm detection methods in the literature leverage discrete models. While on the other hand, neural network models have gained much attention for related tasks such as sentiment analysis and opinion extraction, achieving the best results (Socher et al., 2013; dos Santos and Gatti, 2014; Vo and Zhang, 2015; Zhang et al., 2016). Success on these tasks shows potentials of neural network on sarcasm detection (Amir et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016b). There are two main advantages of using neural models. First, neural layers are used to induce features automatically, This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2449 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2449–2460, Osaka, Japan, December 11-17 2016. making manual feature engineering unnecessary. Such neural features can capture long-range and subtle semantic patterns,"
C16-1231,D14-1082,0,0.0158272,"80 100 120 140 160 180 200 Figure 4: Developmental results with respect to different number of history tweet words. 6.3.1 Initialization of Word Embeddings We use the neural model with only local features to evaluate the effect of different word embedding initialization methods. As shown in Figure 2(a), a better accuracy is obtained by using GloVe embeddings for initialization compared with random initialization. The finding is consistent with previous results in the literature on other NLP tasks, which show that pre-trained word embeddings can bring better accuracies (Collobert et al., 2011; Chen and Manning, 2014). 6.3.2 Differentiating Local and Contextual Word Embeddings We can obtain embeddings of contextual tweet words using the same looking-up function as target tweet words, thereby giving each word a unique embedding regardless whether it comes from the target tweet to classify or its history tweets. However, the behavior of contextual tweet words should intuitively be different, because they are used as different features. An interesting research question is that whether separate embeddings lead to improved results. We investigate the question by using two embedding look-up matrices E and E 0 ,"
C16-1231,W14-4012,0,0.0217807,"Missing"
C16-1231,D14-1179,0,0.0042832,"Missing"
C16-1231,J81-4005,0,0.733725,"Missing"
C16-1231,W10-2914,0,0.549973,"89; Utsumi, 2000; Gibbs and Colston, 2007). Detecting sarcasm automatically is useful for opinion mining and reputation management, and hence has received growing interest from the natural language processing community (Joshi et al., 2016a). Social media such as Twitter exhibit rich sarcasm phenomena, and recent work on automatic sarcasm detection has focused on tweet data. Tweet sarcasm detection can be modeled as a binary document classification task. Two main sources of features have been used. First, most previous work extracts rich discrete features according to the tweet content itself (Davidov et al., 2010; Tsur et al., 2010; Gonz´alez-Ib´anez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Riloff et al., 2013; Pt´acˇ ek et al., 2014), including lexical unigrams, bigrams, tweet sentiment, word sentiment, punctuation marks, emoticons, quotes, character ngrams and pronunciations. Some of these work uses more sophisticated features, including POS tags, dependency-based tree structures, Brown clusters and sentiment indicators, which depend on external resources. Overall, ngrams have been among the most useful features. Second, recent work has exploited contextual tweet features for sarcasm de"
C16-1231,C14-1008,0,0.0787993,"ng a set of statistical indicators extracted from both the target tweet and relevant history tweets. Bamman and Smith (2015) study the influences of tweet content features, author features, audience features and environment features, finding that contextual features are very useful for tweet sarcasm detection. So far, most existing sarcasm detection methods in the literature leverage discrete models. While on the other hand, neural network models have gained much attention for related tasks such as sentiment analysis and opinion extraction, achieving the best results (Socher et al., 2013; dos Santos and Gatti, 2014; Vo and Zhang, 2015; Zhang et al., 2016). Success on these tasks shows potentials of neural network on sarcasm detection (Amir et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016b). There are two main advantages of using neural models. First, neural layers are used to induce features automatically, This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2449 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2449–2460, Osaka, Japan, D"
C16-1231,filatova-2012-irony,0,0.0138126,"al., 2015; Bamman and Smith, 2015). We consider both traditional lexical features and the contextual features from history tweets under a unified neural network framework. Our observation is consistent with prior work: both sources of features are highly effective for sarcasm detection (Rajadesingan et al., 2015; Bamman and Smith, 2015).. To our knowledge, we are among the first to investigate the effect of neural networks on this task (Amir et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016b). Corpora. With respect to sarcasm corpora, early work relied on small-scale manual annotation. Filatova (2012) constructed a sarcasm corpus from Amazon product reviews using crowdsourcing. Davidov et al. (2010) discussed the strong influence of hashtags on sarcasm detection. Inspired by this, Gonz´alezIb´anez et al. (2011) used sarcasm-related hashtags as gold labels for sarcasm, creating a tweet corpus by treating tweets without such hashtags as negative examples. Their work is similar in spirit to the work of Go et al. (2009), who constructed a tweet sentiment automatically by taking emoticons as gold sentiment labels. The method of Gonz´alez-Ib´anez et al. (2011) was adopted by Pt´acˇ ek et al. (20"
C16-1231,W16-0425,0,0.628753,"eatures, author features, audience features and environment features, finding that contextual features are very useful for tweet sarcasm detection. So far, most existing sarcasm detection methods in the literature leverage discrete models. While on the other hand, neural network models have gained much attention for related tasks such as sentiment analysis and opinion extraction, achieving the best results (Socher et al., 2013; dos Santos and Gatti, 2014; Vo and Zhang, 2015; Zhang et al., 2016). Success on these tasks shows potentials of neural network on sarcasm detection (Amir et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016b). There are two main advantages of using neural models. First, neural layers are used to induce features automatically, This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2449 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2449–2460, Osaka, Japan, December 11-17 2016. making manual feature engineering unnecessary. Such neural features can capture long-range and subtle semantic patterns, which are difficult to"
C16-1231,P11-2102,0,0.133285,"Missing"
C16-1231,P15-2124,0,0.0614254,"ve been exploited to enhance sarcasm detection. Tsur et al. (2010) applied features based on semi-supervised syntactic patterns extracted from sarcastic sentences of Amazon product reviews. Davidov et al. (2010) further extracted these features from sarcastic tweets. Riloff et al. (2013) identified a main type of sarcasm, namely contrast between a positive and negative sentiment, which can be regarded as detecting sarcasm using sentiment information. There has been work that comprehensively studies the effect of various features (Gonz´alez-Ib´anez et al., 2011; Gonz´alez-Ib´anez et al., 2011; Joshi et al., 2015). Recently, contextual information has been exploited for sarcasm detection (Wallace et al., 2015; Karoui et al., 2015). In particular, contextual features extracted from history tweets by the same author has shown great effectiveness for tweet sarcasm detection (Rajadesingan et al., 2015; Bamman and Smith, 2015). We consider both traditional lexical features and the contextual features from history tweets under a unified neural network framework. Our observation is consistent with prior work: both sources of features are highly effective for sarcasm detection (Rajadesingan et al., 2015; Bamma"
C16-1231,D16-1104,0,0.225986,"l network to extract contextual features automatically from history tweets. Results show that neural features give improved accuracies for sarcasm detection, with different error distributions compared with discrete manual features. 1 Introduction Sarcasm has received much research attention in linguistics, psychology and cognitive science (Gibbs, 1986; Kreuz and Glucksberg, 1989; Utsumi, 2000; Gibbs and Colston, 2007). Detecting sarcasm automatically is useful for opinion mining and reputation management, and hence has received growing interest from the natural language processing community (Joshi et al., 2016a). Social media such as Twitter exhibit rich sarcasm phenomena, and recent work on automatic sarcasm detection has focused on tweet data. Tweet sarcasm detection can be modeled as a binary document classification task. Two main sources of features have been used. First, most previous work extracts rich discrete features according to the tweet content itself (Davidov et al., 2010; Tsur et al., 2010; Gonz´alez-Ib´anez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Riloff et al., 2013; Pt´acˇ ek et al., 2014), including lexical unigrams, bigrams, tweet sentiment, word sentiment, punctuati"
C16-1231,P14-1062,0,0.0171835,"dataset for Czech. More recently, both Rajadesingan et al. (2015) and Bamman and Smith (2015) followed the method for building a sarcasm corpus. We take the corpus of Rajadesingan et al. (2015) for our experiments. Neural network models. Although only very limited work has been done on using neural networks for sarcasm detection, neural models have seen increasing applications in sentiment analysis, which is a closely-related task. Different neural network architectures have been applied for sentiment analysis, including recursive auto-encoders (Socher et al., 2013), dynamic pooling networks (Kalchbrenner et al., 2014), deep belief networks (Zhou et al., 2014), deep convolutional networks (dos Santos and Gatti, 2014; Tang et al., 2015) and neural CRF (Zhang et al., 2015). This line of work gives highly competitive results, demonstrating large potentials for neural networks on sentiment analysis. One important reason is the power of neural networks in automatic feature induction, which can potentially discover subtle semantic patterns that are difficult to capture by using manual features. Sarcasm detection can benefit from such induction, and several work has already attempted for it (Amir et al., 2016; Gho"
C16-1231,P15-2106,0,0.0204433,"patterns extracted from sarcastic sentences of Amazon product reviews. Davidov et al. (2010) further extracted these features from sarcastic tweets. Riloff et al. (2013) identified a main type of sarcasm, namely contrast between a positive and negative sentiment, which can be regarded as detecting sarcasm using sentiment information. There has been work that comprehensively studies the effect of various features (Gonz´alez-Ib´anez et al., 2011; Gonz´alez-Ib´anez et al., 2011; Joshi et al., 2015). Recently, contextual information has been exploited for sarcasm detection (Wallace et al., 2015; Karoui et al., 2015). In particular, contextual features extracted from history tweets by the same author has shown great effectiveness for tweet sarcasm detection (Rajadesingan et al., 2015; Bamman and Smith, 2015). We consider both traditional lexical features and the contextual features from history tweets under a unified neural network framework. Our observation is consistent with prior work: both sources of features are highly effective for sarcasm detection (Rajadesingan et al., 2015; Bamman and Smith, 2015).. To our knowledge, we are among the first to investigate the effect of neural networks on this task"
C16-1231,W07-0101,0,0.189299,"ly better accuracies compared to the discrete baseline, demonstrating the advantage of the automatically extracted neural features in capturing global semantic information. Further analysis shows that features from history tweets are as useful to the neural model as to the discrete model. We make our source code publicly available under GPL at https://github.com/zhangmeishan/SarcasmDetection. 2 Related Work Features. Sarcasm detection is typically regarded as a classification problem. Discrete models have been used and most existing research efforts have focused on finding effective features. Kreuz and Caucci (2007) studied lexical features for sarcasm detection, finding that words, such as interjections and punctuation, are effective for the task. Carvalho et al. (2009) demonstrated that oral or gestural expressions represented by emoticons and special keyboard characters are useful indicators of sarcasm. Both Kreuz and Caucci (2007) and Carvalho et al. (2009) rely on unigram lexical features for sarcasm detection. More recently, Lukin and Walker (2013) extended the idea by using n-gram features as well as lexicon-syntactic patterns. External sources of information have been exploited to enhance sarcasm"
C16-1231,W13-1104,0,0.0460713,"ally regarded as a classification problem. Discrete models have been used and most existing research efforts have focused on finding effective features. Kreuz and Caucci (2007) studied lexical features for sarcasm detection, finding that words, such as interjections and punctuation, are effective for the task. Carvalho et al. (2009) demonstrated that oral or gestural expressions represented by emoticons and special keyboard characters are useful indicators of sarcasm. Both Kreuz and Caucci (2007) and Carvalho et al. (2009) rely on unigram lexical features for sarcasm detection. More recently, Lukin and Walker (2013) extended the idea by using n-gram features as well as lexicon-syntactic patterns. External sources of information have been exploited to enhance sarcasm detection. Tsur et al. (2010) applied features based on semi-supervised syntactic patterns extracted from sarcastic sentences of Amazon product reviews. Davidov et al. (2010) further extracted these features from sarcastic tweets. Riloff et al. (2013) identified a main type of sarcasm, namely contrast between a positive and negative sentiment, which can be regarded as detecting sarcasm using sentiment information. There has been work that com"
C16-1231,D14-1162,0,0.104612,"omputed by using logistic regression over the output vector o in Eq (1) and (2) for the discrete and neural models, respectively. Online AdaGrad (Duchi et al., 2011) is used to minimize the objective function for both discrete and neural models. All the matrix and vector parameters are initialized by uniform sampling in (−0.01, 0.01). The initial values of the embedding matrix E can be assigned either by using the same random initialization as the other parameters, or by using word embeddings pre-trained over a large-scale tweet corpus. We obtain pre-trained tweet word embeddings using GloVe (Pennington et al., 2014)3 . Embeddings are fine-tuned during training, with E belonging to model parameters. 6 Experiments 6.1 Experimental Settings 6.1.1 Data We use the dataset of Rajadesingan et al. (2015) to conduct our experiments, collected by querying the Twitter API using the keywords #sarcasm and #not, and filtering retweets and non-English tweets automatically. In total, Rajadesingan et al. (2015) collected 9,104 tweets that are self-described as sarcasm by the authors. We stream the tweet corpus using the tweet IDs they provide.4 We remove the #sarcasm and #not hashtags from the tweets, assigning to them t"
C16-1231,C14-1022,0,0.409337,"has received growing interest from the natural language processing community (Joshi et al., 2016a). Social media such as Twitter exhibit rich sarcasm phenomena, and recent work on automatic sarcasm detection has focused on tweet data. Tweet sarcasm detection can be modeled as a binary document classification task. Two main sources of features have been used. First, most previous work extracts rich discrete features according to the tweet content itself (Davidov et al., 2010; Tsur et al., 2010; Gonz´alez-Ib´anez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Riloff et al., 2013; Pt´acˇ ek et al., 2014), including lexical unigrams, bigrams, tweet sentiment, word sentiment, punctuation marks, emoticons, quotes, character ngrams and pronunciations. Some of these work uses more sophisticated features, including POS tags, dependency-based tree structures, Brown clusters and sentiment indicators, which depend on external resources. Overall, ngrams have been among the most useful features. Second, recent work has exploited contextual tweet features for sarcasm detection (Rajadesingan et al., 2015; Bamman and Smith, 2015). Intuitively, the history behaviors for a tweet author can be a good indicato"
C16-1231,D13-1066,0,0.45437,"Missing"
C16-1231,D13-1170,0,0.0286654,"ach to model sarcasm, using a set of statistical indicators extracted from both the target tweet and relevant history tweets. Bamman and Smith (2015) study the influences of tweet content features, author features, audience features and environment features, finding that contextual features are very useful for tweet sarcasm detection. So far, most existing sarcasm detection methods in the literature leverage discrete models. While on the other hand, neural network models have gained much attention for related tasks such as sentiment analysis and opinion extraction, achieving the best results (Socher et al., 2013; dos Santos and Gatti, 2014; Vo and Zhang, 2015; Zhang et al., 2016). Success on these tasks shows potentials of neural network on sarcasm detection (Amir et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016b). There are two main advantages of using neural models. First, neural layers are used to induce features automatically, This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2449 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, page"
C16-1231,P15-1098,0,0.00514376,"a sarcasm corpus. We take the corpus of Rajadesingan et al. (2015) for our experiments. Neural network models. Although only very limited work has been done on using neural networks for sarcasm detection, neural models have seen increasing applications in sentiment analysis, which is a closely-related task. Different neural network architectures have been applied for sentiment analysis, including recursive auto-encoders (Socher et al., 2013), dynamic pooling networks (Kalchbrenner et al., 2014), deep belief networks (Zhou et al., 2014), deep convolutional networks (dos Santos and Gatti, 2014; Tang et al., 2015) and neural CRF (Zhang et al., 2015). This line of work gives highly competitive results, demonstrating large potentials for neural networks on sentiment analysis. One important reason is the power of neural networks in automatic feature induction, which can potentially discover subtle semantic patterns that are difficult to capture by using manual features. Sarcasm detection can benefit from such induction, and several work has already attempted for it (Amir et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016b). This motivates our work. 3 Baseline Discrete Model We follow previous work in"
C16-1231,P15-1100,0,0.0463935,"i-supervised syntactic patterns extracted from sarcastic sentences of Amazon product reviews. Davidov et al. (2010) further extracted these features from sarcastic tweets. Riloff et al. (2013) identified a main type of sarcasm, namely contrast between a positive and negative sentiment, which can be regarded as detecting sarcasm using sentiment information. There has been work that comprehensively studies the effect of various features (Gonz´alez-Ib´anez et al., 2011; Gonz´alez-Ib´anez et al., 2011; Joshi et al., 2015). Recently, contextual information has been exploited for sarcasm detection (Wallace et al., 2015; Karoui et al., 2015). In particular, contextual features extracted from history tweets by the same author has shown great effectiveness for tweet sarcasm detection (Rajadesingan et al., 2015; Bamman and Smith, 2015). We consider both traditional lexical features and the contextual features from history tweets under a unified neural network framework. Our observation is consistent with prior work: both sources of features are highly effective for sarcasm detection (Rajadesingan et al., 2015; Bamman and Smith, 2015).. To our knowledge, we are among the first to investigate the effect of neural"
C16-1231,D15-1073,1,0.836572,"s of Rajadesingan et al. (2015) for our experiments. Neural network models. Although only very limited work has been done on using neural networks for sarcasm detection, neural models have seen increasing applications in sentiment analysis, which is a closely-related task. Different neural network architectures have been applied for sentiment analysis, including recursive auto-encoders (Socher et al., 2013), dynamic pooling networks (Kalchbrenner et al., 2014), deep belief networks (Zhou et al., 2014), deep convolutional networks (dos Santos and Gatti, 2014; Tang et al., 2015) and neural CRF (Zhang et al., 2015). This line of work gives highly competitive results, demonstrating large potentials for neural networks on sentiment analysis. One important reason is the power of neural networks in automatic feature induction, which can potentially discover subtle semantic patterns that are difficult to capture by using manual features. Sarcasm detection can benefit from such induction, and several work has already attempted for it (Amir et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016b). This motivates our work. 3 Baseline Discrete Model We follow previous work in the literature, building a strong d"
C16-1231,C14-1127,0,0.0268479,"n et al. (2015) and Bamman and Smith (2015) followed the method for building a sarcasm corpus. We take the corpus of Rajadesingan et al. (2015) for our experiments. Neural network models. Although only very limited work has been done on using neural networks for sarcasm detection, neural models have seen increasing applications in sentiment analysis, which is a closely-related task. Different neural network architectures have been applied for sentiment analysis, including recursive auto-encoders (Socher et al., 2013), dynamic pooling networks (Kalchbrenner et al., 2014), deep belief networks (Zhou et al., 2014), deep convolutional networks (dos Santos and Gatti, 2014; Tang et al., 2015) and neural CRF (Zhang et al., 2015). This line of work gives highly competitive results, demonstrating large potentials for neural networks on sentiment analysis. One important reason is the power of neural networks in automatic feature induction, which can potentially discover subtle semantic patterns that are difficult to capture by using manual features. Sarcasm detection can benefit from such induction, and several work has already attempted for it (Amir et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016b)."
C16-1235,D13-1172,0,0.0256999,"of threshold, which is in line with intuitively understanding. For obtaining enough negative samples, we chosen 0.3 as the similarity threshold. 1.90 0.56 0.54 1.89 0.52 1.88 0.50 Purity 0.48 1.87 Entropy 0.46 1.86 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Figure 3: Influence of the similarity threshold η 4 Related Work Our work is related to aspect-level sentiment analysis, metric learning, and deep learning. For aspect-level sentiment analysis, there are many methods on clustering aspect phrases. Some topic-model-based approaches jointly extract aspect phrases and group them at the same time (Chen et al., 2013; Moghaddam and Ester, 2012; Lu et al., 2011; Jo and Oh, 2011; Zhao et al., 2010; Lin and He, 2009). Those methods tend to discover coarse-grained and grouped aspect phrases, but not specific opinionated aspect phrase themselves. In addition, Zhai et al. (2011a) showed that they did not perform well even considering pre-existing knowledge. Some other work focuses on grouping aspect phrases. Guo et al. (2009) grouped aspect phrases using multi-level LaSA, which exploits the virtual 2499 context documents and semantic structure of aspect phrase. Zhai et al. (2010) used an EM-based semisupervised"
C16-1235,J81-4005,0,0.707261,"Missing"
C16-1235,N15-1184,0,0.0185674,"to the success of our method is the finding of a proper training algorithm for the neural network model. Inspired by word embedding training methods (Collobert et al., 2011; Mikolov et al., 2013), we take a negative sampling approach. In particular, we take pairs of sentences that contain the same aspect phrase as positive training examples, and pairs of sentences that contain incompatible aspect phrase as negative training examples, maximizing a score margin between positive and negative examples. Here two aspect phrases are incompatible if the distance based on a semantic lexicon is large (Faruqui et al., 2015; Yu and Dredze, 2014). To find a better vector space representation, we add two nonlinear transformation layers, as shown in h(1) and h(2) in Figure 1. This method is similar to the Mahalanobis distance metric learning for face verification (Hu et al., 2014). Model training is performed by back-propagation over all neural nodes. With such vector space being learned, direct K-means clustering can be used to group aspect phrases. Results on a standard benchmark show that our neural network significantly outperforms traditional models. The average results on 4 domains reached 0.51 (Purity) and 1"
C16-1235,P14-1062,0,0.0146159,"ectly concatenated to form feature vectors. In this paper, we adapt this method to the aspect phrase grouping task, and provide an extra attention-based semantic composite model to obtain feature vectors based on word vectors of aspect phrase and its context. Our work is related to word embedding and deep learning. Prior research (Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Tang et al., 2014; Ren et al., 2016b) presented different models to improve the performance of word embedding training, and our training is inspried by negative sampling. Deep learning methods (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013; Vo and Zhang, 2015; Zhang et al., 2015; Zhang et al., 2016; Ren et al., 2016a) have been applied to many tasks related to sentiment analysis. In this paper, we explore attention (Luong et al., 2015; Rush et al., 2015; Ling et al., 2015) with a MLP network to tackle the aspect phrase grouping problem. 5 Conclusion We studied distance metric learning for aspect phrase grouping, exploring a novel deep neural network framework. By leveraging semantic relations between aspect phrase and their contexts, our approach give better performance to strong baselines which"
C16-1235,W06-0301,0,0.0309999,"Liu, 2004; Pang and Lee, 2008), aspect identification from the corpus is a necessary step. Here aspect is the name of a feature of the product, while an aspect phrase is a word or phrase that actually appears in a sentence to indicate the aspect. Different aspect phrases can be used to describe the same aspect. For example, “picture quality” could be referred to “photo”, “image” and “picture”. All aspect phrases in the same group indicate the same aspect. In this paper, we assume that all aspect phrases have been identified by using existing methods (Jin et al., 2009; Kobayashi et al., 2007; Kim and Hovy, 2006), and focus on grouping domain synonymous aspect phrases. Most existing work employed unsupervised methods, exploiting lexical similarity from semantic dictionary as well as context environments (Zhao et al., 2014; Zhai et al., 2011a; Guo et al., 2009). The context for an aspect phrase is formed by aggregating related sentences that mention the same aspect phrase. Thereafter, aspect phrase and context environment are represented using bag-of-word (BoW) models separately, and integrated into a unified learning framework. One limitation of the existing methods is that they do not model the inter"
C16-1235,D14-1181,0,0.00493363,"feature vectors. In this paper, we adapt this method to the aspect phrase grouping task, and provide an extra attention-based semantic composite model to obtain feature vectors based on word vectors of aspect phrase and its context. Our work is related to word embedding and deep learning. Prior research (Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Tang et al., 2014; Ren et al., 2016b) presented different models to improve the performance of word embedding training, and our training is inspried by negative sampling. Deep learning methods (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013; Vo and Zhang, 2015; Zhang et al., 2015; Zhang et al., 2016; Ren et al., 2016a) have been applied to many tasks related to sentiment analysis. In this paper, we explore attention (Luong et al., 2015; Rush et al., 2015; Ling et al., 2015) with a MLP network to tackle the aspect phrase grouping problem. 5 Conclusion We studied distance metric learning for aspect phrase grouping, exploring a novel deep neural network framework. By leveraging semantic relations between aspect phrase and their contexts, our approach give better performance to strong baselines which achieve the"
C16-1235,D07-1114,0,0.0338543,"ntiment analysis (Hu and Liu, 2004; Pang and Lee, 2008), aspect identification from the corpus is a necessary step. Here aspect is the name of a feature of the product, while an aspect phrase is a word or phrase that actually appears in a sentence to indicate the aspect. Different aspect phrases can be used to describe the same aspect. For example, “picture quality” could be referred to “photo”, “image” and “picture”. All aspect phrases in the same group indicate the same aspect. In this paper, we assume that all aspect phrases have been identified by using existing methods (Jin et al., 2009; Kobayashi et al., 2007; Kim and Hovy, 2006), and focus on grouping domain synonymous aspect phrases. Most existing work employed unsupervised methods, exploiting lexical similarity from semantic dictionary as well as context environments (Zhao et al., 2014; Zhai et al., 2011a; Guo et al., 2009). The context for an aspect phrase is formed by aggregating related sentences that mention the same aspect phrase. Thereafter, aspect phrase and context environment are represented using bag-of-word (BoW) models separately, and integrated into a unified learning framework. One limitation of the existing methods is that they d"
C16-1235,D15-1161,0,0.149789,"ight and sharp and the sound is good”, the words “clear”, “bright” and “sharp” are related to the aspect phrase “picture”, while the word “good” is related to the aspect phrase “sound”. By the traditional model, these words are not differentiated when they are taken for the context, thereby causing noise in the grouping of the two aspect phrases. To address this issue, we propose a novel neural network structure that automatically learns the relative importance of each context word with respect to a target/aspect phrase, by leveraging an attention model (Luong et al., 2015; Rush et al., 2015; Ling et al., 2015). As shown in Figure 1, given a sentence that contains an aspect phrase, we use a neural network to find a vector representation of the aspect phrase and its context. For the grouping of a certain aspect phrase, we concatenate all the occurrences of the aspect phrase in a corpus to find its vector form. Thus, the problem of aspect phrase grouping is transformed into a clustering problem in the resulting vector space. Different from traditional methods, which leverage ∗ corresponding author This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org"
C16-1235,D15-1166,0,0.198834,"in the review “the picture is clear, bright and sharp and the sound is good”, the words “clear”, “bright” and “sharp” are related to the aspect phrase “picture”, while the word “good” is related to the aspect phrase “sound”. By the traditional model, these words are not differentiated when they are taken for the context, thereby causing noise in the grouping of the two aspect phrases. To address this issue, we propose a novel neural network structure that automatically learns the relative importance of each context word with respect to a target/aspect phrase, by leveraging an attention model (Luong et al., 2015; Rush et al., 2015; Ling et al., 2015). As shown in Figure 1, given a sentence that contains an aspect phrase, we use a neural network to find a vector representation of the aspect phrase and its context. For the grouping of a certain aspect phrase, we concatenate all the occurrences of the aspect phrase in a corpus to find its vector form. Thus, the problem of aspect phrase grouping is transformed into a clustering problem in the resulting vector space. Different from traditional methods, which leverage ∗ corresponding author This work is licenced under a Creative Commons Attribution 4.0 Int"
C16-1235,N10-1047,0,0.0295388,"95 We exploit lexical similarity to obtain incompatible aspect phrases, which have low similarity in semantic lexicon. In particular, we choose WordNet as the semantic lexicon. Two aspect phrase are incompatible when the WordNet similarity between them is smaller than a threshold η. And the WordNet similarity is calculated by Equation (12) Res(w1 , w2 ) = IC(LCS(w1 , w2 )) (10) IC(w) = −logP (w) (11) Jcn(w1 , w2 ) = 1 IC(w1 ) + IC(w2 ) − 2 × Res(w1 , w2 ) (12) where LCS (lease common subsumer) is the most specific concept that is a shared ancestor of the two concepts represented by the words (Pedersen, 2010). P (w) is the probability of the concept word w. In our experiments, the threshold is set to 0.85. Traditional methods (Zhai et al., 2010; Zhai et al., 2011b) exploit lexical knowledge to provide soft constraint for clustering aspect phrases. They assume that the aspect phrases that have high similarity in semantic lexicon, are likely to belong to the same group. In this cause, our method uses a similar assumption. For obtaining the training data, we apply an extra sample pair generation process. The generated sample aspect phrase pairs are fed into left and right sub neural network of Figure"
C16-1235,D14-1162,0,0.0804157,"Missing"
C16-1235,D15-1044,0,0.220843,"icture is clear, bright and sharp and the sound is good”, the words “clear”, “bright” and “sharp” are related to the aspect phrase “picture”, while the word “good” is related to the aspect phrase “sound”. By the traditional model, these words are not differentiated when they are taken for the context, thereby causing noise in the grouping of the two aspect phrases. To address this issue, we propose a novel neural network structure that automatically learns the relative importance of each context word with respect to a target/aspect phrase, by leveraging an attention model (Luong et al., 2015; Rush et al., 2015; Ling et al., 2015). As shown in Figure 1, given a sentence that contains an aspect phrase, we use a neural network to find a vector representation of the aspect phrase and its context. For the grouping of a certain aspect phrase, we concatenate all the occurrences of the aspect phrase in a corpus to find its vector form. Thus, the problem of aspect phrase grouping is transformed into a clustering problem in the resulting vector space. Different from traditional methods, which leverage ∗ corresponding author This work is licenced under a Creative Commons Attribution 4.0 International License."
C16-1235,D13-1170,0,0.00785405,"tors. In this paper, we adapt this method to the aspect phrase grouping task, and provide an extra attention-based semantic composite model to obtain feature vectors based on word vectors of aspect phrase and its context. Our work is related to word embedding and deep learning. Prior research (Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Tang et al., 2014; Ren et al., 2016b) presented different models to improve the performance of word embedding training, and our training is inspried by negative sampling. Deep learning methods (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013; Vo and Zhang, 2015; Zhang et al., 2015; Zhang et al., 2016; Ren et al., 2016a) have been applied to many tasks related to sentiment analysis. In this paper, we explore attention (Luong et al., 2015; Rush et al., 2015; Ling et al., 2015) with a MLP network to tackle the aspect phrase grouping problem. 5 Conclusion We studied distance metric learning for aspect phrase grouping, exploring a novel deep neural network framework. By leveraging semantic relations between aspect phrase and their contexts, our approach give better performance to strong baselines which achieve the best results in stan"
C16-1235,P14-1146,0,0.0378009,"e pair. However, these methods not perform nonlinear transformation. Hu et al. (2014) employed a MLP-based nonlinear transformation, but its input is the given image descriptor, which can be directly concatenated to form feature vectors. In this paper, we adapt this method to the aspect phrase grouping task, and provide an extra attention-based semantic composite model to obtain feature vectors based on word vectors of aspect phrase and its context. Our work is related to word embedding and deep learning. Prior research (Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Tang et al., 2014; Ren et al., 2016b) presented different models to improve the performance of word embedding training, and our training is inspried by negative sampling. Deep learning methods (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013; Vo and Zhang, 2015; Zhang et al., 2015; Zhang et al., 2016; Ren et al., 2016a) have been applied to many tasks related to sentiment analysis. In this paper, we explore attention (Luong et al., 2015; Rush et al., 2015; Ling et al., 2015) with a MLP network to tackle the aspect phrase grouping problem. 5 Conclusion We studied distance metric learning for aspect ph"
C16-1235,P15-1098,0,0.0334699,"Missing"
C16-1235,W15-1509,0,0.0120389,"mputational Linguistics: Technical Papers, pages 2492–2502, Osaka, Japan, December 11-17 2016. y Distance Metric h(2) 1 h(2) 2 h(1) 1 h(1) 2 x1 x2 c~ 1 c~ 2 a1 a2 p1 p2 c1 c2 Figure 1: Architecture of the proposed method. For a given pair of aspect phrases p1 and p2 , with their contexts c1 and c2 respectively, two vectors x1 and x2 are obtained via attention-based semantic (2) (2) combination, and then mapped into the same feature subspace as h1 and h2 . a bag-of-word feature space, our vector space considers not only words, but also semantic similarities between aspect phrases and contexts (Xu et al., 2015). One challenge to the success of our method is the finding of a proper training algorithm for the neural network model. Inspired by word embedding training methods (Collobert et al., 2011; Mikolov et al., 2013), we take a negative sampling approach. In particular, we take pairs of sentences that contain the same aspect phrase as positive training examples, and pairs of sentences that contain incompatible aspect phrase as negative training examples, maximizing a score margin between positive and negative examples. Here two aspect phrases are incompatible if the distance based on a semantic lex"
C16-1235,P14-2089,0,0.0214248,"method is the finding of a proper training algorithm for the neural network model. Inspired by word embedding training methods (Collobert et al., 2011; Mikolov et al., 2013), we take a negative sampling approach. In particular, we take pairs of sentences that contain the same aspect phrase as positive training examples, and pairs of sentences that contain incompatible aspect phrase as negative training examples, maximizing a score margin between positive and negative examples. Here two aspect phrases are incompatible if the distance based on a semantic lexicon is large (Faruqui et al., 2015; Yu and Dredze, 2014). To find a better vector space representation, we add two nonlinear transformation layers, as shown in h(1) and h(2) in Figure 1. This method is similar to the Mahalanobis distance metric learning for face verification (Hu et al., 2014). Model training is performed by back-propagation over all neural nodes. With such vector space being learned, direct K-means clustering can be used to group aspect phrases. Results on a standard benchmark show that our neural network significantly outperforms traditional models. The average results on 4 domains reached 0.51 (Purity) and 1.74 (Entropy), better"
C16-1235,C10-1143,0,0.388385,"choose WordNet as the semantic lexicon. Two aspect phrase are incompatible when the WordNet similarity between them is smaller than a threshold η. And the WordNet similarity is calculated by Equation (12) Res(w1 , w2 ) = IC(LCS(w1 , w2 )) (10) IC(w) = −logP (w) (11) Jcn(w1 , w2 ) = 1 IC(w1 ) + IC(w2 ) − 2 × Res(w1 , w2 ) (12) where LCS (lease common subsumer) is the most specific concept that is a shared ancestor of the two concepts represented by the words (Pedersen, 2010). P (w) is the probability of the concept word w. In our experiments, the threshold is set to 0.85. Traditional methods (Zhai et al., 2010; Zhai et al., 2011b) exploit lexical knowledge to provide soft constraint for clustering aspect phrases. They assume that the aspect phrases that have high similarity in semantic lexicon, are likely to belong to the same group. In this cause, our method uses a similar assumption. For obtaining the training data, we apply an extra sample pair generation process. The generated sample aspect phrase pairs are fed into left and right sub neural network of Figure 1, respectively. Specifically, each training sentence is utilized with its labelled aspect phrase as a gold sample. Then, we combine each"
C16-1235,D15-1073,1,0.821773,"to the aspect phrase grouping task, and provide an extra attention-based semantic composite model to obtain feature vectors based on word vectors of aspect phrase and its context. Our work is related to word embedding and deep learning. Prior research (Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Tang et al., 2014; Ren et al., 2016b) presented different models to improve the performance of word embedding training, and our training is inspried by negative sampling. Deep learning methods (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013; Vo and Zhang, 2015; Zhang et al., 2015; Zhang et al., 2016; Ren et al., 2016a) have been applied to many tasks related to sentiment analysis. In this paper, we explore attention (Luong et al., 2015; Rush et al., 2015; Ling et al., 2015) with a MLP network to tackle the aspect phrase grouping problem. 5 Conclusion We studied distance metric learning for aspect phrase grouping, exploring a novel deep neural network framework. By leveraging semantic relations between aspect phrase and their contexts, our approach give better performance to strong baselines which achieve the best results in standard benchmark. Our method can be applie"
C16-1235,D10-1006,0,0.0420326,"ugh negative samples, we chosen 0.3 as the similarity threshold. 1.90 0.56 0.54 1.89 0.52 1.88 0.50 Purity 0.48 1.87 Entropy 0.46 1.86 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Figure 3: Influence of the similarity threshold η 4 Related Work Our work is related to aspect-level sentiment analysis, metric learning, and deep learning. For aspect-level sentiment analysis, there are many methods on clustering aspect phrases. Some topic-model-based approaches jointly extract aspect phrases and group them at the same time (Chen et al., 2013; Moghaddam and Ester, 2012; Lu et al., 2011; Jo and Oh, 2011; Zhao et al., 2010; Lin and He, 2009). Those methods tend to discover coarse-grained and grouped aspect phrases, but not specific opinionated aspect phrase themselves. In addition, Zhai et al. (2011a) showed that they did not perform well even considering pre-existing knowledge. Some other work focuses on grouping aspect phrases. Guo et al. (2009) grouped aspect phrases using multi-level LaSA, which exploits the virtual 2499 context documents and semantic structure of aspect phrase. Zhai et al. (2010) used an EM-based semisupervised learning method for clustering aspect phrases, in which the lexical knowledge i"
C16-1235,D14-1169,0,0.0762796,"sentence to indicate the aspect. Different aspect phrases can be used to describe the same aspect. For example, “picture quality” could be referred to “photo”, “image” and “picture”. All aspect phrases in the same group indicate the same aspect. In this paper, we assume that all aspect phrases have been identified by using existing methods (Jin et al., 2009; Kobayashi et al., 2007; Kim and Hovy, 2006), and focus on grouping domain synonymous aspect phrases. Most existing work employed unsupervised methods, exploiting lexical similarity from semantic dictionary as well as context environments (Zhao et al., 2014; Zhai et al., 2011a; Guo et al., 2009). The context for an aspect phrase is formed by aggregating related sentences that mention the same aspect phrase. Thereafter, aspect phrase and context environment are represented using bag-of-word (BoW) models separately, and integrated into a unified learning framework. One limitation of the existing methods is that they do not model the interaction between aspect phrases and their contexts explicitly. For example, in the review “the picture is clear, bright and sharp and the sound is good”, the words “clear”, “bright” and “sharp” are related to the as"
C16-1235,P14-1030,0,\N,Missing
C16-1303,D14-1179,0,0.0238786,"Missing"
C16-1303,P15-1033,0,0.0135975,"al., 2013; Ding et al., 2015) methods. Our work falls into the category of information retrievalbased features by exploiting syntax information derived from a dependency parser. However, different from the aforementioned work, our goal is not to predict stock prices but to measure the economic value of news information content. The proposed Tree-LSTM based model for automatically representing syntactic dependencies is in line with recent research that extends the standard sequential LSTM in order to support more complex structures, such as Grid LSTM (Kalchbrenner et al., 2015), Spatial LSTM (Dyer et al., 2015), and TreeLSTM (Le and Zuidema, 2015; Tai et al., 2015; Zhu et al., 2015; Miwa and Bansal, 2016). We consider the information content prediction as a semantic-heavy task and demonstrate that it can benefit significantly from a novel target-specific dependency Tree-LSTM model. 8 Conclusion We showed that the impact of information in news release can be predicted using a firm’s CAR, and that a proposed target-depend Tree-LSTM model, incorporating contextual information with syntax dependencies, is more effective in representing information content in news text compared to the classic bidirection"
C16-1303,N09-1031,0,0.0307115,"k for TV sales, confident that other units will perform better than earlier anticipated to offset additional losses in the unit. 0.14 Japan’s biggest technology conglomerates reported quarterly results, with weak TV demand a common theme at both Sony Corp and Sharp Corp. Table 4: Learned weights for different news. 7 Related Work Our work is related to research that applies NLP techniques on financial text to predict stock prices and market activities. In terms of corpora, financial news (Leinweber and Sisk, 2011; Xie et al., 2013; Luss and d’Aspremont, 2015; Ding et al., 2015), firm reports (Kogan et al., 2009; Li, 2010; Lee et al., 2014; Qiu et al., 2014) and web content, such as tweets (Bollen et al., 2011; Vu et al., 2012) and forum posts (Das and Chen, 2007; Gilbert and Karahalios, 2010) have been studied. In terms of linguistic features, existing work can be classified into tree major categories: bag-of-words (Kogan et al., 2009; Lee et al., 2014; Qiu et al., 2014), sentiment-based (Das and Chen, 2007; Li, 2010; Bollen et al., 2011; Vu et al., 2012; Luss and d’Aspremont, 2015), and information-retrieval-based (Schumaker and Chen, 2009; Xie et al., 2013; Ding et al., 2015) methods. Our work fal"
C16-1303,S15-1002,0,0.0610306,"., 2013), called BI-LSTM, consisting of two 2-layer LSTMs running on the input sequence in both for− → − → −→ ←− ←−−− ← − ward and backward directions yielding vectors hh1 , h2 , . . . , h|s |i and hh|s |, h|s|−1 , . . . , h1 i, respectively. We exclude stopwords and punctuations from each sentence. The final model outputs the information −→ ←− embedding g by concatenating the final outputs of the two LSTMs, namely h|s |and h|s |. 5.2 Dependency Tree-LSTM A syntactic approach for modeling a sentence s is to use a tree-structured LSTM (Tree-LSTM), embedding the parse tree of a sentence (Le and Zuidema, 2015; Tai et al., 2015; Zhu et al., 2015; Miwa and Bansal, 2016). Our hypothesis is that dependency relations between words convey a certain level of information content. For example, a dependency parser can tell Facebook is the subject which did the action acquired to the object Whatsapp in the sentence Facebook acquired Whatsapp. We parse sentences with ZPar (Zhang and Clark, 2011)3 and adopt the N-ary Tree-LSTM of Tai et al. (2015) with peephole connections to run on a binarized dependency-based parse tree. For the specific task, we leverage the structure of a binary Tree-LSTM, and develop a no"
C16-1303,lee-etal-2014-importance,0,0.168496,"other units will perform better than earlier anticipated to offset additional losses in the unit. 0.14 Japan’s biggest technology conglomerates reported quarterly results, with weak TV demand a common theme at both Sony Corp and Sharp Corp. Table 4: Learned weights for different news. 7 Related Work Our work is related to research that applies NLP techniques on financial text to predict stock prices and market activities. In terms of corpora, financial news (Leinweber and Sisk, 2011; Xie et al., 2013; Luss and d’Aspremont, 2015; Ding et al., 2015), firm reports (Kogan et al., 2009; Li, 2010; Lee et al., 2014; Qiu et al., 2014) and web content, such as tweets (Bollen et al., 2011; Vu et al., 2012) and forum posts (Das and Chen, 2007; Gilbert and Karahalios, 2010) have been studied. In terms of linguistic features, existing work can be classified into tree major categories: bag-of-words (Kogan et al., 2009; Lee et al., 2014; Qiu et al., 2014), sentiment-based (Das and Chen, 2007; Li, 2010; Bollen et al., 2011; Vu et al., 2012; Luss and d’Aspremont, 2015), and information-retrieval-based (Schumaker and Chen, 2009; Xie et al., 2013; Ding et al., 2015) methods. Our work falls into the category of info"
C16-1303,P16-1105,0,0.346585,"LSTMs running on the input sequence in both for− → − → −→ ←− ←−−− ← − ward and backward directions yielding vectors hh1 , h2 , . . . , h|s |i and hh|s |, h|s|−1 , . . . , h1 i, respectively. We exclude stopwords and punctuations from each sentence. The final model outputs the information −→ ←− embedding g by concatenating the final outputs of the two LSTMs, namely h|s |and h|s |. 5.2 Dependency Tree-LSTM A syntactic approach for modeling a sentence s is to use a tree-structured LSTM (Tree-LSTM), embedding the parse tree of a sentence (Le and Zuidema, 2015; Tai et al., 2015; Zhu et al., 2015; Miwa and Bansal, 2016). Our hypothesis is that dependency relations between words convey a certain level of information content. For example, a dependency parser can tell Facebook is the subject which did the action acquired to the object Whatsapp in the sentence Facebook acquired Whatsapp. We parse sentences with ZPar (Zhang and Clark, 2011)3 and adopt the N-ary Tree-LSTM of Tai et al. (2015) with peephole connections to run on a binarized dependency-based parse tree. For the specific task, we leverage the structure of a binary Tree-LSTM, and develop a novel way to represent a dependency relation between two words"
C16-1303,P15-1150,0,0.493764,"d BI-LSTM, consisting of two 2-layer LSTMs running on the input sequence in both for− → − → −→ ←− ←−−− ← − ward and backward directions yielding vectors hh1 , h2 , . . . , h|s |i and hh|s |, h|s|−1 , . . . , h1 i, respectively. We exclude stopwords and punctuations from each sentence. The final model outputs the information −→ ←− embedding g by concatenating the final outputs of the two LSTMs, namely h|s |and h|s |. 5.2 Dependency Tree-LSTM A syntactic approach for modeling a sentence s is to use a tree-structured LSTM (Tree-LSTM), embedding the parse tree of a sentence (Le and Zuidema, 2015; Tai et al., 2015; Zhu et al., 2015; Miwa and Bansal, 2016). Our hypothesis is that dependency relations between words convey a certain level of information content. For example, a dependency parser can tell Facebook is the subject which did the action acquired to the object Whatsapp in the sentence Facebook acquired Whatsapp. We parse sentences with ZPar (Zhang and Clark, 2011)3 and adopt the N-ary Tree-LSTM of Tai et al. (2015) with peephole connections to run on a binarized dependency-based parse tree. For the specific task, we leverage the structure of a binary Tree-LSTM, and develop a novel way to represe"
C16-1303,W12-5503,0,0.0466205,"he unit. 0.14 Japan’s biggest technology conglomerates reported quarterly results, with weak TV demand a common theme at both Sony Corp and Sharp Corp. Table 4: Learned weights for different news. 7 Related Work Our work is related to research that applies NLP techniques on financial text to predict stock prices and market activities. In terms of corpora, financial news (Leinweber and Sisk, 2011; Xie et al., 2013; Luss and d’Aspremont, 2015; Ding et al., 2015), firm reports (Kogan et al., 2009; Li, 2010; Lee et al., 2014; Qiu et al., 2014) and web content, such as tweets (Bollen et al., 2011; Vu et al., 2012) and forum posts (Das and Chen, 2007; Gilbert and Karahalios, 2010) have been studied. In terms of linguistic features, existing work can be classified into tree major categories: bag-of-words (Kogan et al., 2009; Lee et al., 2014; Qiu et al., 2014), sentiment-based (Das and Chen, 2007; Li, 2010; Bollen et al., 2011; Vu et al., 2012; Luss and d’Aspremont, 2015), and information-retrieval-based (Schumaker and Chen, 2009; Xie et al., 2013; Ding et al., 2015) methods. Our work falls into the category of information retrievalbased features by exploiting syntax information derived from a dependency"
C16-1303,P13-1086,0,0.226601,".86 Sony Corp stuck with its full-year profit forecast after slashing its outlook for TV sales, confident that other units will perform better than earlier anticipated to offset additional losses in the unit. 0.14 Japan’s biggest technology conglomerates reported quarterly results, with weak TV demand a common theme at both Sony Corp and Sharp Corp. Table 4: Learned weights for different news. 7 Related Work Our work is related to research that applies NLP techniques on financial text to predict stock prices and market activities. In terms of corpora, financial news (Leinweber and Sisk, 2011; Xie et al., 2013; Luss and d’Aspremont, 2015; Ding et al., 2015), firm reports (Kogan et al., 2009; Li, 2010; Lee et al., 2014; Qiu et al., 2014) and web content, such as tweets (Bollen et al., 2011; Vu et al., 2012) and forum posts (Das and Chen, 2007; Gilbert and Karahalios, 2010) have been studied. In terms of linguistic features, existing work can be classified into tree major categories: bag-of-words (Kogan et al., 2009; Lee et al., 2014; Qiu et al., 2014), sentiment-based (Das and Chen, 2007; Li, 2010; Bollen et al., 2011; Vu et al., 2012; Luss and d’Aspremont, 2015), and information-retrieval-based (Sc"
C16-1303,J11-1005,1,0.806804,"ng the final outputs of the two LSTMs, namely h|s |and h|s |. 5.2 Dependency Tree-LSTM A syntactic approach for modeling a sentence s is to use a tree-structured LSTM (Tree-LSTM), embedding the parse tree of a sentence (Le and Zuidema, 2015; Tai et al., 2015; Zhu et al., 2015; Miwa and Bansal, 2016). Our hypothesis is that dependency relations between words convey a certain level of information content. For example, a dependency parser can tell Facebook is the subject which did the action acquired to the object Whatsapp in the sentence Facebook acquired Whatsapp. We parse sentences with ZPar (Zhang and Clark, 2011)3 and adopt the N-ary Tree-LSTM of Tai et al. (2015) with peephole connections to run on a binarized dependency-based parse tree. For the specific task, we leverage the structure of a binary Tree-LSTM, and develop a novel way to represent a dependency relation between two words using this structure (Section 5.2.1). We propose an algorithm to transform a dependency parse tree to a binary tree, where leaf nodes are words and internal nodes are dependency relations, so that the transformed tree can be embedded using binary Tree-LSTM (Section 5.2.2). Finally we explain how the task of information"
C18-1011,P16-1231,0,0.0611127,"Missing"
C18-1011,C10-1011,0,0.0679555,"Missing"
C18-1011,W08-2102,0,0.206552,"uction probability for the rule r : [i, j] → [i, k][k + 1, j] is given by a softmax distribution, exp(psk ) P ([i, j] → [i, k][k + 1, j]|S, Θ) = Pj−1 . 0) exp(ps 0 k k =i 122 The training objective is to minimize the log probability loss of all unlabeled production rules. X Lrule = − log P (r : [i, j] → [i, k][k + 1, j]|S, Θ) r∈Tub The decoding algorithm is the standard CKY algorithm, which we omit here. The rule model can be regarded as a first-order constituent model, with the probability of each phrase rule being modeled. However, unlike structured learning algorithms (Finkel et al., 2008; Carreras et al., 2008), which use a global score for each tree, our model learns each production rule probability individually. Such local learning has traditionally been found subjective to label bias (Lafferty et al., 2001). Our model relies on input representations solely for resolving this issue. Span Representation. Figure 2b shows one possible network architecture for the rule model by taking the partition point k = 1 for the span [1, 3] as an example. The BiLSTM encoder layer in the bottom is the same as that of the previous span classification model. We obtain the span representation vectors using differenc"
C18-1011,P05-1022,0,0.278507,"Missing"
C18-1011,A00-2018,0,0.676049,"Missing"
C18-1011,W14-4012,0,0.106835,"Missing"
C18-1011,D16-1257,0,0.438385,"Missing"
C18-1011,P16-1017,0,0.0362282,"Missing"
C18-1011,J03-4003,0,0.724109,"us et al., 1993). Sections 02-21, section 22 and section 23 are used for training, development and test sets, respectively. Our Chinese data are the version 5.1 of the Penn Chinese Treebank (CTB) (Xue et al., 2005). The training set consists of articles 001-270 and 440-1151, the development set contains articles 301-325 and the test set includes articles 271-300. We use automatically reassigned POS tags in the same way as Cross and Huang (2016b) for English and Dyer et al. (2016) for Chinese. We use ZPar (Zhang and Clark, 2011)1 to binarize both English and Chinese data with the head rules of Collins (2003). The head directions of the binarization results are ignored during training. The types of English and Chinese constituent span labels after binarization are 52 and 56, respectively. The maximum number of greedy decoding steps for generating consecutive constituent labels is limited to 4 for both English and Chinese. We evaluate parsing performance in terms of both unlabeled bracketing metrics and labeled bracketing metrics including unlabeled F1 (UF)2 , labeled precision (LP), labeled recall (LR) and labeled bracketing F1 (LF) after debinarization using EVALB3 . Unknown words. For English, w"
C18-1011,P16-2006,0,0.18638,"al., 2010) and dual decomposition (Koo et al., 2010) techniques have been exploited by graph-based parser to integrate non-local features. Transition-based parsers (Nivre, 2003; Nivre, 2008; Zhang and Nivre, 2011; Bohnet, 2010; Huang et al., 2012) are also known for leveraging non-local features for achieving high accuracies. For most state-of-the-art statistical parsers, a global training objective over the entire parse tree has been defined to avoid label bias (Lafferty et al., 2001). For neural parsing, on the other hand, local models have been shown to give highly competitive accuracies (Cross and Huang, 2016b; Stern et al., 2017) as compared to those that employ long-range features (Watanabe and Sumita, 2015; Zhou et al., 2015; Andor et al., 2016; Durrett and Klein, 2015). Highly local features have been used in recent state-of-the-art models (Stern et al., 2017; Dozat and Manning, 2016; Shi et al., 2017). In particular, Dozat and Manning (2016) show that a locally trained arc-factored model can give the best reported accuracies on dependency parsing. The surprising result has been largely attributed to the representation power of long short-term memory (LSTM) encoders (Kiperwasser and Goldberg,"
C18-1011,D16-1001,0,0.125346,"al., 2010) and dual decomposition (Koo et al., 2010) techniques have been exploited by graph-based parser to integrate non-local features. Transition-based parsers (Nivre, 2003; Nivre, 2008; Zhang and Nivre, 2011; Bohnet, 2010; Huang et al., 2012) are also known for leveraging non-local features for achieving high accuracies. For most state-of-the-art statistical parsers, a global training objective over the entire parse tree has been defined to avoid label bias (Lafferty et al., 2001). For neural parsing, on the other hand, local models have been shown to give highly competitive accuracies (Cross and Huang, 2016b; Stern et al., 2017) as compared to those that employ long-range features (Watanabe and Sumita, 2015; Zhou et al., 2015; Andor et al., 2016; Durrett and Klein, 2015). Highly local features have been used in recent state-of-the-art models (Stern et al., 2017; Dozat and Manning, 2016; Shi et al., 2017). In particular, Dozat and Manning (2016) show that a locally trained arc-factored model can give the best reported accuracies on dependency parsing. The surprising result has been largely attributed to the representation power of long short-term memory (LSTM) encoders (Kiperwasser and Goldberg,"
C18-1011,P15-1030,0,0.0182706,"onstituent labels. 5 Table 7: UF, oralce LF and speed. Related Work Globally trained discriminative models have given highly competitive accuracies on graph-based constituent parsing. The key is to explicitly consider connections between output substructures in order to avoid label bias. State-of-the-art statistical methods use a single model to score a feature representation for all phrase-structure rules in a parse tree (Taskar et al., 2004; Finkel et al., 2008; Carreras et al., 2008). More sophisticated features that span over more than one rule have been used for reranking (Huang, 2008b). Durrett and Klein (2015) used neural networks to augment manual indicator features for CRF parsing. Structured learning has been used for transition-based constituent parsing also (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhang and Clark, 2011; Zhu et al., 2013), and neural network models have been used to substitute indicator features for transition-based parsing (Watanabe and Sumita, 2015; Dyer et al., 2016; Goldberg et al., 2014; Kiperwasser and Goldberg, 2016; Cross and Huang, 2016a; Coavoux and Crabb´e, 2016; Shi et al., 2017). Compared to the above methods on constituent parsing, our method does not use gl"
C18-1011,N16-1024,0,0.335699,"ese. Following standard conventions, our English data are obtained from the Wall Street Journal (WSJ) of the Penn Treebank (PTB) (Marcus et al., 1993). Sections 02-21, section 22 and section 23 are used for training, development and test sets, respectively. Our Chinese data are the version 5.1 of the Penn Chinese Treebank (CTB) (Xue et al., 2005). The training set consists of articles 001-270 and 440-1151, the development set contains articles 301-325 and the test set includes articles 271-300. We use automatically reassigned POS tags in the same way as Cross and Huang (2016b) for English and Dyer et al. (2016) for Chinese. We use ZPar (Zhang and Clark, 2011)1 to binarize both English and Chinese data with the head rules of Collins (2003). The head directions of the binarization results are ignored during training. The types of English and Chinese constituent span labels after binarization are 52 and 56, respectively. The maximum number of greedy decoding steps for generating consecutive constituent labels is limited to 4 for both English and Chinese. We evaluate parsing performance in terms of both unlabeled bracketing metrics and labeled bracketing metrics including unlabeled F1 (UF)2 , labeled pr"
C18-1011,P15-1147,0,0.0265308,"Missing"
C18-1011,P08-1109,0,0.228319,". The unlabeled production probability for the rule r : [i, j] → [i, k][k + 1, j] is given by a softmax distribution, exp(psk ) P ([i, j] → [i, k][k + 1, j]|S, Θ) = Pj−1 . 0) exp(ps 0 k k =i 122 The training objective is to minimize the log probability loss of all unlabeled production rules. X Lrule = − log P (r : [i, j] → [i, k][k + 1, j]|S, Θ) r∈Tub The decoding algorithm is the standard CKY algorithm, which we omit here. The rule model can be regarded as a first-order constituent model, with the probability of each phrase rule being modeled. However, unlike structured learning algorithms (Finkel et al., 2008; Carreras et al., 2008), which use a global score for each tree, our model learns each production rule probability individually. Such local learning has traditionally been found subjective to label bias (Lafferty et al., 2001). Our model relies on input representations solely for resolving this issue. Span Representation. Figure 2b shows one possible network architecture for the rule model by taking the partition point k = 1 for the span [1, 3] as an example. The BiLSTM encoder layer in the bottom is the same as that of the previous span classification model. We obtain the span representation"
C18-1011,P17-2025,0,0.249983,"Missing"
C18-1011,D09-1087,0,0.0789978,"Missing"
C18-1011,D10-1002,0,0.0602934,"Missing"
C18-1011,N12-1015,0,0.0558415,"Missing"
C18-1011,P08-1067,0,0.243284,"res for major constituent labels. 5 Table 7: UF, oralce LF and speed. Related Work Globally trained discriminative models have given highly competitive accuracies on graph-based constituent parsing. The key is to explicitly consider connections between output substructures in order to avoid label bias. State-of-the-art statistical methods use a single model to score a feature representation for all phrase-structure rules in a parse tree (Taskar et al., 2004; Finkel et al., 2008; Carreras et al., 2008). More sophisticated features that span over more than one rule have been used for reranking (Huang, 2008b). Durrett and Klein (2015) used neural networks to augment manual indicator features for CRF parsing. Structured learning has been used for transition-based constituent parsing also (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhang and Clark, 2011; Zhu et al., 2013), and neural network models have been used to substitute indicator features for transition-based parsing (Watanabe and Sumita, 2015; Dyer et al., 2016; Goldberg et al., 2014; Kiperwasser and Goldberg, 2016; Cross and Huang, 2016a; Coavoux and Crabb´e, 2016; Shi et al., 2017). Compared to the above methods on constituent parsing"
C18-1011,Q16-1023,0,0.173271,"ies (Cross and Huang, 2016b; Stern et al., 2017) as compared to those that employ long-range features (Watanabe and Sumita, 2015; Zhou et al., 2015; Andor et al., 2016; Durrett and Klein, 2015). Highly local features have been used in recent state-of-the-art models (Stern et al., 2017; Dozat and Manning, 2016; Shi et al., 2017). In particular, Dozat and Manning (2016) show that a locally trained arc-factored model can give the best reported accuracies on dependency parsing. The surprising result has been largely attributed to the representation power of long short-term memory (LSTM) encoders (Kiperwasser and Goldberg, 2016). An interesting research question is to what extent the encoding power can be leveraged for constituent parsing. We investigate the problem by building a chart-based model that is local to unlabeled constituent spans (Abney, 1991) and CFG-rules, which have been explored by early PCFG models (Collins, 2003; Klein and Manning, 2003). In particular, our models first predict unlabeled CFG trees leveraging biaffine modelling (Dozat and Manning, 2016). Then, constituent labels are assigned on unlabeled trees by using a tree-LSTM to encode the syntactic structure, and a LSTM decoder for yielding lab"
C18-1011,P03-1054,0,0.124367,"Missing"
C18-1011,P10-1001,0,0.0689372,"Missing"
C18-1011,D10-1125,0,0.082034,"Missing"
C18-1011,E17-1117,0,0.0913519,"Missing"
C18-1011,Q17-1029,1,0.886697,"Missing"
C18-1011,Q17-1004,1,0.880264,"Missing"
C18-1011,I17-1007,0,0.0460348,"o calculate the partition score psk . In particular, we investigate two scoring methods. Linear Model. In the linear model, the partition score is calculated by a linear affine transformation. For the splitting point k, T T r[k + 1, j] + bll,k psk = wll,k r[i, k] + wlr,k T and wT are two vectors, and b where wll,k ll,k is a size 1 parameter. ll,k Biaffine model. Since the possible splitting points for spans are varied with the length of span, we also try a biaffine scoring model (as shown in Figure 2b), which is good at handling variable-sized classification problems (Dozat and Manning, 2016; Ma and Hovy, 2017). The biaffine model produces the score lpsk between the parent span [i, j] and the left child span [i, k] using a biaffine scorer lpsk = (r[i, j] ⊕ 1)T Wpl (r[i, k] ⊕ 1) (9) where Wpl is model parameters. Similarly, we calculate the score rpsk between the parent span [i, j] and the right child span [k + 1, j] using Wpr and bpr as parameters. The overall partition score psk is therefore given by psk = lpsk + rpsk . 2.3 Label Generator Lexicalized Tree-LSTM Encoder. Shown in Figure 1c, we use lexicalized tree LSTM (Teng and Zhang, 2016) for encoding, which shows good representation abilities fo"
C18-1011,J93-2004,0,0.0611825,"r LSTM hidden units 25 Tree-LSTM hidden units 200 POS tag embeddings 32 32 Label LSTM layers 1 Constituent label embeddings Label LSTM hidden units 200 Last output layer hidden units 128 Maximum training epochs 50 Dropout English: 0.5, Chinese 0.3 Trainer SGD Initial learning rate 0.1 Per-epoch decay 0.05 φ ELU (Clevert et al., 2015) Table 1: Hyper-parameters for training. 3 Experiments 3.1 Experimental Settings Data. We perform experiments for both English and Chinese. Following standard conventions, our English data are obtained from the Wall Street Journal (WSJ) of the Penn Treebank (PTB) (Marcus et al., 1993). Sections 02-21, section 22 and section 23 are used for training, development and test sets, respectively. Our Chinese data are the version 5.1 of the Penn Chinese Treebank (CTB) (Xue et al., 2005). The training set consists of articles 001-270 and 440-1151, the development set contains articles 301-325 and the test set includes articles 271-300. We use automatically reassigned POS tags in the same way as Cross and Huang (2016b) for English and Dyer et al. (2016) for Chinese. We use ZPar (Zhang and Clark, 2011)1 to binarize both English and Chinese data with the head rules of Collins (2003)."
C18-1011,D10-1004,0,0.0657175,"Missing"
C18-1011,P06-1043,0,0.155401,"Missing"
C18-1011,W03-3017,0,0.270605,"Missing"
C18-1011,J08-4003,0,0.0960082,"Missing"
C18-1011,N07-1051,0,0.274043,"Missing"
C18-1011,W05-1513,0,0.0420146,"g. The key is to explicitly consider connections between output substructures in order to avoid label bias. State-of-the-art statistical methods use a single model to score a feature representation for all phrase-structure rules in a parse tree (Taskar et al., 2004; Finkel et al., 2008; Carreras et al., 2008). More sophisticated features that span over more than one rule have been used for reranking (Huang, 2008b). Durrett and Klein (2015) used neural networks to augment manual indicator features for CRF parsing. Structured learning has been used for transition-based constituent parsing also (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhang and Clark, 2011; Zhu et al., 2013), and neural network models have been used to substitute indicator features for transition-based parsing (Watanabe and Sumita, 2015; Dyer et al., 2016; Goldberg et al., 2014; Kiperwasser and Goldberg, 2016; Cross and Huang, 2016a; Coavoux and Crabb´e, 2016; Shi et al., 2017). Compared to the above methods on constituent parsing, our method does not use global structured learning, but instead learns local constituent patterns, relying on a bi-directional LSTM encoder for capturing non-local structural relations in the input. Our w"
C18-1011,N06-2033,0,0.125527,"Missing"
C18-1011,D17-1002,0,0.0572373,"ving high accuracies. For most state-of-the-art statistical parsers, a global training objective over the entire parse tree has been defined to avoid label bias (Lafferty et al., 2001). For neural parsing, on the other hand, local models have been shown to give highly competitive accuracies (Cross and Huang, 2016b; Stern et al., 2017) as compared to those that employ long-range features (Watanabe and Sumita, 2015; Zhou et al., 2015; Andor et al., 2016; Durrett and Klein, 2015). Highly local features have been used in recent state-of-the-art models (Stern et al., 2017; Dozat and Manning, 2016; Shi et al., 2017). In particular, Dozat and Manning (2016) show that a locally trained arc-factored model can give the best reported accuracies on dependency parsing. The surprising result has been largely attributed to the representation power of long short-term memory (LSTM) encoders (Kiperwasser and Goldberg, 2016). An interesting research question is to what extent the encoding power can be leveraged for constituent parsing. We investigate the problem by building a chart-based model that is local to unlabeled constituent spans (Abney, 1991) and CFG-rules, which have been explored by early PCFG models (Coll"
C18-1011,P12-1046,0,0.0596208,"Missing"
C18-1011,P13-1045,0,0.034862,"Missing"
C18-1011,P17-1076,0,0.296834,"omposition (Koo et al., 2010) techniques have been exploited by graph-based parser to integrate non-local features. Transition-based parsers (Nivre, 2003; Nivre, 2008; Zhang and Nivre, 2011; Bohnet, 2010; Huang et al., 2012) are also known for leveraging non-local features for achieving high accuracies. For most state-of-the-art statistical parsers, a global training objective over the entire parse tree has been defined to avoid label bias (Lafferty et al., 2001). For neural parsing, on the other hand, local models have been shown to give highly competitive accuracies (Cross and Huang, 2016b; Stern et al., 2017) as compared to those that employ long-range features (Watanabe and Sumita, 2015; Zhou et al., 2015; Andor et al., 2016; Durrett and Klein, 2015). Highly local features have been used in recent state-of-the-art models (Stern et al., 2017; Dozat and Manning, 2016; Shi et al., 2017). In particular, Dozat and Manning (2016) show that a locally trained arc-factored model can give the best reported accuracies on dependency parsing. The surprising result has been largely attributed to the representation power of long short-term memory (LSTM) encoders (Kiperwasser and Goldberg, 2016). An interesting"
C18-1011,P15-1150,0,0.156843,"Different from the span model, the rule model considers the probability P ([i, j] → [i, k][k + 1, j]|S) for the production rule that the span [i, j] is composed by two children spans [i, k] and [k + 1, j], where i ≤ k < j. For example, in Figure 1a, the rule model assigns high probability to the rule [0, 2] → [0, 0][1, 2] instead of the rule [0, 2] → [0, 1][2, 2]. Given the local probabilities, we use CKY algorithm to find the unlabeled binarized parses. The label generator encodes a binarized unlabeled tree and to predict constituent labels for every span. The encoder is a binary tree-LSTM (Tai et al., 2015; Zhu et al., 2015), which recursively composes the representation vectors for tree nodes bottom-up. Based on the representation vector of a constituent span, a LSTM decoder (Cho et al., 2014; Sutskever et al., 2014) generates chains of constituent labels, which 120 can represent unary rules. For example, the decoder outputs “VP →S→ </L>” for the span [4, 4] and “NP→ </L>” for the span [0,2] in Figure 1c where </L> is a stopping symbol. 2.1 Span Model Given an unlabeled binarized tree Tub for the sentence S, S = w0 , w1 . . . wn−1 , the span model trains a neural network model P (Y[i,j] |S, Θ)"
C18-1011,W04-3201,0,0.0979978,"F LF Speed(sents/s) 95.87 89.57 BinarySpan 92.16 96.79 22.12 95.98 89.51 MultiSpan 92.50 97.03 21.55 96.66 90.31 BiaffineRule 92.22 97.12 6.00 Table 6: LF scores for major constituent labels. 5 Table 7: UF, oralce LF and speed. Related Work Globally trained discriminative models have given highly competitive accuracies on graph-based constituent parsing. The key is to explicitly consider connections between output substructures in order to avoid label bias. State-of-the-art statistical methods use a single model to score a feature representation for all phrase-structure rules in a parse tree (Taskar et al., 2004; Finkel et al., 2008; Carreras et al., 2008). More sophisticated features that span over more than one rule have been used for reranking (Huang, 2008b). Durrett and Klein (2015) used neural networks to augment manual indicator features for CRF parsing. Structured learning has been used for transition-based constituent parsing also (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhang and Clark, 2011; Zhu et al., 2013), and neural network models have been used to substitute indicator features for transition-based parsing (Watanabe and Sumita, 2015; Dyer et al., 2016; Goldberg et al., 2014; Kipe"
C18-1011,P16-1218,0,0.0403747,"a global score for each tree, our model learns each production rule probability individually. Such local learning has traditionally been found subjective to label bias (Lafferty et al., 2001). Our model relies on input representations solely for resolving this issue. Span Representation. Figure 2b shows one possible network architecture for the rule model by taking the partition point k = 1 for the span [1, 3] as an example. The BiLSTM encoder layer in the bottom is the same as that of the previous span classification model. We obtain the span representation vectors using difference vectors (Wang and Chang, 2016; Cross and Huang, 2016b). Formally, the span representation vector sr[i, j] is given by, s[i, j] = [fj+1 − fi ; ri − rj+1 ], sr[i, j] = [s[0, i − 1]; s[i, j]; s[j + 1, n − 1]]. (7) We first combine the difference vectors (fj+1 − fi ) and (ri − rj+1 ) to obtain a simple span representation vector s[i, j]. In order to take more contextual information such as fp where p > j + 1 and rq where q < i, we concatenate s[0, i − 1], s[i, j], and s[j + 1, n − 1] to produce the final span representation vector sr[i, j]. We then transform sr[i, j] to an output vector r[i, j] using an activation function φ,"
C18-1011,P14-1069,0,0.0174806,"Petrov and Klein (2007) Zhang and Clark (2009) Watanabe and Sumita (2015) Dyer et al. (2016) BinarySpan MultiSpan BiaffineRule LR LP LF 81.9 84.8 83.3 78.6 78.0 78.3 84.3 84.6 85.9 87.1 86.5 86.6 88.0 87.3 87.1 87.5 87.3 Table 5: Results on the Chinese Treebank 5.1 test set. Compared with the in-order transition-based parser, our best model improves the labeled F1 score by 1.2 (86.1 → 87.3). In addition, MultiSpan and BiaffineRule achieve better performance than the reranking system using recurrent neural network grammars (Dyer et al., 2016) and methods that do joint POS tagging and parsing (Wang and Xue, 2014; Wang et al., 2015). 4 Analysis Constituent label. Table 6 shows the LF scores for eight major constituent labels on PTB test set. BinarySpan consistently underperforms to the other two models. The error distribution of MultiSpan and BiaffineRule are different. For constituent labels including SBAR, WHNP and QP, BiaffineRule is the winner. This is likely because the partition point distribution of these labels are less trivial than other labels. For NP, PP, ADVP and ADJP, MultiSpan obtains better scores than BiaffineRule, showing the importance of the explicit type information for correctly i"
C18-1011,P15-1110,0,0.176328,"Missing"
C18-1011,P15-1113,0,0.126102,"d parser to integrate non-local features. Transition-based parsers (Nivre, 2003; Nivre, 2008; Zhang and Nivre, 2011; Bohnet, 2010; Huang et al., 2012) are also known for leveraging non-local features for achieving high accuracies. For most state-of-the-art statistical parsers, a global training objective over the entire parse tree has been defined to avoid label bias (Lafferty et al., 2001). For neural parsing, on the other hand, local models have been shown to give highly competitive accuracies (Cross and Huang, 2016b; Stern et al., 2017) as compared to those that employ long-range features (Watanabe and Sumita, 2015; Zhou et al., 2015; Andor et al., 2016; Durrett and Klein, 2015). Highly local features have been used in recent state-of-the-art models (Stern et al., 2017; Dozat and Manning, 2016; Shi et al., 2017). In particular, Dozat and Manning (2016) show that a locally trained arc-factored model can give the best reported accuracies on dependency parsing. The surprising result has been largely attributed to the representation power of long short-term memory (LSTM) encoders (Kiperwasser and Goldberg, 2016). An interesting research question is to what extent the encoding power can be leveraged for cons"
C18-1011,W09-3825,1,0.75956,"itly consider connections between output substructures in order to avoid label bias. State-of-the-art statistical methods use a single model to score a feature representation for all phrase-structure rules in a parse tree (Taskar et al., 2004; Finkel et al., 2008; Carreras et al., 2008). More sophisticated features that span over more than one rule have been used for reranking (Huang, 2008b). Durrett and Klein (2015) used neural networks to augment manual indicator features for CRF parsing. Structured learning has been used for transition-based constituent parsing also (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhang and Clark, 2011; Zhu et al., 2013), and neural network models have been used to substitute indicator features for transition-based parsing (Watanabe and Sumita, 2015; Dyer et al., 2016; Goldberg et al., 2014; Kiperwasser and Goldberg, 2016; Cross and Huang, 2016a; Coavoux and Crabb´e, 2016; Shi et al., 2017). Compared to the above methods on constituent parsing, our method does not use global structured learning, but instead learns local constituent patterns, relying on a bi-directional LSTM encoder for capturing non-local structural relations in the input. Our work is inspired by the"
C18-1011,J11-1005,1,0.91119,"ish data are obtained from the Wall Street Journal (WSJ) of the Penn Treebank (PTB) (Marcus et al., 1993). Sections 02-21, section 22 and section 23 are used for training, development and test sets, respectively. Our Chinese data are the version 5.1 of the Penn Chinese Treebank (CTB) (Xue et al., 2005). The training set consists of articles 001-270 and 440-1151, the development set contains articles 301-325 and the test set includes articles 271-300. We use automatically reassigned POS tags in the same way as Cross and Huang (2016b) for English and Dyer et al. (2016) for Chinese. We use ZPar (Zhang and Clark, 2011)1 to binarize both English and Chinese data with the head rules of Collins (2003). The head directions of the binarization results are ignored during training. The types of English and Chinese constituent span labels after binarization are 52 and 56, respectively. The maximum number of greedy decoding steps for generating consecutive constituent labels is limited to 4 for both English and Chinese. We evaluate parsing performance in terms of both unlabeled bracketing metrics and labeled bracketing metrics including unlabeled F1 (UF)2 , labeled precision (LP), labeled recall (LR) and labeled bra"
C18-1011,P11-2033,1,0.866754,"Missing"
C18-1011,P15-1117,1,0.844457,"Missing"
C18-1011,P13-1043,1,0.860404,"ructures in order to avoid label bias. State-of-the-art statistical methods use a single model to score a feature representation for all phrase-structure rules in a parse tree (Taskar et al., 2004; Finkel et al., 2008; Carreras et al., 2008). More sophisticated features that span over more than one rule have been used for reranking (Huang, 2008b). Durrett and Klein (2015) used neural networks to augment manual indicator features for CRF parsing. Structured learning has been used for transition-based constituent parsing also (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhang and Clark, 2011; Zhu et al., 2013), and neural network models have been used to substitute indicator features for transition-based parsing (Watanabe and Sumita, 2015; Dyer et al., 2016; Goldberg et al., 2014; Kiperwasser and Goldberg, 2016; Cross and Huang, 2016a; Coavoux and Crabb´e, 2016; Shi et al., 2017). Compared to the above methods on constituent parsing, our method does not use global structured learning, but instead learns local constituent patterns, relying on a bi-directional LSTM encoder for capturing non-local structural relations in the input. Our work is inspired by the biaffine dependency parser of Dozat and Ma"
C18-1239,C16-1303,1,0.895256,", which suffers from feature sparsity problems. With the recent trends in deep learning for NLP, neural networks have also been leveraged to learn dense representations for text elements, which can address the sparsity of discrete features in statistical models. Such representations can implicitly represent sentiment, event and factual information, which can be extremely challenging for sparse indicator features. In particular, Ding et al. (2015) show that deep learning representations of event structures yield better accuracies for stock market prediction compared to discrete event features. Chang et al. (2016), Duan et al. (2018) use neural networks to directly learn representations of news abstracts, showing that it is effective for predicting the cumulative abnormal returns of public companies. One limitation of Ding et al. (2015) and Chang et al. (2016), however, is that these methods only model news titles and abstract texts, which are typically single sentences. Ding et al. (2015) show that a model that uses only news content gives inferior results compared to one trained using news titles only, and that adding news content information to a title-driven model does not significantly improve the"
C18-1239,D14-1148,1,0.936087,"tion Texts from the Internet was shown to be statistically correlated with stock market trends (Antweiler and Frank, 2004). Natural language processing (NLP) techniques have been applied to extract information from company filings (Lee et al., 2014), financial news articles (Xie et al., 2013) and social media texts (Bollen et al., 2011) in order to gain understandings of financial markets. In particular, traditional methods exploited statistical signals, such as lexical and syntactic features (Wang and Hua, 2014; Schumaker and Chen, 2009), sentiment (Bollen et al., 2011) and event structures (Ding et al., 2014; Ding et al., 2015; Ding et al., 2016) from these text sources, which suffers from feature sparsity problems. With the recent trends in deep learning for NLP, neural networks have also been leveraged to learn dense representations for text elements, which can address the sparsity of discrete features in statistical models. Such representations can implicitly represent sentiment, event and factual information, which can be extremely challenging for sparse indicator features. In particular, Ding et al. (2015) show that deep learning representations of event structures yield better accuracies fo"
C18-1239,C16-1201,1,0.788438,"to be statistically correlated with stock market trends (Antweiler and Frank, 2004). Natural language processing (NLP) techniques have been applied to extract information from company filings (Lee et al., 2014), financial news articles (Xie et al., 2013) and social media texts (Bollen et al., 2011) in order to gain understandings of financial markets. In particular, traditional methods exploited statistical signals, such as lexical and syntactic features (Wang and Hua, 2014; Schumaker and Chen, 2009), sentiment (Bollen et al., 2011) and event structures (Ding et al., 2014; Ding et al., 2015; Ding et al., 2016) from these text sources, which suffers from feature sparsity problems. With the recent trends in deep learning for NLP, neural networks have also been leveraged to learn dense representations for text elements, which can address the sparsity of discrete features in statistical models. Such representations can implicitly represent sentiment, event and factual information, which can be extremely challenging for sparse indicator features. In particular, Ding et al. (2015) show that deep learning representations of event structures yield better accuracies for stock market prediction compared to d"
C18-1239,N18-1051,1,0.840339,"feature sparsity problems. With the recent trends in deep learning for NLP, neural networks have also been leveraged to learn dense representations for text elements, which can address the sparsity of discrete features in statistical models. Such representations can implicitly represent sentiment, event and factual information, which can be extremely challenging for sparse indicator features. In particular, Ding et al. (2015) show that deep learning representations of event structures yield better accuracies for stock market prediction compared to discrete event features. Chang et al. (2016), Duan et al. (2018) use neural networks to directly learn representations of news abstracts, showing that it is effective for predicting the cumulative abnormal returns of public companies. One limitation of Ding et al. (2015) and Chang et al. (2016), however, is that these methods only model news titles and abstract texts, which are typically single sentences. Ding et al. (2015) show that a model that uses only news content gives inferior results compared to one trained using news titles only, and that adding news content information to a title-driven model does not significantly improve the results. Intuitivel"
C18-1239,P16-1089,0,0.0690722,"Missing"
C18-1239,lee-etal-2014-importance,0,0.0211814,"most informative sentences for market modeling. Results show that document representations can give better performance for estimating cumulative abnormal returns of companies when compared to titles and abstracts. Our model is especially effective when it used to combine information from multiple document sources compared to the sentence-level baselines. 1 Introduction Texts from the Internet was shown to be statistically correlated with stock market trends (Antweiler and Frank, 2004). Natural language processing (NLP) techniques have been applied to extract information from company filings (Lee et al., 2014), financial news articles (Xie et al., 2013) and social media texts (Bollen et al., 2011) in order to gain understandings of financial markets. In particular, traditional methods exploited statistical signals, such as lexical and syntactic features (Wang and Hua, 2014; Schumaker and Chen, 2009), sentiment (Bollen et al., 2011) and event structures (Ding et al., 2014; Ding et al., 2015; Ding et al., 2016) from these text sources, which suffers from feature sparsity problems. With the recent trends in deep learning for NLP, neural networks have also been leveraged to learn dense representations"
C18-1239,P15-1107,0,0.46682,"al., 2015), using an embedding vector of the target firm of concern et (c) as the initial state vector for the sentence-level Bi-LSTMs. We average the hidden states of each word in the sentence to obtain the target-dependent news abstract representation va . The vector 2825 et (c) for the company of interest is initialized by averaging the words of its constituents and are finetuned during training. 3.2 Context-sensitive Sentence Representation To preserve the semantic structures of documents and make the sentence representations aware of their contexts, we leverage a hierarchical structure (Li et al., 2015) to encode sentences (the abstracts are not considered) in a document. A sentence-level LSTM is first used to encode words into hidden state vectors, and then a document-level LSTM is applied to encode sentences into hidden state vectors. At the sentence level, given a sentence {w1 , w2 , · · · , wn }, we obtain their embedding forms {~e(w1 ), ~e(w2 ), · · · , ~e(wn )} via a lookup table. A Bi-LSTM is used to capture sentence-level con− → − → − → w w text from {~e(w1 ), ~e(w2 ), · · · , ~e(wn )}, yielding two sequences of hidden states {hw 1 , h2 , · · · , hn } and ← − ← − ← − ← − − → w w w w"
C18-1239,P15-1150,0,0.00951371,"method with a set of baseline representation learning methods for df learning including target-dependent abstract representation method of Chang et al. (2016), and state-of-the-art targetindependent document representations. Target-dependent News Abstract representation (TGT-CTX-LSTM) Chang et al. (2016) give the current state-of-the-art accuracies for CAR prediction by using the abstract only. They used a syntactic parser to analyze the dependency structure of the abstract (as a sentence), and then transform it to a target-specific dependency tree form. They leverage a tree structured LSTM (Tai et al., 2015) to learn abstract representation according to the target-specific tree form. Paragraph Vector We take the paragraph vector embedding of Le and Mikolov (2014) as an unsupervised full document representation baseline. This method gives remarkable performances on tasks such as document classification (Yang et al., 2016) and sentiment analysis (Tang et al., 2015). Target-dependent sentence combination (TD-AVG) This model first learns a context-sensitive representation for all sentences using Bi-LSTM and conditional encoding (Rockt¨aschel et al., 2015). Average pooling is then applied on the sente"
C18-1239,D15-1167,1,0.770456,"rediction by using the abstract only. They used a syntactic parser to analyze the dependency structure of the abstract (as a sentence), and then transform it to a target-specific dependency tree form. They leverage a tree structured LSTM (Tai et al., 2015) to learn abstract representation according to the target-specific tree form. Paragraph Vector We take the paragraph vector embedding of Le and Mikolov (2014) as an unsupervised full document representation baseline. This method gives remarkable performances on tasks such as document classification (Yang et al., 2016) and sentiment analysis (Tang et al., 2015). Target-dependent sentence combination (TD-AVG) This model first learns a context-sensitive representation for all sentences using Bi-LSTM and conditional encoding (Rockt¨aschel et al., 2015). Average pooling is then applied on the sentence representations to obtain the final document representation. This baseline is equivalent to a conditionally encoded version of the target-specific model of Li et al. (2015), but with a different training objective (i.e., classification instead of auto-encoder). We refer to it as TDAVG. We use conditional encoding (Rockt¨aschel et al., 2015) on the sentence"
C18-1239,P14-1109,0,0.134517,"mbine information from multiple document sources compared to the sentence-level baselines. 1 Introduction Texts from the Internet was shown to be statistically correlated with stock market trends (Antweiler and Frank, 2004). Natural language processing (NLP) techniques have been applied to extract information from company filings (Lee et al., 2014), financial news articles (Xie et al., 2013) and social media texts (Bollen et al., 2011) in order to gain understandings of financial markets. In particular, traditional methods exploited statistical signals, such as lexical and syntactic features (Wang and Hua, 2014; Schumaker and Chen, 2009), sentiment (Bollen et al., 2011) and event structures (Ding et al., 2014; Ding et al., 2015; Ding et al., 2016) from these text sources, which suffers from feature sparsity problems. With the recent trends in deep learning for NLP, neural networks have also been leveraged to learn dense representations for text elements, which can address the sparsity of discrete features in statistical models. Such representations can implicitly represent sentiment, event and factual information, which can be extremely challenging for sparse indicator features. In particular, Ding"
C18-1239,P13-1086,0,0.143318,"ing. Results show that document representations can give better performance for estimating cumulative abnormal returns of companies when compared to titles and abstracts. Our model is especially effective when it used to combine information from multiple document sources compared to the sentence-level baselines. 1 Introduction Texts from the Internet was shown to be statistically correlated with stock market trends (Antweiler and Frank, 2004). Natural language processing (NLP) techniques have been applied to extract information from company filings (Lee et al., 2014), financial news articles (Xie et al., 2013) and social media texts (Bollen et al., 2011) in order to gain understandings of financial markets. In particular, traditional methods exploited statistical signals, such as lexical and syntactic features (Wang and Hua, 2014; Schumaker and Chen, 2009), sentiment (Bollen et al., 2011) and event structures (Ding et al., 2014; Ding et al., 2015; Ding et al., 2016) from these text sources, which suffers from feature sparsity problems. With the recent trends in deep learning for NLP, neural networks have also been leveraged to learn dense representations for text elements, which can address the spa"
C18-1239,N16-1174,0,0.690061,"tence in the news content to automatically learn its relative importance with regard to the target company. The attention weights are learned automatically towards a final predictive goal. The model is full data-driven, which do not rely on an external syntactic parser as the first step to obtain its target-specific linguistic structures. In addition to Chang et al. (2016) model, which uses only abstract information, we also compare with several state-of-the-art baselines for learning document representations, such as paragraph vector (Le and Mikolov, 2014) and hierarchical attention network (Yang et al., 2016), giving the best reported performances. The advantage of our approach over sentence-level baseline is especially obvious when it is used to combine information from multiple news document sources. In addition, a case study shows that our model can select the sentences that most intuitively help predict stock returns from a full news document. Our contributions can be summarized as follows: • We propose a target-specific document representation model, which leverages the abstracts as evidences to select informative sentences from the documents while disregarding noise. • We are the first, to o"
C18-1327,Q16-1026,0,0.0725864,"rted in the paper. Liu et al. (2018) report lower average F-scores on NER when reproducing the structure of Lample et al. (2016), and on POS tagging when reproducing Ma and Hovy (2016). Most literature compares results with others by citing the scores directly (Huang et al., 2015; Lample et al., 2016) without re-implementing them under the same setting, resulting in less persuasiveness on the advantage of their models. In addition, conclusions from different reports can be contradictory. For example, most work observes that stochastic gradient descent (SGD) gives best performance on NER task (Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016), while Reimers and Gurevych (2017b) report that SGD is the worst optimizer on the same datasets. The comparison between different deep neural models is challenging due to sensitivity on experimental settings. We list six inconsistent configurations in literature, which lead to difficulties for fair comparison. • Dataset. Most work reports sequence labeling results on both CoNLL 2003 English NER (Tjong Kim Sang and De Meulder, 2003) and PTB POS (Marcus et al., 1993) datasets (Collobert et al., 2011; Huang et al., 2015; Ma and Hovy, 2016). Ling et al. (2"
C18-1327,J81-4005,0,0.736734,"Missing"
C18-1327,W15-3904,0,0.0717131,"Missing"
C18-1327,W03-0426,0,0.0902503,"and blue circles represent character embeddings, word embeddings, character sequence representations and word sequence representations, respectively. 2 Related Work Collobert et al. (2011) proposed a seminal neural architecture for sequence labeling. It captures word sequence information with a one-layer CNN based on pretrained word embeddings and handcrafted neural features, followed with a CRF output layer. dos Santos et al. (2015) extended this model by integrating character-level CNN features. Strubell et al. (2017) built a deeper dilated CNN architecture to capture larger local features. Hammerton (2003) was the first to exploit LSTM for sequence labeling. Huang et al. (2015) built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (Lample et al., 2016; Liu et al., 2018), GRU (Yang et al., 2016), and CNN (Chiu and Nichols, 2016; Ma and Hovy, 2016) features. Yang et al. (2017a) proposed a neural reranking model to improve NER models. These models achieve state-of-the-art results in the literature. Reimers and Gurevych (2017b) compared several word-based LSTM models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than"
C18-1327,D17-1206,0,0.0615077,"2016; Lample et al., 2016; Strubell et al., 2017) report results on the NER dataset only. dos Santos et al. (2015) conducts experiments on NER for Portuguese and Spanish. Most work uses the development set to select hyperparameters (Lample et al., 2016; Ma and Hovy, 2016), while others add development set into training set (Chiu and Nichols, 2016; Peters et al., 2017). Reimers and Gurevych (2017b) use a smaller dataset (13862 vs 14987 sentences). Different from Ma and Hovy (2016) and Liu et al. (2018), Huang et al. (2015) choose a different data split on the POS dataset. Liu et al. (2018) and Hashimoto et al. (2017) use different development sets for chunking. • Preprocessing. A typical data preprocessing step is to normize digit characters (Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Strubell et al., 2017). Reimers and Gurevych (2017b) use fine-grained representations for less frequent words. Ma and Hovy (2016) do not use preprocessing. • Features. Strubell et al. (2017) and Chiu and Nichols (2016) apply word spelling features and Huang et al. (2015) further integrate context features. Collobert et al. (2011) and Huang et al. (2015) use neural features to represent external gazetteer"
C18-1327,N16-1030,0,0.104885,"ocess, we reach several practical conclusions which can be useful to practitioners. 1 Introduction Sequence labeling models have been used for fundamental NLP tasks such as POS tagging, chunking and named entity recognition (NER). Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015) with handcrafted features and task-specific resources. With advances in deep learning, neural models have given state-of-the-art results on many sequence labeling tasks (Ling et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). Words and characters are encoded in distributed representations (Mikolov et al., 2013) and sentence-level features are learned automatically during end-to-end training. Many existing state-of-the-art neural sequence labeling models utilize word-level Long Short-Term Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). As an alternative, Convolution Neural Network (CNN) (LeCun et al., 1989) has also been used for"
C18-1327,D15-1176,0,0.298236,"son and analysis process, we reach several practical conclusions which can be useful to practitioners. 1 Introduction Sequence labeling models have been used for fundamental NLP tasks such as POS tagging, chunking and named entity recognition (NER). Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015) with handcrafted features and task-specific resources. With advances in deep learning, neural models have given state-of-the-art results on many sequence labeling tasks (Ling et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). Words and characters are encoded in distributed representations (Mikolov et al., 2013) and sentence-level features are learned automatically during end-to-end training. Many existing state-of-the-art neural sequence labeling models utilize word-level Long Short-Term Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). As an alternative, Convolution Neural Network (CNN) (LeCun et al., 1989) h"
C18-1327,D15-1104,0,0.0387964,"enchmarks (i.e. NER, Chunking, and POS tagging). Misconceptions and inconsistent conclusions in existing literature are examined and clarified under statistical experiments. In the comparison and analysis process, we reach several practical conclusions which can be useful to practitioners. 1 Introduction Sequence labeling models have been used for fundamental NLP tasks such as POS tagging, chunking and named entity recognition (NER). Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015) with handcrafted features and task-specific resources. With advances in deep learning, neural models have given state-of-the-art results on many sequence labeling tasks (Ling et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). Words and characters are encoded in distributed representations (Mikolov et al., 2013) and sentence-level features are learned automatically during end-to-end training. Many existing state-of-the-art neural sequence labeling models utilize word-level Long Short-Term Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependenc"
C18-1327,P16-1101,0,0.0677124,"al practical conclusions which can be useful to practitioners. 1 Introduction Sequence labeling models have been used for fundamental NLP tasks such as POS tagging, chunking and named entity recognition (NER). Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015) with handcrafted features and task-specific resources. With advances in deep learning, neural models have given state-of-the-art results on many sequence labeling tasks (Ling et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). Words and characters are encoded in distributed representations (Mikolov et al., 2013) and sentence-level features are learned automatically during end-to-end training. Many existing state-of-the-art neural sequence labeling models utilize word-level Long Short-Term Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). As an alternative, Convolution Neural Network (CNN) (LeCun et al., 1989) has also been used for its ability of para"
C18-1327,J93-2004,0,0.0645412,"rk observes that stochastic gradient descent (SGD) gives best performance on NER task (Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016), while Reimers and Gurevych (2017b) report that SGD is the worst optimizer on the same datasets. The comparison between different deep neural models is challenging due to sensitivity on experimental settings. We list six inconsistent configurations in literature, which lead to difficulties for fair comparison. • Dataset. Most work reports sequence labeling results on both CoNLL 2003 English NER (Tjong Kim Sang and De Meulder, 2003) and PTB POS (Marcus et al., 1993) datasets (Collobert et al., 2011; Huang et al., 2015; Ma and Hovy, 2016). Ling et al. (2015) give results only on POS dataset, while some This work is licenced under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 3879 Proceedings of the 27th International Conference on Computational Linguistics, pages 3879–3889 Santa Fe, New Mexico, USA, August 20-26, 2018. Models No Char Char LSTM Char CNN Word LSTM+CRF Huang et al. (2015)* Lample et al. (2016) Strubell et al. (2017)* Lample et al. (2016) Rei (2017) Liu et al. (2018) Ma"
C18-1327,W14-1609,0,0.0177832,"comparison on three benchmarks (i.e. NER, Chunking, and POS tagging). Misconceptions and inconsistent conclusions in existing literature are examined and clarified under statistical experiments. In the comparison and analysis process, we reach several practical conclusions which can be useful to practitioners. 1 Introduction Sequence labeling models have been used for fundamental NLP tasks such as POS tagging, chunking and named entity recognition (NER). Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015) with handcrafted features and task-specific resources. With advances in deep learning, neural models have given state-of-the-art results on many sequence labeling tasks (Ling et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). Words and characters are encoded in distributed representations (Mikolov et al., 2013) and sentence-level features are learned automatically during end-to-end training. Many existing state-of-the-art neural sequence labeling models utilize word-level Long Short-Term Memory (LSTM) structures to represent global sequence information and a CRF layer t"
C18-1327,D14-1162,0,0.0783559,"2016; Liu et al., 2018), we adopt the standard splits by using sections 0-18 as training set, sections 19-21 as development set and sections 22-24 as test set. No preprocessing is performed on either dataset except for normalizing digits. The dataset statistics are listed in Table 2. Hyperparameters. Table 3 shows the hyperparameters used in our experiments, which mostly follow Ma and Hovy (2016), including the learning rate η = 0.015 for word LSTM models. For word CNN based models, a large η leads to convergence problem. We take η = 0.005 with more epochs (200) instead. GloVe 100-dimension (Pennington et al., 2014) is used to initialize word embeddings and character embeddings are randomly initialized. We use mini-batch stochastic gradient descent (SGD) with a decayed learning rate to update parameters. For NER and chunking, we the BIOES tag scheme. Evaluation. Standard precision (P), recall (R) and F1-score (F) are used as the evaluation metrics for NER and chunking; token accuracy is used to evaluate the performance of POS tagger. Development datasets are used to select the optimal model among all epochs, and we report scores of the selected model on the test dataset. To reduce the volatility of the s"
C18-1327,P17-1161,0,0.141902,"al models have given state-of-the-art results on many sequence labeling tasks (Ling et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). Words and characters are encoded in distributed representations (Mikolov et al., 2013) and sentence-level features are learned automatically during end-to-end training. Many existing state-of-the-art neural sequence labeling models utilize word-level Long Short-Term Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). As an alternative, Convolution Neural Network (CNN) (LeCun et al., 1989) has also been used for its ability of parallel computing, leading to an efficient training and decoding process. Despite them being dominant in the research literature, reproducing published results for neural models can be challenging, even if the codes are available open source. For example, Reimers and Gurevych (2017b) conduct a large number of experiments using the code of Ma and Hovy (2016), but cannot obtain comparable results as reported in the paper. Liu et al. (2018) report lower average F-scores on NER when re"
C18-1327,W09-1119,0,0.931582,"duct a systematic model comparison on three benchmarks (i.e. NER, Chunking, and POS tagging). Misconceptions and inconsistent conclusions in existing literature are examined and clarified under statistical experiments. In the comparison and analysis process, we reach several practical conclusions which can be useful to practitioners. 1 Introduction Sequence labeling models have been used for fundamental NLP tasks such as POS tagging, chunking and named entity recognition (NER). Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015) with handcrafted features and task-specific resources. With advances in deep learning, neural models have given state-of-the-art results on many sequence labeling tasks (Ling et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). Words and characters are encoded in distributed representations (Mikolov et al., 2013) and sentence-level features are learned automatically during end-to-end training. Many existing state-of-the-art neural sequence labeling models utilize word-level Long Short-Term Memory (LSTM) structures to represent global sequence informat"
C18-1327,P17-1194,0,0.152732,"d PTB POS (Marcus et al., 1993) datasets (Collobert et al., 2011; Huang et al., 2015; Ma and Hovy, 2016). Ling et al. (2015) give results only on POS dataset, while some This work is licenced under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 3879 Proceedings of the 27th International Conference on Computational Linguistics, pages 3879–3889 Santa Fe, New Mexico, USA, August 20-26, 2018. Models No Char Char LSTM Char CNN Word LSTM+CRF Huang et al. (2015)* Lample et al. (2016) Strubell et al. (2017)* Lample et al. (2016) Rei (2017) Liu et al. (2018) Ma and Hovy (2016) Chiu and Nichols (2016)* Peters et al. (2017) Word LSTM Ma and Hovy (2016) Strubell et al. (2017)* Word CNN Strubell et al. (2017)* Lample et al. (2016) Word CNN+CRF Collobert et al. (2011)* dos Santos et al. (2015) Strubell et al. (2017)* No existing work Ma and Hovy (2016) dos Santos et al. (2015) Santos and Zadrozny (2014) No existing work Table 1: Neural sequence labeling models in literatures. * represents using handcrafted neural features. papers (Chiu and Nichols, 2016; Lample et al., 2016; Strubell et al., 2017) report results on the NER dataset on"
C18-1327,D17-1035,0,0.0607435,"Long Short-Term Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). As an alternative, Convolution Neural Network (CNN) (LeCun et al., 1989) has also been used for its ability of parallel computing, leading to an efficient training and decoding process. Despite them being dominant in the research literature, reproducing published results for neural models can be challenging, even if the codes are available open source. For example, Reimers and Gurevych (2017b) conduct a large number of experiments using the code of Ma and Hovy (2016), but cannot obtain comparable results as reported in the paper. Liu et al. (2018) report lower average F-scores on NER when reproducing the structure of Lample et al. (2016), and on POS tagging when reproducing Ma and Hovy (2016). Most literature compares results with others by citing the scores directly (Huang et al., 2015; Lample et al., 2016) without re-implementing them under the same setting, resulting in less persuasiveness on the advantage of their models. In addition, conclusions from different reports can be"
C18-1327,D17-1283,0,0.0999257,"NER (Tjong Kim Sang and De Meulder, 2003) and PTB POS (Marcus et al., 1993) datasets (Collobert et al., 2011; Huang et al., 2015; Ma and Hovy, 2016). Ling et al. (2015) give results only on POS dataset, while some This work is licenced under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 3879 Proceedings of the 27th International Conference on Computational Linguistics, pages 3879–3889 Santa Fe, New Mexico, USA, August 20-26, 2018. Models No Char Char LSTM Char CNN Word LSTM+CRF Huang et al. (2015)* Lample et al. (2016) Strubell et al. (2017)* Lample et al. (2016) Rei (2017) Liu et al. (2018) Ma and Hovy (2016) Chiu and Nichols (2016)* Peters et al. (2017) Word LSTM Ma and Hovy (2016) Strubell et al. (2017)* Word CNN Strubell et al. (2017)* Lample et al. (2016) Word CNN+CRF Collobert et al. (2011)* dos Santos et al. (2015) Strubell et al. (2017)* No existing work Ma and Hovy (2016) dos Santos et al. (2015) Santos and Zadrozny (2014) No existing work Table 1: Neural sequence labeling models in literatures. * represents using handcrafted neural features. papers (Chiu and Nichols, 2016; Lample et al., 2016; Strubell et al., 2017) rep"
C18-1327,W00-0726,0,0.885013,"ers et al., 2017; Liu et al., 2018). Others report the best result among different trials (Ma and Hovy, 2016), which cannot be compared directly. • Hardware environment can also affect system accuracy. Liu et al. (2018) observes that the system gives better accuracy on NER task when trained using GPU as compared to using CPU. Besides, the running speeds are highly affected by the hardware environment. To address the above concerns, we systematically analyze neural sequence labeling models on three benchmarks: CoNLL 2003 NER (Tjong Kim Sang and De Meulder, 2003), CoNLL 2000 chunking (Tjong Kim Sang and Buchholz, 2000) and PTB POS tagging (Marcus et al., 1993). Table 1 shows a summary of the models we investigate, which can be categorized under three settings: (i) character sequence representations ; (ii) word sequence representations; (iii) inference layer. Although various combinations of these three settings have been proposed in the literature, others have not been examined. We compare all models in Table 1, which includes most state-of-the-art methods. To make fair comparisons, we build a unified framework1 to reproduce the twelve neural sequence labeling architectures in Table 1. Experiments show that"
C18-1327,W03-0419,0,0.890406,"Missing"
C18-1327,N03-1033,0,0.0138934,"cabulary (OOV) entities/chunks/words. 4.1 Settings Data. The NER dataset has been standardly split in Tjong Kim Sang and De Meulder (2003). It contains four named entity types: P ERSON , L OCATION , O RGANIZATION , and M ISC. The chunking task is evaluated on CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000). We follow Reimers and Gurevych (2017a) by using sections 15-18 of Wall Street Journal (WSJ) for training, section 19 as development set and section 20 as test set. For the POS tagging task, we use the WSJ portion of Peen Treebank, which has 45 POS tags. Following previous works (Toutanova et al., 2003; Santos and Zadrozny, 2014; Ma and Hovy, 2016; Liu et al., 2018), we adopt the standard splits by using sections 0-18 as training set, sections 19-21 as development set and sections 22-24 as test set. No preprocessing is performed on either dataset except for normalizing digits. The dataset statistics are listed in Table 2. Hyperparameters. Table 3 shows the hyperparameters used in our experiments, which mostly follow Ma and Hovy (2016), including the learning rate η = 0.015 for word LSTM models. For word CNN based models, a large η leads to convergence problem. We take η = 0.005 with more ep"
C18-1327,yang-etal-2017-neural-reranking,1,0.915653,"er CNN based on pretrained word embeddings and handcrafted neural features, followed with a CRF output layer. dos Santos et al. (2015) extended this model by integrating character-level CNN features. Strubell et al. (2017) built a deeper dilated CNN architecture to capture larger local features. Hammerton (2003) was the first to exploit LSTM for sequence labeling. Huang et al. (2015) built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM (Lample et al., 2016; Liu et al., 2018), GRU (Yang et al., 2016), and CNN (Chiu and Nichols, 2016; Ma and Hovy, 2016) features. Yang et al. (2017a) proposed a neural reranking model to improve NER models. These models achieve state-of-the-art results in the literature. Reimers and Gurevych (2017b) compared several word-based LSTM models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value. They investigated the influence of various hyperparameters and configurations. Our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects: 1) Their experiments are based on a BiLSTM with handcrafted word features, while our expe"
D08-1059,W06-2920,0,0.344212,"use an approximate decoder while a transition-based parser is not necessarily deterministic. To make the concepts clear, we classify the two types of parser by the following two criteria: 1. whether or not the outputs are built by explicit transition-actions, such as ”Shift” and ”Reduce”; 2. whether it is dependency graphs or transitionactions that the parsing model assigns scores to. By this classification, beam-search can be applied to both graph-based and transition-based parsers. Representative of each method, MSTParser and MaltParser gave comparable accuracies in the CoNLL-X shared task (Buchholz and Marsi, 2006). However, they make different types of errors, which can be seen as a reflection of their theoretical differences (McDonald and Nivre, 2007). MSTParser has the strength of exact inference, but its choice of features is constrained by the requirement of efficient dynamic programming. MaltParser is deterministic, yet its comparatively larger feature range is an advantage. By comparing the two, three interesting research questions arise: (1) how to increase the flexibility in defining features for graph-based parsing; (2) how to add search to transition-based parsing; and (3) how to combine the"
D08-1059,W06-2925,0,0.0306015,"Missing"
D08-1059,P04-1015,0,0.708526,"s, the agenda contains an empty sentence. At each processing stage, existing partial candidates from the agenda are extended in all possible ways according to the Covington algorithm. The top B newly generated candidates are then put to the agenda. After all input words are processed, the best candidate output from the agenda is taken as the final output. The projectivity of the output dependency trees is guaranteed by the incremental Covington process. The time complexity of this algorithm is O(n2 ), where n is the length of the input sentence. During training, the “early update” strategy of Collins and Roark (2004) is used: when the correct state item falls out of the beam at any stage, parsing is stopped immediately, and the model is updated using the current best partial item. The intuition is to improve learning by avoiding irrelevant information: when all the items in the current agenda are incorrect, further parsing steps will be irrelevant because the correct partial output no longer exists in the candidate ranking. Table 1 shows the feature templates from the MSTParser (McDonald and Pereira, 2006), which are defined in terms of the context of a word, its parent and its sibling. To give more templ"
D08-1059,W02-1001,0,0.921413,"(x) = arg max Score(y) y∈GEN(x) where GEN(x) denotes the set of possible parses for the input x. To repeat our earlier comments, in this paper we do not consider the method of finding the arg max to be part of the definition of graph-based parsing, only the fact that the dependency graph itself is being scored, and factored into scores attached to the dependency links. The score of an output parse y is given by a linear model: Score(y) = Φ(y) · w ~ where Φ(y) is the global feature vector from y and w ~ is the weight vector of the model. We use the discriminative perceptron learning algorithm (Collins, 2002; McDonald et al., 2005) to train the values of w. ~ The algorithm is shown in Figure 1. Averaging parameters is a way to reduce overfitting for perceptron training (Collins, 2002), and is applied to all our experiments. While the MSTParser uses exact-inference (Eisner, 1996), we apply beam-search to decoding. This is done by extending the deterministic Covington algorithm for projective dependency parsing (Covington, 2001). As shown in Figure 2, the decoder works incrementally, building a state item (i.e. partial parse tree) word by word. When each word is processed, links are added between t"
D08-1059,C96-1058,0,0.736463,"ndency graph itself is being scored, and factored into scores attached to the dependency links. The score of an output parse y is given by a linear model: Score(y) = Φ(y) · w ~ where Φ(y) is the global feature vector from y and w ~ is the weight vector of the model. We use the discriminative perceptron learning algorithm (Collins, 2002; McDonald et al., 2005) to train the values of w. ~ The algorithm is shown in Figure 1. Averaging parameters is a way to reduce overfitting for perceptron training (Collins, 2002), and is applied to all our experiments. While the MSTParser uses exact-inference (Eisner, 1996), we apply beam-search to decoding. This is done by extending the deterministic Covington algorithm for projective dependency parsing (Covington, 2001). As shown in Figure 2, the decoder works incrementally, building a state item (i.e. partial parse tree) word by word. When each word is processed, links are added between the current word and its predecessors. Beam-search is applied by keeping the B best items in the agenda at each processing stage, while partial candidates are compared by scores from the graph-based model, according to partial graph up to the current word. Before decoding star"
D08-1059,D07-1097,0,0.037297,"approximated large-margin algorithm. This model is the most similar to our transition-based model, while the differences include the choice of learning and decoding algorithms, the definition of feature templates and our application of the “early update” strategy. Our combined parser makes the biggest contribution of this paper. In contrast to the models above, it includes both graph-based and transition-based components. An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007). A more recent approach (Nivre and McDonald, 2008) combined MSTParser and MaltParser by using the output of one parser for features in the other. Both Hall et al. (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models. In contrast, our parser combines two components in a single model, in which all parameters are trained consistently. 7 the combined parser was highly competitive compared to the best systems in the literature. The idea of combining different approaches to the same problem using beam-search and a global model could be applied to other pa"
D08-1059,P07-1050,0,0.0153262,"uracy on the data. Our observations on parsing Chinese are essentially the same as for English. Our combined parser outperforms both the pure graph-based and the pure transition-based parsers. It gave the best accuracy we are aware of for dependency parsing using CTB. 6 Related work Our graph-based parser is derived from the work of McDonald and Pereira (2006). Instead of performing exact inference by dynamic programming, we incorporated the linear model and feature templates from McDonald and Pereira (2006) into our beam-search framework, while adding new global features. Nakagawa (2007) and Hall (2007) also showed the effectiveness of global features in improving the accuracy of graph-based parsing, using the approximate Gibbs sampling method and a reranking approach, respectively. Our transition-based parser is derived from the deterministic parser of Nivre et al. (2006). We incorporated the transition process into our beamsearch framework, in order to study the influence of search on this algorithm. Existing efforts to add search to deterministic parsing include Sagae and Lavie (2006b), which applied best-first search to constituent parsing, and Johansson and Nugues (2006) and Duan et al."
D08-1059,W06-2930,0,0.0455436,"l features. Nakagawa (2007) and Hall (2007) also showed the effectiveness of global features in improving the accuracy of graph-based parsing, using the approximate Gibbs sampling method and a reranking approach, respectively. Our transition-based parser is derived from the deterministic parser of Nivre et al. (2006). We incorporated the transition process into our beamsearch framework, in order to study the influence of search on this algorithm. Existing efforts to add search to deterministic parsing include Sagae and Lavie (2006b), which applied best-first search to constituent parsing, and Johansson and Nugues (2006) and Duan et al. (2007), which applied beamsearch to dependency parsing. All three methods estimate the probability of each transition action, and score a state item by the product of the probabilities of all its corresponding actions. But different from our transition-based parser, which trains all transitions for a parse globally, these models train the probability of each action separately. Based on the work of Johansson and Nugues (2006), Johansson and Nugues (2007) studied global training with an approximated large-margin algorithm. This model is the most similar to our transition-based m"
D08-1059,D07-1123,0,0.0831456,"search to deterministic parsing include Sagae and Lavie (2006b), which applied best-first search to constituent parsing, and Johansson and Nugues (2006) and Duan et al. (2007), which applied beamsearch to dependency parsing. All three methods estimate the probability of each transition action, and score a state item by the product of the probabilities of all its corresponding actions. But different from our transition-based parser, which trains all transitions for a parse globally, these models train the probability of each action separately. Based on the work of Johansson and Nugues (2006), Johansson and Nugues (2007) studied global training with an approximated large-margin algorithm. This model is the most similar to our transition-based model, while the differences include the choice of learning and decoding algorithms, the definition of feature templates and our application of the “early update” strategy. Our combined parser makes the biggest contribution of this paper. In contrast to the models above, it includes both graph-based and transition-based components. An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful"
D08-1059,N03-1017,0,0.00622671,"t its choice of features is constrained by the requirement of efficient dynamic programming. MaltParser is deterministic, yet its comparatively larger feature range is an advantage. By comparing the two, three interesting research questions arise: (1) how to increase the flexibility in defining features for graph-based parsing; (2) how to add search to transition-based parsing; and (3) how to combine the two parsing approaches so that the strengths of each are utilized. In this paper, we study these questions under one framework: beam-search. Beam-search has been successful in many NLP tasks (Koehn et al., 2003; 562 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 562–571, c Honolulu, October 2008. 2008 Association for Computational Linguistics Inputs: training examples (xi , yi ) Initialization: set w ~ =0 Algorithm: // R training iterations; N examples for t = 1..R, i = 1..N : zi = arg maxy∈GEN(xi ) Φ(y) · w ~ if zi 6= yi : w ~ =w ~ + Φ(yi ) − Φ(zi ) Outputs: w ~ Figure 1: The perceptron learning algorithm Collins and Roark, 2004), and can achieve accuracy that is close to exact inference. Moreover, a beamsearch decoder does not impose restrictions on t"
D08-1059,P08-1068,0,0.276446,"Missing"
D08-1059,D07-1013,0,0.278812,"g that, in contrast to MaltParser, which trains each action decision individually, our training algorithm globally optimizes all action decisions for a parse. Again, “early update” and averaging parameters are applied to the training process. 4 Training Dev Test P 0 T 0 ∈act(y) Score(T , sT 0 ) P 0 ~T T 0 ∈act(y) Φ(T , sT 0 ) · w = w~T · P T 0 ∈act(y) Φ(T Words 950,028 40,117 56,684 PTB We therefore combine the two models to give: ScoreC (y) = ScoreG (y) + ScoreT (y) = ΦG (y) · w~G + ΦT (y) · w~T The graph-based and transition-based approaches adopt very different views of dependency parsing. McDonald and Nivre (2007) showed that the MSTParser and MaltParser produce different errors. This observation suggests a combined approach: by using both graph-based information and transition-based information, parsing accuracy can be improved. The beam-search framework we have developed facilitates such a combination. Our graph-based and transition-based parsers share many similarities. Both build a parse tree incrementally, keeping an agenda of comparable state items. Both rank state items by their current scores, and use the averaged perceptron with early update for training. The key differences are the scoring mo"
D08-1059,E06-1011,0,0.909394,"is O(n2 ), where n is the length of the input sentence. During training, the “early update” strategy of Collins and Roark (2004) is used: when the correct state item falls out of the beam at any stage, parsing is stopped immediately, and the model is updated using the current best partial item. The intuition is to improve learning by avoiding irrelevant information: when all the items in the current agenda are incorrect, further parsing steps will be irrelevant because the correct partial output no longer exists in the candidate ranking. Table 1 shows the feature templates from the MSTParser (McDonald and Pereira, 2006), which are defined in terms of the context of a word, its parent and its sibling. To give more templates, features from templates 1 – 5 are also conjoined with 564 1 2 3 Parent word (P) Child word (C) 4 A tag Bt between P, C Neighbour words of P, C, left (PL/CL) and right (PR/CR) P and C 5 sibling (S) of C 6 Pw; Pt; Pwt Cw; Ct; Cwt PwtCwt; PwtCw; PwCwt; PwtCt; PtCwt; PwCw; PtCt PtBtCt PtPLtCtCLt; PtPLtCtCRt; PtPRtCtCLt; PtPRtCtCRt; PtPLtCLt; PtPLtCRt; PtPRtCLt; PtPRtCRt; PLtCtCLt; PLtCtCRt; PRtCtCLt; PRtCtCRt; PtCtCLt; PtCtCRt; PtPLtCt; PtPRtCt CwSw; CtSt; CwSt; CtSw; PtCtSt; Table 1: Feature"
D08-1059,P05-1012,0,0.976325,"core(y) y∈GEN(x) where GEN(x) denotes the set of possible parses for the input x. To repeat our earlier comments, in this paper we do not consider the method of finding the arg max to be part of the definition of graph-based parsing, only the fact that the dependency graph itself is being scored, and factored into scores attached to the dependency links. The score of an output parse y is given by a linear model: Score(y) = Φ(y) · w ~ where Φ(y) is the global feature vector from y and w ~ is the weight vector of the model. We use the discriminative perceptron learning algorithm (Collins, 2002; McDonald et al., 2005) to train the values of w. ~ The algorithm is shown in Figure 1. Averaging parameters is a way to reduce overfitting for perceptron training (Collins, 2002), and is applied to all our experiments. While the MSTParser uses exact-inference (Eisner, 1996), we apply beam-search to decoding. This is done by extending the deterministic Covington algorithm for projective dependency parsing (Covington, 2001). As shown in Figure 2, the decoder works incrementally, building a state item (i.e. partial parse tree) word by word. When each word is processed, links are added between the current word and its"
D08-1059,D07-1100,0,0.0320952,"he previous best accuracy on the data. Our observations on parsing Chinese are essentially the same as for English. Our combined parser outperforms both the pure graph-based and the pure transition-based parsers. It gave the best accuracy we are aware of for dependency parsing using CTB. 6 Related work Our graph-based parser is derived from the work of McDonald and Pereira (2006). Instead of performing exact inference by dynamic programming, we incorporated the linear model and feature templates from McDonald and Pereira (2006) into our beam-search framework, while adding new global features. Nakagawa (2007) and Hall (2007) also showed the effectiveness of global features in improving the accuracy of graph-based parsing, using the approximate Gibbs sampling method and a reranking approach, respectively. Our transition-based parser is derived from the deterministic parser of Nivre et al. (2006). We incorporated the transition process into our beamsearch framework, in order to study the influence of search on this algorithm. Existing efforts to add search to deterministic parsing include Sagae and Lavie (2006b), which applied best-first search to constituent parsing, and Johansson and Nugues (2006)"
D08-1059,P08-1108,0,0.177125,"del is the most similar to our transition-based model, while the differences include the choice of learning and decoding algorithms, the definition of feature templates and our application of the “early update” strategy. Our combined parser makes the biggest contribution of this paper. In contrast to the models above, it includes both graph-based and transition-based components. An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007). A more recent approach (Nivre and McDonald, 2008) combined MSTParser and MaltParser by using the output of one parser for features in the other. Both Hall et al. (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models. In contrast, our parser combines two components in a single model, in which all parameters are trained consistently. 7 the combined parser was highly competitive compared to the best systems in the literature. The idea of combining different approaches to the same problem using beam-search and a global model could be applied to other parsing tasks, such as constituent parsing, and possi"
D08-1059,W06-2933,0,0.042082,"Missing"
D08-1059,N06-2033,0,0.0971182,"om McDonald and Pereira (2006) into our beam-search framework, while adding new global features. Nakagawa (2007) and Hall (2007) also showed the effectiveness of global features in improving the accuracy of graph-based parsing, using the approximate Gibbs sampling method and a reranking approach, respectively. Our transition-based parser is derived from the deterministic parser of Nivre et al. (2006). We incorporated the transition process into our beamsearch framework, in order to study the influence of search on this algorithm. Existing efforts to add search to deterministic parsing include Sagae and Lavie (2006b), which applied best-first search to constituent parsing, and Johansson and Nugues (2006) and Duan et al. (2007), which applied beamsearch to dependency parsing. All three methods estimate the probability of each transition action, and score a state item by the product of the probabilities of all its corresponding actions. But different from our transition-based parser, which trains all transitions for a parse globally, these models train the probability of each action separately. Based on the work of Johansson and Nugues (2006), Johansson and Nugues (2007) studied global training with an ap"
D08-1059,P06-2089,0,0.0111382,"om McDonald and Pereira (2006) into our beam-search framework, while adding new global features. Nakagawa (2007) and Hall (2007) also showed the effectiveness of global features in improving the accuracy of graph-based parsing, using the approximate Gibbs sampling method and a reranking approach, respectively. Our transition-based parser is derived from the deterministic parser of Nivre et al. (2006). We incorporated the transition process into our beamsearch framework, in order to study the influence of search on this algorithm. Existing efforts to add search to deterministic parsing include Sagae and Lavie (2006b), which applied best-first search to constituent parsing, and Johansson and Nugues (2006) and Duan et al. (2007), which applied beamsearch to dependency parsing. All three methods estimate the probability of each transition action, and score a state item by the product of the probabilities of all its corresponding actions. But different from our transition-based parser, which trains all transitions for a parse globally, these models train the probability of each action separately. Based on the work of Johansson and Nugues (2006), Johansson and Nugues (2007) studied global training with an ap"
D08-1059,N04-1032,0,0.0174794,"Missing"
D08-1059,W03-3023,0,0.901952,"wing McDonald et al. (2005). Bracketed sentences from the Penn Treebank (PTB) 3 are split into training, development and test sets MSTParser 1 Graph [M] Transition Graph [MA] MSTParser 2 Combined [TM] Combined [TMA] Word 90.7 91.2 91.4 91.4 91.5 92.0 92.1 Complete 36.7 40.8 41.8 42.5 42.1 45.0 45.4 Table 5: Accuracy comparisons using PTB 3 Figure 6: The influence of beam size on the transitionbased parser, using the development data X-axis: number of training iterations Y-axis: word precision as shown in Table 4, and then translated into dependency structures using the head-finding rules from Yamada and Matsumoto (2003). Before parsing, POS tags are assigned to the input sentence using our reimplementation of the POStagger from Collins (2002). Like McDonald et al. (2005), we evaluate the parsing accuracy by the precision of lexical heads (the percentage of input words, excluding punctuation, that have been assigned the correct parent) and by the percentage of complete matches, in which all words excluding punctuation have been assigned the correct parent. 5.1 Development experiments Since the beam size affects all three parsers, we study its influence first; here we show the effect on the transition-based pa"
D08-1059,D07-1101,0,\N,Missing
D10-1082,N06-1022,0,0.0154052,"of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. In this paper we follow the line of single-model research, in particular the global linear model of Z&C08. We show that effective decoding can be achieved with standard beam-search, which gives significant speed improvements compared to the decoding algorithm of Z&C08, and achieves accuracies that are competitive with the state-of-the-art. Our research is also in line with recent research on improving the speed of NLP systems with little or no accuracy loss (Charniak et al., 2006; Roark and Hollingshead, 2008). Our speed improvement is achieved by the use of a single-beam decoder. Given an input sentence, candidate outputs are built incrementally, one character at a time. When each character is processed, it is combined with existing candidates in all possible ways to generate new candidates, and an agenda is used to keep the N -best candidate outputs from 843 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 843–852, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics the begining of"
D10-1082,P04-1015,0,0.768721,"word. Before a word is confirmed as a full word, we only apply to it features that represent its current partial status, such as character bigrams, its starting character and its part-ofspeech, etc. Full word features, including the first and last characters of a word, are applied immediately after a word is confirmed as complete. An important component for our proposed system is the training process, which needs to ensure that the model scores a partial word with predicted POS properly. We use the averaged perceptron (Collins, 2002) for training, together with the “early update” mechanism of Collins and Roark (2004). Rather than updating the parameters after decoding is complete, the modified algorithm updates parameters at any processing step if the correct partial candidate falls out of the beam. In our experiments using the Chinese Treebank 1 The next incoming characters are also a useful source of information for predicting the POS. However, our system achieved competitive accuracy with Z&C08 without such character lookahead features. 845 data, our system ran an order of magnitude faster than our Z&C08 system with little loss of accuracy. The accuracy of our system was competitive with other recent m"
D10-1082,W02-1001,0,0.816261,"l the next incoming character is separated from the last character of this word. Before a word is confirmed as a full word, we only apply to it features that represent its current partial status, such as character bigrams, its starting character and its part-ofspeech, etc. Full word features, including the first and last characters of a word, are applied immediately after a word is confirmed as complete. An important component for our proposed system is the training process, which needs to ensure that the model scores a partial word with predicted POS properly. We use the averaged perceptron (Collins, 2002) for training, together with the “early update” mechanism of Collins and Roark (2004). Rather than updating the parameters after decoding is complete, the modified algorithm updates parameters at any processing step if the correct partial candidate falls out of the beam. In our experiments using the Chinese Treebank 1 The next incoming characters are also a useful source of information for predicting the POS. However, our system achieved competitive accuracy with Z&C08 without such character lookahead features. 845 data, our system ran an order of magnitude faster than our Z&C08 system with li"
D10-1082,P08-1102,0,0.66486,"Missing"
D10-1082,C08-1049,0,0.762106,"t partial candidate as well as a full output. Effective scoring of partial structures allows the decoder to give high accuracy with a small beam-size of 16. In our 10-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature. 1 Introduction and Motivation Several approaches have been proposed to solve word segmentation and POS-tagging jointly, including the reranking approach (Shi and Wang, 2007; Jiang et al., 2008b), the hybrid approach (Nakagawa and Uchimoto, 2007; Jiang et al., 2008a), and the single-model approach (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009). These methods led to accuracy improvements over the traditional, pipelined segmentation and POS-tagging baseline by avoiding segmentation error propagation and making use of part-of-speech information to improve segmentation. The single-model approach to joint segmentation and POS-tagging offers consistent training of all information, concerning words, characters and partsof-speech. However, exact inference with dynamic pr"
D10-1082,P09-1058,0,0.765266,"-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature. 1 Introduction and Motivation Several approaches have been proposed to solve word segmentation and POS-tagging jointly, including the reranking approach (Shi and Wang, 2007; Jiang et al., 2008b), the hybrid approach (Nakagawa and Uchimoto, 2007; Jiang et al., 2008a), and the single-model approach (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009). These methods led to accuracy improvements over the traditional, pipelined segmentation and POS-tagging baseline by avoiding segmentation error propagation and making use of part-of-speech information to improve segmentation. The single-model approach to joint segmentation and POS-tagging offers consistent training of all information, concerning words, characters and partsof-speech. However, exact inference with dynamic programming can be infeasible if features are defined over a large enough range of the output, such as over a two-word history. In our previous work (Zhang and Clark, 2008),"
D10-1082,P07-2055,0,0.539688,"t. Effective scoring of partial structures allows the decoder to give high accuracy with a small beam-size of 16. In our 10-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature. 1 Introduction and Motivation Several approaches have been proposed to solve word segmentation and POS-tagging jointly, including the reranking approach (Shi and Wang, 2007; Jiang et al., 2008b), the hybrid approach (Nakagawa and Uchimoto, 2007; Jiang et al., 2008a), and the single-model approach (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009). These methods led to accuracy improvements over the traditional, pipelined segmentation and POS-tagging baseline by avoiding segmentation error propagation and making use of part-of-speech information to improve segmentation. The single-model approach to joint segmentation and POS-tagging offers consistent training of all information, concerning words, characters and partsof-speech. However, exact inference with dynamic programming can be infeasible if features are defined"
D10-1082,W04-3236,0,0.49528,"y with a small beam-size of 16. In our 10-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature. 1 Introduction and Motivation Several approaches have been proposed to solve word segmentation and POS-tagging jointly, including the reranking approach (Shi and Wang, 2007; Jiang et al., 2008b), the hybrid approach (Nakagawa and Uchimoto, 2007; Jiang et al., 2008a), and the single-model approach (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009). These methods led to accuracy improvements over the traditional, pipelined segmentation and POS-tagging baseline by avoiding segmentation error propagation and making use of part-of-speech information to improve segmentation. The single-model approach to joint segmentation and POS-tagging offers consistent training of all information, concerning words, characters and partsof-speech. However, exact inference with dynamic programming can be infeasible if features are defined over a large enough range of the output, such as over a two-word histor"
D10-1082,C08-1094,0,0.0139416,"uctures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. In this paper we follow the line of single-model research, in particular the global linear model of Z&C08. We show that effective decoding can be achieved with standard beam-search, which gives significant speed improvements compared to the decoding algorithm of Z&C08, and achieves accuracies that are competitive with the state-of-the-art. Our research is also in line with recent research on improving the speed of NLP systems with little or no accuracy loss (Charniak et al., 2006; Roark and Hollingshead, 2008). Our speed improvement is achieved by the use of a single-beam decoder. Given an input sentence, candidate outputs are built incrementally, one character at a time. When each character is processed, it is combined with existing candidates in all possible ways to generate new candidates, and an agenda is used to keep the N -best candidate outputs from 843 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 843–852, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics the begining of the sentence to the current ch"
D10-1082,P07-1106,1,0.772654,"ing methods. Kruengkrai et al. (2009) and Zhang and Clark (2008) are the most similar to our system among related work. Both systems use a discriminatively trained linear model to score candidate outputs. The work of Kruengkrai et al. (2009) is based on Nakagawa and Uchimoto (2007), which separates the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters. In contrast, our model is conceptually simpler, and does not differentiate known words and unknown words. Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling. Our learning and decoding algorithms are also different from Kruengkrai et al. (2009). While Kruengkrai et al. (2009) perform dynamic programming and MIRA learning, we use beam-search to perform incremental decoding, and the early-update version of the perceptron algorithm to train the model. Dynamic programming is exact inference, for which the time complexity is decided by the locality of feature templates. In contrast, beam-search is approximate and can run in linear time. The parameter updating for our algorithm is co"
D10-1082,P08-1101,1,0.880027,"rd features so that feature templates can be instantiated incrementally, according to whether the current character is separated or appended; (2) deciding the POS-tag of a potential word when its first character is processed. Early-update is used with perceptron training so that the linear model gives a high score to a correct partial candidate as well as a full output. Effective scoring of partial structures allows the decoder to give high accuracy with a small beam-size of 16. In our 10-fold crossvalidation experiments with the Chinese Treebank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. The accuracy of our system on the standard CTB 5 test was competitive with the best in the literature. 1 Introduction and Motivation Several approaches have been proposed to solve word segmentation and POS-tagging jointly, including the reranking approach (Shi and Wang, 2007; Jiang et al., 2008b), the hybrid approach (Nakagawa and Uchimoto, 2007; Jiang et al., 2008a), and the single-model approach (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009). These methods led to accuracy improvements over the traditional, pipelined segmentation and POS-tagging"
D11-1106,W00-1401,0,0.0252535,"is used to control the volume of accepted hypotheses, so that only a very small portion of the whole search space is explored. The search algorithm is guided by perceptron training, which ensures that the explored path in the search space consists of highly probable hypotheses. This framework of best-first search guided by learning is a general contribution of the paper, which could be applied to problems outside grammaticality improvement. We evaluate our system using the generation task of word-order recovery, which is to recover the original word order of a fully scrambled input sentence (Bangalore et al., 2000; Wan et al., 2009). This problem is an instance of our general task formulation, but without any input constraints, or content word selection (since all input words are used). It is straightforward to use this task to evaluate our system and compare with existing approaches. Our system gave 40.1 BLEU score, higher than the dependency-based system of Wan et al. (2009), for which a BLEU score of 33.7 was reported. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1147–1157, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computationa"
D11-1106,C10-1009,0,0.245301,"Missing"
D11-1106,J98-2004,0,0.0321789,"all that can be constructed via selecting and ordering a subset of words from the input multiset. The complexity of this problem is much higher than a typical parsing problem, since there is an exponential number of word choices for the output sentence, each 1148 with a factorial number of orderings. Moreover, dynamic programming packing for parsers, such as a CKY chart, is not applicable, because of the lack of a fixed word order. We perform approximate search using a bestfirst algorithm. Starting from single words, candidate parses are constructed bottom-up. Similar to a best-first parser (Caraballo and Charniak, 1998), the highest scored hypothesis is expanded first. A hypothesis is expanded by applying CCG unary rules to the hypothesis, or by combining the hypothesis with existing hypotheses using CCG binary rules. We use beam search to control the number of accepted hypotheses, so that the computational complexity of expanding each hypothesis is linear in the size of the beam. Since there is no guarantee that a goal hypothesis will be found in polynomial time, we apply a robustness mechanism (Riezler et al., 2002; White, 2004), and construct a default output when no goal hypothesis is found within a time"
D11-1106,J07-4004,1,0.78207,"based system of Wan et al. (2009), for which a BLEU score of 33.7 was reported. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1147–1157, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics 2 The Grammar Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalized grammar formalism, which associates words with lexical categories. Lexical categories are detailed grammatical labels, typically expressing subcategorisation information. CCG, and parsing with CCG, has been described in detail elsewhere (Clark and Curran, 2007; Hockenmaier, 2003); here we provide only a short description. During CCG parsing, adjacent categories are combined using CCG’s combinatory rules. For example, a verb phrase in English (S NP ) can combine with an NP to its left: NP S NP ⇒ S In addition to binary rule instances, such as the one above, there are also unary rules which operate on a single category in order to change its type. For example, forward type-raising can change a subject NP into a complex category looking to the right for a verb phrase: NP ⇒ S /(S NP ) Following Hockenmaier (2003), we extract the grammar by reading r"
D11-1106,W02-1001,0,0.00347054,"+ lmost POS + rmost POS the binary rule the binary rule + head word rule + head word + non-head word bigrams resulting from combination POS bigrams resulting from combi. word trigrams resulting from combi. POS trigrams resulting from combi. resulting lexical categary trigrams resulting word + POS bigrams resulting POS + word bigrams resulting POS + word + POS trigrams unary rule unary rule + headw Table 1: Feature template definitions. edge is recovered. We use the perceptron (Rosenblatt, 1958) to perform parameter updates. The traditional perceptron has been adapted to structural prediction (Collins, 2002) and search optimization problems (Daum´e III and Marcu, 2005; Shen et al., 2007). Our training algorithm can be viewed as an adaptation of the perceptron to our best-first framework for search efficiency and accuracy. We choose to update parameters as soon as the best edge from the agenda is not a gold-standard edge. The intuition is that all gold edges are forced to be above all non-gold edges on the agenda. This is a strong precondition for parameter updates. An alternative is to update when a gold-standard edge falls off the chart, which corresponds to the precondition for parameter update"
D11-1106,P08-1022,0,0.0177719,"Missing"
D11-1106,N10-1115,0,0.017723,"Missing"
D11-1106,J07-3004,0,0.0988866,"iption. During CCG parsing, adjacent categories are combined using CCG’s combinatory rules. For example, a verb phrase in English (S NP ) can combine with an NP to its left: NP S NP ⇒ S In addition to binary rule instances, such as the one above, there are also unary rules which operate on a single category in order to change its type. For example, forward type-raising can change a subject NP into a complex category looking to the right for a verb phrase: NP ⇒ S /(S NP ) Following Hockenmaier (2003), we extract the grammar by reading rule instances directly from the derivations in CCGbank (Hockenmaier and Steedman, 2007), rather than defining the combinatory rule schema manually as in Clark and Curran (2007). 3 The Search Algorithm The input to the search algorithm is a set of words, each word having a count that specifies the maximum number of times it can appear in the output. Typically, most input words can occur only once in the output. However, punctuation marks and function words can be given a higher count. Depending on the fluency of the base output (e.g. the output of the base SMT system), some constraints can be given to specific input words, limiting their order or identifying them as an atomic phr"
D11-1106,P03-1046,0,0.0347058,"l. (2009), for which a BLEU score of 33.7 was reported. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1147–1157, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics 2 The Grammar Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalized grammar formalism, which associates words with lexical categories. Lexical categories are detailed grammatical labels, typically expressing subcategorisation information. CCG, and parsing with CCG, has been described in detail elsewhere (Clark and Curran, 2007; Hockenmaier, 2003); here we provide only a short description. During CCG parsing, adjacent categories are combined using CCG’s combinatory rules. For example, a verb phrase in English (S NP ) can combine with an NP to its left: NP S NP ⇒ S In addition to binary rule instances, such as the one above, there are also unary rules which operate on a single category in order to change its type. For example, forward type-raising can change a subject NP into a complex category looking to the right for a verb phrase: NP ⇒ S /(S NP ) Following Hockenmaier (2003), we extract the grammar by reading rule instances direct"
D11-1106,2007.mtsummit-ucnlg.1,0,0.202786,"yntax-based algorithm based on CCG. The goal of the search problem is to find an optimal parse tree among all that can be constructed through selection and ordering of the input words. The search problem, which is significantly harder than parsing, is solved by guided learning for best-first search. In a standard word ordering task, our system gives a BLEU score of 40.1, higher than the previous result of 33.7 achieved by a dependency-based system. 1 Introduction Machine-produced text, such as SMT output, often lacks grammaticality and fluency, especially when using n-gram language modelling (Knight, 2007). Recent efforts have been made to improve grammaticality using local language models (Blackwood et al., 2010) and global dependency structures (Wan et al., 2009). We study grammaticality improvement using a syntax-based system. The task is effectively a text-to-text generation problem where the goal is to produce a grammatical sentence from an ungrammatical and fragmentary input. The input can range from a bag-ofwords (Wan et al., 2009) to a fully-ordered sentence (Blackwood et al., 2010). A general form of the problem is to construct a grammatical sentence from a set of un-ordered input word"
D11-1106,J10-4005,0,0.0101328,"ce most input words typically occur once, they can be indexed and represented by a bitvector, which allows a constant time input check. The few multiple-occurrence words are stored in a count array. In the best-first process, edges to be expanded are ordered by their scores, and stored in an agenda. Edges that have been expanded are stored in a chart. There are many ways in which edges could be ordered and compared. Here the chart is organised as a set of beams, each containing a fixed number of edges with a particular size. This is similar to typical decoding algorithms for phrase-based SMT (Koehn, 2010). In each beam, edges are ordered by their scores, and low score edges are pruned. In addition to pruning by the beam, only the highest scored edge is kept among all that share the same signature. 3.2 The Search Process Figure 1 shows pseudocode for the search algorithm. During initialization, the agenda (a) and chart (c) are cleared. All candidate lexical categories are assigned to each input word, and the resulting leaf edges are put onto the agenda. In the main loop, the best edge (e) is popped from the agenda. If e is a goal hypothesis, it is appended to a list of goals (goal), and the loo"
D11-1106,P02-1040,0,0.101893,"as the model. In our experiments, N is chosen according to results on development data. 6 Experiments We use CCGBank (Hockenmaier and Steedman, 2007) for experimental data. CCGbank is the CCG version of the Penn Treebank. Sections 02–21 are used for training, section 00 is used for development and section 23 for the final test. Original sentences from CCGBank are transformed into bags of words, with sequence information removed, and passed to our system as input data. The system outputs are compared to the original sentences for evaluation. Following Wan et al. (2009), we use the BLEU metric (Papineni et al., 2002) for string comparison. Whilst BLEU is not an ideal measure of fluency or grammaticality, being based on n-gram precision, it is currently widely used for automatic evaluation and allows us to compare directly with existing work (Wan et al., 2009). In addition to the surface string, our system also produces the CCG parse given an input bag of words. The quality of the parse tree can reflect both the grammaticality of the surface string and the quality of the trained grammar model. However, there is no direct way to automatically evaluate parse trees since output word choice and order can be di"
D11-1106,P02-1035,0,0.0119772,"words, candidate parses are constructed bottom-up. Similar to a best-first parser (Caraballo and Charniak, 1998), the highest scored hypothesis is expanded first. A hypothesis is expanded by applying CCG unary rules to the hypothesis, or by combining the hypothesis with existing hypotheses using CCG binary rules. We use beam search to control the number of accepted hypotheses, so that the computational complexity of expanding each hypothesis is linear in the size of the beam. Since there is no guarantee that a goal hypothesis will be found in polynomial time, we apply a robustness mechanism (Riezler et al., 2002; White, 2004), and construct a default output when no goal hypothesis is found within a time limit. 3.1 Data Structures Edges are the basic structures that represent hypotheses. Each edge is a CCG constituent, spanning a sequence of words. Similar to partial parses in a typical chart parser, edges have recursive structures. Depending on the number of subedges, edges can be classified into leaf edges, unary edges and binary edges. Leaf edges, which represent input words, are constructed first in the search process. Existing edges are expanded to generate new edges via unary and binary CCG rule"
D11-1106,D08-1052,0,0.14834,"Missing"
D11-1106,P07-1096,0,0.401666,"ad word + non-head word bigrams resulting from combination POS bigrams resulting from combi. word trigrams resulting from combi. POS trigrams resulting from combi. resulting lexical categary trigrams resulting word + POS bigrams resulting POS + word bigrams resulting POS + word + POS trigrams unary rule unary rule + headw Table 1: Feature template definitions. edge is recovered. We use the perceptron (Rosenblatt, 1958) to perform parameter updates. The traditional perceptron has been adapted to structural prediction (Collins, 2002) and search optimization problems (Daum´e III and Marcu, 2005; Shen et al., 2007). Our training algorithm can be viewed as an adaptation of the perceptron to our best-first framework for search efficiency and accuracy. We choose to update parameters as soon as the best edge from the agenda is not a gold-standard edge. The intuition is that all gold edges are forced to be above all non-gold edges on the agenda. This is a strong precondition for parameter updates. An alternative is to update when a gold-standard edge falls off the chart, which corresponds to the precondition for parameter updates of Daum´e III and Marcu (2005). However, due to the complexity of our search ta"
D11-1106,E09-1097,0,0.514386,"dering of the input words. The search problem, which is significantly harder than parsing, is solved by guided learning for best-first search. In a standard word ordering task, our system gives a BLEU score of 40.1, higher than the previous result of 33.7 achieved by a dependency-based system. 1 Introduction Machine-produced text, such as SMT output, often lacks grammaticality and fluency, especially when using n-gram language modelling (Knight, 2007). Recent efforts have been made to improve grammaticality using local language models (Blackwood et al., 2010) and global dependency structures (Wan et al., 2009). We study grammaticality improvement using a syntax-based system. The task is effectively a text-to-text generation problem where the goal is to produce a grammatical sentence from an ungrammatical and fragmentary input. The input can range from a bag-ofwords (Wan et al., 2009) to a fully-ordered sentence (Blackwood et al., 2010). A general form of the problem is to construct a grammatical sentence from a set of un-ordered input words. However, in cases where the base system produces fluent subsequences within the sentence, constraints on the choice and 1147 Stephen Clark University of Cambri"
D11-1106,D09-1043,0,0.217373,"Missing"
D13-1129,P05-1001,0,0.0728574,"peech tags, and dependency trees, and learn the preference of features via adjusting feature weights. ∗ Corresponding author Several methods have been proposed to alleviate this problem by using large amounts of unannotated data, ranging from self-training and co-training (McClosky et al., 2006; Sagae and Tsujii, 2007) to more complex methods that collect statistical information from unannotated sentences and use them as additional features (Koo et al., 2008; Chen et al., 2009). In this paper, we propose an alternative approach to semi-supervised dependency parsing via feature transformation (Ando and Zhang, 2005). More 1303 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1303–1313, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics specifically, we transform base features to a higherlevel space. The base features defined over surface words, part-of-speech tags, and dependency trees are high dimensional and have been explored in the above previous studies. The higher-level features, which we call meta features, are low dimensional, and newly defined in this paper. The key idea behind is that we build connections b"
D13-1129,C10-1011,0,0.194357,"cy Parsing † Wenliang Chen† , Min Zhang†∗, and Yue Zhang‡ School of Computer Science and Technology, Soochow University, China ‡ Singapore University of Technology and Design, Singapore {wlchen, mzhang}@suda.edu.cn yue zhang@sutd.edu.sg Abstract In the supervised learning scenarios, many previous studies explore rich feature representation that leads to significant improvements. McDonald and Pereira (2006) and Carreras (2007) define secondorder features over two adjacent arcs in secondorder graph-based models. Koo and Collins (2010) use third-order features in a third-order graph-based model. Bohnet (2010) considers information of more surrounding words for the graph-based models, while Zhang and Nivre (2011) define a set of rich features including the word valency and the third-order context features for transition-based models. All these models utilize richer and more complex feature representations and achieve better performance than the earlier models that utilize the simpler features (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). However, the richer feature representations result in a high-dimensional feature space. Features in such a space may suffer from the"
D13-1129,W06-2920,0,0.054748,"elp of a large amount of automatically parsed data. The meta features are used together with base features in our final parser. Our studies indicate that our proposed approach is very effective in processing unseen data and features. Experiments on Chinese and English data sets show that the final parser achieves the best-reported accuracy on the Chinese data and comparable accuracy with the best known parsers on the English data. 1 Introduction In recent years, supervised learning models have achieved lots of progress in the dependency parsing task, as can be found in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). The supervised models take annotated data as training data, utilize features defined over surface words, part-of-speech tags, and dependency trees, and learn the preference of features via adjusting feature weights. ∗ Corresponding author Several methods have been proposed to alleviate this problem by using large amounts of unannotated data, ranging from self-training and co-training (McClosky et al., 2006; Sagae and Tsujii, 2007) to more complex methods that collect statistical information from unannotated sentences and use them as additional features (Koo et al., 2008;"
D13-1129,D07-1101,0,0.0389168,"odel scores each subgraph using a linear representation. Then scoring function score(x, g) is, score(x, g) = f(x, g) · w (2) where f(x, g) is a high-dimensional feature vector based on features defined over g and x and w refers to the weights for the features. The maximum spanning tree is the highest scoring tree in Y (Gx ). The task of decoding algorithms in the parsing model for an input sentence x is to find y ∗ , where y ∗ = arg max score(x, y) y∈Y (Gx ) = arg max ∑ score(x, g) y∈Y (Gx ) g∈y = arg max ∑ f(x, g) · w (3) y∈Y (Gx ) g∈y In our system, we use the decoding algorithm proposed by Carreras (2007), which is a secondorder CKY-style algorithm (Eisner, 1996) and feature weights w are learned during training using the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; McDonald et al., 2005). 2.2 Base features Previous studies have defined different sets of features for the graph-based parsing models, such as the first-order features defined in McDonald et al. (2005), the second-order parent-siblings features defined in McDonald and Pereira (2006), and the second-order parent-child-grandchild features defined in Carreras (2007). Bohnet (2010) explorers a richer set of featur"
D13-1129,D09-1060,1,0.941947,"Nivre et al., 2007). The supervised models take annotated data as training data, utilize features defined over surface words, part-of-speech tags, and dependency trees, and learn the preference of features via adjusting feature weights. ∗ Corresponding author Several methods have been proposed to alleviate this problem by using large amounts of unannotated data, ranging from self-training and co-training (McClosky et al., 2006; Sagae and Tsujii, 2007) to more complex methods that collect statistical information from unannotated sentences and use them as additional features (Koo et al., 2008; Chen et al., 2009). In this paper, we propose an alternative approach to semi-supervised dependency parsing via feature transformation (Ando and Zhang, 2005). More 1303 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1303–1313, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics specifically, we transform base features to a higherlevel space. The base features defined over surface words, part-of-speech tags, and dependency trees are high dimensional and have been explored in the above previous studies. The higher-level feat"
D13-1129,P12-1023,1,0.629615,"us systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo10 refers to the thirdorder parser with model1 of Koo and Collins (2010), Zhang11 refers to the parser of Zhang and Nivre (2011), Li12 refers to the unlabeled parser of Li et al. (2012), Koo08 refers to the parser of Koo et al. (2008), Suzuki09 refers to the parser of Suzuki et al. (2009), Chen09 refers to the parser of Chen et al. (2009), Zhou11 refers to the parser of Zhou et al. (2011), Suzuki11 refers to the parser of Suzuki et al. (2011), and Chen12 refers to the parser of Chen et al. (2012). The results showed that our meta parser outperformed most of the previous systems and obtained the comparable accuracy with the best result of Suzuki11 (Suzuki et al., 2011) which combined the clustering-based word representations of Koo et al. (2008) and a condensed feature representation. However, our approach is much simpler than theirs and we believe that our meta parser can be further improved by combining their methods. Sup Semi System McDonald06 Koo10 Zhang11 Li12 Our Baseline Koo08 Suzuki09 Chen09 Zhou11 Suzuki11 Chen12 MetaParser UAS 91.5 93.04 92.9 93.12 92.76 93.16 93.79 93.16 92."
D13-1129,C96-1058,0,0.0106219,"scoring function score(x, g) is, score(x, g) = f(x, g) · w (2) where f(x, g) is a high-dimensional feature vector based on features defined over g and x and w refers to the weights for the features. The maximum spanning tree is the highest scoring tree in Y (Gx ). The task of decoding algorithms in the parsing model for an input sentence x is to find y ∗ , where y ∗ = arg max score(x, y) y∈Y (Gx ) = arg max ∑ score(x, g) y∈Y (Gx ) g∈y = arg max ∑ f(x, g) · w (3) y∈Y (Gx ) g∈y In our system, we use the decoding algorithm proposed by Carreras (2007), which is a secondorder CKY-style algorithm (Eisner, 1996) and feature weights w are learned during training using the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; McDonald et al., 2005). 2.2 Base features Previous studies have defined different sets of features for the graph-based parsing models, such as the first-order features defined in McDonald et al. (2005), the second-order parent-siblings features defined in McDonald and Pereira (2006), and the second-order parent-child-grandchild features defined in Carreras (2007). Bohnet (2010) explorers a richer set of features than the above sets. We further extend the features defi"
D13-1129,I11-1136,0,0.350016,"003) and the Chinese head rules of Zhang and Clark (2008). We followed the standard data splits as shown in Table 3. Following the work of Koo et al. (2008), we used a tagger trained on training data to provide part-of-speech (POS) tags for the development and test sets, and used 10-way jackknifing to generate part-of-speech tags for the training set. We used the MXPOST (Ratnaparkhi, 1996) tagger for English and the CRF-based tagger for Chinese. We used gold standard segmentation in the CTB5. The data partition of Chinese were chosen to match previous work (Duan et al., 2007; Li et al., 2011; Hatori et al., 2011). I ate the meat with a fork I!!!!ate!!!!the!!!!meat!!!!with!!!!a!!!!fork!!!!. Tk:!hw,!dw,!cw,!d(h,d,c) Fb:!ate,!meat,!with,!RIGHTSIB &quot; (fb)=Mk [Mk];![Mk],!VV;![Mk],!ate Figure 1: An example of generating meta features ing to the mapping function, we obtain the mapped value Mk . Finally, we have the three meta features “[Mk ]”, “[Mk ], V V ”, and “[Mk ], ate”, where V V is the part-of-speech tag of word “ate”. In this way, we can generate all the meta features for the graphbased model. PTB (sections) CTB5 (files) train 2-21 dev 22 test 23 001-815 1001-1136 886-931 1148-1151 816-885 1137-1147 T"
D13-1129,P10-1001,0,0.0356174,"Missing"
D13-1129,P08-1068,0,0.756769,"z and Marsi, 2006; Nivre et al., 2007). The supervised models take annotated data as training data, utilize features defined over surface words, part-of-speech tags, and dependency trees, and learn the preference of features via adjusting feature weights. ∗ Corresponding author Several methods have been proposed to alleviate this problem by using large amounts of unannotated data, ranging from self-training and co-training (McClosky et al., 2006; Sagae and Tsujii, 2007) to more complex methods that collect statistical information from unannotated sentences and use them as additional features (Koo et al., 2008; Chen et al., 2009). In this paper, we propose an alternative approach to semi-supervised dependency parsing via feature transformation (Ando and Zhang, 2005). More 1303 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1303–1313, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics specifically, we transform base features to a higherlevel space. The base features defined over surface words, part-of-speech tags, and dependency trees are high dimensional and have been explored in the above previous studies. T"
D13-1129,P09-1058,0,0.0289575,"(Xue et al., 2005) for Chinese. The tool “Penn2Malt”1 was used 1 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html 1307 For the unannotated data in English, we used the BLLIP WSJ corpus (Charniak et al., 2000) containing about 43 million words.2 We used the MXPOST tagger trained on the training data to assign part-ofspeech tags and used the Baseline parser to process the sentences of the Brown corpus. For the unannotated data in Chinese, we used the Xinhua portion of Chinese Gigaword3 Version 2.0 (LDC2009T14) (Huang, 2009), which has approximately 311 million words. We used the MMA system (Kruengkrai et al., 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse the sentences in the Gigaword data. In collecting the base features, we removed the features which occur only once in the English data and less than four times in the Chinese data. The feature occurrences of one time and four times are based on the development data performance. We measured the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of to2 We ensured that the text used for building the meta features did not include the sentences of the Penn Treeb"
D13-1129,D11-1109,1,0.948858,"and Matsumoto (2003) and the Chinese head rules of Zhang and Clark (2008). We followed the standard data splits as shown in Table 3. Following the work of Koo et al. (2008), we used a tagger trained on training data to provide part-of-speech (POS) tags for the development and test sets, and used 10-way jackknifing to generate part-of-speech tags for the training set. We used the MXPOST (Ratnaparkhi, 1996) tagger for English and the CRF-based tagger for Chinese. We used gold standard segmentation in the CTB5. The data partition of Chinese were chosen to match previous work (Duan et al., 2007; Li et al., 2011; Hatori et al., 2011). I ate the meat with a fork I!!!!ate!!!!the!!!!meat!!!!with!!!!a!!!!fork!!!!. Tk:!hw,!dw,!cw,!d(h,d,c) Fb:!ate,!meat,!with,!RIGHTSIB &quot; (fb)=Mk [Mk];![Mk],!VV;![Mk],!ate Figure 1: An example of generating meta features ing to the mapping function, we obtain the mapped value Mk . Finally, we have the three meta features “[Mk ]”, “[Mk ], V V ”, and “[Mk ], ate”, where V V is the part-of-speech tag of word “ate”. In this way, we can generate all the meta features for the graphbased model. PTB (sections) CTB5 (files) train 2-21 dev 22 test 23 001-815 1001-1136 886-931 1148-11"
D13-1129,C12-1103,1,0.727694,"Figure 2 shows the average accuracy scores of the Baseline parsers against to the bins. From the figure, we found that for both two languages the Baseline parsers performed worse while the sentences contained more unknown features. 48.0 48.05 47.15 46.61 51.36 100 Table 10: Relevant results for English. Sup denotes the supervised parsers, Semi denotes the parsers with semisupervised methods. 4.5.2 Chinese Table 11 shows the comparative results, where Li11 refers to the parser of Li et al. (2011), Hatori11 refers to the parser of Hatori et al. (2011), and Li12 refers to the unlabeled parser of Li et al. (2012). The reported scores on this data were produced by the supervised learning methods and our Baseline (supervised) parser provided the comparable accuracy. We found that the score of our meta parser for this data was the best reported so far and significantly higher than the previous scores. Note that we used the auto-assigned POS tags in the test set to match the above previous studies. System Li11 Hatori11 Li12 Our Baseline MetaParser UAS 80.79 81.33 81.21 81.01 83.08 COMP 29.11 29.90 29.71 32.21 Table 11: Relevant results for Chinese 4.6 Analysis Here, we analyzed the effect of the meta feat"
D13-1129,J93-2004,0,0.0515184,"first step, we use a baseline parser to parse a large amount of unannotated sentences. Then we collect the base features from the parse trees. The collected features are transformed into predefined discrete values via a transformation function. Based on the transformed values, we define a set of meta features. Finally, the meta features are incorporated directly into parsing models. To demonstrate the effectiveness of the proposed approach, we apply it to the graph-based parsing models (McDonald and Nivre, 2007). We conduct experiments on the standard data split of the Penn English Treebank (Marcus et al., 1993) and the Chinese Treebank Version 5.1 (Xue et al., 2005). The results indicate that the approach significantly improves the accuracy. In summary, we make the following contributions: • We define a simple yet useful transformation function to transform base features to meta features automatically. The meta features build connections between known and unknown base features, and relieve the data sparseness problem. • Compared to the base features, the number of meta features is remarkably small. • We build semi-supervised dependency parsers that achieve the best accuracy on the Chinese data and c"
D13-1129,P06-1043,0,0.247812,"1 Introduction In recent years, supervised learning models have achieved lots of progress in the dependency parsing task, as can be found in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). The supervised models take annotated data as training data, utilize features defined over surface words, part-of-speech tags, and dependency trees, and learn the preference of features via adjusting feature weights. ∗ Corresponding author Several methods have been proposed to alleviate this problem by using large amounts of unannotated data, ranging from self-training and co-training (McClosky et al., 2006; Sagae and Tsujii, 2007) to more complex methods that collect statistical information from unannotated sentences and use them as additional features (Koo et al., 2008; Chen et al., 2009). In this paper, we propose an alternative approach to semi-supervised dependency parsing via feature transformation (Ando and Zhang, 2005). More 1303 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1303–1313, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics specifically, we transform base features to a higherlevel spac"
D13-1129,D07-1013,0,0.343727,"y problems. In our approach, the base features are grouped and each group relates to a meta feature. In the first step, we use a baseline parser to parse a large amount of unannotated sentences. Then we collect the base features from the parse trees. The collected features are transformed into predefined discrete values via a transformation function. Based on the transformed values, we define a set of meta features. Finally, the meta features are incorporated directly into parsing models. To demonstrate the effectiveness of the proposed approach, we apply it to the graph-based parsing models (McDonald and Nivre, 2007). We conduct experiments on the standard data split of the Penn English Treebank (Marcus et al., 1993) and the Chinese Treebank Version 5.1 (Xue et al., 2005). The results indicate that the approach significantly improves the accuracy. In summary, we make the following contributions: • We define a simple yet useful transformation function to transform base features to meta features automatically. The meta features build connections between known and unknown base features, and relieve the data sparseness problem. • Compared to the base features, the number of meta features is remarkably small."
D13-1129,E06-1011,0,0.00735014,"Gx ) = arg max ∑ score(x, g) y∈Y (Gx ) g∈y = arg max ∑ f(x, g) · w (3) y∈Y (Gx ) g∈y In our system, we use the decoding algorithm proposed by Carreras (2007), which is a secondorder CKY-style algorithm (Eisner, 1996) and feature weights w are learned during training using the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; McDonald et al., 2005). 2.2 Base features Previous studies have defined different sets of features for the graph-based parsing models, such as the first-order features defined in McDonald et al. (2005), the second-order parent-siblings features defined in McDonald and Pereira (2006), and the second-order parent-child-grandchild features defined in Carreras (2007). Bohnet (2010) explorers a richer set of features than the above sets. We further extend the features defined by Bohnet (2010) by introducing more lexical features as the base features. The base feature templates are listed in Table 1, where h, d refer to the head, the dependent respectively, c refers to d’s sibling or child, b refers to the word between h and d, +1 (−1) refers to the next (previous) word, w and p refer to the surface word and part-of-speech tag respectively, [wp] refers to the surface word or p"
D13-1129,P05-1012,0,0.657697,"006) and Carreras (2007) define secondorder features over two adjacent arcs in secondorder graph-based models. Koo and Collins (2010) use third-order features in a third-order graph-based model. Bohnet (2010) considers information of more surrounding words for the graph-based models, while Zhang and Nivre (2011) define a set of rich features including the word valency and the third-order context features for transition-based models. All these models utilize richer and more complex feature representations and achieve better performance than the earlier models that utilize the simpler features (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). However, the richer feature representations result in a high-dimensional feature space. Features in such a space may suffer from the data sparseness problem and thus have less discriminative power on unseen data. If input sentences contain unknown features that are not included in training data, the parsers can usually give lower accuracy. In current dependency parsing models, conventional features (i.e. base features) defined over surface words and part-of-speech tags in a relatively high-dimensional feature space may suffer from the data"
D13-1129,C04-1010,0,0.0156794,"s over two adjacent arcs in secondorder graph-based models. Koo and Collins (2010) use third-order features in a third-order graph-based model. Bohnet (2010) considers information of more surrounding words for the graph-based models, while Zhang and Nivre (2011) define a set of rich features including the word valency and the third-order context features for transition-based models. All these models utilize richer and more complex feature representations and achieve better performance than the earlier models that utilize the simpler features (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). However, the richer feature representations result in a high-dimensional feature space. Features in such a space may suffer from the data sparseness problem and thus have less discriminative power on unseen data. If input sentences contain unknown features that are not included in training data, the parsers can usually give lower accuracy. In current dependency parsing models, conventional features (i.e. base features) defined over surface words and part-of-speech tags in a relatively high-dimensional feature space may suffer from the data sparseness problem and thus exhibit less discriminat"
D13-1129,W96-0213,0,0.490388,"re with the right direction. In the auto-parsed data, this feature occurs 200 times and ranks between TOP10 and TOP30. Accordto convert the data into dependency structures with the English head rules of Yamada and Matsumoto (2003) and the Chinese head rules of Zhang and Clark (2008). We followed the standard data splits as shown in Table 3. Following the work of Koo et al. (2008), we used a tagger trained on training data to provide part-of-speech (POS) tags for the development and test sets, and used 10-way jackknifing to generate part-of-speech tags for the training set. We used the MXPOST (Ratnaparkhi, 1996) tagger for English and the CRF-based tagger for Chinese. We used gold standard segmentation in the CTB5. The data partition of Chinese were chosen to match previous work (Duan et al., 2007; Li et al., 2011; Hatori et al., 2011). I ate the meat with a fork I!!!!ate!!!!the!!!!meat!!!!with!!!!a!!!!fork!!!!. Tk:!hw,!dw,!cw,!d(h,d,c) Fb:!ate,!meat,!with,!RIGHTSIB &quot; (fb)=Mk [Mk];![Mk],!VV;![Mk],!ate Figure 1: An example of generating meta features ing to the mapping function, we obtain the mapped value Mk . Finally, we have the three meta features “[Mk ]”, “[Mk ], V V ”, and “[Mk ], ate”, where V V"
D13-1129,D07-1111,0,0.737038,"t years, supervised learning models have achieved lots of progress in the dependency parsing task, as can be found in the CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). The supervised models take annotated data as training data, utilize features defined over surface words, part-of-speech tags, and dependency trees, and learn the preference of features via adjusting feature weights. ∗ Corresponding author Several methods have been proposed to alleviate this problem by using large amounts of unannotated data, ranging from self-training and co-training (McClosky et al., 2006; Sagae and Tsujii, 2007) to more complex methods that collect statistical information from unannotated sentences and use them as additional features (Koo et al., 2008; Chen et al., 2009). In this paper, we propose an alternative approach to semi-supervised dependency parsing via feature transformation (Ando and Zhang, 2005). More 1303 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1303–1313, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics specifically, we transform base features to a higherlevel space. The base features defi"
D13-1129,P08-1076,0,0.0200172,"0 1 2 3 BIN 4 5 Figure 4: Improvement relative to numbers of active meta features on Chinese (average per word) Several previous studies relevant to our approach have been conducted. Koo et al. (2008) used a word clusters trained on a large amount of unannotated data and designed a set of new features based on the clusters for dependency parsing models. Chen et al. (2009) extracted subtree structures from a large amount of data and represented them as the additional features to improve dependency parsing. Suzuki et al. (2009) extended a Semi-supervised Structured Conditional Model (SSSCM) of Suzuki and Isozaki (2008) to the dependency parsing problem and combined their method with the word clustering feature representation of Koo et al. (2008). Chen et al. (2012) proposed an approach to representing high-order features for graphbased dependency parsing models using a dependency language model and beam search. In future work, we may consider to combine their methods with ours to improve performance. 1311 Several previous studies used co-training/selftraining methods. McClosky et al. (2006) presented a self-training method combined with a reranking algorithm for constituency parsing. Sagae and Tsujii (2007)"
D13-1129,D09-1058,0,0.449073,"ctive meta features on English (average per word) 50 Better Worse Percentage 40 30 20 10 0 1 2 3 BIN 4 5 Figure 4: Improvement relative to numbers of active meta features on Chinese (average per word) Several previous studies relevant to our approach have been conducted. Koo et al. (2008) used a word clusters trained on a large amount of unannotated data and designed a set of new features based on the clusters for dependency parsing models. Chen et al. (2009) extracted subtree structures from a large amount of data and represented them as the additional features to improve dependency parsing. Suzuki et al. (2009) extended a Semi-supervised Structured Conditional Model (SSSCM) of Suzuki and Isozaki (2008) to the dependency parsing problem and combined their method with the word clustering feature representation of Koo et al. (2008). Chen et al. (2012) proposed an approach to representing high-order features for graphbased dependency parsing models using a dependency language model and beam search. In future work, we may consider to combine their methods with ours to improve performance. 1311 Several previous studies used co-training/selftraining methods. McClosky et al. (2006) presented a self-training"
D13-1129,P11-2112,0,0.19353,". Table 1309 Table 10 shows the performance of the previous systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo10 refers to the thirdorder parser with model1 of Koo and Collins (2010), Zhang11 refers to the parser of Zhang and Nivre (2011), Li12 refers to the unlabeled parser of Li et al. (2012), Koo08 refers to the parser of Koo et al. (2008), Suzuki09 refers to the parser of Suzuki et al. (2009), Chen09 refers to the parser of Chen et al. (2009), Zhou11 refers to the parser of Zhou et al. (2011), Suzuki11 refers to the parser of Suzuki et al. (2011), and Chen12 refers to the parser of Chen et al. (2012). The results showed that our meta parser outperformed most of the previous systems and obtained the comparable accuracy with the best result of Suzuki11 (Suzuki et al., 2011) which combined the clustering-based word representations of Koo et al. (2008) and a condensed feature representation. However, our approach is much simpler than theirs and we believe that our meta parser can be further improved by combining their methods. Sup Semi System McDonald06 Koo10 Zhang11 Li12 Our Baseline Koo08 Suzuki09 Chen09 Zhou11 Suzuki11 Chen12 MetaParse"
D13-1129,W03-3023,0,0.167445,") define secondorder features over two adjacent arcs in secondorder graph-based models. Koo and Collins (2010) use third-order features in a third-order graph-based model. Bohnet (2010) considers information of more surrounding words for the graph-based models, while Zhang and Nivre (2011) define a set of rich features including the word valency and the third-order context features for transition-based models. All these models utilize richer and more complex feature representations and achieve better performance than the earlier models that utilize the simpler features (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). However, the richer feature representations result in a high-dimensional feature space. Features in such a space may suffer from the data sparseness problem and thus have less discriminative power on unseen data. If input sentences contain unknown features that are not included in training data, the parsers can usually give lower accuracy. In current dependency parsing models, conventional features (i.e. base features) defined over surface words and part-of-speech tags in a relatively high-dimensional feature space may suffer from the data sparseness problem and thus"
D13-1129,D08-1059,1,0.331194,"procedure using template Tk = “hw , dw , cw , d(h, d, c)” (the second template of Table 1-(c) ), which contains the surface forms of the head, the dependent, its sibling, and the directions of the dependencies among h, d, and c. We can have a base feature “ate, meat, with, RIGHTSIB”, where “RIGHTSIB” refers to the parent-siblings structure with the right direction. In the auto-parsed data, this feature occurs 200 times and ranks between TOP10 and TOP30. Accordto convert the data into dependency structures with the English head rules of Yamada and Matsumoto (2003) and the Chinese head rules of Zhang and Clark (2008). We followed the standard data splits as shown in Table 3. Following the work of Koo et al. (2008), we used a tagger trained on training data to provide part-of-speech (POS) tags for the development and test sets, and used 10-way jackknifing to generate part-of-speech tags for the training set. We used the MXPOST (Ratnaparkhi, 1996) tagger for English and the CRF-based tagger for Chinese. We used gold standard segmentation in the CTB5. The data partition of Chinese were chosen to match previous work (Duan et al., 2007; Li et al., 2011; Hatori et al., 2011). I ate the meat with a fork I!!!!ate"
D13-1129,P11-2033,1,0.339716,"ogy, Soochow University, China ‡ Singapore University of Technology and Design, Singapore {wlchen, mzhang}@suda.edu.cn yue zhang@sutd.edu.sg Abstract In the supervised learning scenarios, many previous studies explore rich feature representation that leads to significant improvements. McDonald and Pereira (2006) and Carreras (2007) define secondorder features over two adjacent arcs in secondorder graph-based models. Koo and Collins (2010) use third-order features in a third-order graph-based model. Bohnet (2010) considers information of more surrounding words for the graph-based models, while Zhang and Nivre (2011) define a set of rich features including the word valency and the third-order context features for transition-based models. All these models utilize richer and more complex feature representations and achieve better performance than the earlier models that utilize the simpler features (McDonald et al., 2005; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). However, the richer feature representations result in a high-dimensional feature space. Features in such a space may suffer from the data sparseness problem and thus have less discriminative power on unseen data. If input sentences conta"
D13-1129,P11-1156,0,0.436585,"Missing"
D13-1129,D07-1096,0,\N,Missing
D14-1021,W09-0437,0,0.0218632,"2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibility of function word usage, rich morphology and paraphrasing all add to the difficulty of rule extraction. In addition, restricting target word orders by hard translation rules can also hurt output fluency. ∗ * Work done while visiting Singapore University of Technology and Design (SUTD) 177 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 177–182, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics below λ · Pmax are filtered out, where Pmax is the probability of t"
D14-1021,P07-1020,0,0.0211782,"algorithm, and one potential of the SMT architecture is that it can directly benefit from advances in statistical NLG technology. As discussed in the introduction, our work is closely related to previous studies on syntactic MT, with the salient difference that we do not rely on hard translation rules, but allow free target synthesis. The contrast can be summarized as “translation by parsing” vs “translation by generation”. There has been a line of research on generation for translation. Soricut and Marcu (2006) use a form of weighted IDL-expressions (Nederhof and Satta, 2004) for generation. Bangalore et al. (2007) treats MT as a combination of global lexical transfer and word ordering; their generation component does not perform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis comp"
D14-1021,P08-1024,0,0.0633776,"Missing"
D14-1021,J93-2003,0,0.0506297,"minimizing engineering efforts. Shown in Figure 1, the end-to-end system consists of two main components: lexical transfer and synthesis. The former provides candidate translations for (overlapping) source words and phrases. Although lexicons and rules can be used for this step, we take a simple statistical alignment-based approach. The latter searches for a target translation by constructing dependency trees bottom-up. The process can be viewed as a syntax-based generation process from a bag of overlapping translation options. 2.1 Lexical transfer We perform word alignment using IBM model 4 (Brown et al., 1993), and then extract phrase pairs according to the alignment and automaticallyannotated target syntax. In particular, consistent (Och et al., 1999) and cohesive (Fox, 2002) phrase pairs are extracted from intersected alignments in both directions: the target side must form a projective span, with a single root, and the source side must be contiguous. A resulting phrase pair consists of the source phrase, its target translation, as well as the head position and head part-of-speech (POS) of the target span, which are useful for target synthesis. We further restrict that neither the source nor the"
D14-1021,P05-1033,0,0.0949535,"et equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli"
D14-1021,E14-1028,0,0.178381,"Missing"
D14-1021,W11-2107,0,0.0896313,"Missing"
D14-1021,C12-1121,0,0.0359306,"Missing"
D14-1021,W99-0604,0,0.134086,"r provides candidate translations for (overlapping) source words and phrases. Although lexicons and rules can be used for this step, we take a simple statistical alignment-based approach. The latter searches for a target translation by constructing dependency trees bottom-up. The process can be viewed as a syntax-based generation process from a bag of overlapping translation options. 2.1 Lexical transfer We perform word alignment using IBM model 4 (Brown et al., 1993), and then extract phrase pairs according to the alignment and automaticallyannotated target syntax. In particular, consistent (Och et al., 1999) and cohesive (Fox, 2002) phrase pairs are extracted from intersected alignments in both directions: the target side must form a projective span, with a single root, and the source side must be contiguous. A resulting phrase pair consists of the source phrase, its target translation, as well as the head position and head part-of-speech (POS) of the target span, which are useful for target synthesis. We further restrict that neither the source nor the target side of a valid phrase pair contains over s words. Given an input source sentence, the lexical transfer unit finds all valid target transl"
D14-1021,P05-1034,0,0.0509849,"thesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translat"
D14-1021,N13-1025,0,0.0341822,"Missing"
D14-1021,D08-1052,0,0.0180782,"ependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibility of function word"
D14-1021,W02-1039,0,0.0634627,"s for (overlapping) source words and phrases. Although lexicons and rules can be used for this step, we take a simple statistical alignment-based approach. The latter searches for a target translation by constructing dependency trees bottom-up. The process can be viewed as a syntax-based generation process from a bag of overlapping translation options. 2.1 Lexical transfer We perform word alignment using IBM model 4 (Brown et al., 1993), and then extract phrase pairs according to the alignment and automaticallyannotated target syntax. In particular, consistent (Och et al., 1999) and cohesive (Fox, 2002) phrase pairs are extracted from intersected alignments in both directions: the target side must form a projective span, with a single root, and the source side must be contiguous. A resulting phrase pair consists of the source phrase, its target translation, as well as the head position and head part-of-speech (POS) of the target span, which are useful for target synthesis. We further restrict that neither the source nor the target side of a valid phrase pair contains over s words. Given an input source sentence, the lexical transfer unit finds all valid target translation options for overlap"
D14-1021,N04-1035,0,0.0948608,"phrases to their target equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage p"
D14-1021,P06-1139,0,0.0331291,"urrently rather separated research fields. The system is not strongly dependent on the specific generation algorithm, and one potential of the SMT architecture is that it can directly benefit from advances in statistical NLG technology. As discussed in the introduction, our work is closely related to previous studies on syntactic MT, with the salient difference that we do not rely on hard translation rules, but allow free target synthesis. The contrast can be summarized as “translation by parsing” vs “translation by generation”. There has been a line of research on generation for translation. Soricut and Marcu (2006) use a form of weighted IDL-expressions (Nederhof and Satta, 2004) for generation. Bangalore et al. (2007) treats MT as a combination of global lexical transfer and word ordering; their generation component does not perform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reorderin"
D14-1021,P12-2061,0,0.0155088,"actic MT, with the salient difference that we do not rely on hard translation rules, but allow free target synthesis. The contrast can be summarized as “translation by parsing” vs “translation by generation”. There has been a line of research on generation for translation. Soricut and Marcu (2006) use a form of weighted IDL-expressions (Nederhof and Satta, 2004) for generation. Bangalore et al. (2007) treats MT as a combination of global lexical transfer and word ordering; their generation component does not perform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming th"
D14-1021,P09-1091,0,0.221722,"ng on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et a"
D14-1021,E09-1097,0,0.299711,"et word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et al. (2006), Blunsom et al. (2008), Flanigan et al. (2013) and Yu et al. (2013) in that we build a discriminative model for SM"
D14-1021,D09-1043,0,0.0746518,"U comparable to the state-of-the-art syntactic SMT systems. Figure 1: Overall system architecture. A potential solution to the problems above is to treat translation as a generation task, representing syntactic correspondences using soft features. Both adequacy and fluency can potentially be improved by giving full flexibility to target synthesis, and leaving all options to the statistical model. The main challenge to this method is a significant increase in the search space (Knight, 1999). To this end, recent advances in tackling complex search tasks for text generation offer some solutions (White and Rajkumar, 2009; Zhang and Clark, 2011). In this short paper, we present a preliminary investigation on the possibility of building a syntactic SMT system that does not use hard translation rules, by utilizing recent advances in statistical natural language generation (NLG). The overall architecture is shown in Figure 1. Translation is performed by first parsing the source sentence, then transferring source words and phrases to their target equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis s"
D14-1021,J99-4005,0,0.137947,"ined consistently in a discriminative model. Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems. Figure 1: Overall system architecture. A potential solution to the problems above is to treat translation as a generation task, representing syntactic correspondences using soft features. Both adequacy and fluency can potentially be improved by giving full flexibility to target synthesis, and leaving all options to the statistical model. The main challenge to this method is a significant increase in the search space (Knight, 1999). To this end, recent advances in tackling complex search tasks for text generation offer some solutions (White and Rajkumar, 2009; Zhang and Clark, 2011). In this short paper, we present a preliminary investigation on the possibility of building a syntactic SMT system that does not use hard translation rules, by utilizing recent advances in statistical natural language generation (NLG). The overall architecture is shown in Figure 1. Translation is performed by first parsing the source sentence, then transferring source words and phrases to their target equivalences, and finally synthesizing t"
D14-1021,J10-4005,0,0.0465794,"Missing"
D14-1021,P12-3004,1,0.847822,"Chinese-English dataset, which consists of training sentence pairs from the dialog task (dialog) and Basic Travel and Expression Corpus (BTEC). The union of dialog and BTEC are taken as our training set, which contains 30,033 sentence pairs. For system tuning, we use the IWSLT 2004 test set (also released as the second development test set of IWSLT 2010), which contains 500 sentences. For final test, we use the IWSLT 2003 test set (also released as the first development test set of IWSLT 2010), which contains 506 sentences. The Chinese sentences in the datasets are segmented using NiuTrans3 (Xiao et al., 2012), while POS-tagging of both English and Chinese is performed using ZPar4 version 0.5 (Zhang and Clark, 2011). We train the English POS-tagger using the WSJ sections of the Penn Treebank (Marcus et al., 1993), turned into lower-case. For syntactic parsing of both English and Chinese, we use the default models of ZPar 0.5. We choose three baseline systems: a string-totree (S2T) system, a tree-to-string (T2S) system and a tree-to-tree (T2T) system (Koehn, 2010). The Moses release 1.0 implementations of all three systems are used, with default parameter settings. IRSTLM5 release 5.80.03 (Federico"
D14-1021,D11-1020,1,0.858858,"both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibility of function word usage, rich morphol"
D14-1021,P06-1096,0,0.0607434,"Missing"
D14-1021,P06-1077,1,0.766363,"s, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), whi"
D14-1021,W06-1606,0,0.034268,"output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibi"
D14-1021,J93-2004,0,0.0459525,"Missing"
D14-1021,D13-1112,0,0.0442095,"Missing"
D14-1021,D11-1106,1,0.885569,"of-the-art syntactic SMT systems. Figure 1: Overall system architecture. A potential solution to the problems above is to treat translation as a generation task, representing syntactic correspondences using soft features. Both adequacy and fluency can potentially be improved by giving full flexibility to target synthesis, and leaving all options to the statistical model. The main challenge to this method is a significant increase in the search space (Knight, 1999). To this end, recent advances in tackling complex search tasks for text generation offer some solutions (White and Rajkumar, 2009; Zhang and Clark, 2011). In this short paper, we present a preliminary investigation on the possibility of building a syntactic SMT system that does not use hard translation rules, by utilizing recent advances in statistical natural language generation (NLG). The overall architecture is shown in Figure 1. Translation is performed by first parsing the source sentence, then transferring source words and phrases to their target equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), w"
D14-1021,E12-1075,1,0.701608,") translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et al. (2006), Blunsom et al. (2008), Flanigan et al. (2013) and Yu et al. (2013) in that we build a discriminative model for SMT. Acknowledgement The work has been supported by the Singapore Ministration of Education Tier 2 project T2"
D14-1021,P07-1002,0,\N,Missing
D14-1021,C14-1104,1,\N,Missing
D14-1093,J09-4006,0,0.05641,"rces by joining the partially annotated sentences derived using each resource, training our CRF model with these partially annotated sentences and the fully annotated PD sentences. The tests are performed on medicine and computer domains. Table 7 shows the results, where further improvements are made on both domains when the two types of resources are combined. 6 Related Work There has been a line of research on making use of unlabeled data for word segmentation. Zhao and Kit (2008) improve segmentation performance by mutual information between characters, collected from large unlabeled data; Li and Sun (2009) use punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words; Sun and Xu (2011) explore several statistical features derived from unlabeled data to help improve character-based word segmentation. These investigations mainly focus on in-domain accuracies. Liu and Zhang (2012) 871 0.78 0.9525 0.9522 0.8775 0.9225 0.8750 0.9200 0.76 0.9175 0.75 0.8725 0.77 0.9519 0.74 0.9150 0.8700 0.73 0.9516 0 50 100 150 0 (a) Finance 50 100 150 (b) Medicine 0.87 0.750 0.938 0.9222 0.745 0.936 0.9219 0.740 0.934 0.9225 0.86 0.9216 0.85 0.84 0.735"
D14-1093,C12-2073,1,0.579408,"om different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled domain-specific lexicons to improve target-domain segmentation accuracies. Both 864 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864–874, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computation"
D14-1093,P08-1099,0,0.00794877,"e structure perceptron (Collins, 2002), although they study two different sources of information. In contrast to their work, we unify both types of information under the CRF framework. CRF has been used for Chinese word segmentation (Tseng, 2005; Shi and Wang, 2007; Zhao and Kit, 2008; Wang et al., 2011). However, most previous work train a CRF by using full annotation only. In contrast, we study CRF based segmentation by using both full and partial annotation. Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introduce knowledge by incorporating a manually annotated feature distribution into the regularizer, and the JESS-CM (Suzuki and Isozaki, 2008), which use a EM-like method to iteratively optimize the parameter on both the annotated data and unlabeled data. In contrast, we directly incorporate the likelihood of partial annotation into the objective function. The work that is the most similar to ours is Tsuboi et al. (2008), who modify the CRF learning objective for partial data. They focus on Japanese lexical analysis using manually collected partial data, while we investigate the effec"
D14-1093,W02-1001,0,0.0713606,"main difference into two types: different vocabulary and different POS distributions. While the first type of difference can be effectively resolved by using lexicon for each domain, the second type of difference needs to be resolved by using annotated sentences. They found that given the same manual annotation time, a combination of the lexicon and sentence is the most effective. Jiang et al. (2013) use 160K Wikipedia sentences to improves segmentation accuracies on several domains. Both Zhang et al. (2014) and Jiang et al. (2013) work on discriminative models using the structure perceptron (Collins, 2002), although they study two different sources of information. In contrast to their work, we unify both types of information under the CRF framework. CRF has been used for Chinese word segmentation (Tseng, 2005; Shi and Wang, 2007; Zhao and Kit, 2008; Wang et al., 2011). However, most previous work train a CRF by using full annotation only. In contrast, we study CRF based segmentation by using both full and partial annotation. Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introd"
D14-1093,J05-4005,0,0.291234,"Missing"
D14-1093,C12-2116,0,0.0277869,"pedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled domain-specific lexicons to improve target-domain segmentation accuracies. Both 864 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864–874, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics to improve segmentation. However, no empirical results have been reported on a unified approach to deal with different types of free data. We use a conditional rand"
D14-1093,D12-1075,0,0.028153,"he best reported in the literature. 1 浦 东 开 发 与 法 制 建 设 b m b m b m b m b m b m b m b m b m e e e e e e e e e s s s s s s s s s Figure 1: The segmentation problem, illustrated using the sentence “浦东 (Pudong) 开发 (development) 与 (and) 法制 (legal) 建设 (construction)”. Possible segmentation labels are drawn under each character, where b, m, e, s stand for the beginning, middle, end of a multi-character word, and a single character word, respectively. The path shows the correct segmentation by choosing one label for each character. methods are competitive given the same amount of annotation effects (Garrette and Baldridge, 2012; Zhang et al., 2014). However, obtaining manually annotated data can be expensive. On the other hand, there are free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and semiannotated web pages such as Wikipedia. In the last case, word-boundary information is contained in hyperlinks and other markup annotations. Such free data offer a useful alternative for improving the segmentation performance, especially on domains that are not identical to newswire, and for which little annotation is available."
D14-1093,D11-1090,0,0.225529,"is contained in hyperlinks and other markup annotations. Such free data offer a useful alternative for improving the segmentation performance, especially on domains that are not identical to newswire, and for which little annotation is available. In this paper, we investigate techniques for adopting freely available data to help improve the performance on Chinese word segmentation. We propose a simple but robust method for constructing partial segmentation from different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token-"
D14-1093,P08-1076,0,0.0121424,"formation under the CRF framework. CRF has been used for Chinese word segmentation (Tseng, 2005; Shi and Wang, 2007; Zhao and Kit, 2008; Wang et al., 2011). However, most previous work train a CRF by using full annotation only. In contrast, we study CRF based segmentation by using both full and partial annotation. Several other variants of CRF model has been proposed in the machine learning literature, such as the generalized expectation method (Mann and McCallum, 2008), which introduce knowledge by incorporating a manually annotated feature distribution into the regularizer, and the JESS-CM (Suzuki and Isozaki, 2008), which use a EM-like method to iteratively optimize the parameter on both the annotated data and unlabeled data. In contrast, we directly incorporate the likelihood of partial annotation into the objective function. The work that is the most similar to ours is Tsuboi et al. (2008), who modify the CRF learning objective for partial data. They focus on Japanese lexical analysis using manually collected partial data, while we investigate the effect of partial annotation from freely available sources for Chinese segmentation. 7 Conclusion In this paper, we investigated the problem of domain adapt"
D14-1093,P09-1059,0,0.511663,"and for which little annotation is available. In this paper, we investigate techniques for adopting freely available data to help improve the performance on Chinese word segmentation. We propose a simple but robust method for constructing partial segmentation from different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled"
D14-1093,P13-1075,0,0.533434,"Missing"
D14-1093,C08-1113,0,0.183793,"-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverage manually assembled domain-specific lexicons to improve target-domain segmentation accuracies. Both 864 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 864–874, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics to improve segmentation. However, no empirical results have been reported on a unified approach to deal with different types of free data. We use a conditional random fields (Lafferty et al., 2001; Tsuboi et al., 2008) variant that can leverage the partial annotations obtained from different sources of free annotation. Training is achieved by a modification to the learning objective, incorporating partial annotation likelihood, so that a single model can be trained consistently with a mixture of full and partial annotation. Experimental results show that our method of using partially annotated data can consistently improves cross-domain segmentation performance. We obtain results which are competitive to the best reported in the literature. Our segmentor is freely released at https://github.com/ ExpResults/"
D14-1093,I11-1035,0,0.0409989,"perlinks and other markup annotations. Such free data offer a useful alternative for improving the segmentation performance, especially on domains that are not identical to newswire, and for which little annotation is available. In this paper, we investigate techniques for adopting freely available data to help improve the performance on Chinese word segmentation. We propose a simple but robust method for constructing partial segmentation from different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised"
D14-1093,C96-1035,0,0.0297556,"form of partial annotation with same unresolved ambiguities, as shown in Figure 2, and use them together with available full annotation (Figure 1) as the training data for the segmentor. In this section, we describe in detail how to obtain partially annotated sentences from each resource, respectively. 2.1 Lexicons In this scenario, we assume that there are unlabeled sentences along with a lexicon for the target domain. We obtain partially segmented sentences by extracting word boundaries from the unlabeled sentences with the help of the lexicon. Previous matching methods (Wu and Tseng, 1993; Wong and Chan, 1996) for Chinese word segmentation largely rely on the lexicons, and are generally considered being weak in ambiguity resolution (Gao 865 People’s Daily Wikipedia 看到 (saw) 海南 (Hainan) 旅游业 (tourist industry) 充满 (full) 希望 (hope) saw tourist industry in Hainan is full of hope 主要(mainly) 是(is) 旅游 (tourist) 业 (industry) 和(and) 软件 (software) 产业(industry) mainly is tourist industry and software industry (a) Case of incompatible annotation on “旅游业(tourist industry)” between People’s Daily and Wikipedia. Literature Computer 《说文解字 (Shuo Wen Jie Zi, a book) 段(segmented) 注(annotated) 》 the segmented and annot"
D14-1093,W03-1728,0,0.317988,"omains that are not identical to newswire, and for which little annotation is available. In this paper, we investigate techniques for adopting freely available data to help improve the performance on Chinese word segmentation. We propose a simple but robust method for constructing partial segmentation from different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-sup"
D14-1093,P07-1106,1,0.561664,"identical to newswire, and for which little annotation is available. In this paper, we investigate techniques for adopting freely available data to help improve the performance on Chinese word segmentation. We propose a simple but robust method for constructing partial segmentation from different sources of free data, including unlabeled data and the Wikipedia. There has been work on making use of both unlabeled data (Sun and Xu, 2011; Wang et al., 2011) and Wikipedia (Jiang et al., 2013) Introduction Statistical Chinese word segmentation gains high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums and Internet literature (Liu and Zhang, 2012). Several methods have been proposed for solving the domain adaptation problem for segmentation, which include the traditional token- and typesupervised methods (Song et al., 2012; Zhang et al., 2014). While token-supervised methods rely on manually annotated target-domain sentences, type-supervised methods leverag"
D14-1093,E14-1062,1,0.310899,"ture. 1 浦 东 开 发 与 法 制 建 设 b m b m b m b m b m b m b m b m b m e e e e e e e e e s s s s s s s s s Figure 1: The segmentation problem, illustrated using the sentence “浦东 (Pudong) 开发 (development) 与 (and) 法制 (legal) 建设 (construction)”. Possible segmentation labels are drawn under each character, where b, m, e, s stand for the beginning, middle, end of a multi-character word, and a single character word, respectively. The path shows the correct segmentation by choosing one label for each character. methods are competitive given the same amount of annotation effects (Garrette and Baldridge, 2012; Zhang et al., 2014). However, obtaining manually annotated data can be expensive. On the other hand, there are free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and semiannotated web pages such as Wikipedia. In the last case, word-boundary information is contained in hyperlinks and other markup annotations. Such free data offer a useful alternative for improving the segmentation performance, especially on domains that are not identical to newswire, and for which little annotation is available. In this paper, we in"
D14-1093,I08-1002,0,0.0330375,"kipedia data. (3) along with To make the most use of free annotation, we combine available free lexicon and natural annotation resources by joining the partially annotated sentences derived using each resource, training our CRF model with these partially annotated sentences and the fully annotated PD sentences. The tests are performed on medicine and computer domains. Table 7 shows the results, where further improvements are made on both domains when the two types of resources are combined. 6 Related Work There has been a line of research on making use of unlabeled data for word segmentation. Zhao and Kit (2008) improve segmentation performance by mutual information between characters, collected from large unlabeled data; Li and Sun (2009) use punctuation information in a large raw corpus to learn a segmentation model, and achieve better recognition of OOV words; Sun and Xu (2011) explore several statistical features derived from unlabeled data to help improve character-based word segmentation. These investigations mainly focus on in-domain accuracies. Liu and Zhang (2012) 871 0.78 0.9525 0.9522 0.8775 0.9225 0.8750 0.9200 0.76 0.9175 0.75 0.8725 0.77 0.9519 0.74 0.9150 0.8700 0.73 0.9516 0 50 100 15"
D14-1148,N07-4013,0,0.00860597,"Missing"
D14-1148,I13-1036,1,0.810861,"n of our work is to extract and use structured events instead of bags-of-words in prediction models. However, structured event extraction can be a costly task, requiring predefined event types and manual event templates (Ji and Grishman, 2008; Li et al., 2013). Partly due to this, the bags-of-words-based document representation has been the mainstream method for a long time. To tackle this issue, we resort to Open IE, extracting event tuples from wide-coverage data with1416 out requiring any human input (e.g. templates). Our system is based on the system of Fader et al. (2011) and the work of Ding et al. (2013); it does not require predefined target event types and labeled training examples. Given a natural language sentence obtained from news texts, the following procedure is used to extract structured events: 1. Event Phrase Extraction. We extract the predicate verb P of a sentence based on the dependency parser of Zhang and Clark (2011), and then find the longest sequence of words Pv , such that Pv starts at P and satisfies the syntactic and lexical constraints proposed by Fader et al. (2011). The content of these two constraints are as follows: • Syntactic constraint: every multi-word event phra"
D14-1148,D11-1142,0,0.072062,"vent. In this paper, we propose using structured information to represent events, and develop a prediction model to analyze the relationship between events and the stock market. The problem is important because it provides insights into understanding the underlying mechanisms of the influence of events on the stock market. There are two main challenges to this method. On the one hand, how to obtain structured event information from large-scale news streams is a challenging problem. We propose to apply Open Information Extraction techniques (Open IE; Banko et al. (2007); Etzioni et al. (2011); Fader et al. (2011)), which do not require predefined event types or manually labeled corpora. Subsequently, two ontologies (i.e. VerbNet and WordNet) are used to generalize structured event features in order to reduce their sparseness. On the other hand, the problem of accurately predicting stock price movement using structured events is challenging, since events and the stock market can have complex relations, which can be influenced by hidden factors. In addition to the commonly used linear models, we build a deep neural network model, which takes structured events as input and learn the potential relationshi"
D14-1148,P08-1030,0,0.00859551,".}. However, terms alone might fail to accurately predict the stock price movement of Microsoft, Nokia, Oracle and Google, because they cannot indicate the actor and object of the event. To our knowledge, no effort has been reported in the literature to empirically investigate structured event representations for stock market prediction. 2.2 Event Extraction A main contribution of our work is to extract and use structured events instead of bags-of-words in prediction models. However, structured event extraction can be a costly task, requiring predefined event types and manual event templates (Ji and Grishman, 2008; Li et al., 2013). Partly due to this, the bags-of-words-based document representation has been the mainstream method for a long time. To tackle this issue, we resort to Open IE, extracting event tuples from wide-coverage data with1416 out requiring any human input (e.g. templates). Our system is based on the system of Fader et al. (2011) and the work of Ding et al. (2013); it does not require predefined target event types and labeled training examples. Given a natural language sentence obtained from news texts, the following procedure is used to extract structured events: 1. Event Phrase Ext"
D14-1148,kipper-etal-2006-extending,0,0.0190528,"respectively. The structured event is extracted as (Private sector, adds, 114,000 jobs). 2.3 Event Generalization Our goal is to train a model that is able to make predictions based on various expressions of the same event. For example, “Microsoft swallows Nokia’s phone business for $7.2 billion” and “Microsoft purchases Nokia’s phone business” report the same event. To improve the accuracy of our prediction model, we should endow the event extraction algorithm with generalization capacity. To this end, we leverage knowledge from two well-known ontologies, WordNet (Miller, 1995) and VerbNet (Kipper et al., 2006). The process of event generalization consists of two steps. First, we construct a morphological analysis tool based on the WordNet stemmer to extract lemma forms of inflected words. For example, in “Instant view: Private sector adds 114,000 jobs in July.”, the words “adds” and “jobs” are transformed to “add” and “job”, respectively. Second, we generalize each verb to its class name in VerbNet. For example, “add” belongs to the multiply class. After generalization, the event (Private sector, adds, 114,000 jobs) becomes (private sector, multiply class, 114,000 job). Similar methods on event gen"
D14-1148,N09-1031,0,0.144143,"is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of Apple Inc. fell as trading began in New York on Thursday morning, the day after its former CEO Steve Jobs passed away. Google’s stock fell after grim earnings came out. Accurate extraction of events from financial news may play an important role in stock market prediction. However, previous work represents news documents mainly using simple features, such as bags-of-words, noun phrases, and named entities (Lavrenko et al., 2000; Kogan et al., 2009; Luss and d’Aspremont, 2012; Schumaker and Chen, 2009). With these unstructured features, it is difficult to capture key events embedded in financial news, and even more difficult to model the impact of events on stock market prediction. For example, representing the event “Apple has sued Samsung Electronics for copying ‘the look and feel’ 1415 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1415–1425, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics of its iPad tablet and iPhone smartphone.” using term-le"
D14-1148,P13-1008,0,0.0238942,"e might fail to accurately predict the stock price movement of Microsoft, Nokia, Oracle and Google, because they cannot indicate the actor and object of the event. To our knowledge, no effort has been reported in the literature to empirically investigate structured event representations for stock market prediction. 2.2 Event Extraction A main contribution of our work is to extract and use structured events instead of bags-of-words in prediction models. However, structured event extraction can be a costly task, requiring predefined event types and manual event templates (Ji and Grishman, 2008; Li et al., 2013). Partly due to this, the bags-of-words-based document representation has been the mainstream method for a long time. To tackle this issue, we resort to Open IE, extracting event tuples from wide-coverage data with1416 out requiring any human input (e.g. templates). Our system is based on the system of Fader et al. (2011) and the work of Ding et al. (2013); it does not require predefined target event types and labeled training examples. Given a natural language sentence obtained from news texts, the following procedure is used to extract structured events: 1. Event Phrase Extraction. We extrac"
D14-1148,P14-1109,0,0.0332284,"n a debate on whether the market can be predicted. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock ∗ This work was done while the first author was visiting Singapore University of Technology and Design and price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural Language Processing (NLP) techniques to help analyze the effect of web texts on stock market prediction, finding that events reported in financial news are important evidence to stock price movement prediction. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of Apple Inc. fell as trading began in New York on Thursday morning, the day after its former CE"
D14-1148,P13-1086,0,0.18568,"nts. There has been a debate on whether the market can be predicted. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock ∗ This work was done while the first author was visiting Singapore University of Technology and Design and price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural Language Processing (NLP) techniques to help analyze the effect of web texts on stock market prediction, finding that events reported in financial news are important evidence to stock price movement prediction. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of Apple Inc. fell as trading began in New York on Thursday morning, the da"
D14-1148,J11-1005,1,0.270595,"as been the mainstream method for a long time. To tackle this issue, we resort to Open IE, extracting event tuples from wide-coverage data with1416 out requiring any human input (e.g. templates). Our system is based on the system of Fader et al. (2011) and the work of Ding et al. (2013); it does not require predefined target event types and labeled training examples. Given a natural language sentence obtained from news texts, the following procedure is used to extract structured events: 1. Event Phrase Extraction. We extract the predicate verb P of a sentence based on the dependency parser of Zhang and Clark (2011), and then find the longest sequence of words Pv , such that Pv starts at P and satisfies the syntactic and lexical constraints proposed by Fader et al. (2011). The content of these two constraints are as follows: • Syntactic constraint: every multi-word event phrase must begin with a verb, end with a preposition, and be a contiguous sequence of words in the sentence. • Lexical constraint: an event phrase should appear with at least a minimal number of distinct argument pairs in a large corpus. For each event 2. Argument Extraction. phrase Pv identified in the step above, we find the nearest n"
D14-1148,P13-2005,0,0.189604,"nies and governments. There has been a debate on whether the market can be predicted. The Random Walk Theory (Malkiel, 1973) hypothesizes that prices are determined randomly and hence it is impossible to outperform the market. However, with advances of AI, it has been shown empirically that stock ∗ This work was done while the first author was visiting Singapore University of Technology and Design and price movement is predictable (Bondt and Thaler, 1985; Jegadeesh, 1990; Lo and MacKinlay, 1990; Jegadeesh and Titman, 1993). Recent work (Das and Chen, 2007; Tetlock, 2007; Tetlock et al., 2008; Si et al., 2013; Xie et al., 2013; Wang and Hua, 2014) has applied Natural Language Processing (NLP) techniques to help analyze the effect of web texts on stock market prediction, finding that events reported in financial news are important evidence to stock price movement prediction. As news events affect human decisions and the volatility of stock prices is influenced by human trading, it is reasonable to say that events can influence the stock market. Figure 1 shows two pieces of financial news about Apple Inc. and Google Inc., respectively. Shares of Apple Inc. fell as trading began in New York on Thursd"
D14-1201,N07-4013,0,0.0396802,"Missing"
D14-1201,N03-1003,0,0.125028,"Missing"
D14-1201,W08-0336,0,0.0129261,"e, respectively. Definition 6 (semantic pattern) A semantic pattern is the semantic abstraction of a relation. It is the combination of a syntactic pattern and a semantic signature. For instance, the syntactic pattern {nsubj-NR(A) Pred[. ’] prep-u pobjNN(A)}, combined with the semantic signature (Af, Di), results in the semantic pattern {nsubjNR(Af) Pred[.’] prep-u pobj-NN(Di)}. 1871 3.1 3.1.1 Parsing and Base NP Extraction ZORE analyzes the syntactic structures of input texts by applying a pipeline of NLP tools. Each sentence is segmented into a list of words by using the Stanford segmenter (Chang et al., 2008), and parsed by using ZPar (Zhang and Clark, 2011), with POS tags and constituent structures by the CTB standard (Xue et al., 2005). The resulting constituent trees are transformed into projective trees with Stanford dependencies by using the Stanford parser (Chang et al., 2009). Figure 4 shows the parse tree of the sentence in Figure 1. Next, base noun phrases (NPs) are extracted from the dependency tree. Here a base NP is a maximum phrase whose words can only have POS from the first row of Table 1. The head word of a base NP can be either a noun, a pronoun, a number or a measure word (the se"
D14-1201,W09-2307,0,0.0151972,"emantic signature (Af, Di), results in the semantic pattern {nsubjNR(Af) Pred[.’] prep-u pobj-NN(Di)}. 1871 3.1 3.1.1 Parsing and Base NP Extraction ZORE analyzes the syntactic structures of input texts by applying a pipeline of NLP tools. Each sentence is segmented into a list of words by using the Stanford segmenter (Chang et al., 2008), and parsed by using ZPar (Zhang and Clark, 2011), with POS tags and constituent structures by the CTB standard (Xue et al., 2005). The resulting constituent trees are transformed into projective trees with Stanford dependencies by using the Stanford parser (Chang et al., 2009). Figure 4 shows the parse tree of the sentence in Figure 1. Next, base noun phrases (NPs) are extracted from the dependency tree. Here a base NP is a maximum phrase whose words can only have POS from the first row of Table 1. The head word of a base NP can be either a noun, a pronoun, a number or a measure word (the second row of Table 1). The dependency labels within a base NP can only be from the third row of Table 1. Obviously, a base NP does not contain other base NPs, and is also not contained by any other base NP. Figure 2: Architecture of ZORE. 3.1.2 Figure 3: Parsing result of the exa"
D14-1201,I05-2023,0,0.0366978,"c pattern and the logarithmic number of semantic patterns that have the same size, respectively. tion (Nakashole et al., 2012; Moro and Navigli, 2013), and relation extraction can benefit from pattern generalization (Mausam et al., 2012). By using double propagation, not only can we make relation and pattern extraction benefit from each other, but we can also tag relations and patterns with semantic categories in a joint process. There has been a line of research on Chinese relation extraction, where both feature-based (Zhou et al., 2005; Li et al., 2008) and kernel-based (Zhang et al., 2006; Che et al., 2005) methods have been applied. In addition, semantic ontologies such as Extended Cilin have been shown useful for Chinese relation extraction (Liu et al., 2013). However, these studies have focused on traditional IE, with pre-defined relations. In contrast, we investigate ORE for Chinese, finding that semantic ontologies useful for this task also. Tseng et al. (2014) is the only previous research focusing on Chinese ORE. Their system can be considered as a pipeline of word segmentation, POS-tagging and parsing, while our work gives semantic interpretation and explicitly deals with statistical err"
D14-1201,C10-3004,0,0.200845,"racts relations by using syntactic dependency patterns, while associating them with explicit semantic information. An example is shown is Figure 1, where the relation (c n ê (Obama) oÚ (President) , Pred[.’ (graduate)], M Ã (Harvard) { Æ (Law School)) is extracted from the given sentence “cnê (Obama) o Ú (President) . ’ (graduate) u (from) M Ã (Harvard) { Æ (Law School)”, and generalized into the syntactic-semantic pattern {nsubjNR(Af) Pred[. ’ (graduate)] prep-u (from) pobj-NN(Di)}. Here, Af and Di stand for human and institution, respectively, according to a Chinese taxonomy Extended Cilin (Che et al., 2010). Rather than extracting binary relations and then generalizing them into semantic patterns, which most previous work does (Mausam et al., 2012; Nakashole et al., 2012; Moro and Navigli, 2012; Moro and Navigli, 2013), we develop a novel method that extracts relations and patterns simultaneously. A double propagation algorithm is used to make relation and pattern information reinforce each other, so that negative effects from automatic syntactic and semantic analysis errors can be mitigated. In this way, semantic pattern information is leveraged to improve relation extraction. We manually annot"
D14-1201,D11-1142,0,0.754848,"en Information Extraction ZORE is applied to web text to extract general relations and their semantic types. Our definition of relations follow previous work on ORE (Moro and Navigli, 2013), but with language-specific adjustments. In this section, we use the sentence in Figure 1 as an instance to describe the basic definitions for ZORE. Definition 1 (predicate phrase) A predicate phrase is a sequence of words that contains at least one verb or copula, and governs one or more noun phrases syntactically. For instance, a predicate phrase for the sentence in Figure 1 is “.’ (graduate)”. Following Fader et al. (2011), Mausam et al. (2012) and Nakashole et al. (2012), in case of light verb constructions, the verb and its direct object jointly serve as predicate phrase. We do not include prepositions into the predicate phrases. Definition 2 (argument) An argument is a base noun phrase governed by a predicate phrase directly or indirectly with a preposition. For instance, “cnê (Obama) oÚ (President)” and “MÃ (Harvard) { Æ (Law School)” are two arguments of the predicate phrase “.’ (graduate)”. Definition 3 (relation) A binary relation is a triple that consists of the predicate phrase Pred and its two argumen"
D14-1201,D08-1008,0,0.0161974,"Missing"
D14-1201,P08-2023,0,0.018081,"denote the number of relations that belong to a semantic pattern and the logarithmic number of semantic patterns that have the same size, respectively. tion (Nakashole et al., 2012; Moro and Navigli, 2013), and relation extraction can benefit from pattern generalization (Mausam et al., 2012). By using double propagation, not only can we make relation and pattern extraction benefit from each other, but we can also tag relations and patterns with semantic categories in a joint process. There has been a line of research on Chinese relation extraction, where both feature-based (Zhou et al., 2005; Li et al., 2008) and kernel-based (Zhang et al., 2006; Che et al., 2005) methods have been applied. In addition, semantic ontologies such as Extended Cilin have been shown useful for Chinese relation extraction (Liu et al., 2013). However, these studies have focused on traditional IE, with pre-defined relations. In contrast, we investigate ORE for Chinese, finding that semantic ontologies useful for this task also. Tseng et al. (2014) is the only previous research focusing on Chinese ORE. Their system can be considered as a pipeline of word segmentation, POS-tagging and parsing, while our work gives semantic"
D14-1201,E14-4003,0,0.0135171,"g relations and patterns with semantic categories in a joint process. There has been a line of research on Chinese relation extraction, where both feature-based (Zhou et al., 2005; Li et al., 2008) and kernel-based (Zhang et al., 2006; Che et al., 2005) methods have been applied. In addition, semantic ontologies such as Extended Cilin have been shown useful for Chinese relation extraction (Liu et al., 2013). However, these studies have focused on traditional IE, with pre-defined relations. In contrast, we investigate ORE for Chinese, finding that semantic ontologies useful for this task also. Tseng et al. (2014) is the only previous research focusing on Chinese ORE. Their system can be considered as a pipeline of word segmentation, POS-tagging and parsing, while our work gives semantic interpretation and explicitly deals with statistical errors in parsing by a novel double propagation algorithm between patterns and relations. 1878 6 Conclusion and Future Work We presented a Chinese ORE system that integrates relation extraction with semantic pattern generalization by double propagation. Experimental results on two datasets demonstrated the effectiveness of the proposed algorithm. We make the ZORE sys"
D14-1201,D12-1048,0,0.558972,"ure 1, where the relation (c n ê (Obama) oÚ (President) , Pred[.’ (graduate)], M Ã (Harvard) { Æ (Law School)) is extracted from the given sentence “cnê (Obama) o Ú (President) . ’ (graduate) u (from) M Ã (Harvard) { Æ (Law School)”, and generalized into the syntactic-semantic pattern {nsubjNR(Af) Pred[. ’ (graduate)] prep-u (from) pobj-NN(Di)}. Here, Af and Di stand for human and institution, respectively, according to a Chinese taxonomy Extended Cilin (Che et al., 2010). Rather than extracting binary relations and then generalizing them into semantic patterns, which most previous work does (Mausam et al., 2012; Nakashole et al., 2012; Moro and Navigli, 2012; Moro and Navigli, 2013), we develop a novel method that extracts relations and patterns simultaneously. A double propagation algorithm is used to make relation and pattern information reinforce each other, so that negative effects from automatic syntactic and semantic analysis errors can be mitigated. In this way, semantic pattern information is leveraged to improve relation extraction. We manually annotate two sets of data, from news text and Wikipedia, respectively. Experiments on both data sets show that the double propagation algorithm give"
D14-1201,P10-1013,0,0.0371524,"algorithm. Empirical results on two data sets show the effectiveness of the proposed system. 1 Introduction Traditional Information Extraction (IE) systems train extractors for pre-specified relations (Kim and Moldovan, 1993). This approach cannot scale to the web, where target relations are not defined in advance. Open Relation Extraction (ORE) attempts to solve this problem by shallow-parsingbased, syntax-based or semantic-role-based pattern matching without pre-defined relation types, and has achieved great success on open-domain corpora ranging from news to Wikipedia (Banko et al., 2007; Wu and Weld, 2010; Nakashole et al., 2012; Etzioni et al., 2011; Moro and Navigli, 2013). Many NLP and IR applications, including selectional preference learning, commonsense knowledge and entailment rule mining, have benefited from ORE (Ritter et al., 2010). However, most existing ORE systems focus on English, and little research has been reported on other languages. In addition, existing ORE techniques are mainly concerned with the extraction of textual relations, without trying to give semantic analysis, which is the advantage of traditional IE. Our goal in this paper is to present a syntaxbased Chinese (Zh"
D14-1201,D13-1043,0,0.00555274,"Work English has been the major language on which ORE research has been conducted. Previous work on English ORE has evolved from shallowsyntactic (Banko et al., 2007; Fader et al., 2011; Merhav et al., 2012) to full-syntactic (Nakashole et al., 2012; Mausam et al., 2012; Moro and Navigli, 2013; Xu et al., 2013) and semantic (Johansson and Nugues, 2008) systems. It has been shown that a full-syntactic system based on dependency grammar can give significantly better results than shallow syntactic systems based on surface POS-patterns, yet enjoy higher efficiency compared with semantic systems (Mesquita et al., 2013). Our investigation on Chinese ORE takes root in full dependency syntax and is hence able to identify patterns that involve long-range dependencies. Considering the characteristics of the Chinese language, such as the lack of morphology and function words, and the high segmentation and word sense ambiguities, we incorporate semantic ontology information into the design of the system to improve the output quality without sacrificing efficiency. The state-of-the-art systems most closely related to our approach are PATTY (Nakashole et al., 2012) and the system of Moro and Navigli (2013). Both, ho"
D14-1201,D12-1104,0,0.462354,"al results on two data sets show the effectiveness of the proposed system. 1 Introduction Traditional Information Extraction (IE) systems train extractors for pre-specified relations (Kim and Moldovan, 1993). This approach cannot scale to the web, where target relations are not defined in advance. Open Relation Extraction (ORE) attempts to solve this problem by shallow-parsingbased, syntax-based or semantic-role-based pattern matching without pre-defined relation types, and has achieved great success on open-domain corpora ranging from news to Wikipedia (Banko et al., 2007; Wu and Weld, 2010; Nakashole et al., 2012; Etzioni et al., 2011; Moro and Navigli, 2013). Many NLP and IR applications, including selectional preference learning, commonsense knowledge and entailment rule mining, have benefited from ORE (Ritter et al., 2010). However, most existing ORE systems focus on English, and little research has been reported on other languages. In addition, existing ORE techniques are mainly concerned with the extraction of textual relations, without trying to give semantic analysis, which is the advantage of traditional IE. Our goal in this paper is to present a syntaxbased Chinese (Zh) ORE system, ZORE, whic"
D14-1201,N13-1107,0,0.0205451,"Missing"
D14-1201,C00-2136,0,0.0165916,"propagation consists of three steps. In Step 1, monosemic arguments in candidate relations are tagged with a semantic category, such as Af and Di, to obtain semantic patterns. In Step 2 and Step 3, untagged ambiguous and unknown words are tagged by perfect matching and partial matching, respectively. In the end of each step, semantic patterns are generalized from extracted and tagged relations, and then used to help relation tagging in the next step. Because of the two-way information exchange, we call this method double propagation. The method can also be treated as similar to bootstrapping (Yangarber et al., 2000; Qiu et al., 2009). 3.2.1 Step 1: Tagging Monosemic Arguments Each argument in a relation candidate is a base NP. Since base NPs are endocentric, we can take the semantic category of the head word of a base NP as the semantic category of the base NP. In a taxonomy, each word is associated with one or more semantic categories. In this step, however, only monosemic words are tagged, while both ambiguous words and unknown words are left untagged. Most named entities are not included in the taxonomy. However, after POS-tagging, most of them are detected as NR (proper noun). As a result, they are"
D14-1201,J11-1005,1,0.58038,") A semantic pattern is the semantic abstraction of a relation. It is the combination of a syntactic pattern and a semantic signature. For instance, the syntactic pattern {nsubj-NR(A) Pred[. ’] prep-u pobjNN(A)}, combined with the semantic signature (Af, Di), results in the semantic pattern {nsubjNR(Af) Pred[.’] prep-u pobj-NN(Di)}. 1871 3.1 3.1.1 Parsing and Base NP Extraction ZORE analyzes the syntactic structures of input texts by applying a pipeline of NLP tools. Each sentence is segmented into a list of words by using the Stanford segmenter (Chang et al., 2008), and parsed by using ZPar (Zhang and Clark, 2011), with POS tags and constituent structures by the CTB standard (Xue et al., 2005). The resulting constituent trees are transformed into projective trees with Stanford dependencies by using the Stanford parser (Chang et al., 2009). Figure 4 shows the parse tree of the sentence in Figure 1. Next, base noun phrases (NPs) are extracted from the dependency tree. Here a base NP is a maximum phrase whose words can only have POS from the first row of Table 1. The head word of a base NP can be either a noun, a pronoun, a number or a measure word (the second row of Table 1). The dependency labels within"
D14-1201,P06-1104,0,0.00994359,"belong to a semantic pattern and the logarithmic number of semantic patterns that have the same size, respectively. tion (Nakashole et al., 2012; Moro and Navigli, 2013), and relation extraction can benefit from pattern generalization (Mausam et al., 2012). By using double propagation, not only can we make relation and pattern extraction benefit from each other, but we can also tag relations and patterns with semantic categories in a joint process. There has been a line of research on Chinese relation extraction, where both feature-based (Zhou et al., 2005; Li et al., 2008) and kernel-based (Zhang et al., 2006; Che et al., 2005) methods have been applied. In addition, semantic ontologies such as Extended Cilin have been shown useful for Chinese relation extraction (Liu et al., 2013). However, these studies have focused on traditional IE, with pre-defined relations. In contrast, we investigate ORE for Chinese, finding that semantic ontologies useful for this task also. Tseng et al. (2014) is the only previous research focusing on Chinese ORE. Their system can be considered as a pipeline of word segmentation, POS-tagging and parsing, while our work gives semantic interpretation and explicitly deals w"
D14-1201,P05-1053,0,0.045873,"ki. Size and Count denote the number of relations that belong to a semantic pattern and the logarithmic number of semantic patterns that have the same size, respectively. tion (Nakashole et al., 2012; Moro and Navigli, 2013), and relation extraction can benefit from pattern generalization (Mausam et al., 2012). By using double propagation, not only can we make relation and pattern extraction benefit from each other, but we can also tag relations and patterns with semantic categories in a joint process. There has been a line of research on Chinese relation extraction, where both feature-based (Zhou et al., 2005; Li et al., 2008) and kernel-based (Zhang et al., 2006; Che et al., 2005) methods have been applied. In addition, semantic ontologies such as Extended Cilin have been shown useful for Chinese relation extraction (Liu et al., 2013). However, these studies have focused on traditional IE, with pre-defined relations. In contrast, we investigate ORE for Chinese, finding that semantic ontologies useful for this task also. Tseng et al. (2014) is the only previous research focusing on Chinese ORE. Their system can be considered as a pipeline of word segmentation, POS-tagging and parsing, while our wo"
D14-1201,C14-1026,1,0.822118,"s in parsing by a novel double propagation algorithm between patterns and relations. 1878 6 Conclusion and Future Work We presented a Chinese ORE system that integrates relation extraction with semantic pattern generalization by double propagation. Experimental results on two datasets demonstrated the effectiveness of the proposed algorithm. We make the ZORE system, together with the large scale relations and pattern synsets extracted by ZORE, freely available at (https://sourceforge. net/projects/zore/). Another version of ZORE (ZORE-PMT), which is based on the dependency tagset from PMT1.0 (Qiu et al., 2014), is also provided. Our error analysis demonstrates that the quality of syntactic parsing is crucial to the accuracy of syntax-based Chinese ORE. Improvements to syntactic analysis is likely to lead to improved ORE. In addition, the idea of double propagation can be generalized into information propagation between relation extraction and syntactic analysis. We plan to investigate the use of ORE in improving syntactic analysis in future work. Acknowledgments We gratefully acknowledge the invaluable assistance of Ji Ma, Wanxiang Che and Yijia Liu. We also thank the anonymous reviewers for their"
D14-1201,P10-1044,0,0.0162106,"Missing"
D15-1043,J05-3002,0,0.103727,"h of syntactic and N-gram models can benefit from large-scale raw text. Compared with N-gram models, syntactic models give overall better performance, but they require much more training time. In addition, the two models lead to different error distributions in word ordering. A combination of the two models integrates the advantages of each model, achieving the best result in a standard benchmark. 1 Introduction N-gram language models have been used in a wide range of the generation tasks, such as machine translation (Koehn et al., 2003; Chiang, 2007; Galley et al., 2004), text summarization (Barzilay and McKeown, 2005) and realization (Guo et al., 2011). Such models are trained from large-scale raw text, capturing distributions of local word Ngrams, which can be used to improve the fluency of synthesized text. More recently, syntactic language models have been used as a complement or alternative to Ngram language models for machine translation (Charniak et al., 2003; Shen et al., 2008; Schwartz et al., 2011), syntactic analysis (Chen et al., 2012) and tree linearization (Song et al., 2014). Compared with N-gram models, syntactic models capture rich structural information, and can be more effective in improv"
D15-1043,2003.mtsummit-papers.6,0,0.093559,"the best result in a standard benchmark. 1 Introduction N-gram language models have been used in a wide range of the generation tasks, such as machine translation (Koehn et al., 2003; Chiang, 2007; Galley et al., 2004), text summarization (Barzilay and McKeown, 2005) and realization (Guo et al., 2011). Such models are trained from large-scale raw text, capturing distributions of local word Ngrams, which can be used to improve the fluency of synthesized text. More recently, syntactic language models have been used as a complement or alternative to Ngram language models for machine translation (Charniak et al., 2003; Shen et al., 2008; Schwartz et al., 2011), syntactic analysis (Chen et al., 2012) and tree linearization (Song et al., 2014). Compared with N-gram models, syntactic models capture rich structural information, and can be more effective in improving the fluency of large constituents, long-range dependencies and overall sentential grammaticality. However, Syntactic 369 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 369–378, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Initial State ([], set(1...n), Ø) st"
D15-1043,P12-1023,0,0.121891,"ch is to order a set of input words into a grammatical and fluent sentence. The task can be regarded as an abstract language modeling problem, although methods have been explored extending it for tree linearization (Zhang, 2013), broader text generation (Song et al., 2014) and machine translation (Zhang et al., 2014). We choose the model of Liu et al.(2015) as the syntactic language model. There has been two main types of syntactic language models in the literature, the first being relatively more oriented to syntactic structure, without an explicit emphasis on word orders (Shen et al., 2008; Chen et al., 2012). As a result, this type of syntactic language models are typically used jointly with N-gram model for text-to-text tasks. The second type models syntactic structures incrementally, thereby can be used to directly score surface orders (Schwartz et al., 2011; Liu et al., 2015). We choose the discriminative model of Liu et al. (2015), which gives state-of-the-art results for word ordering. We try to answer the following research questions by comparing the syntactic model and the Ngram model using the same search algorithm. • What is the influence of automaticallyparsed training data on the perfo"
D15-1043,D11-1106,1,0.666035,"The system repeatedly applies transition actions to consume words from ρ and construct output on σ. Figure 1 shows the deduction system, where ρ is unordered and any word in ρ can be shifted onto the stack σ. The set of actions are SHIFT, L- ARC and R- ARC. The SHIFT actions add a word to the stack. For the L- ARC and R- ARC actions, new arcs {j ← i} and {j → i} are constructed respectively. Under these possible actions, the unordered word set “potatoes0 Tom1 likes2 ” is generated as shown in Figure 2, and the result is “Tom1 ←likes2 →potatoes0 ”. We apply the learning and search framework of Zhang and Clark (2011a). Pseudocode of the search algorithm is shown in Algorithm 1. [] refers to an empty stack, and set(1...n) represents the full set of input words W and n is the number of distinct words. candidates stores possible states, and agenda stores temporary states transited from possible actions. GETACTIONS generates a set of possible actions depending on the current state s. APPLY generates a new state by applying action on the current state s. N - BEST produces the top k candidates in agenda. Finally, the algorithm returns the highest-score state best in the agenda. A global linear model is used to"
D15-1043,E14-1028,0,0.229311,"Missing"
D15-1043,N10-1031,0,0.018749,"xperiments, we use ZPar5 (Zhu et al., 2013) for automatic constituent parsing. In order to study the influence of parsing accuracy of the training data, we also use ten-fold jackknifing to construct WSJ training data with different accuracies. The data is randomly split into ten equal-size subsets, and each subset is automatically parsed with a parser trained on the other Evaluation metrics We follow previous work and use the BLEU metric (Papineni et al., 2002) for evaluation. Since BLEU only scores N-gram precisions, it can be in favour of N-gram language models. We additionally use METEOR3 (Denkowski and Lavie, 2010) to evaluate the system performances. The BLEU metric measures the fluency of generated sentence without considering long range ordering. The METEOR metric can potentially fix this problem using a set of mapping between generated sentences and references to evaluate distortion. The following example illustrates the difference between BLEU and METEOR on long range reordering, where the reference is (1) [The document is necessary for developer ,]0 [so you can not follow this document to get right options .]1 and the generated output sentence is (2) [so you can not follow this document to get rig"
D15-1043,E12-1075,1,0.80949,"Missing"
D15-1043,N04-1035,0,0.0467192,"vely small impact on syntactic models. Both of syntactic and N-gram models can benefit from large-scale raw text. Compared with N-gram models, syntactic models give overall better performance, but they require much more training time. In addition, the two models lead to different error distributions in word ordering. A combination of the two models integrates the advantages of each model, achieving the best result in a standard benchmark. 1 Introduction N-gram language models have been used in a wide range of the generation tasks, such as machine translation (Koehn et al., 2003; Chiang, 2007; Galley et al., 2004), text summarization (Barzilay and McKeown, 2005) and realization (Guo et al., 2011). Such models are trained from large-scale raw text, capturing distributions of local word Ngrams, which can be used to improve the fluency of synthesized text. More recently, syntactic language models have been used as a complement or alternative to Ngram language models for machine translation (Charniak et al., 2003; Shen et al., 2008; Schwartz et al., 2011), syntactic analysis (Chen et al., 2012) and tree linearization (Song et al., 2014). Compared with N-gram models, syntactic models capture rich structural"
D15-1043,D14-1021,1,0.628344,"training, which are expensive to obtain manually. In addition, they can be slower compared to Ngram models. In this paper, we make an empirical comparison between syntactic and N-gram language models on the task of word ordering (Wan et al., 2009; Zhang and Clark, 2011a; De Gispert et al., 2014), which is to order a set of input words into a grammatical and fluent sentence. The task can be regarded as an abstract language modeling problem, although methods have been explored extending it for tree linearization (Zhang, 2013), broader text generation (Song et al., 2014) and machine translation (Zhang et al., 2014). We choose the model of Liu et al.(2015) as the syntactic language model. There has been two main types of syntactic language models in the literature, the first being relatively more oriented to syntactic structure, without an explicit emphasis on word orders (Shen et al., 2008; Chen et al., 2012). As a result, this type of syntactic language models are typically used jointly with N-gram model for text-to-text tasks. The second type models syntactic structures incrementally, thereby can be used to directly score surface orders (Schwartz et al., 2011; Liu et al., 2015). We choose the discrimi"
D15-1043,N03-1017,0,0.026263,"-parsed training data has a relatively small impact on syntactic models. Both of syntactic and N-gram models can benefit from large-scale raw text. Compared with N-gram models, syntactic models give overall better performance, but they require much more training time. In addition, the two models lead to different error distributions in word ordering. A combination of the two models integrates the advantages of each model, achieving the best result in a standard benchmark. 1 Introduction N-gram language models have been used in a wide range of the generation tasks, such as machine translation (Koehn et al., 2003; Chiang, 2007; Galley et al., 2004), text summarization (Barzilay and McKeown, 2005) and realization (Guo et al., 2011). Such models are trained from large-scale raw text, capturing distributions of local word Ngrams, which can be used to improve the fluency of synthesized text. More recently, syntactic language models have been used as a complement or alternative to Ngram language models for machine translation (Charniak et al., 2003; Shen et al., 2008; Schwartz et al., 2011), syntactic analysis (Chen et al., 2012) and tree linearization (Song et al., 2014). Compared with N-gram models, synt"
D15-1043,P13-1043,1,0.88399,"n’ and ‘a big cat’) as a single word. This avoids unnecessary ambiguities in combination between their subcomponents. The syntactic model requires that the training sentences have syntactic dependency structure. However, only the WSJ data contains goldstandard annotations. In order to obtain automatically annotated dependency trees, we train a constituent parser using the gold-standard bracketed sentences from WSJ, and automatically parse the Giga Word data. The results are turned into dependency trees using Penn2Malt4 , after base noun phrases are extracted. In our experiments, we use ZPar5 (Zhu et al., 2013) for automatic constituent parsing. In order to study the influence of parsing accuracy of the training data, we also use ten-fold jackknifing to construct WSJ training data with different accuracies. The data is randomly split into ten equal-size subsets, and each subset is automatically parsed with a parser trained on the other Evaluation metrics We follow previous work and use the BLEU metric (Papineni et al., 2002) for evaluation. Since BLEU only scores N-gram precisions, it can be in favour of N-gram language models. We additionally use METEOR3 (Denkowski and Lavie, 2010) to evaluate the"
D15-1043,N15-1012,1,0.830866,"machine translation (Zhang et al., 2014). We choose the model of Liu et al.(2015) as the syntactic language model. There has been two main types of syntactic language models in the literature, the first being relatively more oriented to syntactic structure, without an explicit emphasis on word orders (Shen et al., 2008; Chen et al., 2012). As a result, this type of syntactic language models are typically used jointly with N-gram model for text-to-text tasks. The second type models syntactic structures incrementally, thereby can be used to directly score surface orders (Schwartz et al., 2011; Liu et al., 2015). We choose the discriminative model of Liu et al. (2015), which gives state-of-the-art results for word ordering. We try to answer the following research questions by comparing the syntactic model and the Ngram model using the same search algorithm. • What is the influence of automaticallyparsed training data on the performance of syntactic models. Because manual syntactic annotations are relatively limited and highly expensive, it is necessary to use large-scale automatically-parsed sentences for training syntactic language models. As a result, the syntactic structures that a word ordering s"
D15-1043,J93-2004,0,0.0503521,"Missing"
D15-1043,P02-1040,0,0.0921338,", and automatically parse the Giga Word data. The results are turned into dependency trees using Penn2Malt4 , after base noun phrases are extracted. In our experiments, we use ZPar5 (Zhu et al., 2013) for automatic constituent parsing. In order to study the influence of parsing accuracy of the training data, we also use ten-fold jackknifing to construct WSJ training data with different accuracies. The data is randomly split into ten equal-size subsets, and each subset is automatically parsed with a parser trained on the other Evaluation metrics We follow previous work and use the BLEU metric (Papineni et al., 2002) for evaluation. Since BLEU only scores N-gram precisions, it can be in favour of N-gram language models. We additionally use METEOR3 (Denkowski and Lavie, 2010) to evaluate the system performances. The BLEU metric measures the fluency of generated sentence without considering long range ordering. The METEOR metric can potentially fix this problem using a set of mapping between generated sentences and references to evaluate distortion. The following example illustrates the difference between BLEU and METEOR on long range reordering, where the reference is (1) [The document is necessary for dev"
D15-1043,P11-1063,0,0.103437,"(Song et al., 2014) and machine translation (Zhang et al., 2014). We choose the model of Liu et al.(2015) as the syntactic language model. There has been two main types of syntactic language models in the literature, the first being relatively more oriented to syntactic structure, without an explicit emphasis on word orders (Shen et al., 2008; Chen et al., 2012). As a result, this type of syntactic language models are typically used jointly with N-gram model for text-to-text tasks. The second type models syntactic structures incrementally, thereby can be used to directly score surface orders (Schwartz et al., 2011; Liu et al., 2015). We choose the discriminative model of Liu et al. (2015), which gives state-of-the-art results for word ordering. We try to answer the following research questions by comparing the syntactic model and the Ngram model using the same search algorithm. • What is the influence of automaticallyparsed training data on the performance of syntactic models. Because manual syntactic annotations are relatively limited and highly expensive, it is necessary to use large-scale automatically-parsed sentences for training syntactic language models. As a result, the syntactic structures tha"
D15-1043,P08-1066,0,0.186355,"et al., 2014), which is to order a set of input words into a grammatical and fluent sentence. The task can be regarded as an abstract language modeling problem, although methods have been explored extending it for tree linearization (Zhang, 2013), broader text generation (Song et al., 2014) and machine translation (Zhang et al., 2014). We choose the model of Liu et al.(2015) as the syntactic language model. There has been two main types of syntactic language models in the literature, the first being relatively more oriented to syntactic structure, without an explicit emphasis on word orders (Shen et al., 2008; Chen et al., 2012). As a result, this type of syntactic language models are typically used jointly with N-gram model for text-to-text tasks. The second type models syntactic structures incrementally, thereby can be used to directly score surface orders (Schwartz et al., 2011; Liu et al., 2015). We choose the discriminative model of Liu et al. (2015), which gives state-of-the-art results for word ordering. We try to answer the following research questions by comparing the syntactic model and the Ngram model using the same search algorithm. • What is the influence of automaticallyparsed traini"
D15-1043,E09-1097,0,0.384736,"d Xinhua News Agency (XIN) subsets of the English Giga Word Fifth Edition (Parker et al., 2011). As the development data, we use WSJ section 0 for parameter tuning. For testing, we use data from various domain, which consist of WSJ section 23, Washington Post/Bloomberg(WPB) subsets of the English Giga Word Fifth Edition and SANCL blog data, as shown in Table 2. Example sentence in various test domains are shown in Table 3. 3.3 Data preparation For all the experiments, we assume that the input is a bag of words without order, and the output is a fully ordered sentence. Following previous work (Wan et al., 2009; Zhang, 2013; Liu et al., 2015), we treat base noun phrases (i.e. noun phrases do not contains other noun phrases, such as ‘Pierre Vinken’ and ‘a big cat’) as a single word. This avoids unnecessary ambiguities in combination between their subcomponents. The syntactic model requires that the training sentences have syntactic dependency structure. However, only the WSJ data contains goldstandard annotations. In order to obtain automatically annotated dependency trees, we train a constituent parser using the gold-standard bracketed sentences from WSJ, and automatically parse the Giga Word data."
D15-1043,J11-1005,1,0.942788,"The system repeatedly applies transition actions to consume words from ρ and construct output on σ. Figure 1 shows the deduction system, where ρ is unordered and any word in ρ can be shifted onto the stack σ. The set of actions are SHIFT, L- ARC and R- ARC. The SHIFT actions add a word to the stack. For the L- ARC and R- ARC actions, new arcs {j ← i} and {j → i} are constructed respectively. Under these possible actions, the unordered word set “potatoes0 Tom1 likes2 ” is generated as shown in Figure 2, and the result is “Tom1 ←likes2 →potatoes0 ”. We apply the learning and search framework of Zhang and Clark (2011a). Pseudocode of the search algorithm is shown in Algorithm 1. [] refers to an empty stack, and set(1...n) represents the full set of input words W and n is the number of distinct words. candidates stores possible states, and agenda stores temporary states transited from possible actions. GETACTIONS generates a set of possible actions depending on the current state s. APPLY generates a new state by applying action on the current state s. N - BEST produces the top k candidates in agenda. Finally, the algorithm returns the highest-score state best in the agenda. A global linear model is used to"
D15-1043,C08-1038,0,\N,Missing
D15-1043,P04-1015,0,\N,Missing
D15-1043,J07-2003,0,\N,Missing
D15-1073,C14-1008,0,0.0907572,"erpool and Chelsea in tweets. 612 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 612–621, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. other hand, there do exist cases where entity boundaries and sentiment classes reinforce each other. For example, in a tweet such as ‘I like X.’, the contextual pattern indicate both a positive sentiment and an entity in the place of X. Recently, neural network models have been increasingly used for sentiment analysis (Socher et al., 2013; Kalchbrenner et al., 2014; dos Santos and Gatti, 2014), achieving highly competitive results, which show large potentials of neural network models for this task. The main advantages of neural networks are two-fold. First, neural models use real-valued hidden layers to automatically learn feature combinations, which can capture complex semantic information that are difficult to express using traditional discrete manual features. Second, neural networks take distributed word embeddings as inputs, which can be trained from large-scale raw text, thus alleviating the scarcity of annotated data to some extent. In this paper, we exploit structured neura"
D15-1073,D14-1012,0,0.0108147,"potentials remain the same as the baseline. By Integrated Models Gleaning different sources of information, neural features and discrete linear features comple616 where (~xn , ~yn )|N n=1 are the set of training examples, λ is a regularization parameter, and l(~xn , ~yn , Θ) is the loss function towards one example (~xn , ~yn ). The loss function is defined as: ments each other. As a result, a model that integrates both features can potentially achieve performance improvements. Most work attempts to add neural word embeddings into a discrete linear model (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014), or add discreted features into a neural model (Ma et al., 2014). We make a novel combination of the discrete models and the neural models by integrating both types of inputs into a same CRF framework.2 The architectures of the integrated models are shown in Figure 5. The main difference between Figure 5 and Figure 3 is the input layer. The integrated model takes both continuous word embeddings, which are shown in grey nodes, and discrete manual features, which are shown in black or white nodes, as the input. A separate hidden layer is given to each type of input nodes, with the hidden layer"
D15-1073,P11-1016,0,0.63938,"acting the mentions to all persons and organizations, together with the sentiments towards each mention, from a news archive or a collection of novels. There are two sub tasks in targeted sentiment analysis, namely entity recognition and sentiment classification for each entity mention which apply to both scenarios above. In scenario (1), entity recognition is relatively trivial, and can typically be achieved by pattern matching. Partly due to this reason, most previous work has addressed targeted sentiment analysis as a pure classification task, assuming that target mentions have been given (Jiang et al., 2011; Chen et al., 2012; Dong et al., 2014; Vo and Zhang, 2015). For scenario (2), a named entity recognition (NER) system can be used to extract targets, before the same targeted sentiment classification algorithms are applied. There has also been work that concentrates on extracting opinion targets (Jin et al., 2009; Jakob and Gurevych, 2010). In both cases, the data in Figure 1 can be used for training sentiment classifiers. Mitchell et al. (2013) took a different approach, extracting named entities and their sentiment classes jointly. They model the joint task Introduction Targeted sentiment a"
D15-1073,J92-4003,0,0.0963337,"label dependencies, which are constraints for decoding. For example, if yi = O, then zi must be φ. We apply Viterbi decoding for all tasks, and training is performed using a max-margin objective, which is discussed in Section 6. Our training algorithm is different from that of Mitchell et al. (2013), but gives similar discrete CRF accuracies in our experiments. Wang and Mori (2009) also applied a max-margin trainig strategy to train CRF models. The set of features is taken from Mitchell et al. (2013) without changes, as shown in Table 1. Here the cluster features refer to Brown word clusters (Brown et al., 1992). n o Ψ(~x, yi ) = exp θ~ · f~(~x, yi ) , |x| Y |x| XY Neural Models We extend the discrete baseline system with two salient changes, which are illustrated in Figure 4. First, the input discrete features are replaced with continuous word embeddings. Each node in the input takes a real value between 0 and 1, as represented by grey nodes in Figure 4. Second, a hidden , 1 Note the difference between neural and NULL sentiments. The former indicates that a target does not bare any sentiment, and the latter simply means that the term is not a part of a target. 615 ··· O   B   I O ···  B  ··"
D15-1073,P14-1062,0,0.0178598,"towards Manchester United, Liverpool and Chelsea in tweets. 612 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 612–621, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. other hand, there do exist cases where entity boundaries and sentiment classes reinforce each other. For example, in a tweet such as ‘I like X.’, the contextual pattern indicate both a positive sentiment and an entity in the place of X. Recently, neural network models have been increasingly used for sentiment analysis (Socher et al., 2013; Kalchbrenner et al., 2014; dos Santos and Gatti, 2014), achieving highly competitive results, which show large potentials of neural network models for this task. The main advantages of neural networks are two-fold. First, neural models use real-valued hidden layers to automatically learn feature combinations, which can capture complex semantic information that are difficult to express using traditional discrete manual features. Second, neural networks take distributed word embeddings as inputs, which can be trained from large-scale raw text, thus alleviating the scarcity of annotated data to some extent. In this paper"
D15-1073,P14-1038,0,0.00815065,"n targeted sentiment analysis, identifying and classifying all targets in a sentence simultaneously. As discussed in the introduction, targeted sentiment analysis falls into two main settings. The first is targeted sentiment classification, assuming that entity mentions are given. Most previous work fall under this category (Jiang et al., 2011; Chen et al., 2012; Dong et al., 2014). The second is open domain targeted sentiment, which has been discussed by Mitchell et al. (2013). The task jointly extracts entities and sentiment classes, and is analogous to joint entity and relation extraction (Li and Ji, 2014) in that both are information extraction tasks with multi-label outputs. Our work is related to the line of work on using neural networks for sentiment analysis. Socher et al. (2011) use recursive auto-encoders for sentiment analysis on the sentence level. They further extend the method to a syntactic treebank annotated with sentiment labels (Socher et al., 2013). More recently, Kalchbrenner et al. (2014) use a dynamic pooling network to include the structure 3 Discrete CRF Baselines As shown in Figure 2, the input ~x to our tasks is a word sequence. Assuming no external resources, there is no"
D15-1073,C10-1074,0,0.0944627,"Missing"
D15-1073,P14-1014,1,0.824114,"leaning different sources of information, neural features and discrete linear features comple616 where (~xn , ~yn )|N n=1 are the set of training examples, λ is a regularization parameter, and l(~xn , ~yn , Θ) is the loss function towards one example (~xn , ~yn ). The loss function is defined as: ments each other. As a result, a model that integrates both features can potentially achieve performance improvements. Most work attempts to add neural word embeddings into a discrete linear model (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014), or add discreted features into a neural model (Ma et al., 2014). We make a novel combination of the discrete models and the neural models by integrating both types of inputs into a same CRF framework.2 The architectures of the integrated models are shown in Figure 5. The main difference between Figure 5 and Figure 3 is the input layer. The integrated model takes both continuous word embeddings, which are shown in grey nodes, and discrete manual features, which are shown in black or white nodes, as the input. A separate hidden layer is given to each type of input nodes, with the hidden layer for the embeddings being the same as the neural baseline:  ~hi ="
D15-1073,D13-1171,0,0.51862,"Missing"
D15-1073,W04-3236,0,0.0154897,"Missing"
D15-1073,P13-1161,0,0.0372023,"e HMM to extract opinionbaring expressions and opinion targets. Li et al. (2010) improve the results by using CRF to identify the opinion expressions and targets jointly. The task is sometimes referred to as fine-grained sentiment analysis (Wiebe et al., 2005). It is different from our setting in that the predicate-argument relation between opinion-baring expressions and target entities are not explicitly modeled. +   + ···  ··· ··· ··· my (O) baby (B) Farah (I) step 2: sentiment (a) pipeline ··· Φ   ··· O +     B + ···    I ···  ··· ··· ··· my baby Farah (b) joint Recently, Yang and Cardie (2013) use CRF to extract opinion-baring expressions, opinion holders and opinion targets simultaneously. Their method is also centralized on opinion-baring expressions and therefore in line with Jin et al. (2009) and Li et al. (2010). In contrast, targeted sentiment analysis directly studies entity mentions and the sentiment on each mention, without explicitly modeling the way in which the opinion is expressed. As a result, our task is more useful for applications such as broad-stroke reputation management, but offer less fine-grained operational insight. It requires less fine-grained manual annota"
D15-1073,D11-1014,0,0.0511645,"main settings. The first is targeted sentiment classification, assuming that entity mentions are given. Most previous work fall under this category (Jiang et al., 2011; Chen et al., 2012; Dong et al., 2014). The second is open domain targeted sentiment, which has been discussed by Mitchell et al. (2013). The task jointly extracts entities and sentiment classes, and is analogous to joint entity and relation extraction (Li and Ji, 2014) in that both are information extraction tasks with multi-label outputs. Our work is related to the line of work on using neural networks for sentiment analysis. Socher et al. (2011) use recursive auto-encoders for sentiment analysis on the sentence level. They further extend the method to a syntactic treebank annotated with sentiment labels (Socher et al., 2013). More recently, Kalchbrenner et al. (2014) use a dynamic pooling network to include the structure 3 Discrete CRF Baselines As shown in Figure 2, the input ~x to our tasks is a word sequence. Assuming no external resources, there is no POS given to each input word xi . For 614 surface features word identity; word length; message length; punctuation characters; has digit; has dash; is lower case; is 3 or 4 letters;"
D15-1073,N13-1063,0,0.0127686,"the edge clique potentials remain the same as the baseline. By Integrated Models Gleaning different sources of information, neural features and discrete linear features comple616 where (~xn , ~yn )|N n=1 are the set of training examples, λ is a regularization parameter, and l(~xn , ~yn , Θ) is the loss function towards one example (~xn , ~yn ). The loss function is defined as: ments each other. As a result, a model that integrates both features can potentially achieve performance improvements. Most work attempts to add neural word embeddings into a discrete linear model (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014), or add discreted features into a neural model (Ma et al., 2014). We make a novel combination of the discrete models and the neural models by integrating both types of inputs into a same CRF framework.2 The architectures of the integrated models are shown in Figure 5. The main difference between Figure 5 and Figure 3 is the input layer. The integrated model takes both continuous word embeddings, which are shown in grey nodes, and discrete manual features, which are shown in black or white nodes, as the input. A separate hidden layer is given to each type of input nodes, wit"
D15-1073,D13-1170,0,0.0153076,"xts, or the sentiment towards Manchester United, Liverpool and Chelsea in tweets. 612 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 612–621, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. other hand, there do exist cases where entity boundaries and sentiment classes reinforce each other. For example, in a tweet such as ‘I like X.’, the contextual pattern indicate both a positive sentiment and an entity in the place of X. Recently, neural network models have been increasingly used for sentiment analysis (Socher et al., 2013; Kalchbrenner et al., 2014; dos Santos and Gatti, 2014), achieving highly competitive results, which show large potentials of neural network models for this task. The main advantages of neural networks are two-fold. First, neural models use real-valued hidden layers to automatically learn feature combinations, which can capture complex semantic information that are difficult to express using traditional discrete manual features. Second, neural networks take distributed word embeddings as inputs, which can be trained from large-scale raw text, thus alleviating the scarcity of annotated data to"
D15-1073,P08-1101,1,0.324745,"and the middle of a negative entity, respectively. The collapsed labels allow joint entity recognition and sentiment classification to be achieved using a standard sequence labeler. Mitchell et al. (2013) compare a pipeline model, a joint model and a collapsed model under the same conditional random field (CRF) framework, finding that the pipeline method outperforms the joint model on a tweet dataset. Intuitively, the interaction between entity boundaries and sentiment classes might not be as strong as that between more closely-coupled sources of information, such as word boundaries and POS (Zhang and Clark, 2008), or named entities and constituents (Finkel and Manning, 2009), for which joint models significantly outperform pipeline models. On the 2 Related Work Targeted sentiment analysis is closely related prior work on aspect-oriented (Hu and Liu, 2004), feature-oriented (Popescu and Etzioni, 2007) and topic-oriented (Yi et al., 2003) sentiment analysis. These related tasks are typically concentrated on product review settings. In contrast, targeted sentiment analysis has a more general setting. Recently, Wang et al. (2011) proposed a topicoriented model, which extracts sentiments towards certain to"
D15-1073,C14-1127,0,0.0637696,"In contrast, targeted sentiment analysis directly studies entity mentions and the sentiment on each mention, without explicitly modeling the way in which the opinion is expressed. As a result, our task is more useful for applications such as broad-stroke reputation management, but offer less fine-grained operational insight. It requires less fine-grained manual annotation. ··· O   B+   I+ ···  ··· ··· ··· my baby Farah (c) collapsed Figure 3: Discrete CRF models for pipeline, joint and collapsed targeted sentiment labeling. of a sentence automatically, before classifying its sentiment. Zhou et al. (2014) apply deep belief networks for semi-supervised sentiment classification. dos Santos and Gatti (2014) use deep convolution neural networks with rich features to classify sentiments over tweets and movie reviews. These methods use different models to represent sentence structures, performing sentiment analysis on the sentence level, without modeling targets. Dong et al. (2014) perform targeted sentiment classification by using a recursive neural network to model the transmission of sentiment signal from opinion baring expressions to a target. They assume that the target mention is given, and pe"
D15-1073,P10-1040,0,0.30476,"model parameter, and the edge clique potentials remain the same as the baseline. By Integrated Models Gleaning different sources of information, neural features and discrete linear features comple616 where (~xn , ~yn )|N n=1 are the set of training examples, λ is a regularization parameter, and l(~xn , ~yn , Θ) is the loss function towards one example (~xn , ~yn ). The loss function is defined as: ments each other. As a result, a model that integrates both features can potentially achieve performance improvements. Most work attempts to add neural word embeddings into a discrete linear model (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014), or add discreted features into a neural model (Ma et al., 2014). We make a novel combination of the discrete models and the neural models by integrating both types of inputs into a same CRF framework.2 The architectures of the integrated models are shown in Figure 5. The main difference between Figure 5 and Figure 3 is the input layer. The integrated model takes both continuous word embeddings, which are shown in grey nodes, and discrete manual features, which are shown in black or white nodes, as the input. A separate hidden layer is given to each type of"
D15-1073,I13-1183,0,0.0195219,"Missing"
D15-1073,D10-1101,0,\N,Missing
D15-1073,N09-1037,0,\N,Missing
D15-1073,H05-2017,0,\N,Missing
D15-1073,H05-1043,0,\N,Missing
D15-1073,P14-2009,0,\N,Missing
D15-1153,P81-1022,0,0.738436,"Missing"
D15-1153,D14-1082,0,0.627029,"f a traditional linear sparse feature model and a multi-layer neural network model for deterministic transition-based dependency parsing, by integrating the sparse features into the neural model. Correlations are drawn between the hybrid model and previous work on integrating word embedding features into a discrete linear model. By analyzing the results of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods. 1 ··· (a) discrete linear (eg. MaltParser) ··· ··· (b) continuous NN (eg. Chen and Manning (2014)) ··· (c) Turian et al. (2010) ··· ··· transform (d) Guo et al. (2014) Introduction ··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Determinis"
D15-1153,P13-1104,0,0.0355427,"Missing"
D15-1153,P15-1033,0,0.0878404,"Missing"
D15-1153,Q14-1010,0,0.015097,"nstruct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 (a) and (b). ··· (e) this paper Figure 1: Five deterministic transition-based parsers with discrete and continuous features. A neural network model takes continuous vector representations of words as inputs, which can be pre-trai"
D15-1153,P15-2042,0,0.0265467,"Missing"
D15-1153,D14-1012,0,0.357814,"model for deterministic transition-based dependency parsing, by integrating the sparse features into the neural model. Correlations are drawn between the hybrid model and previous work on integrating word embedding features into a discrete linear model. By analyzing the results of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods. 1 ··· (a) discrete linear (eg. MaltParser) ··· ··· (b) continuous NN (eg. Chen and Manning (2014)) ··· (c) Turian et al. (2010) ··· ··· transform (d) Guo et al. (2014) Introduction ··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy loca"
D15-1153,W13-3518,0,0.0120721,"d parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 (a) and (b). ··· (e) this paper Figure 1: Five deterministic transition-based parsers with discrete and continuous features. A neural network model takes continuous vector representations of words as inputs"
D15-1153,P14-2128,1,0.902091,"d significantly. In addition, deterministic parsers use standard neural classifiers, which allows isolated study of feature influences. 3 Following Chen and Manning (2014), training of all the models using a cross-entropy loss objective with a L2-regularization, and mini-batched AdaGrad (Duchi et al., 2011). We unify below the five deterministic parsing models in Figure 1. 3.1 Baseline linear (L) We build a baseline linear model using logistic regression (Figure 1(a)). Given a parsing state x, a vector of discrete features Φd (x) is extracted according to the arc-standard feature templates of Ma et al. (2014a), which is based on the arc-eager templates of Zhang and Nivre (2011). The score of an action a is defined by → −  Score(a) = σ Φd (x) · θ d,a , where σ represents the sigmoid activation func→ − tion, θ d is the set of model parameters, denoting the feature weights with respect to actions, a can be SHIFT, LEFT(l) and RIGHT(l). 3.2 Baseline Neural (NN) We take the Neural model of Chen and Manning (2014) as another baseline (Figure 1(b)). Given a parsing state x, the words are first mapped into continuous vectors by using a set of pre-trained word embeddings. Denote the mapping as Φe (x). In"
D15-1153,P14-1014,1,0.896451,"d significantly. In addition, deterministic parsers use standard neural classifiers, which allows isolated study of feature influences. 3 Following Chen and Manning (2014), training of all the models using a cross-entropy loss objective with a L2-regularization, and mini-batched AdaGrad (Duchi et al., 2011). We unify below the five deterministic parsing models in Figure 1. 3.1 Baseline linear (L) We build a baseline linear model using logistic regression (Figure 1(a)). Given a parsing state x, a vector of discrete features Φd (x) is extracted according to the arc-standard feature templates of Ma et al. (2014a), which is based on the arc-eager templates of Zhang and Nivre (2011). The score of an action a is defined by → −  Score(a) = σ Φd (x) · θ d,a , where σ represents the sigmoid activation func→ − tion, θ d is the set of model parameters, denoting the feature weights with respect to actions, a can be SHIFT, LEFT(l) and RIGHT(l). 3.2 Baseline Neural (NN) We take the Neural model of Chen and Manning (2014) as another baseline (Figure 1(b)). Given a parsing state x, the words are first mapped into continuous vectors by using a set of pre-trained word embeddings. Denote the mapping as Φe (x). In"
D15-1153,W04-2407,0,0.0572757,"··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 (a) and (b). ··· (e) this paper Figure 1: Five deterministic transition-based parsers with discrete and continuous features. A neural network model takes continuous vector representat"
D15-1153,J08-4003,0,0.273943,"inear model. By analyzing the results of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods. 1 ··· (a) discrete linear (eg. MaltParser) ··· ··· (b) continuous NN (eg. Chen and Manning (2014)) ··· (c) Turian et al. (2010) ··· ··· transform (d) Guo et al. (2014) Introduction ··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action c"
D15-1153,D12-1110,0,0.0536815,"he Penn Treebank. Their work shows that more sophisticated neural network structures with long term memories can significantly improve the accuracy over local classifiers. Their work is orthogonal to ours. 6 Related Work As discussed in the introduction, our work is related to previous work on integrating word embeddings into discrete models (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014). Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering). There has been recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014). Both models have essentially the same structure as our model. In contrast to their work, we systematically compare various ways to integrate discrete and continuous features, for the dependency parsing task. Our model is"
D15-1153,P13-1045,0,0.104511,"et al. (2014), we use compounded clusters learnt by K-means algorithm of different granularities. 3.5 #Sent 30,060 1,336 1,640 1,744 1,195 1,906 NN Turian +T This (d) reviews Figure 2: Dev results on fine-tuning (UAS). Following Chen and Manning (2014), we use the pre-trained word embedding released by Collobert et al. (2011), and set h = 200 for the hidden layer size, λ = 10−8 for L2 regularization, and α = 0.01 for the initial learning rate of Adagrad. 4.2 Development Results Fine-tuning of embeddings. Chen and Manning (2014) fine-tune word embeddings in supervised training, consistent with Socher et al. (2013). Intuitively, fine-tuning embeddings allows in-vocabulary words to join the parameter space, thereby giving better fitting to in-domain data. However, it also forfeits the benefit of large-scale pre-training, because out-of-vocabulary (OOV) words do not have their embeddings fine-tuned. In this sense, the method of Chen and Manning resembles a traditional supervised sparse linear model, which can be weak on OOV. On the other hand, the semi-supervised learning methods such as Turian et al. (2010) and Guo et al. (2014), do not fine-tune the word embeddings. Embeddings are taken as inputs rather"
D15-1153,P14-1146,0,0.0317137,"ir work shows that more sophisticated neural network structures with long term memories can significantly improve the accuracy over local classifiers. Their work is orthogonal to ours. 6 Related Work As discussed in the introduction, our work is related to previous work on integrating word embeddings into discrete models (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014). Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering). There has been recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014). Both models have essentially the same structure as our model. In contrast to their work, we systematically compare various ways to integrate discrete and continuous features, for the dependency parsing task. Our model is also different fro"
D15-1153,D14-1101,0,0.0197133,"uo et al., 2014). Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering). There has been recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014). Both models have essentially the same structure as our model. In contrast to their work, we systematically compare various ways to integrate discrete and continuous features, for the dependency parsing task. Our model is also different from Ma et al. (2014b) in the hidden layer. While they use a form of restricted Boltzmann machine to pre-train the embeddings and hidden layer from large-scale ngrams, we fully rely on supervised learning to train complex feature combinations. Wang and Manning (2013) consider integrating embeddings and discrete features into a neural CRF. They show that combin"
D15-1153,P10-1040,0,0.727528,"ture model and a multi-layer neural network model for deterministic transition-based dependency parsing, by integrating the sparse features into the neural model. Correlations are drawn between the hybrid model and previous work on integrating word embedding features into a discrete linear model. By analyzing the results of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods. 1 ··· (a) discrete linear (eg. MaltParser) ··· ··· (b) continuous NN (eg. Chen and Manning (2014)) ··· (c) Turian et al. (2010) ··· ··· transform (d) Guo et al. (2014) Introduction ··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers w"
D15-1153,I13-1183,0,0.0799544,"Missing"
D15-1153,N13-1063,0,0.0390131,"Missing"
D15-1153,N15-1155,0,0.021729,"more sophisticated neural network structures with long term memories can significantly improve the accuracy over local classifiers. Their work is orthogonal to ours. 6 Related Work As discussed in the introduction, our work is related to previous work on integrating word embeddings into discrete models (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014). Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering). There has been recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014). Both models have essentially the same structure as our model. In contrast to their work, we systematically compare various ways to integrate discrete and continuous features, for the dependency parsing task. Our model is also different from Ma et al. (2014b"
D15-1153,D08-1059,1,0.78857,"results of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods. 1 ··· (a) discrete linear (eg. MaltParser) ··· ··· (b) continuous NN (eg. Chen and Manning (2014)) ··· (c) Turian et al. (2010) ··· ··· transform (d) Guo et al. (2014) Introduction ··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning ("
D15-1153,P11-2033,1,0.92863,"pared with previous methods. 1 ··· (a) discrete linear (eg. MaltParser) ··· ··· (b) continuous NN (eg. Chen and Manning (2014)) ··· (c) Turian et al. (2010) ··· ··· transform (d) Guo et al. (2014) Introduction ··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear mode"
D15-1153,C12-2136,1,0.859868,"hods. 1 ··· (a) discrete linear (eg. MaltParser) ··· ··· (b) continuous NN (eg. Chen and Manning (2014)) ··· (c) Turian et al. (2010) ··· ··· transform (d) Guo et al. (2014) Introduction ··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1"
D15-1153,P14-1019,0,0.0132147,"NN (eg. Chen and Manning (2014)) ··· (c) Turian et al. (2010) ··· ··· transform (d) Guo et al. (2014) Introduction ··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 (a) and (b). ··· (e) this paper Figure 1: Five deterministic transitio"
D15-1153,P15-1117,1,0.859084,"Missing"
D15-1153,D12-1133,0,\N,Missing
D15-1211,P06-2005,0,0.187504,"et al., 2011; Gimpel et al., 2011; Han and Baldwin, 2011). One of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefo"
D15-1211,P15-1168,0,0.0201398,"Gimpel et al. (2011) and Foster et al. (2011) annotate English microblog posts with POS tags. Han and Baldwin (2011) release a microblog corpus annotated with normalized words. Duan et al. (2012) develop a Chinese microblog corpus annotated with segmentation for SIGHAN bakeoff. Wang et al. (2013) release a Chinese microblog corpus for word segmentation and informal word detection. However, there are no microblog corpora annotated Chinese word segmentation, POS tags, and normalized sentences. Our work is alse related to the work of word segmentation (Zhang and Clark, 2007; Zhang et al., 2013; Chen et al., 2015) and joint word segmentation and POS-tagging (Jiang et al., 2008; Zhang and Clark, 2010). A comprehensive survey is out of the scope of this paper, but interested readers can refer to Pei et al. (Pei et al., 2014) for a recent literature review of the fields. To evaluate our model, we develop an annotated microblog corpus with word segmentation, POS tags, and normalization. Furthermore, we train our model by using a standard segmented and POS tagged corpus. We also present a comprehensive evaluation in terms of precision and recall on our 1844 microblog test corpus. Such an evaluation has not"
D15-1211,W09-2010,0,0.201583,"as shown that off-the-shelf NLP tools can perform poorly on microblogs (Foster et al., 2011; Gimpel et al., 2011; Han and Baldwin, 2011). One of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual depen"
D15-1211,P11-2008,0,0.121397,"Missing"
D15-1211,W11-0704,0,0.029173,"cular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on recall, while the pattern matching performs better on precision. Li and Yarowsky"
D15-1211,P11-1038,0,0.32066,"three systems. In particular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on recall, while the pattern matching performs better on precis"
D15-1211,D12-1039,0,0.402557,"ization, which can be expensive. In this paper, we propose a joint model for Chinese text normalization, word-segmentation and POS tagging, which can be trained using standard segmentation and POS tagging annotation, overcoming the lack of an annotated corpus on Chinese microblogs. Our model is based on Zhang and Clark (2010), with an extended set of transition actions to handle joint normalization. In our model, word segmentation and POS tagging are based on normalized text transformed from informal text. Assuming that the majority of informal words can be normalized into formal equivalents (Han et al., 2012; Li and Yarowsky, 2008), we seek standard forms of informal words from an automatically constructed normalization dictionary. To evaluate our model, we developed an annotated corpus of microblog texts. Results show that our model achieves the best performances on three tasks compared with several baseline systems. 2 Text Normalization Text normalization is a relatively new research topic. There are no precise definitions of a text 1837 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1837–1846, c Lisbon, Portugal, 17-21 September 2015. 2015 Associa"
D15-1211,P08-1102,0,0.0537527,"Missing"
D15-1211,D14-1011,0,0.0548928,"start of a new word in a state, SEPS operates SEP and replaces the last word by a possible standard form. 3.4 Features In the experiments, we use the segmentation feature templates of Zhang and Clark (2011). These features are effective for segmentation on formal text. However, for text normalization, these features contain insufficient information. Our experiments show that by using Zhang and Clark’s features, the F-Score on normalization is only 0.4207. Prior work has shown that the language statistic information is important for text normalization (Wang et al., 2013; Li and Yarowsky, 2008; Kaji and Kitsuregawa, 2014). As a result, we extract language model features by using word-based language model learned from a large quantity of standard texts. In particular, 1-gram, 2-gram, 3-gram features are extracted. Every type of n-gram is divided into ten probability ranges. For example, if the probability of the word bigram: “ ‹›- '” (high pressure) is in the 2nd range, the feature is represented as “word-2-gram=2”. In our experiments, language models are trained on the Gigaword corpus1 with SRILM tools2 . To train a word-based language model, we segmented the corpus using our re-implementation of Zhang and Cla"
D15-1211,P09-1058,0,0.0310177,"ttp://www.speech.sri.com/projects/srilm/ Text wR_ðwŸ (Overseas returnees is also referred to as turtles.) õ Ø b † } º i Ùõ ëØp ØIpf (A tree, seemingly a little high, fails a lot of people. Well, this tree is called high number (advanced mathematics)) mance of text normalization, but also increases the performance of word-segmentation. 4 4.1 Extension for Joint Segmentation, Normalization and POS tagging Joint Segmentation and POS Tagging In order to reduce the error propagation of word segmentation, joint models have been applied to some NLP tasks, such as POS tagging (Zhang and Clark, 2010; Kruengkrai et al., 2009) and Parsing (Zhang et al., 2014a; Qian and Liu, 2012; Zhang et al., 2014b). We take the joint word segmentation and POS tagging model of Zhang and Clark (2010) as the joint baseline. It extends from transition-based segmenter, adding POS arguments to the original actions. In Figure 1, when the current character ci is processing, the transition system for ST would operate as follows : (1) APP(ci ), removing ci from Q, and appending it to the last (partial) word in S with the same POS tag, . (2) SEP(ci , pos), removing ci from Q, making the last word in S as completed, and adding ci as a new pa"
D15-1211,D08-1108,0,0.117583,"be expensive. In this paper, we propose a joint model for Chinese text normalization, word-segmentation and POS tagging, which can be trained using standard segmentation and POS tagging annotation, overcoming the lack of an annotated corpus on Chinese microblogs. Our model is based on Zhang and Clark (2010), with an extended set of transition actions to handle joint normalization. In our model, word segmentation and POS tagging are based on normalized text transformed from informal text. Assuming that the majority of informal words can be normalized into formal equivalents (Han et al., 2012; Li and Yarowsky, 2008), we seek standard forms of informal words from an automatically constructed normalization dictionary. To evaluate our model, we developed an annotated corpus of microblog texts. Results show that our model achieves the best performances on three tasks compared with several baseline systems. 2 Text Normalization Text normalization is a relatively new research topic. There are no precise definitions of a text 1837 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1837–1846, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational L"
D15-1211,P14-1028,0,0.0258405,"microblog corpus annotated with segmentation for SIGHAN bakeoff. Wang et al. (2013) release a Chinese microblog corpus for word segmentation and informal word detection. However, there are no microblog corpora annotated Chinese word segmentation, POS tags, and normalized sentences. Our work is alse related to the work of word segmentation (Zhang and Clark, 2007; Zhang et al., 2013; Chen et al., 2015) and joint word segmentation and POS-tagging (Jiang et al., 2008; Zhang and Clark, 2010). A comprehensive survey is out of the scope of this paper, but interested readers can refer to Pei et al. (Pei et al., 2014) for a recent literature review of the fields. To evaluate our model, we develop an annotated microblog corpus with word segmentation, POS tags, and normalization. Furthermore, we train our model by using a standard segmented and POS tagged corpus. We also present a comprehensive evaluation in terms of precision and recall on our 1844 microblog test corpus. Such an evaluation has not been conducted in previous work due to the lack of annotated corpora for Chinese microblogs. 8 Conclusion We proposed a joint model of word segmentation, POS tagging and normalization, in which the three tasks ben"
D15-1211,D12-1046,0,0.0377294,"eas returnees is also referred to as turtles.) õ Ø b † } º i Ùõ ëØp ØIpf (A tree, seemingly a little high, fails a lot of people. Well, this tree is called high number (advanced mathematics)) mance of text normalization, but also increases the performance of word-segmentation. 4 4.1 Extension for Joint Segmentation, Normalization and POS tagging Joint Segmentation and POS Tagging In order to reduce the error propagation of word segmentation, joint models have been applied to some NLP tasks, such as POS tagging (Zhang and Clark, 2010; Kruengkrai et al., 2009) and Parsing (Zhang et al., 2014a; Qian and Liu, 2012; Zhang et al., 2014b). We take the joint word segmentation and POS tagging model of Zhang and Clark (2010) as the joint baseline. It extends from transition-based segmenter, adding POS arguments to the original actions. In Figure 1, when the current character ci is processing, the transition system for ST would operate as follows : (1) APP(ci ), removing ci from Q, and appending it to the last (partial) word in S with the same POS tag, . (2) SEP(ci , pos), removing ci from Q, making the last word in S as completed, and adding ci as a new partial word with a POS tag “pos”. Given the sentence “"
D15-1211,J11-1005,1,0.920973,"alization dictionary, avoiding diversity on informal words. 3 3.1 Joint Segmentation and Normalization Transition-based Segmentation We adapt the segmenter of Zhang and Clark (2007) as our baseline segmenter. Given an input sentence x, the baseline segmenter finds a segmentation by maximizing: F (x) = argmax Score(y) yGen(x) (1) where Gen(x) denotes the set of all possible segmentations for an input sentence. Zhang and Clark (2007) proposed a graphbased scoring model, with features based on complete words and word sequences. We adapt their method slightly, under a transition-based framework (Zhang and Clark, 2011), which gives us a consistent way of defining all models in this paper. Stack ... S2 S1 S0 Queue C1 ... Cn Figure 1: A state of transition-based model. Here a transition model is defined as a quadruple M = (C, T, W, Ct ), where C is a state space, T is a set of transitions, each of which is a function: C → C, W is an input sentence c1 ... cn , Ct is a set of terminal states. A model scores the output by scoring the corresponding transition sequence. As shown in Figure 1, a state is a tuple ST = (S, Q), where S contains partially segmented sequences, and Q = (ci , ci+1 , ..., cn ) is the sequen"
D15-1211,D13-1031,0,0.0249816,"guistic information. Gimpel et al. (2011) and Foster et al. (2011) annotate English microblog posts with POS tags. Han and Baldwin (2011) release a microblog corpus annotated with normalized words. Duan et al. (2012) develop a Chinese microblog corpus annotated with segmentation for SIGHAN bakeoff. Wang et al. (2013) release a Chinese microblog corpus for word segmentation and informal word detection. However, there are no microblog corpora annotated Chinese word segmentation, POS tags, and normalized sentences. Our work is alse related to the work of word segmentation (Zhang and Clark, 2007; Zhang et al., 2013; Chen et al., 2015) and joint word segmentation and POS-tagging (Jiang et al., 2008; Zhang and Clark, 2010). A comprehensive survey is out of the scope of this paper, but interested readers can refer to Pei et al. (Pei et al., 2014) for a recent literature review of the fields. To evaluate our model, we develop an annotated microblog corpus with word segmentation, POS tags, and normalization. Furthermore, we train our model by using a standard segmented and POS tagged corpus. We also present a comprehensive evaluation in terms of precision and recall on our 1844 microblog test corpus. Such an"
D15-1211,P14-1125,1,0.925763,"ne of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefore two tasks should be solved jointly. Wang and Kan (2013) prop"
D15-1211,E14-1062,1,0.939348,"ne of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefore two tasks should be solved jointly. Wang and Kan (2013) prop"
D15-1211,D14-1037,0,0.0801416,"periments can partly reflect some conclusions. Table 7 shows the results of normalization by different systems. The performance of our model is the best among the three systems. In particular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese in"
D15-1211,P13-1072,0,0.0766895,"2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefore two tasks should be solved jointly. Wang and Kan (2013) proposed a joint model to process word segmentation and informal word detection. However, text normalization was not included in the joint model. Kaji et al (2014) proposed a joint model for word segmentation, POS tagging and normalization for Japanese Microblogs, which was trained on a partially annotated microblog corpus. Their method requires special annotation for text normalization, which can be expensive. In this paper, we propose a joint model for Chinese text normalization, word-segmentation and POS tagging, which can be trained using standard segmentation and POS tagging annotation,"
D15-1211,W12-2106,0,0.0705481,"Missing"
D15-1211,I13-1015,0,0.511944,"discourse-level normalization. In this paper we focus on lexical-level normalization, which aims to transform informal words into their standard forms. Lexical normalization can be regarded as a spelling correction problem. However, researches on spelling correction focus on typographic and cognitive/orthographic errors (Kukich, 1992), while text normalization focuses on lexical variants, such as phonetic substitutions, abbreviation and paraphrases. Unlike English, for which informal words are detected according to whether they are out of vocabulary, Chinese informal words manifest diversity. Wang et al. (2013) divided informal words into three types: phonetic substitutions, abbreviations and neologisms. Li and Yarowsky (2008) classified them into four types: homophone, abbreviation, transliteration and others. Due to variant characteristics, they normalise informal words by training a model per type, leading to increased system complexity. Research reveals that most lexical variants have an unambiguous standard form (Han et al., 2012; Li and Yarowsky, 2008). The validity of this assumption is also empirically assessed on our corpus annotation in Section 6.1. Based on this assumption, we seek standa"
D15-1211,I05-3013,0,0.0401119,"ook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on recall, while the pattern matching performs better on precision. Li and Yarowsky (2008) tackle the problem of identifying informal/formal Chinese word pairs by generating candidates from Baidu search engine and ranking using a conditional log-linear model. Zhang et al. (2014c) analyze the phenomena of mixed text in Chinese microblogs, proposing a two-stage method to normalise mixed texts. However, their models employ pipelined words segm"
D15-1211,D13-1007,0,0.151872,"f NLP tools can perform poorly on microblogs (Foster et al., 2011; Gimpel et al., 2011; Han and Baldwin, 2011). One of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word s"
D15-1211,P07-1106,1,0.905444,"to variant characteristics, they normalise informal words by training a model per type, leading to increased system complexity. Research reveals that most lexical variants have an unambiguous standard form (Han et al., 2012; Li and Yarowsky, 2008). The validity of this assumption is also empirically assessed on our corpus annotation in Section 6.1. Based on this assumption, we seek standard forms of informal words from a constructed normalization dictionary, avoiding diversity on informal words. 3 3.1 Joint Segmentation and Normalization Transition-based Segmentation We adapt the segmenter of Zhang and Clark (2007) as our baseline segmenter. Given an input sentence x, the baseline segmenter finds a segmentation by maximizing: F (x) = argmax Score(y) yGen(x) (1) where Gen(x) denotes the set of all possible segmentations for an input sentence. Zhang and Clark (2007) proposed a graphbased scoring model, with features based on complete words and word sequences. We adapt their method slightly, under a transition-based framework (Zhang and Clark, 2011), which gives us a consistent way of defining all models in this paper. Stack ... S2 S1 S0 Queue C1 ... Cn Figure 1: A state of transition-based model. Here a"
D15-1211,D10-1082,1,0.862042,"r, text normalization was not included in the joint model. Kaji et al (2014) proposed a joint model for word segmentation, POS tagging and normalization for Japanese Microblogs, which was trained on a partially annotated microblog corpus. Their method requires special annotation for text normalization, which can be expensive. In this paper, we propose a joint model for Chinese text normalization, word-segmentation and POS tagging, which can be trained using standard segmentation and POS tagging annotation, overcoming the lack of an annotated corpus on Chinese microblogs. Our model is based on Zhang and Clark (2010), with an extended set of transition actions to handle joint normalization. In our model, word segmentation and POS tagging are based on normalized text transformed from informal text. Assuming that the majority of informal words can be normalized into formal equivalents (Han et al., 2012; Li and Yarowsky, 2008), we seek standard forms of informal words from an automatically constructed normalization dictionary. To evaluate our model, we developed an annotated corpus of microblog texts. Results show that our model achieves the best performances on three tasks compared with several baseline sys"
D15-1211,W12-6307,0,\N,Missing
D15-1211,C10-2022,0,\N,Missing
D15-1291,P14-2131,0,0.167747,"lational similarities, compared to other tasks in Chinese NLP, including syntactic parsing, information extraction and machine translation. We work on three specific problems. First, we study the effect of dependency-based word embeddings for analogy detection. There are two variations of Mikolov et al’s skip-gram embedding model, one training the distributed word representation of a word using its context words in local ngram window (Mikolov et al., 2013a), and the other training the distributed representation of a word using words in a syntactic dependency context (Levy and Goldberg, 2014b; Bansal et al., 2014). The latter has attracted much recent atten2441 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2441–2450, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. tion due to its potential in capturing more syntactic regularities. It has been shown to outperform the former in a variety of NLP tasks, and can potentially also improve relation similarity. Our experiments on both English and Chinese show that the dependency-context embeddings consistently under-perform ngram-context embeddings. We give some theoretica"
D15-1291,C10-3004,0,0.0471201,"Missing"
D15-1291,D07-1059,0,0.226851,"n, without stressing the difference between distributed and distributional (i.e. counting-based) word representations. Most work on embedding-based analogy detection uses relational similarities as a measure of the quality of embeddings. However, relatively little has been done in the opposite direction, exploring how to leverage embeddings for improving relational similarity algorithms. We empirically study the use of word embeddings for Chinese analogy detection and mining, leveraging syntactic dependencies, which has been shown to be closely associated with semantic relations (Levin, 1993; Chiu et al., 2007). Compared with many other languages, this association is particularly strong for Chinese, which is fully configurational and lacks morphology. To our knowledge, relatively little work has been reported on Chinese relational similarities, compared to other tasks in Chinese NLP, including syntactic parsing, information extraction and machine translation. We work on three specific problems. First, we study the effect of dependency-based word embeddings for analogy detection. There are two variations of Mikolov et al’s skip-gram embedding model, one training the distributed word representation of"
D15-1291,W09-0205,0,0.0604932,"Missing"
D15-1291,S12-1049,0,0.0490416,"Missing"
D15-1291,S12-1047,0,0.0467716,"Missing"
D15-1291,P14-2050,0,0.0715629,"larity measures the correspondence between word-word relations (Medin et al., 1990). It is relevant to many tasks in NLP (Turney, 2006), such as word sense disambiguation, information extraction, question answering, information retrieval, semantic role identification and metaphor detection. Typical tasks on relational similarity include analogy detection, which measures the degree of relational similarities, and analogy mining, which extracts analogous word pairs from unstructured text. Recently, distributed word representations (i.e. embeddings) (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014b) have been used for unsupervised analogy detection. Mikolov et al. use attributional similarities between words in a relation to compute relational similarities, and show that the method outperforms the best system in the SemEval 2012 shared task on analogy detection. Levy and Goldberg (2014b) further improve Mikolov’s relational similarity measure method using novel arithmetic combinations of attributional similarities. For simplicity, we call the method of Mikolov et al. embeddingbased analogy detection, without stressing the difference between distributed and distributional (i.e. counting"
D15-1291,W14-1618,0,0.0605222,"larity measures the correspondence between word-word relations (Medin et al., 1990). It is relevant to many tasks in NLP (Turney, 2006), such as word sense disambiguation, information extraction, question answering, information retrieval, semantic role identification and metaphor detection. Typical tasks on relational similarity include analogy detection, which measures the degree of relational similarities, and analogy mining, which extracts analogous word pairs from unstructured text. Recently, distributed word representations (i.e. embeddings) (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014b) have been used for unsupervised analogy detection. Mikolov et al. use attributional similarities between words in a relation to compute relational similarities, and show that the method outperforms the best system in the SemEval 2012 shared task on analogy detection. Levy and Goldberg (2014b) further improve Mikolov’s relational similarity measure method using novel arithmetic combinations of attributional similarities. For simplicity, we call the method of Mikolov et al. embeddingbased analogy detection, without stressing the difference between distributed and distributional (i.e. counting"
D15-1291,N13-1090,0,0.157511,"analogy mining. 1 Introduction Relational similarity measures the correspondence between word-word relations (Medin et al., 1990). It is relevant to many tasks in NLP (Turney, 2006), such as word sense disambiguation, information extraction, question answering, information retrieval, semantic role identification and metaphor detection. Typical tasks on relational similarity include analogy detection, which measures the degree of relational similarities, and analogy mining, which extracts analogous word pairs from unstructured text. Recently, distributed word representations (i.e. embeddings) (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014b) have been used for unsupervised analogy detection. Mikolov et al. use attributional similarities between words in a relation to compute relational similarities, and show that the method outperforms the best system in the SemEval 2012 shared task on analogy detection. Levy and Goldberg (2014b) further improve Mikolov’s relational similarity measure method using novel arithmetic combinations of attributional similarities. For simplicity, we call the method of Mikolov et al. embeddingbased analogy detection, without stressing the difference betw"
D15-1291,E09-1071,0,0.0546846,"Missing"
D15-1291,C14-1026,1,0.802945,"t of dependency embeddings DT (dependency context), using the Skip-Gram model. WORD 2 VEC 2 is used to train NG5 and NG2, and WORD 2 VECF 3 is used to train DEP and DT. The negative-sampling parameter is set to 15 in all the training processes. All embeddings are trained on a free Chinese news archive4 that contains about 170 millions sentences and 3.4 billions words. We segment and parse these sentences using the MVT implementation of ZPar 0.75 (Zhang and Clark, 2011), which is trained on a large-scale annotated corpus and achieves state-of-the-art analyzing accuracy on contemporary Chinese (Qiu et al., 2014)6 . Targets and contexts for word and dependency embeddings were filtered with a minimum frequency of 100 and 10, respectively, and all the four types of embeddings are trained with 200 dimensions. 6.2 Datasets and Evaluation Metrics Three datasets are used for evaluating Chinese embeddings. First, we construct a set of semantic analogy questions. This set contains five types of semantic analogy questions, including capital-country (136 word pairs, and 18354 analogy questions), provincial capital-province (28, 756), city-province (637, 386262), family member (male-female) (18, 306) and currenc"
D15-1291,C10-1112,0,0.0773371,"Missing"
D15-1291,P10-1040,0,0.0587047,"ame relation with the seed. Compared with relation classification and analogy detection, analogy mining can be practically more useful because it requires less given information, and provides a large quantity of analogous word pairs automatically. 2.2 Skip-gram Word Embeddings As a by-product of neural language models (Bengio et al., 2003; Mnih and Hinton, 2007), word embeddings are distributed vector representations of words, trained using local contexts. They capture linguistic regularities in languages (Mikolov et al., 2013b) and have been used in various tasks (Collobert and Weston, 2008; Turian et al., 2010; Socher et al., 2011). In this paper, we apply the Skip-gram method of Mikolov et al. (2013a) for training embeddings, which works by maximizing the probability of a word given a context of multiple words. Mikolov et al. (2013b) use an ngram window as the context, and observe that the resulting embeddings are highly useful for unsupervised analogy detection. 2442 2.3 Embedding-based Analogy Detection Formally, the task of analogy detection is to find a word b* given a pair of words a:b and a word a* such that a*:b* is analogous to a:b. Mikolov et al. (2013b) show that the task can be solved b"
D15-1291,J06-3003,0,0.882862,"tic dependencies on improving Chinese analogy detection based on distributed word representations, showing that a dependency-based embeddings does not perform better than an ngram-based embeddings, but dependency structures can be used to improve analogy detection by filtering candidates. In addition, we show that distributed representations of dependency structure can be used for measuring relational similarities, thereby help analogy mining. 1 Introduction Relational similarity measures the correspondence between word-word relations (Medin et al., 1990). It is relevant to many tasks in NLP (Turney, 2006), such as word sense disambiguation, information extraction, question answering, information retrieval, semantic role identification and metaphor detection. Typical tasks on relational similarity include analogy detection, which measures the degree of relational similarities, and analogy mining, which extracts analogous word pairs from unstructured text. Recently, distributed word representations (i.e. embeddings) (Mikolov et al., 2013a; Mikolov et al., 2013b; Levy and Goldberg, 2014b) have been used for unsupervised analogy detection. Mikolov et al. use attributional similarities between word"
D15-1291,Q13-1029,0,0.0153107,"kground Relational Similarity Tasks There are three main tasks for relational similarity. This first is relation classification, which has been used in Task 2 of SemEval 2012 (Jurgens et al., 2012). In this task, all four words in two word pairs are given, and one needs to judge whether Figure 1: Dependency tree of the sentence “1991c (in 1991) § (,) cnê (Obama) oÚ (President) .’ (graduate) u (from) MÃ (Harvard) {Æ (Law School)”. they belong to a same relation type. In order to address this task, various supervised methods have been used (Bollegala et al., 2008; Herdaˇgdelen and Baroni, 2009; Turney, 2013). The second task is analogy detection (Mikolov et al., 2013b), which takes three words in two word pairs, and searches for a most suitable word from the vocabulary to recover the hidden word. This task has been addressed using word embeddings (Mikolov et al., 2013b; Levy and Goldberg, 2014b). The third task is analogy mining (Chiu et al., 2007), which takes one word pair belonging to a certain semantic relation as a seed, and searches for all the word pairs that share the same relation with the seed. Compared with relation classification and analogy detection, analogy mining can be practicall"
D15-1291,I05-1007,0,0.103034,"Missing"
D15-1291,J11-1005,1,0.702841,"t with 5 words to the left of the target word and 5 words to the right), NG2 (2 words to the left and right) and DEP (dependency context), and one set of dependency embeddings DT (dependency context), using the Skip-Gram model. WORD 2 VEC 2 is used to train NG5 and NG2, and WORD 2 VECF 3 is used to train DEP and DT. The negative-sampling parameter is set to 15 in all the training processes. All embeddings are trained on a free Chinese news archive4 that contains about 170 millions sentences and 3.4 billions words. We segment and parse these sentences using the MVT implementation of ZPar 0.75 (Zhang and Clark, 2011), which is trained on a large-scale annotated corpus and achieves state-of-the-art analyzing accuracy on contemporary Chinese (Qiu et al., 2014)6 . Targets and contexts for word and dependency embeddings were filtered with a minimum frequency of 100 and 10, respectively, and all the four types of embeddings are trained with 200 dimensions. 6.2 Datasets and Evaluation Metrics Three datasets are used for evaluating Chinese embeddings. First, we construct a set of semantic analogy questions. This set contains five types of semantic analogy questions, including capital-country (136 word pairs, and"
D16-1070,D14-1082,0,0.0427993,"Missing"
D16-1070,D15-1141,0,0.0247885,"main adaptation method of Daum´e III (2007), keeping a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different treebanks into a single combined labelset. A summary of such multi-view methods is shown in Figure 1(b), which demonstrates their main differences compared to stacking (Figure 1(a)). Recently, neural network has gained increasing research attention, with highly competitive results being reported for numerous NLP tasks, including word segmentation (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015), POS-tagging (Ma et al., 2014; Plank et al., 2016), and parsing (Chen and 731 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 731–741, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Training integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training. In addition, the advantage of neural multi-view learning over its discrete counterpart are many-fold. First, it is free from the necessity of manual cross-labelset interactive feature en"
D16-1070,P05-1066,0,0.0194503,"Missing"
D16-1070,P07-1033,0,0.182874,"Missing"
D16-1070,de-marneffe-etal-2006-generating,0,0.0491802,"Missing"
D16-1070,P15-1033,0,0.0250021,"Missing"
D16-1070,I05-3017,0,0.0514718,"dency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emerson, 2003; Emerson, 2005; Xue, 2003). These give rise to the research problem ∗ Work done when the first author was visiting SUTD. The second approach is based on multi-view learning (Johansson, 2013; Li et al., 2015). The idea is to address both annotation styles simultaneously by sharing common feature representations. In particular, Johansson (2013) trained dependency parsers using the domain adaptation method of Daum´e III (2007), keeping a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different treebanks into a s"
D16-1070,P04-1059,0,0.0471347,"RF baseline model. This is due to the higher computation cost of a deep neural network on a CPU. Compared with the CRF baseline, the CRF multi-view model is significantly slower because of its large feature set and the multi-label search space. However, the NN multi-view model achieves almost the same time cost with the NN baseline, and is much more efficient than the CRF counterpart. This shows the efficiency advantage of the NN multi-view model by parameter sharing and output splitting. 7 Related Work Early research on heterogeneous annotations focuses on annotation conversion. For example, Gao et al. (2004) proposed a transformation-based method to convert the annotation style of a word segmentation corpus to that of another. Manually designed transformation templates are used, which makes it difficult to generalize the method to other 4 http://hlt.suda.edu.cn/zhli/resources/zhenghua-acl2015resources.zip 738 tasks and treebanks. Jiang et al. (2009) described a stacking-based model for heterogeneous annotations, using a pipeline to integrate the knowledge from one corpus to another. Sun and Wan (2012) proposed a structure-based stacking model, which makes use of structured features such as sub-wo"
D16-1070,P09-1059,1,0.902162,"only better accuracy improvements, but also an order of magnitude faster speed compared to its discrete baseline, adding little time cost compared to a neural model trained on a single treebank. 1 The task has been tackled using two typical approaches. The first is based on stacking (Wolpert, 1992; Breiman, 1996; Wu et al., 2003). As shown in Figure 1(a), the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features. This method has been used for leveraging two different treebanks for word segmentation (Jiang et al., 2009; Sun and Wan, 2012) and dependency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granulariti"
D16-1070,J15-1005,1,0.892533,"Missing"
D16-1070,W07-2416,0,0.0115331,", the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features. This method has been used for leveraging two different treebanks for word segmentation (Jiang et al., 2009; Sun and Wan, 2012) and dependency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emerson, 2003; Emerson, 2005; Xue, 2003). These give rise to the research problem ∗ Work done when the first author was visiting SUTD. The second approach is based on multi-view learning (Johansson, 2013; Li et al., 2015). The idea is to address both annotation styles simultaneously by sharing common feature representations. In particular, Johansson"
D16-1070,N13-1013,0,0.0763771,"to its discrete baseline, adding little time cost compared to a neural model trained on a single treebank. 1 The task has been tackled using two typical approaches. The first is based on stacking (Wolpert, 1992; Breiman, 1996; Wu et al., 2003). As shown in Figure 1(a), the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features. This method has been used for leveraging two different treebanks for word segmentation (Jiang et al., 2009; Sun and Wan, 2012) and dependency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emerson, 2003; Emerson, 2005; Xue, 2003). These give rise to the researc"
D16-1070,N16-1179,0,0.0168262,"d bt , bc are model parameters. Finally, w  ci is concatenated with word embedding to form final word represeni : i = tation rw rw eiw ⊕ w  ci The output layer employs a conditional random field (CRF) to infer the POS ti of each word wi based on the feature layer outputs. The conditional probability of a tag sequence given an input sentence is: ∏ ∏n x, y i ) ni=1 Φ(x, y i , y i−1 ) i=1 Ψ( , p(y |x) = Z(x) where Z(x) is the partition function: 2.2 Feature Layer Recently, bi-directional LSTM has been successfully applied in various NLP tasks (Liu et al., 2015; Zhou and Xu, 2015; Klerke et al., 2016; Plank et al., 2016). The feature layer uses a bi-directional LSTM to extract a feature vector hi for each word wi , rei ⊕ i−1 ⊕ i−2 ⊕ rw rw spectively. An input vector xi = (rw i+2 ) is used to represent each word w i . i+1 ⊕  rw rw We use a LSTM variation with peephole connections (Graves and Schmidhuber, 2005) to extract features based on x(1:n) . The model computes a hidden vector hi for each input xi , passing information from h1 , ..., hi−1 to hn via a sequence of cell states 733 Z(x) = n ∑∏  y i=1 Ψ(x, y i ) n ∏ Φ(x, y i , y i−1 ) i=1 In particular, the output cliqu"
D16-1070,N16-1030,0,0.0251489,"he aforementioned methods on heterogeneous annotations are investigated mainly for discrete models. It remains an interesting research question how effective multiple treebanks can be utilized by neural NLP models, and we aim to investigate this empirically. We follow Li et al. (2015), taking POS-tagging for case study, using the methods of Jiang et al. (2009) and Li et al. (2015) as the discrete stacking and multi-view training baselines, respectively, and building neural network counterparts to their models for empirical comparison. The base tagger is a neural CRF model (Huang et al., 2015; Lample et al., 2016), which gives competitive accuracies to discrete CRF taggers. Results show that neural stacking allows deeper 732 We adopt a neural CRF with a Long-Short-TermMemory (LSTM) (Hochreiter and Schmidhuber, 1997) feature layer for baseline POS tagger. As shown in Figure 2, the model consists of three main neural layers: the input layer calculates dense representation of input words using attention model on character embeddings; the feature layer employs a bi-directional LSTM model to extract non-local features from input vectors; the output layer uses a CRF structure to infer the most likely label f"
D16-1070,P15-1172,0,0.076347,"guistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emerson, 2003; Emerson, 2005; Xue, 2003). These give rise to the research problem ∗ Work done when the first author was visiting SUTD. The second approach is based on multi-view learning (Johansson, 2013; Li et al., 2015). The idea is to address both annotation styles simultaneously by sharing common feature representations. In particular, Johansson (2013) trained dependency parsers using the domain adaptation method of Daum´e III (2007), keeping a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different treebanks into a single combined labelset. A summary of such multi-view methods is shown in Figure 1(b), which demonstrates their main differences compared to stacking (Figure 1(a)). Recently, neural network has"
D16-1070,D15-1168,0,0.0153753,"of current word wi . Wt , Ut , Wc and bt , bc are model parameters. Finally, w  ci is concatenated with word embedding to form final word represeni : i = tation rw rw eiw ⊕ w  ci The output layer employs a conditional random field (CRF) to infer the POS ti of each word wi based on the feature layer outputs. The conditional probability of a tag sequence given an input sentence is: ∏ ∏n x, y i ) ni=1 Φ(x, y i , y i−1 ) i=1 Ψ( , p(y |x) = Z(x) where Z(x) is the partition function: 2.2 Feature Layer Recently, bi-directional LSTM has been successfully applied in various NLP tasks (Liu et al., 2015; Zhou and Xu, 2015; Klerke et al., 2016; Plank et al., 2016). The feature layer uses a bi-directional LSTM to extract a feature vector hi for each word wi , rei ⊕ i−1 ⊕ i−2 ⊕ rw rw spectively. An input vector xi = (rw i+2 ) is used to represent each word w i . i+1 ⊕  rw rw We use a LSTM variation with peephole connections (Graves and Schmidhuber, 2005) to extract features based on x(1:n) . The model computes a hidden vector hi for each input xi , passing information from h1 , ..., hi−1 to hn via a sequence of cell states 733 Z(x) = n ∑∏  y i=1 Ψ(x, y i ) n ∏ Φ(x, y i , y i"
D16-1070,P14-1014,1,0.44397,"III (2007), keeping a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different treebanks into a single combined labelset. A summary of such multi-view methods is shown in Figure 1(b), which demonstrates their main differences compared to stacking (Figure 1(a)). Recently, neural network has gained increasing research attention, with highly competitive results being reported for numerous NLP tasks, including word segmentation (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015), POS-tagging (Ma et al., 2014; Plank et al., 2016), and parsing (Chen and 731 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 731–741, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Training integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training. In addition, the advantage of neural multi-view learning over its discrete counterpart are many-fold. First, it is free from the necessity of manual cross-labelset interactive feature engineering, which is far from t"
D16-1070,P08-1108,0,0.0109935,"ude faster speed compared to its discrete baseline, adding little time cost compared to a neural model trained on a single treebank. 1 The task has been tackled using two typical approaches. The first is based on stacking (Wolpert, 1992; Breiman, 1996; Wu et al., 2003). As shown in Figure 1(a), the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features. This method has been used for leveraging two different treebanks for word segmentation (Jiang et al., 2009; Sun and Wan, 2012) and dependency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emerson, 2003; Emerson, 2005; Xue, 2003). These give r"
D16-1070,P14-1028,0,0.0577997,"rsers using the domain adaptation method of Daum´e III (2007), keeping a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different treebanks into a single combined labelset. A summary of such multi-view methods is shown in Figure 1(b), which demonstrates their main differences compared to stacking (Figure 1(a)). Recently, neural network has gained increasing research attention, with highly competitive results being reported for numerous NLP tasks, including word segmentation (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015), POS-tagging (Ma et al., 2014; Plank et al., 2016), and parsing (Chen and 731 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 731–741, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Training integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training. In addition, the advantage of neural multi-view learning over its discrete counterpart are many-fold. First, it is free from the necessity of manual cross-labelset in"
D16-1070,P16-2067,0,0.11174,"ng a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different treebanks into a single combined labelset. A summary of such multi-view methods is shown in Figure 1(b), which demonstrates their main differences compared to stacking (Figure 1(a)). Recently, neural network has gained increasing research attention, with highly competitive results being reported for numerous NLP tasks, including word segmentation (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015), POS-tagging (Ma et al., 2014; Plank et al., 2016), and parsing (Chen and 731 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 731–741, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Training integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training. In addition, the advantage of neural multi-view learning over its discrete counterpart are many-fold. First, it is free from the necessity of manual cross-labelset interactive feature engineering, which is far from trivial for representi"
D16-1070,D13-1062,0,0.0224561,"the above discrete methods, our neural stacking method offers further feature integration by directly connecting the feature layer of the source tagger with the input layer of the target tagger. It also allows the finetuning of the source tagger. As one of the reviewers mentioned, two extensions of CRFs, dynamic CRFs (Sutton et al., 2004) and hidden-state CRFs (Quattoni et al., 2004), can also perform similar deep integration and fine-tuning. For multi-view training, Johansson (2013) used a shared feature representation along with separate individual feature representation for each treebank. Qiu et al. (2013) proposed a multi-task learning model to jointly predict two labelsets given an input sentences. The joint model uses the union of baseline features for each labelset, without considering additional features to capture the interaction between the two labelsets. Li et al. (2015) improves upon this method by using a tighter integration between the two labelsets, treating the Cartesian product of the base labels as a single combined labelset, and exploiting joint features from two labelsets. Though capturing label interaction, their method suffers speed penalty from the sharply increased search s"
D16-1070,W03-1719,0,0.0179464,"n and Wan, 2012) and dependency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emerson, 2003; Emerson, 2005; Xue, 2003). These give rise to the research problem ∗ Work done when the first author was visiting SUTD. The second approach is based on multi-view learning (Johansson, 2013; Li et al., 2015). The idea is to address both annotation styles simultaneously by sharing common feature representations. In particular, Johansson (2013) trained dependency parsers using the domain adaptation method of Daum´e III (2007), keeping a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different tre"
D16-1070,P12-1025,0,0.178224,"y improvements, but also an order of magnitude faster speed compared to its discrete baseline, adding little time cost compared to a neural model trained on a single treebank. 1 The task has been tackled using two typical approaches. The first is based on stacking (Wolpert, 1992; Breiman, 1996; Wu et al., 2003). As shown in Figure 1(a), the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features. This method has been used for leveraging two different treebanks for word segmentation (Jiang et al., 2009; Sun and Wan, 2012) and dependency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emers"
D16-1070,P15-1032,0,0.0373545,"Missing"
D16-1070,W03-0433,0,0.0528727,"rogeneous annotations using neural network models, building a neural network counterpart to discrete stacking and multiview learning, respectively, finding that neural models have their unique advantages thanks to the freedom from manual feature engineering. Neural model achieves not only better accuracy improvements, but also an order of magnitude faster speed compared to its discrete baseline, adding little time cost compared to a neural model trained on a single treebank. 1 The task has been tackled using two typical approaches. The first is based on stacking (Wolpert, 1992; Breiman, 1996; Wu et al., 2003). As shown in Figure 1(a), the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features. This method has been used for leveraging two different treebanks for word segmentation (Jiang et al., 2009; Sun and Wan, 2012) and dependency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada an"
D16-1070,O03-4002,0,0.105777,"Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emerson, 2003; Emerson, 2005; Xue, 2003). These give rise to the research problem ∗ Work done when the first author was visiting SUTD. The second approach is based on multi-view learning (Johansson, 2013; Li et al., 2015). The idea is to address both annotation styles simultaneously by sharing common feature representations. In particular, Johansson (2013) trained dependency parsers using the domain adaptation method of Daum´e III (2007), keeping a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different treebanks into a single combin"
D16-1070,W03-3023,0,0.027284,"l., 2003). As shown in Figure 1(a), the main idea is to have a model trained using a source treebank, which is then used to guide a target treebank model by offering source-style features. This method has been used for leveraging two different treebanks for word segmentation (Jiang et al., 2009; Sun and Wan, 2012) and dependency parsing (Nivre and McDonald, 2008; Johansson, 2013). Introduction For many languages, multiple treebanks have been annotated according to different guidelines. For example, several linguistic theories have been used for defining English dependency treebanks, including Yamada and Matsumoto (2003), LTH (Johansson and Nugues, 2007) and Stanford dependencies (De Marneffe et al., 2006). For German, there exist TIGER (Brants et al., 2002) and T¨uBa-D/Z (Telljohann et al., 2006). For Chinese, treebanks have been made available under various segmentation granularities (Sproat and Emerson, 2003; Emerson, 2005; Xue, 2003). These give rise to the research problem ∗ Work done when the first author was visiting SUTD. The second approach is based on multi-view learning (Johansson, 2013; Li et al., 2015). The idea is to address both annotation styles simultaneously by sharing common feature represe"
D16-1070,D08-1059,1,0.401755,"xd , Θ), 735 For neural multi-view model, we follow Li et al. (2015) and take a the corpus-weighting strategy to sample a number of training instances from both corpora for each training iteration, as shown in Algorithm 1. At each epoch, we randomly sample from the two datasets according to a corpus weights ratio, namely the ratio between the number of sentences in each dataset used for training, to form a training set for the epoch. Experiments 96 We adopt the Penn Chinese Treebank version 5.0 (CTB5) (Xue et al., 2005) as our main corpus, with the standard data split following previous work (Zhang and Clark, 2008; Li et al., 2015). People’s Daily (PD) is used as second corpus with a different scheme. We filter out PD sentences longer than 200 words. Details of the datasets are listed in Table 1. The standard token-wise POS tagging accuracy is used as the evaluation metric. The systems are implemented with LibN3L (Zhang et al., 2016). For all the neural models, we set the hidden layer size to 100, the initial learning rate for Adagrad to 0.01 and the regularization parameter λ to 10−8 . word2vec1 is used to pretrain word embeddings. The Chinese Giga-word corpus version 5 (Graff and Chen, 2005), segment"
D16-1070,J11-1005,1,0.489625,"2015). People’s Daily (PD) is used as second corpus with a different scheme. We filter out PD sentences longer than 200 words. Details of the datasets are listed in Table 1. The standard token-wise POS tagging accuracy is used as the evaluation metric. The systems are implemented with LibN3L (Zhang et al., 2016). For all the neural models, we set the hidden layer size to 100, the initial learning rate for Adagrad to 0.01 and the regularization parameter λ to 10−8 . word2vec1 is used to pretrain word embeddings. The Chinese Giga-word corpus version 5 (Graff and Chen, 2005), segmented by zpar2 (Zhang and Clark, 2011), is used for the training corpus for word embeddings. The size of word embedding is 50. 6.2 Development Experiments We use the development dataset for two main purposes. First, under each setting, we tune the model parameters, such as the number of training epochs. Second, we study the influence of several important hyper-parameters using the development dataset. For example, for the NN multi-view learning model, the corpus weights ratio (section 5) plays an important role for the performance. We determine the parameters of the model by studying the accuracy along with the increasing epochs."
D16-1070,P16-1147,0,0.102299,"his method by using a tighter integration between the two labelsets, treating the Cartesian product of the base labels as a single combined labelset, and exploiting joint features from two labelsets. Though capturing label interaction, their method suffers speed penalty from the sharply increased search space. In contrast to their methods, our neural approach enables parameter sharing in the hidden layers, thereby modeling label interaction without directly combining the two output labelsets. This leads to a lean model with almost the same time efficiency as a single-label baseline. Recently, Zhang and Weiss (2016) proposed a stack-propagation model for learning a stacked pipeline of POS tagging and dependency parsing. Their method is similar to our neural stacking in fine-tuning the stacked module which yields features for the target model. While their multi-task learning is on heterogenous tasks, our multi-task learning is defined on heterogenous treebanks. 8 Conclusion We investigated two methods for utilizing heterogeneous annotations for neural network models, showing that they have respective advantages compared to their discrete counterparts. In particular, neural stacking allows tighter feature"
D16-1070,L16-1034,1,0.832797,"atio between the number of sentences in each dataset used for training, to form a training set for the epoch. Experiments 96 We adopt the Penn Chinese Treebank version 5.0 (CTB5) (Xue et al., 2005) as our main corpus, with the standard data split following previous work (Zhang and Clark, 2008; Li et al., 2015). People’s Daily (PD) is used as second corpus with a different scheme. We filter out PD sentences longer than 200 words. Details of the datasets are listed in Table 1. The standard token-wise POS tagging accuracy is used as the evaluation metric. The systems are implemented with LibN3L (Zhang et al., 2016). For all the neural models, we set the hidden layer size to 100, the initial learning rate for Adagrad to 0.01 and the regularization parameter λ to 10−8 . word2vec1 is used to pretrain word embeddings. The Chinese Giga-word corpus version 5 (Graff and Chen, 2005), segmented by zpar2 (Zhang and Clark, 2011), is used for the training corpus for word embeddings. The size of word embedding is 50. 6.2 Development Experiments We use the development dataset for two main purposes. First, under each setting, we tune the model parameters, such as the number of training epochs. Second, we study the inf"
D16-1070,D13-1061,0,0.0166608,"rained dependency parsers using the domain adaptation method of Daum´e III (2007), keeping a copy of shared features and a separate copy of features for each treebank. Li et al. (2015) trained POS taggers by coupling the labelsets from two different treebanks into a single combined labelset. A summary of such multi-view methods is shown in Figure 1(b), which demonstrates their main differences compared to stacking (Figure 1(a)). Recently, neural network has gained increasing research attention, with highly competitive results being reported for numerous NLP tasks, including word segmentation (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015), POS-tagging (Ma et al., 2014; Plank et al., 2016), and parsing (Chen and 731 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 731–741, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Training integration of the source model beyond one-best outputs, and further the fine-tuning of the source model during the target model training. In addition, the advantage of neural multi-view learning over its discrete counterpart are many-fold. First, it is free from the necessity of manual"
D16-1070,P15-1109,0,0.0129467,"i . Wt , Ut , Wc and bt , bc are model parameters. Finally, w  ci is concatenated with word embedding to form final word represeni : i = tation rw rw eiw ⊕ w  ci The output layer employs a conditional random field (CRF) to infer the POS ti of each word wi based on the feature layer outputs. The conditional probability of a tag sequence given an input sentence is: ∏ ∏n x, y i ) ni=1 Φ(x, y i , y i−1 ) i=1 Ψ( , p(y |x) = Z(x) where Z(x) is the partition function: 2.2 Feature Layer Recently, bi-directional LSTM has been successfully applied in various NLP tasks (Liu et al., 2015; Zhou and Xu, 2015; Klerke et al., 2016; Plank et al., 2016). The feature layer uses a bi-directional LSTM to extract a feature vector hi for each word wi , rei ⊕ i−1 ⊕ i−2 ⊕ rw rw spectively. An input vector xi = (rw i+2 ) is used to represent each word w i . i+1 ⊕  rw rw We use a LSTM variation with peephole connections (Graves and Schmidhuber, 2005) to extract features based on x(1:n) . The model computes a hidden vector hi for each input xi , passing information from h1 , ..., hi−1 to hn via a sequence of cell states 733 Z(x) = n ∑∏  y i=1 Ψ(x, y i ) n ∏ Φ(x, y i , y i−1 ) i=1 In particu"
D16-1070,P15-1117,1,0.472129,"plementations of our neural network stacking and multi-view learning models are available under GPL, at B model Train B model Corpus B & A labels A model Corpus B Train A model Corpus A Testing Output B B model Raw sentence Output A A model (a) stacking Multi-view model Training Train multi-view model Corpus B Corpus A Testing Output A https://github.com/chenhongshen/NNHetSeq. Output B Multi-view model 2 Baseline Neural Network Tagger Raw sentence (b) multi-view learning Figure 1: Two main approaches to utilizing heterogeneous annotations. Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015). On the other hand, the aforementioned methods on heterogeneous annotations are investigated mainly for discrete models. It remains an interesting research question how effective multiple treebanks can be utilized by neural NLP models, and we aim to investigate this empirically. We follow Li et al. (2015), taking POS-tagging for case study, using the methods of Jiang et al. (2009) and Li et al. (2015) as the discrete stacking and multi-view training baselines, respectively, and building neural network counterparts to their models for empirical comparison. The base tagger is a neural CRF model"
D16-1098,D15-1041,0,0.0154229,"information flow: an input gate it , a forget gate ft , a memory cell ct , an output gate ot , and a hidden state ht . The computation of each vector is as follows: it = σ(W (i) xt + U (i) ht−1 + V (i) ct−1 + b(i) ) ft = 1.0 − it ct = ft ct−1 + it tanh(W (u) xt + U (u) ht−1 + b(u) ) ot = σ(W (o) xt + U (o) ht−1 + V (o) ct + b(o) ) ht = ot tanh(ct ) Here σ denotes component-wise sigmoid function and is component-wise multiplication. The representation of xt is from four sources: an embedding for the word wt , two hidden states of the last LSTM cells in a character-level bidirectional −→ LSTM (Ballesteros et al., 2015) (denoted as cht ←− and cht , respectively), and a learned vector Part-ofSpeech (POS) representation (post ). A linear transformation is applied to the vector representations before feeding them into a component-wise ReLU (Nair and Hinton, 2010) function. −→ ←− xt = max{0, V (x) [wt ; cht ; cht ; post ] + b(x) } The hidden state vectors at the t-th word from both → − ← − directions (denote as ht and ht , respectively) are passed through the ReLU function, before a softmax layer for semantic role detection. 2.2 Stack-LSTM Dependency Parser We employ the Stack-LSTM model of Dyer et al. (2015) fo"
D16-1098,D10-1072,0,0.132658,"joint parsing and semantic role labeling. 1 Introduction The correlation between syntax and semantics has been a fundamental problem in natural language processing (Steedman, 2000). As a shallow semantic task, semantic role labeling (SRL) models have traditionally been built upon syntactic parsing results (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Punyakanok et al., 2005). It has been shown that parser output features play a crucial role for accurate SRL (Pradhan et al., 2005; Surdeanu et al., 2007). On the reverse direction, semantic role features have been used to improve parsing (Boxwell et al., 2010). Existing methods typically use semantic features to rerank n-best lists of syntactic parsing models (Surdeanu et al., 2008; Hajiˇc et al., 2009). There has also been attempts to learn syntactic parsing and semantic role labeling models jointly, but most such efforts have led to negative results (Sutton and McCallum, 2005; Van Den Bosch et al., 2012; Boxwell et al., 2010). ∗ Work done while the first author was visiting SUTD. With the rise of deep learning, neural network models have been used for semantic role labeling (Collobert et al., 2011). Recently, it has been shown that a neural seman"
D16-1098,P15-1033,0,0.380165,"2012; Boxwell et al., 2010). ∗ Work done while the first author was visiting SUTD. With the rise of deep learning, neural network models have been used for semantic role labeling (Collobert et al., 2011). Recently, it has been shown that a neural semantic role labeler can give state-of-the-art accuracies without using parser output features, thanks to the use of recurrent neural network structures that automatically capture syntactic information (Zhou and Xu, 2015; Wang et al., 2015). In the parsing domain, neural network models have also been shown to give state-of-the-art results recently (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015). The availability of parser-independent neural SRL models allows parsing and SRL to be performed by both parsing→SRL and SRL→parsing pipelines, and gives rise to the interesting research question whether mutual benefits between syntax and semantic roles can be better exploited under the neural setting. Different from traditional models that rely on manual feature combinations for joint learning tasks (Sutton and McCallum, 2005; Zhang and Clark, 2008a; Finkel and Manning, 2009; Lewis et al., 2015), neural network models induce non-linear feature combinat"
D16-1098,N09-1037,0,0.0606335,"Missing"
D16-1098,J02-3001,0,0.250908,"ng neural network models, by studying a parsing→SRL pipeline, a SRL→parsing pipeline, and a simple joint model by embedding sharing. The integration of syntactic and semantic features gives promising results in a Chinese Semantic Treebank, demonstrating large potentials of neural models for joint parsing and semantic role labeling. 1 Introduction The correlation between syntax and semantics has been a fundamental problem in natural language processing (Steedman, 2000). As a shallow semantic task, semantic role labeling (SRL) models have traditionally been built upon syntactic parsing results (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Punyakanok et al., 2005). It has been shown that parser output features play a crucial role for accurate SRL (Pradhan et al., 2005; Surdeanu et al., 2007). On the reverse direction, semantic role features have been used to improve parsing (Boxwell et al., 2010). Existing methods typically use semantic features to rerank n-best lists of syntactic parsing models (Surdeanu et al., 2008; Hajiˇc et al., 2009). There has also been attempts to learn syntactic parsing and semantic role labeling models jointly, but most such efforts have led to negative results (Sutton and Mc"
D16-1098,P02-1031,0,0.0543216,"y studying a parsing→SRL pipeline, a SRL→parsing pipeline, and a simple joint model by embedding sharing. The integration of syntactic and semantic features gives promising results in a Chinese Semantic Treebank, demonstrating large potentials of neural models for joint parsing and semantic role labeling. 1 Introduction The correlation between syntax and semantics has been a fundamental problem in natural language processing (Steedman, 2000). As a shallow semantic task, semantic role labeling (SRL) models have traditionally been built upon syntactic parsing results (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Punyakanok et al., 2005). It has been shown that parser output features play a crucial role for accurate SRL (Pradhan et al., 2005; Surdeanu et al., 2007). On the reverse direction, semantic role features have been used to improve parsing (Boxwell et al., 2010). Existing methods typically use semantic features to rerank n-best lists of syntactic parsing models (Surdeanu et al., 2008; Hajiˇc et al., 2009). There has also been attempts to learn syntactic parsing and semantic role labeling models jointly, but most such efforts have led to negative results (Sutton and McCallum, 2005; Van Den Bos"
D16-1098,kingsbury-palmer-2002-treebank,0,0.073839,"at the same time. On the other hand, due to different neural structures, there is no sharing of other parameters. The joint model offers the simplest version of shared training (Collobert et al., 2011), but does not employ shared decoding (Sutton and McCallum, 2005; Zhang and Clark, 2008b). Syntax and semantic roles are assigned separately, avoiding error propagation. 3 3.1 Experiments Experimental Settings Datasets We choose Chinese Semantic Treebank (Qiu et al., 2016) for our experiments. Similar to the CoNLL corpora (Surdeanu et al., 2008; Hajiˇc et al., 2009) and different from PropBank (Kingsbury and Palmer, 2002; Xue and Palmer, 2005), it is a dependency-based corpus rather than a constituent-based corpus. The corpus contains syntactic dependency arc and semantic role annotations in a consistent form, hence facilitating the joint task. We follow the standard split for the training, development and test sets, as shown in Table 1. Training Details. There is a large number of singletons in the training set and a large number of out-of-vocabulary (OOV) words in the development set. We use the mechanism of Dyer et al. (2015) to stochastically set singletons as UNK token in each training iteration with a p"
D16-1098,D15-1169,0,0.067439,"also been shown to give state-of-the-art results recently (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015). The availability of parser-independent neural SRL models allows parsing and SRL to be performed by both parsing→SRL and SRL→parsing pipelines, and gives rise to the interesting research question whether mutual benefits between syntax and semantic roles can be better exploited under the neural setting. Different from traditional models that rely on manual feature combinations for joint learning tasks (Sutton and McCallum, 2005; Zhang and Clark, 2008a; Finkel and Manning, 2009; Lewis et al., 2015), neural network models induce non-linear feature combinations automatically from input word and Part-of-Speech (POS) embeddings. This allows more complex feature sharing between multiple tasks to be achieved effectively (Collobert et al., 2011). We take a first step1 in such investigation by cou1 Recently, Swayamdipta et al. (2016) independently proposed a similar idea to perform joint syntactic and semantic dependency parsing. Their work mainly focuses on extending actions of a greedy transition-based parser to support the joint task, achieving good performance on an English shared task, whi"
D16-1098,W05-0634,0,0.027425,"and semantic features gives promising results in a Chinese Semantic Treebank, demonstrating large potentials of neural models for joint parsing and semantic role labeling. 1 Introduction The correlation between syntax and semantics has been a fundamental problem in natural language processing (Steedman, 2000). As a shallow semantic task, semantic role labeling (SRL) models have traditionally been built upon syntactic parsing results (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Punyakanok et al., 2005). It has been shown that parser output features play a crucial role for accurate SRL (Pradhan et al., 2005; Surdeanu et al., 2007). On the reverse direction, semantic role features have been used to improve parsing (Boxwell et al., 2010). Existing methods typically use semantic features to rerank n-best lists of syntactic parsing models (Surdeanu et al., 2008; Hajiˇc et al., 2009). There has also been attempts to learn syntactic parsing and semantic role labeling models jointly, but most such efforts have led to negative results (Sutton and McCallum, 2005; Van Den Bosch et al., 2012; Boxwell et al., 2010). ∗ Work done while the first author was visiting SUTD. With the rise of deep learning, neural"
D16-1098,W05-0639,0,0.0713843,"Missing"
D16-1098,W08-2121,0,0.0588699,"Missing"
D16-1098,W05-0636,0,0.243944,"urafsky, 2002; Gildea and Palmer, 2002; Punyakanok et al., 2005). It has been shown that parser output features play a crucial role for accurate SRL (Pradhan et al., 2005; Surdeanu et al., 2007). On the reverse direction, semantic role features have been used to improve parsing (Boxwell et al., 2010). Existing methods typically use semantic features to rerank n-best lists of syntactic parsing models (Surdeanu et al., 2008; Hajiˇc et al., 2009). There has also been attempts to learn syntactic parsing and semantic role labeling models jointly, but most such efforts have led to negative results (Sutton and McCallum, 2005; Van Den Bosch et al., 2012; Boxwell et al., 2010). ∗ Work done while the first author was visiting SUTD. With the rise of deep learning, neural network models have been used for semantic role labeling (Collobert et al., 2011). Recently, it has been shown that a neural semantic role labeler can give state-of-the-art accuracies without using parser output features, thanks to the use of recurrent neural network structures that automatically capture syntactic information (Zhou and Xu, 2015; Wang et al., 2015). In the parsing domain, neural network models have also been shown to give state-of-the"
D16-1098,K16-1019,0,0.138225,"benefits between syntax and semantic roles can be better exploited under the neural setting. Different from traditional models that rely on manual feature combinations for joint learning tasks (Sutton and McCallum, 2005; Zhang and Clark, 2008a; Finkel and Manning, 2009; Lewis et al., 2015), neural network models induce non-linear feature combinations automatically from input word and Part-of-Speech (POS) embeddings. This allows more complex feature sharing between multiple tasks to be achieved effectively (Collobert et al., 2011). We take a first step1 in such investigation by cou1 Recently, Swayamdipta et al. (2016) independently proposed a similar idea to perform joint syntactic and semantic dependency parsing. Their work mainly focuses on extending actions of a greedy transition-based parser to support the joint task, achieving good performance on an English shared task, while we use a neural network for multi-task learning and we 968 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 968–974, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics pling a state-of-the-art neural semantic role labeler (Wang et al., 2015) and a state"
D16-1098,P15-1150,0,0.318136,"ns of a greedy transition-based parser to support the joint task, achieving good performance on an English shared task, while we use a neural network for multi-task learning and we 968 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 968–974, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics pling a state-of-the-art neural semantic role labeler (Wang et al., 2015) and a state-of-the-art neural parser (Dyer et al., 2015). First, we propose a novel parsing→SRL pipeline using a tree Long ShortTerm Memory (LSTM) model (Tai et al., 2015) to represent parser outputs, before feeding them to the neural SRL model as inputs. Second, we investigate a SRL→parsing pipeline, using semantic role label embeddings to enrich parser features. Third, we build a joint training model by embedding sharing, which is the most shallow level of parameter sharing between deep neural networks. This simple strategy is immune to significant differences between the network structures of the two models, which prevent direct sharing of deeper network parameters. We choose a Chinese semantic role treebank (Qiu et al., 2016) for preliminary experiments, wh"
D16-1098,D15-1186,0,0.104258,"ole labeling models jointly, but most such efforts have led to negative results (Sutton and McCallum, 2005; Van Den Bosch et al., 2012; Boxwell et al., 2010). ∗ Work done while the first author was visiting SUTD. With the rise of deep learning, neural network models have been used for semantic role labeling (Collobert et al., 2011). Recently, it has been shown that a neural semantic role labeler can give state-of-the-art accuracies without using parser output features, thanks to the use of recurrent neural network structures that automatically capture syntactic information (Zhou and Xu, 2015; Wang et al., 2015). In the parsing domain, neural network models have also been shown to give state-of-the-art results recently (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015). The availability of parser-independent neural SRL models allows parsing and SRL to be performed by both parsing→SRL and SRL→parsing pipelines, and gives rise to the interesting research question whether mutual benefits between syntax and semantic roles can be better exploited under the neural setting. Different from traditional models that rely on manual feature combinations for joint learning tasks (Sutton and McCallum, 2005;"
D16-1098,P15-1032,0,0.0219225,"l., 2010). ∗ Work done while the first author was visiting SUTD. With the rise of deep learning, neural network models have been used for semantic role labeling (Collobert et al., 2011). Recently, it has been shown that a neural semantic role labeler can give state-of-the-art accuracies without using parser output features, thanks to the use of recurrent neural network structures that automatically capture syntactic information (Zhou and Xu, 2015; Wang et al., 2015). In the parsing domain, neural network models have also been shown to give state-of-the-art results recently (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015). The availability of parser-independent neural SRL models allows parsing and SRL to be performed by both parsing→SRL and SRL→parsing pipelines, and gives rise to the interesting research question whether mutual benefits between syntax and semantic roles can be better exploited under the neural setting. Different from traditional models that rely on manual feature combinations for joint learning tasks (Sutton and McCallum, 2005; Zhang and Clark, 2008a; Finkel and Manning, 2009; Lewis et al., 2015), neural network models induce non-linear feature combinations automatically f"
D16-1098,P08-1101,1,0.87768,"In the parsing domain, neural network models have also been shown to give state-of-the-art results recently (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015). The availability of parser-independent neural SRL models allows parsing and SRL to be performed by both parsing→SRL and SRL→parsing pipelines, and gives rise to the interesting research question whether mutual benefits between syntax and semantic roles can be better exploited under the neural setting. Different from traditional models that rely on manual feature combinations for joint learning tasks (Sutton and McCallum, 2005; Zhang and Clark, 2008a; Finkel and Manning, 2009; Lewis et al., 2015), neural network models induce non-linear feature combinations automatically from input word and Part-of-Speech (POS) embeddings. This allows more complex feature sharing between multiple tasks to be achieved effectively (Collobert et al., 2011). We take a first step1 in such investigation by cou1 Recently, Swayamdipta et al. (2016) independently proposed a similar idea to perform joint syntactic and semantic dependency parsing. Their work mainly focuses on extending actions of a greedy transition-based parser to support the joint task, achieving"
D16-1098,D08-1059,1,0.557084,"In the parsing domain, neural network models have also been shown to give state-of-the-art results recently (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015). The availability of parser-independent neural SRL models allows parsing and SRL to be performed by both parsing→SRL and SRL→parsing pipelines, and gives rise to the interesting research question whether mutual benefits between syntax and semantic roles can be better exploited under the neural setting. Different from traditional models that rely on manual feature combinations for joint learning tasks (Sutton and McCallum, 2005; Zhang and Clark, 2008a; Finkel and Manning, 2009; Lewis et al., 2015), neural network models induce non-linear feature combinations automatically from input word and Part-of-Speech (POS) embeddings. This allows more complex feature sharing between multiple tasks to be achieved effectively (Collobert et al., 2011). We take a first step1 in such investigation by cou1 Recently, Swayamdipta et al. (2016) independently proposed a similar idea to perform joint syntactic and semantic dependency parsing. Their work mainly focuses on extending actions of a greedy transition-based parser to support the joint task, achieving"
D16-1098,P15-1109,0,0.34246,"sing and semantic role labeling models jointly, but most such efforts have led to negative results (Sutton and McCallum, 2005; Van Den Bosch et al., 2012; Boxwell et al., 2010). ∗ Work done while the first author was visiting SUTD. With the rise of deep learning, neural network models have been used for semantic role labeling (Collobert et al., 2011). Recently, it has been shown that a neural semantic role labeler can give state-of-the-art accuracies without using parser output features, thanks to the use of recurrent neural network structures that automatically capture syntactic information (Zhou and Xu, 2015; Wang et al., 2015). In the parsing domain, neural network models have also been shown to give state-of-the-art results recently (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015). The availability of parser-independent neural SRL models allows parsing and SRL to be performed by both parsing→SRL and SRL→parsing pipelines, and gives rise to the interesting research question whether mutual benefits between syntax and semantic roles can be better exploited under the neural setting. Different from traditional models that rely on manual feature combinations for joint learning tasks (Sutton"
D16-1098,P15-1117,1,0.850158,"ne while the first author was visiting SUTD. With the rise of deep learning, neural network models have been used for semantic role labeling (Collobert et al., 2011). Recently, it has been shown that a neural semantic role labeler can give state-of-the-art accuracies without using parser output features, thanks to the use of recurrent neural network structures that automatically capture syntactic information (Zhou and Xu, 2015; Wang et al., 2015). In the parsing domain, neural network models have also been shown to give state-of-the-art results recently (Dyer et al., 2015; Weiss et al., 2015; Zhou et al., 2015). The availability of parser-independent neural SRL models allows parsing and SRL to be performed by both parsing→SRL and SRL→parsing pipelines, and gives rise to the interesting research question whether mutual benefits between syntax and semantic roles can be better exploited under the neural setting. Different from traditional models that rely on manual feature combinations for joint learning tasks (Sutton and McCallum, 2005; Zhang and Clark, 2008a; Finkel and Manning, 2009; Lewis et al., 2015), neural network models induce non-linear feature combinations automatically from input word and P"
D16-1115,P16-1068,0,0.468518,"irwise ranking problem by ranking the order of pair essays based on their quality. Features consist of word, POS n-grams features, complex grammatical features and so on. Chen and He (2013) formulated AES into a listwise ranking problem by considering the order relation among the whole essays and features contain syntactical features, grammar and fluency features as well as content and promptspecific features. Phandi et al. (2015) use correlated Bayesian Linear Ridge Regression (cBLRR) focusing on domain-adaptation tasks. All these previous methods use discrete handcrafted features. Recently, Alikaniotis et al. (2016) also employ a neural model to learn features for essay scoring automatically, which leverages a score-specific word embedding (SSWE) for word representations and a two-layer bidirectional long-short term memory network (LSTM) to learn essay representations. Alikaniotis et al. (2016) show that by combining SSWE, LSTM outperforms traditional SVM model. On the other hand, using LSTM alone does not give significantly more accuracies compared to SVM. This conforms to our preliminary experiments with the LSTM structure. Here, we use CNN without any specific embeddings and show that our neural model"
D16-1115,D13-1180,0,0.707153,"domain-adaptation essay scoring tasks, our neural model empirically outperforms discrete models. 1 Introduction Automatic essay scoring (AES) is the task of building a computer-based grading system, with the aim of reducing the involvement of human raters as far as possible. AES is challenging since it relies not only on grammars, but also on semantics, discourse and pragmatics. Traditional approaches treat AES as a classification (Larkey, 1998; Rudner and Liang, 2002), regression (Attali and Burstein, 2004; Phandi et al., 2015), or ranking classification problem (Yannakoudakis et al., 2011; Chen and He, 2013), addressing AES by supervised learning. Features are typically bag-of-words, spelling errors and lengths, such word length, sentence length and essay length, etc. Some grammatical features are considered to assess the quality of essays (Yannakoudakis et al., 2011). A drawback is feature engineering, which can be time-consuming, since features need to be carefully handcrafted and selected to fit the approriate model. A further drawback of manual feature templates is that they are sparse, instantiated by discrete pattern-matching. As a result, parsers and semantic analyzers are necessary as a p"
D16-1115,C14-1008,0,0.0222825,"A further drawback of manual feature templates is that they are sparse, instantiated by discrete pattern-matching. As a result, parsers and semantic analyzers are necessary as a preprocessing step to offer syntactic and semantic patterns for feature extraction. Given variable qualities of student essays, such analyzers can be highly unreliable. Neural network approaches have been shown to be capable of inducing dense syntactic and semantic features automatcially, giving competitive results to manually designed features for several tasks (Kalchbrenner et al., 2014; Johnson and Zhang, 2014; dos Santos and Gatti, 2014). In this paper, we empirically investigate a neural network method to learn features automatically for AES, without the need of external pre-processing. In particular, we build a hierarchical CNN model, with one lower layer representing sentence structures and one upper layer representing essay structure based on sentence representations. We compare automatically-induced features by the model with state-of-art baseline handcrafted features. Empirical results show that neural features learned by CNN are very effective in essay scoring task, covering more high-level and abstract information com"
D16-1115,P14-1062,0,0.0462784,"y handcrafted and selected to fit the approriate model. A further drawback of manual feature templates is that they are sparse, instantiated by discrete pattern-matching. As a result, parsers and semantic analyzers are necessary as a preprocessing step to offer syntactic and semantic patterns for feature extraction. Given variable qualities of student essays, such analyzers can be highly unreliable. Neural network approaches have been shown to be capable of inducing dense syntactic and semantic features automatcially, giving competitive results to manually designed features for several tasks (Kalchbrenner et al., 2014; Johnson and Zhang, 2014; dos Santos and Gatti, 2014). In this paper, we empirically investigate a neural network method to learn features automatically for AES, without the need of external pre-processing. In particular, we build a hierarchical CNN model, with one lower layer representing sentence structures and one upper layer representing essay structure based on sentence representations. We compare automatically-induced features by the model with state-of-art baseline handcrafted features. Empirical results show that neural features learned by CNN are very effective in essay scoring task,"
D16-1115,D14-1181,0,0.00368051,"likaniotis et al. (2016) show that by combining SSWE, LSTM outperforms traditional SVM model. On the other hand, using LSTM alone does not give significantly more accuracies compared to SVM. This conforms to our preliminary experiments with the LSTM structure. Here, we use CNN without any specific embeddings and show that our neural models could outperform baseline discrete models on both in-domain and cross-domain senarios. CNN has been used in many NLP applications, such as sequence labeling (Collobert et al., 2011) , sentences modeling (Kalchbrenner et al., 2014), sentences classification (Kim, 2014), text categorization (Johnson and Zhang, 2014; Zhang et al., 2015) and sentimental analysis (dos Santos and Gatti, 2014), 1073 Feature Type Length POS Prompt Bag-of-words Feature Description Number of characters, words, sentences, etc. Relative and absolute number of bad POS n-grams Relative and absolute number of words and their synonyms in the essay appearing in the prompt Count of useful unigrams and bigrams (unstemmed, stemmed and spell corrected) Table 1: Feature description used by EASE. etc. In this paper, we explore CNN representation ability for AES tasks on both in-domain and domain"
D16-1115,P11-1019,0,0.19904,"baselines. For in-domain and domain-adaptation essay scoring tasks, our neural model empirically outperforms discrete models. 1 Introduction Automatic essay scoring (AES) is the task of building a computer-based grading system, with the aim of reducing the involvement of human raters as far as possible. AES is challenging since it relies not only on grammars, but also on semantics, discourse and pragmatics. Traditional approaches treat AES as a classification (Larkey, 1998; Rudner and Liang, 2002), regression (Attali and Burstein, 2004; Phandi et al., 2015), or ranking classification problem (Yannakoudakis et al., 2011; Chen and He, 2013), addressing AES by supervised learning. Features are typically bag-of-words, spelling errors and lengths, such word length, sentence length and essay length, etc. Some grammatical features are considered to assess the quality of essays (Yannakoudakis et al., 2011). A drawback is feature engineering, which can be time-consuming, since features need to be carefully handcrafted and selected to fit the approriate model. A further drawback of manual feature templates is that they are sparse, instantiated by discrete pattern-matching. As a result, parsers and semantic analyzers"
D16-1169,W11-0705,0,0.157838,"s. all words in the document that exist in a sentiment lexicon (Turney, 2002; Hu and Liu, 2004). This simple method has been shown to give surprisingly competitive accuracies in several sentiment analysis benchmarks (Kiritchenko et al., 2014), and is still the standard practice for specific research communities with mature domain-specific lexicons, such as finance (Kearney and Liu, 2014) and product reviews (Ding et al., 2008). Introduction Sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005; Esuli and Sebastiani, 2006) have been a useful resource for opinion mining (Kim and Hovy, 2004; Agarwal et al., 2011; Moilanen and Pulman, 2007; Choi and Cardie, 2008; Mohammad et al., 2013; Guerini et al., 2013; Vo and Zhang, 2015). Containing sentiment attributes of words such as polarities and strengths, they can serve to provide a word-level foundation for analyzing the sentiment of sentences and documents. We investigate an effective way to use sentiment lexicon features. A traditional way of deciding the sentiment of a document is to use the sum of sentiment values of More sophisticated sentence-level features such as the counts of positive and negative words, their total strength, and the maximum str"
D16-1169,baccianella-etal-2010-sentiwordnet,0,0.0278947,"se four sentiment lexicons, namely TS-Lex, S140-Lex, SD-Lex and SWN-Lex. TS-Lex3 is a large-scale sentiment lexicon built from Twitter by Tang et al. (2014a) for learning sentiment-specific phrase embeddings. S140-Lex4 is the Sentiment140 lexicon, which is built from point-wise mutual information using distant supervision (Go et al., 2009; Mohammad et al., 2013). SD-Lex is built from SST. We construct a sentiment lexicon from the training set by excluding all neutral words and adding the aforementioned offset -2 to each entry. SWN-Lex is a sentiment lexicon extracted from SentimentWordNet3.0 (Baccianella et al., 2010). For words with different partof-speech tags, we keep the minimum negative score or the maximum positive score. The original score in the SentimentWordNet3.0 is a probability value between 0 and 1, and we scale it to [-2, 2]5 . When building these lexicons, we only use the sentiment scores for unigrams. Ambiguous words are discarded. Both TS-Lex and S140-Lex are Twitter-specific sentiment lexicons. They are used in the Twitter sentiment classification task. SD-Lex and SWN-Lex are exploited for the Stanford dataset. The statistics of lexicons are listed in Table 4. 3 http://ir.hit.edu.cn/ dyta"
D16-1169,D08-1083,0,0.0763274,"iment lexicon (Turney, 2002; Hu and Liu, 2004). This simple method has been shown to give surprisingly competitive accuracies in several sentiment analysis benchmarks (Kiritchenko et al., 2014), and is still the standard practice for specific research communities with mature domain-specific lexicons, such as finance (Kearney and Liu, 2014) and product reviews (Ding et al., 2008). Introduction Sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005; Esuli and Sebastiani, 2006) have been a useful resource for opinion mining (Kim and Hovy, 2004; Agarwal et al., 2011; Moilanen and Pulman, 2007; Choi and Cardie, 2008; Mohammad et al., 2013; Guerini et al., 2013; Vo and Zhang, 2015). Containing sentiment attributes of words such as polarities and strengths, they can serve to provide a word-level foundation for analyzing the sentiment of sentences and documents. We investigate an effective way to use sentiment lexicon features. A traditional way of deciding the sentiment of a document is to use the sum of sentiment values of More sophisticated sentence-level features such as the counts of positive and negative words, their total strength, and the maximum strength, etc, have also been exploited (Kim and Hovy"
D16-1169,P07-1124,0,0.0256396,"ome examples of which are shown in Figure 1. The composition effects can exhibit intricacies such as negation over intensification (e.g. not very good), shifting (e.g. not terrific) vs flip1629 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1629–1638, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics ping negation (e.g. not acceptable), content word negation (e.g. removes my doubts) and unbounded dependencies (e.g. No body gives a good performance). Second, they cannot effectively deal with word sense variations (Devitt and Ahmad, 2007; Denecke, 2009). Guerini et al. (2013) show challenges in modeling the correlation between contextdependent posterior word sentiments and their context independent priors. For example, the sentiment value of “cold” varies between “cold beer”, “cold pizza” and “cold person” due to sense and context differences. Such variations raise difficulties for a sentiment classifier with bag-of-word nature, since they can depend on semantic information over long phrases or the full sentence. We investigate a method that can potentially address the above issues, by using a recurrent neural network to capt"
D16-1169,P14-2009,0,0.0143689,"onality is formally simpler compared with fine-grained rules such as (Taboada et al., 2011). However, it is sufficient for describing the resulting effect of complex and context-dependent operations, with the semantic composition process being modeled by LSTM. Our sentiment analyzer also enjoys a more competitive LSTM baseline compared to a traditional discrete models. Our work is also related to recent work on using deep neural networks for sentence-level sentiment analysis, which exploits convolutional (Kalchbrenner et al., 2014; Kim, 2014; Ren et al., 2016), recursive (Socher et al., 2013; Dong et al., 2014; Nguyen and Shirai, 2015) and recurrent neural networks (Liu et al., 2015; Wang et al., 2015; Zhang et al., 2016), giving highly competitive accuracies. As our baseline, LSTM (Tai et al., 2015; Li et al., 2015) stands among the best neural methods. Our model is different from these prior methods in mainly two aspects. First, we introduce sentiment lexicon features, which effectively improve classification accuracies. Second, we learn extra operation details, namely the weights on each word, automatically as hidden variables. While the baseline uses LSTM features to perform end-to-end mapping"
D16-1169,P15-1033,0,0.0197304,"mentum (Sutskever et al., 2013). The decay rate is 0.1. For initial learning rate, L2 and other hyper-parameters, we adopt the default values provided by the CNN toolkits. We select the best model parameter according to the classification accuracy on the development set. For the Twitter data, we use the glove.twitter.27B7 as pretrained word embeddings. For the Stanford dataset, following Li et al. (2015), we use glove.840B.300d8 as pretrained word embeddings. Words that do not exist in both the training set and the pretrained lookup table are treated as outof-vocabulary (OOV) words. Following Dyer et al. (2015), singletons in the training data are randomly mapped to UNK with a probability punk during training. We set punk = 0.1. All word embeddings are fine-tuned. We use dropout (Srivastava et al., 2014) in the input layer to prevent overfitting during training. One-layered BiLSTM is used for all tasks. The dimension of the hidden vector in LSTM is 150. The size of the second layer in Figure 3 is 64. 4.3 Development Results Table 5 shows results on the Twitter development set. Bi-LSTM is our model using the bias score Sbias only, which is equivalent to bidirectional LSTM model of Li et al. (2015) an"
D16-1169,esuli-sebastiani-2006-sentiwordnet,0,0.0334676,"es, achieving the best results on a Twitter benchmark. 1 Figure 1: Example sentiment compositions. all words in the document that exist in a sentiment lexicon (Turney, 2002; Hu and Liu, 2004). This simple method has been shown to give surprisingly competitive accuracies in several sentiment analysis benchmarks (Kiritchenko et al., 2014), and is still the standard practice for specific research communities with mature domain-specific lexicons, such as finance (Kearney and Liu, 2014) and product reviews (Ding et al., 2008). Introduction Sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005; Esuli and Sebastiani, 2006) have been a useful resource for opinion mining (Kim and Hovy, 2004; Agarwal et al., 2011; Moilanen and Pulman, 2007; Choi and Cardie, 2008; Mohammad et al., 2013; Guerini et al., 2013; Vo and Zhang, 2015). Containing sentiment attributes of words such as polarities and strengths, they can serve to provide a word-level foundation for analyzing the sentiment of sentences and documents. We investigate an effective way to use sentiment lexicon features. A traditional way of deciding the sentiment of a document is to use the sum of sentiment values of More sophisticated sentence-level features suc"
D16-1169,D13-1125,0,0.0233386,"Missing"
D16-1169,P14-1062,0,0.0142722,"composition features automatically. Our weighted-sum representation of semantic compositionality is formally simpler compared with fine-grained rules such as (Taboada et al., 2011). However, it is sufficient for describing the resulting effect of complex and context-dependent operations, with the semantic composition process being modeled by LSTM. Our sentiment analyzer also enjoys a more competitive LSTM baseline compared to a traditional discrete models. Our work is also related to recent work on using deep neural networks for sentence-level sentiment analysis, which exploits convolutional (Kalchbrenner et al., 2014; Kim, 2014; Ren et al., 2016), recursive (Socher et al., 2013; Dong et al., 2014; Nguyen and Shirai, 2015) and recurrent neural networks (Liu et al., 2015; Wang et al., 2015; Zhang et al., 2016), giving highly competitive accuracies. As our baseline, LSTM (Tai et al., 2015; Li et al., 2015) stands among the best neural methods. Our model is different from these prior methods in mainly two aspects. First, we introduce sentiment lexicon features, which effectively improve classification accuracies. Second, we learn extra operation details, namely the weights on each word, automatically as hidde"
D16-1169,C04-1200,0,0.0579433,"entiment compositions. all words in the document that exist in a sentiment lexicon (Turney, 2002; Hu and Liu, 2004). This simple method has been shown to give surprisingly competitive accuracies in several sentiment analysis benchmarks (Kiritchenko et al., 2014), and is still the standard practice for specific research communities with mature domain-specific lexicons, such as finance (Kearney and Liu, 2014) and product reviews (Ding et al., 2008). Introduction Sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005; Esuli and Sebastiani, 2006) have been a useful resource for opinion mining (Kim and Hovy, 2004; Agarwal et al., 2011; Moilanen and Pulman, 2007; Choi and Cardie, 2008; Mohammad et al., 2013; Guerini et al., 2013; Vo and Zhang, 2015). Containing sentiment attributes of words such as polarities and strengths, they can serve to provide a word-level foundation for analyzing the sentiment of sentences and documents. We investigate an effective way to use sentiment lexicon features. A traditional way of deciding the sentiment of a document is to use the sum of sentiment values of More sophisticated sentence-level features such as the counts of positive and negative words, their total strengt"
D16-1169,D14-1181,0,0.0173819,"tically. Our weighted-sum representation of semantic compositionality is formally simpler compared with fine-grained rules such as (Taboada et al., 2011). However, it is sufficient for describing the resulting effect of complex and context-dependent operations, with the semantic composition process being modeled by LSTM. Our sentiment analyzer also enjoys a more competitive LSTM baseline compared to a traditional discrete models. Our work is also related to recent work on using deep neural networks for sentence-level sentiment analysis, which exploits convolutional (Kalchbrenner et al., 2014; Kim, 2014; Ren et al., 2016), recursive (Socher et al., 2013; Dong et al., 2014; Nguyen and Shirai, 2015) and recurrent neural networks (Liu et al., 2015; Wang et al., 2015; Zhang et al., 2016), giving highly competitive accuracies. As our baseline, LSTM (Tai et al., 2015; Li et al., 2015) stands among the best neural methods. Our model is different from these prior methods in mainly two aspects. First, we introduce sentiment lexicon features, which effectively improve classification accuracies. Second, we learn extra operation details, namely the weights on each word, automatically as hidden variables"
D16-1169,S15-1002,0,0.0824706,"Missing"
D16-1169,D15-1278,0,0.37323,"which the weight of each sentiment word together with a sentence-level sentiment bias score are predicted. Such weights are context-sensitive, and can express flipping negation by having negative values. The advantages of the recurrent network model over existing semantic-composition-aware discrete models such as (Choi and Cardie, 2008) include its capability of representing non-local and subtle semantic features without suffering from the challenge of designing sparse manual features. On the other hand, compared with neural network models, which recently give the state-of-the-art accuracies (Li et al., 2015; Tai et al., 2015), our model has the advantage of leveraging sentiment lexicons as a useful resource. To our knowledge, we are the first to integrate the operation into sentiment lexicons and a deep neural model for sentiment analysis. The conceptually simple model gives strong empirical performances. Results on standard sentiment benchmarks show that our method gives competitive 1630 Figure 2: Overall model structure. The sentiment score of the sentence “not a bad movie at all” is a weighted sum of the scores of sentiment words “not”, ”bad” and a sentence-level bias score b. score(not) and"
D16-1169,S13-2053,0,0.144747,"2002; Hu and Liu, 2004). This simple method has been shown to give surprisingly competitive accuracies in several sentiment analysis benchmarks (Kiritchenko et al., 2014), and is still the standard practice for specific research communities with mature domain-specific lexicons, such as finance (Kearney and Liu, 2014) and product reviews (Ding et al., 2008). Introduction Sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005; Esuli and Sebastiani, 2006) have been a useful resource for opinion mining (Kim and Hovy, 2004; Agarwal et al., 2011; Moilanen and Pulman, 2007; Choi and Cardie, 2008; Mohammad et al., 2013; Guerini et al., 2013; Vo and Zhang, 2015). Containing sentiment attributes of words such as polarities and strengths, they can serve to provide a word-level foundation for analyzing the sentiment of sentences and documents. We investigate an effective way to use sentiment lexicon features. A traditional way of deciding the sentiment of a document is to use the sum of sentiment values of More sophisticated sentence-level features such as the counts of positive and negative words, their total strength, and the maximum strength, etc, have also been exploited (Kim and Hovy, 2004; Wilson et al.,"
D16-1169,S13-2052,0,0.0381509,"product, the model can also correctly identify the compositional changes on the sentiment values of each word given a sentential context. Our code is released at https://github.com/zeeeyang/lexicon rnn. 2 Related Work There exist many statistical methods that exploit sentiment lexicons (Kim and Hovy, 2004; Agarwal et al., 2011; Mohammad et al., 2013; Guerini et al., 2013; Tang et al., 2014b; Vo and Zhang, 2015; Cambria, 2016). Mohammad et al. (2013) leverage a large sentiment lexicon in a SVM model, achieving the best results in the SemEval 2013 benchmark on sentence-level sentiment analysis (Nakov et al., 2013). Compared to these methods, our model has two main advantages. First, we use a recurrent neural network to model context, thereby exploiting non-local semantic information. Second, our model offers context-sensitive operational details on each word. Several previous methods move beyond bag-ofword models in leveraging lexicons. Most notably, Moilanen and Pulman (2007) introduce the ideas from compositional semantics (Montague, 1974) into sentiment operations, developing a set of composition rules for handling negations. Along the line, Taboada et al. (2011) developed a lexicon and a collection"
D16-1169,D15-1298,0,0.0072796,"simpler compared with fine-grained rules such as (Taboada et al., 2011). However, it is sufficient for describing the resulting effect of complex and context-dependent operations, with the semantic composition process being modeled by LSTM. Our sentiment analyzer also enjoys a more competitive LSTM baseline compared to a traditional discrete models. Our work is also related to recent work on using deep neural networks for sentence-level sentiment analysis, which exploits convolutional (Kalchbrenner et al., 2014; Kim, 2014; Ren et al., 2016), recursive (Socher et al., 2013; Dong et al., 2014; Nguyen and Shirai, 2015) and recurrent neural networks (Liu et al., 2015; Wang et al., 2015; Zhang et al., 2016), giving highly competitive accuracies. As our baseline, LSTM (Tai et al., 2015; Li et al., 2015) stands among the best neural methods. Our model is different from these prior methods in mainly two aspects. First, we introduce sentiment lexicon features, which effectively improve classification accuracies. Second, we learn extra operation details, namely the weights on each word, automatically as hidden variables. While the baseline uses LSTM features to perform end-to-end mapping between sentences and sent"
D16-1169,D11-1014,0,0.0617203,"he dataset according to the released ids. The statistics of the dataset are shown in Table 1. The movie review dataset is Stanford Sentiment Treebank2 (SST) (Socher et al., 2013). For each sentence in this treebank, a corresponding constituent 2 http://nlp.stanford.edu/sentiment/index.html Lexicon SD-Lex SWN-Lex TS-Lex S140-Lex Polarity books dvds electronics music videogames Positive 19 19 19 20 20 Negative 29 20 19 20 20 Table 3: Document distribution of the mixed domain dataset. tree is given. Each internal constituent node is annotated with a sentiment label ranging from 0 to 4. We follow Socher et al. (2011) and Li et al. (2015) to perform five-class and binary classification, with the data statistics being shown in Table 2. In order to examine cross-domain robustness, we apply our model on a product review corpus (T¨ackstr¨om and McDonald, 2011), which contains 196 documents covering 5 domains: books, dvds, electronics, music and videogames. The document distribution is listed in Table 3. Lexicons. We use four sentiment lexicons, namely TS-Lex, S140-Lex, SD-Lex and SWN-Lex. TS-Lex3 is a large-scale sentiment lexicon built from Twitter by Tang et al. (2014a) for learning sentiment-specific phrase"
D16-1169,D12-1110,0,0.103596,"Missing"
D16-1169,D13-1170,0,0.212902,"of semantic compositionality is formally simpler compared with fine-grained rules such as (Taboada et al., 2011). However, it is sufficient for describing the resulting effect of complex and context-dependent operations, with the semantic composition process being modeled by LSTM. Our sentiment analyzer also enjoys a more competitive LSTM baseline compared to a traditional discrete models. Our work is also related to recent work on using deep neural networks for sentence-level sentiment analysis, which exploits convolutional (Kalchbrenner et al., 2014; Kim, 2014; Ren et al., 2016), recursive (Socher et al., 2013; Dong et al., 2014; Nguyen and Shirai, 2015) and recurrent neural networks (Liu et al., 2015; Wang et al., 2015; Zhang et al., 2016), giving highly competitive accuracies. As our baseline, LSTM (Tai et al., 2015; Li et al., 2015) stands among the best neural methods. Our model is different from these prior methods in mainly two aspects. First, we introduce sentiment lexicon features, which effectively improve classification accuracies. Second, we learn extra operation details, namely the weights on each word, automatically as hidden variables. While the baseline uses LSTM features to perform"
D16-1169,J11-2001,0,0.771968,"ues of More sophisticated sentence-level features such as the counts of positive and negative words, their total strength, and the maximum strength, etc, have also been exploited (Kim and Hovy, 2004; Wilson et al., 2005; Agarwal et al., 2011). Such lexicon features have been shown highly effective, leading to the best accuracies in the SemEval shared task (Mohammad et al., 2013). On the other hand, they are typically based on bag-of-word models, hence suffering two limitations. First, they do not explicitly handle semantic compositionality (Polanyi and Zaenen, 2006; Moilanen and Pulman, 2007; Taboada et al., 2011), some examples of which are shown in Figure 1. The composition effects can exhibit intricacies such as negation over intensification (e.g. not very good), shifting (e.g. not terrific) vs flip1629 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1629–1638, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics ping negation (e.g. not acceptable), content word negation (e.g. removes my doubts) and unbounded dependencies (e.g. No body gives a good performance). Second, they cannot effectively deal with word sense variation"
D16-1169,P15-1150,0,0.0850195,"of each sentiment word together with a sentence-level sentiment bias score are predicted. Such weights are context-sensitive, and can express flipping negation by having negative values. The advantages of the recurrent network model over existing semantic-composition-aware discrete models such as (Choi and Cardie, 2008) include its capability of representing non-local and subtle semantic features without suffering from the challenge of designing sparse manual features. On the other hand, compared with neural network models, which recently give the state-of-the-art accuracies (Li et al., 2015; Tai et al., 2015), our model has the advantage of leveraging sentiment lexicons as a useful resource. To our knowledge, we are the first to integrate the operation into sentiment lexicons and a deep neural model for sentiment analysis. The conceptually simple model gives strong empirical performances. Results on standard sentiment benchmarks show that our method gives competitive 1630 Figure 2: Overall model structure. The sentiment score of the sentence “not a bad movie at all” is a weighted sum of the scores of sentiment words “not”, ”bad” and a sentence-level bias score b. score(not) and score(bad) are prio"
D16-1169,C14-1018,0,0.533316,"nd score(bad) are prior scores obtained from sentiment lexicons. γ1 and γ3 are context-sensitive weights for sentiment words “not” and “bad”, respectively. accuracies to the state-of-the-art models in the literature. As a by-product, the model can also correctly identify the compositional changes on the sentiment values of each word given a sentential context. Our code is released at https://github.com/zeeeyang/lexicon rnn. 2 Related Work There exist many statistical methods that exploit sentiment lexicons (Kim and Hovy, 2004; Agarwal et al., 2011; Mohammad et al., 2013; Guerini et al., 2013; Tang et al., 2014b; Vo and Zhang, 2015; Cambria, 2016). Mohammad et al. (2013) leverage a large sentiment lexicon in a SVM model, achieving the best results in the SemEval 2013 benchmark on sentence-level sentiment analysis (Nakov et al., 2013). Compared to these methods, our model has two main advantages. First, we use a recurrent neural network to model context, thereby exploiting non-local semantic information. Second, our model offers context-sensitive operational details on each word. Several previous methods move beyond bag-ofword models in leveraging lexicons. Most notably, Moilanen and Pulman (2007) in"
D16-1169,P14-1146,0,0.194207,"nd score(bad) are prior scores obtained from sentiment lexicons. γ1 and γ3 are context-sensitive weights for sentiment words “not” and “bad”, respectively. accuracies to the state-of-the-art models in the literature. As a by-product, the model can also correctly identify the compositional changes on the sentiment values of each word given a sentential context. Our code is released at https://github.com/zeeeyang/lexicon rnn. 2 Related Work There exist many statistical methods that exploit sentiment lexicons (Kim and Hovy, 2004; Agarwal et al., 2011; Mohammad et al., 2013; Guerini et al., 2013; Tang et al., 2014b; Vo and Zhang, 2015; Cambria, 2016). Mohammad et al. (2013) leverage a large sentiment lexicon in a SVM model, achieving the best results in the SemEval 2013 benchmark on sentence-level sentiment analysis (Nakov et al., 2013). Compared to these methods, our model has two main advantages. First, we use a recurrent neural network to model context, thereby exploiting non-local semantic information. Second, our model offers context-sensitive operational details on each word. Several previous methods move beyond bag-ofword models in leveraging lexicons. Most notably, Moilanen and Pulman (2007) in"
D16-1169,P02-1053,0,0.0366344,"ropose a context-sensitive lexicon-based method based on a simple weighted-sum model, using a recurrent neural network to learn the sentiments strength, intensification and negation of lexicon sentiments in composing the sentiment value of sentences. Results show that our model can not only learn such operation details, but also give significant improvements over state-of-the-art recurrent neural network baselines without lexical features, achieving the best results on a Twitter benchmark. 1 Figure 1: Example sentiment compositions. all words in the document that exist in a sentiment lexicon (Turney, 2002; Hu and Liu, 2004). This simple method has been shown to give surprisingly competitive accuracies in several sentiment analysis benchmarks (Kiritchenko et al., 2014), and is still the standard practice for specific research communities with mature domain-specific lexicons, such as finance (Kearney and Liu, 2014) and product reviews (Ding et al., 2008). Introduction Sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005; Esuli and Sebastiani, 2006) have been a useful resource for opinion mining (Kim and Hovy, 2004; Agarwal et al., 2011; Moilanen and Pulman, 2007; Choi and Cardie, 2008; Moha"
D16-1169,P15-1130,0,0.00883294,"However, it is sufficient for describing the resulting effect of complex and context-dependent operations, with the semantic composition process being modeled by LSTM. Our sentiment analyzer also enjoys a more competitive LSTM baseline compared to a traditional discrete models. Our work is also related to recent work on using deep neural networks for sentence-level sentiment analysis, which exploits convolutional (Kalchbrenner et al., 2014; Kim, 2014; Ren et al., 2016), recursive (Socher et al., 2013; Dong et al., 2014; Nguyen and Shirai, 2015) and recurrent neural networks (Liu et al., 2015; Wang et al., 2015; Zhang et al., 2016), giving highly competitive accuracies. As our baseline, LSTM (Tai et al., 2015; Li et al., 2015) stands among the best neural methods. Our model is different from these prior methods in mainly two aspects. First, we introduce sentiment lexicon features, which effectively improve classification accuracies. Second, we learn extra operation details, namely the weights on each word, automatically as hidden variables. While the baseline uses LSTM features to perform end-to-end mapping between sentences and sentiments, our model uses them to induce the lexicon weights, via whic"
D16-1169,H05-1044,0,0.0996943,"ithout lexical features, achieving the best results on a Twitter benchmark. 1 Figure 1: Example sentiment compositions. all words in the document that exist in a sentiment lexicon (Turney, 2002; Hu and Liu, 2004). This simple method has been shown to give surprisingly competitive accuracies in several sentiment analysis benchmarks (Kiritchenko et al., 2014), and is still the standard practice for specific research communities with mature domain-specific lexicons, such as finance (Kearney and Liu, 2014) and product reviews (Ding et al., 2008). Introduction Sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005; Esuli and Sebastiani, 2006) have been a useful resource for opinion mining (Kim and Hovy, 2004; Agarwal et al., 2011; Moilanen and Pulman, 2007; Choi and Cardie, 2008; Mohammad et al., 2013; Guerini et al., 2013; Vo and Zhang, 2015). Containing sentiment attributes of words such as polarities and strengths, they can serve to provide a word-level foundation for analyzing the sentiment of sentences and documents. We investigate an effective way to use sentiment lexicon features. A traditional way of deciding the sentiment of a document is to use the sum of sentiment values of More sophisticate"
D16-1169,S14-2077,0,0.0223877,"Missing"
D16-1224,D15-1198,0,0.142364,"ations, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment, before finally generating the sentence"
D16-1224,W13-2322,0,0.526909,"ing for a given AMR graph. We attack the task by first partitioning the AMR graph into smaller fragments, and then generating the translation for each fragment, before finally deciding the order by solving an asymmetric generalized traveling salesman problem (AGTSP). A Maximum Entropy classifier is trained to estimate the traveling costs, and a TSP solver is used to find the optimized solution. The final model reports a BLEU score of 22.44 on the SemEval-2016 Task8 dataset. 1 ARG0 go-01 ARG0 boy Figure 1: AMR graph for “The boy wants to go”. Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. Shown in Figure 1, the nodes of an AMR graph (e.g. “boy”, “go-01” and “want01”) represent concepts, and the edges (e.g. “ARG0” and “ARG1”) represent relations between concepts. AMR jointly encodes a set of different semantic phenomena, which makes it useful in applications like question answering and semantics-based machine translation. AMR has served as an intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of"
D16-1224,C10-1012,0,0.0287772,"input AMR graph into a tree before linearization, we apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012). Our work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation. 5 Conclusion In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and that the TSP solver finds better solutions than beam search. Acknowledgments We are grateful for the help of Jeffrey Flanigan, L"
D16-1224,P13-2131,0,0.0614093,"tence for the whole AMR by ordering the translations. To cut the AMR and translate each fragment, we match the input AMR with rules, each consisting of a rooted, connected AMR fragment and a corresponding translation. These rules serve in a similar way to rules in SMT models. We learn the rules by a modified version of the sampling algorithm of Peng 2084 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2084–2089, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics et al. (2015), and use the rule matching algorithm of Cai and Knight (2013). For decoding the fragments and synthesizing the output, we define a cut to be a subset of matched rules without overlap that covers the AMR, and an ordered cut to be a cut with the rules being ordered. To generate a sentence for the whole AMR, we search for an ordered cut, and concatenate translations of all rules in the cut. TSP is used to traverse different cuts and determine the best order. Intuitively, our method is similar to phrase-based SMT, which first cuts the input sentence into phrases, then obtains the translation for each source phrase, before finally generating the target sente"
D16-1224,P14-1134,0,0.339027,"machine translation. AMR has served as an intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragment"
D16-1224,N16-1087,0,0.350788,"ammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment, before finally generating the sentence for the whole AMR by ordering the translations. To cut the AMR and translate each fragment, we match the input AMR with rules, eac"
D16-1224,C12-1083,0,0.362052,"on (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. Shown in Figure 1, the nodes of an AMR graph (e.g. “boy”, “go-01” and “want01”) represent concepts, and the edges (e.g. “ARG0” and “ARG1”) represent relations between concepts. AMR jointly encodes a set of different semantic phenomena, which makes it useful in applications like question answering and semantics-based machine translation. AMR has served as an intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been do"
D16-1224,N03-1017,0,0.0278261,"hose AMR fragments share common concepts. Otherwise the traveling cost is evaluated by a maximum entropy model, which will be discussed in detail in Section 2.4. 2.3 Rule Acquisition We extract rules from a corpus of (sentence, AMR) pairs using the method of Peng et al. (2015). Given 2086 an aligned (sentence, AMR) pair, a phrase-fragment pair is a pair ([i, j], f ), where [i, j] is a span of the sentence and f represents a connected and rooted AMR fragment. A fragment decomposition forest consists of all possible phrase-fragment pairs that satisfy the alignment agreement for phrase-based MT (Koehn et al., 2003). The rules that we use for generation are the result of applying an MCMC procedure to learn a set of likely phrase-fragment pairs from the forests containing all possible pairs. One difference from the work of Peng et al. (2015) is that, while they require the string side to be tight (does not include unaligned words on both sides), we expand the tight phrases to incorporate unaligned words on both sides. The intuition is that they do text-to-AMR parsing, which often involves discarding function words, while our task is AMR-to-text generation, and we need to be able to fill in these unaligned"
D16-1224,P02-1040,0,0.0987956,"have lower path length than others. 3 3.1 Dev 13.13 13.15 17.68 17.19 21.12 23.00 Test 16.94 14.93 18.09 17.75 22.44 23.00 Table 1: Main results. training instances, 1368 dev instances and 1371 test instances. Each instance consists of an AMR graph and a sentence representing the same meaning. Rules are extracted from the training data, and hyperparameters are tuned on the dev set. For tuning and testing, we filter out sentences that have more than 30 words, resulting in 1103 dev instances and 1055 test instances. We train a 4-gram language model (LM) with gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric. To solve the AGTSP, we use Or-tool3 . Our graph-to-string rules are reminiscent of phrase-to-string rules in phrase-based MT (PBMT). We compare our system to a baseline (PBMT) that first linearizes the input AMR graph by breadth first traversal, and then adopts the PBMT system from Moses4 to translate the linearized AMR into a sentence. To traverse the children of an AMR concept, we use the original order in the text file. The MT system is trained with the default setting on the same dataset and LM. We also compare with JAMRgen5 (Flanigan et al., 2016), which is trai"
D16-1224,K15-1004,1,0.941008,"intermediate representation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the tra"
D16-1224,D15-1136,0,0.160292,"-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment, before finally ge"
D16-1224,N15-3006,0,0.10334,"sentation for various text-to-text NLP applications, such as statistical machine translation (SMT) (Jones et al., 2012). The task of AMR-to-text generation is to generate grammatical text containing the same semantic meaning as a given AMR graph. This task is important yet also challenging since each AMR graph usually has multiple corresponding sentences, and syntactic structure and function words are abstracted away when transforming a sentence into AMR (Banarescu et al., 2013). There has been work dealing with text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). On the other hand, relatively little work has been done on AMR-to-text generation. One recent exception is Flanigan et al. (2016), who first generate a spanning tree for the input AMR graph, and then apply a tree transducer to generate the sentence. Here, we directly generate the sentence from an input AMR by treating AMR-to-text generation as a variant of the traveling salesman problem (TSP). Given an AMR as input, our method first cuts the graph into several rooted and connected fragments (sub-graphs), and then finds the translation for each fragment"
D16-1224,E09-1097,0,0.0297715,"(2016) and our work here study sentence generation from a given AMR graph. Different from Flanigan et al. (2016) who map a input AMR graph into a tree before linearization, we apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012). Our work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation. 5 Conclusion In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and th"
D16-1224,N15-1040,0,0.0534278,"Missing"
D16-1224,D09-1043,0,0.0313619,"apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012). Our work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation. 5 Conclusion In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and that the TSP solver finds better solutions than beam search. Acknowledgments We are grateful for the help of Jeffrey Flanigan, Lin Zhao, and Yifan He. This work was funded by NSF IIS-144"
D16-1224,P09-1038,0,0.143191,"4 System PBMT OnlyConceptRule OnlyInducedRule OnlyBigramLM All JAMR-gen Traveling cost Considering an AGTSP graph whose nodes are clustered into m groups, we define the traveling cost for a tour T in Equation 1: cost(ns , ne ) = − m X log p(“yes”|nTi , nTi+1 ) (1) i=0 where nT0 = ns , nTm+1 = ne and each nTi (i ∈ [1 . . . m]) belongs to a group that is different from all others. Here p(“yes”|nj , ni ) represents a learned score for a move from nj to ni . The choices before nTi are independent from choosing nTi+1 given nTi because of the Markovian property of the TSP problem. Previous methods (Zaslavskiy et al., 2009) evaluate traveling costs p(nTi+1 |nTi ) by using a language model. Inevitably some rules may only cover one translation word, making only bigram language models naturally applicable. Zaslavskiy et al. (2009) introduces a method for incorporating a trigram language model. However, as a result, the number of nodes in the AGTSP graph grows exponentially. To tackle the problem, we treat it as a local binary (“yes” or “no”) classification problem whether we should move to nj from ni . We train a maximum entropy model, where p(“yes”|ni , nj ) is defined as: p(“yes”|ni , nj ) = k hX i 1 exp λi fi (“"
D16-1224,J15-3005,1,0.852028,"k here study sentence generation from a given AMR graph. Different from Flanigan et al. (2016) who map a input AMR graph into a tree before linearization, we apply synchronous rules consisting of AMR graph fragments and text to directly transfer a AMR graph into a sentence. In addition to AMR parsing and generation, there has also been work using AMR as a semantic representation in machine translation (Jones et al., 2012). Our work also belongs to the task of text generation (Reiter and Dale, 1997). There has been work on generating natural language text from a bag of words (Wan et al., 2009; Zhang and Clark, 2015), surface syntactic trees (Zhang, 2013; Song et al., 2014), deep semantic graphs (Bohnet et al., 2010) and logical forms (White, 2004; White and Rajkumar, 2009). We are among the first to investigate generation from AMR, which is a different type of semantic representation. 5 Conclusion In conclusion, we showed that a TSP solver with a few real-valued features can be useful for AMR-totext generation. Our method is based on a set of graph to string rules, yet significantly better than a PBMT-based baseline. This shows that our rule induction algorithm is effective and that the TSP solver finds"
D16-2004,W11-1704,0,\N,Missing
D16-2004,W02-1011,0,\N,Missing
D16-2004,S15-2082,0,\N,Missing
D16-2004,S14-2004,0,\N,Missing
D16-2004,R11-1113,0,\N,Missing
D16-2004,S16-1002,0,\N,Missing
D17-1006,N15-1122,0,0.0125799,"nts e1 , e2 , ..., en−1 into a sequence of hidden vectors h1 , h2 , ..., hn−1 , which encode the temporal order. Given a next event candidate ec , the recurrent network takes one further step from hn−1 to derive its hidden vector hc , which encodes ec . Third, hc is paired with h1 , h2 , ..., hn−1 individually, and passed to a dynamic memory network to learn the relatedness score s. s is used to denote the connectedness between the candidate subsequent event and the context event chain. Figure 2: Multiple choice narrative cloze. The gold subsequent event is marked in bold. Zaman et al., 2013; Abend et al., 2015), evaluated using different metrics. There has also been work using graph models to induce frames, which emphasize more on learning event structures and less on temporal orders (Chambers, 2013; Cheung et al., 2013). The above methods focus on one of the two subtasks we consider here. Frermann et al. (2014) used a Bayesian model to jointly cluster web collections of explicit event sequence and learn input event-pair temporal orders. However, their work is under a different input setting (Regneri et al., 2010), not learning event chains from texts. Mostafazadeh et al. (2016) proposed the story c"
D17-1006,D13-1178,0,0.0459495,"ask for script modeling from raw text. Below we summarize such investigations. With respect to event representation, Chambers and Jurafsky (2008) casted narrative events as triples of the form hevent, dependencyi, where the event is typically represented by a verb and the dependency represents typed dependency relations between the event and a protagonist, such as “subject” and “object”. Chambers and Jurafsky (2008) organized narrative chains around a central actor, or protagonist, mining events that share a common protagonist from texts by using a syntactic parser and a coreference resolver. Balasubramanian et al. (2013) observed that the protagonist representation of event chains can suffer from weaknesses such as lack of coherence, and proposed to represent events as harg1 , relation, arg2 i, where arg1 and arg2 represent the subject and object, respectively. Such representation is inspired by open information extraction (Mausam et al., 2012), and offers richer features for event pair modeling. Pichotta and Mooney (2014) adpoted a similar idea, using v(es , eo , ep ) to represent an event, where v is a verb lemma, es is the subject, eo is the object, and ep is an entity with prepositional relation to v. The"
D17-1006,D13-1185,0,0.0412288,"hn−1 to derive its hidden vector hc , which encodes ec . Third, hc is paired with h1 , h2 , ..., hn−1 individually, and passed to a dynamic memory network to learn the relatedness score s. s is used to denote the connectedness between the candidate subsequent event and the context event chain. Figure 2: Multiple choice narrative cloze. The gold subsequent event is marked in bold. Zaman et al., 2013; Abend et al., 2015), evaluated using different metrics. There has also been work using graph models to induce frames, which emphasize more on learning event structures and less on temporal orders (Chambers, 2013; Cheung et al., 2013). The above methods focus on one of the two subtasks we consider here. Frermann et al. (2014) used a Bayesian model to jointly cluster web collections of explicit event sequence and learn input event-pair temporal orders. However, their work is under a different input setting (Regneri et al., 2010), not learning event chains from texts. Mostafazadeh et al. (2016) proposed the story close task (SCT), which is to predict the ending given a unfinished story. Our narrative chain prediction task can be regarded as a sub task in the story close task, which can contribute as a m"
D17-1006,P08-1090,0,0.307554,"ious event sequence in a scenario. We investigate the modeling of stereotypical event chains, which is remotely similar to language modeling, but with events being more sparse and flexibly ordered than words. Our work follows a recent line of NLP research on script learning. Stereotypical knowledge about partially-ordered events, together with their participant roles such as “customer”, “waiter”, and “table”, is conventionally referred to as scripts (Schank et al., 1977). NLP algorithms have been investigated for automatically inducing scripts from unstructured texts (Mooney and DeJong, 1985; Chambers and Jurafsky, 2008). In particular, Chambers and Jurafsky (2008) made a first attempt to learn scripts from test inducing event Introduction Frequently recurring sequences of events in prototypical scenarios, such as visiting a restaurant and driving to work, are a useful source of world knowledge. Two examples are shown in Figure 1, which are different variations of the “restaurant visiting” scenario, where events are partially ordered and can be flexible. Such knowledge is useful for natural language understanding because texts typically do not include event details when mentioning a scenario. For example, the"
D17-1006,P82-1020,0,0.804879,"Missing"
D17-1006,E12-1034,0,0.72433,"the embeddings of event and arguments by ordering them into a pseudo sentence. Modi (2016) utilized word embeddings of verbs and arguments directly, using a hidden layer to automatically consolidate word embedding into a single structured event embeddings. We follow Modi (2016) and use a hidden layer to learn event argument compositions given word embeddings, training the composition function as a part of the event chain learning process. is also studied as templates in information extraction (Sundheim, 1991). Chambers and Jurafsky (2008) pioneered the recent line of work on script induction (Jans et al., 2012; Pichotta and Mooney, 2016; Granroth-Wilding and Clark, 2016), where the focus is on modeling narrative event chains, a crucial subtask for script modeling from raw text. Below we summarize such investigations. With respect to event representation, Chambers and Jurafsky (2008) casted narrative events as triples of the form hevent, dependencyi, where the event is typically represented by a verb and the dependency represents typed dependency relations between the event and a protagonist, such as “subject” and “object”. Chambers and Jurafsky (2008) organized narrative chains around a central act"
D17-1006,D12-1048,0,0.041918,"and a protagonist, such as “subject” and “object”. Chambers and Jurafsky (2008) organized narrative chains around a central actor, or protagonist, mining events that share a common protagonist from texts by using a syntactic parser and a coreference resolver. Balasubramanian et al. (2013) observed that the protagonist representation of event chains can suffer from weaknesses such as lack of coherence, and proposed to represent events as harg1 , relation, arg2 i, where arg1 and arg2 represent the subject and object, respectively. Such representation is inspired by open information extraction (Mausam et al., 2012), and offers richer features for event pair modeling. Pichotta and Mooney (2014) adpoted a similar idea, using v(es , eo , ep ) to represent an event, where v is a verb lemma, es is the subject, eo is the object, and ep is an entity with prepositional relation to v. Their representation is used by subsequent work such as Modi (2016) and Granroth-Wilding and Clark (2016). We follow Pichotta and Mooney (2016) in our event representation form. Mitigating the sparsity issue of event representations, neural methods can capture temporal orders between events beyond skip n-grams. Our model integrates"
D17-1006,E14-1024,0,0.783552,"(2008) organized narrative chains around a central actor, or protagonist, mining events that share a common protagonist from texts by using a syntactic parser and a coreference resolver. Balasubramanian et al. (2013) observed that the protagonist representation of event chains can suffer from weaknesses such as lack of coherence, and proposed to represent events as harg1 , relation, arg2 i, where arg1 and arg2 represent the subject and object, respectively. Such representation is inspired by open information extraction (Mausam et al., 2012), and offers richer features for event pair modeling. Pichotta and Mooney (2014) adpoted a similar idea, using v(es , eo , ep ) to represent an event, where v is a verb lemma, es is the subject, eo is the object, and ep is an entity with prepositional relation to v. Their representation is used by subsequent work such as Modi (2016) and Granroth-Wilding and Clark (2016). We follow Pichotta and Mooney (2016) in our event representation form. Mitigating the sparsity issue of event representations, neural methods can capture temporal orders between events beyond skip n-grams. Our model integrates the advantages of strong-order learning and event-pair learning by using LSTM h"
D17-1006,P10-1100,0,0.0542295,"choice narrative cloze. The gold subsequent event is marked in bold. Zaman et al., 2013; Abend et al., 2015), evaluated using different metrics. There has also been work using graph models to induce frames, which emphasize more on learning event structures and less on temporal orders (Chambers, 2013; Cheung et al., 2013). The above methods focus on one of the two subtasks we consider here. Frermann et al. (2014) used a Bayesian model to jointly cluster web collections of explicit event sequence and learn input event-pair temporal orders. However, their work is under a different input setting (Regneri et al., 2010), not learning event chains from texts. Mostafazadeh et al. (2016) proposed the story close task (SCT), which is to predict the ending given a unfinished story. Our narrative chain prediction task can be regarded as a sub task in the story close task, which can contribute as a major approach. On the other hand, information beyond event chains can be useful for the story close task. 3 Model 4.1 Event Representation We learn vector representations of standard events by composing pre-trained word embeddings of its verb and arguments. The skipgram model (Mikolov et al., 2013) is used to train word"
D17-1006,D15-1195,0,0.106682,"Missing"
D17-1006,K16-1008,0,0.239407,"al., 2015). While the above methods are statistical, neural network models have recently been used for event sequence modeling. Granroth-Wilding and Clark (2016) used a Siamese Network instead of PMI to calculate the coherence between two events. Rudinger et al. (2015) extended the idea of Jans et al. (2012) by using a log-bilinear neural language model (Mnih and Hinton, 2007) to calculate event probabilities. By learning embeddings for reducing sparsity, the above models give much better results compared to the models of Chambers and Jurafsky (2008) and Jans et al. (2012). Similar in spirit, Modi (2016) predicted the probability of an event belonging to a certain event chain by modeling known events in the chain as a bag of vectors, showing that it outperforms discrete statistical methods. These neural methods are consistent with the earlier statistical models in leveraging event-pair relations. • We make a systematic comparison of LSTM and pair-based event sequence learning methods using the same benchmarks. Pichotta and Mooney (2016) experimented with LSTM for script learning, using an existing sequence of events to predict the probability of a next event, which outperformed strong discret"
D17-1006,W14-1606,0,0.0485016,"mporal order of events in a full sequence. Event-pair models have so far been the dominant method in the literature. Earlier work used discrete event representations and estimated event relations by statistical counting. As mentioned earlier, Chambers and Jurafsky (2008) used PMI to calculate event relations, and Jans et al. (2012) used skip bigram probabilites to the same end, which is order-sensitive. Most subsequent methods followed Jans et al. (2012) in using skip n-grams (Pichotta and Mooney, 2014; Rudinger et al., 2015). Other related work includes learning temporal relations of events (Modi and Titov, 2014; Uz59 sequent event given a chain of events. For evaluation, we solve the multi-choice narrative cloze task: given a chain of events and a set of candidate next events, the most likely candidate is chosen as the output. Entities X = Customer, Y = Waiter Context(ei) walk(X, restaurant), seat(X), order(X, food), serve(Y, food) eat(X, food), make(X, payment), c1: receive(X, response) c2: drive(X, mile) c3: seem(X) c4: discover(X, truth) c5: leave(X, restaurant) 4 ? The overall structure of our model is shown in Figure 3, which has three main components. First, given an event v(a0 , a1 , a2 ), a"
D17-1006,1985.tmi-1.17,0,0.167931,"happen next given a previous event sequence in a scenario. We investigate the modeling of stereotypical event chains, which is remotely similar to language modeling, but with events being more sparse and flexibly ordered than words. Our work follows a recent line of NLP research on script learning. Stereotypical knowledge about partially-ordered events, together with their participant roles such as “customer”, “waiter”, and “table”, is conventionally referred to as scripts (Schank et al., 1977). NLP algorithms have been investigated for automatically inducing scripts from unstructured texts (Mooney and DeJong, 1985; Chambers and Jurafsky, 2008). In particular, Chambers and Jurafsky (2008) made a first attempt to learn scripts from test inducing event Introduction Frequently recurring sequences of events in prototypical scenarios, such as visiting a restaurant and driving to work, are a useful source of world knowledge. Two examples are shown in Figure 1, which are different variations of the “restaurant visiting” scenario, where events are partially ordered and can be flexible. Such knowledge is useful for natural language understanding because texts typically do not include event details when mentionin"
D17-1006,H91-1059,0,0.53854,"ranroth-Wilding and Clark (2016) leveraged the skip-gram model of Mikolov et al. (2013) for training the embeddings of event and arguments by ordering them into a pseudo sentence. Modi (2016) utilized word embeddings of verbs and arguments directly, using a hidden layer to automatically consolidate word embedding into a single structured event embeddings. We follow Modi (2016) and use a hidden layer to learn event argument compositions given word embeddings, training the composition function as a part of the event chain learning process. is also studied as templates in information extraction (Sundheim, 1991). Chambers and Jurafsky (2008) pioneered the recent line of work on script induction (Jans et al., 2012; Pichotta and Mooney, 2016; Granroth-Wilding and Clark, 2016), where the focus is on modeling narrative event chains, a crucial subtask for script modeling from raw text. Below we summarize such investigations. With respect to event representation, Chambers and Jurafsky (2008) casted narrative events as triples of the form hevent, dependencyi, where the event is typically represented by a verb and the dependency represents typed dependency relations between the event and a protagonist, such"
D17-1006,N16-1098,0,0.0966759,"n bold. Zaman et al., 2013; Abend et al., 2015), evaluated using different metrics. There has also been work using graph models to induce frames, which emphasize more on learning event structures and less on temporal orders (Chambers, 2013; Cheung et al., 2013). The above methods focus on one of the two subtasks we consider here. Frermann et al. (2014) used a Bayesian model to jointly cluster web collections of explicit event sequence and learn input event-pair temporal orders. However, their work is under a different input setting (Regneri et al., 2010), not learning event chains from texts. Mostafazadeh et al. (2016) proposed the story close task (SCT), which is to predict the ending given a unfinished story. Our narrative chain prediction task can be regarded as a sub task in the story close task, which can contribute as a major approach. On the other hand, information beyond event chains can be useful for the story close task. 3 Model 4.1 Event Representation We learn vector representations of standard events by composing pre-trained word embeddings of its verb and arguments. The skipgram model (Mikolov et al., 2013) is used to train word vectors. For arguments that consist of more than one word, we use"
D17-1006,S13-2001,0,0.0542293,"Missing"
D17-1006,P07-2009,0,\N,Missing
D17-1006,E14-1006,0,\N,Missing
D17-1079,P14-2131,0,0.0170647,"Missing"
D17-1079,P16-1039,0,0.348057,"ing self-training and tri-training methods for leveraging auto-segmented data. Neural parsers have benefited from automatically labeled data via dependencycontext word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarow"
D17-1079,D13-1129,1,0.896381,"Missing"
D17-1079,C14-1078,1,0.843812,"Missing"
D17-1079,P15-1168,0,0.622783,"cter embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra cla"
D17-1079,D14-1093,1,0.462459,"Missing"
D17-1079,D15-1141,0,0.602486,"Missing"
D17-1079,I05-3025,0,0.0362245,"Empirical Methods in Natural Language Processing, pages 760–766 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics B Scoring Layer B M Concat M E Concat E S Concat Concat S Concat Representation Layer LSTMb LSTMf Concat Look up Table LSTMb LSTMf Concat Concat 马 </S>马 LSTMb LSTMf Concat Concat 上 马上 LSTMb LSTMf Concat Concat 就 上就 Concat 来 就来 来</S> Figure 1: Baseline model architecture. representation at the ith position. acter in the sentence is assigned a segment label from left to right, including {B, M, E, S}, to indicate the segmentation (Xue, 2003; Low et al., 2005; Zhao et al., 2006). B, M, E represent the character is the beginning, middle or end of a multi-character word, respectively. S represents that the current character is a single character word. Following Chen et al. (2015b), a standard biLSTM model (Graves, 2008) is used to assign segmentation label for each character. As shown in Figure 1, our model consists of a representation layer and a scoring layer. The representation layer utilizes a bi-LSTM to capture the context of each character in the sentence. Given a sentence {w1 , w2 , w3 , · · · , wN }, where wi is the ith character in the sent"
D17-1079,P15-1167,0,0.0923753,"estigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014)"
D17-1079,D16-1257,0,0.0260578,"Missing"
D17-1079,N06-1020,0,0.0153109,"their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra classifiers, taking the instances with the same labels for additional training data. A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features. ∗ 2 Baseline Segmentation Model Chinese word segmentation can be regarded as a character sequence labeling task, where each charEqual contributions 760 Proceedings of the 2017 Conference on Empir"
D17-1079,D10-1002,0,0.0159819,"f feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra classifiers, taking the instances with the same labels for additional training data. A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features. ∗ 2 Baseline Segmentation Model Chinese word segmentation can be regarded as a character sequence labeling task, where each charEqual contributions 760 Proceedings of the 2017 Conference on Empirical Methods in Natu"
D17-1079,N16-1118,0,0.0378788,"Missing"
D17-1079,P14-2050,0,0.0361789,"unigram wi and character bigram wi−1 wi , respectively. A forward word representation efi is calculated as follows: ri = concat2 (rilstm−f , rilstm−b ) = tanh(W2 [rilstm−f ; rilstm−b ]) Given the representation ri , we use a scoring unit to score for each potential segment label. Given ri , the score of segment label M is: i fM = WM h, where h = concat3 (ri , eM ), = tanh(W3 [ri ; eM ]) WM is the score matrix for label M, and eM is the label embedding for label M. 3 Word-Context Character Embeddings Our model structure is a derivation from the skipgram model (Mikolov et al., 2013), similar to Levy and Goldberg (2014). Given a sentence with length n: {w1 , w2 , w3 , · · · wn } and its corresponding segment labels: {l1 , l2 , l3 , · · · ln }, the pre-training context of current character wt is the around characters in the windows with size c, together with their corresponding segment labels (Figure 2). Characters wi and labels li in the context are represented by vectors ecwi ∈ Rd and ecli ∈ Rd , respectively, where d is the embedding dimensionality. The word-context embedding of character wt is represented as ewt ∈ Rd , which is trained by predicting the surrounding context representations ecw′ efi = conca"
D17-1079,P14-1043,0,0.01322,"d Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra classifiers, taking the instances with the same labels for additional training data. A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features. ∗ 2 Baseline Segmentation Model Chinese word segmentation can be regarded as a character sequence labeling task, where each charEqual contributions 760 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 760–766 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics B Sco"
D17-1079,D16-1046,0,0.0609597,"Missing"
D17-1079,N15-1142,0,0.0336542,"dicting the surrounding context representations ecw′ efi = concat1 (ewi , ewi−1 wi ), = tanh(W1 [ewi ; ewi−1 wi ]) A backward representation ebi can be obtained in the same way. Then efi and ebi are fed into forward and backward LSTM units at current position, obtaining the corresponding forward and backward LSTM representations rilstm−f and rilstm−b , respectively. In the scoring layer, we first obtain a linear combination of rilstm−f and rilstm−b , which is the final 761 and ecli , parameterizing the labeled segmentation information in the embedding parameters. To capture order information (Ling et al., 2015), we use different embedding matrices for context embedding in different context positions, training different embeddings for the same word when they reside on different locations as the context word. In particular, our context window size is five. As a result, each word has four different versions of ec , namely ec−1 , ec−2 , ec+1 , and ec+2 , each taking a distinct embedding matrix. Given the context window [w−2 , w−1 , w, w+1 , w+2 ], w−1 is the left first context word of the focus word w, ec−1,wi will be selected from embedding matrix E−1 , and w+1 is the right first word of w, ec+1,wi wil"
D17-1079,P14-1028,0,0.295876,"embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li,"
D17-1079,N09-1007,0,0.0471648,"Missing"
D17-1079,I05-3027,0,0.631704,"Missing"
D17-1079,W06-0127,0,0.253287,"in Natural Language Processing, pages 760–766 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics B Scoring Layer B M Concat M E Concat E S Concat Concat S Concat Representation Layer LSTMb LSTMf Concat Look up Table LSTMb LSTMf Concat Concat 马 </S>马 LSTMb LSTMf Concat Concat 上 马上 LSTMb LSTMf Concat Concat 就 上就 Concat 来 就来 来</S> Figure 1: Baseline model architecture. representation at the ith position. acter in the sentence is assigned a segment label from left to right, including {B, M, E, S}, to indicate the segmentation (Xue, 2003; Low et al., 2005; Zhao et al., 2006). B, M, E represent the character is the beginning, middle or end of a multi-character word, respectively. S represents that the current character is a single character word. Following Chen et al. (2015b), a standard biLSTM model (Graves, 2008) is used to assign segmentation label for each character. As shown in Figure 1, our model consists of a representation layer and a scoring layer. The representation layer utilizes a bi-LSTM to capture the context of each character in the sentence. Given a sentence {w1 , w2 , w3 , · · · , wN }, where wi is the ith character in the sentence, and N is the s"
D17-1079,D13-1061,0,0.12946,"endencycontext word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-trai"
D17-1079,P13-1043,1,0.546747,"Missing"
D17-1079,P15-1032,0,0.00999897,"Missing"
D17-1079,P16-2092,0,0.103199,"Missing"
D17-1079,O03-4002,0,0.708664,"nference on Empirical Methods in Natural Language Processing, pages 760–766 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics B Scoring Layer B M Concat M E Concat E S Concat Concat S Concat Representation Layer LSTMb LSTMf Concat Look up Table LSTMb LSTMf Concat Concat 马 </S>马 LSTMb LSTMf Concat Concat 上 马上 LSTMb LSTMf Concat Concat 就 上就 Concat 来 就来 来</S> Figure 1: Baseline model architecture. representation at the ith position. acter in the sentence is assigned a segment label from left to right, including {B, M, E, S}, to indicate the segmentation (Xue, 2003; Low et al., 2005; Zhao et al., 2006). B, M, E represent the character is the beginning, middle or end of a multi-character word, respectively. S represents that the current character is a single character word. Following Chen et al. (2015b), a standard biLSTM model (Graves, 2008) is used to assign segmentation label for each character. As shown in Figure 1, our model consists of a representation layer and a scoring layer. The representation layer utilizes a bi-LSTM to capture the context of each character in the sentence. Given a sentence {w1 , w2 , w3 , · · · , wN }, where wi is the ith cha"
D17-1079,P95-1026,0,0.657468,"2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling large-scale data. Self-training (Yarowsky, 1995; McClosky et al., 2006; Huang et al., 2010; Liu and Zhang, 2012) labels additional data by using the base classifier itself, and tri-training (Zhou and Li, 2005; Li et al., 2014) uses two extra classifiers, taking the instances with the same labels for additional training data. A second semi-supervised learning method in NLP is knowledge distillation, which extracts knowledge from large-scale auto-labeled data as features. ∗ 2 Baseline Segmentation Model Chinese word segmentation can be regarded as a character sequence labeling task, where each charEqual contributions 760 Proceedings of the 2"
D17-1079,D13-1031,0,0.0880344,"Missing"
D17-1079,E14-1062,1,0.919559,"Missing"
D17-1079,P16-1040,1,0.632749,"espectively, significantly out-performing self-training and tri-training methods for leveraging auto-segmented data. Neural parsers have benefited from automatically labeled data via dependencycontext word embeddings. We investigate training character embeddings on a word-based context in a similar way, showing that the simple method significantly improves state-of-the-art neural word segmentation models, beating tritraining baselines for leveraging autosegmented data. 1 Yue Zhang Singapore University of Technology and Design Introduction Neural network Chinese word segmentation (CWS) models (Zhang et al., 2016; Liu et al., 2016; Cai and Zhao, 2016) appeal for their strong ability of feature representation, employing unigram and bigram character embeddings as input features (Zheng et al., 2013; Pei et al., 2014; Ma and Hinrichs, 2015; Chen et al., 2015a). They give state-of-the-art performances. We investigate leveraging automatically segmented texts for enhancing their accuracies. Such semi-supervised methods can be divided into two main categories. The first one is bootstrapping, which includes self-training and tritraining. The idea is to generate more training instances by automatically labeling"
D17-1079,P07-1106,1,0.78255,"Missing"
D17-1170,D16-1171,0,0.229437,"ws of the user on other products, and two types of outputs, namely a customized rating score and a customized review. The ideal solution should consider the interaction between all given types of information, jointly predicting the two types of outputs. This poses significant challenges to statistical models, which require manually defined features to capture relevant patterns from training data. Deep learning is a relatively more feasible choice, offering viabilities of information fusion by fully connected hidden layers (Collobert et al., 2011; Henderson et al., 2013; Zhang and Weiss, 2016; Chen et al., 2016a). We leverage this advantage in building our model. In particular, we use a sub RNN to model the semantic content of each review. A sub product model is used to consolidate existing reviews for the target product, and a user model is built by consolidating the reviews of the given user into a single vector form. To address potential sparsity of a user’s history reviews, neighbor users are identified by collaborative filtering (Ding et al., 2006), and a vector representation is learned by using a neural neighborhood model. Finally, a deep memory network is utilized to find the association bet"
D17-1170,C10-1039,0,0.181478,"arization, which constructs natural language summaries for multiple product reviews (Hu and Liu, 2004). Most previous work extracts opinion words and aspect terms. Typical approaches include association mining of frequent candidate aspects (Hu and Liu, 2004; Qiu et al., 2011), sequence labeling based methods (Jakob and Gurevych, 2010; Yang and Cardie, 2013), as well as topic modeling techniques (Lin and He, 2009). Recently, word embeddings and recurrent neural networks are also used to extract aspect terms (Irsoy and Cardie, 2014; Liu et al., 2015). While all the methods above are extractive, Ganesan et al. (2010) presented a graph-based summarization framework to generate concise abstractive summaries of highly redundant opinions, and Wang and Ling (2016) used an attention-based neural network model to absorb information from multiple text units and generate summaries of movie reviews. We also 1627 perform abstractive summarization. However, different from the above research, which summarize existing reviews, we generate customized reviews for a unreviewed product. Recommendation. has been solved on mainly purchase history. There are two main approaches, which are content-based and collaborativefilter"
D17-1170,I13-1156,0,0.0216174,"ee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015), Review rating prediction aims to predict the numeric rating of a given review. Pang and Lee (2005) pioneered this task by regarding it as a classification/regression problem. Most subsequent work focuses on designing effective textural features of reviews (Qu et al., 2010; Li et al., 2011; Wan, 2013). User information has been widely investigated in sentiment analysis. Gao et al. (2013) developed user-specific features to capture user leniency, and Li et al. (2014) incorporated textual topic and userword factors through topic modeling. For integrating user information into neural network models, Tang et al. (2015) predicted the rating score given a review by using both lexical semantic information and a user embedding model. Chen et al. (2016b) proposed a neural network to incorporate global user and product information for sentiment classification via an attention mechanism. Different from the above research, which focuses on predicting the opinion on existing reviews, our"
D17-1170,J13-4006,0,0.0790331,"ng reviews of the target product, and the reviews of the user on other products, and two types of outputs, namely a customized rating score and a customized review. The ideal solution should consider the interaction between all given types of information, jointly predicting the two types of outputs. This poses significant challenges to statistical models, which require manually defined features to capture relevant patterns from training data. Deep learning is a relatively more feasible choice, offering viabilities of information fusion by fully connected hidden layers (Collobert et al., 2011; Henderson et al., 2013; Zhang and Weiss, 2016; Chen et al., 2016a). We leverage this advantage in building our model. In particular, we use a sub RNN to model the semantic content of each review. A sub product model is used to consolidate existing reviews for the target product, and a user model is built by consolidating the reviews of the given user into a single vector form. To address potential sparsity of a user’s history reviews, neighbor users are identified by collaborative filtering (Ding et al., 2006), and a vector representation is learned by using a neural neighborhood model. Finally, a deep memory netwo"
D17-1170,P82-1020,0,0.825439,"Missing"
D17-1170,D14-1080,0,0.0309754,"eviewed. Opinion Summarization. Our work also overlaps with to the area of opinion summarization, which constructs natural language summaries for multiple product reviews (Hu and Liu, 2004). Most previous work extracts opinion words and aspect terms. Typical approaches include association mining of frequent candidate aspects (Hu and Liu, 2004; Qiu et al., 2011), sequence labeling based methods (Jakob and Gurevych, 2010; Yang and Cardie, 2013), as well as topic modeling techniques (Lin and He, 2009). Recently, word embeddings and recurrent neural networks are also used to extract aspect terms (Irsoy and Cardie, 2014; Liu et al., 2015). While all the methods above are extractive, Ganesan et al. (2010) presented a graph-based summarization framework to generate concise abstractive summaries of highly redundant opinions, and Wang and Ling (2016) used an attention-based neural network model to absorb information from multiple text units and generate summaries of movie reviews. We also 1627 perform abstractive summarization. However, different from the above research, which summarize existing reviews, we generate customized reviews for a unreviewed product. Recommendation. has been solved on mainly purchase h"
D17-1170,D10-1101,0,0.0454337,"view text. The difference originates from the objective. Previous research aims to predict opinions on reviewed products, while our task is to recommend opinion on new products, which the user has not reviewed. Opinion Summarization. Our work also overlaps with to the area of opinion summarization, which constructs natural language summaries for multiple product reviews (Hu and Liu, 2004). Most previous work extracts opinion words and aspect terms. Typical approaches include association mining of frequent candidate aspects (Hu and Liu, 2004; Qiu et al., 2011), sequence labeling based methods (Jakob and Gurevych, 2010; Yang and Cardie, 2013), as well as topic modeling techniques (Lin and He, 2009). Recently, word embeddings and recurrent neural networks are also used to extract aspect terms (Irsoy and Cardie, 2014; Liu et al., 2015). While all the methods above are extractive, Ganesan et al. (2010) presented a graph-based summarization framework to generate concise abstractive summaries of highly redundant opinions, and Wang and Ling (2016) used an attention-based neural network model to absorb information from multiple text units and generate summaries of movie reviews. We also 1627 perform abstractive su"
D17-1170,D14-1181,0,0.00425637,"a neural neighborhood model. Finally, a deep memory network is utilized to find the association between the user and target product, jointly yielding the rating score and customised review. Experiments on a Yelp dataset show that the model outperforms several pipelined baselines. We make our source code publicly available under GPL at https://github.com/ wangzq870305/opinion_recommend. 2 Related Work Sentiment Analysis. Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015), Review rating prediction aims to predict the numeric rating of a given review. Pang and Lee (2005) pioneered this task by regarding it as a classification/regression problem. Most subsequent work focuses on designing effective textural features of reviews (Qu et al., 2010; Li et al., 2011; Wan, 2013). User information has been widely investigated in sentiment analysis. Gao et al. (2013) developed user-specific features to capture user leniency, and Li et al. (2014) incorporated"
D17-1170,P09-1028,0,0.201408,"a vector. Taking the hidden state {hU1 , ...hU2 , ..., hUnu } of user model as input, the attention model outputs, a continuous vector vU ∈ Rd×1 , which is computed as a weighted sum of each hidden state hUi , namely vU = nu X αi hUi (1) the neighborhood model vN using the neighborhood reviews RN = {rN1 , rN2 , ..., rNnn } with an attention recurrent network. A key issue in building the neighborhood model is how to find neighbors of a certain user. In this study, we use matrix factorization (Koren, 2008) to detect neighbors, which is a standard approach for recommendation (Ding et al., 2006; Li et al., 2009; He et al., 2016). In particular, users’ rating scores of products are used to build a productusers matrix M ∈ Rnt ×nu with nt products and nu users. We approximate it using three factors, specifying soft membership of products and users (Ding et al., 2006) by finding: min ||M − F ST T || F,S,T s.t.S ≥ 0, F ≥ 0, T ≥ 0 where F ∈ Rnt ×K represents the posterior probability of K topic clusters for each product; S ∈ RK×K encodes the distribution of each topic k; and T ∈ RK×nu indicates the posterior probability of K topic clusters for each user. As a result of matrix factorization, we directly ob"
D17-1170,W04-1013,0,0.0112387,"g reviews of the target service (restaurant) are used for the product model. The rating score given by each user to the target service is considered as the gold customized rating score, and the review of the target service given by 1631 1 2 https://code.google.com/p/word2vec/ https://www.yelp.com/academic dataset each user is used as the gold-standard customized review for the user. The remaining reviews of each user are used for training the user model. We use 3,000 user-product pairs to train the model, 1,000 pairs as testing data, and remaining data for development. We use the ROUGE-1.5.5 (Lin, 2004) toolkit for evaluating the performance of customized review generation, and report unigram overlap (ROUGE-1) as a means of assessing informativeness. Mean Square Error (MSE) (Wan, 2013; Tang et al., 2015) is used as the evaluation metric for measuring the performance of customized rating score prediction. MSE penalizes more severe errors more heavily. Joint -user -neighbor -user,-neighbor -rating -generation Rating 0.904 1.254 1.162 1.342 1.042 Generation 0.267 0.220 0.245 0.205 0.254 - Table 2: Feature ablation tests. 0.28 0.27 ROUGE-1 0.26 0.25 0.24 0.23 0.22 0.21 0.20 4.2 0 Hyper-parameter"
D17-1170,D15-1168,0,0.112943,"rediction aims to predict the numeric rating of a given review. Pang and Lee (2005) pioneered this task by regarding it as a classification/regression problem. Most subsequent work focuses on designing effective textural features of reviews (Qu et al., 2010; Li et al., 2011; Wan, 2013). User information has been widely investigated in sentiment analysis. Gao et al. (2013) developed user-specific features to capture user leniency, and Li et al. (2014) incorporated textual topic and userword factors through topic modeling. For integrating user information into neural network models, Tang et al. (2015) predicted the rating score given a review by using both lexical semantic information and a user embedding model. Chen et al. (2016b) proposed a neural network to incorporate global user and product information for sentiment classification via an attention mechanism. Different from the above research, which focuses on predicting the opinion on existing reviews, our task is to recommend the score that a user would give to a new product without knowing his review text. The difference originates from the objective. Previous research aims to predict opinions on reviewed products, while our task is"
D17-1170,P10-2060,0,0.0204773,"l to have a brief summary of all reviews, which ideally should be customized to the reader. To address the limitations above, we propose a new task called opinion recommendation, which is to generate a customized review score of the product that the user is likely to give, as well as a customized review that the user would have written for the target product, if the user had reviewed the product. The proposed opinion recommendation task is closely related to several existing lines of work in NLP. The first is sentiment analysis (Hu and Liu, 2004; Pang and Lee, 2008) and opinion summarization (Nishikawa et al., 2010; Wang and Ling, 2016), which is to give a rating score or summary based on existing customer reviews. Our 1626 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1626–1637 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics task is different in that we aim to generate user rating scores and review of a product unreviewed by the user. The second is recommendation (Su and Khoshgoftaar, 2009; Yang et al., 2014), which is to give a ranking score to a certain product or service based on the purchase history of the u"
D17-1170,P05-1015,0,0.115561,"a Yelp dataset show that the model outperforms several pipelined baselines. We make our source code publicly available under GPL at https://github.com/ wangzq870305/opinion_recommend. 2 Related Work Sentiment Analysis. Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015), Review rating prediction aims to predict the numeric rating of a given review. Pang and Lee (2005) pioneered this task by regarding it as a classification/regression problem. Most subsequent work focuses on designing effective textural features of reviews (Qu et al., 2010; Li et al., 2011; Wan, 2013). User information has been widely investigated in sentiment analysis. Gao et al. (2013) developed user-specific features to capture user leniency, and Li et al. (2014) incorporated textual topic and userword factors through topic modeling. For integrating user information into neural network models, Tang et al. (2015) predicted the rating score given a review by using both lexical semantic inf"
D17-1170,J11-1002,0,0.0470612,"r would give to a new product without knowing his review text. The difference originates from the objective. Previous research aims to predict opinions on reviewed products, while our task is to recommend opinion on new products, which the user has not reviewed. Opinion Summarization. Our work also overlaps with to the area of opinion summarization, which constructs natural language summaries for multiple product reviews (Hu and Liu, 2004). Most previous work extracts opinion words and aspect terms. Typical approaches include association mining of frequent candidate aspects (Hu and Liu, 2004; Qiu et al., 2011), sequence labeling based methods (Jakob and Gurevych, 2010; Yang and Cardie, 2013), as well as topic modeling techniques (Lin and He, 2009). Recently, word embeddings and recurrent neural networks are also used to extract aspect terms (Irsoy and Cardie, 2014; Liu et al., 2015). While all the methods above are extractive, Ganesan et al. (2010) presented a graph-based summarization framework to generate concise abstractive summaries of highly redundant opinions, and Wang and Ling (2016) used an attention-based neural network model to absorb information from multiple text units and generate summ"
D17-1170,C10-1103,0,0.029068,"mend. 2 Related Work Sentiment Analysis. Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015), Review rating prediction aims to predict the numeric rating of a given review. Pang and Lee (2005) pioneered this task by regarding it as a classification/regression problem. Most subsequent work focuses on designing effective textural features of reviews (Qu et al., 2010; Li et al., 2011; Wan, 2013). User information has been widely investigated in sentiment analysis. Gao et al. (2013) developed user-specific features to capture user leniency, and Li et al. (2014) incorporated textual topic and userword factors through topic modeling. For integrating user information into neural network models, Tang et al. (2015) predicted the rating score given a review by using both lexical semantic information and a user embedding model. Chen et al. (2016b) proposed a neural network to incorporate global user and product information for sentiment classification via an atte"
D17-1170,D15-1044,0,0.608319,"in their temporal order, they do not reflect the taste of a particular user. We use the customised product model to integrate user information and product information (as reflected by the product model), resulting in a single vector that represents a customised product. From this vector we are able to synthesis both a customised review and a customised rating score. 3.5 Customized Review Generation The goal of customized review generation is to generate a review YR from the customized product representation vC , composed by a sequence of words yR1 , ..., yRnr . We use a standard LSTM decoder (Rush et al., 2015) to decompose the prediction of YR into a sequence of word-level predictions: logP (YR |vC ) = X P (yRj |yR1 , ..., yRj−1 , vC ) (10) j where each word yRj is predicted conditional on the previously generated yR1 , ..., yRj−1 and the customized product vector vC . The probability is estimated by using standard word softmax: P (yRj |yR1 , ..., yRj−1 , vC ) = softmax(hRj ) (7) i (8) (11) where hRj is the hidden state variable at timestamp j, which is modeled as LST M (uj−1 , hRj ). Here a LSTM is used to generate a new state hRj from the representation of the previous state hRj−1 and uj−1 . uj−1"
D17-1170,D13-1170,0,0.00455968,"a deep memory network is utilized to find the association between the user and target product, jointly yielding the rating score and customised review. Experiments on a Yelp dataset show that the model outperforms several pipelined baselines. We make our source code publicly available under GPL at https://github.com/ wangzq870305/opinion_recommend. 2 Related Work Sentiment Analysis. Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015), Review rating prediction aims to predict the numeric rating of a given review. Pang and Lee (2005) pioneered this task by regarding it as a classification/regression problem. Most subsequent work focuses on designing effective textural features of reviews (Qu et al., 2010; Li et al., 2011; Wan, 2013). User information has been widely investigated in sentiment analysis. Gao et al. (2013) developed user-specific features to capture user leniency, and Li et al. (2014) incorporated textual topic and userword factors through topic"
D17-1170,P15-1150,0,0.0358071,"Missing"
D17-1170,D16-1169,1,0.926115,"ms. Typical approaches include association mining of frequent candidate aspects (Hu and Liu, 2004; Qiu et al., 2011), sequence labeling based methods (Jakob and Gurevych, 2010; Yang and Cardie, 2013), as well as topic modeling techniques (Lin and He, 2009). Recently, word embeddings and recurrent neural networks are also used to extract aspect terms (Irsoy and Cardie, 2014; Liu et al., 2015). While all the methods above are extractive, Ganesan et al. (2010) presented a graph-based summarization framework to generate concise abstractive summaries of highly redundant opinions, and Wang and Ling (2016) used an attention-based neural network model to absorb information from multiple text units and generate summaries of movie reviews. We also 1627 perform abstractive summarization. However, different from the above research, which summarize existing reviews, we generate customized reviews for a unreviewed product. Recommendation. has been solved on mainly purchase history. There are two main approaches, which are content-based and collaborativefiltering (CF) based (Adomavicius and Tuzhilin, 2005; Yang et al., 2014), respectively. Most existing social recommendation systems are CF-based, and c"
D17-1170,N16-1036,0,0.0182483,"ulti-task learning has been recognised as a strength of neural network models for natural language processing (Collobert et al., 2011; Henderson et al., 2013; Zhang and Weiss, 2016; Chen et al., 2016a), where hidden feature layers are shared between different tasks that have common basis. Our work can be regarded as an instance of such multi-tasks learning via shared parameters, which has been widely used in the research community recently. Dynamic memory network models have been applied for NLP tasks such as question answering (Sukhbaatar et al., 2015; Kumar et al., 2016), language modeling (Tran et al., 2016) and machine translation (Wang et al., 2016). There are typically used to find abstract semantic representations of texts towards certain tasks, which are consistent with our main need, namely abstractCustomized Review Rating Score LSTM Deep memory Hop n Attention Attention Attention LSTM LSTM LSTM … r1 r2 … rn User Model … r1 r2 Product Model rm r1 r2 rn Neighborhood Model Figure 2: Overview of proposed model. ing the representation of a product that is biased towards the taste of a certain user. We use a variation of the memory network model for obtaining user-specific review representation."
D17-1170,P13-2094,0,0.132863,"lysis. Our task is related to document-level sentiment classification (Pang and Lee, 2008) for various neural network models have been used, including convolutional neural networks (Kim, 2014), recursive neural network (Socher et al., 2013) and recurrent neural network (Teng et al., 2016; Tai et al., 2015), Review rating prediction aims to predict the numeric rating of a given review. Pang and Lee (2005) pioneered this task by regarding it as a classification/regression problem. Most subsequent work focuses on designing effective textural features of reviews (Qu et al., 2010; Li et al., 2011; Wan, 2013). User information has been widely investigated in sentiment analysis. Gao et al. (2013) developed user-specific features to capture user leniency, and Li et al. (2014) incorporated textual topic and userword factors through topic modeling. For integrating user information into neural network models, Tang et al. (2015) predicted the rating score given a review by using both lexical semantic information and a user embedding model. Chen et al. (2016b) proposed a neural network to incorporate global user and product information for sentiment classification via an attention mechanism. Different fr"
D17-1170,N16-1007,0,0.334873,"y of all reviews, which ideally should be customized to the reader. To address the limitations above, we propose a new task called opinion recommendation, which is to generate a customized review score of the product that the user is likely to give, as well as a customized review that the user would have written for the target product, if the user had reviewed the product. The proposed opinion recommendation task is closely related to several existing lines of work in NLP. The first is sentiment analysis (Hu and Liu, 2004; Pang and Lee, 2008) and opinion summarization (Nishikawa et al., 2010; Wang and Ling, 2016), which is to give a rating score or summary based on existing customer reviews. Our 1626 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1626–1637 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics task is different in that we aim to generate user rating scores and review of a product unreviewed by the user. The second is recommendation (Su and Khoshgoftaar, 2009; Yang et al., 2014), which is to give a ranking score to a certain product or service based on the purchase history of the user and other customer"
D17-1170,D16-1027,0,0.0312537,"strength of neural network models for natural language processing (Collobert et al., 2011; Henderson et al., 2013; Zhang and Weiss, 2016; Chen et al., 2016a), where hidden feature layers are shared between different tasks that have common basis. Our work can be regarded as an instance of such multi-tasks learning via shared parameters, which has been widely used in the research community recently. Dynamic memory network models have been applied for NLP tasks such as question answering (Sukhbaatar et al., 2015; Kumar et al., 2016), language modeling (Tran et al., 2016) and machine translation (Wang et al., 2016). There are typically used to find abstract semantic representations of texts towards certain tasks, which are consistent with our main need, namely abstractCustomized Review Rating Score LSTM Deep memory Hop n Attention Attention Attention LSTM LSTM LSTM … r1 r2 … rn User Model … r1 r2 Product Model rm r1 r2 rn Neighborhood Model Figure 2: Overview of proposed model. ing the representation of a product that is biased towards the taste of a certain user. We use a variation of the memory network model for obtaining user-specific review representation. 3 Model Formally, the input to our model is"
D17-1170,P13-1161,0,0.0286993,"originates from the objective. Previous research aims to predict opinions on reviewed products, while our task is to recommend opinion on new products, which the user has not reviewed. Opinion Summarization. Our work also overlaps with to the area of opinion summarization, which constructs natural language summaries for multiple product reviews (Hu and Liu, 2004). Most previous work extracts opinion words and aspect terms. Typical approaches include association mining of frequent candidate aspects (Hu and Liu, 2004; Qiu et al., 2011), sequence labeling based methods (Jakob and Gurevych, 2010; Yang and Cardie, 2013), as well as topic modeling techniques (Lin and He, 2009). Recently, word embeddings and recurrent neural networks are also used to extract aspect terms (Irsoy and Cardie, 2014; Liu et al., 2015). While all the methods above are extractive, Ganesan et al. (2010) presented a graph-based summarization framework to generate concise abstractive summaries of highly redundant opinions, and Wang and Ling (2016) used an attention-based neural network model to absorb information from multiple text units and generate summaries of movie reviews. We also 1627 perform abstractive summarization. However, di"
D17-1170,N16-1174,0,0.0155053,"standard LSTM (Hochreiter and Schmidhuber, 1997) is used to learn the hidden states of an user’s reviews to build the user model. Denoting the recurrent function at step t as LSTM(xt , ht−1 ), we obtain a sequence of hidden state vectors {hU1 , hU2 , ..., hUnu } recurrently by feeding {ed (rU1 ), ed (rU2 ), ..., ed (rUnu )} as inputs, where hUi = LSTM(ed (rUi ), hUi−1 ). The initial state and all standard LSTM parameters are randomly initialized and tuned during training. Not all reviews contribute equally to the representation of a user. We use the attention mechanism (Bahdanau et al., 2014; Yang et al., 2016) to extract the reviews that are relatively more important, aggregating the representation of reviews to form a vector. Taking the hidden state {hU1 , ...hU2 , ..., hUnu } of user model as input, the attention model outputs, a continuous vector vU ∈ Rd×1 , which is computed as a weighted sum of each hidden state hUi , namely vU = nu X αi hUi (1) the neighborhood model vN using the neighborhood reviews RN = {rN1 , rN2 , ..., rNnn } with an attention recurrent network. A key issue in building the neighborhood model is how to find neighbors of a certain user. In this study, we use matrix factoriz"
D17-1170,P16-1147,0,0.0460813,"ms. Typical approaches include association mining of frequent candidate aspects (Hu and Liu, 2004; Qiu et al., 2011), sequence labeling based methods (Jakob and Gurevych, 2010; Yang and Cardie, 2013), as well as topic modeling techniques (Lin and He, 2009). Recently, word embeddings and recurrent neural networks are also used to extract aspect terms (Irsoy and Cardie, 2014; Liu et al., 2015). While all the methods above are extractive, Ganesan et al. (2010) presented a graph-based summarization framework to generate concise abstractive summaries of highly redundant opinions, and Wang and Ling (2016) used an attention-based neural network model to absorb information from multiple text units and generate summaries of movie reviews. We also 1627 perform abstractive summarization. However, different from the above research, which summarize existing reviews, we generate customized reviews for a unreviewed product. Recommendation. has been solved on mainly purchase history. There are two main approaches, which are content-based and collaborativefiltering (CF) based (Adomavicius and Tuzhilin, 2005; Yang et al., 2014), respectively. Most existing social recommendation systems are CF-based, and c"
D17-1170,D16-1070,1,\N,Missing
D17-1182,P16-1231,0,0.123962,"methods, which utilize well-studied structured prediction methods to address the problem (Li and Ji, 2014; Miwa and Sasaki, 2014). As has been commonly understood, learning local decisions for structured prediction can lead to label bias (Lafferty et al., 2001), which prevents globally optimal structures from receiving optimal scores by the model. We address this potential issue by building a structural neural model for end-to-end relation extraction, following a recent line of efforts on globally optimized models for neural structured prediction (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016; Wiseman and Rush, 2016). In particular, we follow Miwa and Sasaki (2014), casting the task as an end-to-end tablefilling problem. This is different from the actionbased method of Li and Ji (2014), yet has shown to 1730 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1730–1740 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics be more flexible and accurate (Miwa and Sasaki, 2014). We take a different approach to representation learning, addressing two potential limitations of Miwa and Bansal (2016). First,"
D17-1182,D15-1041,0,0.00705726,"table-filling labels during decoding for training and testing. For example, T (i, j) must be ⊥ if T (i, i) or T (j, j) equals O. hw e0w L ew L et L hchar CNN character sequence Figure 3: Word representations. tuned during training and represented by ew , and the other being a pre-trained external word embedding from Ew0 , which is fixed and represented by e0w .3 For a POS tag t, its embedding et is obtained from a look-up table Et similar to Ew . The above two components have also been used by Miwa and Bansal (2016). We further enhance the word representation by using its character sequence (Ballesteros et al., 2015; Lample et al., 2016), taking a convolution neural network (CNN) to derive a character-based word representation hchar , which has been demonstrated effective for several NLP tasks (dos Santos and Gatti, 2014). We obtain the final hw i based on a non-linear feed0 forward layer on ew ⊕ ew ⊕ et ⊕ hchar , where ⊕ denotes concatenation. 2.3.2 Label Representation In addition to the word sequence, the history label sequence l1 l2 · · · li−1 , and especially the labels representing detected entities, are also useful disambiguation. For example, the previous entity boundary label can be helpful to d"
D17-1182,H05-1091,0,0.522928,"and Ji (2014), yet has shown to 1730 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1730–1740 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics be more flexible and accurate (Miwa and Sasaki, 2014). We take a different approach to representation learning, addressing two potential limitations of Miwa and Bansal (2016). First, Miwa and Bansal (2016) rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008). However, parsing errors can lead to encoding inaccuracies of tree-LSTMs, thereby hurting relation extraction potentially. We take an alternative approach to integrating syntactic information, by taking the hidden LSTM layers of a bi-affine attention parser (Dozat and Manning, 2016) to augment input representations. Pretrained for parsing, such hidden layers contain rich syntactic information on each word, yet do not explicitly represent parsing decisions, thereby avoiding potential issues caused by incorrect parses. Our method is also free from a particular syntactic form"
D17-1182,P15-2142,0,0.0264483,") is the major evaluation metric. tity label LSTM, we only use the segment features of entityi and entityj . 2.3.5 Syntactic Features Previous work has shown that syntactic features are useful for relation extraction (Zhou et al., 2005). For example, the shortest dependency path has been used by several relation extraction models (Bunescu and Mooney, 2005; Miwa and Bansal, 2016). Here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures. In particular, we take state-of-the-art syntactic parsers that use encoder-decoder neural models (Buys and Blunsom, 2015; Kiperwasser and Goldberg, 2016), where the encoder represents the syntactic features of the input sentences. For example, LSTM hidden states over the input word/tag sequences has been used frequently as syntactic features (Kiperwasser and Goldberg, 2016). Such features represent input words with syntactic information. The parser decoder also leverages partially-parsed results, such as features from partial syntactic trees, although we do not use explicit output features. Table 1 shows the encoder structures of three state-of-the-art dependency parsers. Our method is to leverage trained synta"
D17-1182,C10-1018,0,0.0209201,"g framework proposed by Miwa and Sasaki (2014). Feature representations are learned from several LSTM structures over the inputs, and a novel simple method is used to integrate syntactic information. Experiments show the effectiveness of both global normalization and syntactic features. Our final model achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning and decoding with a sin"
D17-1182,P11-1056,0,0.226015,"n syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks. 1 Introduction Extracting entities (Florian et al., 2006, 2010) and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016). End-to-end learning prevents error propagation in the pipeline approach, and allows cross-task dependencies to be modeled explicitly for entity recognition. As a result, it gives better relation extraction accuracies compared to pipelines. Miwa and Bansal (2016) were among the first to use neural networ"
D17-1182,W02-1001,0,0.0343994,"Goldberg, 2016; Dozat and Manning, 2016), relation classification (Xu et al., 2015; Vu et al., 2016; Miwa and Bansal, 2016) and sentiment analysis (Li et al., 2015; Teng et al., 2016). Based on the output of LSTM structures, Wang and Chang (2016) introduce segment features, and apply it to dependency parsing. The same method is applied to constituent parsing by Cross and Huang (2016). We exploit this segmental representation for relation extraction. Global optimization and normalization has been successfully applied on many NLP tasks that involve structural prediction (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2010; Zhang and Clark, 2011), using traditional discrete features. For neural models, it has recently received increasing interests (Zhou et al., 2015; Andor et al., 2016; Xu, 2016; Wiseman and Rush, 2016), and im5 Conclusion Acknowledgments We thank the anonymous reviewers for their constructive comments, which help to improve the paper, and Zhiyang Teng for dumping intermediate outputs from the bi-affine parser. This work is supported by National Natural Science Foundation of China (NSFC) grants 61602160 and 61672211, Natural Science Foundation of Heilongjiang Province (Ch"
D17-1182,P04-1015,0,0.0146416,"t each step. When each action of table filling is taken, all hypotheses in the agenda are expanded by enumerating the next labels, and the B highest-scored resulting tables are used to replace the agenda for the next step. Search begins with the agenda containing an empty table, and finishes when all cells of the tables in the agenda have been filled. When the beam size is 1, the algorithm is the same as greedy decoding. When the beam size is larger than 1, however, error propagation is alleviated. For training, the same beam search algorithm is applied to training examples, and early-update (Collins and Roark, 2004) is used to fix search errors. 3 Experiments 3.1 Data and Evaluation We evaluate the proposed model on two datasets, namely the ACE05 data and the corpus of Roth and Yih (2004) (CONLL04), respectively. The ACE05 dataset defines seven coarse-grained entity types and six coarse-grained relation categories, while the CONLL04 dataset defines four entity types and five relation categories. For the ACE05 dataset, we follow Li and Ji (2014) and Miwa and Bansal (2016), splitting and preprocessing the dataset into training, development and test sets.5 For the CONLL04 dataset, we follow Miwa and Sasaki"
D17-1182,D16-1001,0,0.0106253,"(Tai et al., 2015). Second, Miwa and Bansal (2016) did not explicitly learn the representation of segments when predicting entity boundaries or making relation classification decisions, which can be intuitively highly useful, and has been investigated in several studies (Wang and Chang, 2016; Zhang et al., 2016). We take the LSTM-Minus method of Wang and Chang (2016), modelling a segment as the difference between its last and first LSTM hidden vectors. This method is highly efficient, yet gives as accurate results as compared to more complex neural network structures to model a span of words (Cross and Huang, 2016). Evaluation on two benchmark datasets shows that our method outperforms previous methods of Miwa and Bansal (2016), Li and Ji (2014) and Miwa and Sasaki (2014), giving the best reported results on both benchmarks. Detailed analysis shows that our integration of syntactic features is as effective as traditional approaches based on discrete parser outputs. We make our code publicly ORG-AFF PHYS Associated Press writer Patrick McDowell in Kuwait City ORG PER PER GPE Figure 1: Relation extraction. The example is chosen from the ACE05 dataset, where ORG, PER and GPE denote organization, person and"
D17-1182,P04-1054,0,0.0428757,"is different from the actionbased method of Li and Ji (2014), yet has shown to 1730 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1730–1740 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics be more flexible and accurate (Miwa and Sasaki, 2014). We take a different approach to representation learning, addressing two potential limitations of Miwa and Bansal (2016). First, Miwa and Bansal (2016) rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008). However, parsing errors can lead to encoding inaccuracies of tree-LSTMs, thereby hurting relation extraction potentially. We take an alternative approach to integrating syntactic information, by taking the hidden LSTM layers of a bi-affine attention parser (Dozat and Manning, 2016) to augment input representations. Pretrained for parsing, such hidden layers contain rich syntactic information on each word, yet do not explicitly represent parsing decisions, thereby avoiding potential issues caused by incorrect parses. Our method"
D17-1182,doddington-etal-2004-automatic,0,0.0236899,"representations. In addition, we present a novel method to integrate syntactic information to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks. 1 Introduction Extracting entities (Florian et al., 2006, 2010) and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016). End-to-end learning prevents error propagation in the pipeline approach, and allows cross-task dependencies to be modeled explicitly for entity recogniti"
D17-1182,P81-1022,0,0.59941,"Missing"
D17-1182,P15-1033,0,0.00489077,"j), j] denote the two entities above, where s(·) denotes the start position of an entity, the resulted segments are [0, s(i) − 1] (i.e., left, in Figure 5(b)), [s(i), i] (i.e., entityi ), [i+1, s(j)−1] (i.e., middle), [s(j), j] (i.e., entityj ) and [j + 1, n] (i.e., right), respectively. For the word LSTMs, we extract all five segment features, while the en1733 Models S-LSTM (2015) K&G (2016) D&M (2016) Encoder 1-Layer LSTM 2-Layer Bi-LSTM 4-Layer Bi-LSTM LAS 90.9 91.9 93.8 Table 1: Encoder structures and performances of three state-of-the-art dependency parsers, where S-LSTM (2015) refers to Dyer et al. (2015), K&G (2016) refers to the best parser of Kiperwasser and Goldberg (2016), D&M (2016) refers to Dozat and Manning (2016), and LAS (labeled attachment score) is the major evaluation metric. tity label LSTM, we only use the segment features of entityi and entityj . 2.3.5 Syntactic Features Previous work has shown that syntactic features are useful for relation extraction (Zhou et al., 2005). For example, the shortest dependency path has been used by several relation extraction models (Bunescu and Mooney, 2005; Miwa and Bansal, 2016). Here we propose a novel method to integrate syntax, without ne"
D17-1182,N04-1001,0,0.0979213,"d-to-end relation extraction, achieving the best results on standard benchmarks. 4 We investigated a globally normalized end-to-end relation extraction model using neural network, based on the table-filling framework proposed by Miwa and Sasaki (2014). Feature representations are learned from several LSTM structures over the inputs, and a novel simple method is used to integrate syntactic information. Experiments show the effectiveness of both global normalization and syntactic features. Our final model achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relati"
D17-1182,P06-1060,0,0.112679,"t global optimization can achieve better performances compared to local classification. We build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn context representations. In addition, we present a novel method to integrate syntactic information to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks. 1 Introduction Extracting entities (Florian et al., 2006, 2010) and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li"
D17-1182,D10-1033,0,0.0224257,"ults on standard benchmarks. 4 We investigated a globally normalized end-to-end relation extraction model using neural network, based on the table-filling framework proposed by Miwa and Sasaki (2014). Feature representations are learned from several LSTM structures over the inputs, and a novel simple method is used to integrate syntactic information. Experiments show the effectiveness of both global normalization and syntactic features. Our final model achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work condu"
D17-1182,C16-1239,0,0.0470022,"Missing"
D17-1182,P05-1051,0,0.0147433,"ion (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016), and we follow this line of work in the study. LSTM features have been extensively exploited for NLP tasks, including tagging (Huang et al., 2015; Lample et al., 2016), parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016), relation classification (Xu et al., 2015; Vu et al., 2016; Miwa and Bansal, 2016) and sentiment analysis (Li et al., 2015; Teng et al., 2016)"
D17-1182,N07-1015,0,0.0321668,"cal classification. We build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn context representations. In addition, we present a novel method to integrate syntactic information to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks. 1 Introduction Extracting entities (Florian et al., 2006, 2010) and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et"
D17-1182,Q16-1023,0,0.0225602,"start position of an entity, the resulted segments are [0, s(i) − 1] (i.e., left, in Figure 5(b)), [s(i), i] (i.e., entityi ), [i+1, s(j)−1] (i.e., middle), [s(j), j] (i.e., entityj ) and [j + 1, n] (i.e., right), respectively. For the word LSTMs, we extract all five segment features, while the en1733 Models S-LSTM (2015) K&G (2016) D&M (2016) Encoder 1-Layer LSTM 2-Layer Bi-LSTM 4-Layer Bi-LSTM LAS 90.9 91.9 93.8 Table 1: Encoder structures and performances of three state-of-the-art dependency parsers, where S-LSTM (2015) refers to Dyer et al. (2015), K&G (2016) refers to the best parser of Kiperwasser and Goldberg (2016), D&M (2016) refers to Dozat and Manning (2016), and LAS (labeled attachment score) is the major evaluation metric. tity label LSTM, we only use the segment features of entityi and entityj . 2.3.5 Syntactic Features Previous work has shown that syntactic features are useful for relation extraction (Zhou et al., 2005). For example, the shortest dependency path has been used by several relation extraction models (Bunescu and Mooney, 2005; Miwa and Bansal, 2016). Here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures. In particular, w"
D17-1182,C16-1087,0,0.00686066,"marks. 4 We investigated a globally normalized end-to-end relation extraction model using neural network, based on the table-filling framework proposed by Miwa and Sasaki (2014). Feature representations are learned from several LSTM structures over the inputs, and a novel simple method is used to integrate syntactic information. Experiments show the effectiveness of both global normalization and syntactic features. Our final model achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference"
D17-1182,N16-1030,0,0.0744593,"ng decoding for training and testing. For example, T (i, j) must be ⊥ if T (i, i) or T (j, j) equals O. hw e0w L ew L et L hchar CNN character sequence Figure 3: Word representations. tuned during training and represented by ew , and the other being a pre-trained external word embedding from Ew0 , which is fixed and represented by e0w .3 For a POS tag t, its embedding et is obtained from a look-up table Et similar to Ew . The above two components have also been used by Miwa and Bansal (2016). We further enhance the word representation by using its character sequence (Ballesteros et al., 2015; Lample et al., 2016), taking a convolution neural network (CNN) to derive a character-based word representation hchar , which has been demonstrated effective for several NLP tasks (dos Santos and Gatti, 2014). We obtain the final hw i based on a non-linear feed0 forward layer on ew ⊕ ew ⊕ et ⊕ hchar , where ⊕ denotes concatenation. 2.3.2 Label Representation In addition to the word sequence, the history label sequence l1 l2 · · · li−1 , and especially the labels representing detected entities, are also useful disambiguation. For example, the previous entity boundary label can be helpful to deciding the boundary l"
D17-1182,D15-1278,0,0.032968,"arate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016), and we follow this line of work in the study. LSTM features have been extensively exploited for NLP tasks, including tagging (Huang et al., 2015; Lample et al., 2016), parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016), relation classification (Xu et al., 2015; Vu et al., 2016; Miwa and Bansal, 2016) and sentiment analysis (Li et al., 2015; Teng et al., 2016). Based on the output of LSTM structures, Wang and Chang (2016) introduce segment features, and apply it to dependency parsing. The same method is applied to constituent parsing by Cross and Huang (2016). We exploit this segmental representation for relation extraction. Global optimization and normalization has been successfully applied on many NLP tasks that involve structural prediction (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2010; Zhang and Clark, 2011), using traditional discrete features. For neural models, it has recently received increasing interests"
D17-1182,P14-1038,0,0.396461,"006, 2010) and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016). End-to-end learning prevents error propagation in the pipeline approach, and allows cross-task dependencies to be modeled explicitly for entity recognition. As a result, it gives better relation extraction accuracies compared to pipelines. Miwa and Bansal (2016) were among the first to use neural networks for end-to-end relation extraction, showing highly promising results. In particular, they used bidirectional LSTM (Graves et al., 2013) to learn hidden word representations under a sentential context, and further leveraged t"
D17-1182,P16-1200,0,0.0424043,"o benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016), and we follow this line of work in the study. LSTM features have been extensively exploited for NLP tasks, including tagging (Huang et al., 2015; Lample et al., 2016), parsing (Kiperwasser and Goldberg, 2"
D17-1182,N10-1069,0,0.0162108,"; Dozat and Manning, 2016), relation classification (Xu et al., 2015; Vu et al., 2016; Miwa and Bansal, 2016) and sentiment analysis (Li et al., 2015; Teng et al., 2016). Based on the output of LSTM structures, Wang and Chang (2016) introduce segment features, and apply it to dependency parsing. The same method is applied to constituent parsing by Cross and Huang (2016). We exploit this segmental representation for relation extraction. Global optimization and normalization has been successfully applied on many NLP tasks that involve structural prediction (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2010; Zhang and Clark, 2011), using traditional discrete features. For neural models, it has recently received increasing interests (Zhou et al., 2015; Andor et al., 2016; Xu, 2016; Wiseman and Rush, 2016), and im5 Conclusion Acknowledgments We thank the anonymous reviewers for their constructive comments, which help to improve the paper, and Zhiyang Teng for dumping intermediate outputs from the bi-affine parser. This work is supported by National Natural Science Foundation of China (NSFC) grants 61602160 and 61672211, Natural Science Foundation of Heilongjiang Province (China) grant F2016036, Sp"
D17-1182,P16-1105,0,0.065397,"man, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016). End-to-end learning prevents error propagation in the pipeline approach, and allows cross-task dependencies to be modeled explicitly for entity recognition. As a result, it gives better relation extraction accuracies compared to pipelines. Miwa and Bansal (2016) were among the first to use neural networks for end-to-end relation extraction, showing highly promising results. In particular, they used bidirectional LSTM (Graves et al., 2013) to learn hidden word representations under a sentential context, and further leveraged treestructured LSTM (Tai et al., 2015) to encod"
D17-1182,D09-1013,0,0.0871128,"del achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016), and we follow this line of work in the study. LSTM features have been extensively exploited for NLP tasks, including tagging (Huang et al., 2015; Lample et al., 201"
D17-1182,N16-1065,0,0.0197766,"Missing"
D17-1182,D14-1200,0,0.328873,"lations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016). End-to-end learning prevents error propagation in the pipeline approach, and allows cross-task dependencies to be modeled explicitly for entity recognition. As a result, it gives better relation extraction accuracies compared to pipelines. Miwa and Bansal (2016) were among the first to use neural networks for end-to-end relation extraction, showing highly promising results. In particular, they used bidirectional LSTM (Graves et al., 2013) to learn hidden word representations under a sentential context, and further leveraged treestructured LSTM (Tai"
D17-1182,P17-1159,1,0.833271,"Miwa and Bansal (2016), because these paths rely on explicit discrete outputs from a syntactic parser. Our method can avoid the problem since we do not compute parser outputs. On the other hand, the computation complexity is largely reduced by using our method since sequential LSTMs are based on inputs only, while the dependency path LSTMs should be computed based on the dynamic entity detection outputs. When beam search is exploited during decoding, increasing number of dependency paths can be used by a surge of entity pairs from beam outputs. Our method can be extended into neural stacking Wang et al. (2017), by doing back-propagation training of the parser parameters during model training, which are leave for future work. 2.4 Training and Search 2.4.1 Local Optimization Previous work (Miwa and Bansal, 2016; Gupta et al., 2016) trains model parameters by modeling each step for labeling one input sentence separately. Given a partial table T , its neural representation hT is first obtained, and then compute the next label scores {l1 , l2 , · · · , ls } using Equation 1. The output scores are regularized into a probability distribution {pl1 , pl2 , · · · , pls } by using a softmax layer. The trainin"
D17-1182,P13-1147,0,0.154678,"optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn context representations. In addition, we present a novel method to integrate syntactic information to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks. 1 Introduction Extracting entities (Florian et al., 2006, 2010) and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016). End-to-end learning prevents erro"
D17-1182,P16-1218,0,0.311701,"uch as dependency grammar, constituent grammar or combinatory categorial grammar, requiring only hidden representations on word that contain syntactic information. In contrast, the method of Miwa and Bansal (2016) must consider tree LSTM formulations that are specific to grammar formalisms, which can be structurally different (Tai et al., 2015). Second, Miwa and Bansal (2016) did not explicitly learn the representation of segments when predicting entity boundaries or making relation classification decisions, which can be intuitively highly useful, and has been investigated in several studies (Wang and Chang, 2016; Zhang et al., 2016). We take the LSTM-Minus method of Wang and Chang (2016), modelling a segment as the difference between its last and first LSTM hidden vectors. This method is highly efficient, yet gives as accurate results as compared to more complex neural network structures to model a span of words (Cross and Huang, 2016). Evaluation on two benchmark datasets shows that our method outperforms previous methods of Miwa and Bansal (2016), Li and Ji (2014) and Miwa and Sasaki (2014), giving the best reported results on both benchmarks. Detailed analysis shows that our integration of syntact"
D17-1182,D10-1034,0,0.0129758,"d on the table-filling framework proposed by Miwa and Sasaki (2014). Feature representations are learned from several LSTM structures over the inputs, and a novel simple method is used to integrate syntactic information. Experiments show the effectiveness of both global normalization and syntactic features. Our final model achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning an"
D17-1182,P15-1113,0,0.0655736,"nlike existing statistical methods, which utilize well-studied structured prediction methods to address the problem (Li and Ji, 2014; Miwa and Sasaki, 2014). As has been commonly understood, learning local decisions for structured prediction can lead to label bias (Lafferty et al., 2001), which prevents globally optimal structures from receiving optimal scores by the model. We address this potential issue by building a structural neural model for end-to-end relation extraction, following a recent line of efforts on globally optimized models for neural structured prediction (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016; Wiseman and Rush, 2016). In particular, we follow Miwa and Sasaki (2014), casting the task as an end-to-end tablefilling problem. This is different from the actionbased method of Li and Ji (2014), yet has shown to 1730 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1730–1740 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics be more flexible and accurate (Miwa and Sasaki, 2014). We take a different approach to representation learning, addressing two potential limitations of Miwa and Ba"
D17-1182,C08-1088,0,0.0168489,"wn to 1730 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1730–1740 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics be more flexible and accurate (Miwa and Sasaki, 2014). We take a different approach to representation learning, addressing two potential limitations of Miwa and Bansal (2016). First, Miwa and Bansal (2016) rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008). However, parsing errors can lead to encoding inaccuracies of tree-LSTMs, thereby hurting relation extraction potentially. We take an alternative approach to integrating syntactic information, by taking the hidden LSTM layers of a bi-affine attention parser (Dozat and Manning, 2016) to augment input representations. Pretrained for parsing, such hidden layers contain rich syntactic information on each word, yet do not explicitly represent parsing decisions, thereby avoiding potential issues caused by incorrect parses. Our method is also free from a particular syntactic formalism, such as depen"
D17-1182,D16-1137,0,0.0235343,"Missing"
D17-1182,W09-1119,0,0.0333976,", achieving the best results on standard benchmarks. 4 We investigated a globally normalized end-to-end relation extraction model using neural network, based on the table-filling framework proposed by Miwa and Sasaki (2014). Feature representations are learned from several LSTM structures over the inputs, and a novel simple method is used to integrate syntactic information. Experiments show the effectiveness of both global normalization and syntactic features. Our final model achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both t"
D17-1182,W04-2401,0,0.0552428,"2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016), and we follow this line of work in the study. LSTM features have been extensively exploited for NLP tasks, including tagging (Huang et al., 2015; Lample et al., 2016), parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016), relation classification (Xu et al., 2015; Vu et al., 2016; Miwa and Bansal, 2016) and sentiment analysis (Li et al., 2015; Teng et al., 2016). Based on the outpu"
D17-1182,C14-1008,0,0.0164801,"uned during training and represented by ew , and the other being a pre-trained external word embedding from Ew0 , which is fixed and represented by e0w .3 For a POS tag t, its embedding et is obtained from a look-up table Et similar to Ew . The above two components have also been used by Miwa and Bansal (2016). We further enhance the word representation by using its character sequence (Ballesteros et al., 2015; Lample et al., 2016), taking a convolution neural network (CNN) to derive a character-based word representation hchar , which has been demonstrated effective for several NLP tasks (dos Santos and Gatti, 2014). We obtain the final hw i based on a non-linear feed0 forward layer on ew ⊕ ew ⊕ et ⊕ hchar , where ⊕ denotes concatenation. 2.3.2 Label Representation In addition to the word sequence, the history label sequence l1 l2 · · · li−1 , and especially the labels representing detected entities, are also useful disambiguation. For example, the previous entity boundary label can be helpful to deciding the boundary label of the current word. During relation classification, the types of the entities involved can indicate the relation category between them. We exploit the diagonal label sequence of part"
D17-1182,P11-1053,0,0.0409213,"build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn context representations. In addition, we present a novel method to integrate syntactic information to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks. 1 Introduction Extracting entities (Florian et al., 2006, 2010) and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016). End-t"
D17-1182,D16-1181,0,0.0194133,"t of LSTM structures, Wang and Chang (2016) introduce segment features, and apply it to dependency parsing. The same method is applied to constituent parsing by Cross and Huang (2016). We exploit this segmental representation for relation extraction. Global optimization and normalization has been successfully applied on many NLP tasks that involve structural prediction (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2010; Zhang and Clark, 2011), using traditional discrete features. For neural models, it has recently received increasing interests (Zhou et al., 2015; Andor et al., 2016; Xu, 2016; Wiseman and Rush, 2016), and im5 Conclusion Acknowledgments We thank the anonymous reviewers for their constructive comments, which help to improve the paper, and Zhiyang Teng for dumping intermediate outputs from the bi-affine parser. This work is supported by National Natural Science Foundation of China (NSFC) grants 61602160 and 61672211, Natural Science Foundation of Heilongjiang Province (China) grant F2016036, Special business expenses in Heilongjiang Province (China) grant 2016-KYYWF-0183, the Singapore Ministry of Education (MOE) AcRF Tier 2 grant T2MOE201301 and SRG ISTD 2012 038 fr"
D17-1182,D15-1206,0,0.026816,"tions jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016), and we follow this line of work in the study. LSTM features have been extensively exploited for NLP tasks, including tagging (Huang et al., 2015; Lample et al., 2016), parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016), relation classification (Xu et al., 2015; Vu et al., 2016; Miwa and Bansal, 2016) and sentiment analysis (Li et al., 2015; Teng et al., 2016). Based on the output of LSTM structures, Wang and Chang (2016) introduce segment features, and apply it to dependency parsing. The same method is applied to constituent parsing by Cross and Huang (2016). We exploit this segmental representation for relation extraction. Global optimization and normalization has been successfully applied on many NLP tasks that involve structural prediction (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2010; Zhang and Clark, 2011), using traditional dis"
D17-1182,P16-1040,1,0.45235,"mar, constituent grammar or combinatory categorial grammar, requiring only hidden representations on word that contain syntactic information. In contrast, the method of Miwa and Bansal (2016) must consider tree LSTM formulations that are specific to grammar formalisms, which can be structurally different (Tai et al., 2015). Second, Miwa and Bansal (2016) did not explicitly learn the representation of segments when predicting entity boundaries or making relation classification decisions, which can be intuitively highly useful, and has been investigated in several studies (Wang and Chang, 2016; Zhang et al., 2016). We take the LSTM-Minus method of Wang and Chang (2016), modelling a segment as the difference between its last and first LSTM hidden vectors. This method is highly efficient, yet gives as accurate results as compared to more complex neural network structures to model a span of words (Cross and Huang, 2016). Evaluation on two benchmark datasets shows that our method outperforms previous methods of Miwa and Bansal (2016), Li and Ji (2014) and Miwa and Sasaki (2014), giving the best reported results on both benchmarks. Detailed analysis shows that our integration of syntactic features is as eff"
D17-1182,J11-1005,1,0.341065,"16), relation classification (Xu et al., 2015; Vu et al., 2016; Miwa and Bansal, 2016) and sentiment analysis (Li et al., 2015; Teng et al., 2016). Based on the output of LSTM structures, Wang and Chang (2016) introduce segment features, and apply it to dependency parsing. The same method is applied to constituent parsing by Cross and Huang (2016). We exploit this segmental representation for relation extraction. Global optimization and normalization has been successfully applied on many NLP tasks that involve structural prediction (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2010; Zhang and Clark, 2011), using traditional discrete features. For neural models, it has recently received increasing interests (Zhou et al., 2015; Andor et al., 2016; Xu, 2016; Wiseman and Rush, 2016), and im5 Conclusion Acknowledgments We thank the anonymous reviewers for their constructive comments, which help to improve the paper, and Zhiyang Teng for dumping intermediate outputs from the bi-affine parser. This work is supported by National Natural Science Foundation of China (NSFC) grants 61602160 and 61672211, Natural Science Foundation of Heilongjiang Province (China) grant F2016036, Special business expenses"
D17-1182,P05-1052,0,0.212735,"rformances compared to local classification. We build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn context representations. In addition, we present a novel method to integrate syntactic information to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks. 1 Introduction Extracting entities (Florian et al., 2006, 2010) and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and"
D17-1182,P15-1150,0,0.113895,"Missing"
D17-1182,P05-1053,0,0.473255,"based method of Li and Ji (2014), yet has shown to 1730 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1730–1740 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics be more flexible and accurate (Miwa and Sasaki, 2014). We take a different approach to representation learning, addressing two potential limitations of Miwa and Bansal (2016). First, Miwa and Bansal (2016) rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008). However, parsing errors can lead to encoding inaccuracies of tree-LSTMs, thereby hurting relation extraction potentially. We take an alternative approach to integrating syntactic information, by taking the hidden LSTM layers of a bi-affine attention parser (Dozat and Manning, 2016) to augment input representations. Pretrained for parsing, such hidden layers contain rich syntactic information on each word, yet do not explicitly represent parsing decisions, thereby avoiding potential issues caused by incorrect parses. Our method is also free from a"
D17-1182,D16-1169,1,0.777112,"Missing"
D17-1182,D07-1076,0,0.0443029,"eural network, based on the table-filling framework proposed by Miwa and Sasaki (2014). Feature representations are learned from several LSTM structures over the inputs, and a novel simple method is used to integrate syntactic information. Experiments show the effectiveness of both global normalization and syntactic features. Our final model achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows t"
D17-1182,P15-1117,1,0.597622,"ecisions. This is unlike existing statistical methods, which utilize well-studied structured prediction methods to address the problem (Li and Ji, 2014; Miwa and Sasaki, 2014). As has been commonly understood, learning local decisions for structured prediction can lead to label bias (Lafferty et al., 2001), which prevents globally optimal structures from receiving optimal scores by the model. We address this potential issue by building a structural neural model for end-to-end relation extraction, following a recent line of efforts on globally optimized models for neural structured prediction (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016; Wiseman and Rush, 2016). In particular, we follow Miwa and Sasaki (2014), casting the task as an end-to-end tablefilling problem. This is different from the actionbased method of Li and Ji (2014), yet has shown to 1730 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1730–1740 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics be more flexible and accurate (Miwa and Sasaki, 2014). We take a different approach to representation learning, addressing two potential"
D17-1182,N16-1103,0,0.0150134,"tions are learned from several LSTM structures over the inputs, and a novel simple method is used to integrate syntactic information. Experiments show the effectiveness of both global normalization and syntactic features. Our final model achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014;"
D17-1296,P16-1231,0,0.109656,"ons taken always equals to the number of input sentence for every valid path, it is straightforward to use beam search. We use beamsearch for both training and testing. The early update strategy from Collins and Roark (2004) is applied for training. In particular, each training sequence is decoded, and we keep track of the location of the gold path in the beam. If the gold path falls out of the beam at step t, decoding process is stopped and parameter update is performed using the gold path as a positive example, and beam items as negative examples. We also use the global optimization method (Andor et al., 2016; Zhou et al., 2015) to train our beam-search model. Scheduled Sampling Scheduled sampling (Bengio et al., 2015) can also be used to reduce error propagation. The training goal of the greedy baseline is to maximize the likelihood of each action given the current model state, which means that the correct action is taken at each step. Doing inference, the action predicted by the model itself is taken instead. This discrepancy between training and inference can yield errors that accumulate quickly along the searching process. Scheduled sampling is used to solve the discrepancy by gently changing"
D17-1296,D16-1254,0,0.0289924,"Missing"
D17-1296,N01-1016,0,0.785208,"nnotation layers are provided: one for syntactic bracketing (MRG files), and the other for disfluencies (DPS files). The Switchboard annotation project was not fully completed. Because disfluency annotation is cheaper to produce, many of the DPS training files do not have matching MRG files. Only 619,236 of the 1,482,845 tokens in the DPS disfluency detection training data have gold-standard syntactic parses. To directly compare with transitionbased parsing methods (Honnibal and Johnson, 2014; Wu et al., 2015), we also use the subcorpus of PARSED/MRG/SWBD. Following the experiment settings in Charniak and Johnson (2001), the training subcorpus contains directories 2 and 3 in PARSED/MRG/SWBD and directory 4 is split into test, development sets and others. Following Honnibal and Johnson (2014), we lower-case the text and remove all punctuations and partial words2 . We also discard the ‘um’ and ‘uh’ tokens and merge ‘you know’ and ‘i mean’ into single tokens. Automatic POS-tags generated from pocket crf (Qian and Liu, 2013) are used as POStag in our experiments. For Chinese experiments, we collect 25k spoken sentences from meeting minutes, which are transcribed using the iflyrec toolkit3 , and annotate them wit"
D17-1296,P04-1015,0,0.377117,"As shown in Figure 2, the model state consists of four components: (i) O, a conventional sequential LSTM (Hochreiter and Schmidhuber, 1997) to store the words that have been labeled as fluency. (ii) S, a stack LSTM to represent partial disfluency chunks, which captures chunklevel information. (iii) A, a conventional sequential LSTM to represent history of actions. (iiii) B, a Bi-LSTM to represent words that have not yet been processed. A sequence of transition actions are used to consume input tokens and construct the output from left to right. To reduce error propagation, we use beam-search (Collins and Roark, 2004) and scheduled sampling (Bengio et al., 2015), respectively. We evaluate our model on the commonly used English Switchboard test set and a in-house annotated Chinese data set. Results show that our model outperforms previous state-of-the-art systems. The code is released1 . 2 Background For a background, we briefly introduce transitionbased parsing and its extention for joint disfluency detection. An arc-eager transition-based parsing system consists of a stack σ containing words being processed, a buffer β containing words to be processed and a memory A storing dependency 1 https://github.com"
D17-1296,P15-1033,0,0.413262,"S to boston to denver Figure 2: model state when processing the sentence “want a flight to boston to denver”. arcs which have been generated. There are four types of transition actions (Nivre, 2008) • Shift : Remove the front of the buffer and push it to the stack. • Reduce : Pop the top of the stack. • LeftArc : Pop the top of the stack, and link the popped word to the front of the buffer. • RightArc : Link the front of the buffer to the top of the stack, remove the front of the buffer and push it to the stack. Many neural network parsers have been constructed under this framework, such as (Dyer et al., 2015), who use different LSTM structure to represent information from σ to β. For disfluency detection, the input is a sentence with disfluencies from automatic speech recognition (ASR). We denote the word sequence as w1n = (w1 , ..., wn ). The output of the task is a sequence of binary tags denoted as D1n = (d1 , ..., dn ), where each di corresponds to the word wi , indicating whether wi is a disfluent word or not. Hence the task can be modeled as searching for the best sequenc D∗ given the stream of words w1n D∗ = argmaxD P (D1n |w1n ) Wu et al. (2015) proposes a statistical transitionbased disfl"
D17-1296,N15-1029,0,0.310716,"Missing"
D17-1296,N09-2028,0,0.441731,", it is very important to capture long-range dependencies for disfluency detection. Since there is large parallelism between the reparandum chunk and the following repair chunk (for example, in Figure 1, the reparandum begins with to and ends before another occurrence of to), it is also useful to exploit chunk-level representation, which explicitly makes use of resulted infelicity disfluency chunks. Common approaches take disfluency detection as a sequence labeling problem, where each sentential word is assigned with a label (Zayats et al., 2016; Hough and Schlangen, 2015; Qian and Liu, 2013; Georgila, 2009). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Another drawback of these approaches is that they are unable to exploit chunk2785 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2785–2794 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level features. Semi-CRF (Ferguson et al., 2015) is used to alleviate this issue to some extent. SemiCRF models still have their inefficiencies because they can only use the local chun"
D17-1296,Q14-1011,0,0.60128,"spans or distances. Another drawback of these approaches is that they are unable to exploit chunk2785 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2785–2794 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level features. Semi-CRF (Ferguson et al., 2015) is used to alleviate this issue to some extent. SemiCRF models still have their inefficiencies because they can only use the local chunk information limited by the markov assumption when decoding. A different line of work (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) adopts transition-based parsing models for disfluency detection. This line of work can be seen as a joint of disfluency detection and parsing. The main advantage of the joint models is that they can capture long-range dependency of disfluencies as well as chunk-level information. However, they introduce additional annotated syntactic structure, which is very expensive to produce, and can cause noise by significantly enlarging the output search space. Inspired by the above observations, we investigate a transition-based model without syntactic information. Our model increment"
D17-1296,N16-1030,0,0.0415195,"uencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced a transition-based model for disfluency detection, which does not use any syntax information, learning represention of both chunks and global contexts. Experiments showed that our model achieves the state"
D17-1296,N06-2019,0,0.507578,"lish Switchboard test data. attention-based model can capture a global representation of the input sentence by using a RNN when encoding. It can strongly capture long-range dependencies and achieves good performance, but are also not powerful enough to capture chunklevel information. To capture chunk-level information, Ferguson et al. (2015) try to use semi-CRF for disfluency detection, and reports improved results. Semi-CRF models still have their inefficiencies because they can only use the local chunk information limited by the markov assumption when decoding. Many syntax-based approaches (Lease and Johnson, 2006; Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) have been proposed which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre,"
D17-1296,P14-1038,0,0.0262712,"n, 2014; Wu et al., 2015) have been proposed which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced"
D17-1296,N15-1142,0,0.0289563,"d Liu, 2013) are used as POStag in our experiments. For Chinese experiments, we collect 25k spoken sentences from meeting minutes, which are transcribed using the iflyrec toolkit3 , and annotate them with only disfluency annotations according to the guideline proposed by Meteer et al. (1995). 2 words are recognized as partial words if they are tagged as ‘XX’ or end with ‘-’ 3 the iflyrec toolkit is available at http://www.iflyrec.com/ Neural Network Training Pretrained Word Embeddings. Following Dyer et al. (2015) and Wang et al. (2016), we use a variant of the skip n-gram model introduced by Ling et al. (2015), named “structured skip n-gram”, to create word embeddings. The AFP portion of English Gigaword corpus (version 5) is used as the training corpus. Word embeddings for Chinese are trained on Chinese baike corpus. We use an embedding dimension of 100 for English, 300 for chinese. Hyper-Parameters. Both the Bi-LSTMs and the stack LSTMs have two hidden layers and their dimensions are set to 100. Pretrained word embeddings have 100 dimensions and the learned word embeddings have also 100 dimensions. Pos-tag embeddings have 12 dimensions. The dimension of action embeddings is set to 20. 4.3 Perform"
D17-1296,J93-2004,0,0.0579037,"(wi wi+1 , wi+k wi+k+1 ), −4 ≤ k ≤ +4 and k 6= 0: if wi wi+1 equals wi+k wi+k+1 , the value is 1, others 0 Duplicate(pi pi+1 , pi+k pi+k+1 ), −4 ≤ k ≤ +4 and k 6= 0: if pi pi+1 equals pi+k pi+k+1 , the value is 1, others 0 similarity features f uzzyM atch(wi , wi+k ), k ∈ {−1, +1}: similarity = 2 ∗ num same letters/(len(wi ) + len(wi+k )). if similarity > 0.8, the value is 1, others 0 Table 3: Discrete features used in our transition-based neural networks. p-POS tag. w-word. 4 Experiments 4.1 4.2 Settings Dataset. Our training data include the Switchboard portion of the English Penn Treebank (Marcus et al., 1993) and a in-house Chinese data set. For English, two annotation layers are provided: one for syntactic bracketing (MRG files), and the other for disfluencies (DPS files). The Switchboard annotation project was not fully completed. Because disfluency annotation is cheaper to produce, many of the DPS training files do not have matching MRG files. Only 619,236 of the 1,482,845 tokens in the DPS disfluency detection training data have gold-standard syntactic parses. To directly compare with transitionbased parsing methods (Honnibal and Johnson, 2014; Wu et al., 2015), we also use the subcorpus of PA"
D17-1296,J08-4003,0,0.0365412,"y introduce transitionbased parsing and its extention for joint disfluency detection. An arc-eager transition-based parsing system consists of a stack σ containing words being processed, a buffer β containing words to be processed and a memory A storing dependency 1 https://github.com/hitwsl/transition disfluency OUT DEL et=max{0,W[st;bt;ot;at]+d} O ot at st B bt Bi-LSTM Subtraction want a flight A DEL DEL TOP S to boston to denver Figure 2: model state when processing the sentence “want a flight to boston to denver”. arcs which have been generated. There are four types of transition actions (Nivre, 2008) • Shift : Remove the front of the buffer and push it to the stack. • Reduce : Pop the top of the stack. • LeftArc : Pop the top of the stack, and link the popped word to the front of the buffer. • RightArc : Link the front of the buffer to the top of the stack, remove the front of the buffer and push it to the stack. Many neural network parsers have been constructed under this framework, such as (Dyer et al., 2015), who use different LSTM structure to represent information from σ to β. For disfluency detection, the input is a sentence with disfluencies from automatic speech recognition (ASR)."
D17-1296,N13-1102,0,0.666823,"fifteen words. Hence, it is very important to capture long-range dependencies for disfluency detection. Since there is large parallelism between the reparandum chunk and the following repair chunk (for example, in Figure 1, the reparandum begins with to and ends before another occurrence of to), it is also useful to exploit chunk-level representation, which explicitly makes use of resulted infelicity disfluency chunks. Common approaches take disfluency detection as a sequence labeling problem, where each sentential word is assigned with a label (Zayats et al., 2016; Hough and Schlangen, 2015; Qian and Liu, 2013; Georgila, 2009). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Another drawback of these approaches is that they are unable to exploit chunk2785 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2785–2794 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level features. Semi-CRF (Ferguson et al., 2015) is used to alleviate this issue to some extent. SemiCRF models still have their inefficiencies because they can only u"
D17-1296,D13-1013,0,0.490055,"ated disfluencies with longer spans or distances. Another drawback of these approaches is that they are unable to exploit chunk2785 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2785–2794 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level features. Semi-CRF (Ferguson et al., 2015) is used to alleviate this issue to some extent. SemiCRF models still have their inefficiencies because they can only use the local chunk information limited by the markov assumption when decoding. A different line of work (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) adopts transition-based parsing models for disfluency detection. This line of work can be seen as a joint of disfluency detection and parsing. The main advantage of the joint models is that they can capture long-range dependency of disfluencies as well as chunk-level information. However, they introduce additional annotated syntactic structure, which is very expensive to produce, and can cause noise by significantly enlarging the output search space. Inspired by the above observations, we investigate a transition-based model without syntactic info"
D17-1296,C16-1027,1,0.916833,"t set and a set of in-house annotated Chinese data. 1 RP Figure 1: Sentence with disfluencies annotated in English Switchboard corpus. RM=Reparandum, IM=Interregnum, RP=Repair. The preceding RM is corrected by the following RP. Type repair repair repetition restart Annotation [ I just + I ] enjoy working [ we want + {well} in our area we want ] to [it’s + {uh} it’s ] almost like [ we would like + ] let’s go to the Table 1: Different types of disfluencies. Introduction Disfluency detection is the task of recognizing non-fluent word sequences in spoken language transcripts (Zayats et al., 2016; Wang et al., 2016; Wu et al., 2015). As shown in Figure 1, standard annotation of disfluency structure (Shriberg, 1994) indicates the reparandum (words that are discarded, or corrected by the following words), the interruption point (+) marking the end of the reparandum, the associated repair, and an optional interregnum after the interruption point (filled pauses, discourse cue words, etc.). Ignoring the interregnum, disfluencies can be categorized into three types: restarts, repetitions, and corrections, based on whether the repair is empty, the same as the reparandum or different, respectively. Table 1 give"
D17-1296,P16-1218,0,0.0207443,"Scheduled sampling is used to solve the discrepancy by gently changing the training process from a fully guided scheme using the true previous action, towards a less guided scheme which mostly uses the predicting action instead. We take the action gaining higher p(zt |et ) with a certain probability p, and a probability (1 − p) for the correct action when training. 3.2 State Representation For better capturing non-local context information, we use LSTM structures to represent different components of each state, including buffer, action, stack, and output. In particular, we exploit LSTM-Minus (Wang and Chang, 2016) to model the buffer segment, conventional LSTM to model the action and ouptut segment, and stack LSTM (Dyer et al., 2015) to model the stack segments, which demonstrates highly effectively in parsing task. Buffer Representation In order to construct more informative representation, we use a Bi-LSTM to represent the buffer following the work of Wang and Chang (2016), where the subtraction between a unidirectional 2788 O want a flight S to boston B to denver hb(to) hf(to) hb(denver) hf(denver) to boston to bb = hb(denver) - hb(to) denver bf = hf(to) - hf(denver) Figure 3: Illustration for learn"
D17-1296,P15-1113,0,0.024077,"ngineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced a transition-based model for disfluency detection, which does not use any syntax information, learning represention of both chunks and global contexts. Experiments showed that our model achieves the state-of-the-art F-scores on both the commonly used English Switchboard test set and a in-house annotated Chinese data set. Acknowledgments We thank the anonymous reviewers for their valuable suggestions. This work wa"
D17-1296,P15-1048,0,0.410628,"in-house annotated Chinese data. 1 RP Figure 1: Sentence with disfluencies annotated in English Switchboard corpus. RM=Reparandum, IM=Interregnum, RP=Repair. The preceding RM is corrected by the following RP. Type repair repair repetition restart Annotation [ I just + I ] enjoy working [ we want + {well} in our area we want ] to [it’s + {uh} it’s ] almost like [ we would like + ] let’s go to the Table 1: Different types of disfluencies. Introduction Disfluency detection is the task of recognizing non-fluent word sequences in spoken language transcripts (Zayats et al., 2016; Wang et al., 2016; Wu et al., 2015). As shown in Figure 1, standard annotation of disfluency structure (Shriberg, 1994) indicates the reparandum (words that are discarded, or corrected by the following words), the interruption point (+) marking the end of the reparandum, the associated repair, and an optional interregnum after the interruption point (filled pauses, discourse cue words, etc.). Ignoring the interregnum, disfluencies can be categorized into three types: restarts, repetitions, and corrections, based on whether the repair is empty, the same as the reparandum or different, respectively. Table 1 gives a few examples."
D17-1296,P13-1013,1,0.811416,"which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced a transition-based model for disfluency detect"
D17-1296,P14-1125,1,0.862598,"Missing"
D17-1296,P16-1040,1,0.852594,"luency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced a transition-based model for disfluency detection, which does not use any syntax information, learning represention of both chunks and global contexts. Experiments showed that our model achieves the state-of-the-art F-scores on both the commonly used English Switchboard test set and a in-ho"
D17-1296,P11-2033,1,0.739192,"nd Johnson, 2006; Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) have been proposed which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has"
D17-1296,P15-1117,1,0.930687,"als to the number of input sentence for every valid path, it is straightforward to use beam search. We use beamsearch for both training and testing. The early update strategy from Collins and Roark (2004) is applied for training. In particular, each training sequence is decoded, and we keep track of the location of the gold path in the beam. If the gold path falls out of the beam at step t, decoding process is stopped and parameter update is performed using the gold path as a positive example, and beam items as negative examples. We also use the global optimization method (Andor et al., 2016; Zhou et al., 2015) to train our beam-search model. Scheduled Sampling Scheduled sampling (Bengio et al., 2015) can also be used to reduce error propagation. The training goal of the greedy baseline is to maximize the likelihood of each action given the current model state, which means that the correct action is taken at each step. Doing inference, the action predicted by the model itself is taken instead. This discrepancy between training and inference can yield errors that accumulate quickly along the searching process. Scheduled sampling is used to solve the discrepancy by gently changing the training process"
D17-1296,P13-1043,1,0.849581,"li and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) have been proposed which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigat"
D18-1246,D17-1209,0,0.168044,"ia the dependency PREP OF NN path “exon-19 ! gene ! EGFR” is lost from the original subgraph. Second, using LSTMs on both DAGs, information of only ancestors and descendants can be incorporated for each word. Sibling information, which may also be important, is not included. A potential solution to the problems above is to model a graph as a whole, learning its representation without breaking it into two DAGs. Due to the existence of cycles, naive extension of tree LSTMs cannot serve this goal. Recently, graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Bastings et al., 2017) and graph recurrent networks (GRN) (Song et al., 2018; Zhang et al., 2018) have been proposed for representing graph structures for NLP tasks. Such methods encode a given graph by hierarchically learning representations of neighboring nodes in the graphs via their connecting edges. While GCNs use CNN for information exchange, GRNs take gated recurrent steps to this end. For fair comparison with DAG LSTMs, we build a graph LSTM by extending Song et al. (2018), which strictly follow the configurations of Peng et al. (2017) such as the source of features and hyper parameter settings. In particul"
D18-1246,P10-1160,0,0.346512,"to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng et al. (2017) proposed a graph-structured LSTM for n-ary relation extraction. As shown in Figure 1 (a), graphs are constructed from input sentences with dependency edges,"
D18-1246,N18-1082,0,0.0254715,"Missing"
D18-1246,D15-1205,0,0.0337129,"a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng et al. (2017) proposed"
D18-1246,W09-2415,0,0.154489,"Missing"
D18-1246,N07-1015,0,0.0610435,"tion in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary rela"
D18-1246,P14-1038,0,0.0304269,"Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng"
D18-1246,P14-5010,0,0.00328524,", or a multi-class classification problem of detecting which relation holds for the entity mentions. Take Table 1 as an example. The binary classification task is to determine whether gefitinib would have an effect on this type of cancer, given a cancer patient with 858E mutation on gene EGFR. The multi-class classification task is to detect the exact drug effect: response, resistance, sensitivity, etc. 3 Baseline: Bi-directional DAG LSTM Peng et al. (2017) formulate the task as a graphstructured problem in order to adopt rich dependency and discourse features. In particular, Stanford parser (Manning et al., 2014) is used to assign syntactic structure to input sentences, and heads of two consecutive sentences are connected to represent discourse information, resulting in a graph structure. For each input graph G = (V, E), the nodes V are words within input sentences, and each edge e 2 E connects two words that either have a relation or are adjacent to each other. Each edge is denoted as a triple (i, j, l), where i and j are the indices of the source and target words, respectively, and the edge label l indicates either a dependency or discourse relation (such as “nsubj”) or a relative position (such as"
D18-1246,D17-1159,0,0.24825,"between “exon-19” and “EGFR” via the dependency PREP OF NN path “exon-19 ! gene ! EGFR” is lost from the original subgraph. Second, using LSTMs on both DAGs, information of only ancestors and descendants can be incorporated for each word. Sibling information, which may also be important, is not included. A potential solution to the problems above is to model a graph as a whole, learning its representation without breaking it into two DAGs. Due to the existence of cycles, naive extension of tree LSTMs cannot serve this goal. Recently, graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Bastings et al., 2017) and graph recurrent networks (GRN) (Song et al., 2018; Zhang et al., 2018) have been proposed for representing graph structures for NLP tasks. Such methods encode a given graph by hierarchically learning representations of neighboring nodes in the graphs via their connecting edges. While GCNs use CNN for information exchange, GRNs take gated recurrent steps to this end. For fair comparison with DAG LSTMs, we build a graph LSTM by extending Song et al. (2018), which strictly follow the configurations of Peng et al. (2017) such as the source of features and hyper paramet"
D18-1246,P05-1061,0,0.240902,"comparisons with the binary relation extraction results. However, the performance gaps between GS GLSTM and Bidir DAG LSTM dramatically increase, showing the superiority of GS GLSTM over Bidir DAG LSTM in utilizing context information. 7 B INARY 50.7 71.7* Table 6: Average test accuracies for multi-class relation extraction with all instances (“Cross”). GS GLSTM 85 80 T ERNARY 51.7 71.1* Related Work N -ary relation extraction N -ary relation extractions can be traced back to MUC-7 (Chinchor, 1998), which focuses on entity-attribution relations. It has also been studied in biomedical domain (McDonald et al., 2005), but only the instances within a single sentence are considered. Previous work on cross-sentence relation extraction relies on either explicit co-reference annotation (Gerber and Chai, 2010; Yoshikawa et al., 2011), or the assumption that the whole document refers to a single coherent event (Wick et al., 2006; Swampillai and Stevenson, 2011). Both simplify the problem and reduce the need for learning better contextual representation of entity mentions. A notable exception is Quirk and Poon (2017), who adopt distant supervision and integrated contextual evidence of diverse types without relyin"
D18-1246,P16-1105,0,0.546468,"ral language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng et al. (2017) proposed a graph-structured LST"
D18-1246,P15-1150,0,0.174145,"Missing"
D18-1246,W06-1671,0,0.0844934,"extraction with all instances (“Cross”). GS GLSTM 85 80 T ERNARY 51.7 71.1* Related Work N -ary relation extraction N -ary relation extractions can be traced back to MUC-7 (Chinchor, 1998), which focuses on entity-attribution relations. It has also been studied in biomedical domain (McDonald et al., 2005), but only the instances within a single sentence are considered. Previous work on cross-sentence relation extraction relies on either explicit co-reference annotation (Gerber and Chai, 2010; Yoshikawa et al., 2011), or the assumption that the whole document refers to a single coherent event (Wick et al., 2006; Swampillai and Stevenson, 2011). Both simplify the problem and reduce the need for learning better contextual representation of entity mentions. A notable exception is Quirk and Poon (2017), who adopt distant supervision and integrated contextual evidence of diverse types without relying on these assumptions. However, they only study binary relations. We follow Peng et al. (2017) by studying ternary cross-sentence relations. Graph encoder Liang et al. (2016) build a graph LSTM model for semantic object parsing, which aims to segment objects within an image into more fine-grained, semanticall"
D18-1246,J05-1004,1,0.140836,"An example showing that tumors with L858E mutation in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine"
D18-1246,D18-1110,1,0.848544,"ng node states sequentially: for each input graph, a start node and a node sequence are chosen, which determines the order of recurrent state updates. In contrast, our graph LSTM do not need ordering of graph nodes, and is highly parallelizable. 2233 Graph convolutional networks (GCNs) and very recently graph recurrent networks (GRNs) have been used to model graph structures in NLP tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), machine translation (Bastings et al., 2017), text generation (Song et al., 2018), text representation (Zhang et al., 2018) and semantic parsing (Xu et al., 2018b,a). In particular, Zhang et al. (2018) use GRN to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs and Transformer (Vaswani et al., 2017) on classification and sequence labeling tasks; Song et al. (2018) build a GRN for encoding AMR graphs, showing that the representation is superior compared to BiLSTM on serialized AMR. Our work is in line with their work in the investigation of GRN on NLP. To our knowledge, we are the first to use GRN for representing dependency and discourse structures. Under"
D18-1246,Q17-1008,0,0.11233,"parallelization. On a standard benchmark, our model shows the best result in the literature. 1 Table 1: An example showing that tumors with L858E mutation in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which co"
D18-1246,D14-1162,0,0.0965546,"or nonresponse”, “sensitivity”, “response”, “resistance” and “None”. We follow Peng et al. (2017) and binarize multi-class labels by grouping all relation classes as “Yes” and treat “None” as “No”. 6.2 Settings Following Peng et al. (2017), five-fold crossvalidation is used for evaluating the models,3 and the final test accuracy is calculated by averaging the test accuracies over all five folds. For each fold, we randomly separate 200 instances from the training set for development. The batch size is set as 8 for all experiments. Word embeddings are initialized with the 100-dimensional GloVe (Pennington et al., 2014) vectors, pretrained on 6 billion words from Wikipedia and web text. The edge label embeddings are 3-dimensional and randomly 2 The dataset is available at http://hanover.azurewebsites.net. 3 The released data has been separated into 5 portions, and we follow the exact split. 2230 Model Quirk and Poon (2017) Peng et al. (2017) - EMBED Peng et al. (2017) - FULL + multi-task Bidir DAG LSTM GS GLSTM Figure 3: Dev accuracies against transition steps for the graph state LSTM model. initialized. Pretrained word embeddings are not updated during training. The dimension of hidden vectors in LSTM units"
D18-1246,P13-1147,0,0.0141763,"ond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct"
D18-1246,E17-1110,0,0.142755,"ation by allowing more parallelization. On a standard benchmark, our model shows the best result in the literature. 1 Table 1: An example showing that tumors with L858E mutation in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows"
D18-1246,P18-1150,1,0.937565,"s lost from the original subgraph. Second, using LSTMs on both DAGs, information of only ancestors and descendants can be incorporated for each word. Sibling information, which may also be important, is not included. A potential solution to the problems above is to model a graph as a whole, learning its representation without breaking it into two DAGs. Due to the existence of cycles, naive extension of tree LSTMs cannot serve this goal. Recently, graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Bastings et al., 2017) and graph recurrent networks (GRN) (Song et al., 2018; Zhang et al., 2018) have been proposed for representing graph structures for NLP tasks. Such methods encode a given graph by hierarchically learning representations of neighboring nodes in the graphs via their connecting edges. While GCNs use CNN for information exchange, GRNs take gated recurrent steps to this end. For fair comparison with DAG LSTMs, we build a graph LSTM by extending Song et al. (2018), which strictly follow the configurations of Peng et al. (2017) such as the source of features and hyper parameter settings. In particular, the full input graph is modeled as a single state,"
D18-1246,D17-1182,1,0.86292,", relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentions form a ternary relation yet appear in distinct sentences. Peng et al. (2017) proposed a graph-structured LSTM for n-ary relation"
D18-1246,P18-1030,1,0.910776,"ginal subgraph. Second, using LSTMs on both DAGs, information of only ancestors and descendants can be incorporated for each word. Sibling information, which may also be important, is not included. A potential solution to the problems above is to model a graph as a whole, learning its representation without breaking it into two DAGs. Due to the existence of cycles, naive extension of tree LSTMs cannot serve this goal. Recently, graph convolutional networks (GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017; Bastings et al., 2017) and graph recurrent networks (GRN) (Song et al., 2018; Zhang et al., 2018) have been proposed for representing graph structures for NLP tasks. Such methods encode a given graph by hierarchically learning representations of neighboring nodes in the graphs via their connecting edges. While GCNs use CNN for information exchange, GRNs take gated recurrent steps to this end. For fair comparison with DAG LSTMs, we build a graph LSTM by extending Song et al. (2018), which strictly follow the configurations of Peng et al. (2017) such as the source of features and hyper parameter settings. In particular, the full input graph is modeled as a single state, with words in the gr"
D18-1246,P05-1052,0,0.0482608,"at tumors with L858E mutation in EGFR gene respond to gefitinib treatment. Introduction As a central task in natural language processing, relation extraction has been investigated on news, web text and biomedical domains. It has been shown to be useful for detecting explicit facts, such as cause-effect (Hendrickx et al., 2009), and predicting the effectiveness of a medicine on a cancer caused by mutation of a certain gene in the biomedical domain (Quirk and Poon, 2017; Peng et al., 2017). While most existing work extracts relations within a sentence (Zelenko et al., 2003; Palmer et al., 2005; Zhao and Grishman, 2005; Jiang and Zhai, 2007; Plank and Moschitti, 2013; Li and Ji, 2014; Gormley et al., 2015; Miwa and Bansal, 2016; Zhang et al., 2017), the task of cross-sentence relation extraction has received increasing attention (Gerber and Chai, 2010; Yoshikawa et al., 2011). Recently, Peng ⇤ Equal contribution et al. (2017) extend cross-sentence relation extraction by further detecting relations among several entity mentions (n-ary relation). Table 1 shows an example, which conveys the fact that cancers caused by the 858E mutation on EGFR gene can respond to the gefitinib medicine. The three entity mentio"
D18-1246,R11-1004,0,0.0888552,"instances (“Cross”). GS GLSTM 85 80 T ERNARY 51.7 71.1* Related Work N -ary relation extraction N -ary relation extractions can be traced back to MUC-7 (Chinchor, 1998), which focuses on entity-attribution relations. It has also been studied in biomedical domain (McDonald et al., 2005), but only the instances within a single sentence are considered. Previous work on cross-sentence relation extraction relies on either explicit co-reference annotation (Gerber and Chai, 2010; Yoshikawa et al., 2011), or the assumption that the whole document refers to a single coherent event (Wick et al., 2006; Swampillai and Stevenson, 2011). Both simplify the problem and reduce the need for learning better contextual representation of entity mentions. A notable exception is Quirk and Poon (2017), who adopt distant supervision and integrated contextual evidence of diverse types without relying on these assumptions. However, they only study binary relations. We follow Peng et al. (2017) by studying ternary cross-sentence relations. Graph encoder Liang et al. (2016) build a graph LSTM model for semantic object parsing, which aims to segment objects within an image into more fine-grained, semantically meaningful parts. The nodes of"
D19-1020,D17-1209,0,0.142049,"endency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary informati"
D19-1020,P18-1026,0,0.0260887,"nce. First, a dependency parser is used to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative i"
D19-1020,H05-1091,0,0.867277,"the corresponding author nmod comp comp amod comp ... observed ... interaction of orexin receptor antagonist almorexant (a) nmod comp comp nmod comp amod comp ... observed ... interaction of orexin receptor antagonist almorexant (b) Figure 1: (a) 1-best dependency tree and (b) dependency forest for a medical-domain sentence, where edge label “comp” represents “compound”. Associated mentions are in different colors. Some irrelevant words and edges are omitted for simplicity. Previous work has shown that dependency syntax is important for guiding relation extraction (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Liu et al., 2015; Gormley et al., 2015; Xu et al., 2015a,b; Miwa and Bansal, 2016; Zhang et al., 2018b), especially in biological and medical domains (Quirk and Poon, 2017; Peng et al., 2017; Song et al., 2018b). Compared with sequential surface-level structures, such as POS tags, dependency trees help to model word-toword relations more easily by drawing direct connections between distant words that are syntactically correlated. Take the phrase “effect on the medicine” for example; “effect” and “medicine” are directly connected in a dependency tree, regardless of how many modifiers are adde"
D19-1020,W11-2905,0,0.0727527,"Missing"
D19-1020,W11-0216,0,0.0751446,"Missing"
D19-1020,C96-1058,0,0.0517257,"w recall given an imperfect parser. We investigate two algorithms to generate high-quality forests by judging “quality” from different perspectives: one focusing on arcs, and the other focusing on trees. E DGEWISE This algorithm focuses on the local relation of each individual edge and uses parser probabilities as confidence scores to assess edge qualities. Starting from the whole parser search space, it keeps all the edges with scores greater than a threshold . The time complexity is O(N 2 ), where N represents the sentence length.1 KB EST E ISNER This algorithm extends the Eisner algorithm (Eisner, 1996) with cube pruning (Huang and Chiang, 2005) for finding K highest-scored tree structures. The Eisner algorithm is a standard method for decoding 1-best trees for graph-based dependency parsing. Based on bottom-up dynamic programming, it stores the 1-best subtree for each span and takes O(N 3 ) time complexity for decoding a sentence of N words. KB EST E ISNER keeps a sorted list of K-best hypotheses for each span. Cube pruning (Huang and Chiang, 2005) is adopted to generate the Kbest list for each larger span from the K-best lists of its sub-spans. After the bottom-up decoding, we merge the fi"
D19-1020,D15-1205,1,0.779495,"Missing"
D19-1020,P19-1024,0,0.154261,"Missing"
D19-1020,W09-2415,0,0.0207045,"Missing"
D19-1020,W05-1506,0,0.102938,"r. We investigate two algorithms to generate high-quality forests by judging “quality” from different perspectives: one focusing on arcs, and the other focusing on trees. E DGEWISE This algorithm focuses on the local relation of each individual edge and uses parser probabilities as confidence scores to assess edge qualities. Starting from the whole parser search space, it keeps all the edges with scores greater than a threshold . The time complexity is O(N 2 ), where N represents the sentence length.1 KB EST E ISNER This algorithm extends the Eisner algorithm (Eisner, 1996) with cube pruning (Huang and Chiang, 2005) for finding K highest-scored tree structures. The Eisner algorithm is a standard method for decoding 1-best trees for graph-based dependency parsing. Based on bottom-up dynamic programming, it stores the 1-best subtree for each span and takes O(N 3 ) time complexity for decoding a sentence of N words. KB EST E ISNER keeps a sorted list of K-best hypotheses for each span. Cube pruning (Huang and Chiang, 2005) is adopted to generate the Kbest list for each larger span from the K-best lists of its sub-spans. After the bottom-up decoding, we merge the final K-bests by combining identical dependen"
D19-1020,D15-1137,0,0.031279,"2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semant"
D19-1020,I05-1006,0,0.184,"Missing"
D19-1020,Q17-1029,1,0.895317,"Missing"
D19-1020,P15-2047,0,0.103796,"Missing"
D19-1020,D11-1149,0,0.0191059,"ms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine"
D19-1020,P18-1249,0,0.0333378,"Missing"
D19-1020,D17-1159,0,0.500712,"iment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of"
D19-1020,J93-2004,0,0.064101,"able 1: Statistics on forests generated with various (upper half) and K (lower half) on the development set. • D EP T REE: Our baseline using 1-best dependency trees, as shown in Section 4. 7.4 • E DGEWISE PS and E DGEWISE: Our models using the forests generated by our E DGEWISE algorithm with or without parser scores. • KB EST E ISNER PS and KB EST E ISNER: Our model using the forests generated by our KB EST E ISNER algorithm with or without parser scores, respectively. 7.3 Settings We take a state-of-the-art deep biaffine parser (Dozat and Manning, 2017), trained on the Penn Treebank (PTB) (Marcus and Marcinkiewicz, 1993) converted to Universal Dependency, to obtain 1-best trees and full search spaces for generating forests. Using standard PTB data split (02–21 for training, 22 for development and 23 for testing), it gives UAS and LAS scores of 95.7 and 94.6, respectively. For the other hyper-parameters, word embeddings are initialized with the 200-dimensional BioASQ vectors5 , pretrained on 10M abstracts of biomedical articles, and are fixed during training. The dimension of hidden vectors in Bi-LSTM is set to 200, and the number of message passing steps T is set to 2 based on Zhang et al. (2018b). We use Ada"
D19-1020,P08-2026,0,0.0861827,"Missing"
D19-1020,W16-3009,0,0.0314234,"Missing"
D19-1020,C10-1123,0,0.0261185,"ature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering"
D19-1020,P08-1023,0,0.0537829,"t al., 2017) and a recent dataset focused on phenotype-gene relations (PGR) (Sousa et al., 2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees f"
D19-1020,N18-1080,0,0.246962,"present an edge for simplicity, and p✏ is the parser probability for edge ✏. The edge probabilities are not adjusted during end-task training. 6 Training Relation loss Given a set of training instances, each containing a sentence s with two target mentions ⇠ and ⇣, and a dependency structure D (tree or forest), we train our models with a crossentropy loss between the gold-standard relations r and model distribution: lR = log p(r|s, ⇠, ⇣, D; ✓), (13) where ✓ represents the model parameters. Using additional NER loss For training on BioCreative VI CPR, we follow previous work (Liu et al., 2017; Verga et al., 2018) to take NER loss as additional supervision, though the mention boundaries are known during testing. lN ER = N 1 X log p(tn |s, D; ✓), N (14) n=1 where tn is the gold NE tag of wn with the “BIO” scheme. Both losses are conditionally independent given the deep features produced by our Experiments We conduct experiments on two medical benchmarks to test the usefulness of dependency forest. 7.1 Data BioCreative VI CPR (Krallinger et al., 2017) This task2 focuses on the relations between chemical compounds (such as drugs) and proteins (such as genes). The full corpus contains 1020, 612 and 800 ext"
D19-1020,P16-1105,0,0.0839479,"Missing"
D19-1020,Q17-1008,0,0.357248,"on. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target entity mentions (⇠ and ⇣). We focus on the classic binary relation extraction setting (Quirk and Poon, 2017), where the number of associated mentions is two."
D19-1020,D15-1062,0,0.231787,"Missing"
D19-1020,E17-1110,0,0.0869635,"wed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target entity mentions (⇠ and ⇣). We focus on the classic binary relation extraction setting (Quirk and Poon, 2017), where the number of associated mentions is two. The output is a relation from a predefined relation set R = (r1 , . . . , rM , None), where “None” means that no relation holds for the entities. Two steps are taken for predicting the correct relation given an input sentence. First, a dependency parser is used to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode bot"
D19-1020,W08-0504,0,0.0520198,"Missing"
D19-1020,C10-2133,0,0.0320316,"from parsing noise. Results on two biomedical benchmarks show that our method outperforms the standard tree-based methods, giving the state-of-the-art results in the literature. 1 Introduction The sheer amount of medical articles and their rapid growth prevent researchers from receiving comprehensive literature knowledge by direct reading. This can hamper both medical research and clinical diagnosis. NLP techniques have been used for automating the knowledge extraction process from the medical literature (Friedman et al., 2001; Yu and Agichtein, 2003; Hirschman et al., 2005; Xu et al., 2010; Sondhi et al., 2010; Abacha and Zweigenbaum, 2011). Along this line of work, a long-standing task is relation extraction, which mines factual knowledge from free text by labeling relations between entity mentions. As shown in Figure 1, the sub-clause “previously observed cytochrome P450 3A4 ( CYP3A4 ) interaction of the dual orexin receptor antagonist almorexant” contains two entities, namely “orexin receptor” and “almorexant”. There is an “adversary” relation between these two entities, denoted as“CPR:6”. ⇤ Yue Zhang is the corresponding author nmod comp comp amod comp ... observed ... interaction of orexin rec"
D19-1020,Q19-1002,1,0.928194,"e usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated"
D19-1020,P18-1150,1,0.94662,"with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target enti"
D19-1020,D18-1110,1,0.810915,"neration (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi repres"
D19-1020,D15-1206,0,0.201939,"Missing"
D19-1020,C18-1120,0,0.0200441,"focused on phenotype-gene relations (PGR) (Sousa et al., 2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labe"
D19-1020,D18-1246,1,0.942065,"with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target enti"
D19-1020,P18-1030,1,0.867698,"to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative information exchange between directly con"
D19-1020,N19-1152,0,0.0439742,"Missing"
D19-1020,D18-1244,0,0.376206,"to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative information exchange between directly con"
D19-1057,W13-2322,0,0.0123005,"the SRL task, most of the previous research focuses on 1) PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) annotations, i.e., the CoNLL 2005 (Carreras and M`arquez, 2005) and CoNLL 2009 (Hajiˇc et al., 2009) shared tasks; 2) OntoNotes annotations (Weischedel et al., 2011), i.e., the CoNLL 2005 and CoNLL 2012 datasets and more; 3) and FrameNet (Baker et al., 1998) annotations. For the non-English languages, not all of them are widely available. Apart from these, in the broad range of semantic processing, other formalisms non-exhaustively include abstract meaning representation (Banarescu et al., 2013), universal decompositional semantics (White et al., 2016), and semantic dependency parsing (Oepen et al., 2015). Abend and Rappoport (2017) give a better overview of various semantic representations. In this paper, we primarily work on the Chinese and English datasets from the • We introduce the relation-aware approach to employ syntactic dependencies into the selfattention-based SRL model. • We compare our approach with previous studies, and achieve state-of-the-art results with and without external resources, i.e., in the so-called closed and open settings. 2 Related work Traditional semant"
D19-1057,W07-1402,0,0.0496309,"d models use BiLSTM Introduction The task of semantic role labeling (SRL) is to recognize arguments for a given predicate in one sentence and assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, Novemb"
D19-1057,W05-0620,0,0.570923,"Missing"
D19-1057,P81-1022,0,0.476074,"Missing"
D19-1057,D15-1112,0,0.249955,"Missing"
D19-1057,W11-1608,0,0.0274094,"ndencies. For the main architecture of the SRL model, many neural-network-based models use BiLSTM Introduction The task of semantic role labeling (SRL) is to recognize arguments for a given predicate in one sentence and assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conferen"
D19-1057,W04-2705,0,0.255775,"Missing"
D19-1057,J02-3001,0,0.484379,"onal semantics (White et al., 2016), and semantic dependency parsing (Oepen et al., 2015). Abend and Rappoport (2017) give a better overview of various semantic representations. In this paper, we primarily work on the Chinese and English datasets from the • We introduce the relation-aware approach to employ syntactic dependencies into the selfattention-based SRL model. • We compare our approach with previous studies, and achieve state-of-the-art results with and without external resources, i.e., in the so-called closed and open settings. 2 Related work Traditional semantic role labeling task (Gildea and Jurafsky, 2002) presumes that the syntactic structure of the sentence is given, either being a constituent tree or a dependency tree, like in the CoNLL shared tasks (Carreras and M`arquez, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). 617 Positional Embedding encodes the order of the input word sequence. We follow Vaswani et al. (2017) to use time positional embedding, which is formulated as follows: P E(t, 2i) = sin(t/100002i/d ) P E(t, 2i + 1) = cos(t/100002i/d ) (1) where t is the position, i means the dimension, and d is the dimension of the model input embedding. 3.1.2 Encoder Layer The self-attent"
D19-1057,S15-2153,0,0.0398758,"2004) annotations, i.e., the CoNLL 2005 (Carreras and M`arquez, 2005) and CoNLL 2009 (Hajiˇc et al., 2009) shared tasks; 2) OntoNotes annotations (Weischedel et al., 2011), i.e., the CoNLL 2005 and CoNLL 2012 datasets and more; 3) and FrameNet (Baker et al., 1998) annotations. For the non-English languages, not all of them are widely available. Apart from these, in the broad range of semantic processing, other formalisms non-exhaustively include abstract meaning representation (Banarescu et al., 2013), universal decompositional semantics (White et al., 2016), and semantic dependency parsing (Oepen et al., 2015). Abend and Rappoport (2017) give a better overview of various semantic representations. In this paper, we primarily work on the Chinese and English datasets from the • We introduce the relation-aware approach to employ syntactic dependencies into the selfattention-based SRL model. • We compare our approach with previous studies, and achieve state-of-the-art results with and without external resources, i.e., in the so-called closed and open settings. 2 Related work Traditional semantic role labeling task (Gildea and Jurafsky, 2002) presumes that the syntactic structure of the sentence is given"
D19-1057,J05-1004,0,0.520095,"ive multi-head self-attention model to incorporate syntax, which is called LISA (Linguistically-Informed Self-Attention). We follow their approach of replacing one attention head with the dependency head information, but use a softer way to capture the pairwise relationship between input elements (Shaw et al., 2018). • We present detailed experiments on different aspects of incorporating syntactic information into the SRL model, in what quality, in which representation and how to integrate. For the datasets and annotations of the SRL task, most of the previous research focuses on 1) PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) annotations, i.e., the CoNLL 2005 (Carreras and M`arquez, 2005) and CoNLL 2009 (Hajiˇc et al., 2009) shared tasks; 2) OntoNotes annotations (Weischedel et al., 2011), i.e., the CoNLL 2005 and CoNLL 2012 datasets and more; 3) and FrameNet (Baker et al., 1998) annotations. For the non-English languages, not all of them are widely available. Apart from these, in the broad range of semantic processing, other formalisms non-exhaustively include abstract meaning representation (Banarescu et al., 2013), universal decompositional semantics (White et al., 2016), and s"
D19-1057,P18-2058,0,0.176608,"Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018) replace one attention head with pre-trained syntactic dependency information, which can be viewed as a hard way to inject syntax into the neural model. Enlightened by the machine translation model proposed by Shaw et al. (20"
D19-1057,D14-1162,0,0.0991902,"onfigurations, and report the labeled precision (P), labeled recall (R) and labeled f-score (F1) for the semantic dependencies. Word Representations Most of our experiments are conducted in the closed setting without any external word embeddings or data resources than those provided by the CoNLL-2009 datasets. In the closed setting, word embedding is initialized by a Gaussian distribution with mean 0 and variance √1d , where d is the dimension of embedding size of each layer. For the experiments with external resources in the open setting, we utilize 1) word embeddings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pretrained on Wikipedia and Gigaword for English; and 2) ELMo3 (Peters et al., 2018) and BERT4 (Devlin et al., 2018), two recently proposed effective deep contextualized word representations5 . where MD is the one-hot dependency head matrix and ER means the embedding of dependency relation information, such as R EL or R EL PATH. 3.3.3 test UAS LAS 80.70 78.46 89.05 85.60 92.14 89.23 Table 1: Syntactic dependency performance for different parsers. AUTO indicates the automatic dependency trees provided by the Co"
D19-1057,P17-1044,0,0.649356,"ructural information into the SRL model; 2) Deeper integration of the syntactic information achieves better results than the simple concatenation to the inputs; 3) External pre-trained contextualized word representations help to boost the SRL performance further, which is not entirely overlapping with the syntactic information. In summary, the contributions of our work are: Roth and Lapata (2016) utilize an LSTM model to obtain embeddings from the syntactic dependency paths; while Marcheggiani and Titov (2017) construct Graph Convolutional Networks to encode the dependency structure. Although He et al. (2017)’s approach is a pure end-to-end learning, they have included an analysis of adding syntactic dependency information into English SRL in the discussion section. Cai et al. (2018) have compared syntax-agnostic and syntax-aware approaches and Xia et al. (2019) have compared different ways to represent and encode the syntactic knowledge. In another line of research, Tan et al. (2017) utilize the Transformer network for the encoder instead of the BiLSTM. Strubell et al. (2018) present a novel and effective multi-head self-attention model to incorporate syntax, which is called LISA (Linguistically-"
D19-1057,N18-1202,0,0.0608331,"are conducted in the closed setting without any external word embeddings or data resources than those provided by the CoNLL-2009 datasets. In the closed setting, word embedding is initialized by a Gaussian distribution with mean 0 and variance √1d , where d is the dimension of embedding size of each layer. For the experiments with external resources in the open setting, we utilize 1) word embeddings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese and the published embeddings with 100 dimensions pretrained on Wikipedia and Gigaword for English; and 2) ELMo3 (Peters et al., 2018) and BERT4 (Devlin et al., 2018), two recently proposed effective deep contextualized word representations5 . where MD is the one-hot dependency head matrix and ER means the embedding of dependency relation information, such as R EL or R EL PATH. 3.3.3 test UAS LAS 80.70 78.46 89.05 85.60 92.14 89.23 Table 1: Syntactic dependency performance for different parsers. AUTO indicates the automatic dependency trees provided by the CoNLL-09 Chinese dataset. B I AFFINE means the trees are generated by BiaffineParser with pre-trained word embedding on the Gigaword corpus while B IAFFINE B ERT is the sa"
D19-1057,C08-1050,0,0.128118,"al Results on the Chinese Test Data Based on the above experiments and analyses, we present the overall results of our model in this subsection. We train the three models (I NPUT, LISA, and R EL AWE) with their best settings without any external knowledge as C LOSED, and we take the same models with B ERT as O PEN. The D EP PATH &R EL PATH from G OLD without external knowledge serves as the G OLD for reference. Since we have been focusing on the task of argument identification and labeling, for both C LOSED and O PEN, we follow Roth and Lapata (2016) to use existing systems’ predicate senses (Johansson and Nugues, 2008) to exclude them from comparison. Table 7 shows that our O PEN model achieves more than 3 points of f1-score than the stateof-the-art result, and R EL AWE with D EP PATH &R EL PATH achieves the best in both C LOSED and O PEN settings. Notice that our best C LOSED model can almost perform as well as the state-of-the-art model while the latter utilizes pretrained word embeddings. Besides, performance gap between three models under O PEN setting is very small. It indicates that the representation ability of BERT is so powerful and may contains rich syntactic information. At last, the G OLD result"
D19-1057,W17-4305,0,0.0916819,"of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018)"
D19-1057,P16-1113,0,0.641761,"ong with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Str"
D19-1057,D18-1262,1,0.84272,"Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018) replace one attention head with pre-trained syntactic dependency information, which can be viewed as a hard way to inject syntax into the neural model. Enlightened by the machine translation model proposed"
D19-1057,N18-2074,0,0.114208,"e et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018) replace one attention head with pre-trained syntactic dependency information, which can be viewed as a hard way to inject syntax into the neural model. Enlightened by the machine translation model proposed by Shaw et al. (2018), we introduce the Relation-Aware method to incorporate syntactic dependencies, which is a softer way to encode richer structural information. Various experiments for the Chinese SRL on the CoNLL-2009 dataset are conducted to evaluate our hypotheses. From the empirical results, we observe that: 1) The quality of the syntactic information is essential when we incorporate structural information into the SRL model; 2) Deeper integration of the syntactic information achieves better results than the simple concatenation to the inputs; 3) External pre-trained contextualized word representations help"
D19-1057,K17-1041,0,0.201985,"For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018) replace one attention head with pre-trained syntactic dependency information, which can be viewed as a hard way to inject syntax into the neural mo"
D19-1057,D07-1002,0,0.127289,"icate in one sentence and assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly"
D19-1057,D16-1177,0,0.0617731,"Missing"
D19-1057,P14-2120,0,0.015133,"oduction The task of semantic role labeling (SRL) is to recognize arguments for a given predicate in one sentence and assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Assoc"
D19-1057,N19-1075,0,0.449868,"rformance for the Chinese SRL task on the CoNLL-2009 dataset. 1 $ $ 中国 China SBJ ROOT 鼓励 encourage 外商 foreign merchant 投资 invest 农业 agriculture COMP COMP COMP Figure 1: An example of one sentence with its syntactic dependency tree and semantic roles. Arcs above the sentence are semantic role annotations for the predicate “鼓励 (encourage)” and below the sentence are syntactic dependency annotations of the whole sentence. The meaning of this sentence is “China encourages foreign merchants to invest in agriculture”. et al. (2017) have observed that only good syntax helps with the SRL performance. Xia et al. (2019) have explored what kind of syntactic information or structure is better suited for the SRL model. Cai et al. (2018) have compared syntax-agnostic and syntax-aware approaches and claim that the syntax-agnostic model surpasses the syntax-aware ones. In this paper, we focus on analyzing the relationship between the syntactic dependency information and the SRL performance. In particular, we investigate the following four aspects: 1) Quality of the syntactic information: whether the performance of the syntactic parser output affects the SRL performance; 2) Representation of the syntactic informati"
D19-1057,D18-1548,0,0.193596,"Missing"
D19-1057,P16-2033,0,0.0392647,"nd assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into tw"
D19-1057,P03-1002,0,0.334907,") is to recognize arguments for a given predicate in one sentence and assign labels to them, including “who” did “what” to “whom”, “when”, “where”, etc. Figure 1 is an example sentence with both semantic roles and syntactic dependencies. Since the nature of semantic roles is more abstract than the syntactic dependencies, SRL has a wide range of applications in different areas, e.g., text classification (Sinoara et al., 2016), text summarization (Genest and Lapalme, 2011; Khan et al., 2015), recognizing textual entailment (Burchardt et al., 2007; Stern and Dagan, 2014), information extraction (Surdeanu et al., 2003), question answering (Shen and Lapata, 2007; Yih et al., 2016), and so on. Traditionally, syntax is the bridge to reach semantics. However, along with the popularity of the end-to-end models in the NLP community, various recent studies have been discussing the necessity of syntax in the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neur"
D19-1057,P15-1109,0,0.0304126,"the context of SRL. For instance, He 616 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 616–626, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Recent neural-network-based approaches can be roughly categorized into two classes: 1) making use of the syntactic information (FitzGerald et al., 2015; Roth and Lapata, 2016; Qian et al., 2017; Marcheggiani and Titov, 2017), and 2) pure endto-end learning from tokens to semantic labels, e.g., Zhou and Xu (2015); Marcheggiani et al. (2017). as the encoder (e.g., Cai et al. (2018); Li et al. (2018); He et al. (2018)), while recently selfattention-based encoder becomes popular due to both the effectiveness and the efficiency (Vaswani et al., 2017; Tan et al., 2017; Strubell et al., 2018). By its nature, the self-attention-based model directly captures the relation between words in the sentence, which is convenient to incorporate syntactic dependency information. Strubell et al. (2018) replace one attention head with pre-trained syntactic dependency information, which can be viewed as a hard way to inje"
D19-1057,W08-2121,0,0.366947,"Missing"
D19-1092,Q16-1022,0,0.0637934,"Missing"
D19-1092,D11-1005,0,0.0237765,"scale training instances full of automatic dependencies by parsing parallel sentences (Hwa et al., 2005; Rasooli and Collins, 2015). The two broad methods are orthogonal to each other, and can both make use of the lexicalized dependency models trained with cross-lingual word representations (Rasooli and Collins, 2017, 2019). Source Treebank Adaption. There has been much work on unsupervised cross-lingual dependency parsing by direct source treebank transferring. Several researchers investigate delexicalized models where only non-lexical features are used in the models (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015). All the features in these models are language independent, and are consistent across languages and treebanks. Thus they can be applied into target languages directly. Subsequent research proposes to exploit lexicalized features to enhance the parsing models, by resorting to cross-lingual word representations (T¨ackstr¨om et al., 2012; Guo et al., 2015; Duong et al., 2015b,a; Zhang and Barzilay, 2015; Guo et al., 2016b; Ammar et al., 2016; Wick et al., 2016; de Lhoneux et al., 2018). Cross-lingua"
D19-1092,P81-1022,0,0.713689,"Missing"
D19-1092,K15-1012,0,0.0618523,"l researchers investigate delexicalized models where only non-lexical features are used in the models (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015). All the features in these models are language independent, and are consistent across languages and treebanks. Thus they can be applied into target languages directly. Subsequent research proposes to exploit lexicalized features to enhance the parsing models, by resorting to cross-lingual word representations (T¨ackstr¨om et al., 2012; Guo et al., 2015; Duong et al., 2015b,a; Zhang and Barzilay, 2015; Guo et al., 2016b; Ammar et al., 2016; Wick et al., 2016; de Lhoneux et al., 2018). Cross-lingual word clusters and cross-lingual word embeddings are two main sources of features for transferring knowledge between source and target language sentences. These studies enable us to train lexicalized models on code-mixed treebanks as well. Thus here we integrate the cross-lingual word representations as well, which gives more direct interaction between source and target words. Our work follows another mainstream method of this line of work, namely treebank translation"
D19-1092,P15-2139,0,0.0388253,"l researchers investigate delexicalized models where only non-lexical features are used in the models (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015). All the features in these models are language independent, and are consistent across languages and treebanks. Thus they can be applied into target languages directly. Subsequent research proposes to exploit lexicalized features to enhance the parsing models, by resorting to cross-lingual word representations (T¨ackstr¨om et al., 2012; Guo et al., 2015; Duong et al., 2015b,a; Zhang and Barzilay, 2015; Guo et al., 2016b; Ammar et al., 2016; Wick et al., 2016; de Lhoneux et al., 2018). Cross-lingual word clusters and cross-lingual word embeddings are two main sources of features for transferring knowledge between source and target language sentences. These studies enable us to train lexicalized models on code-mixed treebanks as well. Thus here we integrate the cross-lingual word representations as well, which gives more direct interaction between source and target words. Our work follows another mainstream method of this line of work, namely treebank translation"
D19-1092,N13-1073,0,0.0291554,"nducted on the Google Universal Dependency Treebanks (v2.0) (McDonald et al., 2013; Nivre et al., 2016), using English as the source language, and choosing six languages, including Spanish (ES), German (DE), French (FR), Italian (IT), Portuguese (PT) and Swedish (sv), as the target languages. Google 1001 Translate3 is used to translate the sentences in the English training set into other languages. In order to generate high-quality word-level alignments, we merge the translated sentence pairs and the parallel data of EuroParl (Koehn, 2005) to obtain word alignments. We use the fastAlign tool (Dyer et al., 2013) to obtain word alignments. We use the cross-lingual word embeddings and clusters by Guo et al. (2016b) for the baseline system. The dimension size of word embeddings is 50 and the word cluster number across of all languages is 256. For network building and training, we use the same setting as Dozat and Manning (2016), including the dimensional sizes, the dropout ratio, as well as the parameter optimization method. We assume that no labeled corpus is available for the target language. Thus training is performed for 50 iterations over the whole training data without early-stopping. To evaluate"
D19-1092,P09-1042,0,0.0834438,"cts by heuristic rules. In contrast, we use partial translation instead to avoid unnecessary noise. Related Work Existing work on cross-lingual transfer can be classified into two categories. The first aims to train a dependency parsing model on source treebanks (McDonald et al., 2011; Guo et al., 2016a,b), or their adapted versions (Zhao et al., 2009; Tiedemann et al., 2014; Wang et al., 2017) in the target language. The second category, 1 Annotation Projection. The annotation projection approach relies on a set of parallel sentences between the source and target languages (Hwa et al., 2005; Ganchev et al., 2009). In parhttps://github.com/zhangmeishan/CodeMixedTreebank 998 ... s 1x ∗ hLSTM · · · hLSTM . For head finding, two nonn 1 linear feed-forward neural layers are used on dep dep hLSTM · · · hLSTM to obtain h1 · · · hn and n 1 hhead · · · hhead n . We compute the score for each de1 pendency ix j by: s nx ∗ BiAffine dep h1 ... hhead 1 dep ... hlstm 1 hhead n hn hlstm n dep six j = BiAffine(hi , hhead ) j BiLSTM × 3 x1 L e(c1 ) e(w1 ) e(t1 ) c1 w1 t1 ... xn ... ... e(cn ) e(wn ) e(tn ) L cn wn The above process is also used for scoring a lal tn beled dependency ix j, by extending the 1-dim vector s"
D19-1092,P15-1119,0,0.294242,"s in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 997–1006, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics lost or mistaken dependency arcs. We consider a different approach for translation-based syntactic knowledge transfer, which aims at making the best use of source syntax with the minimum noise being introduced. To this end, we leverage recent advances in cross-lingual word representations, such as cross-lingual word clusters (T¨ackstr¨om et al., 2012) and cross-lingual word embeddings (Guo et al., 2015), which allow words from different languages to reside within a consistent feature vector space according to structural similarities between words. Thus, they offer a bridge on the lexical level between different languages (Ammar et al., 2016). A cross-lingual model can be trained by directly using cross-lingual word representations on a source treebank (Guo et al., 2015). Using this method, knowledge transfer can be achieved on the level of token correspondences. We take this approach as a naive baseline. To further introduce structural feature transfer, we transform a source treebank into a"
D19-1092,2005.mtsummit-papers.11,0,0.0272045,"roposed models in this section. 5.1 Settings Our experiments are conducted on the Google Universal Dependency Treebanks (v2.0) (McDonald et al., 2013; Nivre et al., 2016), using English as the source language, and choosing six languages, including Spanish (ES), German (DE), French (FR), Italian (IT), Portuguese (PT) and Swedish (sv), as the target languages. Google 1001 Translate3 is used to translate the sentences in the English training set into other languages. In order to generate high-quality word-level alignments, we merge the translated sentence pairs and the parallel data of EuroParl (Koehn, 2005) to obtain word alignments. We use the fastAlign tool (Dyer et al., 2013) to obtain word alignments. We use the cross-lingual word embeddings and clusters by Guo et al. (2016b) for the baseline system. The dimension size of word embeddings is 50 and the word cluster number across of all languages is 256. For network building and training, we use the same setting as Dozat and Manning (2016), including the dimensional sizes, the dropout ratio, as well as the parameter optimization method. We assume that no labeled corpus is available for the target language. Thus training is performed for 50 ite"
D19-1092,N16-1121,0,0.576819,"e details. Figure 2: The overall architecture of the BiAffine parser. ticular, a source parser trained on the source treebank is used to parse the source-side sentences of the parallel corpus. The source dependencies are then projected onto the target sentences according to word alignments. Different strategies can be applied for the dependency projection task (Ma and Xia, 2014; Rasooli and Collins, 2015; Xiao and Guo, 2015; Agi´c et al., 2016; Schlichtkrull and Søgaard, 2017). For example, one can project only dependency arcs whose words are aligned to target-side words with high confidence (Lacroix et al., 2016). The resulting treebank can be highly noisy due to the auto-parsed source dependency trees. Recently Lacroix et al. (2016) and Rasooli and Collins (2017) propose to filter the results from the large-scale parallel corpus. Our work is different in that the source dependencies are from gold-standard treebanks. 3 4 Code-Mixed Treebank Translation We derive code-mixed trees from source dependency trees by partial translation, projecting words and the corresponding dependencies having highconfidence alignments with machine-translated target sentences. Our approach assumes that sentence level trans"
D19-1092,D18-1543,0,0.0391853,"Missing"
D19-1092,P14-1126,0,0.370007,"T algorithm to ensure tree-structural outputs. For training, we accumulate the crossentropy loss at the word-level by treating the normalized scores as prediction probabilities. The reader is referred to Dozat and Manning (2016) for more details. Figure 2: The overall architecture of the BiAffine parser. ticular, a source parser trained on the source treebank is used to parse the source-side sentences of the parallel corpus. The source dependencies are then projected onto the target sentences according to word alignments. Different strategies can be applied for the dependency projection task (Ma and Xia, 2014; Rasooli and Collins, 2015; Xiao and Guo, 2015; Agi´c et al., 2016; Schlichtkrull and Søgaard, 2017). For example, one can project only dependency arcs whose words are aligned to target-side words with high confidence (Lacroix et al., 2016). The resulting treebank can be highly noisy due to the auto-parsed source dependency trees. Recently Lacroix et al. (2016) and Rasooli and Collins (2017) propose to filter the results from the large-scale parallel corpus. Our work is different in that the source dependencies are from gold-standard treebanks. 3 4 Code-Mixed Treebank Translation We derive co"
D19-1092,D11-1006,0,0.824924,"l syntactic transfer. Take dependency grammar for instance. Given a source treebank, machine translation is used to find target translations of its sentences. Then word alignment is used to find mappings between source and target words, so that source syntactic dependencies can be projected to the target translations. Following, a postprocessing step is applied by removing unaligned target words, in order to ensure that the resulting target syntax forms a valid dependency tree, The method has shown promising performance for unsupervised cross-lingual dependency parsing among transfer methods (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Rasooli and Collins, 2015; Guo et al., 2016b). ∗ en The treebank translation method, however, suffers from various sources of noise. For example, machine translation errors directly affect the resulting treebank, by introducing ungrammatical word sequences. In addition, the alignments between source and target words may not be isomorphic due to inherent differences between languages or paraphrasing during translation. For example, in the case of Figure 1, the English words “are” and “being”, and the Swedish word “med”, do not have corresponding word-level translatio"
D19-1092,P12-1066,0,0.0595013,"dependencies by parsing parallel sentences (Hwa et al., 2005; Rasooli and Collins, 2015). The two broad methods are orthogonal to each other, and can both make use of the lexicalized dependency models trained with cross-lingual word representations (Rasooli and Collins, 2017, 2019). Source Treebank Adaption. There has been much work on unsupervised cross-lingual dependency parsing by direct source treebank transferring. Several researchers investigate delexicalized models where only non-lexical features are used in the models (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015). All the features in these models are language independent, and are consistent across languages and treebanks. Thus they can be applied into target languages directly. Subsequent research proposes to exploit lexicalized features to enhance the parsing models, by resorting to cross-lingual word representations (T¨ackstr¨om et al., 2012; Guo et al., 2015; Duong et al., 2015b,a; Zhang and Barzilay, 2015; Guo et al., 2016b; Ammar et al., 2016; Wick et al., 2016; de Lhoneux et al., 2018). Cross-lingual word clusters and cross-lingual word embed"
D19-1092,D15-1039,0,0.671112,"instance. Given a source treebank, machine translation is used to find target translations of its sentences. Then word alignment is used to find mappings between source and target words, so that source syntactic dependencies can be projected to the target translations. Following, a postprocessing step is applied by removing unaligned target words, in order to ensure that the resulting target syntax forms a valid dependency tree, The method has shown promising performance for unsupervised cross-lingual dependency parsing among transfer methods (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Rasooli and Collins, 2015; Guo et al., 2016b). ∗ en The treebank translation method, however, suffers from various sources of noise. For example, machine translation errors directly affect the resulting treebank, by introducing ungrammatical word sequences. In addition, the alignments between source and target words may not be isomorphic due to inherent differences between languages or paraphrasing during translation. For example, in the case of Figure 1, the English words “are” and “being”, and the Swedish word “med”, do not have corresponding word-level translation. In addition, it can be perfect to express “as soon"
D19-1092,Q17-1020,0,0.576924,"xed treebank can bring significantly better performance compared to a fully translated treebank, resulting in averaged improvements of 4.30 points on LAS. The code and related data will be released publicly available under Apache License 2.0.1 2 namely annotation projection, aims to produce a set of large-scale training instances full of automatic dependencies by parsing parallel sentences (Hwa et al., 2005; Rasooli and Collins, 2015). The two broad methods are orthogonal to each other, and can both make use of the lexicalized dependency models trained with cross-lingual word representations (Rasooli and Collins, 2017, 2019). Source Treebank Adaption. There has been much work on unsupervised cross-lingual dependency parsing by direct source treebank transferring. Several researchers investigate delexicalized models where only non-lexical features are used in the models (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015). All the features in these models are language independent, and are consistent across languages and treebanks. Thus they can be applied into target languages directly. Subsequent research proposes to e"
D19-1092,N19-1385,0,0.393276,"Missing"
D19-1092,P15-2040,0,0.222173,"et al., 2005; Rasooli and Collins, 2015). The two broad methods are orthogonal to each other, and can both make use of the lexicalized dependency models trained with cross-lingual word representations (Rasooli and Collins, 2017, 2019). Source Treebank Adaption. There has been much work on unsupervised cross-lingual dependency parsing by direct source treebank transferring. Several researchers investigate delexicalized models where only non-lexical features are used in the models (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015). All the features in these models are language independent, and are consistent across languages and treebanks. Thus they can be applied into target languages directly. Subsequent research proposes to exploit lexicalized features to enhance the parsing models, by resorting to cross-lingual word representations (T¨ackstr¨om et al., 2012; Guo et al., 2015; Duong et al., 2015b,a; Zhang and Barzilay, 2015; Guo et al., 2016b; Ammar et al., 2016; Wick et al., 2016; de Lhoneux et al., 2018). Cross-lingual word clusters and cross-lingual word embeddings are two main sources of features for transferrin"
D19-1092,E17-1021,0,0.012567,"ntropy loss at the word-level by treating the normalized scores as prediction probabilities. The reader is referred to Dozat and Manning (2016) for more details. Figure 2: The overall architecture of the BiAffine parser. ticular, a source parser trained on the source treebank is used to parse the source-side sentences of the parallel corpus. The source dependencies are then projected onto the target sentences according to word alignments. Different strategies can be applied for the dependency projection task (Ma and Xia, 2014; Rasooli and Collins, 2015; Xiao and Guo, 2015; Agi´c et al., 2016; Schlichtkrull and Søgaard, 2017). For example, one can project only dependency arcs whose words are aligned to target-side words with high confidence (Lacroix et al., 2016). The resulting treebank can be highly noisy due to the auto-parsed source dependency trees. Recently Lacroix et al. (2016) and Rasooli and Collins (2017) propose to filter the results from the large-scale parallel corpus. Our work is different in that the source dependencies are from gold-standard treebanks. 3 4 Code-Mixed Treebank Translation We derive code-mixed trees from source dependency trees by partial translation, projecting words and the correspo"
D19-1092,N13-1126,0,0.31626,"Missing"
D19-1092,N12-1052,0,0.152277,"Missing"
D19-1092,W15-1824,0,0.512515,"eebanks, giving highly competitive performances among cross-lingual parsing methods. 1 we are being accused unfairly sv vi beskylls med or¨att d¨ar root here advmod advmod nsubjpass vi(we) beskylls(accused) or¨att(unfairly) d¨ar(there) (a) full-scale translation. root nsubjpass auxpass vi(we) sv being en advmod advmod beskylls(accused) sv or¨att(unfairly) sv here en (b) this method, partial translation. Figure 1: An example to illustrate our method, where the source and target languages are English (en) and Swedish (sv), respectively. Introduction Treebank translation (Tiedemann et al., 2014; Tiedemann, 2015; Tiedemann and Agi´c, 2016) has been considered as a method for cross-lingual syntactic transfer. Take dependency grammar for instance. Given a source treebank, machine translation is used to find target translations of its sentences. Then word alignment is used to find mappings between source and target words, so that source syntactic dependencies can be projected to the target translations. Following, a postprocessing step is applied by removing unaligned target words, in order to ensure that the resulting target syntax forms a valid dependency tree, The method has shown promising performan"
D19-1092,W14-1614,0,0.512706,"Missing"
D19-1092,P17-1159,1,0.856702,"ne translation. In addition, the targetside sentences are produced by machine translation. Previous work aims to build a well-formed tree (Tiedemann and Agi´c, 2016) from source dependencies, solving word alignment conflicts by heuristic rules. In contrast, we use partial translation instead to avoid unnecessary noise. Related Work Existing work on cross-lingual transfer can be classified into two categories. The first aims to train a dependency parsing model on source treebanks (McDonald et al., 2011; Guo et al., 2016a,b), or their adapted versions (Zhao et al., 2009; Tiedemann et al., 2014; Wang et al., 2017) in the target language. The second category, 1 Annotation Projection. The annotation projection approach relies on a set of parallel sentences between the source and target languages (Hwa et al., 2005; Ganchev et al., 2009). In parhttps://github.com/zhangmeishan/CodeMixedTreebank 998 ... s 1x ∗ hLSTM · · · hLSTM . For head finding, two nonn 1 linear feed-forward neural layers are used on dep dep hLSTM · · · hLSTM to obtain h1 · · · hn and n 1 hhead · · · hhead n . We compute the score for each de1 pendency ix j by: s nx ∗ BiAffine dep h1 ... hhead 1 dep ... hlstm 1 hhead n hn hlstm n dep six"
D19-1092,K15-1008,0,0.0161341,". For training, we accumulate the crossentropy loss at the word-level by treating the normalized scores as prediction probabilities. The reader is referred to Dozat and Manning (2016) for more details. Figure 2: The overall architecture of the BiAffine parser. ticular, a source parser trained on the source treebank is used to parse the source-side sentences of the parallel corpus. The source dependencies are then projected onto the target sentences according to word alignments. Different strategies can be applied for the dependency projection task (Ma and Xia, 2014; Rasooli and Collins, 2015; Xiao and Guo, 2015; Agi´c et al., 2016; Schlichtkrull and Søgaard, 2017). For example, one can project only dependency arcs whose words are aligned to target-side words with high confidence (Lacroix et al., 2016). The resulting treebank can be highly noisy due to the auto-parsed source dependency trees. Recently Lacroix et al. (2016) and Rasooli and Collins (2017) propose to filter the results from the large-scale parallel corpus. Our work is different in that the source dependencies are from gold-standard treebanks. 3 4 Code-Mixed Treebank Translation We derive code-mixed trees from source dependency trees by"
D19-1092,C18-1327,1,0.767354,"only. • Src+Mix: The BiAffine model trained on the combination dataset of the source and code-mixed treebanks. Models We compare performances on the following models: 3 71.5 The Src and Tgt methods have been discussed in Section 4. The PartProj model is another way to leverage imperfect word alignments (Lacroix et al., 2016). The training corpus of PartProj may be incomplete dependency trees with a number of words missing heads, because no word is deleted from machine translation outputs. The POS tags of words in PartProj with low-confidence alignments are obtained by a supervised POS tagger (Yang et al., 2018) trained on the corresponding universal treebank. 5.3 Development Results We conduct several developmental experiments on the Swedish dataset to examine important factors to our model. 5.3.1 Influence of The Translation Ratio λ Our model has an important hyper-parameter λ to control the percentage of translation. Figure 6 shows the influence of this factor, where the percentages increase from 0 to 1 by intervals of 0.1. A 1002 Model Mix −Sentence Reordering −Word Deletion −Both UAS 80.33 79.79 79.82 79.46 LAS 71.29 70.47 70.64 69.59 tence reordering, the mix model shows decreases of 0.82 and 0"
D19-1092,I08-3008,0,0.118085,"produce a set of large-scale training instances full of automatic dependencies by parsing parallel sentences (Hwa et al., 2005; Rasooli and Collins, 2015). The two broad methods are orthogonal to each other, and can both make use of the lexicalized dependency models trained with cross-lingual word representations (Rasooli and Collins, 2017, 2019). Source Treebank Adaption. There has been much work on unsupervised cross-lingual dependency parsing by direct source treebank transferring. Several researchers investigate delexicalized models where only non-lexical features are used in the models (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015). All the features in these models are language independent, and are consistent across languages and treebanks. Thus they can be applied into target languages directly. Subsequent research proposes to exploit lexicalized features to enhance the parsing models, by resorting to cross-lingual word representations (T¨ackstr¨om et al., 2012; Guo et al., 2015; Duong et al., 2015b,a; Zhang and Barzilay, 2015; Guo et al., 2016b; Ammar et al., 2016; Wick et al., 2016; de Lhoneux et al.,"
D19-1092,D15-1213,0,0.0994529,"te delexicalized models where only non-lexical features are used in the models (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015). All the features in these models are language independent, and are consistent across languages and treebanks. Thus they can be applied into target languages directly. Subsequent research proposes to exploit lexicalized features to enhance the parsing models, by resorting to cross-lingual word representations (T¨ackstr¨om et al., 2012; Guo et al., 2015; Duong et al., 2015b,a; Zhang and Barzilay, 2015; Guo et al., 2016b; Ammar et al., 2016; Wick et al., 2016; de Lhoneux et al., 2018). Cross-lingual word clusters and cross-lingual word embeddings are two main sources of features for transferring knowledge between source and target language sentences. These studies enable us to train lexicalized models on code-mixed treebanks as well. Thus here we integrate the cross-lingual word representations as well, which gives more direct interaction between source and target words. Our work follows another mainstream method of this line of work, namely treebank translation (Tiedemann et al., 2014; Tie"
D19-1092,P09-1007,0,0.0497553,"treebank into the target language by machine translation. In addition, the targetside sentences are produced by machine translation. Previous work aims to build a well-formed tree (Tiedemann and Agi´c, 2016) from source dependencies, solving word alignment conflicts by heuristic rules. In contrast, we use partial translation instead to avoid unnecessary noise. Related Work Existing work on cross-lingual transfer can be classified into two categories. The first aims to train a dependency parsing model on source treebanks (McDonald et al., 2011; Guo et al., 2016a,b), or their adapted versions (Zhao et al., 2009; Tiedemann et al., 2014; Wang et al., 2017) in the target language. The second category, 1 Annotation Projection. The annotation projection approach relies on a set of parallel sentences between the source and target languages (Hwa et al., 2005; Ganchev et al., 2009). In parhttps://github.com/zhangmeishan/CodeMixedTreebank 998 ... s 1x ∗ hLSTM · · · hLSTM . For head finding, two nonn 1 linear feed-forward neural layers are used on dep dep hLSTM · · · hLSTM to obtain h1 · · · hn and n 1 hhead · · · hhead n . We compute the score for each de1 pendency ix j by: s nx ∗ BiAffine dep h1 ... hhead 1"
D19-1301,P00-1041,0,0.612774,"is more focused on the relevant parts for the summary than the conventional attention mechanism, and greatly advances the stateof-the-art performance on the abstractive sentence summarization task. We release the code at https://github.com/travel-go/ Abstractive-Text-Summarization. 1 Introduction Abstractive sentence summarization aims at generating concise and informative summaries based on the core meaning of source sentences. Previous endeavors tackle the problem through either rule-based methods (Dorr et al., 2003) or statistical models trained on relatively small scale training corpora (Banko et al., 2000). Following its successful applications on machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), the sequence-to-sequence framework is also applied on the abstractive sentence summarization task using large-scale sentence summary corpora (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., ∗ Equal contribution. 2016), obtaining better performance compared to the traditional methods. One central component in state-of-the-art sequence to sequence models is the use of attention for building connections between the source sequence and target words, so that a more informed deci"
D19-1301,N16-1012,0,0.447297,"uction Abstractive sentence summarization aims at generating concise and informative summaries based on the core meaning of source sentences. Previous endeavors tackle the problem through either rule-based methods (Dorr et al., 2003) or statistical models trained on relatively small scale training corpora (Banko et al., 2000). Following its successful applications on machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), the sequence-to-sequence framework is also applied on the abstractive sentence summarization task using large-scale sentence summary corpora (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., ∗ Equal contribution. 2016), obtaining better performance compared to the traditional methods. One central component in state-of-the-art sequence to sequence models is the use of attention for building connections between the source sequence and target words, so that a more informed decision can be made for generating a target word by considering the most relevant parts of the source sequence (Bahdanau et al., 2015; Vaswani et al., 2017). For abstractive sentence summarization, such attention mechanisms can be useful for selecting the most salient words for a short summary,"
D19-1301,C08-1018,0,0.0465846,"2000; Knight and Marcu, 2000; Neto et al., 2002), while the latter grasps the core meaning of the source text and re-state it in short text as abstractive summary (Banko et al., 2000; Rush et al., 2015). In this paper, we focus on abstractive summarization, and especially on abstractive sentence summarization. Previous work deals with the abstractive sentence summarization task by using either rule based methods (Dorr et al., 2003), or statistical methods utilizing a source-summary parallel corpus to train a machine translation model (Banko et al., 2000), or a syntax based transduction model (Cohn and Lapata, 2008; Woodsend et al., 2010). In recent years, sequence-to-sequence neural framework becomes predominant on this task by encoding long source texts and decoding into short summaries together with the attention mechanism. RNN is the most commonly adopted and extensively explored architecture (Chopra et al., 2016; Nallapati et al., 2016; Li et al., 2017). A CNN-based architecture is recently employed by Gehring et al. (2017) using ConvS2S, which applies CNN on both encoder and decoder. Later, Wang et al. (2018) build upon ConvS2S with topic words embedding and encoding, and train the system with rei"
D19-1301,W03-0501,0,0.276518,"ty. Experiments on benchmark datasets show that, the proposed contrastive attention mechanism is more focused on the relevant parts for the summary than the conventional attention mechanism, and greatly advances the stateof-the-art performance on the abstractive sentence summarization task. We release the code at https://github.com/travel-go/ Abstractive-Text-Summarization. 1 Introduction Abstractive sentence summarization aims at generating concise and informative summaries based on the core meaning of source sentences. Previous endeavors tackle the problem through either rule-based methods (Dorr et al., 2003) or statistical models trained on relatively small scale training corpora (Banko et al., 2000). Following its successful applications on machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), the sequence-to-sequence framework is also applied on the abstractive sentence summarization task using large-scale sentence summary corpora (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., ∗ Equal contribution. 2016), obtaining better performance compared to the traditional methods. One central component in state-of-the-art sequence to sequence models is the use of attention for b"
D19-1301,N18-1033,0,0.0551411,"on training. Other explorations with respect to the characteristics of the abstractive summarization task include copying mechanism that copies words from source sequences for composing summaries (Gu et al., 2016; Gulcehre et al., 2016; Song et al., 2018b), the selection mechanism that elaborately selects important parts of source sentences (Zhou et al., 2017; Lin et al., 2018), the distraction mechanism that avoids repeated attention on the same area (Chen et al., 2016), and the sequence level training that avoids exposure bias in teacher forcing methods (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018). Such methods are built on conventional attention, and are orthogonal to our proposed contrastive attention mechanism. 3 Approach We use two categories of attention for summary generation. One is the conventional attention that attends to relevant parts of source sentence, the other is the opponent attention that contrarily attends to irrelevant or less relevant parts. Both categories of attention output probability distributions over summary words, which are jointly optimized by encouraging the contribution from the conventional attention and discouraging the contribution from the opponent a"
D19-1301,P16-1154,0,0.154751,"d so that they can be easily discriminated. In comparison, we apply the contrastive attention mechanism for sentence level summarization by contrastively attending to relevant parts and irrelevant or less relevant parts. Furthermore, we propose a novel softmax softmin functionality to train the attention mechanism, which is different to Song et al. (2018a), who use mean squared error loss for attention training. Other explorations with respect to the characteristics of the abstractive summarization task include copying mechanism that copies words from source sequences for composing summaries (Gu et al., 2016; Gulcehre et al., 2016; Song et al., 2018b), the selection mechanism that elaborately selects important parts of source sentences (Zhou et al., 2017; Lin et al., 2018), the distraction mechanism that avoids repeated attention on the same area (Chen et al., 2016), and the sequence level training that avoids exposure bias in teacher forcing methods (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018). Such methods are built on conventional attention, and are orthogonal to our proposed contrastive attention mechanism. 3 Approach We use two categories of attention for summary generation. On"
D19-1301,P16-1014,0,0.0325471,"n be easily discriminated. In comparison, we apply the contrastive attention mechanism for sentence level summarization by contrastively attending to relevant parts and irrelevant or less relevant parts. Furthermore, we propose a novel softmax softmin functionality to train the attention mechanism, which is different to Song et al. (2018a), who use mean squared error loss for attention training. Other explorations with respect to the characteristics of the abstractive summarization task include copying mechanism that copies words from source sequences for composing summaries (Gu et al., 2016; Gulcehre et al., 2016; Song et al., 2018b), the selection mechanism that elaborately selects important parts of source sentences (Zhou et al., 2017; Lin et al., 2018), the distraction mechanism that avoids repeated attention on the same area (Chen et al., 2016), and the sequence level training that avoids exposure bias in teacher forcing methods (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018). Such methods are built on conventional attention, and are orthogonal to our proposed contrastive attention mechanism. 3 Approach We use two categories of attention for summary generation. One is the conventional a"
D19-1301,D15-1229,0,0.398146,"he annotated Gigaword corpus and preprocess it identically to Rush et al. (2015), which results in around 3.8M training samples, 190K validation samples and 1951 test samples for evaluation. The source-summary pairs are formed through pairing the first sentence of each article with its headline. We use DUC2004 as another English data set only for testing in our experiments. It contains 500 documents, each containing four human-generated reference summaries. The length of the summary is capped at 75 bytes. The last data set we used is a large corpus of Chinese short text summarization (LCSTS) (Hu et al., 2015), which is collected from the Chinese microblogging website Sina Weibo. We follow the data split of the original paper, with 2.4M sourcesummary pairs from the first part of the corpus for training, 725 pairs from the last part with high annotation score for testing. 4.2 Experimental Setup We employ Transformer as our basis architecture4 . Six layers are stacked in both the encoder and decoder, and the dimensions of the embedding vectors and all hidden vectors are set 512. The inner layer of the feed-forward sublayer has the dimensionality of 2048. We set eight heads in the multihead attention."
D19-1301,A00-2024,0,0.265608,"ccuracies compared with RNN and CNN alternatives. When equipped with the proposed contrastive attention mechanism, our Transformer model achieves the best reported results on all data. The visualization of attentions shows that through using the contrastive attention mechanism, our attention is more focused on relevant parts than the baseline. We release our code at XXX. 2 Related Work Automatic summarization has been investigated in two main paradigms: the extractive method and the abstractive method. The former extracts important pieces of source document and concatenates them sequentially (Jing and McKeown, 2000; Knight and Marcu, 2000; Neto et al., 2002), while the latter grasps the core meaning of the source text and re-state it in short text as abstractive summary (Banko et al., 2000; Rush et al., 2015). In this paper, we focus on abstractive summarization, and especially on abstractive sentence summarization. Previous work deals with the abstractive sentence summarization task by using either rule based methods (Dorr et al., 2003), or statistical methods utilizing a source-summary parallel corpus to train a machine translation model (Banko et al., 2000), or a syntax based transduction model (Cohn"
D19-1301,D17-1222,0,0.108139,"Missing"
D19-1301,W04-1013,0,0.0328382,"third layer for deriving the opponent attention in the English experiments, and select the second head of the third layer in the Chinese experiments. All dimensions in the contrastive architecture are set 64. The λ in Equation (9) is tuned on the development set in each experiment. During training, we use the Adam optimizer with β1 = 0.9, β2 = 0.98, ε= 10−9 . The initial learning rate is 0.0005. The inverse square root schedule is applied for initial warm up and annealing (Vaswani et al., 2017). During training, we use a dropout rate of 0.3 on all datasets. During evaluation, we employ ROUGE (Lin, 2004) as our evaluation metric. Since standard Rouge package is used to evaluate the English summarization systems, we also follow the method of Hu et al. (2015) to map Chinese words into numerical IDs in order to evaluate the performance on the Chinese data set. 4.3 4.3.1 Results English Results The experimental results on the English evaluation sets are listed in Table 1. We report the full-length F-1 scores of ROUGE-1 (R-1), ROUGE2 (R-2), and ROUGE-L (R-L) on the evaluation set of the annotated Gigaword, while report the recall-based scores of the R-1, R-2, and R-L on the evaluation set of DUC20"
D19-1301,P18-2027,0,0.499364,"relevant parts and irrelevant or less relevant parts. Furthermore, we propose a novel softmax softmin functionality to train the attention mechanism, which is different to Song et al. (2018a), who use mean squared error loss for attention training. Other explorations with respect to the characteristics of the abstractive summarization task include copying mechanism that copies words from source sequences for composing summaries (Gu et al., 2016; Gulcehre et al., 2016; Song et al., 2018b), the selection mechanism that elaborately selects important parts of source sentences (Zhou et al., 2017; Lin et al., 2018), the distraction mechanism that avoids repeated attention on the same area (Chen et al., 2016), and the sequence level training that avoids exposure bias in teacher forcing methods (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018). Such methods are built on conventional attention, and are orthogonal to our proposed contrastive attention mechanism. 3 Approach We use two categories of attention for summary generation. One is the conventional attention that attends to relevant parts of source sentence, the other is the opponent attention that contrarily attends to irrelevant or less rel"
D19-1301,C18-1146,0,0.0906615,"Missing"
D19-1301,D10-1050,0,0.0175144,"2000; Neto et al., 2002), while the latter grasps the core meaning of the source text and re-state it in short text as abstractive summary (Banko et al., 2000; Rush et al., 2015). In this paper, we focus on abstractive summarization, and especially on abstractive sentence summarization. Previous work deals with the abstractive sentence summarization task by using either rule based methods (Dorr et al., 2003), or statistical methods utilizing a source-summary parallel corpus to train a machine translation model (Banko et al., 2000), or a syntax based transduction model (Cohn and Lapata, 2008; Woodsend et al., 2010). In recent years, sequence-to-sequence neural framework becomes predominant on this task by encoding long source texts and decoding into short summaries together with the attention mechanism. RNN is the most commonly adopted and extensively explored architecture (Chopra et al., 2016; Nallapati et al., 2016; Li et al., 2017). A CNN-based architecture is recently employed by Gehring et al. (2017) using ConvS2S, which applies CNN on both encoder and decoder. Later, Wang et al. (2018) build upon ConvS2S with topic words embedding and encoding, and train the system with reinforcement learning. The"
D19-1301,P17-1101,0,0.35429,"tively attending to relevant parts and irrelevant or less relevant parts. Furthermore, we propose a novel softmax softmin functionality to train the attention mechanism, which is different to Song et al. (2018a), who use mean squared error loss for attention training. Other explorations with respect to the characteristics of the abstractive summarization task include copying mechanism that copies words from source sequences for composing summaries (Gu et al., 2016; Gulcehre et al., 2016; Song et al., 2018b), the selection mechanism that elaborately selects important parts of source sentences (Zhou et al., 2017; Lin et al., 2018), the distraction mechanism that avoids repeated attention on the same area (Chen et al., 2016), and the sequence level training that avoids exposure bias in teacher forcing methods (Ayana et al., 2016; Li et al., 2018; Edunov et al., 2018). Such methods are built on conventional attention, and are orthogonal to our proposed contrastive attention mechanism. 3 Approach We use two categories of attention for summary generation. One is the conventional attention that attends to relevant parts of source sentence, the other is the opponent attention that contrarily attends to irr"
D19-1301,K16-1028,0,0.158268,"with the abstractive sentence summarization task by using either rule based methods (Dorr et al., 2003), or statistical methods utilizing a source-summary parallel corpus to train a machine translation model (Banko et al., 2000), or a syntax based transduction model (Cohn and Lapata, 2008; Woodsend et al., 2010). In recent years, sequence-to-sequence neural framework becomes predominant on this task by encoding long source texts and decoding into short summaries together with the attention mechanism. RNN is the most commonly adopted and extensively explored architecture (Chopra et al., 2016; Nallapati et al., 2016; Li et al., 2017). A CNN-based architecture is recently employed by Gehring et al. (2017) using ConvS2S, which applies CNN on both encoder and decoder. Later, Wang et al. (2018) build upon ConvS2S with topic words embedding and encoding, and train the system with reinforcement learning. The most related work to our contrastive attention mechanism is in the field of computer vision. Song et al. (2018a) first propose the contrastive attention mechanism for person re-identification. In their work, based on a pre-provided person and background segmentation, the two regions are contrastively atten"
D19-1301,D15-1044,0,0.701741,"arization. 1 Introduction Abstractive sentence summarization aims at generating concise and informative summaries based on the core meaning of source sentences. Previous endeavors tackle the problem through either rule-based methods (Dorr et al., 2003) or statistical models trained on relatively small scale training corpora (Banko et al., 2000). Following its successful applications on machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), the sequence-to-sequence framework is also applied on the abstractive sentence summarization task using large-scale sentence summary corpora (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., ∗ Equal contribution. 2016), obtaining better performance compared to the traditional methods. One central component in state-of-the-art sequence to sequence models is the use of attention for building connections between the source sequence and target words, so that a more informed decision can be made for generating a target word by considering the most relevant parts of the source sequence (Bahdanau et al., 2015; Vaswani et al., 2017). For abstractive sentence summarization, such attention mechanisms can be useful for selecting the most salient words"
D19-1377,S12-1050,0,0.032979,", 2017) and Spider (Yu et al., 2018b). One salient difference between Spider and prior work is that Spider uses different databases across domains for training and testing, which can verify the generalization power of a semantic parsing model. Compared with WikiSQL, Spider further has multiple tables in each database and correspondingly more complex queries. We thus consider Spider for sourcing our dataset. Existing semantic parsing datasets for Chinese include a small corpus for assigning semantic roles (Sun and Jurafsky, 2004) and SemEval-2016 Task 9 for Chinese semantic dependency parsing (Che et al., 2012), but these data are not related to SQL. To our knowledge, we are the first to release a Chinese SQL semantic parsing dataset. There has been a line of work improving the model of Yu et al. (2018a) since the release of the Spider dataset (Guo et al., 2019; Bogin et al., 2019; Lin et al., 2019). At the time of our investigation, however, the models are not published. We thus chose the model of Yu et al. (2018a) as our baseline. The choice of more different neural models is orthogonal to our dataset contribution, but can empirically give more insights about the conclusions. 3 Dataset We translat"
D19-1377,P16-1074,0,0.0151007,"rectly identify both the column name and the table name for this corrected sentence. Since zero-pronouns are frequent Predicted query 代表 的 不同 党派 是 什么 ？ 显示 各 党 的 党名 和 代表 人数 。 SELECT Date , COUNT(*) FROM What are the different parties of election GROUP BY Seats representative? Show the party name and the number of representatives. 代表 的 不同 党派 是 什么 ？ 显示 各 党 的 党名 和 各 党 的 代表 人数 。 What are the different parties of SELECT Party , COUNT(*) FROM representative? Show the party name representative GROUP BY Party and the number of representatives in each party. Figure 5: Chinese zero pronoun. for Chinese (Chen and Ng, 2016), they give added difficulty for its semantic parsing. 6 Conclusion We constructed a first resource named CSpider for Chinese sentence to SQL, evaluating the performance of a strong English model on this dataset. Results show that the input representation, embedding forms and linguistic factors all have the influence on the Chinese-specific task. Our dataset can serve as a starting point for further research on this task, which can be beneficial to the investigation of Chinese QA and dialogue models. Acknowledgments We thank the anonymous reviewers for their detailed and constructive comments."
D19-1377,H94-1010,0,0.44194,"e-translated questions. In addition, the overall accuracy for Chinese SQL semantic parsing can be comparable to that for English. We found that cross-lingual word embeddings are useful for matching Chinese questions with English table columns and keywords and that language characteristics have a significant influence on parsing results. We release our dataset named CSpider and code at https://github.com/taolusi/chisp. 2 Related Work Existing datasets for semantic parsing can be classified into two major categories. The first uses logic for semantic representation, including ATIS (Price, 1990; Dahl et al., 1994) and GeroQuery (Zelle and Mooney, 1996). The second and dominant category of datasets uses SQL, which includes Restaurants (Tang and Mooney, 2001; Popescu et al., 2003), Academic (Iyer et al., 2017), Yelp and IMDB (Yaghmazadeh et al., 2017), Ad3652 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3652–3658, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics vising (Finegan-Dollak et al., 2018) and the recently proposed WikiSQL (Zhong et al.,"
D19-1377,P18-1068,0,0.0158971,"for a semantic parser, and different embedding schemes. Results show that word-based semantic parser is subject to segmentation errors and cross-lingual word embeddings are useful for text-to-SQL. 1 Introduction The task of semantic parsing is highly useful for tasks such as dialogue (Chen et al., 2013; Gupta et al., 2018; Einolghozati et al., 2019) and question answering (Gildea and Jurafsky, 2002; Yih et al., 2015; Reddy et al., 2016). Among a wide range of possible semantic representations, SQL offers a standardized interface to knowledge bases across tasks (Astrova, 2009; Xu et al., 2017; Dong and Lapata, 2018; Lee et al., 2011). Recently, Yu et al. (2018b) released a manually labelled dataset for parsing natural language questions into complex SQL, which facilitates related research. Yu et al. (2018b)’s dataset is exclusive for English questions. Intuitively, the same semantic parsing task can be applied cross-lingual, since SQL is a universal semantic representation and database interface. However, for languages other than English, there can be added difficulties parsing into SQL. Take Chinese for example, the additional challenges can be at least two-fold. First, structures of relational databas"
D19-1377,P18-1033,0,0.0393703,"for semantic representation, including ATIS (Price, 1990; Dahl et al., 1994) and GeroQuery (Zelle and Mooney, 1996). The second and dominant category of datasets uses SQL, which includes Restaurants (Tang and Mooney, 2001; Popescu et al., 2003), Academic (Iyer et al., 2017), Yelp and IMDB (Yaghmazadeh et al., 2017), Ad3652 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3652–3658, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics vising (Finegan-Dollak et al., 2018) and the recently proposed WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b). One salient difference between Spider and prior work is that Spider uses different databases across domains for training and testing, which can verify the generalization power of a semantic parsing model. Compared with WikiSQL, Spider further has multiple tables in each database and correspondingly more complex queries. We thus consider Spider for sourcing our dataset. Existing semantic parsing datasets for Chinese include a small corpus for assigning semantic roles (Sun and Jurafsky, 2004) and SemEval-2016"
D19-1377,J02-3001,0,0.0210265,"s arise from the uniqueness of the language, which requires word segmentation, and also from the fact that SQL keywords and columns of DB tables are typically written in English. We compare character- and wordbased encoders for a semantic parser, and different embedding schemes. Results show that word-based semantic parser is subject to segmentation errors and cross-lingual word embeddings are useful for text-to-SQL. 1 Introduction The task of semantic parsing is highly useful for tasks such as dialogue (Chen et al., 2013; Gupta et al., 2018; Einolghozati et al., 2019) and question answering (Gildea and Jurafsky, 2002; Yih et al., 2015; Reddy et al., 2016). Among a wide range of possible semantic representations, SQL offers a standardized interface to knowledge bases across tasks (Astrova, 2009; Xu et al., 2017; Dong and Lapata, 2018; Lee et al., 2011). Recently, Yu et al. (2018b) released a manually labelled dataset for parsing natural language questions into complex SQL, which facilitates related research. Yu et al. (2018b)’s dataset is exclusive for English questions. Intuitively, the same semantic parsing task can be applied cross-lingual, since SQL is a universal semantic representation and database i"
D19-1377,P19-1444,0,0.183154,"h WikiSQL, Spider further has multiple tables in each database and correspondingly more complex queries. We thus consider Spider for sourcing our dataset. Existing semantic parsing datasets for Chinese include a small corpus for assigning semantic roles (Sun and Jurafsky, 2004) and SemEval-2016 Task 9 for Chinese semantic dependency parsing (Che et al., 2012), but these data are not related to SQL. To our knowledge, we are the first to release a Chinese SQL semantic parsing dataset. There has been a line of work improving the model of Yu et al. (2018a) since the release of the Spider dataset (Guo et al., 2019; Bogin et al., 2019; Lin et al., 2019). At the time of our investigation, however, the models are not published. We thus chose the model of Yu et al. (2018a) as our baseline. The choice of more different neural models is orthogonal to our dataset contribution, but can empirically give more insights about the conclusions. 3 Dataset We translate all English questions in the Spider dataset into Chinese.1 The work is undertaken by 2 NLP researchers and 1 computer science student. Each question is first translated by one annotator, and then cross-checked and corrected by a second annotator. Finall"
D19-1377,D18-1300,0,0.0275364,"low-resource language in this task area. Interesting research questions arise from the uniqueness of the language, which requires word segmentation, and also from the fact that SQL keywords and columns of DB tables are typically written in English. We compare character- and wordbased encoders for a semantic parser, and different embedding schemes. Results show that word-based semantic parser is subject to segmentation errors and cross-lingual word embeddings are useful for text-to-SQL. 1 Introduction The task of semantic parsing is highly useful for tasks such as dialogue (Chen et al., 2013; Gupta et al., 2018; Einolghozati et al., 2019) and question answering (Gildea and Jurafsky, 2002; Yih et al., 2015; Reddy et al., 2016). Among a wide range of possible semantic representations, SQL offers a standardized interface to knowledge bases across tasks (Astrova, 2009; Xu et al., 2017; Dong and Lapata, 2018; Lee et al., 2011). Recently, Yu et al. (2018b) released a manually labelled dataset for parsing natural language questions into complex SQL, which facilitates related research. Yu et al. (2018b)’s dataset is exclusive for English questions. Intuitively, the same semantic parsing task can be applied"
D19-1377,P17-1089,0,0.0354166,"Chinese questions with English table columns and keywords and that language characteristics have a significant influence on parsing results. We release our dataset named CSpider and code at https://github.com/taolusi/chisp. 2 Related Work Existing datasets for semantic parsing can be classified into two major categories. The first uses logic for semantic representation, including ATIS (Price, 1990; Dahl et al., 1994) and GeroQuery (Zelle and Mooney, 1996). The second and dominant category of datasets uses SQL, which includes Restaurants (Tang and Mooney, 2001; Popescu et al., 2003), Academic (Iyer et al., 2017), Yelp and IMDB (Yaghmazadeh et al., 2017), Ad3652 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3652–3658, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics vising (Finegan-Dollak et al., 2018) and the recently proposed WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b). One salient difference between Spider and prior work is that Spider uses different databases across domains for training and testing, which can verify the gener"
D19-1377,P19-1314,0,0.0759206,"Missing"
D19-1377,J82-2003,0,0.428431,"Missing"
D19-1377,D14-1162,0,0.0833874,"s paper. We provide a visualization of the model in Figure 1. 5 Experiments 5.1 We focus on comparing different word segmentation methods and different embedding representations. As discussed above, column names are selected by attention over column embeddings using sentence representation as a key. Hence there must be a link between the embeddings of columns and those of the questions. Since columns are written in English and questions in Chinese, we consider two embedding methods. The first method is to use two separate sets of embeddings for Chinese and English, respectively. We use Glove (Pennington et al., 2014)2 for embeddings of English keywords, column names etc., and Tencent embeddings (Song et al., 2018)3 for Chinese. The second method is to directly use the cross-lingual word embeddings. To this end, the Tencent multilingual embeddings are chosen, which contain both Chinese and English words in a multi-lingual embedding matrix. Evaluation Metrics. We follow Yu et al. (2018b), evaluating the results using two major 2 3 https://nlp.stanford.edu/projects/glove/ https://ai.tencent.com/ailab/nlp/embedding.html types of metrics. The first is exact matching accuracy, namely the percentage of questions"
D19-1377,H90-1020,0,0.162016,"sed of machine-translated questions. In addition, the overall accuracy for Chinese SQL semantic parsing can be comparable to that for English. We found that cross-lingual word embeddings are useful for matching Chinese questions with English table columns and keywords and that language characteristics have a significant influence on parsing results. We release our dataset named CSpider and code at https://github.com/taolusi/chisp. 2 Related Work Existing datasets for semantic parsing can be classified into two major categories. The first uses logic for semantic representation, including ATIS (Price, 1990; Dahl et al., 1994) and GeroQuery (Zelle and Mooney, 1996). The second and dominant category of datasets uses SQL, which includes Restaurants (Tang and Mooney, 2001; Popescu et al., 2003), Academic (Iyer et al., 2017), Yelp and IMDB (Yaghmazadeh et al., 2017), Ad3652 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3652–3658, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics vising (Finegan-Dollak et al., 2018) and the recently proposed Wi"
D19-1377,Q16-1010,0,0.0629,"Missing"
D19-1377,N18-2028,0,0.0696324,"ferent word segmentation methods and different embedding representations. As discussed above, column names are selected by attention over column embeddings using sentence representation as a key. Hence there must be a link between the embeddings of columns and those of the questions. Since columns are written in English and questions in Chinese, we consider two embedding methods. The first method is to use two separate sets of embeddings for Chinese and English, respectively. We use Glove (Pennington et al., 2014)2 for embeddings of English keywords, column names etc., and Tencent embeddings (Song et al., 2018)3 for Chinese. The second method is to directly use the cross-lingual word embeddings. To this end, the Tencent multilingual embeddings are chosen, which contain both Chinese and English words in a multi-lingual embedding matrix. Evaluation Metrics. We follow Yu et al. (2018b), evaluating the results using two major 2 3 https://nlp.stanford.edu/projects/glove/ https://ai.tencent.com/ailab/nlp/embedding.html types of metrics. The first is exact matching accuracy, namely the percentage of questions that have exactly the same SQL output as its reference. The second is component matching F1, namel"
D19-1377,N04-1032,0,0.0332783,"tics vising (Finegan-Dollak et al., 2018) and the recently proposed WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018b). One salient difference between Spider and prior work is that Spider uses different databases across domains for training and testing, which can verify the generalization power of a semantic parsing model. Compared with WikiSQL, Spider further has multiple tables in each database and correspondingly more complex queries. We thus consider Spider for sourcing our dataset. Existing semantic parsing datasets for Chinese include a small corpus for assigning semantic roles (Sun and Jurafsky, 2004) and SemEval-2016 Task 9 for Chinese semantic dependency parsing (Che et al., 2012), but these data are not related to SQL. To our knowledge, we are the first to release a Chinese SQL semantic parsing dataset. There has been a line of work improving the model of Yu et al. (2018a) since the release of the Spider dataset (Guo et al., 2019; Bogin et al., 2019; Lin et al., 2019). At the time of our investigation, however, the models are not published. We thus chose the model of Yu et al. (2018a) as our baseline. The choice of more different neural models is orthogonal to our dataset contribution,"
D19-1377,P18-1144,1,0.88619,"Missing"
D19-1377,P17-1078,1,0.81296,"Missing"
D19-1377,P15-1128,0,0.0399519,"of the language, which requires word segmentation, and also from the fact that SQL keywords and columns of DB tables are typically written in English. We compare character- and wordbased encoders for a semantic parser, and different embedding schemes. Results show that word-based semantic parser is subject to segmentation errors and cross-lingual word embeddings are useful for text-to-SQL. 1 Introduction The task of semantic parsing is highly useful for tasks such as dialogue (Chen et al., 2013; Gupta et al., 2018; Einolghozati et al., 2019) and question answering (Gildea and Jurafsky, 2002; Yih et al., 2015; Reddy et al., 2016). Among a wide range of possible semantic representations, SQL offers a standardized interface to knowledge bases across tasks (Astrova, 2009; Xu et al., 2017; Dong and Lapata, 2018; Lee et al., 2011). Recently, Yu et al. (2018b) released a manually labelled dataset for parsing natural language questions into complex SQL, which facilitates related research. Yu et al. (2018b)’s dataset is exclusive for English questions. Intuitively, the same semantic parsing task can be applied cross-lingual, since SQL is a universal semantic representation and database interface. However,"
D19-1377,D18-1193,0,0.251376,"es. Results show that word-based semantic parser is subject to segmentation errors and cross-lingual word embeddings are useful for text-to-SQL. 1 Introduction The task of semantic parsing is highly useful for tasks such as dialogue (Chen et al., 2013; Gupta et al., 2018; Einolghozati et al., 2019) and question answering (Gildea and Jurafsky, 2002; Yih et al., 2015; Reddy et al., 2016). Among a wide range of possible semantic representations, SQL offers a standardized interface to knowledge bases across tasks (Astrova, 2009; Xu et al., 2017; Dong and Lapata, 2018; Lee et al., 2011). Recently, Yu et al. (2018b) released a manually labelled dataset for parsing natural language questions into complex SQL, which facilitates related research. Yu et al. (2018b)’s dataset is exclusive for English questions. Intuitively, the same semantic parsing task can be applied cross-lingual, since SQL is a universal semantic representation and database interface. However, for languages other than English, there can be added difficulties parsing into SQL. Take Chinese for example, the additional challenges can be at least two-fold. First, structures of relational databases, in particular names and column names of DB"
D19-1377,D18-1425,0,0.0911494,"Missing"
D19-1422,W13-3520,0,0.0176873,", 2018). Our results are same as Table 6 of Yang et al. (2018). Model Plank et al. (2016) Huang et al. (2015) Ma and Hovy (2016) Liu et al. (2017) Yang et al. (2018) Zhang et al. (2018c) Yasunaga et al. (2018) Xin et al. (2018) Transformer-softmax (Guo et al., 2019) BiLSTM-softmax (Yang et al., 2018) BiLSTM-CRF (Yang et al., 2018) BiLSTM-LAN Settings Hyper-Parameters. We use 100-dimensional GloVe (Pennington et al., 2014) word embeddings for English POS tagging (WSJ, UD v2.2 EN) and name entity recognition, 300-dimensional Glove word embeddings for CCG supertagging and 64dimensional Polyglot (Al-Rfou et al., 2013) word embeddings for multilingual POS tagging. The detail values of the hyper-parameters for all experiments are summarized in Appendix A. Evaluation. F-1 score are used for NER. Other tasks are evaluated based on the accuracy. We repeat the same experiment five times with the same hyperparameters and report the max accuracy, average accuracy and standard deviation for POS tagging. For fair comparison, all experiments are implemented in NCRF++ (Yang and Zhang, 2018) and conducted using a GeForce GTX 1080Ti with 11GB memory. Mean±std 97.47±0.02 97.50±0.03 97.48±0.02 97.58±0.04 Accuracy 97.22 97"
D19-1422,J07-3004,0,0.0613871,"nts. For NER, we use the OntoNotes 5.0 (Hovy et al., 2006; Pradhan et al., 2013). Following previous work,we adopt the official OntoNotes data split used for co-reference resolution in the CoNLL-2012 shared task (Pradhan et al., 2012). We train our CCG supertagging 4119 Model BiLSTM-CRF (POS) BiLSTM-LAN (POS) BiLSTM-CRF (CCG) BiLSTM-LAN (CCG) train (s) 181.32 128.75 884.67 369.98 test (st/s) 781.90 805.32 599.18 713.70 Model BiLSTM-CRF† BiLSTM-CRF‡ BiLSTM-softmax† BiLSTM-LAN Table 4: Comparison of the training time for one iteration and decoding speed. st indicates sentences model on CCGBank (Hockenmaier and Steedman, 2007). Sections 2-21 are used for training, section 0 for development and section 23 as the test set. The performance is calculated on the 425 most frequent labels. Table 1 shows the numbers of sentences, tokens and labels for training, development and test, respectively. 5.2 5.3 Development Experiments We report a set of WSJ development experiments to show our investigation on key configurations of BiLSTM-LAN and BiLSTM-CRF. Label embedding size. Table 2 shows the effect of the label embedding size. Notable improvement can be achieved when the label embedding size increases from 200 to 400 in our"
D19-1422,D17-1047,0,0.0347079,"nd higher efficiencies than BiLSTM-CRF and BiLSTM-softmax with similar number of parameters. It gives highly competitive results compared with topperformance systems on WSJ, OntoNotes 5.0 and CCGBank without external training. In addition to accuracy and efficiency, BiLSTM-LAN is also more interpretable than BiLSTM-CRF thanks to visualizable label embeddings and label distributions. Our code and models are released at https://github.com/Nealcly/LAN. 2 Related Work Neural Attention. Attention has been shown useful in neural machine translation (Bahdanau et al., 2014), sentiment classification (Chen et al., 2017; Liu and Zhang, 2017), relation classification (Zhou et al., 2016), read comprehension (Hermann et al., 2015), sentence summarization (Rush et al., 2015), parsing (Li et al., 2016), question answering (Wang et al., 2018b) and text understanding (Kadlec et al., 2016). Self-attention network (SAN) (Vaswani et al., 2017) has been used for semantic role labeling (Strubell et al., 2018), text classification (Xu et al., 2018; Wu et al., 2018) and other tasks. Our work is similar to Vaswani et al. (2017) in the sense that we also build a hierarchical attentive neural network for sequence representat"
D19-1422,N06-2015,0,0.0135483,"ataset For English POS tagging, we use the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993), which has 45 POS tags. We adopt the same partitions used by previous work (Manning, 2011; Yang et al., 2018), selecting sections 0-18 as the training set, sections 19-21 as the development set and sections 22-24 as the test set. For multilingual POS tagging, we use treebanks from Universal Dependencies(UD) v2.2 (Silveira et al., 2014; Nivre et al., 2018) with the standard splits, choosing 8 resource-rich languages in our experiments. For NER, we use the OntoNotes 5.0 (Hovy et al., 2006; Pradhan et al., 2013). Following previous work,we adopt the official OntoNotes data split used for co-reference resolution in the CoNLL-2012 shared task (Pradhan et al., 2012). We train our CCG supertagging 4119 Model BiLSTM-CRF (POS) BiLSTM-LAN (POS) BiLSTM-CRF (CCG) BiLSTM-LAN (CCG) train (s) 181.32 128.75 884.67 369.98 test (st/s) 781.90 805.32 599.18 713.70 Model BiLSTM-CRF† BiLSTM-CRF‡ BiLSTM-softmax† BiLSTM-LAN Table 4: Comparison of the training time for one iteration and decoding speed. st indicates sentences model on CCGBank (Hockenmaier and Steedman, 2007). Sections 2-21 are used f"
D19-1422,Q16-1026,0,0.211004,"Missing"
D19-1422,D18-1217,0,0.0189461,"21 (a) 5 iterations (b) 15 iterations (c) 38 iterations Figure 4: t-SNE plot of label embeddings after different numbers of training iterations. OntoNotes 5.0. Durrett and Klein (2014) propose a joint model for coreference resolution, entity linking and NER. Chiu and Nichols (2016) use a BiLSTM with CNN character encoding. Shen et al. (2017) introduce active learning to get better performance. Strubell et al. (2017) present an iterated dilated convolutions, which is a faster alternative to BiLSTM. Ghaddar and Langlais (2018) demonstrate that lexical features are actually quite useful for NER. Clark et al. (2018) present a cross-view training for neural sequence models. BiLSTM-LAN obtains highly competitive results compared with various of top-performance models without training on external data. CCGBank. In CCG supertagging, the major challenge is a larger set of lexical tags |L| and supertag constraints over long distance dependencies. As shown in Table 9, BiLSTMLAN significantly outperforms both BiLSTMsoftmax and BiLSTM-CRF (p <0.01), showing the advantage of LAN. Xu et al. (2015) and Vaswani et al. (2016a) explore BiRNN-softmax and BiLSTM-softmax, respectively. Søgaard and Goldberg (2016) present"
D19-1422,P16-1086,0,0.0183296,"y, BiLSTM-LAN is also more interpretable than BiLSTM-CRF thanks to visualizable label embeddings and label distributions. Our code and models are released at https://github.com/Nealcly/LAN. 2 Related Work Neural Attention. Attention has been shown useful in neural machine translation (Bahdanau et al., 2014), sentiment classification (Chen et al., 2017; Liu and Zhang, 2017), relation classification (Zhou et al., 2016), read comprehension (Hermann et al., 2015), sentence summarization (Rush et al., 2015), parsing (Li et al., 2016), question answering (Wang et al., 2018b) and text understanding (Kadlec et al., 2016). Self-attention network (SAN) (Vaswani et al., 2017) has been used for semantic role labeling (Strubell et al., 2018), text classification (Xu et al., 2018; Wu et al., 2018) and other tasks. Our work is similar to Vaswani et al. (2017) in the sense that we also build a hierarchical attentive neural network for sequence representation. The difference lies in that our main goal is to investigate the encoding of exponential label sequences, whereas their work focuses on encoding of a word sequence only. Label Embeddings. Label embedding was first used in the field of computer vision for facilita"
D19-1422,Q16-1023,0,0.0237717,"and Xu, 2015; Ma and Hovy, 2016). This, however, sees mixed results. For example, previous work (Reimers and Gurevych, 2017; Yang et al., 2018) has shown that BiLSTM-softmax gives better accuracies compared to BiLSTMCRF for part-of-speech (POS) tagging. In addition, the state-of-the-art neural Combinatory Categorial Grammar (CCG) supertaggers do not use CRF (Xu et al., 2015; Lewis et al., 2016). One possible reason is that the strong representation power of neural sentence encoders such as BiLSTMs allow models to capture implicit long-range label dependencies from input word sequences alone (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016; Teng and Zhang, 2018), thereby allowing the output layer to make local predictions. In contrast, though explicitly capturing output label dependencies, CRF can be limited by its Markov assumptions, particularly when being used on top of neural encoders. In addition, CRF can be computationally expensive when the number of labels is large, due to the use of Viterbi decoding. One interesting research question is whether there is neural alternative to CRF for neural sequence labeling, which is both faster and more accurate. To this question, we investigate a neural netwo"
D19-1422,P81-1022,0,0.690361,"Missing"
D19-1422,Q14-1037,0,0.0434608,"Missing"
D19-1422,C18-1161,0,0.029799,"s are different from Table 2 of Yasunaga et al. (2018), since they reported results on UD v1.2 4121 (a) 5 iterations (b) 15 iterations (c) 38 iterations Figure 4: t-SNE plot of label embeddings after different numbers of training iterations. OntoNotes 5.0. Durrett and Klein (2014) propose a joint model for coreference resolution, entity linking and NER. Chiu and Nichols (2016) use a BiLSTM with CNN character encoding. Shen et al. (2017) introduce active learning to get better performance. Strubell et al. (2017) present an iterated dilated convolutions, which is a faster alternative to BiLSTM. Ghaddar and Langlais (2018) demonstrate that lexical features are actually quite useful for NER. Clark et al. (2018) present a cross-view training for neural sequence models. BiLSTM-LAN obtains highly competitive results compared with various of top-performance models without training on external data. CCGBank. In CCG supertagging, the major challenge is a larger set of lexical tags |L| and supertag constraints over long distance dependencies. As shown in Table 9, BiLSTMLAN significantly outperforms both BiLSTMsoftmax and BiLSTM-CRF (p <0.01), showing the advantage of LAN. Xu et al. (2015) and Vaswani et al. (2016a) exp"
D19-1422,N16-1030,0,0.0836602,"denotes a character embedding lookup table. We adopt BiLSTM for character encoding. xci denotes the output of character-level encoding. A word is represented by concatenating its word embedding and its character representation: xi = [ew (wi ); xci ] where ew denotes a word embedding lookup table. 3.2 Sequence Representation Layer For sequence encoding, the input is a sentence x = {x1 , · · ·, xn }. Word representations are fed into a BiLSTM layer, yielding a sequence of for→ We implement BiLSTM-CRF and BiLSTMsoftmax baseline models with character-level features (Dos Santos and Zadrozny, 2014; Lample et al., 2016), which consist of a word representation layer, a sequence representation layer and a local softmax or CRF inference layer. 3.1 w1 , w2 , ..., wn , where cij denotes the jth character in the ith word, each cij is represented using Word Representation Layer ← ← w of backward hidden states {hw 1 , · · ·, hn }, respectively. Finally, the two hidden states are concatenated for a final representation → ← w w hw i = [hi ; hi ] w Hw = {hw 1 , · · ·, hn } 3.3 Following Dos Santos and Zadrozny (2014) and Lample et al. (2016), we use character information for POS tagging. Given a word sequence → w ward"
D19-1422,N16-1026,0,0.0357186,"Missing"
D19-1422,D16-1035,0,0.0259056,"s 5.0 and CCGBank without external training. In addition to accuracy and efficiency, BiLSTM-LAN is also more interpretable than BiLSTM-CRF thanks to visualizable label embeddings and label distributions. Our code and models are released at https://github.com/Nealcly/LAN. 2 Related Work Neural Attention. Attention has been shown useful in neural machine translation (Bahdanau et al., 2014), sentiment classification (Chen et al., 2017; Liu and Zhang, 2017), relation classification (Zhou et al., 2016), read comprehension (Hermann et al., 2015), sentence summarization (Rush et al., 2015), parsing (Li et al., 2016), question answering (Wang et al., 2018b) and text understanding (Kadlec et al., 2016). Self-attention network (SAN) (Vaswani et al., 2017) has been used for semantic role labeling (Strubell et al., 2018), text classification (Xu et al., 2018; Wu et al., 2018) and other tasks. Our work is similar to Vaswani et al. (2017) in the sense that we also build a hierarchical attentive neural network for sequence representation. The difference lies in that our main goal is to investigate the encoding of exponential label sequences, whereas their work focuses on encoding of a word sequence only. Label E"
D19-1422,E17-2091,1,0.851836,"ies than BiLSTM-CRF and BiLSTM-softmax with similar number of parameters. It gives highly competitive results compared with topperformance systems on WSJ, OntoNotes 5.0 and CCGBank without external training. In addition to accuracy and efficiency, BiLSTM-LAN is also more interpretable than BiLSTM-CRF thanks to visualizable label embeddings and label distributions. Our code and models are released at https://github.com/Nealcly/LAN. 2 Related Work Neural Attention. Attention has been shown useful in neural machine translation (Bahdanau et al., 2014), sentiment classification (Chen et al., 2017; Liu and Zhang, 2017), relation classification (Zhou et al., 2016), read comprehension (Hermann et al., 2015), sentence summarization (Rush et al., 2015), parsing (Li et al., 2016), question answering (Wang et al., 2018b) and text understanding (Kadlec et al., 2016). Self-attention network (SAN) (Vaswani et al., 2017) has been used for semantic role labeling (Strubell et al., 2018), text classification (Xu et al., 2018; Wu et al., 2018) and other tasks. Our work is similar to Vaswani et al. (2017) in the sense that we also build a hierarchical attentive neural network for sequence representation. The difference li"
D19-1422,P16-1101,0,0.474803,"RB 0.9 … … VB 0.2 NN 0.4 … … … … … … … … … … … … … … … … … … … … … … They can fish and also tomatoes here Figure 1: Visualization of hierarchically-refined Label Attention Network for POS tagging. The numbers indicate the label probability distribution for each word. Introduction Conditional random fields (CRF) (Lafferty et al., 2001) is a state-of-the-art model for statistical sequence labeling (Toutanova et al., 2003; Peng et al., 2004; Ratinov and Roth, 2009). Recently, CRF has been integrated with neural encoders as an output layer to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016). This, however, sees mixed results. For example, previous work (Reimers and Gurevych, 2017; Yang et al., 2018) has shown that BiLSTM-softmax gives better accuracies compared to BiLSTMCRF for part-of-speech (POS) tagging. In addition, the state-of-the-art neural Combinatory Categorial Grammar (CCG) supertaggers do not use CRF (Xu et al., 2015; Lewis et al., 2016). One possible reason is that the strong representation power of neural sentence encoders such as BiLSTMs allow models to capture implicit long-range label dependencies from input word sequences alone (Kiperwasser and Goldberg, 2016; D"
D19-1422,J93-2004,0,0.0674411,"LAN, respectively, where |L |is the total number of labels and n is the sequence length. Compared with BiLSTM-CRF, BiLSTMLAN can significantly increase the speed for sequence labeling, especially for CCG supertagging, where the sequence length can be much smaller than the total number of labels. 4.5 training 45 38,219 912,344 50 12,544 204,607 18 59,924 1,088,503 426 39,604 929,552 j where i is the word index, j is the index of labels for each word. 4.4 #l #s #t #l #s #t #l #s #t #l #s #t Dataset For English POS tagging, we use the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993), which has 45 POS tags. We adopt the same partitions used by previous work (Manning, 2011; Yang et al., 2018), selecting sections 0-18 as the training set, sections 19-21 as the development set and sections 22-24 as the test set. For multilingual POS tagging, we use treebanks from Universal Dependencies(UD) v2.2 (Silveira et al., 2014; Nivre et al., 2018) with the standard splits, choosing 8 resource-rich languages in our experiments. For NER, we use the OntoNotes 5.0 (Hovy et al., 2006; Pradhan et al., 2013). Following previous work,we adopt the official OntoNotes data split used for co-refe"
D19-1422,C04-1081,0,0.0631305,"ut NN VB CC NN RB RB PRP 0.9 MD 0.3 NN 0.8 CC 0.9 RB 0.9 NN 0.9 RB 0.9 … … VB 0.7 VB 0.2 … … … … … … … … … … … … … … … … … … … … … … PRP 0.9 MD 0.7 VB 0.5 CC 0.9 RB 0.9 NN 0.9 RB 0.9 … … VB 0.2 NN 0.4 … … … … … … … … … … … … … … … … … … … … … … They can fish and also tomatoes here Figure 1: Visualization of hierarchically-refined Label Attention Network for POS tagging. The numbers indicate the label probability distribution for each word. Introduction Conditional random fields (CRF) (Lafferty et al., 2001) is a state-of-the-art model for statistical sequence labeling (Toutanova et al., 2003; Peng et al., 2004; Ratinov and Roth, 2009). Recently, CRF has been integrated with neural encoders as an output layer to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016). This, however, sees mixed results. For example, previous work (Reimers and Gurevych, 2017; Yang et al., 2018) has shown that BiLSTM-softmax gives better accuracies compared to BiLSTMCRF for part-of-speech (POS) tagging. In addition, the state-of-the-art neural Combinatory Categorial Grammar (CCG) supertaggers do not use CRF (Xu et al., 2015; Lewis et al., 2016). One possible reason is that the strong representation pow"
D19-1422,D14-1162,0,0.080778,"Missing"
D19-1422,P16-2067,0,0.0547277,"Missing"
D19-1422,W13-3516,0,0.0473997,"Missing"
D19-1422,W12-4501,0,0.0268423,"tions used by previous work (Manning, 2011; Yang et al., 2018), selecting sections 0-18 as the training set, sections 19-21 as the development set and sections 22-24 as the test set. For multilingual POS tagging, we use treebanks from Universal Dependencies(UD) v2.2 (Silveira et al., 2014; Nivre et al., 2018) with the standard splits, choosing 8 resource-rich languages in our experiments. For NER, we use the OntoNotes 5.0 (Hovy et al., 2006; Pradhan et al., 2013). Following previous work,we adopt the official OntoNotes data split used for co-reference resolution in the CoNLL-2012 shared task (Pradhan et al., 2012). We train our CCG supertagging 4119 Model BiLSTM-CRF (POS) BiLSTM-LAN (POS) BiLSTM-CRF (CCG) BiLSTM-LAN (CCG) train (s) 181.32 128.75 884.67 369.98 test (st/s) 781.90 805.32 599.18 713.70 Model BiLSTM-CRF† BiLSTM-CRF‡ BiLSTM-softmax† BiLSTM-LAN Table 4: Comparison of the training time for one iteration and decoding speed. st indicates sentences model on CCGBank (Hockenmaier and Steedman, 2007). Sections 2-21 are used for training, section 0 for development and section 23 as the test set. The performance is calculated on the 425 most frequent labels. Table 1 shows the numbers of sentences, tok"
D19-1422,W09-1119,0,0.0991963,"B PRP 0.9 MD 0.3 NN 0.8 CC 0.9 RB 0.9 NN 0.9 RB 0.9 … … VB 0.7 VB 0.2 … … … … … … … … … … … … … … … … … … … … … … PRP 0.9 MD 0.7 VB 0.5 CC 0.9 RB 0.9 NN 0.9 RB 0.9 … … VB 0.2 NN 0.4 … … … … … … … … … … … … … … … … … … … … … … They can fish and also tomatoes here Figure 1: Visualization of hierarchically-refined Label Attention Network for POS tagging. The numbers indicate the label probability distribution for each word. Introduction Conditional random fields (CRF) (Lafferty et al., 2001) is a state-of-the-art model for statistical sequence labeling (Toutanova et al., 2003; Peng et al., 2004; Ratinov and Roth, 2009). Recently, CRF has been integrated with neural encoders as an output layer to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016). This, however, sees mixed results. For example, previous work (Reimers and Gurevych, 2017; Yang et al., 2018) has shown that BiLSTM-softmax gives better accuracies compared to BiLSTMCRF for part-of-speech (POS) tagging. In addition, the state-of-the-art neural Combinatory Categorial Grammar (CCG) supertaggers do not use CRF (Xu et al., 2015; Lewis et al., 2016). One possible reason is that the strong representation power of neural sentence enc"
D19-1422,D15-1044,0,0.0197094,"ance systems on WSJ, OntoNotes 5.0 and CCGBank without external training. In addition to accuracy and efficiency, BiLSTM-LAN is also more interpretable than BiLSTM-CRF thanks to visualizable label embeddings and label distributions. Our code and models are released at https://github.com/Nealcly/LAN. 2 Related Work Neural Attention. Attention has been shown useful in neural machine translation (Bahdanau et al., 2014), sentiment classification (Chen et al., 2017; Liu and Zhang, 2017), relation classification (Zhou et al., 2016), read comprehension (Hermann et al., 2015), sentence summarization (Rush et al., 2015), parsing (Li et al., 2016), question answering (Wang et al., 2018b) and text understanding (Kadlec et al., 2016). Self-attention network (SAN) (Vaswani et al., 2017) has been used for semantic role labeling (Strubell et al., 2018), text classification (Xu et al., 2018; Wu et al., 2018) and other tasks. Our work is similar to Vaswani et al. (2017) in the sense that we also build a hierarchical attentive neural network for sequence representation. The difference lies in that our main goal is to investigate the encoding of exponential label sequences, whereas their work focuses on encoding of a"
D19-1422,W17-2630,0,0.0258941,"y higher than BiLSTM-CRF. This demonstrates the effect of label embeddings, which allows more structured knowledge to be learned in modeling. Final Results 1 Note that our results are different from Table 2 of Yasunaga et al. (2018), since they reported results on UD v1.2 4121 (a) 5 iterations (b) 15 iterations (c) 38 iterations Figure 4: t-SNE plot of label embeddings after different numbers of training iterations. OntoNotes 5.0. Durrett and Klein (2014) propose a joint model for coreference resolution, entity linking and NER. Chiu and Nichols (2016) use a BiLSTM with CNN character encoding. Shen et al. (2017) introduce active learning to get better performance. Strubell et al. (2017) present an iterated dilated convolutions, which is a faster alternative to BiLSTM. Ghaddar and Langlais (2018) demonstrate that lexical features are actually quite useful for NER. Clark et al. (2018) present a cross-view training for neural sequence models. BiLSTM-LAN obtains highly competitive results compared with various of top-performance models without training on external data. CCGBank. In CCG supertagging, the major challenge is a larger set of lexical tags |L| and supertag constraints over long distance depend"
D19-1422,silveira-etal-2014-gold,0,0.0643431,"Missing"
D19-1422,P16-2038,0,0.0195523,"useful for NER. Clark et al. (2018) present a cross-view training for neural sequence models. BiLSTM-LAN obtains highly competitive results compared with various of top-performance models without training on external data. CCGBank. In CCG supertagging, the major challenge is a larger set of lexical tags |L| and supertag constraints over long distance dependencies. As shown in Table 9, BiLSTMLAN significantly outperforms both BiLSTMsoftmax and BiLSTM-CRF (p <0.01), showing the advantage of LAN. Xu et al. (2015) and Vaswani et al. (2016a) explore BiRNN-softmax and BiLSTM-softmax, respectively. Søgaard and Goldberg (2016) present a multi-task learning architecture with BiRNN to improve the performance. Lewis et al. (2016) train BiLSTM-softmax using tri-training. Vaswani et al. (2016b) combine a LSTM language model and BiLSTM over the supertags. Tu and Gimpel (2019) introduce the inference network (Tu and Gimpel, 2018) in CCG supertagging to speed up the training and decoding for BiLSTM-CRF. Compared with these methods, BiLSTM-LAN obtains new state-of-theart results on CCGBank, matching the tri-training performance of Lewis et al. (2016), without training on external data. Model Durrett and Klein (2014) Chiu an"
D19-1422,D18-1548,0,0.0536638,"Missing"
D19-1422,D17-1283,0,0.068371,"gs, which allows more structured knowledge to be learned in modeling. Final Results 1 Note that our results are different from Table 2 of Yasunaga et al. (2018), since they reported results on UD v1.2 4121 (a) 5 iterations (b) 15 iterations (c) 38 iterations Figure 4: t-SNE plot of label embeddings after different numbers of training iterations. OntoNotes 5.0. Durrett and Klein (2014) propose a joint model for coreference resolution, entity linking and NER. Chiu and Nichols (2016) use a BiLSTM with CNN character encoding. Shen et al. (2017) introduce active learning to get better performance. Strubell et al. (2017) present an iterated dilated convolutions, which is a faster alternative to BiLSTM. Ghaddar and Langlais (2018) demonstrate that lexical features are actually quite useful for NER. Clark et al. (2018) present a cross-view training for neural sequence models. BiLSTM-LAN obtains highly competitive results compared with various of top-performance models without training on external data. CCGBank. In CCG supertagging, the major challenge is a larger set of lexical tags |L| and supertag constraints over long distance dependencies. As shown in Table 9, BiLSTMLAN significantly outperforms both BiLSTM"
D19-1422,C18-1011,1,0.841693,"xed results. For example, previous work (Reimers and Gurevych, 2017; Yang et al., 2018) has shown that BiLSTM-softmax gives better accuracies compared to BiLSTMCRF for part-of-speech (POS) tagging. In addition, the state-of-the-art neural Combinatory Categorial Grammar (CCG) supertaggers do not use CRF (Xu et al., 2015; Lewis et al., 2016). One possible reason is that the strong representation power of neural sentence encoders such as BiLSTMs allow models to capture implicit long-range label dependencies from input word sequences alone (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016; Teng and Zhang, 2018), thereby allowing the output layer to make local predictions. In contrast, though explicitly capturing output label dependencies, CRF can be limited by its Markov assumptions, particularly when being used on top of neural encoders. In addition, CRF can be computationally expensive when the number of labels is large, due to the use of Viterbi decoding. One interesting research question is whether there is neural alternative to CRF for neural sequence labeling, which is both faster and more accurate. To this question, we investigate a neural network model for output label sequences. In particul"
D19-1422,N03-1033,0,0.29794,"st Layer First Layer Input NN VB CC NN RB RB PRP 0.9 MD 0.3 NN 0.8 CC 0.9 RB 0.9 NN 0.9 RB 0.9 … … VB 0.7 VB 0.2 … … … … … … … … … … … … … … … … … … … … … … PRP 0.9 MD 0.7 VB 0.5 CC 0.9 RB 0.9 NN 0.9 RB 0.9 … … VB 0.2 NN 0.4 … … … … … … … … … … … … … … … … … … … … … … They can fish and also tomatoes here Figure 1: Visualization of hierarchically-refined Label Attention Network for POS tagging. The numbers indicate the label probability distribution for each word. Introduction Conditional random fields (CRF) (Lafferty et al., 2001) is a state-of-the-art model for statistical sequence labeling (Toutanova et al., 2003; Peng et al., 2004; Ratinov and Roth, 2009). Recently, CRF has been integrated with neural encoders as an output layer to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016). This, however, sees mixed results. For example, previous work (Reimers and Gurevych, 2017; Yang et al., 2018) has shown that BiLSTM-softmax gives better accuracies compared to BiLSTMCRF for part-of-speech (POS) tagging. In addition, the state-of-the-art neural Combinatory Categorial Grammar (CCG) supertaggers do not use CRF (Xu et al., 2015; Lewis et al., 2016). One possible reason is that the strong"
D19-1422,N19-1335,0,0.0134424,"he major challenge is a larger set of lexical tags |L| and supertag constraints over long distance dependencies. As shown in Table 9, BiLSTMLAN significantly outperforms both BiLSTMsoftmax and BiLSTM-CRF (p <0.01), showing the advantage of LAN. Xu et al. (2015) and Vaswani et al. (2016a) explore BiRNN-softmax and BiLSTM-softmax, respectively. Søgaard and Goldberg (2016) present a multi-task learning architecture with BiRNN to improve the performance. Lewis et al. (2016) train BiLSTM-softmax using tri-training. Vaswani et al. (2016b) combine a LSTM language model and BiLSTM over the supertags. Tu and Gimpel (2019) introduce the inference network (Tu and Gimpel, 2018) in CCG supertagging to speed up the training and decoding for BiLSTM-CRF. Compared with these methods, BiLSTM-LAN obtains new state-of-theart results on CCGBank, matching the tri-training performance of Lewis et al. (2016), without training on external data. Model Durrett and Klein (2014) Chiu and Nichols (2016) Shen et al. (2017) Strubell et al. (2017) Ghaddar and Langlais (2018) Clark et al. (2018)∗ BiLSTM-softmax (Strubell et al., 2017) BiLSTM-CRF (Strubell et al., 2017) BiLSTM-LAN F1 84.04 86.28 86.52 86.84 87.95 88.81 83.76 86.99 88.1"
D19-1422,N16-1027,0,0.429241,"g of a word sequence only. Label Embeddings. Label embedding was first used in the field of computer vision for facilitating zero-shot learning (Palatucci et al., 2009; Socher et al., 2013; Zhang and Saligrama, 2016). The basic idea is to improve the performance of classifying previously unseen class instances by learning output label knowledge. In NLP, label embeddings have been exploited for better text classification (Tang et al., 2015; Nam et al., 2016; Wang et al., 2018a). However, relatively little work has been done investigating label embeddings for sequence labeling. One exception is Vaswani et al. (2016b), who use supertag embeddings in the output layer of a CCG supertagger, through a combination of local classification model rescored using a supertag language model. In contrast, we model deep label interactions using a dynamically refined 4116 Figure 2: Architecture of hierarchically-refined label attention network. sequence representation network. To our knowledge, we are the first to investigate a hierarchical attention network over a label space. Neural CRF. There has been methods that aim to speed up neural CRF (Tu and Gimpel, 2018), and to solve the Markov constraint of neural CRF. In"
D19-1422,P18-1216,0,0.0253229,"ining. In addition to accuracy and efficiency, BiLSTM-LAN is also more interpretable than BiLSTM-CRF thanks to visualizable label embeddings and label distributions. Our code and models are released at https://github.com/Nealcly/LAN. 2 Related Work Neural Attention. Attention has been shown useful in neural machine translation (Bahdanau et al., 2014), sentiment classification (Chen et al., 2017; Liu and Zhang, 2017), relation classification (Zhou et al., 2016), read comprehension (Hermann et al., 2015), sentence summarization (Rush et al., 2015), parsing (Li et al., 2016), question answering (Wang et al., 2018b) and text understanding (Kadlec et al., 2016). Self-attention network (SAN) (Vaswani et al., 2017) has been used for semantic role labeling (Strubell et al., 2018), text classification (Xu et al., 2018; Wu et al., 2018) and other tasks. Our work is similar to Vaswani et al. (2017) in the sense that we also build a hierarchical attentive neural network for sequence representation. The difference lies in that our main goal is to investigate the encoding of exponential label sequences, whereas their work focuses on encoding of a word sequence only. Label Embeddings. Label embedding was first us"
D19-1422,C18-1215,0,0.0195486,"ining. In addition to accuracy and efficiency, BiLSTM-LAN is also more interpretable than BiLSTM-CRF thanks to visualizable label embeddings and label distributions. Our code and models are released at https://github.com/Nealcly/LAN. 2 Related Work Neural Attention. Attention has been shown useful in neural machine translation (Bahdanau et al., 2014), sentiment classification (Chen et al., 2017; Liu and Zhang, 2017), relation classification (Zhou et al., 2016), read comprehension (Hermann et al., 2015), sentence summarization (Rush et al., 2015), parsing (Li et al., 2016), question answering (Wang et al., 2018b) and text understanding (Kadlec et al., 2016). Self-attention network (SAN) (Vaswani et al., 2017) has been used for semantic role labeling (Strubell et al., 2018), text classification (Xu et al., 2018; Wu et al., 2018) and other tasks. Our work is similar to Vaswani et al. (2017) in the sense that we also build a hierarchical attentive neural network for sequence representation. The difference lies in that our main goal is to investigate the encoding of exponential label sequences, whereas their work focuses on encoding of a word sequence only. Label Embeddings. Label embedding was first us"
D19-1422,N15-1030,0,0.0277942,"el distributions are then fed as the inputs to the next layer, so that long-range label dependencies can be considered. In the second layer, the network can learn to assign a noun tag to fish3 by taking into account the highly confident tagging information of tomatoes6 (NN), resulting in the pattern “can2 (VB) fish3 (NN)”. As shown in Figure 2, our model consists of stacked attentive BiLSTM layers, each of which takes a sequence of vectors as input and yields a sequence of hidden state vectors together with a sequence of label distributions. The model performs attention over label embeddings (Wang et al., 2015; Zhang et al., 2018a) for deriving a marginal label distributions, which are in turn used to calculate a weighted sum of label embeddings. Finally, the resulting packed label vector is used together with input word vectors as the hidden state vector for the current layer. Thus our model is named label attention network (LAN). For sequence labeling, the input to the whole model is a sentence and the output is the label distributions of each word in the final layer. BiLSTM-LAN can be viewed as a form of multi-layered BiLSTM-softmax sequence labeler. In particular, a single-layer BiLSTM-LAN is i"
D19-1422,D18-1279,0,0.361186,". For BiLSTM-CRF, previous work has shown that one BiLSTM layer is the most effecitve for POS tagging (Ma and Hovy, 2016; Yang et al., 2018). Table 2 compares different numbers of BiLSTM layers and hidden sizes. Max 97.49 97.51 97.51 97.65 Table 5: Result for POS tagging on WSJ. † Yang et al. (2018) and ‡ Yasunaga et al. (2018) are baseline models re-implemented in NCRF++ (Yang and Zhang, 2018). Our results are same as Table 6 of Yang et al. (2018). Model Plank et al. (2016) Huang et al. (2015) Ma and Hovy (2016) Liu et al. (2017) Yang et al. (2018) Zhang et al. (2018c) Yasunaga et al. (2018) Xin et al. (2018) Transformer-softmax (Guo et al., 2019) BiLSTM-softmax (Yang et al., 2018) BiLSTM-CRF (Yang et al., 2018) BiLSTM-LAN Settings Hyper-Parameters. We use 100-dimensional GloVe (Pennington et al., 2014) word embeddings for English POS tagging (WSJ, UD v2.2 EN) and name entity recognition, 300-dimensional Glove word embeddings for CCG supertagging and 64dimensional Polyglot (Al-Rfou et al., 2013) word embeddings for multilingual POS tagging. The detail values of the hyper-parameters for all experiments are summarized in Appendix A. Evaluation. F-1 score are used for NER. Other tasks are evaluated b"
D19-1422,P18-2123,0,0.0186367,"ps://github.com/Nealcly/LAN. 2 Related Work Neural Attention. Attention has been shown useful in neural machine translation (Bahdanau et al., 2014), sentiment classification (Chen et al., 2017; Liu and Zhang, 2017), relation classification (Zhou et al., 2016), read comprehension (Hermann et al., 2015), sentence summarization (Rush et al., 2015), parsing (Li et al., 2016), question answering (Wang et al., 2018b) and text understanding (Kadlec et al., 2016). Self-attention network (SAN) (Vaswani et al., 2017) has been used for semantic role labeling (Strubell et al., 2018), text classification (Xu et al., 2018; Wu et al., 2018) and other tasks. Our work is similar to Vaswani et al. (2017) in the sense that we also build a hierarchical attentive neural network for sequence representation. The difference lies in that our main goal is to investigate the encoding of exponential label sequences, whereas their work focuses on encoding of a word sequence only. Label Embeddings. Label embedding was first used in the field of computer vision for facilitating zero-shot learning (Palatucci et al., 2009; Socher et al., 2013; Zhang and Saligrama, 2016). The basic idea is to improve the performance of classifyin"
D19-1422,P15-2041,0,0.157709,"e-of-the-art model for statistical sequence labeling (Toutanova et al., 2003; Peng et al., 2004; Ratinov and Roth, 2009). Recently, CRF has been integrated with neural encoders as an output layer to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016). This, however, sees mixed results. For example, previous work (Reimers and Gurevych, 2017; Yang et al., 2018) has shown that BiLSTM-softmax gives better accuracies compared to BiLSTMCRF for part-of-speech (POS) tagging. In addition, the state-of-the-art neural Combinatory Categorial Grammar (CCG) supertaggers do not use CRF (Xu et al., 2015; Lewis et al., 2016). One possible reason is that the strong representation power of neural sentence encoders such as BiLSTMs allow models to capture implicit long-range label dependencies from input word sequences alone (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016; Teng and Zhang, 2018), thereby allowing the output layer to make local predictions. In contrast, though explicitly capturing output label dependencies, CRF can be limited by its Markov assumptions, particularly when being used on top of neural encoders. In addition, CRF can be computationally expensive when the number"
D19-1422,C18-1327,1,0.903884,"re 1: Visualization of hierarchically-refined Label Attention Network for POS tagging. The numbers indicate the label probability distribution for each word. Introduction Conditional random fields (CRF) (Lafferty et al., 2001) is a state-of-the-art model for statistical sequence labeling (Toutanova et al., 2003; Peng et al., 2004; Ratinov and Roth, 2009). Recently, CRF has been integrated with neural encoders as an output layer to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016). This, however, sees mixed results. For example, previous work (Reimers and Gurevych, 2017; Yang et al., 2018) has shown that BiLSTM-softmax gives better accuracies compared to BiLSTMCRF for part-of-speech (POS) tagging. In addition, the state-of-the-art neural Combinatory Categorial Grammar (CCG) supertaggers do not use CRF (Xu et al., 2015; Lewis et al., 2016). One possible reason is that the strong representation power of neural sentence encoders such as BiLSTMs allow models to capture implicit long-range label dependencies from input word sequences alone (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016; Teng and Zhang, 2018), thereby allowing the output layer to make local predictions. In"
D19-1422,P18-4013,1,0.856891,"eved when the label embedding size increases from 200 to 400 in our model, but the accuracy does not further increase when the size increases beyond 400. We fix the label embedding size to 400 for our model. Number of Layers. For BiLSTM-CRF, previous work has shown that one BiLSTM layer is the most effecitve for POS tagging (Ma and Hovy, 2016; Yang et al., 2018). Table 2 compares different numbers of BiLSTM layers and hidden sizes. Max 97.49 97.51 97.51 97.65 Table 5: Result for POS tagging on WSJ. † Yang et al. (2018) and ‡ Yasunaga et al. (2018) are baseline models re-implemented in NCRF++ (Yang and Zhang, 2018). Our results are same as Table 6 of Yang et al. (2018). Model Plank et al. (2016) Huang et al. (2015) Ma and Hovy (2016) Liu et al. (2017) Yang et al. (2018) Zhang et al. (2018c) Yasunaga et al. (2018) Xin et al. (2018) Transformer-softmax (Guo et al., 2019) BiLSTM-softmax (Yang et al., 2018) BiLSTM-CRF (Yang et al., 2018) BiLSTM-LAN Settings Hyper-Parameters. We use 100-dimensional GloVe (Pennington et al., 2014) word embeddings for English POS tagging (WSJ, UD v2.2 EN) and name entity recognition, 300-dimensional Glove word embeddings for CCG supertagging and 64dimensional Polyglot (Al-Rfou"
D19-1422,N18-1089,0,0.249686,"e effect of the label embedding size. Notable improvement can be achieved when the label embedding size increases from 200 to 400 in our model, but the accuracy does not further increase when the size increases beyond 400. We fix the label embedding size to 400 for our model. Number of Layers. For BiLSTM-CRF, previous work has shown that one BiLSTM layer is the most effecitve for POS tagging (Ma and Hovy, 2016; Yang et al., 2018). Table 2 compares different numbers of BiLSTM layers and hidden sizes. Max 97.49 97.51 97.51 97.65 Table 5: Result for POS tagging on WSJ. † Yang et al. (2018) and ‡ Yasunaga et al. (2018) are baseline models re-implemented in NCRF++ (Yang and Zhang, 2018). Our results are same as Table 6 of Yang et al. (2018). Model Plank et al. (2016) Huang et al. (2015) Ma and Hovy (2016) Liu et al. (2017) Yang et al. (2018) Zhang et al. (2018c) Yasunaga et al. (2018) Xin et al. (2018) Transformer-softmax (Guo et al., 2019) BiLSTM-softmax (Yang et al., 2018) BiLSTM-CRF (Yang et al., 2018) BiLSTM-LAN Settings Hyper-Parameters. We use 100-dimensional GloVe (Pennington et al., 2014) word embeddings for English POS tagging (WSJ, UD v2.2 EN) and name entity recognition, 300-dimensional Glove word"
D19-1422,D18-1484,0,0.285547,"e then fed as the inputs to the next layer, so that long-range label dependencies can be considered. In the second layer, the network can learn to assign a noun tag to fish3 by taking into account the highly confident tagging information of tomatoes6 (NN), resulting in the pattern “can2 (VB) fish3 (NN)”. As shown in Figure 2, our model consists of stacked attentive BiLSTM layers, each of which takes a sequence of vectors as input and yields a sequence of hidden state vectors together with a sequence of label distributions. The model performs attention over label embeddings (Wang et al., 2015; Zhang et al., 2018a) for deriving a marginal label distributions, which are in turn used to calculate a weighted sum of label embeddings. Finally, the resulting packed label vector is used together with input word vectors as the hidden state vector for the current layer. Thus our model is named label attention network (LAN). For sequence labeling, the input to the whole model is a sentence and the output is the label distributions of each word in the final layer. BiLSTM-LAN can be viewed as a form of multi-layered BiLSTM-softmax sequence labeler. In particular, a single-layer BiLSTM-LAN is identical to a single"
D19-1422,P18-1030,1,0.897829,"e then fed as the inputs to the next layer, so that long-range label dependencies can be considered. In the second layer, the network can learn to assign a noun tag to fish3 by taking into account the highly confident tagging information of tomatoes6 (NN), resulting in the pattern “can2 (VB) fish3 (NN)”. As shown in Figure 2, our model consists of stacked attentive BiLSTM layers, each of which takes a sequence of vectors as input and yields a sequence of hidden state vectors together with a sequence of label distributions. The model performs attention over label embeddings (Wang et al., 2015; Zhang et al., 2018a) for deriving a marginal label distributions, which are in turn used to calculate a weighted sum of label embeddings. Finally, the resulting packed label vector is used together with input word vectors as the hidden state vector for the current layer. Thus our model is named label attention network (LAN). For sequence labeling, the input to the whole model is a sentence and the output is the label distributions of each word in the final layer. BiLSTM-LAN can be viewed as a form of multi-layered BiLSTM-softmax sequence labeler. In particular, a single-layer BiLSTM-LAN is identical to a single"
D19-1422,P15-1109,0,0.039518,"C 0.9 RB 0.9 NN 0.9 RB 0.9 … … VB 0.2 NN 0.4 … … … … … … … … … … … … … … … … … … … … … … They can fish and also tomatoes here Figure 1: Visualization of hierarchically-refined Label Attention Network for POS tagging. The numbers indicate the label probability distribution for each word. Introduction Conditional random fields (CRF) (Lafferty et al., 2001) is a state-of-the-art model for statistical sequence labeling (Toutanova et al., 2003; Peng et al., 2004; Ratinov and Roth, 2009). Recently, CRF has been integrated with neural encoders as an output layer to capture label transition patterns (Zhou and Xu, 2015; Ma and Hovy, 2016). This, however, sees mixed results. For example, previous work (Reimers and Gurevych, 2017; Yang et al., 2018) has shown that BiLSTM-softmax gives better accuracies compared to BiLSTMCRF for part-of-speech (POS) tagging. In addition, the state-of-the-art neural Combinatory Categorial Grammar (CCG) supertaggers do not use CRF (Xu et al., 2015; Lewis et al., 2016). One possible reason is that the strong representation power of neural sentence encoders such as BiLSTMs allow models to capture implicit long-range label dependencies from input word sequences alone (Kiperwasser a"
D19-1422,P16-2034,0,0.0260707,"ilar number of parameters. It gives highly competitive results compared with topperformance systems on WSJ, OntoNotes 5.0 and CCGBank without external training. In addition to accuracy and efficiency, BiLSTM-LAN is also more interpretable than BiLSTM-CRF thanks to visualizable label embeddings and label distributions. Our code and models are released at https://github.com/Nealcly/LAN. 2 Related Work Neural Attention. Attention has been shown useful in neural machine translation (Bahdanau et al., 2014), sentiment classification (Chen et al., 2017; Liu and Zhang, 2017), relation classification (Zhou et al., 2016), read comprehension (Hermann et al., 2015), sentence summarization (Rush et al., 2015), parsing (Li et al., 2016), question answering (Wang et al., 2018b) and text understanding (Kadlec et al., 2016). Self-attention network (SAN) (Vaswani et al., 2017) has been used for semantic role labeling (Strubell et al., 2018), text classification (Xu et al., 2018; Wu et al., 2018) and other tasks. Our work is similar to Vaswani et al. (2017) in the sense that we also build a hierarchical attentive neural network for sequence representation. The difference lies in that our main goal is to investigate th"
D19-1422,D18-1408,0,\N,Missing
E12-1075,J05-3002,0,0.0596554,"her when the N -gram model is incorporated. Table 6 compares our system with Z&C using lexical category pruning (β = 0.0001) and a 5s timeout for fair comparison. The results are similar to Table 5: our large-margin training systems outperforms the baseline by 1.5 BLEU points, and adding the N -gram model gave a further 1.4 point improvement. The scores could be significantly increased by using a larger timeout, as shown in our earlier development experiments. 7 Related Work There is a recent line of research on text-totext generation, which studies the linearization of dependency structures (Barzilay and McKeown, 2005; Filippova and Strube, 2007; Filippova and Strube, 2009; Bohnet et al., 2010; Guo et al., 2011). Unlike our system, and Wan et al. (2009), input dependencies provide additional information to these systems. Although the search space can be constrained by the assumption of projectivity, permutation of modifiers of the same head word makes exact inference for tree linearization intractable. The above systems typically apply approximate inference, such as beam-search. While syntax-based features are commonly used by these systems for linearization, Filippova and Strube (2009) apply a trigram mod"
E12-1075,C10-1009,1,0.500779,"Missing"
E12-1075,C10-1012,0,0.122116,"ng lexical category pruning (β = 0.0001) and a 5s timeout for fair comparison. The results are similar to Table 5: our large-margin training systems outperforms the baseline by 1.5 BLEU points, and adding the N -gram model gave a further 1.4 point improvement. The scores could be significantly increased by using a larger timeout, as shown in our earlier development experiments. 7 Related Work There is a recent line of research on text-totext generation, which studies the linearization of dependency structures (Barzilay and McKeown, 2005; Filippova and Strube, 2007; Filippova and Strube, 2009; Bohnet et al., 2010; Guo et al., 2011). Unlike our system, and Wan et al. (2009), input dependencies provide additional information to these systems. Although the search space can be constrained by the assumption of projectivity, permutation of modifiers of the same head word makes exact inference for tree linearization intractable. The above systems typically apply approximate inference, such as beam-search. While syntax-based features are commonly used by these systems for linearization, Filippova and Strube (2009) apply a trigram model to control local fluency within constituents. A dependencybased N-gram mod"
E12-1075,J93-2003,0,0.0273705,"+ Φ(e+ ) − Φ(e− ) 2 k Φ(e+ ) − Φ(e− ) k In this update, the global feature vectors Φ(e+ ) and Φ(e− ) are used. Unlike Z&C, the scores of sub-edges of e+ and e− are also udpated, so that the sub-edges of e− are less prioritized than those of e+ . We show empirically that this training algorithm significantly outperforms the perceptron training of the baseline system in Section 5. An advantage of our new training algorithm is that it enables the accommodation of a separately trained N -gram model into the system. 4 Incorporating an N-gram language model Since the seminal work of the IBM models (Brown et al., 1993), N -gram language models es ∈e Here gδ (e) = α · gδfour (e) + β · gδtri (e) + γ · gδbi (e) is the sum of log-probabilities of the new N grams resulting from the construction of e. For leaf edges and unary-branching edges, no new N grams result from their construction (i.e. gδ = 0). For a binary-branching edge, new N -grams result from the surface-string concatenation of its subedges. The sum of log-probabilities of the new fourgrams, trigrams and bigrams contribute to gδ with weights α, β and γ, respectively. For training, there are at least three methods to tune α, β, γ and θ. One simple met"
E12-1075,J98-2004,0,0.0347926,"D(new, e′ ) end if end for for e′ ∈ new do A DD(a, e′ ) end for A DD(c, e) end while decoding finishes. Otherwise it is extended with unary rules, and combined with existing edges in the chart using binary rules to produce new edges. The resulting edges are scored and put onto the agenda, while the original edge is put onto the chart. The process repeats until a goal edge is found, or a timeout limit is reached. In the latter case, a default output is produced using existing edges in the chart. Pseudocode for the decoder is shown as Algorithm 1. Again it is reminiscent of a best-first parser (Caraballo and Charniak, 1998) in the use of an agenda and a chart, but is fundamentally different due to the fact that there is no input order. 2.3 Statistical model and feature templates The baseline system uses a linear model to score hypotheses. For an edge e, its score is defined as: f (e) = Φ(e) · θ, where Φ(e) represents the feature vector of e and θ is the parameter vector of the model. During decoding, feature vectors are computed incrementally. When an edge is constructed, its score is computed from the scores of its sub-edges and the incrementally added structure: f (e) = Φ(e) · θ   X  Φ(es ) + φ(e) · θ = es"
E12-1075,J07-2003,0,0.382627,"ifficult problem. Finding the best permutation for a set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the traveling salesman problem. In practice, exploring the whole search space of permutations is often prevented by adding constraints. Stephen Clark University of Cambridge Computer Laboratory sc609@cam.ac.uk In phrase-based machine translation (Koehn et al., 2003; Koehn et al., 2007), a distortion limit is used to constrain the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomial time inference is feasible. In fluency improvement (Blackwood et al., 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) uses a dependency grammar to solve word ordering, and Zhang and Clark (2011) uses CCG (Steedman, 2000) for word ordering and word choice. The use of syntax models makes th"
E12-1075,J07-4004,1,0.915656,"t of input words, the baseline system builds a CCG derivation by choosing and ordering words from the input set. The scoring model is trained using CCGBank (Hockenmaier and Steedman, 2007), and best-first decoding is applied. We apply the same decoding framework in this paper, but apply an improved training process, and incorporate an N -gram language model into the syntax model. In this section, we describe and discuss the baseline statistical model and decoding framework, motivating our extensions. 2.1 Combinatory Categorial Grammar CCG , and parsing with CCG , has been described elsewhere (Clark and Curran, 2007; Hockenmaier and Steedman, 2002); here we provide only a short description. CCG (Steedman, 2000) is a lexicalized grammar formalism, which associates each word in a sentence with a lexical category. There is a small number of basic lexical categories, such as noun (N), noun phrase (NP), and prepositional phrase (PP). Complex lexical categories are formed recursively from basic categories and slashes, which indicate the directions of arguments. The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory"
E12-1075,P07-1041,0,0.123404,"s incorporated. Table 6 compares our system with Z&C using lexical category pruning (β = 0.0001) and a 5s timeout for fair comparison. The results are similar to Table 5: our large-margin training systems outperforms the baseline by 1.5 BLEU points, and adding the N -gram model gave a further 1.4 point improvement. The scores could be significantly increased by using a larger timeout, as shown in our earlier development experiments. 7 Related Work There is a recent line of research on text-totext generation, which studies the linearization of dependency structures (Barzilay and McKeown, 2005; Filippova and Strube, 2007; Filippova and Strube, 2009; Bohnet et al., 2010; Guo et al., 2011). Unlike our system, and Wan et al. (2009), input dependencies provide additional information to these systems. Although the search space can be constrained by the assumption of projectivity, permutation of modifiers of the same head word makes exact inference for tree linearization intractable. The above systems typically apply approximate inference, such as beam-search. While syntax-based features are commonly used by these systems for linearization, Filippova and Strube (2009) apply a trigram model to control local fluency"
E12-1075,N09-2057,0,0.0459596,"ares our system with Z&C using lexical category pruning (β = 0.0001) and a 5s timeout for fair comparison. The results are similar to Table 5: our large-margin training systems outperforms the baseline by 1.5 BLEU points, and adding the N -gram model gave a further 1.4 point improvement. The scores could be significantly increased by using a larger timeout, as shown in our earlier development experiments. 7 Related Work There is a recent line of research on text-totext generation, which studies the linearization of dependency structures (Barzilay and McKeown, 2005; Filippova and Strube, 2007; Filippova and Strube, 2009; Bohnet et al., 2010; Guo et al., 2011). Unlike our system, and Wan et al. (2009), input dependencies provide additional information to these systems. Although the search space can be constrained by the assumption of projectivity, permutation of modifiers of the same head word makes exact inference for tree linearization intractable. The above systems typically apply approximate inference, such as beam-search. While syntax-based features are commonly used by these systems for linearization, Filippova and Strube (2009) apply a trigram model to control local fluency within constituents. A depen"
E12-1075,N10-1115,0,0.0219988,"score of an edge e becomes: the perspective of correctness, it is unnecessary to find a margin between the sub-edges of e+ and those of e− , since both are gold-standard edges. However, since the score of an edge not only represents its correctness, but also affects its priority on the agenda, promoting the sub-edge of e+ can lead to “easier” edges being constructed before “harder” ones (i.e. those that are less likely to be correct), and therefore improve the output accuracy. This perspective has been observed by other works of learning-guided-search (Shen et al., 2007; Shen and Joshi, 2008; Goldberg and Elhadad, 2010). Intuitively, the score difference between easy gold-standard and harder gold-standard edges should not be as great as the difference between gold-standard and non-goldstandard edges. The perceptron update cannot provide such control of separation, because the amount of update is fixed to 1. As described earlier, we treat parameter update as finding a separation between correct and incorrect edges, in which the global feature vectors Φ, rather than φ, are considered. Given a positive example e+ and a negative example e− , we make a minimum update so that the score of e+ is higher than that of"
E12-1075,W11-2833,0,0.102068,"Missing"
E12-1075,P02-1043,0,0.0497799,"seline system builds a CCG derivation by choosing and ordering words from the input set. The scoring model is trained using CCGBank (Hockenmaier and Steedman, 2007), and best-first decoding is applied. We apply the same decoding framework in this paper, but apply an improved training process, and incorporate an N -gram language model into the syntax model. In this section, we describe and discuss the baseline statistical model and decoding framework, motivating our extensions. 2.1 Combinatory Categorial Grammar CCG , and parsing with CCG , has been described elsewhere (Clark and Curran, 2007; Hockenmaier and Steedman, 2002); here we provide only a short description. CCG (Steedman, 2000) is a lexicalized grammar formalism, which associates each word in a sentence with a lexical category. There is a small number of basic lexical categories, such as noun (N), noun phrase (NP), and prepositional phrase (PP). Complex lexical categories are formed recursively from basic categories and slashes, which indicate the directions of arguments. The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory rules are encoded as rule instanc"
E12-1075,J07-3004,0,0.0135301,"m for the challenging task of the general word ordering problem. Second, we develop a novel method for incorporating a large-scale language model into a syntax-based generation system. Finally, we analyse large-margin training in the context of learning-guided best-first search, offering a novel solution to this computationally hard problem. 2 The statistical model and decoding algorithm We take Z&C as our baseline system. Given a multi-set of input words, the baseline system builds a CCG derivation by choosing and ordering words from the input set. The scoring model is trained using CCGBank (Hockenmaier and Steedman, 2007), and best-first decoding is applied. We apply the same decoding framework in this paper, but apply an improved training process, and incorporate an N -gram language model into the syntax model. In this section, we describe and discuss the baseline statistical model and decoding framework, motivating our extensions. 2.1 Combinatory Categorial Grammar CCG , and parsing with CCG , has been described elsewhere (Clark and Curran, 2007; Hockenmaier and Steedman, 2002); here we provide only a short description. CCG (Steedman, 2000) is a lexicalized grammar formalism, which associates each word in a"
E12-1075,N03-1017,0,0.0251502,"order for a multiset of words. The word ordering problem can also include word choice, where only a subset of the input words are used to produce the output. Word ordering is a difficult problem. Finding the best permutation for a set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the traveling salesman problem. In practice, exploring the whole search space of permutations is often prevented by adding constraints. Stephen Clark University of Cambridge Computer Laboratory sc609@cam.ac.uk In phrase-based machine translation (Koehn et al., 2003; Koehn et al., 2007), a distortion limit is used to constrain the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomial time inference is feasible. In fluency improvement (Blackwood et al., 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (200"
E12-1075,P07-2045,0,0.00438473,"t of words. The word ordering problem can also include word choice, where only a subset of the input words are used to produce the output. Word ordering is a difficult problem. Finding the best permutation for a set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the traveling salesman problem. In practice, exploring the whole search space of permutations is often prevented by adding constraints. Stephen Clark University of Cambridge Computer Laboratory sc609@cam.ac.uk In phrase-based machine translation (Koehn et al., 2003; Koehn et al., 2007), a distortion limit is used to constrain the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomial time inference is feasible. In fluency improvement (Blackwood et al., 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) uses a dependency"
E12-1075,P02-1040,0,0.0899325,"and does not add to its asymptotic complexity, due to the heuristic nature of the decoder. 5 Experiments We use sections 2–21 of CCGBank to train our syntax model, section 00 for development and section 23 for the final test. Derivations from CCGBank are transformed into inputs by turning their surface strings into multi-sets of words. Following Z&C, we treat base noun phrases (i.e. NP s that do not recursively contain other NPs) as atomic units for the input. Output sequences are compared with the original sentences to evaluate their quality. We follow previous work and use the BLEU metric (Papineni et al., 2002) to compare outputs with references. Z&C use two methods to construct leaf edges. The first is to assign lexical categories according to a dictionary. There are 26.8 lexical categories for each word on average using this method, corresponding to 26.8 leaf edges. The other method is to use a pre-processing step — a CCG supertagger (Clark and Curran, 2007) — to prune candidate lexical categories according to the goldCCGBank training development GigaWord v4 AFP XIN Sentences 39,604 1,913 Sentences 30,363,052 15,982,098 Tokens 929,552 45,422 Tokens 684,910,697 340,666,976 Table 1: Number of senten"
E12-1075,P11-1008,0,0.0150784,"achine translation systems, both of which apply a score from the N gram model component in a derivation-building process. As discussed earlier, polynomial-time decoding is typically feasible for syntax-based machine translation systems without an N -gram language model, due to constraints from the grammar. In these cases, incorporation of N gram language models can significantly increase the complexity of a dynamic-programming decoder (Bar-Hillel et al., 1961). Efficient search has been achieved using chart pruning (Chiang, 2007) and iterative numerical approaches to constrained optimization (Rush and Collins, 2011). In contrast, the incorporation of an N -gram language model into our decoder is more straightforward, and does not add to its asymptotic complexity, due to the heuristic nature of the decoder. 5 Experiments We use sections 2–21 of CCGBank to train our syntax model, section 00 for development and section 23 for the final test. Derivations from CCGBank are transformed into inputs by turning their surface strings into multi-sets of words. Following Z&C, we treat base noun phrases (i.e. NP s that do not recursively contain other NPs) as atomic units for the input. Output sequences are compared w"
E12-1075,D08-1052,0,0.118457,"ax model, so that the score of an edge e becomes: the perspective of correctness, it is unnecessary to find a margin between the sub-edges of e+ and those of e− , since both are gold-standard edges. However, since the score of an edge not only represents its correctness, but also affects its priority on the agenda, promoting the sub-edge of e+ can lead to “easier” edges being constructed before “harder” ones (i.e. those that are less likely to be correct), and therefore improve the output accuracy. This perspective has been observed by other works of learning-guided-search (Shen et al., 2007; Shen and Joshi, 2008; Goldberg and Elhadad, 2010). Intuitively, the score difference between easy gold-standard and harder gold-standard edges should not be as great as the difference between gold-standard and non-goldstandard edges. The perceptron update cannot provide such control of separation, because the amount of update is fixed to 1. As described earlier, we treat parameter update as finding a separation between correct and incorrect edges, in which the global feature vectors Φ, rather than φ, are considered. Given a positive example e+ and a negative example e− , we make a minimum update so that the score"
E12-1075,P07-1096,0,0.0346909,"cores into our syntax model, so that the score of an edge e becomes: the perspective of correctness, it is unnecessary to find a margin between the sub-edges of e+ and those of e− , since both are gold-standard edges. However, since the score of an edge not only represents its correctness, but also affects its priority on the agenda, promoting the sub-edge of e+ can lead to “easier” edges being constructed before “harder” ones (i.e. those that are less likely to be correct), and therefore improve the output accuracy. This perspective has been observed by other works of learning-guided-search (Shen et al., 2007; Shen and Joshi, 2008; Goldberg and Elhadad, 2010). Intuitively, the score difference between easy gold-standard and harder gold-standard edges should not be as great as the difference between gold-standard and non-goldstandard edges. The perceptron update cannot provide such control of separation, because the amount of update is fixed to 1. As described earlier, we treat parameter update as finding a separation between correct and incorrect edges, in which the global feature vectors Φ, rather than φ, are considered. Given a positive example e+ and a negative example e− , we make a minimum up"
E12-1075,E09-1097,0,0.233416,"Ordering Incorporating a Large-Scale Language Model Yue Zhang University of Cambridge Computer Laboratory yz360@cam.ac.uk Graeme Blackwood University of Cambridge Engineering Department gwb24@eng.cam.ac.uk Abstract A fundamental problem in text generation is word ordering. Word ordering is a computationally difficult problem, which can be constrained to some extent for particular applications, for example by using synchronous grammars for statistical machine translation. There have been some recent attempts at the unconstrained problem of generating a sentence from a multi-set of input words (Wan et al., 2009; Zhang and Clark, 2011). By using CCG and learning guided search, Zhang and Clark reported the highest scores on this task. One limitation of their system is the absence of an N-gram language model, which has been used by text generation systems to improve fluency. We take the Zhang and Clark system as the baseline, and incorporate an N-gram model by applying online large-margin training. Our system significantly improved on the baseline by 3.7 BLEU points. 1 Introduction One fundamental problem in text generation is word ordering, which can be abstractly formulated as finding a grammatical o"
E12-1075,J97-3002,0,0.247543,"rdering is a difficult problem. Finding the best permutation for a set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the traveling salesman problem. In practice, exploring the whole search space of permutations is often prevented by adding constraints. Stephen Clark University of Cambridge Computer Laboratory sc609@cam.ac.uk In phrase-based machine translation (Koehn et al., 2003; Koehn et al., 2007), a distortion limit is used to constrain the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomial time inference is feasible. In fluency improvement (Blackwood et al., 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) uses a dependency grammar to solve word ordering, and Zhang and Clark (2011) uses CCG (Steedman, 2000) for word ordering and word choice. The use of synt"
E12-1075,D11-1106,1,0.660638,"ting a Large-Scale Language Model Yue Zhang University of Cambridge Computer Laboratory yz360@cam.ac.uk Graeme Blackwood University of Cambridge Engineering Department gwb24@eng.cam.ac.uk Abstract A fundamental problem in text generation is word ordering. Word ordering is a computationally difficult problem, which can be constrained to some extent for particular applications, for example by using synchronous grammars for statistical machine translation. There have been some recent attempts at the unconstrained problem of generating a sentence from a multi-set of input words (Wan et al., 2009; Zhang and Clark, 2011). By using CCG and learning guided search, Zhang and Clark reported the highest scores on this task. One limitation of their system is the absence of an N-gram language model, which has been used by text generation systems to improve fluency. We take the Zhang and Clark system as the baseline, and incorporate an N-gram model by applying online large-margin training. Our system significantly improved on the baseline by 3.7 BLEU points. 1 Introduction One fundamental problem in text generation is word ordering, which can be abstractly formulated as finding a grammatical order for a multiset of w"
E14-1062,P08-1085,0,0.0501682,"Missing"
E14-1062,W06-1615,0,0.066482,"omain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods with labeled source domain data and unlabeled target domain data (Dai et al., 2007; Raina et al., 2007). The baseline self-training approach (Liu and Zhang, 2012) belongs to this category. The second considers the differences in the two domains in terms of features (Blitzer et al., 2006; Daume III, 2007), classifying features into domain-independent source domain and target domain groups and training these types consistently. The third considers differences between the distributions of instances in the two domains, treating them differently (Jiang and Zhai, 2007). Our type-supervised method is closer to the second category. However, rather than splitting features into domain-independent and domain-specific types, we use domain-specific dictionaries to capture domain differences, and train a model on the source domain only. Our method can be treated as an approach specific to"
E14-1062,P07-1094,0,0.0159784,"Missing"
E14-1062,D10-1056,0,0.0427273,"Missing"
E14-1062,P12-1110,0,0.0349042,"Missing"
E14-1062,P04-1015,0,0.0111374,"the partial results from the beam to generate new partial results, using two types of actions: (1) Append, which appends ci to the last (partial) word in a partial result; (2) Separate(p), which makes the last word in the partial result as completed and adds ci as a new partial word with a POS tag p. Partial results in the beam are scored globally over all actions used to build them, so that the Nbest can be put back to the agenda for the next step. For each action, features are extracted differently. We use the features from Zhang and Clark (2010). Discriminative learning with early-update (Collins and Roark, 2004; Zhang and Clark, 2011) is used to train the model with beam-search. 2.2 Baseline Unsupervised Adaptation by Self-Training A simple unsupervised approach for POS-tagging with unlabeled data is EM. For a generative model such as HMM, EM can locally maximize the likelihood of training data. Given a good start, EM can result in a competitive HMM tagging model (Goldberg et al., 2008). For discriminative models with source-domain training examples, an initial model can be trained using the source-domain data, and self-training can be applied to find a locally-optimized model using raw target domai"
E14-1062,P11-1061,0,0.0132514,"from ours in several aspects: (1) they focus on in-domain POS-tagging, while our concern is cross-domain tagging; (2) they study POS-tagging on segmented sentences, while we investigate joint segmentation and POStagging for Chinese; (3) their tag-dictionaries are not tag-dictionaries literally, but statistics of wordtag associations. with Garrette et al. (2013), we also find that the type-supervised method is a competitive choice to token-supervised adaptation. There has been a line of work on using graphbased label propagation to expand tag-lexicons for POS-tagging (Subramanya et al., 2010; Das and Petrov, 2011). Similar methods have been applied to character-level Chinese tagging (Zeng et al., 2013). We found that label propagation from neither the source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods wi"
E14-1062,P09-1058,0,0.0311136,"omain data to find improved target domain accuracies over bare CTB training. are available for Chinese. For example, the Chinese Treebank (CTB) (Xue et al., 2005) contains over 50,000 manually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can give moderate improvements (Liu and Zhang, 2012). We show that accuracies can be much more significantly improved by using targetdomain knowledge in the form of lexicons. Both token-supervised and type-supervised domain adaptation rely on a set of source-domain annotations; while the former makes additional use of a small set of target annotations, the latter leverages a target-domain lexicon. We take a feature-based method, analogous to that of Daume III (200"
E14-1062,P07-1033,0,0.154058,"Missing"
E14-1062,C12-2073,1,0.643353,"ually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can give moderate improvements (Liu and Zhang, 2012). We show that accuracies can be much more significantly improved by using targetdomain knowledge in the form of lexicons. Both token-supervised and type-supervised domain adaptation rely on a set of source-domain annotations; while the former makes additional use of a small set of target annotations, the latter leverages a target-domain lexicon. We take a feature-based method, analogous to that of Daume III (2007), which tunes domain-dependent versions of features using domain-specific data. Our method tunes a set of lexicon-based features, so that domain-dependent models are derived from ins"
E14-1062,I05-3025,0,0.271209,"Missing"
E14-1062,D12-1075,0,0.0619082,"Missing"
E14-1062,N13-1014,0,0.0106972,"ctical situation. In particular, we split Chinese words into domain-independent and domain-specific categories, and define unlexicalized features for domain-specific words. We train lexicalized domain-independent and unlexicalized domainspecific features using the source domain annotated sentences and a source-domain lexicon, and then apply the resulting model to the target domain by replacing the source-domain lexicon with a target domain lexicon. Combined with unsupervised learning with unlabeled target-domain of sentences, the conceptually simple method worked highly effectively. Following Garrette and Baldridge (2013), we address practical questions 590 Action Lexicon Feature templates Separate in-lex(w−1 ), l(w−1 ) ◦ in-lex(w−1 ), in-lex(w−1 , t−1 ), l(w−1 ) ◦ in-lex(w−1 , t−1 ) ing words those that occur more than 3 times for words specific to the source domain. We assume that the domain-independent lexicon applies to all target domains also. For some target domains, we can obtain domain-specific terminologies easily from the Internet. However, this can be a very small portion depending on the domain. Thus, it may still be necessary to obtain new lexicons by manual annotation. Table 1: Dictionary feature"
E14-1062,J94-2001,0,0.0592843,"Missing"
E14-1062,W04-3236,0,0.0174807,"Missing"
E14-1062,P13-1057,0,0.109698,"cate that careful considerations need to be given for effective typesupervision. In addition, significant manual work might be required to ensure the quality of lexicons. To compare type- and token-supervised tagging, Garrette and Baldridge (2013) performed a set of experiments by conducting each type of annotation for two hours. They showed that for lowresource languages, a tag-dictionary can be reasonably effective if label propagation (Talukdar and Crammer, 2009) and model minimizations (Ravi and Knight, 2009) are applied to expand and filter the lexicons. Similar findings were reported in Garrette et al. (2013). Do the above findings carry over to the Chinese language? In this paper, we perform an empirical study on the effects of tag-dictionaries for domain adaptation of Chinese POS-tagging. We aim to answer the following research questions: (a) Is domain adaptation feasible with only a target-domain lexicon? (b) Can we further improve type-supervised domain adaptation using unlabeled target-domain sentences? (c) Is crafting a tag dictionary for domain adaptation more effective than manually annotating target domain sentences, given similar efforts? Our investigations are performed under two Chines"
E14-1062,C04-1081,0,0.596785,"Missing"
E14-1062,P13-1076,0,0.0190385,"cross-domain tagging; (2) they study POS-tagging on segmented sentences, while we investigate joint segmentation and POStagging for Chinese; (3) their tag-dictionaries are not tag-dictionaries literally, but statistics of wordtag associations. with Garrette et al. (2013), we also find that the type-supervised method is a competitive choice to token-supervised adaptation. There has been a line of work on using graphbased label propagation to expand tag-lexicons for POS-tagging (Subramanya et al., 2010; Das and Petrov, 2011). Similar methods have been applied to character-level Chinese tagging (Zeng et al., 2013). We found that label propagation from neither the source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods with labeled source domain data and unlabeled target domain data (Dai et al., 2007; Raina et"
E14-1062,petrov-etal-2012-universal,0,0.0207072,"Missing"
E14-1062,P09-1057,0,0.0155387,"s replaced with a raw tag dictionary gleaned from data, without any human intervention. These facts indicate that careful considerations need to be given for effective typesupervision. In addition, significant manual work might be required to ensure the quality of lexicons. To compare type- and token-supervised tagging, Garrette and Baldridge (2013) performed a set of experiments by conducting each type of annotation for two hours. They showed that for lowresource languages, a tag-dictionary can be reasonably effective if label propagation (Talukdar and Crammer, 2009) and model minimizations (Ravi and Knight, 2009) are applied to expand and filter the lexicons. Similar findings were reported in Garrette et al. (2013). Do the above findings carry over to the Chinese language? In this paper, we perform an empirical study on the effects of tag-dictionaries for domain adaptation of Chinese POS-tagging. We aim to answer the following research questions: (a) Is domain adaptation feasible with only a target-domain lexicon? (b) Can we further improve type-supervised domain adaptation using unlabeled target-domain sentences? (c) Is crafting a tag dictionary for domain adaptation more effective than manually anno"
E14-1062,P08-1101,1,0.450623,"Missing"
E14-1062,D10-1017,0,0.0102399,"ver, their efforts differ from ours in several aspects: (1) they focus on in-domain POS-tagging, while our concern is cross-domain tagging; (2) they study POS-tagging on segmented sentences, while we investigate joint segmentation and POStagging for Chinese; (3) their tag-dictionaries are not tag-dictionaries literally, but statistics of wordtag associations. with Garrette et al. (2013), we also find that the type-supervised method is a competitive choice to token-supervised adaptation. There has been a line of work on using graphbased label propagation to expand tag-lexicons for POS-tagging (Subramanya et al., 2010; Das and Petrov, 2011). Similar methods have been applied to character-level Chinese tagging (Zeng et al., 2013). We found that label propagation from neither the source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervi"
E14-1062,D10-1082,1,0.765811,"ed target domain accuracies over bare CTB training. are available for Chinese. For example, the Chinese Treebank (CTB) (Xue et al., 2005) contains over 50,000 manually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can give moderate improvements (Liu and Zhang, 2012). We show that accuracies can be much more significantly improved by using targetdomain knowledge in the form of lexicons. Both token-supervised and type-supervised domain adaptation rely on a set of source-domain annotations; while the former makes additional use of a small set of target annotations, the latter leverages a target-domain lexicon. We take a feature-based method, analogous to that of Daume III (2007), which tunes domain-d"
E14-1062,P11-1139,0,0.023051,"fy CTB words into domainindependent and domain-specific categories. Consisting of semantic information for nearly 100,000 common Chinese words, HowNet can serve as a resource of domain-independent Chinese words. We choose out of all words in the source domain training data those that also occur in HowNet for domain-independent words, and out of the remain4 4.1 Experiments Setting We use annotated sentences from the CTB5 for source-domain training, splitting the corpus into training, development and test sections in the same way as previous work (Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011). Following Liu and Zhang (2012), we use the free Internet novel “Zhuxian” (henceforth referred to as ZX; also known as “Jade dynasty”) as our target domain data. The writing style of the novel is in the literature genre, with the style of Ming and Qing novels, very different from news in CTB. Ex591 CTB sentences ZX sentences 乔石会见俄罗斯议员团 天下之大，无奇不有，山川灵秀，亦多妖魔鬼怪。 (Qiaoshi meets the Russian delegates.) (The world was big. It held everything. There were fascinating 李鹏强调要加快推行公务员制度 landscapes. There were haunting ghosts.) (Lipeng stressed on speeding the reform of official regulations.) 时间无多，我去请出诛仙古剑。"
E14-1062,J11-1005,1,0.489487,"the beam to generate new partial results, using two types of actions: (1) Append, which appends ci to the last (partial) word in a partial result; (2) Separate(p), which makes the last word in the partial result as completed and adds ci as a new partial word with a POS tag p. Partial results in the beam are scored globally over all actions used to build them, so that the Nbest can be put back to the agenda for the next step. For each action, features are extracted differently. We use the features from Zhang and Clark (2010). Discriminative learning with early-update (Collins and Roark, 2004; Zhang and Clark, 2011) is used to train the model with beam-search. 2.2 Baseline Unsupervised Adaptation by Self-Training A simple unsupervised approach for POS-tagging with unlabeled data is EM. For a generative model such as HMM, EM can locally maximize the likelihood of training data. Given a good start, EM can result in a competitive HMM tagging model (Goldberg et al., 2008). For discriminative models with source-domain training examples, an initial model can be trained using the source-domain data, and self-training can be applied to find a locally-optimized model using raw target domain sentences. The trainin"
E14-1062,Q13-1001,0,0.0296522,"Missing"
E14-1062,I11-1035,0,0.0274135,"Missing"
E17-1061,P04-1015,0,0.189212,"ed conditions are listed in multi-lines with indentation. wik denotes english wiktionary. similar to that used in Song et al. (2014), listed in Table 7, to generate a candidate set of inflections. An averaged perceptron classifier (Collins, 2002) is trained for each lemma. For distinguishing between singular and plural candidate verb forms, the feature templates in Table 8 are used. ~ a) where θ~ is the model parameter vector and Φ(C, denotes a feature vector consisting of configuration and action components. Given a set of labeled training examples, the averaged perceptron with early update (Collins and Roark, 2004) is used. 4 Morphological Generation The last step is to inflate the lemmas in the sentence. There are three POS categories, including nouns, verbs and articles, for which we need to generate morphological forms. We use Wiktionary2 as a basis and write a small set of rules 2 candidates ← T OP -K(agenda) agenda ← ∅ Rules for be attr[‘partic’] == ‘pres’ → being attr[‘partic’] == ‘past’ → been attr[‘tense’] == ‘past’ sbj.attr[‘num’] == ‘sg’ → was sbj.attr[‘num’] == ‘pl’ → were other → [was,were] attr[‘tense’] == ‘pres’ sbj.attr[‘num’] == ‘sg’ → is sbj.attr[‘num’] == ‘pl’ → are other → [am,is,are]"
E17-1061,W02-1001,0,0.489305,"015), syntactic linearization and morphologization (Song et al., 2014), parsing and NER (Finkel and Manning, 2009), entity and relation extraction (Li and Ji, 2014) and so on. We propose a first joint model for deep realization, integrating linearization, function word prediction and morphological generation. 3 Table 2: Feature templates for the comma prediction system. Indices on the surface string: n – word index; Functions: WORD – word at index; POS – part-of-speech at index; WORD-MOD – modifiers of index; LABEL-MOD – dependency labels of modifiers; BAG – set. eraged perceptron classifier (Collins, 2002) to predict function words, which is consistent with the joint model. Baseline We build a baseline following the pipeline in Figure 1a. Three stages are involved: 1. prediction of function words, inserting the predicted function words in the deep graph, resulting in a shallow graph; 2. linearizing the shallow graph; 3. generating the inflection for each lemma in the string. 3.1 3.2 Linearization The next step is linearizing the graph, which we solve using a novel transition-based algorithm. 3.2.1 Transition-Based Tree Linearization Liu et al. (2015) introduce a transition-based model for tree"
E17-1061,E14-1028,0,0.0468592,"Missing"
E17-1061,J05-3002,0,0.0757316,"g Morph (b) Deep graph Morphological String with Syntactic tree (c) Figure 1: Linearization pipelines (a) NLG pipeline with deep input graph, (b) pipeline based on the meaning text theory, (c) this paper. be ADV SBJ VC AM-TMP P . think A1 meanwhile Introduction C-A1 have Natural language generation (NLG) (Reiter and Dale, 1997; White, 2004) aims to synthesize natural language text given input syntactic, semantic or logical representations. It has been shown useful in various tasks in NLP, including machine translation (Chang and Toutanova, 2007; Zhang et al., 2014), abstractive summarization (Barzilay and McKeown, 2005) and grammatical error correction (Lee and Seneff, 2006). A line of traditional methods treat the problem as a pipeline of several independent steps (Bohnet et al., 2010; Wan et al., 2009; Bangalore et al., 2000; H. Oh and I. Rudnicky, 2000; Langkilde and Knight, 1998). For example, shown in Figure 1b, a pipeline based on the meaning text theory (MTT) (Melˇcuk, 1988) splits NLG into three VC increase A1 price Figure 2: Sample deep graph for the sentence: meanwhile, prices are thought to have increased. Note that words are replaced by their lemmas. The function word to and comma are absent in g"
E17-1061,P08-1022,0,0.114531,"Using SVM models with rich features, Bohnet et al. (2011) achieved state-of-art results on the task of deep realization. While they built a pipeline system, we show that joint models can be used to overcome limitations of the pipeline approach giving the best results. Joint models for NLP have shown effectiveness in recent years. Though having to tackle increased search space, they overcome issues with error propagation in pipelined models. Joint models have been explored for grammar-based approaches to surface realisation using HPSG and CCG (Carroll and Oepen, 2005; Velldal and Oepen, 2006; Espinosa et al., 2008; White and Rajkumar, 2009; White, 2006; Carroll et al., 1999). Pipelined systems suffer from the problem of error propagation. In addition, because the steps are independent of each other, information available in a later stage is not made use of in the earlier stages. We introduce a transition-based (Nivre, 2008) method for joint deep input surface realisation integrating linearization, function word prediction and morphological generation. The model is shown in Fig 1c, as compared with the pipelined baseline in Fig 1a. For a directly comparable baseline, we construct a pipeline system of fu"
E17-1061,W11-2832,0,0.301324,"n. Standard evaluations show that: 1. Our joint model for deep input surface realisation achieves significantly better scores over its pipeline counterpart. 2. We achieve the best results reported on the task. Our system scores 1 BLEU point better over Bohnet et al. (2011) without using any external resources. We make the source code available at https://github.com/SUTDNLP/ ZGen/releases/tag/v0.3. mas for each word being connected by semantic labels (Banarescu et al., 2013; Melˇcuk, 2015). In contrast to shallow syntactic trees, function words in surface forms are not included in deep graphs (Belz et al., 2011). Deep inputs can more commonly occur as input of NLG systems where entities and content words are available, and one has to generate a grammatical sentence using them with only provision for inflections of words and introduction of function words. Such usecases include summarization, dialog generation etc. 2 A pipeline of deep input linearization is shown in Figure 1a. Generation involves predicting the correct word order, deciding inflections and also filling in function words at the appropriate positions. The worst-case complexity is n! for permuting n words, 2n for function word prediction"
E17-1061,N09-1037,0,0.0611765,"Missing"
E17-1061,C10-1012,0,0.12653,"ext theory, (c) this paper. be ADV SBJ VC AM-TMP P . think A1 meanwhile Introduction C-A1 have Natural language generation (NLG) (Reiter and Dale, 1997; White, 2004) aims to synthesize natural language text given input syntactic, semantic or logical representations. It has been shown useful in various tasks in NLP, including machine translation (Chang and Toutanova, 2007; Zhang et al., 2014), abstractive summarization (Barzilay and McKeown, 2005) and grammatical error correction (Lee and Seneff, 2006). A line of traditional methods treat the problem as a pipeline of several independent steps (Bohnet et al., 2010; Wan et al., 2009; Bangalore et al., 2000; H. Oh and I. Rudnicky, 2000; Langkilde and Knight, 1998). For example, shown in Figure 1b, a pipeline based on the meaning text theory (MTT) (Melˇcuk, 1988) splits NLG into three VC increase A1 price Figure 2: Sample deep graph for the sentence: meanwhile, prices are thought to have increased. Note that words are replaced by their lemmas. The function word to and comma are absent in graph. independent steps 1. syntactic generation: generating an unordered and lemma-formed syntactic tree from a semantic graph, introducing function words; 2. syntactic"
E17-1061,P98-1116,0,0.36896,"ve Natural language generation (NLG) (Reiter and Dale, 1997; White, 2004) aims to synthesize natural language text given input syntactic, semantic or logical representations. It has been shown useful in various tasks in NLP, including machine translation (Chang and Toutanova, 2007; Zhang et al., 2014), abstractive summarization (Barzilay and McKeown, 2005) and grammatical error correction (Lee and Seneff, 2006). A line of traditional methods treat the problem as a pipeline of several independent steps (Bohnet et al., 2010; Wan et al., 2009; Bangalore et al., 2000; H. Oh and I. Rudnicky, 2000; Langkilde and Knight, 1998). For example, shown in Figure 1b, a pipeline based on the meaning text theory (MTT) (Melˇcuk, 1988) splits NLG into three VC increase A1 price Figure 2: Sample deep graph for the sentence: meanwhile, prices are thought to have increased. Note that words are replaced by their lemmas. The function word to and comma are absent in graph. independent steps 1. syntactic generation: generating an unordered and lemma-formed syntactic tree from a semantic graph, introducing function words; 2. syntactic linearization: linearizing the unordered syntactic tree; 3. morphological generation: generating the"
E17-1061,W11-2835,0,0.525424,"iting student at Singapore University of Technology and Design. 643 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 643–654, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics joint system with regard to the use of information. Standard evaluations show that: 1. Our joint model for deep input surface realisation achieves significantly better scores over its pipeline counterpart. 2. We achieve the best results reported on the task. Our system scores 1 BLEU point better over Bohnet et al. (2011) without using any external resources. We make the source code available at https://github.com/SUTDNLP/ ZGen/releases/tag/v0.3. mas for each word being connected by semantic labels (Banarescu et al., 2013; Melˇcuk, 2015). In contrast to shallow syntactic trees, function words in surface forms are not included in deep graphs (Belz et al., 2011). Deep inputs can more commonly occur as input of NLG systems where entities and content words are available, and one has to generate a grammatical sentence using them with only provision for inflections of words and introduction of function words. Such u"
E17-1061,P14-1038,0,0.0184628,"ld of n; Functions: WORD – word at index; POS – part-of-speech at index. A1 INF C-A1 to have VC increase A1 price Figure 3: Equivalent shallow graph for Figure 2. Features for predicting count of comma WORD(n); POS(n) BAG(WORD-MOD(n)) BAG(LABEL-MOD(n)) Joint models have been proposed for word segmentation and POS-tagging (Zhang and Clark, 2010), POS-tagging and syntactic chunking (Sutton et al., 2007), segmentation and normalization (Qian et al., 2015), syntactic linearization and morphologization (Song et al., 2014), parsing and NER (Finkel and Manning, 2009), entity and relation extraction (Li and Ji, 2014) and so on. We propose a first joint model for deep realization, integrating linearization, function word prediction and morphological generation. 3 Table 2: Feature templates for the comma prediction system. Indices on the surface string: n – word index; Functions: WORD – word at index; POS – part-of-speech at index; WORD-MOD – modifiers of index; LABEL-MOD – dependency labels of modifiers; BAG – set. eraged perceptron classifier (Collins, 2002) to predict function words, which is consistent with the joint model. Baseline We build a baseline following the pipeline in Figure 1a. Three stages a"
E17-1061,I05-1015,0,0.0399126,"s for the task of shallow-syntactic linearization. Using SVM models with rich features, Bohnet et al. (2011) achieved state-of-art results on the task of deep realization. While they built a pipeline system, we show that joint models can be used to overcome limitations of the pipeline approach giving the best results. Joint models for NLP have shown effectiveness in recent years. Though having to tackle increased search space, they overcome issues with error propagation in pipelined models. Joint models have been explored for grammar-based approaches to surface realisation using HPSG and CCG (Carroll and Oepen, 2005; Velldal and Oepen, 2006; Espinosa et al., 2008; White and Rajkumar, 2009; White, 2006; Carroll et al., 1999). Pipelined systems suffer from the problem of error propagation. In addition, because the steps are independent of each other, information available in a later stage is not made use of in the earlier stages. We introduce a transition-based (Nivre, 2008) method for joint deep input surface realisation integrating linearization, function word prediction and morphological generation. The model is shown in Fig 1c, as compared with the pipelined baseline in Fig 1a. For a directly comparabl"
E17-1061,D15-1043,1,0.826413,"ed. Related Work Related work can be broadly summarized into three areas: abstract word ordering, applications of meaning-text theory and joint modelling of NLP tasks. In abstract word ordering (Wan et al., 2009; Zhang, 2013; Zhang and Clark, 2015), De Gispert et al. (2014) compose phrases over individual words and permute the phrases to achieve linearization. Schmaltz et al. (2016) show that strong surface-level language models are more effective than models trained with syntactic information for the task of linearization. Transitionbased techniques have also been explored (Liu et al., 2015; Liu and Zhang, 2015; Puduppully et al., 2016). To our knowledge, we are the first to use transition-based techniques for deep input linearization. There has been work done in the area of sentence linearization using meaning-text theory (Melˇcuk, 1988). Belz et al. (2011) organized a shared task on both shallow and deep linearization according to meaning-text theory, which provides a standard benchmark for system comparison. Song et al. (2014) achieved the best results for the task of shallow-syntactic linearization. Using SVM models with rich features, Bohnet et al. (2011) achieved state-of-art results on the ta"
E17-1061,P07-1002,0,0.0325333,"reported so far. 1 Shallow graph String Morph (a) Sem graph Synt tree String Morph (b) Deep graph Morphological String with Syntactic tree (c) Figure 1: Linearization pipelines (a) NLG pipeline with deep input graph, (b) pipeline based on the meaning text theory, (c) this paper. be ADV SBJ VC AM-TMP P . think A1 meanwhile Introduction C-A1 have Natural language generation (NLG) (Reiter and Dale, 1997; White, 2004) aims to synthesize natural language text given input syntactic, semantic or logical representations. It has been shown useful in various tasks in NLP, including machine translation (Chang and Toutanova, 2007; Zhang et al., 2014), abstractive summarization (Barzilay and McKeown, 2005) and grammatical error correction (Lee and Seneff, 2006). A line of traditional methods treat the problem as a pipeline of several independent steps (Bohnet et al., 2010; Wan et al., 2009; Bangalore et al., 2000; H. Oh and I. Rudnicky, 2000; Langkilde and Knight, 1998). For example, shown in Figure 1b, a pipeline based on the meaning text theory (MTT) (Melˇcuk, 1988) splits NLG into three VC increase A1 price Figure 2: Sample deep graph for the sentence: meanwhile, prices are thought to have increased. Note that words"
E17-1061,N15-1012,1,0.88711,"M decoder is defined. Related Work Related work can be broadly summarized into three areas: abstract word ordering, applications of meaning-text theory and joint modelling of NLP tasks. In abstract word ordering (Wan et al., 2009; Zhang, 2013; Zhang and Clark, 2015), De Gispert et al. (2014) compose phrases over individual words and permute the phrases to achieve linearization. Schmaltz et al. (2016) show that strong surface-level language models are more effective than models trained with syntactic information for the task of linearization. Transitionbased techniques have also been explored (Liu et al., 2015; Liu and Zhang, 2015; Puduppully et al., 2016). To our knowledge, we are the first to use transition-based techniques for deep input linearization. There has been work done in the area of sentence linearization using meaning-text theory (Melˇcuk, 1988). Belz et al. (2011) organized a shared task on both shallow and deep linearization according to meaning-text theory, which provides a standard benchmark for system comparison. Song et al. (2014) achieved the best results for the task of shallow-syntactic linearization. Using SVM models with rich features, Bohnet et al. (2011) achieved state-of-"
E17-1061,D14-1082,0,0.0455874,"aseline following the pipeline in Figure 1a. Three stages are involved: 1. prediction of function words, inserting the predicted function words in the deep graph, resulting in a shallow graph; 2. linearizing the shallow graph; 3. generating the inflection for each lemma in the string. 3.1 3.2 Linearization The next step is linearizing the graph, which we solve using a novel transition-based algorithm. 3.2.1 Transition-Based Tree Linearization Liu et al. (2015) introduce a transition-based model for tree linearization. The approach extends from transition-based parsers (Nivre and Scholz, 2004; Chen and Manning, 2014), where state consists of stack to hold partially built outputs and a queue to hold input sequence of words. In case of linearization, the input is a set of words. Liu et al. therefore use a set to hold the input instead of a queue. State is represented by a tuple (σ, ρ, A), where σ is stack to store partial derivations, ρ is set of input words and A is the set of dependency relations that have been built. There are three transition actions: • S HIFT-Word-POS – shifts Word from ρ, assigns POS to it and pushes it to top of stack as S0 ; • L EFTA RC-LABEL – constructs dependency LABEL arc S1 ←−−"
E17-1061,meyers-etal-2004-annotating,0,0.0318694,"A RC, I NSERT and I DLE actions to handle function words. If node i has a child node in C, which is not shifted, we predict S PLITA RC and I NSERT. If i is sibling to j, we predict I NSERT. If both the stack and buffer are empty, we predict I DLE. Pseudocode for G ETP OSSIBLE ACTIONS for the joint method is shown in Algorithm 5. 5 Experiments 5.1 Dataset We work on the deep dataset from the Surface Realisation Shared Task (Belz et al., 2011)4 . Sentences are represented as sets of unordered nodes with labeled semantic edges between them. Semantic representation is obtained by merging Nombank (Meyers et al., 2004), Propbank (Palmer et al., 2005) and syntactic dependencies. Edge labeling follows PropBank annotation scheme such as {A0, A1, ... An}. The nodes are annotated with lemma and where appropriate number, tense and participle features. Function words including 3 For example in Figure 2, price is the subject of be and if be is in present tense and price is in plural form, the inflections {am, is, was, were} are impossible and are is the correct inflection for be. We therefore generate transition action as S HIFT-are. 4 649 http://www.nltg.brighton.ac.uk/research/sr-task/ Input (unordered lemma-form"
E17-1061,E09-1097,0,0.203043,"paper. be ADV SBJ VC AM-TMP P . think A1 meanwhile Introduction C-A1 have Natural language generation (NLG) (Reiter and Dale, 1997; White, 2004) aims to synthesize natural language text given input syntactic, semantic or logical representations. It has been shown useful in various tasks in NLP, including machine translation (Chang and Toutanova, 2007; Zhang et al., 2014), abstractive summarization (Barzilay and McKeown, 2005) and grammatical error correction (Lee and Seneff, 2006). A line of traditional methods treat the problem as a pipeline of several independent steps (Bohnet et al., 2010; Wan et al., 2009; Bangalore et al., 2000; H. Oh and I. Rudnicky, 2000; Langkilde and Knight, 1998). For example, shown in Figure 1b, a pipeline based on the meaning text theory (MTT) (Melˇcuk, 1988) splits NLG into three VC increase A1 price Figure 2: Sample deep graph for the sentence: meanwhile, prices are thought to have increased. Note that words are replaced by their lemmas. The function word to and comma are absent in graph. independent steps 1. syntactic generation: generating an unordered and lemma-formed syntactic tree from a semantic graph, introducing function words; 2. syntactic linearization: lin"
E17-1061,C04-1010,0,0.0524741,"l. Baseline We build a baseline following the pipeline in Figure 1a. Three stages are involved: 1. prediction of function words, inserting the predicted function words in the deep graph, resulting in a shallow graph; 2. linearizing the shallow graph; 3. generating the inflection for each lemma in the string. 3.1 3.2 Linearization The next step is linearizing the graph, which we solve using a novel transition-based algorithm. 3.2.1 Transition-Based Tree Linearization Liu et al. (2015) introduce a transition-based model for tree linearization. The approach extends from transition-based parsers (Nivre and Scholz, 2004; Chen and Manning, 2014), where state consists of stack to hold partially built outputs and a queue to hold input sequence of words. In case of linearization, the input is a set of words. Liu et al. therefore use a set to hold the input instead of a queue. State is represented by a tuple (σ, ρ, A), where σ is stack to store partial derivations, ρ is set of input words and A is the set of dependency relations that have been built. There are three transition actions: • S HIFT-Word-POS – shifts Word from ρ, assigns POS to it and pushes it to top of stack as S0 ; • L EFTA RC-LABEL – constructs de"
E17-1061,J08-4003,0,0.161272,"years. Though having to tackle increased search space, they overcome issues with error propagation in pipelined models. Joint models have been explored for grammar-based approaches to surface realisation using HPSG and CCG (Carroll and Oepen, 2005; Velldal and Oepen, 2006; Espinosa et al., 2008; White and Rajkumar, 2009; White, 2006; Carroll et al., 1999). Pipelined systems suffer from the problem of error propagation. In addition, because the steps are independent of each other, information available in a later stage is not made use of in the earlier stages. We introduce a transition-based (Nivre, 2008) method for joint deep input surface realisation integrating linearization, function word prediction and morphological generation. The model is shown in Fig 1c, as compared with the pipelined baseline in Fig 1a. For a directly comparable baseline, we construct a pipeline system of function words prediction, linearization and morphological generation similar to the pipeline of Bohnet et al. (2011), but with the following difference. Our baseline pipeline system makes function word prediction for a deep input graph, whereas Bohnet et al. (2011) have a preprocessing step to construct a syntactic"
E17-1061,D09-1043,0,0.114694,"Missing"
E17-1061,J05-1004,0,0.104494,"to handle function words. If node i has a child node in C, which is not shifted, we predict S PLITA RC and I NSERT. If i is sibling to j, we predict I NSERT. If both the stack and buffer are empty, we predict I DLE. Pseudocode for G ETP OSSIBLE ACTIONS for the joint method is shown in Algorithm 5. 5 Experiments 5.1 Dataset We work on the deep dataset from the Surface Realisation Shared Task (Belz et al., 2011)4 . Sentences are represented as sets of unordered nodes with labeled semantic edges between them. Semantic representation is obtained by merging Nombank (Meyers et al., 2004), Propbank (Palmer et al., 2005) and syntactic dependencies. Edge labeling follows PropBank annotation scheme such as {A0, A1, ... An}. The nodes are annotated with lemma and where appropriate number, tense and participle features. Function words including 3 For example in Figure 2, price is the subject of be and if be is in present tense and price is in plural form, the inflections {am, is, was, were} are impossible and are is the correct inflection for be. We therefore generate transition action as S HIFT-are. 4 649 http://www.nltg.brighton.ac.uk/research/sr-task/ Input (unordered lemma-formed graph): Sem ID PID Lemma SROO"
E17-1061,P02-1040,0,0.0990305,"old tree i.e mismatch between edges of gold tree and input graph. After excluding these instances, we have 34.3k training instances. We also exclude 800 training instances where the function words to and that have more than one child, and around 100 training instances where function words’ parent and child nodes are not connected by an arc in the deep graph. The above cases are deemed annotation mistakes. We thus train on a final subset of 33.4k training instances. The development set comprises 1034 instances and the test set comprises 2398 instances. Evaluation is done using the BLEU metric (Papineni et al., 2002). if i ∈ S IBLING(j) then P ROCESS S IBLING (i, j) 15 num=pl num=sg num=sg partic=past Lexeme are meanwhile . starts housing september thought Table 10: Deep type training instance from Surface Realisation Shared Task 2011. Sem – semantic label, ID – unique ID of node within graph, PID – the ID of the parent, Attr – Attributes such as partic (participle), tense or number, Lexeme – lexeme which is resolved using wiktionary and rules in Table 7. if {j → i} ∈ C∧ A.L EFT C HILD(j) is N IL then T ← T ∪ (R IGHTA RC) if i ∈ D ESCENDANT(j) then P ROCESS D ESCENDANT (i, j) 11 Attr tense=pres 6 that com"
E17-1061,D10-1082,1,0.808929,"P VC Features for predicting function words including to infinitive, that complementizer WORD(n); POS(n); WORD(c) , . Table 1: Feature templates for the prediction of function words- to infinitive and that complementizer. Indices on the surface string: n – word index; c – child of n; Functions: WORD – word at index; POS – part-of-speech at index. A1 INF C-A1 to have VC increase A1 price Figure 3: Equivalent shallow graph for Figure 2. Features for predicting count of comma WORD(n); POS(n) BAG(WORD-MOD(n)) BAG(LABEL-MOD(n)) Joint models have been proposed for word segmentation and POS-tagging (Zhang and Clark, 2010), POS-tagging and syntactic chunking (Sutton et al., 2007), segmentation and normalization (Qian et al., 2015), syntactic linearization and morphologization (Song et al., 2014), parsing and NER (Finkel and Manning, 2009), entity and relation extraction (Li and Ji, 2014) and so on. We propose a first joint model for deep realization, integrating linearization, function word prediction and morphological generation. 3 Table 2: Feature templates for the comma prediction system. Indices on the surface string: n – word index; Functions: WORD – word at index; POS – part-of-speech at index; WORD-MOD –"
E17-1061,N16-1058,1,0.72801,"ted work can be broadly summarized into three areas: abstract word ordering, applications of meaning-text theory and joint modelling of NLP tasks. In abstract word ordering (Wan et al., 2009; Zhang, 2013; Zhang and Clark, 2015), De Gispert et al. (2014) compose phrases over individual words and permute the phrases to achieve linearization. Schmaltz et al. (2016) show that strong surface-level language models are more effective than models trained with syntactic information for the task of linearization. Transitionbased techniques have also been explored (Liu et al., 2015; Liu and Zhang, 2015; Puduppully et al., 2016). To our knowledge, we are the first to use transition-based techniques for deep input linearization. There has been work done in the area of sentence linearization using meaning-text theory (Melˇcuk, 1988). Belz et al. (2011) organized a shared task on both shallow and deep linearization according to meaning-text theory, which provides a standard benchmark for system comparison. Song et al. (2014) achieved the best results for the task of shallow-syntactic linearization. Using SVM models with rich features, Bohnet et al. (2011) achieved state-of-art results on the task of deep realization. Wh"
E17-1061,J11-1005,1,0.826628,"IGHTA RC or L EFTA RC action; 2. i is descendant of j. In this case the parents of i (such that they are descendants of j) and siblings of i through such parents are shifted. 3. i is sibling of j. In this case, parents of i and their descendants are shifted such that A remains consistent. Because the input is a graph, more than one of the above configuration can occur simultaneously. More detailed discussion related to G ET P OSSIBLE ACTIONS is given in Appendix A. 3.2.5 Search and Learning We follow Puduppully et al. (2016) and Liu et al. (2015), applying the learning and search framework of Zhang and Clark (2011). Pseudocode is shown in Algorithm 4. It performs beam search holding k best states in an agenda at each incremental step. At the start of decoding, agenda holds the initial state. At a step, for each state in the 1 Here Lcls represents set of arc labels of child nodes (of word to shift L) shifted on the stack, Lclns represents set of arc labels of child nodes not shifted on the stack, Lcps the POS set of shifted child nodes, Lcpns the POS set of unshifted child nodes, Lsls the set of arc labels of shifted siblings, Lslns the set of arc labels of unshifted siblings, Lsps the POS set of shifted"
E17-1061,J15-3005,1,0.889421,"ep input representation, following the pipeline of Figure 1b (with input as deep graph instead of semantic graph). They construct a syntactic tree from deep input graph followed by function word prediction, linearization and morphological generation. A rich set of features are used at each stage of the pipeline and for each adjacent pair of stages, an SVM decoder is defined. Related Work Related work can be broadly summarized into three areas: abstract word ordering, applications of meaning-text theory and joint modelling of NLP tasks. In abstract word ordering (Wan et al., 2009; Zhang, 2013; Zhang and Clark, 2015), De Gispert et al. (2014) compose phrases over individual words and permute the phrases to achieve linearization. Schmaltz et al. (2016) show that strong surface-level language models are more effective than models trained with syntactic information for the task of linearization. Transitionbased techniques have also been explored (Liu et al., 2015; Liu and Zhang, 2015; Puduppully et al., 2016). To our knowledge, we are the first to use transition-based techniques for deep input linearization. There has been work done in the area of sentence linearization using meaning-text theory (Melˇcuk, 19"
E17-1061,D15-1211,1,0.853166,", . Table 1: Feature templates for the prediction of function words- to infinitive and that complementizer. Indices on the surface string: n – word index; c – child of n; Functions: WORD – word at index; POS – part-of-speech at index. A1 INF C-A1 to have VC increase A1 price Figure 3: Equivalent shallow graph for Figure 2. Features for predicting count of comma WORD(n); POS(n) BAG(WORD-MOD(n)) BAG(LABEL-MOD(n)) Joint models have been proposed for word segmentation and POS-tagging (Zhang and Clark, 2010), POS-tagging and syntactic chunking (Sutton et al., 2007), segmentation and normalization (Qian et al., 2015), syntactic linearization and morphologization (Song et al., 2014), parsing and NER (Finkel and Manning, 2009), entity and relation extraction (Li and Ji, 2014) and so on. We propose a first joint model for deep realization, integrating linearization, function word prediction and morphological generation. 3 Table 2: Feature templates for the comma prediction system. Indices on the surface string: n – word index; Functions: WORD – word at index; POS – part-of-speech at index; WORD-MOD – modifiers of index; LABEL-MOD – dependency labels of modifiers; BAG – set. eraged perceptron classifier (Coll"
E17-1061,D14-1021,1,0.855668,"graph String Morph (a) Sem graph Synt tree String Morph (b) Deep graph Morphological String with Syntactic tree (c) Figure 1: Linearization pipelines (a) NLG pipeline with deep input graph, (b) pipeline based on the meaning text theory, (c) this paper. be ADV SBJ VC AM-TMP P . think A1 meanwhile Introduction C-A1 have Natural language generation (NLG) (Reiter and Dale, 1997; White, 2004) aims to synthesize natural language text given input syntactic, semantic or logical representations. It has been shown useful in various tasks in NLP, including machine translation (Chang and Toutanova, 2007; Zhang et al., 2014), abstractive summarization (Barzilay and McKeown, 2005) and grammatical error correction (Lee and Seneff, 2006). A line of traditional methods treat the problem as a pipeline of several independent steps (Bohnet et al., 2010; Wan et al., 2009; Bangalore et al., 2000; H. Oh and I. Rudnicky, 2000; Langkilde and Knight, 1998). For example, shown in Figure 1b, a pipeline based on the meaning text theory (MTT) (Melˇcuk, 1988) splits NLG into three VC increase A1 price Figure 2: Sample deep graph for the sentence: meanwhile, prices are thought to have increased. Note that words are replaced by thei"
E17-1061,D16-1255,0,0.0281194,"Missing"
E17-1061,P13-1043,1,0.841874,". The rules in Table 7 are used to prune impossible inflections.3 Table 9 shows the transition actions to linearize the graph in Figure 2. These newly introduced transition actions result in variability in the number of transition actions. With function word prediction, the number of transition actions for a bag of n words is not necessarily 2n-1. For example, considering an I NSERT, S PLITA RC-to or S PLITA RC-that action post each S HIFT action, the maximum number of possible actions is 5n-1. This variance in the number of actions can impact the linear separability of state items. Following Zhu et al. (2013), we use I DLE actions as a form of padding method, which results in completed state items being further expanded up to 5n-1 steps. The joint model uses the same perceptron training al4.2 Obtaining Possible Transition Actions Given a Configuration Given a state s = ([σ|j i], ρ, A) and an input graph C, the possible transition actions include as a subset the transition actions in Algorithm 1 for shallow graph linearization. In addition, for each lemma being shifted, we enumerate its inflections and create S HIFT transition actions for each inflection. Further, we predict S PLITA RC, I NSERT and"
E17-2091,C14-1008,0,0.0232355,"; ...; htm ]. We propose three variants of attention to model the relation between context words and targets. 3.1 Vanilla Model We build a vanilla attention model by calculating a weighted value α over each word in sentences. The final representation of the sentence s is then given by1 : Related Work Traditional sentiment classification methods rely on manual discrete features (Pang et al., 2002; Go et al., 2009; Mohammad et al., 2013). Recently, distributed word representation (Socher et al., 2013; Tang et al., 2014; Zhang et al., 2015) and neural network methods (Irsoy and Cardie, 2013; dos Santos and Gatti, 2014; Dong et al., 2014; Zhou et al., 2014; Zhang et al., 2016; Teng et al., 2016; Ren et al., 2016) have shown promising results on this task. The success of such work suggests that using word embeddings and deep neural network structures can automatically exploit the syntactic and semantic structures. Our work is in line with these methods. The seminal work using the attention mechanism is neural machine translation (Bahdanau et al., 2015), where different weights are assigned to source words to implicitly learn alignments for translation. Subsequently, the attention mechanism has been applied i"
E17-2091,P16-1086,0,0.0467992,"tically exploit the syntactic and semantic structures. Our work is in line with these methods. The seminal work using the attention mechanism is neural machine translation (Bahdanau et al., 2015), where different weights are assigned to source words to implicitly learn alignments for translation. Subsequently, the attention mechanism has been applied into various other natural language processing tasks including parsing (Vinyals et al., 2015; Kuncoro et al., 2016; Liu and Zhang, 2017), document classification (Yang et al., 2016), question answering (He and Golub, 2016) and text understanding (Kadlec et al., 2016). For sentiment analysis, the attention mechanism has been applied to cross-lingual sentiment (Zhou s = attention([h0 ; ...; hn ], ht ) = n X αi hi , i where exp(βi ) αi = Pn j exp(βj ) and the weight scores β are calculated by using the target representation and the context word representation, βi = U T tanh(W1 · [hi ; ht ] + b1 ). The sentence representation s is then used to predict the probability distribution p of sentiment labels on the target by: p = sof tmax(W2 s + b2 ). We refer to this vanilla model as BILSTM-ATT. 3.2 Contextualized Attention We make two extensions to the vanilla att"
E17-2091,Q17-1004,1,0.424476,"ng results on this task. The success of such work suggests that using word embeddings and deep neural network structures can automatically exploit the syntactic and semantic structures. Our work is in line with these methods. The seminal work using the attention mechanism is neural machine translation (Bahdanau et al., 2015), where different weights are assigned to source words to implicitly learn alignments for translation. Subsequently, the attention mechanism has been applied into various other natural language processing tasks including parsing (Vinyals et al., 2015; Kuncoro et al., 2016; Liu and Zhang, 2017), document classification (Yang et al., 2016), question answering (He and Golub, 2016) and text understanding (Kadlec et al., 2016). For sentiment analysis, the attention mechanism has been applied to cross-lingual sentiment (Zhou s = attention([h0 ; ...; hn ], ht ) = n X αi hi , i where exp(βi ) αi = Pn j exp(βj ) and the weight scores β are calculated by using the target representation and the context word representation, βi = U T tanh(W1 · [hi ; ht ] + b1 ). The sentence representation s is then used to predict the probability distribution p of sentiment labels on the target by: p = sof tma"
E17-2091,D16-1171,0,0.0224529,"label on the target “miley ray cyrus”. One important problem of targeted sentiment classification is how to model the relation between targets and their context. Earlier methods defined rich features by exploiting POS tags and syntactic structures (Jiang et al., 2011; Dong et 572 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 572–577, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics et al., 2016), aspect-level sentiment (Wang et al., 2016) and user-oriented sentiment (Chen et al., 2016). To our knowledge, we are the first to use the attention mechanism to model sentences with respect to targeted sentiments. the sentence “#nowplaying [lady gaga]0 - let love down” is neural for the target “lady gaga”, where the contribution of “love” is little, despite that the word “love” is a positive word. To address this, we utilize the attention mechanism to calculate the contribution of each word towards targeted sentiment classes, as shown in Figure 1, where the gray level in the spectrum means the contribution of words. In particular, we build a vanilla model using a bidirectional LSTM"
E17-2091,D13-1171,0,0.197937,"Missing"
E17-2091,P14-2009,0,0.701292,"w that by using attention to model the contribution of each word with respect to the target, our model gives significantly improved results over two standard benchmarks. We report the best accuracy for this task. 1 w0 w1 w2 + - 0 target w3 w4 w5 right contexts w6 w7 w8 w9 vanilla attention contextualized attention Figure 1: Structures of modeling target, left and right contexts and the attention over words. al., 2014). Compared with discrete manual features, embedding features are less sparse, and can be learnt from large raw texts, capturing distributional syntactic and semantic information. Dong et al. (2014) use a target-specific recurrent neural network to represent a sentence. Vo and Zhang (2015) use the rich pooling functions to extract the feature vector for a given target. One important contribution of Vo and Zhang (2015) is that they split a sentence into three sections including the target, its left contexts and its right contexts, as shown in Figure 1. Zhang et al. (2016) represent words in the input using a bidirectional gated recurrent neural network, and then use three-way gated neural network structure to model the interaction between the target and its left and right contexts. Tang e"
E17-2091,W02-1011,0,0.033008,"input word sequence w0 , w1 , ..., wn as hidden nodes h0 , h1 , ..., hn : [h0 ; ...; hn ] = BILSTM([w0 ; ...; wn ]), where the target is denoted as ht , which is the average of word embeddings in the target phrase [ht0 ; ...; htm ]. We propose three variants of attention to model the relation between context words and targets. 3.1 Vanilla Model We build a vanilla attention model by calculating a weighted value α over each word in sentences. The final representation of the sentence s is then given by1 : Related Work Traditional sentiment classification methods rely on manual discrete features (Pang et al., 2002; Go et al., 2009; Mohammad et al., 2013). Recently, distributed word representation (Socher et al., 2013; Tang et al., 2014; Zhang et al., 2015) and neural network methods (Irsoy and Cardie, 2013; dos Santos and Gatti, 2014; Dong et al., 2014; Zhou et al., 2014; Zhang et al., 2016; Teng et al., 2016; Ren et al., 2016) have shown promising results on this task. The success of such work suggests that using word embeddings and deep neural network structures can automatically exploit the syntactic and semantic structures. Our work is in line with these methods. The seminal work using the attentio"
E17-2091,D14-1162,0,0.113424,"Missing"
E17-2091,D16-1058,0,0.354115,"nce 2013 :)” is marked with a positive sentiment label on the target “miley ray cyrus”. One important problem of targeted sentiment classification is how to model the relation between targets and their context. Earlier methods defined rich features by exploiting POS tags and syntactic structures (Jiang et al., 2011; Dong et 572 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 572–577, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics et al., 2016), aspect-level sentiment (Wang et al., 2016) and user-oriented sentiment (Chen et al., 2016). To our knowledge, we are the first to use the attention mechanism to model sentences with respect to targeted sentiments. the sentence “#nowplaying [lady gaga]0 - let love down” is neural for the target “lady gaga”, where the contribution of “love” is little, despite that the word “love” is a positive word. To address this, we utilize the attention mechanism to calculate the contribution of each word towards targeted sentiment classes, as shown in Figure 1, where the gray level in the spectrum means the contribution of words. In particular, we"
E17-2091,N16-1174,0,0.0255573,"rk suggests that using word embeddings and deep neural network structures can automatically exploit the syntactic and semantic structures. Our work is in line with these methods. The seminal work using the attention mechanism is neural machine translation (Bahdanau et al., 2015), where different weights are assigned to source words to implicitly learn alignments for translation. Subsequently, the attention mechanism has been applied into various other natural language processing tasks including parsing (Vinyals et al., 2015; Kuncoro et al., 2016; Liu and Zhang, 2017), document classification (Yang et al., 2016), question answering (He and Golub, 2016) and text understanding (Kadlec et al., 2016). For sentiment analysis, the attention mechanism has been applied to cross-lingual sentiment (Zhou s = attention([h0 ; ...; hn ], ht ) = n X αi hi , i where exp(βi ) αi = Pn j exp(βj ) and the weight scores β are calculated by using the target representation and the context word representation, βi = U T tanh(W1 · [hi ; ht ] + b1 ). The sentence representation s is then used to predict the probability distribution p of sentiment labels on the target by: p = sof tmax(W2 s + b2 ). We refer to this vanilla model"
E17-2091,D13-1170,0,0.00980992,"; ...; wn ]), where the target is denoted as ht , which is the average of word embeddings in the target phrase [ht0 ; ...; htm ]. We propose three variants of attention to model the relation between context words and targets. 3.1 Vanilla Model We build a vanilla attention model by calculating a weighted value α over each word in sentences. The final representation of the sentence s is then given by1 : Related Work Traditional sentiment classification methods rely on manual discrete features (Pang et al., 2002; Go et al., 2009; Mohammad et al., 2013). Recently, distributed word representation (Socher et al., 2013; Tang et al., 2014; Zhang et al., 2015) and neural network methods (Irsoy and Cardie, 2013; dos Santos and Gatti, 2014; Dong et al., 2014; Zhou et al., 2014; Zhang et al., 2016; Teng et al., 2016; Ren et al., 2016) have shown promising results on this task. The success of such work suggests that using word embeddings and deep neural network structures can automatically exploit the syntactic and semantic structures. Our work is in line with these methods. The seminal work using the attention mechanism is neural machine translation (Bahdanau et al., 2015), where different weights are assigned t"
E17-2091,D15-1073,1,0.848468,"Missing"
E17-2091,P14-1146,0,0.0393844,"he target is denoted as ht , which is the average of word embeddings in the target phrase [ht0 ; ...; htm ]. We propose three variants of attention to model the relation between context words and targets. 3.1 Vanilla Model We build a vanilla attention model by calculating a weighted value α over each word in sentences. The final representation of the sentence s is then given by1 : Related Work Traditional sentiment classification methods rely on manual discrete features (Pang et al., 2002; Go et al., 2009; Mohammad et al., 2013). Recently, distributed word representation (Socher et al., 2013; Tang et al., 2014; Zhang et al., 2015) and neural network methods (Irsoy and Cardie, 2013; dos Santos and Gatti, 2014; Dong et al., 2014; Zhou et al., 2014; Zhang et al., 2016; Teng et al., 2016; Ren et al., 2016) have shown promising results on this task. The success of such work suggests that using word embeddings and deep neural network structures can automatically exploit the syntactic and semantic structures. Our work is in line with these methods. The seminal work using the attention mechanism is neural machine translation (Bahdanau et al., 2015), where different weights are assigned to source words to i"
E17-2091,C14-1127,0,0.0188604,"attention to model the relation between context words and targets. 3.1 Vanilla Model We build a vanilla attention model by calculating a weighted value α over each word in sentences. The final representation of the sentence s is then given by1 : Related Work Traditional sentiment classification methods rely on manual discrete features (Pang et al., 2002; Go et al., 2009; Mohammad et al., 2013). Recently, distributed word representation (Socher et al., 2013; Tang et al., 2014; Zhang et al., 2015) and neural network methods (Irsoy and Cardie, 2013; dos Santos and Gatti, 2014; Dong et al., 2014; Zhou et al., 2014; Zhang et al., 2016; Teng et al., 2016; Ren et al., 2016) have shown promising results on this task. The success of such work suggests that using word embeddings and deep neural network structures can automatically exploit the syntactic and semantic structures. Our work is in line with these methods. The seminal work using the attention mechanism is neural machine translation (Bahdanau et al., 2015), where different weights are assigned to source words to implicitly learn alignments for translation. Subsequently, the attention mechanism has been applied into various other natural language pro"
E17-2091,C16-1311,0,0.690905,"(2014) use a target-specific recurrent neural network to represent a sentence. Vo and Zhang (2015) use the rich pooling functions to extract the feature vector for a given target. One important contribution of Vo and Zhang (2015) is that they split a sentence into three sections including the target, its left contexts and its right contexts, as shown in Figure 1. Zhang et al. (2016) represent words in the input using a bidirectional gated recurrent neural network, and then use three-way gated neural network structure to model the interaction between the target and its left and right contexts. Tang et al. (2016) learn target-specific sentence representation by combining word embeddings with the corresponding targeted embeddings, and then using two recurrent neural networks to encode the left context and the right context, respectively. The above methods use the different neural network structures to model the relation between contexts and targets, but they did not explicitly model the importance of each word in contributing to the sentiment polarity of the target. For example, Introduction Targeted sentiment analysis investigates the classification of opinions polarities towards specific target entit"
E17-2091,D16-1024,0,0.0296342,"Missing"
E17-2091,D16-1169,1,0.890135,"Missing"
I13-1035,P11-1040,0,0.0835156,"The feature-rich event filter led to significantly higher precision and doubled recall when compared to the state-of-the-art baseline system. In our experiments we observed that a news event can be detected more than once in one time window, which each appearance representing one aspects of the event. Building these sub-events into a hierarchy will be explored in the future. In addition to the above group of work, which represents events with a few messages or features showing the topic information, some researchers try to extract structured information for events. Given a set of seed events, Benson et al. (2011) use a factor graph to extract artist and venue information of a concert event. Popescu et al. (2011) extract main entities, actions and audience opinions. Data from social medias like Twitter are very sparse in presenting thousands of events, while some researchers mainly focus on specific types of events. Sakaki et al. (2010) detected disaster events like earthquakes and typhoons from Twitter. Pohl et al. (2012) tried to detect subevent to assist disaster management with Flickr and YouTube data. Agarwal et al. (2012) analyzed tweets containing specific keywords and report Fire-in Factory and"
I13-1035,N09-1025,0,0.0346455,"horoscope topic heterogeneous collection Table 3: Example Events. have fixed query words and search for related messages from social media websites for data. The query words are challenges to define as they are vital to the quality of dataset, which will greatly influence the results. Becker et al. (2012) tried to generate queries for a planned event to relax the limitation. Our work mainly focus on news event detection problem on Twitter. Rich features have been used in other tasks in NLP, such as POS-tagging (Toutanova et al., 2003), parsing (Zhang and Nivre, 2011) and machine translation (Chiang et al., 2009). Our work is in line with these. t. Tweet segmentation is firstly proposed by Li et al. (2012b) for an named entity recognition system on Twitter. They claim that segments are much more meaningful and easier to read than words. Twevent is the most related work to this paper. We adopt tweet segmentation, and segment tweets into non-overlapping segments that are regarded as bursty feature candidates, and utilize a feature-pivot clustering method to group bursty segments into clusters as events. The difference between this paper and Twevent is that they use a simple measurement (newsworthiness)"
I13-1035,N03-1033,0,0.0216373,"ce voting related to e4 photographer died when chasing justin bieber related to e6 horoscope topic heterogeneous collection Table 3: Example Events. have fixed query words and search for related messages from social media websites for data. The query words are challenges to define as they are vital to the quality of dataset, which will greatly influence the results. Becker et al. (2012) tried to generate queries for a planned event to relax the limitation. Our work mainly focus on news event detection problem on Twitter. Rich features have been used in other tasks in NLP, such as POS-tagging (Toutanova et al., 2003), parsing (Zhang and Nivre, 2011) and machine translation (Chiang et al., 2009). Our work is in line with these. t. Tweet segmentation is firstly proposed by Li et al. (2012b) for an named entity recognition system on Twitter. They claim that segments are much more meaningful and easier to read than words. Twevent is the most related work to this paper. We adopt tweet segmentation, and segment tweets into non-overlapping segments that are regarded as bursty feature candidates, and utilize a feature-pivot clustering method to group bursty segments into clusters as events. The difference between"
I13-1035,P12-1056,0,0.0249372,"d increased precision. 1 Introduction We study news event detection from Twitter messages (tweets). Generally, tweets can be classified into three groups: 1) news events, or breaking news such as “Manchester united Vs Athletic in Jan. 1st”; 2) hot topics that spread among a large amount of Twitter users, such as horoscope topics (e.g. “You have recently experienced a phase of expansionism and it’s... More for Sagittarius”); and 3) heterogeneous collections or, meaningless non-event tweets, such as “Need buddy wanna chat”. Some previous work (Cataldi et al., 2010; Kasiviswanathan et al., 2011; Diao et al., 2012) regards both news events and hot topics as subjects of detection, while other work (Jackoway et al., 2011; Sakaki et al., 2010; Becker et al., 2012) 302 International Joint Conference on Natural Language Processing, pages 302–310, Nagoya, Japan, 14-18 October 2013. The objective function is defined as: news from some topics, includes horoscope topics (“sagittarius; approach; big trouble”) and topics such as “hitler; fox; megan fox; rip; megan; selena gomez”, which contain segments that can also frequently occur in Wikipedia; 2) as a single measure, newsworthiness is subject to a tradeoff betw"
I13-1035,P11-2033,1,0.791098,"er died when chasing justin bieber related to e6 horoscope topic heterogeneous collection Table 3: Example Events. have fixed query words and search for related messages from social media websites for data. The query words are challenges to define as they are vital to the quality of dataset, which will greatly influence the results. Becker et al. (2012) tried to generate queries for a planned event to relax the limitation. Our work mainly focus on news event detection problem on Twitter. Rich features have been used in other tasks in NLP, such as POS-tagging (Toutanova et al., 2003), parsing (Zhang and Nivre, 2011) and machine translation (Chiang et al., 2009). Our work is in line with these. t. Tweet segmentation is firstly proposed by Li et al. (2012b) for an named entity recognition system on Twitter. They claim that segments are much more meaningful and easier to read than words. Twevent is the most related work to this paper. We adopt tweet segmentation, and segment tweets into non-overlapping segments that are regarded as bursty feature candidates, and utilize a feature-pivot clustering method to group bursty segments into clusters as events. The difference between this paper and Twevent is that t"
I17-1006,C10-1011,0,0.0510393,"Missing"
I17-1006,P16-1231,0,0.0713207,"dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar) with PA based on a forest-based objective, showing promising results. Meanwhile, it is still unclear how PAs can be used by other main-stream dependency parsers, such as the traditional linear graph-based parser (LGPar) and transition-based parser (LTPar), and the newly proposed biaffine neural network graph-based parser (Biaffine) (Dozat and Manning, 2017) and globally normalized neural network transition-based parser (GN3Par) (Andor et al., 2016). Introduction Traditional supervised approaches for structural classification assume full annotation (FA), meaning that the training instances have complete manually-labeled structures. In the case of dependency parsing, FA means a complete parse tree is provided for each training sentence. However, recent studies suggest that it is more economic and effective to construct labeled data with partial annotation (PA). A lot of research effort has been attracted to obtain partially-labeled data for different ∗ This paper aims to thoroughly study this issue and make systematic comparison on differ"
I17-1006,W02-1001,0,0.150548,"d we only need to disable some illegal combination operations during dynamic programming. LTPar can also directly learn from PA in a similar way, as shown in Algorithm 1. Constrained decoding is performed to find a pseudo gold-standard reference (line 8). It is more complicate to design constrained decoding for transition-based parsing Directly training parsers with PA As described in Li et al. (2014), CRF parsers such as LLGPar and Biaffine can naturally learn 51 train-1K train-39K #Sentence #Token 1,000 24,358 dev the beam size is 64 and the standard early update is adopted during training (Collins, 2002). For LGPar and LTPar, averaged perceptron is adopted (Collins, 2002). For Biaffine, we directly adopt most hyperparameters of the released code of Dozat and Manning (2017), only removing the components related with dependency labels, since we focus on unlabeled dependency parsing in this work. The LSTM (two forward plus two backward) layers all use 300-dimension hidden cells. Dropout with ratio of 0.75 is applied to most layers before output. The two MLPs both have 100-dimension outputs without hidden layer. Adam optimization is adopted with α1 = α2 = 0.9. For GN3Par, we follow Daniel et al."
I17-1006,P08-1109,0,0.105003,"Missing"
I17-1006,W15-2202,0,0.019127,"conduct experiments on Penn Treebank under three different settings for simulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert pa"
I17-1006,P09-1042,0,0.0314207,", most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar)"
I17-1006,P10-1002,0,0.0598798,"Missing"
I17-1006,P13-1075,0,0.0606609,"Missing"
I17-1006,P10-1001,0,0.026383,"h effort has been attracted to obtain partially-labeled data for different ∗ This paper aims to thoroughly study this issue and make systematic comparison on different approaches for dependency parsing with PA. In sumCorrespondence author 49 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 49–58, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP mary, we make the following contributions. Dynamic programming based exact search are usually applied to find the optimal tree (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). Biaffine belongs to the first-order model and only incorporates scores of single dependencies. In contrast, for LLGPar and LGPar, we follow Li et al. (2014) and adopt the second-order model of McDonald and Pereira (2006) considering scores of single dependencies and adjacent siblings. Biaffine and LLGPar both belong to CRF parser. Please note that the original Biaffine is locally trained on each word. In this work, we follow Ma and Hovy (2017) and add a global CRF loss in the projective case, in order to directly use the proposed approach of Li et al. (2014). In other words, we extend the or"
I17-1006,P92-1017,0,0.548641,"Missing"
I17-1006,C12-2067,0,0.0303579,"oding is also used for completing partial trees. We conduct experiments on Penn Treebank under three different settings for simulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency"
I17-1006,N07-1051,0,0.0542854,"Missing"
I17-1006,C14-1075,1,0.641626,"om the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar) with PA based on a forest-based objec"
I17-1006,P02-1035,0,0.274531,"w) // Unconstrained decoding: LGPar 6: a− = arg maxa→d∈Y(xj ) Score(xj , a → d; w) // Unconstrained decoding: LTPar 7: d+ = arg maxd∈Y(xj ,dp ) Score(xj , d; w) // Constrained decoding: LGPar 8: j a+ = arg maxa→d∈Y(xj ,dp ) Score(xj , a → d; w) // Constrained decoding: LTPar j 9: wk+1 = wk + f (x, d+ ) − f (x, d− ) // Update: LGPar 10: wk+1 = wk + f (x, a+ ) − f (x, a− ) // Update: LTPar 11: k =k+1 12: end for 13: end for 2.2 Transition-based Approach from PA based on the idea of ambiguous labeling, which allows a sentence to have multiple parse trees (forest) as its gold-standard reference (Riezler et al., 2002; Dredze et al., 2009; T¨ackstr¨om et al., 2013). First, a partial tree dp is converted into a forest by adding all possible dependencies pointing to remaining words without heads, with the constraint that a newly added dependency does not violate existing ones in dp . The forest can be formally defined as F(x, dp ) = {d : d ∈ Y(x), dp ⊆ d}, whose conditional probability is the sum of probabilities of all trees that it contains: The transition-based method builds a dependency by applying sequence of shift/reduce actions a, and factorizes the score of a tree into the sum of scores of each actio"
I17-1006,P16-1033,1,0.853955,"enn Treebank under three different settings for simulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into f"
I17-1006,P10-1037,0,0.22981,"-based parser (LTPar). For the test phase, constrained decoding is also used for completing partial trees. We conduct experiments on Penn Treebank under three different settings for simulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc"
I17-1006,D14-1093,1,0.936317,"d other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar) with PA based on a forest-based objective, showing promising results. Meanwhile, it is still unclear how PAs can be used by other main-st"
I17-1006,W09-1104,0,0.197058,"ulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased depe"
I17-1006,I17-1007,0,0.0144737,"ic programming based exact search are usually applied to find the optimal tree (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). Biaffine belongs to the first-order model and only incorporates scores of single dependencies. In contrast, for LLGPar and LGPar, we follow Li et al. (2014) and adopt the second-order model of McDonald and Pereira (2006) considering scores of single dependencies and adjacent siblings. Biaffine and LLGPar both belong to CRF parser. Please note that the original Biaffine is locally trained on each word. In this work, we follow Ma and Hovy (2017) and add a global CRF loss in the projective case, in order to directly use the proposed approach of Li et al. (2014). In other words, we extend the original Biaffine Parser described in Dozat and Manning (2017) by adding a CRF layer. Under the CRF model, the conditional probability of d given x is: • We present a general framework for directly training GN3Par, LGPar and LTPar with PA based on constrained decoding. The basic idea is to use the current feature weights to parse the sentence under the PA-constrained search space, and use the best parse as a pseudo gold-standard reference for feat"
I17-1006,W13-5711,0,0.0354672,"Missing"
I17-1006,N13-1126,0,0.0556355,"Missing"
I17-1006,D14-1097,0,0.0698373,"Missing"
I17-1006,W03-3023,0,0.070973,"et al., 2009; T¨ackstr¨om et al., 2013). First, a partial tree dp is converted into a forest by adding all possible dependencies pointing to remaining words without heads, with the constraint that a newly added dependency does not violate existing ones in dp . The forest can be formally defined as F(x, dp ) = {d : d ∈ Y(x), dp ⊆ d}, whose conditional probability is the sum of probabilities of all trees that it contains: The transition-based method builds a dependency by applying sequence of shift/reduce actions a, and factorizes the score of a tree into the sum of scores of each action in a (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011): Score(x, d; w) = Score(x, a → d; w) ∑|a| = Score(x, ci , ai ; w) i=1 (6) where ai is the action taken at step i and ci is the configuration status after taking action a1 ...ai−1 . Transition-based methods use inexact beam search to find a highest-scoring action sequence. GN3Par uses a neural network to predict scores of different actions given a state (Chen and Manning, 2014; Andor et al., 2016). First, 48 atomic features are embeded and concatenated as the input layer. Then, two hidden layers are applied to get the scores of all feasible actions. Unlike"
I17-1006,P13-2109,0,0.0494961,"Missing"
I17-1006,D14-1010,0,0.149707,"rmance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar) with PA based on a forest-based objective, showing promising results. Meanwhile, it is still unclear how PAs can be used by other main-stream dependency parsers, such as the traditi"
I17-1006,P05-1012,0,0.118936,"nstruct labeled data with partial annotation (PA). A lot of research effort has been attracted to obtain partially-labeled data for different ∗ This paper aims to thoroughly study this issue and make systematic comparison on different approaches for dependency parsing with PA. In sumCorrespondence author 49 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 49–58, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP mary, we make the following contributions. Dynamic programming based exact search are usually applied to find the optimal tree (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). Biaffine belongs to the first-order model and only incorporates scores of single dependencies. In contrast, for LLGPar and LGPar, we follow Li et al. (2014) and adopt the second-order model of McDonald and Pereira (2006) considering scores of single dependencies and adjacent siblings. Biaffine and LLGPar both belong to CRF parser. Please note that the original Biaffine is locally trained on each word. In this work, we follow Ma and Hovy (2017) and add a global CRF loss in the projective case, in order to directly use the pro"
I17-1006,J11-1005,1,0.858367,"Missing"
I17-1006,E06-1011,0,0.0639672,"th partial annotation (PA). A lot of research effort has been attracted to obtain partially-labeled data for different ∗ This paper aims to thoroughly study this issue and make systematic comparison on different approaches for dependency parsing with PA. In sumCorrespondence author 49 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 49–58, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP mary, we make the following contributions. Dynamic programming based exact search are usually applied to find the optimal tree (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). Biaffine belongs to the first-order model and only incorporates scores of single dependencies. In contrast, for LLGPar and LGPar, we follow Li et al. (2014) and adopt the second-order model of McDonald and Pereira (2006) considering scores of single dependencies and adjacent siblings. Biaffine and LLGPar both belong to CRF parser. Please note that the original Biaffine is locally trained on each word. In this work, we follow Ma and Hovy (2017) and add a global CRF loss in the projective case, in order to directly use the proposed approach of Li et al."
I17-1006,P11-2033,1,0.820633,"First, a partial tree dp is converted into a forest by adding all possible dependencies pointing to remaining words without heads, with the constraint that a newly added dependency does not violate existing ones in dp . The forest can be formally defined as F(x, dp ) = {d : d ∈ Y(x), dp ⊆ d}, whose conditional probability is the sum of probabilities of all trees that it contains: The transition-based method builds a dependency by applying sequence of shift/reduce actions a, and factorizes the score of a tree into the sum of scores of each action in a (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011): Score(x, d; w) = Score(x, a → d; w) ∑|a| = Score(x, ci , ai ; w) i=1 (6) where ai is the action taken at step i and ci is the configuration status after taking action a1 ...ai−1 . Transition-based methods use inexact beam search to find a highest-scoring action sequence. GN3Par uses a neural network to predict scores of different actions given a state (Chen and Manning, 2014; Andor et al., 2016). First, 48 atomic features are embeded and concatenated as the input layer. Then, two hidden layers are applied to get the scores of all feasible actions. Unlike the traditional perceptron-like train"
I17-1006,P15-1134,0,0.0363309,"Missing"
I17-1006,W11-2917,0,0.165393,"e test phase, constrained decoding is also used for completing partial trees. We conduct experiments on Penn Treebank under three different settings for simulating PA, i.e., random, most uncertain, and divergent outputs from the five parsers. The results show that LLGPar is most effective in directly learning from PA, and other parsers can achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only"
I17-1006,W03-3017,0,0.110861,"t al., 2013). First, a partial tree dp is converted into a forest by adding all possible dependencies pointing to remaining words without heads, with the constraint that a newly added dependency does not violate existing ones in dp . The forest can be formally defined as F(x, dp ) = {d : d ∈ Y(x), dp ⊆ d}, whose conditional probability is the sum of probabilities of all trees that it contains: The transition-based method builds a dependency by applying sequence of shift/reduce actions a, and factorizes the score of a tree into the sum of scores of each action in a (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011): Score(x, d; w) = Score(x, a → d; w) ∑|a| = Score(x, ci , ai ; w) i=1 (6) where ai is the action taken at step i and ci is the configuration status after taking action a1 ...ai−1 . Transition-based methods use inexact beam search to find a highest-scoring action sequence. GN3Par uses a neural network to predict scores of different actions given a state (Chen and Manning, 2014; Andor et al., 2016). First, 48 atomic features are embeded and concatenated as the input layer. Then, two hidden layers are applied to get the scores of all feasible actions. Unlike the tradition"
I17-1006,J14-2001,0,0.0967859,"n achieve best performance when PAs are completed into full trees by LLGPar. 1 $0 I1 saw2 Sarah3 with4 a5 telescope6 Figure 1: An example partial tree, where only the heads of “saw” and “with” are given. tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti`eres, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1) gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to build dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a traditional log-linear graphbased dependency parser (LLGPar) with PA based on a forest-based objective, showing promising results. Meanwhile, it is still unclear how PAs can be used by other main-stream dependency pars"
I17-1006,P99-1010,0,\N,Missing
I17-1006,D14-1082,0,\N,Missing
I17-1006,D07-1101,0,\N,Missing
I17-1052,N16-1174,0,0.0201665,"el and train the top sentiment analysis model. All models are trained by minimizing the sum of cross-entropy loss and a L2 regularization loss of all trainable weights ∆W . Target-dependent Sentiment Model We use the attention-based model of Liu and Zhang (2017) as our top level model. The overall structure is shown in Figure 5. Given a sentence, it first uses several BiLSTM layers to learn its syntactic features, and then an attention layer is used to select the relative wegihts of the words according to the target entity over the untargeted words in the whole sentence (Bahdanau et al. 2014; Yang et al. 2016). In particular, for a target word, it applies the target word hidden vector to find a weight forevery word (except the target words) in the sentence (see Figure 5). The model also uses a BiLSTM to represent the feature layer from bottom syntactic model fb (b) and the word embedding wet (w) of a word sequence w1 , w2 , ..., wn as the hidden vector of each word. loss = 1 n n P i σ(yi , yi0 ) + λ2 ||∆W ||2 , (11) The model feature inputs (word embeddings, POS-tag embeddings) are the sum of a trainable embedding and a pre-trained (or learned) embedding. All the weight matrix will be initialized w"
I17-1052,P16-1147,0,0.0678698,"Missing"
I17-1052,D16-1070,1,0.814639,"should be useful for sentiment analysis given a target, since target-related semantic information such as predicate-argument structure information is contained in syntactic structures. The main issue of Dong et al. (2014)’s method is that explicit syntactic structures are inaccurate and noisy. We try to avoid this issue by using implicit syntactic information, by integrating the hidden feature layers of a state-of-the-art neural dependency parsing as features to the state-of-the-art targeted sentiment classification models of Liu and Zhang (2017), using neural stacking (Zhang and Weiss 2016; Chen et al. 2016). The main structure of our model is shown in Figure 2. We choose the parser of Dozat and Manning 2 Model As shown in Figure 2, our neural stacking model consists of two brief components: a bottom level syntactic model for obtaining the syntactic information and a top level sentiment model for targetdependent sentiment classification. 2.1 Input representation Given an input sentence, we first obtain its word representations. In particular, we train two separate word embedding sets for the bottom level syntactic model and top level sentiment model, respectively, denoted as web and wet , respect"
I17-1052,J11-1005,1,0.792449,"in Table 2. A best model on the devset is saved for the neural stacking bottom syntax model. Once obtaining the pre-trained bottom syntax model, we build the top sentiment model based on intermediate output syntax model features fb and top word embedding wet . Data We conduct experiments on two datasets, one being the training/dev/test dataset of Zhang et al. (2016) (Z-Set), which consists of the MPQA corpus1 and Mitchell et al. (2013)’s corpus2 , the other being the dataset of the benchmark training/test dataset of ? (T-Set), we label these datasets’ POStags with the open parser tools ZPar (Zhang and Clark, 2011). Two sets of word embedding are used in this experiment: The GloVe3 (Pennington et al., 2014) twitter embedding (100 dimensions) for the bottom model, and the GloVe twit3.3 Hyper-parameters Embedding Size: Our embedding is a superposition of a trainable and a pre-trained word embedding. We fixed the word embedding dimension of web and wet to 100 and 200, respectively to match two pre-trained GloVe word embeddings set from Pennington et al. (2014). 1 http://mpqa.cs.pitt.edu/corpora/mpqa corpus/ http://www.m-mitchell.com/code/index.html 3 https://nlp.stanford.edu/projects/glove/ 2 520 Models PO"
I17-1052,P14-2009,0,0.155984,"vements on various benchmark datasets. We obtain the best accuracies on two different test sets for targeted sentiment. 1 Table 1: Target-dependent sentiment analysis - + 0 L Left Context ⊕ Target R Right Context w1 w2 w3 w4 w5 w6 w7 w8 w9 w10 w11 w12 w13 w14 Figure 1: Sentence level context the target Twitter and a negative (−) sentiment label on the target Facebook. The task has been addressed using neural network models, which learn target-specific representations of the input sentence. These representations are then used for predicting target-dependent sentiment polarities. In particular, Dong et al. (2014) derive the syntactic structure of input sentence using a dependency grammar, before transforming the tree structure to a target-centered form. A recursive neural network is used to transform the dependency syntax of a sentence into a target-specific vector for sentiment classification. More recently Vo and Zhang (2015) split the input sentence into three segments, with the target entity mention being in the center, and its left and right contexts surrounding it, as shown in Figure 1. Introduction Target-dependent sentiment analysis investigates the problem of assigning sentiment polarity labe"
I17-1052,P81-1022,0,0.740008,"Missing"
I17-1052,P11-1016,0,0.109046,"Missing"
I17-1052,D15-1025,0,0.0214339,"Missing"
I17-1052,E17-2091,1,0.902556,"owed Vo and Zhang (2015), avoiding the use of syntactic information explicitly. Zhang et al. (2016) applied a bidirectional Gated RNN to learn a dense representation of the input sentence, and then use a threeway gated network structure to integrate target entity mention and its left and right contexts. The final resulting representation is used for softmax classification. Tang et al. (2015) also use a RNN (LSTM) to represent the input sentence, yet directly integrating the target embedding to each hidden state for deriving a target-specific vector, which is used for sentiment classification. Liu and Zhang (2017) extended both Zhang et al. (2016) and Tang et al. (2015) by introducing the attention mechanism, obtaining the best accuracies on both datasets so far. Intuitively, syntactic information should be useful for sentiment analysis given a target, since target-related semantic information such as predicate-argument structure information is contained in syntactic structures. The main issue of Dong et al. (2014)’s method is that explicit syntactic structures are inaccurate and noisy. We try to avoid this issue by using implicit syntactic information, by integrating the hidden feature layers of a sta"
I17-1052,D13-1171,0,0.0674043,"Missing"
I17-1052,D14-1162,0,0.0789272,"Missing"
I17-1082,N15-1012,1,0.785249,"ized in a cluster graph (CG) as shown in Figure 4 in which nodes represent partial trees, and edge weights represent the combination probability of partial trees represented by the edge nodes calculated using the Equation 4. An edge exists between two nodes if the partial trees at the nodes contain mention about same named entity. From cluster graph we try to extract a connected subgraph (SG) which maximizes the Syntactic Linearization We make use of the syntactic linearization model proposed by Puduppully et al. (2016) to linearize the input set of partial trees. Puduppully et al. (2016) and Liu et al. (2015) propose a transitionbased word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, yields a sentence together with its dependency parse tree that conforms to input syntactic constraints (Zhang, 2013). The system is flexible with respect to input constraints, performing abstract word ordering when no constraints are given, but gives increasingly confined outputs when more POS and dependency relations are specified. We retrain their model1 using autoparsed data obtained using Stanford Dependency Parser. 1 (6) where ShannonEnt is"
I17-1082,W09-1801,0,0.0286989,"rent summary. It can also result in a wrong inference to the reader due to out of context sentence usage. In contrast, abstractive summarization techniques can generate a noisefree summary out of most relevant information in the input corpus. A subset of previous extractive summarization approaches utilized parsed sentence structures to execute noise pruning while extracting content for summary (Morita et al., 2013; Berg-Kirkpatrick et al., 2011). As a first step towards abstracting content for summary generation, sentence compression techniques were introduced (Lin, 2003; Zajic et al., 2006; Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013), but these techniques can merely prune noise, and cannot combine related facts from different sentences to generate new ones. (Banerjee et al., 2015) suggests a better way of doing sentence compression without harming linguistic quality. Recent work attempts to solve the problem of abstractive multi-document summarization (Bing et al., 2015; Li, 2015), claiming that the method has the advantage of generating new sentences. Bing et al. (2015) extracts relevant noun phrases and verb phrases and recombines them to generate new sentences whil"
I17-1082,P13-1020,0,0.0168147,"e to the reader due to out of context sentence usage. In contrast, abstractive summarization techniques can generate a noisefree summary out of most relevant information in the input corpus. A subset of previous extractive summarization approaches utilized parsed sentence structures to execute noise pruning while extracting content for summary (Morita et al., 2013; Berg-Kirkpatrick et al., 2011). As a first step towards abstracting content for summary generation, sentence compression techniques were introduced (Lin, 2003; Zajic et al., 2006; Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013), but these techniques can merely prune noise, and cannot combine related facts from different sentences to generate new ones. (Banerjee et al., 2015) suggests a better way of doing sentence compression without harming linguistic quality. Recent work attempts to solve the problem of abstractive multi-document summarization (Bing et al., 2015; Li, 2015), claiming that the method has the advantage of generating new sentences. Bing et al. (2015) extracts relevant noun phrases and verb phrases and recombines them to generate new sentences while Li (2015) system make use of semantic link network on"
I17-1082,P13-1101,0,0.0178577,"ing et al., 2015; Li, 2015). Extractive summarization has the advantage of output fluency due to direct use of human-written texts. However, extractive summarization cannot ensure a noise free and coherent summary. It can also result in a wrong inference to the reader due to out of context sentence usage. In contrast, abstractive summarization techniques can generate a noisefree summary out of most relevant information in the input corpus. A subset of previous extractive summarization approaches utilized parsed sentence structures to execute noise pruning while extracting content for summary (Morita et al., 2013; Berg-Kirkpatrick et al., 2011). As a first step towards abstracting content for summary generation, sentence compression techniques were introduced (Lin, 2003; Zajic et al., 2006; Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013), but these techniques can merely prune noise, and cannot combine related facts from different sentences to generate new ones. (Banerjee et al., 2015) suggests a better way of doing sentence compression without harming linguistic quality. Recent work attempts to solve the problem of abstractive multi-document summarization (Bing et al., 2"
I17-1082,N16-1058,1,0.922299,"and to generate the sentences from scratch. We build a model to this end by leveraging syntactic dependencies. Input for our model is the set of syntactic dependency trees obtained by parsing sentences in the corpus to be summarized. Relevant and noise pruned partial tree structures are extracted from the set of dependency trees and different subsets of maximally relevant partial dependency structures are identified. Partial trees in different subsets are linearized to generate individual summary sentences. In this work, we utilize transition-based syntactic linearization approach proposed by Puduppully et al. (2016) to linearize a combination of partial trees and to generate a noise free summary sentence. The combinability of a set of partial trees to form a full dependency tree of a valid sentence is estimated using a generative model of syntactic dependency trees (Zhang et al., 2016). As a result, the model is allowed to exhibit its own learnt writing style while generating summary sentences. The summaries generated by our system are evaluated on the DUC 2004, DUC 2007 and TAC 2011 muti-document summarization data-sets. In addition, we relied on human evaluation to evaluate factual accuracy and linguis"
I17-1082,P11-1049,0,0.0216954,"2015). Extractive summarization has the advantage of output fluency due to direct use of human-written texts. However, extractive summarization cannot ensure a noise free and coherent summary. It can also result in a wrong inference to the reader due to out of context sentence usage. In contrast, abstractive summarization techniques can generate a noisefree summary out of most relevant information in the input corpus. A subset of previous extractive summarization approaches utilized parsed sentence structures to execute noise pruning while extracting content for summary (Morita et al., 2013; Berg-Kirkpatrick et al., 2011). As a first step towards abstracting content for summary generation, sentence compression techniques were introduced (Lin, 2003; Zajic et al., 2006; Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013), but these techniques can merely prune noise, and cannot combine related facts from different sentences to generate new ones. (Banerjee et al., 2015) suggests a better way of doing sentence compression without harming linguistic quality. Recent work attempts to solve the problem of abstractive multi-document summarization (Bing et al., 2015; Li, 2015), claiming that th"
I17-1082,E09-1089,0,0.0211778,"on and generates summary sentences exhibiting coher812 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 812–821, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP tive summarization with syntactic dependency trees, which entrust the summarization model to generate summary sentences without exploiting any kind of subsentential or phrasal sequential structures originally present in the input corpus. Our code is released at https://bitbucket. org/litton_kurisinkel/tree_sum 2 Related Work Text summarization can be achieved using extractive (Takamura and Okumura, 2009; Lin and Bilmes, 2011; Wang et al., 2008) and abstractive methods (Bing et al., 2015; Li, 2015). Extractive summarization has the advantage of output fluency due to direct use of human-written texts. However, extractive summarization cannot ensure a noise free and coherent summary. It can also result in a wrong inference to the reader due to out of context sentence usage. In contrast, abstractive summarization techniques can generate a noisefree summary out of most relevant information in the input corpus. A subset of previous extractive summarization approaches utilized parsed sentence struc"
I17-1082,P15-1153,0,0.341417,"l Joint Conference on Natural Language Processing, pages 812–821, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP tive summarization with syntactic dependency trees, which entrust the summarization model to generate summary sentences without exploiting any kind of subsentential or phrasal sequential structures originally present in the input corpus. Our code is released at https://bitbucket. org/litton_kurisinkel/tree_sum 2 Related Work Text summarization can be achieved using extractive (Takamura and Okumura, 2009; Lin and Bilmes, 2011; Wang et al., 2008) and abstractive methods (Bing et al., 2015; Li, 2015). Extractive summarization has the advantage of output fluency due to direct use of human-written texts. However, extractive summarization cannot ensure a noise free and coherent summary. It can also result in a wrong inference to the reader due to out of context sentence usage. In contrast, abstractive summarization techniques can generate a noisefree summary out of most relevant information in the input corpus. A subset of previous extractive summarization approaches utilized parsed sentence structures to execute noise pruning while extracting content for summary (Morita et al., 2"
I17-1082,P16-1046,0,0.0311407,"ty. Recent work attempts to solve the problem of abstractive multi-document summarization (Bing et al., 2015; Li, 2015), claiming that the method has the advantage of generating new sentences. Bing et al. (2015) extracts relevant noun phrases and verb phrases and recombines them to generate new sentences while Li (2015) system make use of semantic link network on basic semantic units (BSUs) to generate summary. Neither of these methods employ a learnt model to generate summary sentences. Instead, they make use sequential structures in the source text itself to construct the summary sentences. Cheng and Lapata (2016) propose a fully data driven approach using neuFigure 1: Overall approach ral network for single document summarization by extracting words. They have treated highlighted text in news articles consisting of very short bulleted lines on the web as summary of the corresponding article. A major challenge of using a fully data driven approach for multi-document abstractive summarization using neural network is the expensive task of creating dataset which can be used to jointly model extraction of relevant content and generation good quality summary sentences. But multidocument abstractive summariz"
I17-1082,D15-1219,0,0.339639,"on Natural Language Processing, pages 812–821, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP tive summarization with syntactic dependency trees, which entrust the summarization model to generate summary sentences without exploiting any kind of subsentential or phrasal sequential structures originally present in the input corpus. Our code is released at https://bitbucket. org/litton_kurisinkel/tree_sum 2 Related Work Text summarization can be achieved using extractive (Takamura and Okumura, 2009; Lin and Bilmes, 2011; Wang et al., 2008) and abstractive methods (Bing et al., 2015; Li, 2015). Extractive summarization has the advantage of output fluency due to direct use of human-written texts. However, extractive summarization cannot ensure a noise free and coherent summary. It can also result in a wrong inference to the reader due to out of context sentence usage. In contrast, abstractive summarization techniques can generate a noisefree summary out of most relevant information in the input corpus. A subset of previous extractive summarization approaches utilized parsed sentence structures to execute noise pruning while extracting content for summary (Morita et al., 2013; Berg-K"
I17-1082,P10-1058,0,0.0273203,"result in a wrong inference to the reader due to out of context sentence usage. In contrast, abstractive summarization techniques can generate a noisefree summary out of most relevant information in the input corpus. A subset of previous extractive summarization approaches utilized parsed sentence structures to execute noise pruning while extracting content for summary (Morita et al., 2013; Berg-Kirkpatrick et al., 2011). As a first step towards abstracting content for summary generation, sentence compression techniques were introduced (Lin, 2003; Zajic et al., 2006; Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013), but these techniques can merely prune noise, and cannot combine related facts from different sentences to generate new ones. (Banerjee et al., 2015) suggests a better way of doing sentence compression without harming linguistic quality. Recent work attempts to solve the problem of abstractive multi-document summarization (Bing et al., 2015; Li, 2015), claiming that the method has the advantage of generating new sentences. Bing et al. (2015) extracts relevant noun phrases and verb phrases and recombines them to generate new sentences while Li (2015) system make use"
I17-1082,N16-1035,0,0.422268,"res are extracted from the set of dependency trees and different subsets of maximally relevant partial dependency structures are identified. Partial trees in different subsets are linearized to generate individual summary sentences. In this work, we utilize transition-based syntactic linearization approach proposed by Puduppully et al. (2016) to linearize a combination of partial trees and to generate a noise free summary sentence. The combinability of a set of partial trees to form a full dependency tree of a valid sentence is estimated using a generative model of syntactic dependency trees (Zhang et al., 2016). As a result, the model is allowed to exhibit its own learnt writing style while generating summary sentences. The summaries generated by our system are evaluated on the DUC 2004, DUC 2007 and TAC 2011 muti-document summarization data-sets. In addition, we relied on human evaluation to evaluate factual accuracy and linguistic quality of generated summary sentences. To our knowledge this is the first work on multi-document abstracExisting work for abstractive multidocument summarization utilise existing phrase structures directly extracted from input documents to generate summary sentences. Th"
I17-1082,W04-1013,0,\N,Missing
I17-1082,W03-1101,0,\N,Missing
I17-1082,P11-1052,0,\N,Missing
J11-1005,W00-1201,0,0.124367,"Missing"
J11-1005,J93-1002,0,0.128678,"a stacking framework. Both Hall et al. (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models. In contrast, our parser combines two components in a single model, in which all parameters are trained consistently. 6. Phrase-Structure Parsing Phrase-structure parsing is the problem of producing the syntactic structure of an input sentence according to a phrase-structure grammar. An example phrase-structure parse tree is shown in Figure 11. Similar to dependency parsing, dominant approaches to phrase-structure parsing include the transition-based method (Briscoe and Carroll 1993), which builds an output parse tree by choosing a series of transition actions such as SHIFT and REDUCE, and the graph-based method (Collins 1999; Charniak 2000), which explores the search space of possible parse trees to find the best output according to graph-based scores. For English constituent parsing using the Penn Treebank, the best performing transition-based parser lags behind the current state-of-the-art (Sagae and Lavie 2005). However, for Chinese constituent parsing using the Chinese Treebank, Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy sc"
J11-1005,W06-2920,0,0.123179,"Missing"
J11-1005,W08-2102,0,0.0193679,"Missing"
J11-1005,W06-2925,0,0.029396,"Missing"
J11-1005,A00-2018,0,0.030083,"wo components in a single model, in which all parameters are trained consistently. 6. Phrase-Structure Parsing Phrase-structure parsing is the problem of producing the syntactic structure of an input sentence according to a phrase-structure grammar. An example phrase-structure parse tree is shown in Figure 11. Similar to dependency parsing, dominant approaches to phrase-structure parsing include the transition-based method (Briscoe and Carroll 1993), which builds an output parse tree by choosing a series of transition actions such as SHIFT and REDUCE, and the graph-based method (Collins 1999; Charniak 2000), which explores the search space of possible parse trees to find the best output according to graph-based scores. For English constituent parsing using the Penn Treebank, the best performing transition-based parser lags behind the current state-of-the-art (Sagae and Lavie 2005). However, for Chinese constituent parsing using the Chinese Treebank, Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds by using an SVM to make a single decision at each point in the parsing process. In Zhang and Clark (2009) we proposed a transition"
J11-1005,D09-1060,0,0.00765181,"are shown together with other systems in Table 16. In the table, each row represents a parsing model. Rows Yamada 2003 and MSTParser represent Yamada and Matsumoto (2003), and MSTParser with templates 1–6 from Table 14 (McDonald and Pereira 2006), respectively. Rows Transition and Combined represent our pure transition-based and combined parsers, respectively. Row Huang 2010 shows the recent work of Huang and Sagae (2010), which applies dynamic-programming packing to transition-based dependency parsing. Rows Koo 2008 and Chen 2009 represent the models of Koo, Carreras, and Collins (2008) and Chen et al. (2009), which perform semi-supervised learning by word-clustering and self-training, respectively. Columns Word and Complete show the precision of lexical Table 15 The training, development, and test data for English dependency parsing. Training Development Test Sections Sentences Words 2–21 22 23 39,832 1,700 2,416 950,028 40,117 56,684 135 Computational Linguistics Volume 37, Number 1 Table 16 Accuracy comparisons between various dependency parsers on English data. Word Complete Yamada 2003 Transition MSTParser Combined Huang 2010 90.3 91.4 91.5 92.1 92.1 38.4 41.8 42.1 45.4 – Koo 2008 Chen 2009 9"
J11-1005,C02-1126,0,0.0191446,"Missing"
J11-1005,J07-4004,1,0.840862,"g only on a decoder for each problem and using a trivial online update procedure for each training example. An advantage of the linear perceptron models we use is that they are global models, assigning a score to a complete hypothesis for each problem rather than assigning scores to parts which are then combined under statistical independence assumptions. Here we are following a recent line of work applying global discriminative models to tagging and wide-coverage parsing problems (Lafferty, McCallum, and Pereira 2001; Collins 2002; Collins and Roark 2004; McDonald, Crammer, and Pereira 2005; Clark and Curran 2007; Carreras, Collins, and Koo 2008; Finkel, Kleeman, and Manning 2008). The flexibility of our framework leads to competitive accuracies for each of the tasks we consider. For word segmentation, we show how the framework can accommodate a word-based approach, rather than the standard and more restrictive character-based tagging approaches. For POS-tagging, we consider joint segmentation and POS-tagging, showing that a single beam-search decoder can be used to achieve a significant accuracy boost over the pipeline baseline. For Chinese and English dependency parsing, we show how both graph-based"
J11-1005,W02-1001,0,0.598348,"Missing"
J11-1005,P04-1015,0,0.786358,"tures. The generalized perceptron is equally flexible, relying only on a decoder for each problem and using a trivial online update procedure for each training example. An advantage of the linear perceptron models we use is that they are global models, assigning a score to a complete hypothesis for each problem rather than assigning scores to parts which are then combined under statistical independence assumptions. Here we are following a recent line of work applying global discriminative models to tagging and wide-coverage parsing problems (Lafferty, McCallum, and Pereira 2001; Collins 2002; Collins and Roark 2004; McDonald, Crammer, and Pereira 2005; Clark and Curran 2007; Carreras, Collins, and Koo 2008; Finkel, Kleeman, and Manning 2008). The flexibility of our framework leads to competitive accuracies for each of the tasks we consider. For word segmentation, we show how the framework can accommodate a word-based approach, rather than the standard and more restrictive character-based tagging approaches. For POS-tagging, we consider joint segmentation and POS-tagging, showing that a single beam-search decoder can be used to achieve a significant accuracy boost over the pipeline baseline. For Chinese"
J11-1005,I05-3017,0,0.190435,"ur system without combination of character-based information, we call our segmentor in Section 3.1 the pure wordbased segmentor and the segmentor that uses character-based features the combined segmentor in our experimental sections. 3.4 Experiments We performed two sets of experiments. In the first set of experiments, we used the Chinese Treebank (CTB) data to study the speed/accuracy tradeoff by varying the size of the beam. In the second set of experiments, we used training and testing sets from the first and second international Chinese word segmentation bakeoffs (Sproat and Emerson 2003; Emerson 2005) to compare the accuracies to other models in the literature, including our segmentor of Zhang and Clark (2007). F-score is used as the accuracy measure: 2pr/(p + r), where precision p is the percentage of words in the decoder output that are segmented correctly, and recall r is the percentage of gold-standard output words that are correctly segmented by the decoder. CWS systems are evaluated by two types of tests. The closed tests require that the system is trained only with a designated training corpus. Any extra knowledge is not allowed, including common surnames, Chinese and Arabic numbers"
J11-1005,P08-1109,0,0.0637353,"Missing"
J11-1005,D07-1097,0,0.0132965,"Missing"
J11-1005,P08-1067,0,0.0131453,"ncy of a dynamicprogramming decoder is restricted by the range of features, due to its requirement for optimal substructure. For our combined dependency parser, the feature set makes a dynamic-programming decoder infeasibly slow. From this perspective, beam-search is in line with other recent research on the improvement of accuracies by incorporating non-local features via approximation, such as belief propagation for dependency parsing (Smith and Eisner 2008), integer linear programming for dependency parsing (Martins, Smith, and Xing 2009), and forest reranking for phrase-structure parsing (Huang 2008). The only prerequisite of the framework is an incremental process, which consumes the input sentence and builds the output structure using a sequence of actions. All four problems studied in the article were first turned into an incremental process, and then solved by applying the framework. The number of distinct actions for a problem is dependent on the complexity of the output. For word segmentation, there are only two actions (append or separate). For transition-based unlabeled dependency parsing, there are four actions (shift, arc-left, arc-right, and reduce). For joint segmentation and"
J11-1005,P10-1110,0,0.19138,"Missing"
J11-1005,P08-1102,0,0.0828904,"ith the current character in the middle. We call these methods character-based word segmentation. The advantage of character-based segmentation is that well-known tagging approaches can be applied directly to the CWS problem. There are various character-based models in the literature. They differ mainly in the learning algorithm and the features used. Several discriminative learning algorithms have been applied to the character-based systems. Examples include Xue (2003), Peng, Feng, and McCallum (2004), and Wang et al. (2006), which use maximum entropy and conditional random field models, and Jiang et al. (2008), which uses the perceptron model. The standard feature set is that defined by Ng and Low (2004), though other feature sets are reported to improve the accuracy (Zhao, Huang, and Li 2006). Zhao, Huang, and Li (2006) also showed that the best accuracy for conditional random field (CRF) models is given by using a set of six character segmentation tags, rather than the standard set {beginning, middle, end, single} shown previously. Standard search algorithms for sequence tagging have been applied to the decoding process, such as the dynamic-programming algorithm and beam-search. A disadvantage of"
J11-1005,C08-1049,0,0.0454952,"ith the current character in the middle. We call these methods character-based word segmentation. The advantage of character-based segmentation is that well-known tagging approaches can be applied directly to the CWS problem. There are various character-based models in the literature. They differ mainly in the learning algorithm and the features used. Several discriminative learning algorithms have been applied to the character-based systems. Examples include Xue (2003), Peng, Feng, and McCallum (2004), and Wang et al. (2006), which use maximum entropy and conditional random field models, and Jiang et al. (2008), which uses the perceptron model. The standard feature set is that defined by Ng and Low (2004), though other feature sets are reported to improve the accuracy (Zhao, Huang, and Li 2006). Zhao, Huang, and Li (2006) also showed that the best accuracy for conditional random field (CRF) models is given by using a set of six character segmentation tags, rather than the standard set {beginning, middle, end, single} shown previously. Standard search algorithms for sequence tagging have been applied to the decoding process, such as the dynamic-programming algorithm and beam-search. A disadvantage of"
J11-1005,D07-1123,0,0.0274584,"Missing"
J11-1005,P08-1068,0,0.0258046,"Missing"
J11-1005,P09-1058,0,0.188763,"ction 3. Due to the large accuracy gain from the baseline, our joint model performed much better. In summary, when compared with existing joint word segmentation and POStagging systems using cross-validation tests, our proposed model achieved the best accuracy boost from the pipelined baseline, and competitive overall accuracy. Our system based on the general framework of this article gave comparable accuracies to our multiple-beam system in Zhang and Clark (2008a), and a speed that is over an order of magnitude higher than the multiple-beam algorithm. 4.4.3 Test Results Using CTB5. We follow Kruengkrai et al. (2009) and split the CTB5 into training, development testing, and testing sets, as shown in Table 11. The data are used 128 Zhang and Clark Syntactic Processing Table 11 Training, development, and test data from CTB5 for joint word segmentation and POS-tagging. Training Dev Test Sections Sentences Words 1–270, 400–931, 1001–1151 301–325 271–300 18,085 350 348 493,892 6,821 8,008 to compare the accuracies of our joint system with models in the literature, and to draw the speed/accuracy tradeoff graph. Kruengkrai et al. (2009) made use of character type knowledge for spaces, numerals, symbols, alphabe"
J11-1005,P03-1056,0,0.0966093,"Missing"
J11-1005,P09-1039,0,0.0172078,"Missing"
J11-1005,P05-1012,0,0.175465,"Missing"
J11-1005,D07-1013,0,0.0352932,"Missing"
J11-1005,E06-1011,0,0.696994,"s on projective dependency parsing. An unlabeled dependency tree is a dependency tree without dependency labels such as Subj and Obj in Figure 8. The same techniques used by unlabeled dependency parsers can be applied to labeled dependency parsing. For example, a shift-reduce unlabeled dependency parser can be extended to perform labeled dependency parsing by splitting a single reduce action into a set of reduce actions each associated with a dependency label. Here we focus on unlabeled dependency parsing. Graph-based (McDonald, Crammer, and Pereira 2005; Carreras, Surdeanu, and Marquez 2006; McDonald and Pereira 2006) and transition-based (Yamada and Matsumoto 2003; Nivre et al. 2006) parsing algorithms offer two different approaches to data-driven dependency parsing. Given an input sentence, a graph-based algorithm finds the highest scoring parse tree from all possible outputs, scoring each complete tree, while a transition-based algorithm builds a parse by a sequence of actions, scoring each action individually. Although graph-based and transition-based parsers can be differentiated in various ways, we prefer to think in terms of the features used in the two approaches as the differentiating factor. In Z"
J11-1005,H05-1066,0,0.0583269,"Missing"
J11-1005,P07-2055,0,0.0184038,"tradeoff graph. Kruengkrai et al. (2009) made use of character type knowledge for spaces, numerals, symbols, alphabets, and Chinese and other characters. In the previous experiments, our system did not use any knowledge beyond the training data. To make the comparison fairer, we included knowledge of English letters and Arabic numbers in this experiment. During both training and decoding, English letters and Arabic numbers are segmented using rules, treating consecutive English letters or Arabic numbers as a single word. The results are shown in Table 12, where row N07 refers to the model of Nakagawa and Uchimoto (2007), rows J08a and J08b refer to the models of Jiang et al. (2008) and Jiang, Mi, and Liu (2008), and row K09 refers to the models of Kruengkrai et al. (2009). Columns SF and JF refer to segmentation and joint segmentation and tagging accuracies, respectively. Our system gave comparable accuracies to these recent works, obtaining the best (same as the error-driven version of K09) joint F-score. The accuracy/speed tradeoff graphs for the joint segmentor and POS-taggers, together with the baseline pipeline system, are shown in Figure 7. For each point in each curve, the development test data were u"
J11-1005,W04-3236,0,0.105735,"The advantage of character-based segmentation is that well-known tagging approaches can be applied directly to the CWS problem. There are various character-based models in the literature. They differ mainly in the learning algorithm and the features used. Several discriminative learning algorithms have been applied to the character-based systems. Examples include Xue (2003), Peng, Feng, and McCallum (2004), and Wang et al. (2006), which use maximum entropy and conditional random field models, and Jiang et al. (2008), which uses the perceptron model. The standard feature set is that defined by Ng and Low (2004), though other feature sets are reported to improve the accuracy (Zhao, Huang, and Li 2006). Zhao, Huang, and Li (2006) also showed that the best accuracy for conditional random field (CRF) models is given by using a set of six character segmentation tags, rather than the standard set {beginning, middle, end, single} shown previously. Standard search algorithms for sequence tagging have been applied to the decoding process, such as the dynamic-programming algorithm and beam-search. A disadvantage of character-based models is the use of limited contextual information. For these methods, context"
J11-1005,W06-2933,0,0.0101416,"Missing"
J11-1005,P08-1108,0,0.0120114,"l search. Johansson and Nugues (2007) also use beam search. Second, we use the perceptron to train whole sequences of transition actions globally, whereas MaltParser uses SVM to train each transition action locally. Our global training corresponds to beamsearch decoding, which searches for a globally optimal sequence of transition actions rather than an optimal action at each step. An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie 2006), which was reported to be useful in improving dependency parsing (Hall et al. 2007). A more recent approach (Nivre and McDonald 2008) combined MSTParser and MaltParser by using the output of one parser for features in the other in a stacking framework. Both Hall et al. (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models. In contrast, our parser combines two components in a single model, in which all parameters are trained consistently. 6. Phrase-Structure Parsing Phrase-structure parsing is the problem of producing the syntactic structure of an input sentence according to a phrase-structure grammar. An example phrase-structure parse tree is shown in Figure 11. Similar to dependen"
J11-1005,C04-1081,0,0.0388234,"Missing"
J11-1005,N07-1051,0,0.0622914,"Missing"
J11-1005,J01-2004,0,0.0167206,"classifier that optimizes each individual choice. Instead of greedy local decoding, we used beamsearch in the decoder. An early work that applies beam-search to constituent parsing is Ratnaparkhi (1999). The main difference between our parser and Ratnaparkhi’s is that we use a global discriminative model, whereas Ratnaparkhi’s parser has separate probabilities of actions chained together in a conditional model. Both our parser and the parser from Collins and Roark (2004) use a global discriminative model and an incremental parsing process. The major difference is that Collins and Roark, like Roark (2001), follow a top–down derivation strategy, whereas we chose to use a shift-reduce process which has been shown to give state-of-the-art accuracies for Chinese (Wang et al. 2006). In addition, we did not include a generative baseline model in the discriminative model, as did Collins and Roark (2004). 7. Discussion We have demonstrated in the previous sections that accuracies competitive with the state-of-the-art can be achieved by our general framework for Chinese word segmentation, joint word segmentation and POS-tagging, Chinese and English dependency parsing, and Chinese phrase-structure parsi"
J11-1005,W05-1513,0,0.160312,"cture parse tree is shown in Figure 11. Similar to dependency parsing, dominant approaches to phrase-structure parsing include the transition-based method (Briscoe and Carroll 1993), which builds an output parse tree by choosing a series of transition actions such as SHIFT and REDUCE, and the graph-based method (Collins 1999; Charniak 2000), which explores the search space of possible parse trees to find the best output according to graph-based scores. For English constituent parsing using the Penn Treebank, the best performing transition-based parser lags behind the current state-of-the-art (Sagae and Lavie 2005). However, for Chinese constituent parsing using the Chinese Treebank, Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds by using an SVM to make a single decision at each point in the parsing process. In Zhang and Clark (2009) we proposed a transition-based constituent parser for Chinese, which is based on the transition process of Wang et al. (2006). Rather than making a single decision at each processing step, our parser uses a global linear model Figure 11 An example Chinese lexicalized phrase-structure parse tree. 139 Co"
J11-1005,N06-2033,0,0.046777,"fferent from MaltParser in two aspects. First, we applied beam-search in decoding, which helps to prevent error propagation of local search. Johansson and Nugues (2007) also use beam search. Second, we use the perceptron to train whole sequences of transition actions globally, whereas MaltParser uses SVM to train each transition action locally. Our global training corresponds to beamsearch decoding, which searches for a globally optimal sequence of transition actions rather than an optimal action at each step. An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie 2006), which was reported to be useful in improving dependency parsing (Hall et al. 2007). A more recent approach (Nivre and McDonald 2008) combined MSTParser and MaltParser by using the output of one parser for features in the other in a stacking framework. Both Hall et al. (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models. In contrast, our parser combines two components in a single model, in which all parameters are trained consistently. 6. Phrase-Structure Parsing Phrase-structure parsing is the problem of producing the syntactic structure of an inp"
J11-1005,D08-1016,0,0.0094007,"Missing"
J11-1005,J96-3004,0,0.0957261,"Missing"
J11-1005,W03-1719,0,0.0675137,"nguish this system from our system without combination of character-based information, we call our segmentor in Section 3.1 the pure wordbased segmentor and the segmentor that uses character-based features the combined segmentor in our experimental sections. 3.4 Experiments We performed two sets of experiments. In the first set of experiments, we used the Chinese Treebank (CTB) data to study the speed/accuracy tradeoff by varying the size of the beam. In the second set of experiments, we used training and testing sets from the first and second international Chinese word segmentation bakeoffs (Sproat and Emerson 2003; Emerson 2005) to compare the accuracies to other models in the literature, including our segmentor of Zhang and Clark (2007). F-score is used as the accuracy measure: 2pr/(p + r), where precision p is the percentage of words in the decoder output that are segmented correctly, and recall r is the percentage of gold-standard output words that are correctly segmented by the decoder. CWS systems are evaluated by two types of tests. The closed tests require that the system is trained only with a designated training corpus. Any extra knowledge is not allowed, including common surnames, Chinese and"
J11-1005,W06-0121,0,0.1268,"ti-character word. The context for disambiguation is normally a five-character window with the current character in the middle. We call these methods character-based word segmentation. The advantage of character-based segmentation is that well-known tagging approaches can be applied directly to the CWS problem. There are various character-based models in the literature. They differ mainly in the learning algorithm and the features used. Several discriminative learning algorithms have been applied to the character-based systems. Examples include Xue (2003), Peng, Feng, and McCallum (2004), and Wang et al. (2006), which use maximum entropy and conditional random field models, and Jiang et al. (2008), which uses the perceptron model. The standard feature set is that defined by Ng and Low (2004), though other feature sets are reported to improve the accuracy (Zhao, Huang, and Li 2006). Zhao, Huang, and Li (2006) also showed that the best accuracy for conditional random field (CRF) models is given by using a set of six character segmentation tags, rather than the standard set {beginning, middle, end, single} shown previously. Standard search algorithms for sequence tagging have been applied to the decodi"
J11-1005,I05-1007,0,0.0212555,"Missing"
J11-1005,O03-4002,0,0.0606094,"Missing"
J11-1005,W03-3023,0,0.502499,"ore, the decoding process consists of 2n − 1 steps, and in each step all state items in the agenda have been built using the same number of actions. Our experiments showed that start-of-the-art accuracy can be achieved by this intuitive method of candidate comparison. 5.2 Experiments for English We used Penn Treebank 3 for our experiments, which was separated into the training, development, and test sets in the same way as McDonald, Crammer, and Pereira (2005), shown in Table 15. Bracketed sentences from the Treebank were translated into dependency structures using the head-finding rules from Yamada and Matsumoto (2003). Before parsing, POS-tags are assigned to the input sentence using our baseline POS -tagger of Zhang and Clark (2008a), which can be seen as the perceptron tagger of Collins (2002) with beam-search. Like McDonald, Crammer, and Pereira (2005), we evaluated the parsing accuracy by the precision of lexical heads (the percentage of input words, excluding punctuation, that have been assigned the correct parent) and by the percentage of complete matches, in which all words excluding punctuation have been assigned the correct parent. A set of development tests, including the convergence of the perce"
J11-1005,N06-2049,0,0.0265563,"Missing"
J11-1005,P07-1106,1,0.282662,"pproaches into a single model which outperforms both in isolation. Finally, for Chinese phrase-structure parsing, we describe a global model for a shift-reduce parsing algorithm, in contrast to current deterministic approaches which use only local models at each step of the parsing process. For all these tasks we present results competitive with the best results in the literature. In Section 2 we describe our general framework of the generic beam-search algorithm and the generalized perceptron. Then in the subsequent sections we describe each task in turn, based on conference papers including Zhang and Clark (2007, 2008a, 2008b, 2009, 2010), presented in our single coherent framework. We give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework. For the segmentation task, we also compare our beam-search framework with alternative decoding algorithms including an exact dynamic-programming method, showing that the beam-search method is significantly faster with comparable accuracy. For the joint segmentation and POS-tagging task, we present a novel solution using the framework in this article, and show that it gives"
J11-1005,P08-1101,1,0.0511217,"r single coherent framework. We give an updated set of results, plus a number of additional experiments which probe further into the advantages and disadvantages of our framework. For the segmentation task, we also compare our beam-search framework with alternative decoding algorithms including an exact dynamic-programming method, showing that the beam-search method is significantly faster with comparable accuracy. For the joint segmentation and POS-tagging task, we present a novel solution using the framework in this article, and show that it gives comparable accuracies to our previous work (Zhang and Clark 2008a), while being more than an order of magnitude faster. In Section 7 we provide further discussion of the framework based on the studies of the individual tasks. We present the main advantages of the framework, and give an analysis of the main reasons for the high speeds and accuracies achieved. We also discuss how this framework can be applied to a potential new task, and show that the comparability of candidates in the incremental process is an important factor to consider. In summary, we study a general framework for incremental structural prediction, showing how the framework can be tailor"
J11-1005,D08-1059,1,0.450126,"mpared with Ng and Low (2004), our baseline model gave slightly better accuracy, consistent with our previous observations about the word segmentors in Section 3. Due to the large accuracy gain from the baseline, our joint model performed much better. In summary, when compared with existing joint word segmentation and POStagging systems using cross-validation tests, our proposed model achieved the best accuracy boost from the pipelined baseline, and competitive overall accuracy. Our system based on the general framework of this article gave comparable accuracies to our multiple-beam system in Zhang and Clark (2008a), and a speed that is over an order of magnitude higher than the multiple-beam algorithm. 4.4.3 Test Results Using CTB5. We follow Kruengkrai et al. (2009) and split the CTB5 into training, development testing, and testing sets, as shown in Table 11. The data are used 128 Zhang and Clark Syntactic Processing Table 11 Training, development, and test data from CTB5 for joint word segmentation and POS-tagging. Training Dev Test Sections Sentences Words 1–270, 400–931, 1001–1151 301–325 271–300 18,085 350 348 493,892 6,821 8,008 to compare the accuracies of our joint system with models in the li"
J11-1005,W09-3825,1,0.917397,"raph-based method (Collins 1999; Charniak 2000), which explores the search space of possible parse trees to find the best output according to graph-based scores. For English constituent parsing using the Penn Treebank, the best performing transition-based parser lags behind the current state-of-the-art (Sagae and Lavie 2005). However, for Chinese constituent parsing using the Chinese Treebank, Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds by using an SVM to make a single decision at each point in the parsing process. In Zhang and Clark (2009) we proposed a transition-based constituent parser for Chinese, which is based on the transition process of Wang et al. (2006). Rather than making a single decision at each processing step, our parser uses a global linear model Figure 11 An example Chinese lexicalized phrase-structure parse tree. 139 Computational Linguistics Volume 37, Number 1 and beam-search decoding, and achieved competitive accuracy. This phrase-structure parser can be expressed as an instance of our general framework. 6.1 Instantiating the General Framework The incremental parsing process of our parser is based on the sh"
J11-1005,D10-1082,1,0.801835,"e-beam decoder, and our new way to decide the number of training iterations in this article. The single-beam results correspond to “Zhang 2007*” in Tables 4 and 5. 120 Zhang and Clark Syntactic Processing POS -tagged output. Joint word segmentation and POS -tagging is a method that addresses these problems. In Zhang and Clark (2008a) we proposed a joint word segmentor and POS-tagger using a multiple-beam decoder, and showed that it outperformed a pipelined baseline. We recently showed that comparable accuracies can be achieved by a single-beam decoder, which runs an order of magnitude faster (Zhang and Clark 2010). In this section, we describe our single-beam system using our general framework, and provide a detailed comparison with our multiple-beam and baseline systems of Zhang and Clark (2008a). 4.1 Instantiating the General Framework Given an input sentence, our joint segmentor and POS-tagger builds an output incrementally, one character at a time. When a character is processed, it is either concatenated with the last word in the partially built output, or taken as a new word. In the latter case, a POS-tag is assigned to the new word. When more characters are concatenated to a word, the POS-tag of"
J11-1005,W06-0127,0,0.0268495,"Missing"
J11-1005,J03-4003,0,\N,Missing
J11-1005,D07-1096,0,\N,Missing
J11-1005,W03-1726,0,\N,Missing
J15-3005,D11-1031,0,0.0548798,"Missing"
J15-3005,J05-3002,0,0.0496661,"Building, 15 JJ Thomson Avenue, Cambridge, UK. E-mail: stephen.clark@cl.cam.ac.uk. Submission received: 17 April 2013; revised version received: 23 April 2014; accepted for publication: 22 June 2014. doi:10.1162/COLI a 00229 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 3 those words. Additional annotation may also be provided with the input—for example, part-of-speech (POS) tags or syntactic dependencies. Applications that can benefit from better text generation algorithms include machine translation (Koehn 2010), abstractive text summarization (Barzilay and McKeown 2005), and grammar correction (Lee and Seneff 2006). Typically, statistical machine translation (SMT) systems (Chiang 2007; Koehn 2010) perform generation into the target language as part of an integrated system, which avoids the high computational complexity of independent word ordering. On the other hand, performing word ordering separately in a pipeline has many potential advantages. For SMT, it offers better modularity between adequacy (translation) and fluency (linearization), and can potentially improve target grammaticality for syntactically different languages (e.g., Chinese and English). M"
J15-3005,W11-2832,0,0.0325166,"Missing"
J15-3005,C10-1009,0,0.143121,"Missing"
J15-3005,C10-1012,0,0.236432,"Missing"
J15-3005,C04-1180,1,0.0644539,"hat, with this new approach, negative examples can be expanded during training and a single beam applied to the chart, resulting in a conceptually simpler and more effective training algorithm and decoder. 3. CCG-Based Word Ordering 3.1 The CCG Grammar We were motivated to use CCG as one of the grammar formalisms for our syntax-based realization system because of its successful application to a number of related tasks, such as wide-coverage parsing (Hockenmaier 2003; Clark and Curran 2007b; Auli and Lopez 2011), semantic parsing (Zettlemoyer and Collins 2005), wide-coverage semantic analysis (Bos et al. 2004), and generation itself (Espinosa, White, and Mehay 2008). The grammar formalism has been described in detail in those papers, and so here we provide only a short description. CCG (Steedman 2000) is a lexicalized grammar formalism that associates words with lexical categories. Lexical categories are detailed grammatical labels, typically expressing subcategorization information. During CCG parsing, and during our search procedure, categories are combined using CCG’s combinatory rules. For example, a verb phrase in English (SNP) can combine with an NP to its left, in this case using the combin"
J15-3005,J98-2004,0,0.113266,"derivation. Hypotheses are constructed bottom–up: starting from single words, smaller phrases are combined into larger ones according to CCG rules. To allow the combination of hypotheses, we use an additional structure to store a set of hypotheses that have been expanded, which we call accepted 506 Zhang and Clark Discriminative Syntax-Based Word Ordering hypotheses. When a hypothesis from the agenda is expanded, it is combined with all accepted hypotheses in all possible ways to produce new hypotheses. The data structure for accepted hypotheses is similar to that used for best-first parsing (Caraballo and Charniak 1998), and we adopt the term chart for this structure. However, note there are important differences to the parsing problem. First, the parsing problem has a fixed word order and is considerably simpler than the word ordering problem we are tackling. Second, although we use the term chart, the structure for accepted hypotheses is not a dynamic programming chart in the same way as for the parsing problem. In our previous papers (Zhang and Clark 2011; Zhang, Blackwood, and Clark 2012), we applied a set of beams to this structure, which makes it similar to the data structure used for phrase-based MT d"
J15-3005,I05-1015,0,0.200152,"Missing"
J15-3005,P07-1002,0,0.0661321,"uage as part of an integrated system, which avoids the high computational complexity of independent word ordering. On the other hand, performing word ordering separately in a pipeline has many potential advantages. For SMT, it offers better modularity between adequacy (translation) and fluency (linearization), and can potentially improve target grammaticality for syntactically different languages (e.g., Chinese and English). More importantly, a standalone word ordering component can in principle be applied to a wide range of text generation tasks, including transfer-based machine translation (Chang and Toutanova 2007). Most word ordering systems use an n-gram language model, which is effective at controling local fluency. Syntax-based language models, in particular dependency language models (Xu, Chelba, and Jelinek 2002), are sometimes used in an attempt to improve global fluency through the capturing of long-range dependencies. In this article, we take a syntax-based approach and consider two grammar formalisms: Combinatory Categorial Grammar (CCG) and dependency grammar. Our system also employs a discriminative model. Coupled with heuristic search, a strength of the model is that arbitrary features can"
J15-3005,J07-2003,0,0.343996,"on received: 23 April 2014; accepted for publication: 22 June 2014. doi:10.1162/COLI a 00229 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 3 those words. Additional annotation may also be provided with the input—for example, part-of-speech (POS) tags or syntactic dependencies. Applications that can benefit from better text generation algorithms include machine translation (Koehn 2010), abstractive text summarization (Barzilay and McKeown 2005), and grammar correction (Lee and Seneff 2006). Typically, statistical machine translation (SMT) systems (Chiang 2007; Koehn 2010) perform generation into the target language as part of an integrated system, which avoids the high computational complexity of independent word ordering. On the other hand, performing word ordering separately in a pipeline has many potential advantages. For SMT, it offers better modularity between adequacy (translation) and fluency (linearization), and can potentially improve target grammaticality for syntactically different languages (e.g., Chinese and English). More importantly, a standalone word ordering component can in principle be applied to a wide range of text generation"
J15-3005,W07-1202,1,0.867426,"Missing"
J15-3005,J07-4004,1,0.74908,"eses are expanded before the gold-standard goal hypothesis is found. In this case, the minimum number of hypotheses is expanded and the output is correct. The best-first search decoder is optimal not only with respect to accuracy but also speed. This ideal situation can hardly be met in practice, but it determines the goal of the training algorithm: to find a model that scores gold-standard hypotheses higher than non-gold ones. Learning-guided search places more challenges on the training of a discriminative model than standard structured prediction problems, for example, CKY parsing for CCG (Clark and Curran 2007b). If we take gold-standard hypotheses as positive training examples, and non-gold hypotheses as negative examples, then the training goal is to find a large separating margin between the scores of all positive examples and all negative examples. For CKY parsing, the highest-scored negative example can be found via optimal Viterbi decoding, according to the current model, and this negative example can be used in place of all negative examples during the updating of parameters. In contrast, our best-first search algorithm cannot find an output in reasonable time unless a good model has already"
J15-3005,P04-1015,0,0.359558,"Missing"
J15-3005,P08-1022,0,0.103695,"Missing"
J15-3005,P07-1041,0,0.0635731,"Missing"
J15-3005,N09-2057,0,0.152421,"Missing"
J15-3005,P10-1035,0,0.0140586,". For example, a verb phrase in English (SNP) can combine with an NP to its left, in this case using the combinatory rule of (backward) function application: NP SNP ⇒ S In addition to binary rule instances, such as this one, there are also unary rules that operate on a single category in order to change its type. For example, forward typeraising can change a subject NP into a complex category looking to the right for a verb phrase: NP ⇒ S/(SNP) Such a type-raised category can then combine with a transitive verb type using the rule of forward composition: S/(SNP) (SNP)/NP ⇒ S/NP Following Fowler and Penn (2010), we extract the grammar by reading rule instances directly from the derivations in CCGbank (Hockenmaier and Steedman 2007), rather than defining the combinatory rule schema manually as in Clark and Curran 508 Zhang and Clark Discriminative Syntax-Based Word Ordering (2007b). Hence the grammar we use can be thought of as a context-free approximation to the mildly content sensitive grammar arising from the use of generalized composition rules (Weir 1988). Hockenmaier (2003) contains a detailed description of the grammar that is obtained in this way, including the various unary type-changing rul"
J15-3005,N10-1115,0,0.0282392,"Missing"
J15-3005,W11-2833,0,0.0541311,"Missing"
J15-3005,P09-1091,0,0.367407,"onstration of the applicability of the system to more realistic text generation scenarios, we consider two further tasks for the dependency-based realization system. The first task considers a variety of input conditions for the dependency-based system, determined by two parameters. The first is whether POS information is provided for each word in the input multi-set. The second is whether syntactic dependencies between the words are provided. The extreme case is when all dependencies are provided, in which case the problem reduces to the tree linearization problem (Filippova and Strube 2009; He et al. 2009). However, the input can also lie between the two extremes of no- and full-dependency information. The second task is the NLG 2011 shared task, which provides a further demonstration of the practical utility of our system. The shared task is closer to a real realization scenario, in that lemmas, rather than inflected words, are provided as input. Hence some modifications are required to our system in order that it can perform some word inflection, as well as deciding on the ordering. The shared task data also uses labeled, rather than unlabeled, syntactic dependencies, and so the system was mo"
J15-3005,J07-3004,0,0.278928,"papers (Zhang and Clark 2011; Zhang, Blackwood, and Clark 2012), we applied a set of beams to this structure, which makes it similar to the data structure used for phrase-based MT decoding (Koehn 2010). However, we will show later that this structure is unnecessary when the model has more discriminative power, and a conceptually simpler single beam can be used. We will also investigate the possibility of applying dynamic-programming-style pruning to the chart. We now give an overview of the training algorithm, which is crucial to both the speed and accuracy of the resulting decoder. CCGBank (Hockenmaier and Steedman 2007) is used to train the model. For each training sentence, the corresponding CCGBank derivation together with all its sub-derivations are treated as gold-standard hypotheses. All other hypotheses that can be constructed from the same bag of words are non-gold hypotheses. From the generation perspective this assumption is too strong, because sentences can have multiple orderings (with multiple derivations) that are both grammatical and fluent. Nevertheless, it is the most feasible choice given the training data available. The efficiency of the decoding algorithm is dependent on the training algor"
J15-3005,N12-1015,0,0.0610609,"Missing"
J15-3005,P10-1110,0,0.0790259,"Missing"
J15-3005,W07-2416,0,0.0158856,"Missing"
J15-3005,P96-1027,0,0.192408,"ng. The shared task data also uses labeled, rather than unlabeled, syntactic dependencies, and so the system was modified to incorporate labels. The final result is that our system gives competitive BLEU scores, compared to the best-performing systems on the shared task. The structured prediction problem we solve is a very hard problem. Due to the use of syntax, and the search for a sentence together with a single CCG derivation or dependency tree, the search space is exponentially larger than the n-gram word permutation problem. No efficient algorithm exists for finding the optimal solution. Kay (1996) recognized the computational difficulty of chart-based generation, which has many similarities to the problem we address in his seminal paper. We tackle the high complexity by using learning-guided best-first search, exploring a small path in the whole search space, which contains the most likely structures according to the discriminative model. One of the contributions of this article is to introduce, and provide a discriminative solution to, this difficult structured prediction problem, which is an interesting machine learning problem in its own right. This article is based on, and signific"
J15-3005,J10-4005,0,0.178397,"Cambridge Computer Laboratory, William Gates Building, 15 JJ Thomson Avenue, Cambridge, UK. E-mail: stephen.clark@cl.cam.ac.uk. Submission received: 17 April 2013; revised version received: 23 April 2014; accepted for publication: 22 June 2014. doi:10.1162/COLI a 00229 © 2015 Association for Computational Linguistics Computational Linguistics Volume 41, Number 3 those words. Additional annotation may also be provided with the input—for example, part-of-speech (POS) tags or syntactic dependencies. Applications that can benefit from better text generation algorithms include machine translation (Koehn 2010), abstractive text summarization (Barzilay and McKeown 2005), and grammar correction (Lee and Seneff 2006). Typically, statistical machine translation (SMT) systems (Chiang 2007; Koehn 2010) perform generation into the target language as part of an integrated system, which avoids the high computational complexity of independent word ordering. On the other hand, performing word ordering separately in a pipeline has many potential advantages. For SMT, it offers better modularity between adequacy (translation) and fluency (linearization), and can potentially improve target grammaticality for synt"
J15-3005,P07-2045,0,0.00311972,"features can be defined to capture complex syntactic patterns in output hypotheses. The discriminative model is trained using syntactically annotated data. From the perspective of search, word ordering is a computationally difficult problem. Finding the best permutation for a set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the traveling salesman problem. In practice, exploring the whole search space of permutations is often prevented by adding constraints. In phrase-based machine translation (Koehn, Och, and Marcu 2003; Koehn et al. 2007), a distortion limit is used to constrain the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomialtime inference is feasible. In fluency improvement (Blackwood, de Gispert, and Byrne 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. In this article we begin by proposing a general system to solve the word ordering problem, which does not rely on constraints (which are typically ta"
J15-3005,N03-1017,0,0.0413498,"Missing"
J15-3005,P10-1001,0,0.0132787,"e decoding algorithm. Here a leaf edge refers to an input word with a POS tag, and a non-leaf edge refers to a phrase or sentence with its dependency tree. Edges are constructed bottom–up, by recursively joining two existing edges and adding an unlabeled dependency link between their head words. As for the CCG system, edges are scored by a global linear model: f (e) =  Φ(e) · θ |e|  is the parameter vector of the model. where Φ(e) represents the feature vector of e and θ Table 2 shows the feature templates we use, which are inspired by the rich feature templates used for dependency parsing (Koo and Collins 2010; Zhang and Nivre 2011). In the table, h, m, s, hl , hr , ml , and mr are the indices of words in the newly constructed edge, where h and m refer to the head and dependent of the newly constructed arc, s refers to the nearest sibling of m (on the same side of h), and hl , hr , ml , and mr refer to the left and rightmost dependents of h and m, respectively. WORD, POS, LVAL, and RVAL are maps from indices to word forms, POS , left valencies, and right valencies of words, respectively. Example feature instances extracted from the sentence in Figure 3 are shown in the example column. Because of th"
J15-3005,P13-1001,0,0.0356189,"Missing"
J15-3005,J93-2004,0,0.0516151,"Missing"
J15-3005,P02-1040,0,0.0981861,"lti-set (bag) of words and the output is an ordered sentence, together with its syntactic analysis (either CCG derivation or dependency tree, depending on the grammar formalism being used). Given an input, our system searches for the highestscored output, according to a syntax-based discriminative model. One advantage of this formulation of the reordering problem, which can perhaps be thought of as a “pure” text realization task, is that systems for solving it are easily evaluated, because all that is required is a set of sentences for reordering and a standard evaluation metric such as BLEU (Papineni et al. 2002). However, one potential criticism of the “pure” problem is that it is unclear how it relates to real realization tasks, since in practice (e.g., in statistical machine translation systems) the input does provide constraints on the possible output orderings. Our general formulation still allows task-specific contraints to be added if appropriate. Hence as a test of the flexibility of our system, 504 Zhang and Clark Discriminative Syntax-Based Word Ordering and a demonstration of the applicability of the system to more realistic text generation scenarios, we consider two further tasks for the d"
J15-3005,W96-0213,0,0.358539,"ng and Clark Discriminative Syntax-Based Word Ordering Table 2 (continued) dependency syntax example surface string patterns for complete sentences WORD (0), WORD (0) · WORD (1), WORD (size − 1), WORD (size − 1) · WORD (size − 2), POS (0), POS (0) · POS (1), POS (0) · POS (1) · POS (2), POS (size − 1), POS (size − 1) · POS (size − 2), POS (size − 1) · POS (size − 2) · POS (size − 3), (VBD, NNP, NN) the allowed POS tag is assigned. For unconstrained words, we assign all possible POS tags according to a tag dictionary compiled from the training data, following standard practice for POS-tagging (Ratnaparkhi 1996). When an edge is expanded, it is combined with all edges in the chart in all possible ways to generate new edges. Two edges can be combined by concatenation of the surface strings in both orders and, in each case, constructing a dependency link between their heads in two ways (corresponding to the two options for the head of the new link). When there is a head constraint on the dependent word, a dependency link can be constructed only if it is consistent with the constraint. This algorithm implements abstract word ordering, partial-tree linearization, and full tree linearization—all generaliz"
J15-3005,D08-1052,0,0.0633386,"Missing"
J15-3005,P07-1096,0,0.0899193,"Missing"
J15-3005,W08-2121,0,0.0158951,"Missing"
J15-3005,E09-1097,0,0.311155,"teedman 2007) and the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) for CCG and dependency data, respectively. CCGbank is the CCG version of the Penn Treebank. Standard splits were used for both: Sections 02–21 for training, Section 00 for development, and Section 23 for the final test. Table 3 gives statistics for the Penn Treebank. For the CCG experiments, original sentences from CCGBank are transformed into bags of words, with sequence information removed, and passed to our system as input data. The system outputs are compared to the original sentences for evaluation. Following Wan et al. (2009), we use the BLEU metric (Papineni et al. 2002) for string comparison. Although BLEU is not the perfect measure of fluency or grammaticality, being based on n-gram precision, it is currently widely used for automatic evaluation and allows us to compare directly with existing work (Wan et al. 2009). Note also that one criticism of BLEU for evaluating machine translation systems (i.e., that it can only register exact matches between the same words in the system and reference translation), does not apply here, because the system output always contains the same words as the original reference sent"
J15-3005,D09-1043,0,0.173991,"Missing"
J15-3005,J97-3002,0,0.105001,"otated data. From the perspective of search, word ordering is a computationally difficult problem. Finding the best permutation for a set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the traveling salesman problem. In practice, exploring the whole search space of permutations is often prevented by adding constraints. In phrase-based machine translation (Koehn, Och, and Marcu 2003; Koehn et al. 2007), a distortion limit is used to constrain the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomialtime inference is feasible. In fluency improvement (Blackwood, de Gispert, and Byrne 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. In this article we begin by proposing a general system to solve the word ordering problem, which does not rely on constraints (which are typically task-specific). In particular, we treat syntax-based word ordering as a structured prediction problem, for which the input is a multi-set"
J15-3005,P02-1025,0,0.121191,"Missing"
J15-3005,E12-1075,1,0.860952,"Missing"
J15-3005,P08-1101,1,0.743215,"Missing"
J15-3005,D10-1082,1,0.886146,"Missing"
J15-3005,J11-1005,1,0.0512939,"of chart-based generation, which has many similarities to the problem we address in his seminal paper. We tackle the high complexity by using learning-guided best-first search, exploring a small path in the whole search space, which contains the most likely structures according to the discriminative model. One of the contributions of this article is to introduce, and provide a discriminative solution to, this difficult structured prediction problem, which is an interesting machine learning problem in its own right. This article is based on, and significantly extends, three conference papers (Zhang and Clark 2011; Zhang, Blackwood, and Clark 2012; Zhang 2013). It includes a more detailed description and discussion of our guided-search approach to syntax-based word ordering, bringing together the CCG- and dependency-based systems under one unified framework. In addition, we discuss the limitations of our previous work, and show that a better model can be developed through scaling of the feature vectors. The resulting model allows fair comparison of constituents of different sizes, and enables the learning algorithms to expand negative examples during training, which leads to significantly improved resu"
J15-3005,P11-2033,1,0.903535,"Missing"
K17-1017,D14-1181,0,0.00305548,"niversity of Technology and Design {fei dong, jie yang}@mymail.sutd.edu.sg yue zhang@sutd.edu.sg Abstract a neural network model is employed to combine word information, resulting in a single dense vector form of the whole essay. A score is given based on a non-linear neural layer on the representation. Without handcrafted features, neural network models have been shown to be more robust than statistical models across different domains (Dong and Zhang, 2016). Both recurrent neural networks (Williams and Zipser, 1989; Mikolov et al., 2010) and convolutional neural networks (LeCun et al., 1998; Kim, 2014) have been used for modelling input essays. In particular, Alikaniotis et al. (2016) and Taghipour and Ng (2016) use a single-layer LSTM (Hochreiter and Schmidhuber, 1997) over the word sequence to model the essay, and Dong and Zhang (2016) use a two-level hierarchical CNN structure to model sentences and documents separately. It has been commonly understood that CNNs can capture local ngram information effectively, while LSTMs are strong in modelling long history. No previous work has compared the effectiveness of LSTMs and CNNs under the same settings for AES. To better understand the contra"
K17-1017,E17-2091,1,0.845592,"rast, we adopt the two-layer structure of Dong and Zhang (2016), comparing CNNs and LSTMs for modelling sentences and documents. Not all sentences contribute equally to the scoring of a given essay, and not all words contribute equally within a sentence. We adopt the neural attention model (Xu et al., 2015; Luong et al., 2015) to automatically calculate weights for convolution features of CNNs and hidden state values of LSTMs, which has been used for obtaining the most pertinent information for machine translation (Luong et al., 2015), sentiment analysis (Shin et al., 2016; Wang et al., 2016; Liu and Zhang, 2017) and other tasks. In our case, the attention mechanism can intuitively select sentences and grams that are more aligned with the props or obviously incorrect. To our knowledge, no prior Neural network models have recently been applied to the task of automatic essay scoring, giving promising results. Existing work used recurrent neural networks and convolutional neural networks to model input essays, giving grades based on a single vector representation of the essay. On the other hand, the relative advantages of RNNs and CNNs have not been compared. In addition, different parts of the essay can"
K17-1017,P16-1068,0,0.595027,"du.sg yue zhang@sutd.edu.sg Abstract a neural network model is employed to combine word information, resulting in a single dense vector form of the whole essay. A score is given based on a non-linear neural layer on the representation. Without handcrafted features, neural network models have been shown to be more robust than statistical models across different domains (Dong and Zhang, 2016). Both recurrent neural networks (Williams and Zipser, 1989; Mikolov et al., 2010) and convolutional neural networks (LeCun et al., 1998; Kim, 2014) have been used for modelling input essays. In particular, Alikaniotis et al. (2016) and Taghipour and Ng (2016) use a single-layer LSTM (Hochreiter and Schmidhuber, 1997) over the word sequence to model the essay, and Dong and Zhang (2016) use a two-level hierarchical CNN structure to model sentences and documents separately. It has been commonly understood that CNNs can capture local ngram information effectively, while LSTMs are strong in modelling long history. No previous work has compared the effectiveness of LSTMs and CNNs under the same settings for AES. To better understand the contrast, we adopt the two-layer structure of Dong and Zhang (2016), comparing CNNs and LS"
K17-1017,D15-1166,0,0.040182,"documents separately. It has been commonly understood that CNNs can capture local ngram information effectively, while LSTMs are strong in modelling long history. No previous work has compared the effectiveness of LSTMs and CNNs under the same settings for AES. To better understand the contrast, we adopt the two-layer structure of Dong and Zhang (2016), comparing CNNs and LSTMs for modelling sentences and documents. Not all sentences contribute equally to the scoring of a given essay, and not all words contribute equally within a sentence. We adopt the neural attention model (Xu et al., 2015; Luong et al., 2015) to automatically calculate weights for convolution features of CNNs and hidden state values of LSTMs, which has been used for obtaining the most pertinent information for machine translation (Luong et al., 2015), sentiment analysis (Shin et al., 2016; Wang et al., 2016; Liu and Zhang, 2017) and other tasks. In our case, the attention mechanism can intuitively select sentences and grams that are more aligned with the props or obviously incorrect. To our knowledge, no prior Neural network models have recently been applied to the task of automatic essay scoring, giving promising results. Existin"
K17-1017,D13-1180,0,0.427943,"to several categories, each score or score range is regarded as one class and the ordinary classification models are employed such as Naive Bayes (NB) and SVMs (Larkey, 1998; Rudner and Liang, 2002). In the regression scenario, each score is treated as continous values for the essay and regression models are considered, like linear regression, Bayesian linear ridge regression (Attali and Burstein, 2004; Phandi et al., 2015). In the preference ranking scenario, AES task is considered as a ranking problem in which pair-wise ranking and list-wise ranking are employed (Yannakoudakis et al., 2011; Chen and He, 2013; Cummins et al., 2016). The former considers the ranking between each pair of essays, while the latter considers the absolute ranking of each essay in the whole set. Formally, an AES model is trained to minimize the difference between its automatically output scores and human given scores on a set of training data: min N X f (yi∗ , yi ), i=1 Wi,j = (i − j)2 , (R − 1)2 (2) where i and j are the reference rating (assigned by a human rater) and the system rating (assigned by an AES system), respectively, and R is the number of possible ratings. An observed score matrix O is calculated such that"
K17-1017,P16-1075,0,0.251728,"es, each score or score range is regarded as one class and the ordinary classification models are employed such as Naive Bayes (NB) and SVMs (Larkey, 1998; Rudner and Liang, 2002). In the regression scenario, each score is treated as continous values for the essay and regression models are considered, like linear regression, Bayesian linear ridge regression (Attali and Burstein, 2004; Phandi et al., 2015). In the preference ranking scenario, AES task is considered as a ranking problem in which pair-wise ranking and list-wise ranking are employed (Yannakoudakis et al., 2011; Chen and He, 2013; Cummins et al., 2016). The former considers the ranking between each pair of essays, while the latter considers the absolute ranking of each essay in the whole set. Formally, an AES model is trained to minimize the difference between its automatically output scores and human given scores on a set of training data: min N X f (yi∗ , yi ), i=1 Wi,j = (i − j)2 , (R − 1)2 (2) where i and j are the reference rating (assigned by a human rater) and the system rating (assigned by an AES system), respectively, and R is the number of possible ratings. An observed score matrix O is calculated such that Oi,j refers to the numb"
K17-1017,D14-1162,0,0.0846655,"esenting texts. Both models use MoT pooling and are sentencelevel models. We compare our model (LSTMCNN-attent) with the baseline models to study CNN representing sentences and LSTM representing texts. Table 2: Statistics of the ASAP dataset; Range refers to score range and Med. refers to median scores. For genre, ARG specifies argumentative essays, RES means response essays and NAR denotes narrative essays. Word Embeddings We take the Stanford’s publicly available GloVe 50-dimensional embeddings2 as word pretrained embeddings, which are trained on 6 billion words from Wikipedia and web text (Pennington et al., 2014). During the training process, word embeddings are fine-tuned. Optimization We use RMSprop (Dauphin et al., 2015) as our optimizer to train the whole model. The initial learning rate η is set to 0.001 and momentum is set to 0.9. Dropout regularization is used to avoid overfitting and drop rate is 0.5. 5 Experiments 5.1 Setup Data The ASAP dataset is used as evaluation data of our AES system. The ASAP dataset consists of 8 different prompts of genres as listed in Table 2. There are no released labeled test data from the ASAP competition, thus we separate test set and development set from the tr"
K17-1017,D16-1115,1,0.842544,"A score is given based on a non-linear neural layer on the representation. Without handcrafted features, neural network models have been shown to be more robust than statistical models across different domains (Dong and Zhang, 2016). Both recurrent neural networks (Williams and Zipser, 1989; Mikolov et al., 2010) and convolutional neural networks (LeCun et al., 1998; Kim, 2014) have been used for modelling input essays. In particular, Alikaniotis et al. (2016) and Taghipour and Ng (2016) use a single-layer LSTM (Hochreiter and Schmidhuber, 1997) over the word sequence to model the essay, and Dong and Zhang (2016) use a two-level hierarchical CNN structure to model sentences and documents separately. It has been commonly understood that CNNs can capture local ngram information effectively, while LSTMs are strong in modelling long history. No previous work has compared the effectiveness of LSTMs and CNNs under the same settings for AES. To better understand the contrast, we adopt the two-layer structure of Dong and Zhang (2016), comparing CNNs and LSTMs for modelling sentences and documents. Not all sentences contribute equally to the scoring of a given essay, and not all words contribute equally within"
K17-1017,C16-1014,1,0.769242,"hen et al. (2010) formulates the AES task into a weakly supervised framework and employ a voting algorithm. Other recent work formulate the task as a preference ranking problem (Yannakoudakis et al., 2011; Phandi et al., 2015). Yannakoudakis et al. Our work is also inline with recent work on building hierarchical sentence-document level representations of documents. Li et al. (2015) build a hierarchical LSTM auto-encoder for documents. Yang et al. (2016) build hierarchical LSTM models with attention for document and Tang et al. (2015) use a hierarchical Gated RNN for sentiment classification. Ren and Zhang (2016) use hierarchical CNN-LSTM model for spam detection. We use a hierarchical CNNLSTM model for essay scoring, which is a regression task. 160 7 Conclusion Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation 9(8):1735–1780. We investigated a recurrent convolutional neural network to learn text representation and grade essays automatically. Our model treated input essays as sentence-document hierarchies, and employed attention pooling to find the pertinent words and sentences. Empirical results on ASAP essay data show that our model outperforms state-of-art ne"
K17-1017,D16-1193,0,0.185585,"stract a neural network model is employed to combine word information, resulting in a single dense vector form of the whole essay. A score is given based on a non-linear neural layer on the representation. Without handcrafted features, neural network models have been shown to be more robust than statistical models across different domains (Dong and Zhang, 2016). Both recurrent neural networks (Williams and Zipser, 1989; Mikolov et al., 2010) and convolutional neural networks (LeCun et al., 1998; Kim, 2014) have been used for modelling input essays. In particular, Alikaniotis et al. (2016) and Taghipour and Ng (2016) use a single-layer LSTM (Hochreiter and Schmidhuber, 1997) over the word sequence to model the essay, and Dong and Zhang (2016) use a two-level hierarchical CNN structure to model sentences and documents separately. It has been commonly understood that CNNs can capture local ngram information effectively, while LSTMs are strong in modelling long history. No previous work has compared the effectiveness of LSTMs and CNNs under the same settings for AES. To better understand the contrast, we adopt the two-layer structure of Dong and Zhang (2016), comparing CNNs and LSTMs for modelling sentences"
K17-1017,D15-1167,0,0.0112373,"into several categories of text quality based on content and style features. Chen et al. (2010) formulates the AES task into a weakly supervised framework and employ a voting algorithm. Other recent work formulate the task as a preference ranking problem (Yannakoudakis et al., 2011; Phandi et al., 2015). Yannakoudakis et al. Our work is also inline with recent work on building hierarchical sentence-document level representations of documents. Li et al. (2015) build a hierarchical LSTM auto-encoder for documents. Yang et al. (2016) build hierarchical LSTM models with attention for document and Tang et al. (2015) use a hierarchical Gated RNN for sentiment classification. Ren and Zhang (2016) use hierarchical CNN-LSTM model for spam detection. We use a hierarchical CNNLSTM model for essay scoring, which is a regression task. 160 7 Conclusion Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation 9(8):1735–1780. We investigated a recurrent convolutional neural network to learn text representation and grade essays automatically. Our model treated input essays as sentence-document hierarchies, and employed attention pooling to find the pertinent words and sentences. Empi"
K17-1017,D16-1058,0,0.0289125,"understand the contrast, we adopt the two-layer structure of Dong and Zhang (2016), comparing CNNs and LSTMs for modelling sentences and documents. Not all sentences contribute equally to the scoring of a given essay, and not all words contribute equally within a sentence. We adopt the neural attention model (Xu et al., 2015; Luong et al., 2015) to automatically calculate weights for convolution features of CNNs and hidden state values of LSTMs, which has been used for obtaining the most pertinent information for machine translation (Luong et al., 2015), sentiment analysis (Shin et al., 2016; Wang et al., 2016; Liu and Zhang, 2017) and other tasks. In our case, the attention mechanism can intuitively select sentences and grams that are more aligned with the props or obviously incorrect. To our knowledge, no prior Neural network models have recently been applied to the task of automatic essay scoring, giving promising results. Existing work used recurrent neural networks and convolutional neural networks to model input essays, giving grades based on a single vector representation of the essay. On the other hand, the relative advantages of RNNs and CNNs have not been compared. In addition, different"
K17-1017,N16-1174,0,0.0519211,"d Liang (2002) explore multinomial Bernoulli Naive Bayes models to classify texts into several categories of text quality based on content and style features. Chen et al. (2010) formulates the AES task into a weakly supervised framework and employ a voting algorithm. Other recent work formulate the task as a preference ranking problem (Yannakoudakis et al., 2011; Phandi et al., 2015). Yannakoudakis et al. Our work is also inline with recent work on building hierarchical sentence-document level representations of documents. Li et al. (2015) build a hierarchical LSTM auto-encoder for documents. Yang et al. (2016) build hierarchical LSTM models with attention for document and Tang et al. (2015) use a hierarchical Gated RNN for sentiment classification. Ren and Zhang (2016) use hierarchical CNN-LSTM model for spam detection. We use a hierarchical CNNLSTM model for essay scoring, which is a regression task. 160 7 Conclusion Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation 9(8):1735–1780. We investigated a recurrent convolutional neural network to learn text representation and grade essays automatically. Our model treated input essays as sentence-document hierarchi"
K17-1017,P11-1019,0,0.343257,"nario, scores are divided into several categories, each score or score range is regarded as one class and the ordinary classification models are employed such as Naive Bayes (NB) and SVMs (Larkey, 1998; Rudner and Liang, 2002). In the regression scenario, each score is treated as continous values for the essay and regression models are considered, like linear regression, Bayesian linear ridge regression (Attali and Burstein, 2004; Phandi et al., 2015). In the preference ranking scenario, AES task is considered as a ranking problem in which pair-wise ranking and list-wise ranking are employed (Yannakoudakis et al., 2011; Chen and He, 2013; Cummins et al., 2016). The former considers the ranking between each pair of essays, while the latter considers the absolute ranking of each essay in the whole set. Formally, an AES model is trained to minimize the difference between its automatically output scores and human given scores on a set of training data: min N X f (yi∗ , yi ), i=1 Wi,j = (i − j)2 , (R − 1)2 (2) where i and j are the reference rating (assigned by a human rater) and the system rating (assigned by an AES system), respectively, and R is the number of possible ratings. An observed score matrix O is ca"
K19-2014,hajic-etal-2012-announcing,0,0.40014,"Missing"
K19-2014,W12-3602,0,0.222709,"a Li2∗, Min Zhang2 1 Alibaba Group, China 2 School of Computer Science and Technology, Soochow University, China {shiyu.zy, junjie.junjiecao, masi.wr}@alibaba-inc.com wjiang0501@stu.suda.edu.cn, kirosummer.nlp@gmail.com {zhli13, minzhang}@suda.edu.cn Abstract Semantic Dependency Parsing (SDP) aims to parse the predicate-argument relationships for all words in the input sentence, leading to bilexical semantic dependency graphs (Oepen et al., 2014, 2015, 2016). This shared task focuses on two different formal types of SDP representations, i.e., DELPH-IN MRS Bi-Lexical Dependencies (abbr. as DM, Ivanova et al., 2012) and Prague Semantic Dependencies (abbr. as PSD, Hajiˇc et al., 2012; Miyao et al., 2014). They are both classified as Flavor 0 representations in the sense that every node in the graph must anchor to one and only one token unit, and vice verse. Compared with syntactic dependency trees, some nodes in an SDP graph may have no incoming edges and some may have multiple ones. Borrowing the idea of Dozat and Manning (2018), we encode the input word sequence with BiLSTMs and predict the edges and labels between words with two MLPs. We also predict the POS tag and frame of each word jointly under the"
K19-2014,S19-2002,1,0.758372,"istic framework (Flavor 1) firstly proposed by Abend and Rappoport (2013). In UCCA graphs, input words are leaf (or terminal) nodes. One non-terminal node governs one or more nodes, which may be discontinuous; and one node can have multiple governing (parent) nodes through multiple edges, consisting of a single primary edge and other remote edges. Relationships between nodes are given by edge labels. The primary edges form a tree structure, whereas the remote edges introduce reentrancy, forming directed acyclic graphs (DAGs).2 We directly adopt the previous graph-based UCCA parser proposed by Jiang et al. (2019), treating UCCA graph parsing as constituent parsing and remote edge recovery under the MTL framework. In this paper, we describe our participating systems in the shared task on CrossFramework Meaning Representation Parsing (MRP) at the 2019 Conference for Computational Language Learning (CoNLL). The task includes five frameworks for graph-based meaning representations, i.e., DM, PSD, EDS, UCCA, and AMR. One common characteristic of our systems is that we employ graph-based methods instead of transition-based methods when predicting edges between nodes. For SDP, we jointly perform edge predict"
K19-2014,N16-1030,0,0.0442481,"ds SDP We construct our SDP parser based on the ideas of Dozat and Manning (2017) and Dozat and Manning (2018). Note that lemmas, POS tags and frames are also included in the MRP evaluation metrics, so our method is a bit different from Dozat and Manning (2018). Edge Prediction. Our basic edge prediction model is similar to the Dozat and Manning (2017) and Dozat and Manning (2018). The input words are first mapped into a dense vector composed by pretrained word embeddings and character-level features. xi = eword ⊕ echar i i where echar is extracted by the bidirectional i character-level LSTM (Lample et al., 2016). They are then fed into a multilayer bidirectional wordlevel LSTM to get contextualized representations. Finally, two modules are applied to predict edges. One is to predict whether or not a directed edge exists between two words (keeping the edges between pairs of words with positive scores); and the other is to predict the most probable label for each potential edge (choosing the label with maximum score). Each of them has two seperate MLPs for head and dependent representations and a biaffine layer for scoring. The training loss is the sum of sigmoid cross-entropy loss for edges and softma"
K19-2014,P13-1023,0,0.187952,"entations in the sense that every node in the graph must anchor to one and only one token unit, and vice verse. Compared with syntactic dependency trees, some nodes in an SDP graph may have no incoming edges and some may have multiple ones. Borrowing the idea of Dozat and Manning (2018), we encode the input word sequence with BiLSTMs and predict the edges and labels between words with two MLPs. We also predict the POS tag and frame of each word jointly under the MTL framework. Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework (Flavor 1) firstly proposed by Abend and Rappoport (2013). In UCCA graphs, input words are leaf (or terminal) nodes. One non-terminal node governs one or more nodes, which may be discontinuous; and one node can have multiple governing (parent) nodes through multiple edges, consisting of a single primary edge and other remote edges. Relationships between nodes are given by edge labels. The primary edges form a tree structure, whereas the remote edges introduce reentrancy, forming directed acyclic graphs (DAGs).2 We directly adopt the previous graph-based UCCA parser proposed by Jiang et al. (2019), treating UCCA graph parsing as constituent parsing a"
K19-2014,P18-1037,0,0.437775,"ency SRL (semantic role labeling) to produce nodes. For the edge prediction, the widely-used Biaffine model is used. Abstract meaning representation (AMR), proposed by Banarescu et al. (2013), is a broadcoverage sentence-level semantic formalism (Flavor 2) to encode the meaning of natural language sentences. AMR can be regarded as a rooted labeled directed acyclic graph. Nodes in AMR graphs represent concepts, and labeled directed edges are relations between the concepts. Due to the time limitation and the complexity of the AMR parsing problem, we directly employ the stateof-the-art parser of Lyu and Titov (2018), which treats AMR parsing as a graph prediction problem. Methodology Summarization. Our participating systems can be characterized in the following aspects: under the MTL framework and it is adopted by the DM, PSD, and UCCA models. For both EDS and AMR, we first produce nodes and then predict edges in a pipeline architecture. We have not attempted to jointly solve multiple semantic frameworks via MTL yet. • BERT. We observe that using BERT as our extra inputs is effective for all the models, except AMR. It is also interesting that BERTlarge does not produce more improvements over BERT-base ba"
K19-2014,W13-2322,0,0.464322,"rings. Such alignment information is not provided in the shared task and seems difficult for us to induce due to time limitation. Therefore, we divide the EDS task into two-stage task: node prediction and edge prediction, and treat both as sequence labeling. To tackle with the explicit, many-to-many relationship between nodes and sub-strings of the underlying sentence (via anchoring), we introduce a similar method used in dependency SRL (semantic role labeling) to produce nodes. For the edge prediction, the widely-used Biaffine model is used. Abstract meaning representation (AMR), proposed by Banarescu et al. (2013), is a broadcoverage sentence-level semantic formalism (Flavor 2) to encode the meaning of natural language sentences. AMR can be regarded as a rooted labeled directed acyclic graph. Nodes in AMR graphs represent concepts, and labeled directed edges are relations between the concepts. Due to the time limitation and the complexity of the AMR parsing problem, we directly employ the stateof-the-art parser of Lyu and Titov (2018), which treats AMR parsing as a graph prediction problem. Methodology Summarization. Our participating systems can be characterized in the following aspects: under the MTL"
K19-2014,S14-2056,0,0.253914,"how University, China {shiyu.zy, junjie.junjiecao, masi.wr}@alibaba-inc.com wjiang0501@stu.suda.edu.cn, kirosummer.nlp@gmail.com {zhli13, minzhang}@suda.edu.cn Abstract Semantic Dependency Parsing (SDP) aims to parse the predicate-argument relationships for all words in the input sentence, leading to bilexical semantic dependency graphs (Oepen et al., 2014, 2015, 2016). This shared task focuses on two different formal types of SDP representations, i.e., DELPH-IN MRS Bi-Lexical Dependencies (abbr. as DM, Ivanova et al., 2012) and Prague Semantic Dependencies (abbr. as PSD, Hajiˇc et al., 2012; Miyao et al., 2014). They are both classified as Flavor 0 representations in the sense that every node in the graph must anchor to one and only one token unit, and vice verse. Compared with syntactic dependency trees, some nodes in an SDP graph may have no incoming edges and some may have multiple ones. Borrowing the idea of Dozat and Manning (2018), we encode the input word sequence with BiLSTMs and predict the edges and labels between words with two MLPs. We also predict the POS tag and frame of each word jointly under the MTL framework. Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguis"
K19-2014,P17-1112,0,0.0478012,"amework and followed by our corresponding approaches. ∗ 1 2 The full UCCA scheme also has implicit nodes and linkage relations, which are excluded in the shared task. Corresponding author http://mrp.nlpl.eu/index.php?page=1 149 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 149–157 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2014 Elementary Dependency Structure (EDS) is a graph-structured semantic representation formalism (Flavor 1) proposed by Oepen and Lønning (2006). Buys and Blunsom (2017) introduce a neural encoder-decoder transition-based model to obtain the EDS graph. They use external knowledge to generate nodes3 . Chen et al. (2018) introduce a novel SHRG (Synchronous Hyperedge Replacement Grammar) extraction algorithm which requires a syntactic tree and alignments between conceptual edges and surface strings. Such alignment information is not provided in the shared task and seems difficult for us to induce due to time limitation. Therefore, we divide the EDS task into two-stage task: node prediction and edge prediction, and treat both as sequence labeling. To tackle with"
K19-2014,K19-2001,0,0.157991,"Missing"
K19-2014,P18-1038,0,0.0536348,"ared task. Corresponding author http://mrp.nlpl.eu/index.php?page=1 149 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 149–157 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2014 Elementary Dependency Structure (EDS) is a graph-structured semantic representation formalism (Flavor 1) proposed by Oepen and Lønning (2006). Buys and Blunsom (2017) introduce a neural encoder-decoder transition-based model to obtain the EDS graph. They use external knowledge to generate nodes3 . Chen et al. (2018) introduce a novel SHRG (Synchronous Hyperedge Replacement Grammar) extraction algorithm which requires a syntactic tree and alignments between conceptual edges and surface strings. Such alignment information is not provided in the shared task and seems difficult for us to induce due to time limitation. Therefore, we divide the EDS task into two-stage task: node prediction and edge prediction, and treat both as sequence labeling. To tackle with the explicit, many-to-many relationship between nodes and sub-strings of the underlying sentence (via anchoring), we introduce a similar method used in"
K19-2014,L16-1630,0,0.0887716,"Missing"
K19-2014,S15-2153,0,0.651634,"Missing"
K19-2014,P18-2077,0,0.232099,"ency graphs (Oepen et al., 2014, 2015, 2016). This shared task focuses on two different formal types of SDP representations, i.e., DELPH-IN MRS Bi-Lexical Dependencies (abbr. as DM, Ivanova et al., 2012) and Prague Semantic Dependencies (abbr. as PSD, Hajiˇc et al., 2012; Miyao et al., 2014). They are both classified as Flavor 0 representations in the sense that every node in the graph must anchor to one and only one token unit, and vice verse. Compared with syntactic dependency trees, some nodes in an SDP graph may have no incoming edges and some may have multiple ones. Borrowing the idea of Dozat and Manning (2018), we encode the input word sequence with BiLSTMs and predict the edges and labels between words with two MLPs. We also predict the POS tag and frame of each word jointly under the MTL framework. Universal Conceptual Cognitive Annotation (UCCA) is a multi-layer linguistic framework (Flavor 1) firstly proposed by Abend and Rappoport (2013). In UCCA graphs, input words are leaf (or terminal) nodes. One non-terminal node governs one or more nodes, which may be discontinuous; and one node can have multiple governing (parent) nodes through multiple edges, consisting of a single primary edge and othe"
K19-2014,S14-2008,0,0.350225,"Missing"
K19-2014,oepen-lonning-2006-discriminant,0,0.184117,"ef introduction of each framework and followed by our corresponding approaches. ∗ 1 2 The full UCCA scheme also has implicit nodes and linkage relations, which are excluded in the shared task. Corresponding author http://mrp.nlpl.eu/index.php?page=1 149 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 149–157 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2014 Elementary Dependency Structure (EDS) is a graph-structured semantic representation formalism (Flavor 1) proposed by Oepen and Lønning (2006). Buys and Blunsom (2017) introduce a neural encoder-decoder transition-based model to obtain the EDS graph. They use external knowledge to generate nodes3 . Chen et al. (2018) introduce a novel SHRG (Synchronous Hyperedge Replacement Grammar) extraction algorithm which requires a syntactic tree and alignments between conceptual edges and surface strings. Such alignment information is not provided in the shared task and seems difficult for us to induce due to time limitation. Therefore, we divide the EDS task into two-stage task: node prediction and edge prediction, and treat both as sequence"
K19-2014,D14-1162,0,0.096733,"rs. Concept Identification Model. The concept identification model chooses a concept c conditioned on the aligned word k based on the BiLSTM state hk , which is defined as Pθ (c|hk , wk ). For more details about the re-categorization and candidate concept, please refer to Lyu and Titov (2018). Relation Identification Model. The relation identification model is arc-factored as: Pφ (R|a, c, w) = m Y 3 Experiments This section describes model parameters used in our models, and the overall results of all the five tasks. 3.1 Model Parameters In both SDP and UCCA tasks, we use 100dimensional GloVe (Pennington et al., 2014) as pretrained embedding and random initialized 50dimensional char embedding. The char lstm output is 100-dimensional. We also utilize the BERT embeddings extracted from the last four transformer layers. The final BERT representation is their normalized weighted sum, which is concatenated with the word embeddings. The other parameters are the same with the previous works (Dozat and Manning, 2018; Jiang et al., 2019). In EDS task, external resources we use are: 1) word embeddings pre-trained with GloVe (Pennington et al., 2014) on the Gigaword corpus for Chinese; and 2) BERT 10 (Devlin et al.,"
K19-2014,P17-1076,0,0.0287443,"MLP MLP Shared BiLSTMs ... xi ... Figure 2: An example of newest version of UCCA. Figure 1: The framework of our SDP Parser. To handle discontinuous node, we trace bottomup from a discontinuous leaf node until we find the specific node whose parent is the lowest common ancestor (LCA) of the discontinuous node and leaf node. Finally we move the edge to make the specific node become the child of the discontinuous, with “-ancestor” added behind the edge label. Please refer to Jiang et al. (2019) for more conversion details. Constituent Parsing. We directly adopt the minimal span-based parser of Stern et al. (2017). Given an input sentence X = {x0 , x1 , · · · , xn }, each word xi is mapped into a dense vector xi and fed into bidirectional LSTM layers. The top-layer output of each position are used to represent the span as cross-entropy loss for labels. 0 ` = `label + `edge Lexical Taggers. This SDP task is more difficult than the ealier 2014 and 2015 SDP tasks (Oepen et al., 2014, 2015), since the gold tokenization result, lemmas, and POS tags are not available in the parser input data and the predictions are parts of the MRP evaluation metrics. We use automatic tokenization result and lemmas provided"
L16-1034,N13-1006,0,0.0685385,"Missing"
L16-1034,P15-1030,0,0.0234407,"tion, pooling, window function and embedding lookup, which are needed by most NLP tasks. We support flexible objective functions and optimization methods, such as max-margin, max likelihood criterions and AdaGrad (Duchi et al., 2011), and also verification functions such as gradient check. One uniqueness of our toolkit is the support of both dense continuous features and sparse indicator features in neural layers, making it convenient also to build traditional discrete models such as the perceptron, logistic regression and CRF, and to combine discrete and continuous features (Ma et al., 2014; Durrett and Klein, 2015; Zhang and Zhang, 2015). Taking word segmentation, POS-tagging and name entity recognition (NER) as typical examples, we show how stateof-the-art discrete, neural and hybrid models can be built using our toolkit. For example, we show how a bidirectional LSTM model can be built for POS tagging in only 23-lines (12 for inference and 11 for back-propagation) of codes, which gives highly competitive accuracies on standard benchmarks. 2. 2.1. Classes Base Layers Shown in Table 1, we provide several basic classes, which are widely used in neural networks and discrete machine learning algorithms, in"
L16-1034,P15-1033,0,0.0306763,"ar models and simple neural models. Besides, this package also integrates several complex layers by composing those basic layers, such as RNN, Attention Pooling, LSTM and gated RNN. Those complex layers can be used to implement deep neural models directly. Keywords: neural network, sequence labeling, recurrent neural network 1. Introduction Deep learning methods have received increasing research attention in natural language processing (NLP), with neural models being built for classification (Kalchbrenner et al., 2014), sequence labeling (Collobert et al., 2011), parsing (Socher et al., 2013; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015), machine translation (Cho et al., 2014), fine-grained sentiment analysis (Zhang et al., 2015) and other tasks. This surge of the interest gives rise to a demand of software libraries, which can facilitate research by allowing fast prototyping and modeling for experimentation. For traditional methods such as conditional random fields (CRF) (Lafferty et al., 2001) and SVM (Vapnik, 1995), there has been various software toolkits, implemented in different programming languages, including Java, Python and C++. These toolkits offer a large degree of variety f"
L16-1034,P14-1062,0,0.0147815,"layer linear and non-linear transformations. By using these layers, we can conveniently implement linear models and simple neural models. Besides, this package also integrates several complex layers by composing those basic layers, such as RNN, Attention Pooling, LSTM and gated RNN. Those complex layers can be used to implement deep neural models directly. Keywords: neural network, sequence labeling, recurrent neural network 1. Introduction Deep learning methods have received increasing research attention in natural language processing (NLP), with neural models being built for classification (Kalchbrenner et al., 2014), sequence labeling (Collobert et al., 2011), parsing (Socher et al., 2013; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015), machine translation (Cho et al., 2014), fine-grained sentiment analysis (Zhang et al., 2015) and other tasks. This surge of the interest gives rise to a demand of software libraries, which can facilitate research by allowing fast prototyping and modeling for experimentation. For traditional methods such as conditional random fields (CRF) (Lafferty et al., 2001) and SVM (Vapnik, 1995), there has been various software toolkits, implemented in different programmin"
L16-1034,D14-1093,1,0.845166,"Missing"
L16-1034,P14-1014,1,0.801107,"such as concatenation, pooling, window function and embedding lookup, which are needed by most NLP tasks. We support flexible objective functions and optimization methods, such as max-margin, max likelihood criterions and AdaGrad (Duchi et al., 2011), and also verification functions such as gradient check. One uniqueness of our toolkit is the support of both dense continuous features and sparse indicator features in neural layers, making it convenient also to build traditional discrete models such as the perceptron, logistic regression and CRF, and to combine discrete and continuous features (Ma et al., 2014; Durrett and Klein, 2015; Zhang and Zhang, 2015). Taking word segmentation, POS-tagging and name entity recognition (NER) as typical examples, we show how stateof-the-art discrete, neural and hybrid models can be built using our toolkit. For example, we show how a bidirectional LSTM model can be built for POS tagging in only 23-lines (12 for inference and 11 for back-propagation) of codes, which gives highly competitive accuracies on standard benchmarks. 2. 2.1. Classes Base Layers Shown in Table 1, we provide several basic classes, which are widely used in neural networks and discrete machin"
L16-1034,P13-1045,0,0.0113582,"iently implement linear models and simple neural models. Besides, this package also integrates several complex layers by composing those basic layers, such as RNN, Attention Pooling, LSTM and gated RNN. Those complex layers can be used to implement deep neural models directly. Keywords: neural network, sequence labeling, recurrent neural network 1. Introduction Deep learning methods have received increasing research attention in natural language processing (NLP), with neural models being built for classification (Kalchbrenner et al., 2014), sequence labeling (Collobert et al., 2011), parsing (Socher et al., 2013; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015), machine translation (Cho et al., 2014), fine-grained sentiment analysis (Zhang et al., 2015) and other tasks. This surge of the interest gives rise to a demand of software libraries, which can facilitate research by allowing fast prototyping and modeling for experimentation. For traditional methods such as conditional random fields (CRF) (Lafferty et al., 2001) and SVM (Vapnik, 1995), there has been various software toolkits, implemented in different programming languages, including Java, Python and C++. These toolkits offer a large"
L16-1034,N03-1033,0,0.143942,"mbedding Ewi of the word wi and its vector representation vci derived from its character sei quence cm 1 (mi is the length of word wi ). vci is constructed according to neural network structures shown in Figure 2. For NER, ti consists of three parts, including Ewi , vci and the word’s POS tag embedding Epi . The deep neural POS tagging model consists of only 23 lines of code, as marked by red superscripts in Table 3, Figure 2 and Figure 1. Besides the neural models above, we also implement discrete models for the three tasks. The discrete features are extracted according to Liu et al. (2014), Toutanova et al. (2003) and Che et al. (2013) for word segmentation, POS tagging and NER, respectively. We simply apply the sparse atomic layer and exploit the same CRF max-margin for training model parameters. Finally, we make combinations of the discrete and neural models by aggregating their output vectors. Results. We conduct experiments on several datasets. For Chinese word segmentation, we exploit PKU, MSR and CTB60 datasets, where the training and testing corpus of PKU and MSR can be downloaded from BakeOff2005 website5 . For POS tagging, we perform experiments on both English and Chinese datasets. For Englis"
L16-1034,P15-1032,0,0.0211264,"sides, this package also integrates several complex layers by composing those basic layers, such as RNN, Attention Pooling, LSTM and gated RNN. Those complex layers can be used to implement deep neural models directly. Keywords: neural network, sequence labeling, recurrent neural network 1. Introduction Deep learning methods have received increasing research attention in natural language processing (NLP), with neural models being built for classification (Kalchbrenner et al., 2014), sequence labeling (Collobert et al., 2011), parsing (Socher et al., 2013; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015), machine translation (Cho et al., 2014), fine-grained sentiment analysis (Zhang et al., 2015) and other tasks. This surge of the interest gives rise to a demand of software libraries, which can facilitate research by allowing fast prototyping and modeling for experimentation. For traditional methods such as conditional random fields (CRF) (Lafferty et al., 2001) and SVM (Vapnik, 1995), there has been various software toolkits, implemented in different programming languages, including Java, Python and C++. These toolkits offer a large degree of variety for building NLP models by using or adapt"
L16-1034,P07-1106,1,0.795528,".42 97.01 97.39 97.20 95.68 95.64 95.66 94.50 N/A N/A 97.20 N/A N/A 95.05 POS Tagging English Chinese Acc Acc 97.23 93.97 97.28 94.02 97.47 95.07 97.24 94.10 NER English Chinese P R F P R F 80.14 79.29 79.71 72.67 73.92 73.29 77.25 80.19 78.69 65.59 71.84 68.57 81.90 83.26 82.57 72.98 80.15 76.40 82.95 76.67 79.68 76.90 63.32 69.45 Table 4: Main results. split Ontonotes 4.0 to get the English and Chinese datasets. Our experimental results are shown in Table 4. As can be seen for the table, our neural models give competitive results compared the state-of-the-art results on each task, which are Zhang and Clark (2007) for Chinese word segmentation, Toutanova et al. (2003) for English POS tagging, Li et al. (2015) for Chinese POS tagging and Che et al. (2013) for English and Chinese NER. 4. Code Our code and examples in this paper is available under GPL at https://github.com/SUTDNLP/, including repositories of LibN3L, NNSegmentation, NNPOSTagging and NNNamedEntity. 5. Acknowledgments We thank the anonymous reviewers for their constructive comments, which helped to improve the paper. This work is supported by the Singapore Ministry of Education (MOE) AcRF Tier 2 grant T2MOE201301, SRG ISTD 2012 038 from Sing"
L16-1034,D15-1153,1,0.841051,"ction and embedding lookup, which are needed by most NLP tasks. We support flexible objective functions and optimization methods, such as max-margin, max likelihood criterions and AdaGrad (Duchi et al., 2011), and also verification functions such as gradient check. One uniqueness of our toolkit is the support of both dense continuous features and sparse indicator features in neural layers, making it convenient also to build traditional discrete models such as the perceptron, logistic regression and CRF, and to combine discrete and continuous features (Ma et al., 2014; Durrett and Klein, 2015; Zhang and Zhang, 2015). Taking word segmentation, POS-tagging and name entity recognition (NER) as typical examples, we show how stateof-the-art discrete, neural and hybrid models can be built using our toolkit. For example, we show how a bidirectional LSTM model can be built for POS tagging in only 23-lines (12 for inference and 11 for back-propagation) of codes, which gives highly competitive accuracies on standard benchmarks. 2. 2.1. Classes Base Layers Shown in Table 1, we provide several basic classes, which are widely used in neural networks and discrete machine learning algorithms, including atomic layers, p"
L16-1034,P14-1125,1,0.882227,"Missing"
L16-1034,D15-1073,1,0.86174,"ch as RNN, Attention Pooling, LSTM and gated RNN. Those complex layers can be used to implement deep neural models directly. Keywords: neural network, sequence labeling, recurrent neural network 1. Introduction Deep learning methods have received increasing research attention in natural language processing (NLP), with neural models being built for classification (Kalchbrenner et al., 2014), sequence labeling (Collobert et al., 2011), parsing (Socher et al., 2013; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015), machine translation (Cho et al., 2014), fine-grained sentiment analysis (Zhang et al., 2015) and other tasks. This surge of the interest gives rise to a demand of software libraries, which can facilitate research by allowing fast prototyping and modeling for experimentation. For traditional methods such as conditional random fields (CRF) (Lafferty et al., 2001) and SVM (Vapnik, 1995), there has been various software toolkits, implemented in different programming languages, including Java, Python and C++. These toolkits offer a large degree of variety for building NLP models by using or adapting the machine learning algorithms. For deep learning, a number of software tools have been d"
L16-1034,P15-1117,1,0.842891,"Missing"
L16-1034,W14-4012,0,\N,Missing
L16-1034,P15-1172,0,\N,Missing
L16-1104,C12-1018,0,0.0325284,"Missing"
L16-1104,D12-1133,0,0.0180185,"inspired by the hierarchical log-bilinear neural language model. In standard WSJ experiments, the neural parser achieves an almost 2.4 time speed up (320 sen/sec) compared to a non-hierarchical baseline without significant accuracy loss (89.06 vs 89.13 F-score). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency parser can be 1000 sentence"
L16-1104,W08-2102,0,0.068385,"Missing"
L16-1104,P05-1022,0,0.122212,"Missing"
L16-1104,A00-2018,0,0.56498,"Missing"
L16-1104,D14-1082,0,0.266842,"al Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency parser can be 1000 sentences per second on a single CPU, which is significant faster than traditional parsers. In this paper, we explore the method of Chen and Manning (2014) for fast deterministic transition-based constituent parsing. This is a more challenging task compared with neural dependency parsing (Table 1). First,"
L16-1104,Q13-1033,0,0.0129105,"). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency parser can be 1000 sentences per second on a single CPU, which is significant faster than traditional parsers. In this paper, we explore the method of Chen and Manning (2014) for fast deterministic transition-based constituent parsing. This is a more challenging task compared with neural d"
L16-1104,P04-1013,0,0.114156,"Missing"
L16-1104,P08-1067,0,0.0719478,"Missing"
L16-1104,J93-2004,0,0.0583985,"on in the action layer, and lj is the jth label in the label layer. We adopt a greedy decoding strategy in the hierarchical parsing process. In each parsing step, the action type ai with the highest probability is first selected, and then the constituent label lj with the highest probability is selected given the optimal action type ai . As in the baseline parser, we adopt the cross-entropy loss as our training objective: L(θ) = − X log p(yi,j |x, Acts) + yi,j ∈A 5. 5.1. λ k θ k2 2 (10) Experiments Set-up We conduct our experiments on the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1993). Following the standard splits of WSJ, sections 2–21 are used as the labeled training data, section 24 is used as the development data and section 23 is used as the evaluation data. Ten-fold jackknifing (Collins, 2000) is used to automatically assign POS tags to the training data. The SVMTool is used as the POS-tagger1 . 5.2. Parameters We carry out a development experiment to measure the correlation between hidden layer size and constituent parsing accuracies. From Table 3, we can see that both the baseline neural parser and the hierarchical neural parser achieve higher parsing accuracies wi"
L16-1104,N06-1020,0,0.105368,"Missing"
L16-1104,J08-4003,0,0.0283774,"89.13 F-score). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency parser can be 1000 sentences per second on a single CPU, which is significant faster than traditional parsers. In this paper, we explore the method of Chen and Manning (2014) for fast deterministic transition-based constituent parsing. This is a more challenging"
L16-1104,N07-1051,0,0.151924,"Missing"
L16-1104,W05-1513,0,0.402545,"parser by using a hierarchical output layer, inspired by the hierarchical log-bilinear neural language model. In standard WSJ experiments, the neural parser achieves an almost 2.4 time speed up (320 sen/sec) compared to a non-hierarchical baseline without significant accuracy loss (89.06 vs 89.13 F-score). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of"
L16-1104,P12-1046,0,0.0380018,"Missing"
L16-1104,P13-1045,0,0.0608923,"Missing"
L16-1104,P07-1080,0,0.167269,"mapping matrix from hidden layer to the action layer and da is the number of action types. W3i ∈ Rdlabel ×dh is the mapping matrix from the hidden layer to the label layer. The probability of a labeled action yi,j , given its history Acts and input x, is computed as: p(yi,j |x, Acts) = p(ai |x, Acts) × p(lj |x, Acts, ai ) (7) Neural where Model Collins (1999) Charniak (2000) Charniak and Johnson (2005)‡ Huang (2008)‡ McClosky et al. (2006)‡ Shindo et al. (2012) Sagae and Lavie (2005) Petrov and Klein (2007) Carreras et al. (2008) Zhu et al. (2013) Zhu et al. (2013) + padding Henderson (2004)‡ Titov and Henderson (2007) Collobert (2011) Billingsley and Curran (2012) Socher et al. (2013)‡ Legrand and Collobert (2014) Watanabe and Sumita (2015) F1 88.2 89.6 91.1 91.7 92.1 92.4 86.0 90.1 91.1 89.9 90.4 90.1 90.0 87.9 84.9 90.4 88.3 90.7 Speed 3.5 5.7 This Work This Work + hierarchical 89.13 89.06 133.6 320.2 3.7 6.2 100.7 89.5 31.7 6.1 22.0 1.8 i p(ai |x, Acts) = eoact P k eoact (8) ak ∈GEN(Acts) j p(lj |x, Acts, ai ) = eolabel (ai ) P k eolabel (ai ) (9) Table 4: Comparisons with previous work. ‡: reranking model. Speed: sentences per second. lk ∈GEN(Acts) Here ai is the ith action in the action layer, and lj"
L16-1104,W09-3825,1,0.909876,"ing node with constituent label X whose child is s0 ; push the new node back onto the stack. • LEFT/RIGHT-X: pop the top two nodes s1 , s0 off the stack; generate a binary-branching node with constituent label X whose left child is s1 and right child is s0 , with the left (LEFT)/right (RIGHT) child as its head; push the new node back to the stack. The shift-reduce actions only build binarized trees. As a result, a binarization process is necessary to convert the Penn Treebank into binarized trees. During this process, temporary nodes are constructed. To accommodate for binarization, we follow Zhang and Clark (2009), adding counterparts to LEFT/RIGHT-X for temporary nodes, namely LEFT/RIGHT-TEMP-X. 3. h = (W1 x + b1 )3 (3) a∈A Here A is the set of all gold labeled actions in the training data. Mini-bached AdaGrad (Duchi et al., 2011) and dropout (Srivastava et al., 2014) are used for optimization. Hierarchical Output Neural Network Due to its large hidden and output layer sizes, the vanilla neural constituent parser is much slower than the dependency counterpart. The main computation cost is the mapping from hidden layer to output layer. Motivated by the hierarchical neural language model (Mnih and Hinto"
L16-1104,P11-2033,1,0.811029,"archical output layer, inspired by the hierarchical log-bilinear neural language model. In standard WSJ experiments, the neural parser achieves an almost 2.4 time speed up (320 sen/sec) compared to a non-hierarchical baseline without significant accuracy loss (89.06 vs 89.13 F-score). Keywords: Hierarchical Model, Neural Networks, Parsing 1. Introduction With the growth of demands for processing massive amounts of web data, the speed of NLP systems has become a key practical performance. Transition-based parsers build outputs by using a sequence of shift-reduce actions (Sagae and Lavie, 2005; Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Zhou et al., 2015). They are attractive by their fast speed, offering linear time complexity with deterministic decoding algorithms. Linear classifiers have traditionally been used to build very fast deterministic parsers (Nivre, 2008; Goldberg and Nivre, 2013). Recently, Chen and Manning (2014) proposed a transitionbased dependency parser, which adopts a neural network as the classifier. It has two advantages compared to the traditional linear model (Nivre, 2008), namely fast speed and automatic feature combination. In particular, the speed of a neural dependency par"
L16-1104,P15-1117,1,0.88504,"Missing"
L16-1104,P13-1043,1,0.834198,"Missing"
L16-1104,P15-1113,0,\N,Missing
L16-1138,P14-2131,0,0.0464055,"Missing"
L16-1138,W06-0130,0,0.242518,"mentation errors can affect the quality, it has been shown that such word embeddings can improve Chinese NLP tasks such as parsing (Wu et al., 2013). On the other hand, character information can also be used for NLP (Zhang et al., 2014a). Zhang et al. (2013) show that parsing can be improved by taking characters as inputs, jointly performing segmentation and syntax parsing. They show that character features can significantly improve the accuracies. Character features are also central to other Chinese NLP tasks such as character-based segmentation (Xue, 2003) and named entity recognition (NER (Chen et al., 2006)). It is an interesting research question whether character embeddings can be useful for Chinese NLP. However, relatively little work has been reported on embedding Chinese characters, and limited effect has been observed (Sun et al., 2014). One possible reason is that the number of characters is much smaller compared to the number of words (104 vs 106 ), and the effect of character embedding on reducing sparsity can be limited. Also due to this reason, it has been shown that unsupervised clustering of characters does not improve in-domain segmentation (Liang, 2005). Another possible reason, h"
L16-1138,D14-1110,0,0.0161911,"del significantly. 2. Multi-prototype Character Embedding Most Chinese characters have multiple senses, but take only one sense in a given context. The task of multi-prototype character embedding is to find a continuous vector representation for each Chinese character sense. We develop a multi-prototype word model by adapting an existing model for word embedding in the literature, making significant changes for the character embedding task. There has been a line of work in training multi-prototype word embeddings (Huang et al., 2012; Guo et al., 2014a; Frey and Dueck, 2007; Tian et al., 2014; Chen et al., 2014). We adapt the Multi-Sense Skip-gram (MSSG) model (Neelakantan et al., 2014), which is based on the Skip-gram model (Mikolov et al., 2013). The Skip-gram model works by predicting a word vector using its context. To accommodate for multiple word senses, the MSSG model works by predicting the sense of the current word given its context, and then updating the current word sense vector and context vector. In particular, each word w is associated with two types of vector representations, one being the output embedding v(w), and the other being the context v(w). Given a special context, the sense o"
L16-1138,C14-1048,0,0.0186181,"mbeddings improves the accuracy of a state-of-the-art CRF model significantly. 2. Multi-prototype Character Embedding Most Chinese characters have multiple senses, but take only one sense in a given context. The task of multi-prototype character embedding is to find a continuous vector representation for each Chinese character sense. We develop a multi-prototype word model by adapting an existing model for word embedding in the literature, making significant changes for the character embedding task. There has been a line of work in training multi-prototype word embeddings (Huang et al., 2012; Guo et al., 2014a; Frey and Dueck, 2007; Tian et al., 2014; Chen et al., 2014). We adapt the Multi-Sense Skip-gram (MSSG) model (Neelakantan et al., 2014), which is based on the Skip-gram model (Mikolov et al., 2013). The Skip-gram model works by predicting a word vector using its context. To accommodate for multiple word senses, the MSSG model works by predicting the sense of the current word given its context, and then updating the current word sense vector and context vector. In particular, each word w is associated with two types of vector representations, one being the output embedding v(w), and the othe"
L16-1138,D14-1012,0,0.0185787,"mbeddings improves the accuracy of a state-of-the-art CRF model significantly. 2. Multi-prototype Character Embedding Most Chinese characters have multiple senses, but take only one sense in a given context. The task of multi-prototype character embedding is to find a continuous vector representation for each Chinese character sense. We develop a multi-prototype word model by adapting an existing model for word embedding in the literature, making significant changes for the character embedding task. There has been a line of work in training multi-prototype word embeddings (Huang et al., 2012; Guo et al., 2014a; Frey and Dueck, 2007; Tian et al., 2014; Chen et al., 2014). We adapt the Multi-Sense Skip-gram (MSSG) model (Neelakantan et al., 2014), which is based on the Skip-gram model (Mikolov et al., 2013). The Skip-gram model works by predicting a word vector using its context. To accommodate for multiple word senses, the MSSG model works by predicting the sense of the current word given its context, and then updating the current word sense vector and context vector. In particular, each word w is associated with two types of vector representations, one being the output embedding v(w), and the othe"
L16-1138,P12-1092,0,0.0522135,"ER task, character embeddings improves the accuracy of a state-of-the-art CRF model significantly. 2. Multi-prototype Character Embedding Most Chinese characters have multiple senses, but take only one sense in a given context. The task of multi-prototype character embedding is to find a continuous vector representation for each Chinese character sense. We develop a multi-prototype word model by adapting an existing model for word embedding in the literature, making significant changes for the character embedding task. There has been a line of work in training multi-prototype word embeddings (Huang et al., 2012; Guo et al., 2014a; Frey and Dueck, 2007; Tian et al., 2014; Chen et al., 2014). We adapt the Multi-Sense Skip-gram (MSSG) model (Neelakantan et al., 2014), which is based on the Skip-gram model (Mikolov et al., 2013). The Skip-gram model works by predicting a word vector using its context. To accommodate for multiple word senses, the MSSG model works by predicting the sense of the current word given its context, and then updating the current word sense vector and context vector. In particular, each word w is associated with two types of vector representations, one being the output embedding"
L16-1138,P14-1062,0,0.0177772,"Missing"
L16-1138,D14-1113,0,0.0223256,"e characters have multiple senses, but take only one sense in a given context. The task of multi-prototype character embedding is to find a continuous vector representation for each Chinese character sense. We develop a multi-prototype word model by adapting an existing model for word embedding in the literature, making significant changes for the character embedding task. There has been a line of work in training multi-prototype word embeddings (Huang et al., 2012; Guo et al., 2014a; Frey and Dueck, 2007; Tian et al., 2014; Chen et al., 2014). We adapt the Multi-Sense Skip-gram (MSSG) model (Neelakantan et al., 2014), which is based on the Skip-gram model (Mikolov et al., 2013). The Skip-gram model works by predicting a word vector using its context. To accommodate for multiple word senses, the MSSG model works by predicting the sense of the current word given its context, and then updating the current word sense vector and context vector. In particular, each word w is associated with two types of vector representations, one being the output embedding v(w), and the other being the context v(w). Given a special context, the sense of a word is predicted as the center 855 top K one fix cilin one pos fix pos"
L16-1138,C14-1016,0,0.0204472,"e-of-the-art CRF model significantly. 2. Multi-prototype Character Embedding Most Chinese characters have multiple senses, but take only one sense in a given context. The task of multi-prototype character embedding is to find a continuous vector representation for each Chinese character sense. We develop a multi-prototype word model by adapting an existing model for word embedding in the literature, making significant changes for the character embedding task. There has been a line of work in training multi-prototype word embeddings (Huang et al., 2012; Guo et al., 2014a; Frey and Dueck, 2007; Tian et al., 2014; Chen et al., 2014). We adapt the Multi-Sense Skip-gram (MSSG) model (Neelakantan et al., 2014), which is based on the Skip-gram model (Mikolov et al., 2013). The Skip-gram model works by predicting a word vector using its context. To accommodate for multiple word senses, the MSSG model works by predicting the sense of the current word given its context, and then updating the current word sense vector and context vector. In particular, each word w is associated with two types of vector representations, one being the output embedding v(w), and the other being the context v(w). Given a special"
L16-1138,P10-1040,0,0.153108,"Missing"
L16-1138,W13-5708,0,0.0145802,". The advantages of embeddings are two-fold. First, they are useful for reducing sparseness compared with discrete words and n-grams. Second, they contain automatically induced features. Written as continuous sequences of characters, Chinese sentences do not have explicit word delimitation. As a result, similar to other Chinese NLP tasks, it is possible to create word embeddings based on automatically segmented Chinese sentences (Zhang et al., 2014b). Although segmentation errors can affect the quality, it has been shown that such word embeddings can improve Chinese NLP tasks such as parsing (Wu et al., 2013). On the other hand, character information can also be used for NLP (Zhang et al., 2014a). Zhang et al. (2013) show that parsing can be improved by taking characters as inputs, jointly performing segmentation and syntax parsing. They show that character features can significantly improve the accuracies. Character features are also central to other Chinese NLP tasks such as character-based segmentation (Xue, 2003) and named entity recognition (NER (Chen et al., 2006)). It is an interesting research question whether character embeddings can be useful for Chinese NLP. However, relatively little w"
L16-1138,O03-4002,0,0.102594,"sentences (Zhang et al., 2014b). Although segmentation errors can affect the quality, it has been shown that such word embeddings can improve Chinese NLP tasks such as parsing (Wu et al., 2013). On the other hand, character information can also be used for NLP (Zhang et al., 2014a). Zhang et al. (2013) show that parsing can be improved by taking characters as inputs, jointly performing segmentation and syntax parsing. They show that character features can significantly improve the accuracies. Character features are also central to other Chinese NLP tasks such as character-based segmentation (Xue, 2003) and named entity recognition (NER (Chen et al., 2006)). It is an interesting research question whether character embeddings can be useful for Chinese NLP. However, relatively little work has been reported on embedding Chinese characters, and limited effect has been observed (Sun et al., 2014). One possible reason is that the number of characters is much smaller compared to the number of words (104 vs 106 ), and the effect of character embedding on reducing sparsity can be limited. Also due to this reason, it has been shown that unsupervised clustering of characters does not improve in-domain"
L16-1138,P13-1013,1,0.852115,"iscrete words and n-grams. Second, they contain automatically induced features. Written as continuous sequences of characters, Chinese sentences do not have explicit word delimitation. As a result, similar to other Chinese NLP tasks, it is possible to create word embeddings based on automatically segmented Chinese sentences (Zhang et al., 2014b). Although segmentation errors can affect the quality, it has been shown that such word embeddings can improve Chinese NLP tasks such as parsing (Wu et al., 2013). On the other hand, character information can also be used for NLP (Zhang et al., 2014a). Zhang et al. (2013) show that parsing can be improved by taking characters as inputs, jointly performing segmentation and syntax parsing. They show that character features can significantly improve the accuracies. Character features are also central to other Chinese NLP tasks such as character-based segmentation (Xue, 2003) and named entity recognition (NER (Chen et al., 2006)). It is an interesting research question whether character embeddings can be useful for Chinese NLP. However, relatively little work has been reported on embedding Chinese characters, and limited effect has been observed (Sun et al., 2014)"
L16-1138,P14-1125,1,0.891171,"Missing"
L18-1596,2010.eamt-1.44,0,0.245165,"Missing"
L18-1596,W04-1217,0,0.0270337,"e. number of tokens) of TT. This normalized term count can serve as a quality indicator in quality estimation tasks (i.e. supervised classification or regression to predict quality scores or class labels) as illustrated in the correlation analysis afterwards. 3.1. Term Classification N-gram technique is commonly used as a languageindependent approach, particularly for under-resourced language. Therefore, the term candidate classification is framed as a N-gram classification task rather than the conventional sequence labelling methods that are commonly seen in previous work (Zhou and Su, 2004; Finkel et al., 2004). From a pragmatic point of view, our features are computed by JATE 2.0 (Zhang et al., 2016). Most representative and language-independent statistic ATR techniques are available in the package. These features (See Table 1) . We briefly describe the features below: TTF, namely Term Total Frequency, is the total frequency of a candidate in the target corpus. This algorithm takes into account frequency information for retrieving words or phrases that are both highly indicative of document content and highly distinctive within a text collection. ATTF takes the average of TTF by dividing it by the"
L18-1596,I11-2003,0,0.0330642,"acy of methods in this approach is that they rely on the morphosynactic analyser of the term extractor that does not recognize all candidate terms and those chunk-based methods, having extended the alignment model with automatically extracted language pair specific rules. As a consequence, this method blurs the distinction between terms and non-terms. • Comparable-corpus Based Bilingual corpora in specialized domains are actually scarce and it is expensive to build high quality parallel texts of specialized domains. A practical solution to this limitation is to make use of comparable corpora (Rocheteau and Daille, 2011; Xu et al., 2015; Hakami and Bollegala, 2017) that are available in large quantities. However, term extraction along this line is often limited to noun phrases (< 5 words) from monolingual comparable corpora. Thus, the recall of such an approach is not satisfactory under some circumstances. For other studies in this approach, ambiguity of term translations and identification of synonymous terms need to be further addressed. • Web-data Based Web data mining is another means to collect terminology pairs (Erdmann et al., 2009; Gaizauskas et al., 2015). Despite the favourable findings from the ev"
L18-1596,W14-4811,1,0.824587,"matically identifying terms in human translations. However, drawbacks in handling low frequency terms and term variations shall be dealt in the future. Keywords: Bilingual terminology, translation quality, supervised learning, correlation analysis 1. Introduction Terminology helps translators organize their domain knowledge, and provides them means (usually terms in various lexical units) to express subject knowledge adequately. Translation scholars and practitioners maintain that terminology correctness is associated with the quality of translation (and interpretation) (Hartley et al., 2004; Xu and Sharoff, 2014; Kim et al., 2015; Brunette, 2000; Karoubi, 2016). The acknowledgement of the contribution of terminology to translation quality is also echoed by the translation industry and users (Secar˘a, 2005; Lommel et al., 2014; Warburton, 2013). Accurately reproducing the content of the original and using appropriate terminology has become the official assessment criteria of some famous in-use translation-errorbased evaluation schemes. For instance, the MeLLANGE project (Secar˘a, 2005) defines more than six terminology errors1 , and the Multidimensional Quality Metrics lists terminology as one of the"
L18-1596,L16-1581,1,0.868196,"Missing"
L18-1596,L16-1359,0,0.0275974,"ality estimation tasks (i.e. supervised classification or regression to predict quality scores or class labels) as illustrated in the correlation analysis afterwards. 3.1. Term Classification N-gram technique is commonly used as a languageindependent approach, particularly for under-resourced language. Therefore, the term candidate classification is framed as a N-gram classification task rather than the conventional sequence labelling methods that are commonly seen in previous work (Zhou and Su, 2004; Finkel et al., 2004). From a pragmatic point of view, our features are computed by JATE 2.0 (Zhang et al., 2016). Most representative and language-independent statistic ATR techniques are available in the package. These features (See Table 1) . We briefly describe the features below: TTF, namely Term Total Frequency, is the total frequency of a candidate in the target corpus. This algorithm takes into account frequency information for retrieving words or phrases that are both highly indicative of document content and highly distinctive within a text collection. ATTF takes the average of TTF by dividing it by the number of documents in which the candidate term occurs. 3775 Feature TTF ATTF TTF-IDF RIDF C"
L18-1596,W04-1219,0,0.0577761,"and the length (i.e. number of tokens) of TT. This normalized term count can serve as a quality indicator in quality estimation tasks (i.e. supervised classification or regression to predict quality scores or class labels) as illustrated in the correlation analysis afterwards. 3.1. Term Classification N-gram technique is commonly used as a languageindependent approach, particularly for under-resourced language. Therefore, the term candidate classification is framed as a N-gram classification task rather than the conventional sequence labelling methods that are commonly seen in previous work (Zhou and Su, 2004; Finkel et al., 2004). From a pragmatic point of view, our features are computed by JATE 2.0 (Zhang et al., 2016). Most representative and language-independent statistic ATR techniques are available in the package. These features (See Table 1) . We briefly describe the features below: TTF, namely Term Total Frequency, is the total frequency of a candidate in the target corpus. This algorithm takes into account frequency information for retrieving words or phrases that are both highly indicative of document content and highly distinctive within a text collection. ATTF takes the average of TTF"
N15-1012,W11-2832,0,0.451539,"follow previous work and conduct experiments on the Penn Treebank (PTB), using Wall Street Jour118 nal sections 2–21 for training, 22 for development testing and 23 for final testing. Gold-standard dependency trees are derived from bracketed sentences in the treebank using Penn2Malt1 , and base noun phrases are treated as a single word (Wan et al., 2009; Zhang, 2013). The BLEU score (Papineni et al., 2002) is used to evaluate the performance of linearization, which has been adopted in former literals (Wan et al., 2009; White and Rajkumar, 2009; Zhang and Clark, 2011b) and recent shared-tasks (Belz et al., 2011). We use our implementation of the best-first system of Zhang (2013), which gives the state-of-the-art results, as the baseline. 4.1 Influence of Beam size We first study the influence of beam size by performing free word ordering on the development test data. BLEU score curves with different beam sizes are shown in Figure 7. From this figure, we can see that the systems with beam 64 and 128 achieve the best results. However, the 128-beam system does not improve the performance significantly (48.2 vs 47.5), but runs twice slower. As a result, we set the beam size to 64 in the remaining experim"
N15-1012,C10-1012,0,0.414105,"([σ|j i], β, A) ([σ|j], β, A ∪ {j → i}) Figure 1: The arc-standard parsing algorithm. Introduction Linearization is the task of ordering a bag of words into a grammatical and fluent sentence. Syntaxbased linearization algorithms generate a sentence along with its syntactic structure. Depending on how much syntactic information is available as inputs, recent work on syntactic linearization can be classified into free word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), which orders a bag of words without syntactic constraints, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by most previous work, and the best results have been achieved by a time-constrained best-first search framework (White, 2004a; White and Rajkumar, 20"
N15-1012,P04-1015,0,0.171585,"and their modifiers. The original feature templates of Zhang and Nivre (2011) also contain information of the front words on the buffer. However, since the buffer is unordered for linearization, we do not include these features. The linearization feature templates are specific for linearization, and captures surface ngram information. Each search state represents a partially linearized sentence. We represents the last word in the partially linearized sentence as w0 and the second last as w−1 . Given a set of labeled training examples, the averaged perceptron (Collins, 2002) with early update (Collins and Roark, 2004; Zhang and Nivre, 2011) is used to train the parameters θ of the model. 3.2 Input Syntactic Constraints The use of syntactic constraints to achieve better linearization performance has been studied in previous work. Wan et al. (2009) employ POS constraints 116 NP . VBD . Dr. Talcott . .2 1 led NP . . NP . .IN a team . 3 of.4 Harvard University . 5 Figure 4: Example partial tree. Words in the same sub dependency trees are grouped by rounded boxes. Word indices do not specify their orders. Base phrases (e.g. Dr. Talcott) are treated as single words. in learning a dependency language model. Zha"
N15-1012,W02-1001,0,0.0628857,"e context information for S0 , S1 and their modifiers. The original feature templates of Zhang and Nivre (2011) also contain information of the front words on the buffer. However, since the buffer is unordered for linearization, we do not include these features. The linearization feature templates are specific for linearization, and captures surface ngram information. Each search state represents a partially linearized sentence. We represents the last word in the partially linearized sentence as w0 and the second last as w−1 . Given a set of labeled training examples, the averaged perceptron (Collins, 2002) with early update (Collins and Roark, 2004; Zhang and Nivre, 2011) is used to train the parameters θ of the model. 3.2 Input Syntactic Constraints The use of syntactic constraints to achieve better linearization performance has been studied in previous work. Wan et al. (2009) employ POS constraints 116 NP . VBD . Dr. Talcott . .2 1 led NP . . NP . .IN a team . 3 of.4 Harvard University . 5 Figure 4: Example partial tree. Words in the same sub dependency trees are grouped by rounded boxes. Word indices do not specify their orders. Base phrases (e.g. Dr. Talcott) are treated as single words. i"
N15-1012,E14-1028,0,0.228722,"Missing"
N15-1012,P09-1091,0,0.647686,"β, A ∪ {j ← i}) ([σ|j i], β, A) ([σ|j], β, A ∪ {j → i}) Figure 1: The arc-standard parsing algorithm. Introduction Linearization is the task of ordering a bag of words into a grammatical and fluent sentence. Syntaxbased linearization algorithms generate a sentence along with its syntactic structure. Depending on how much syntactic information is available as inputs, recent work on syntactic linearization can be classified into free word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), which orders a bag of words without syntactic constraints, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by most previous work, and the best results have been achieved by a time-constrained best-first search framework (White, 2004a; W"
N15-1012,P10-1001,0,0.0355131,"connection between syntactic linearization and syntactic parsing: both build a syntactic tree over a sentence, with the former performing word ordering in addition to derivation construction. As a result, syntactic linearization can be treated as a generalized form of parsing, for which there is no input word order, and therefore extensions to parsing algorithms can be used to perform linearization. For syntactic parsing, the algorithm of Zhang and Nivre (2011) gives competitive accuracies under linear complexity. Compared with parsers that use dynamic programming (McDonald and Pereira, 2006; Koo and Collins, 2010), the efficient beam-search system is more suitable for the NP-hard linearization task. We extend the parser of Zhang and Nivre (2011), so that word ordering is performed in addition to syntactic tree construction. Experimental results show that the transition-based linearization system runs an order of magnitude faster than a state-ofthe-art best-first baseline, with improved accuracies in standard evaluation. Our linearizer is publicly available under GPL at http://sourceforge. net/projects/zgen/. 2 Transition-Based Parsing The task of dependency parsing is to find a dependency tree given an"
N15-1012,E06-1011,0,0.044596,"r method is inspired by the connection between syntactic linearization and syntactic parsing: both build a syntactic tree over a sentence, with the former performing word ordering in addition to derivation construction. As a result, syntactic linearization can be treated as a generalized form of parsing, for which there is no input word order, and therefore extensions to parsing algorithms can be used to perform linearization. For syntactic parsing, the algorithm of Zhang and Nivre (2011) gives competitive accuracies under linear complexity. Compared with parsers that use dynamic programming (McDonald and Pereira, 2006; Koo and Collins, 2010), the efficient beam-search system is more suitable for the NP-hard linearization task. We extend the parser of Zhang and Nivre (2011), so that word ordering is performed in addition to syntactic tree construction. Experimental results show that the transition-based linearization system runs an order of magnitude faster than a state-ofthe-art best-first baseline, with improved accuracies in standard evaluation. Our linearizer is publicly available under GPL at http://sourceforge. net/projects/zgen/. 2 Transition-Based Parsing The task of dependency parsing is to find a"
N15-1012,J08-4003,0,0.0934826,"onstruction. Experimental results show that the transition-based linearization system runs an order of magnitude faster than a state-ofthe-art best-first baseline, with improved accuracies in standard evaluation. Our linearizer is publicly available under GPL at http://sourceforge. net/projects/zgen/. 2 Transition-Based Parsing The task of dependency parsing is to find a dependency tree given an input sentence. Figure 2 shows an example dependency tree, which consists of dependency arcs that represent syntactic relations between pairs of words. A transition-based dependency parsing algorithm (Nivre, 2008) can be formalized as a transition system, S = (C, T, cs , Ct ), where C is the set of states, T is a set of transition actions, cs is the initial state and Ct is a set of terminal states. The parsing process is modeled as an application of a sequence of actions, transducing the initial state into a final state, while constructing de114 0 1 2 3 4 5 6 7 8 9 10 11 Transition S HIFT S HIFT S HIFT S HIFT S HIFT R IGHTA RC R IGHTA RC R IGHTA RC S HIFT R IGHTA RC L EFTA RC σ [] [1] [1 2] [1 2 3] [1 2 3 4] [1 2 3 4 5] [1 2 3 4] [1 2 3] [1 2] [1 2 6] [1 2] [2] β [1...6] [2...6] [3...6] [4...6] [5,6] ["
N15-1012,P02-1040,0,0.0922216,"build arcs between top two words i and j on the stack (line 10-13). If no arc exists between i and j, the next action should shift the parent word of i or a word in i’s sibling tree (line 14-16). 4 Experiments We follow previous work and conduct experiments on the Penn Treebank (PTB), using Wall Street Jour118 nal sections 2–21 for training, 22 for development testing and 23 for final testing. Gold-standard dependency trees are derived from bracketed sentences in the treebank using Penn2Malt1 , and base noun phrases are treated as a single word (Wan et al., 2009; Zhang, 2013). The BLEU score (Papineni et al., 2002) is used to evaluate the performance of linearization, which has been adopted in former literals (Wan et al., 2009; White and Rajkumar, 2009; Zhang and Clark, 2011b) and recent shared-tasks (Belz et al., 2011). We use our implementation of the best-first system of Zhang (2013), which gives the state-of-the-art results, as the baseline. 4.1 Influence of Beam size We first study the influence of beam size by performing free word ordering on the development test data. BLEU score curves with different beam sizes are shown in Figure 7. From this figure, we can see that the systems with beam 64 and"
N15-1012,P05-1025,0,0.015265,"nlabeled and labeled dependency trees (He et al., 2009; Zhang, 2013). These methods mostly use greedy or best-first algorithms to order each tree node. Our work is different by performing word ordering using a transition process. Besides dependency grammar, linearization with other syntactic grammars, such as CFG and CCG (White and Rajkumar, 2009; Zhang and Clark, 2011b), has also been studied. In this paper, we adopt the dependency grammar for transition-based linearization. However, since transition-based parsing algorithms has been successfully applied to different grammars, including CFG (Sagae et al., 2005) and CCG (Xu et al., 2014), our linearization method can be applied to these grammars. 6 Conclusion We studied transition-based syntactic linearization as an extension to transition-based parsing. Compared with best-first systems, the advantage of our transition-based algorithm includes bounded time complexity, and the guarantee to yield full sentences when given a bag of words. Experimental results show that our algorithm achieves improved accuracies, with significantly faster decoding speed compared with a state-of-the-art best-first baseline. We publicly release our code at http: //sourcefo"
N15-1012,E09-1097,0,0.510304,"State ([ ], [1...n], ∅) Final State ([ ], [ ], A) Induction Rules: S HIFT L EFTA RC R IGHTA RC (σ, [i|β], A) ([σ |i], β, A) ([σ|j i], β, A) ([σ|i], β, A ∪ {j ← i}) ([σ|j i], β, A) ([σ|j], β, A ∪ {j → i}) Figure 1: The arc-standard parsing algorithm. Introduction Linearization is the task of ordering a bag of words into a grammatical and fluent sentence. Syntaxbased linearization algorithms generate a sentence along with its syntactic structure. Depending on how much syntactic information is available as inputs, recent work on syntactic linearization can be classified into free word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), which orders a bag of words without syntactic constraints, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search"
N15-1012,D09-1043,0,0.751283,"9; Bohnet et al., 2010; Song et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by most previous work, and the best results have been achieved by a time-constrained best-first search framework (White, 2004a; White and Rajkumar, 2009; Zhang and Clark, 2011b; Song et al., 2014). Though empirically highly accurate, one drawback of this approach is that there is no asymptotic upper bound on the time complexity of finding the first full sentence. As a result, it can take 5–10 seconds to process a sentence, and sometimes fail to yield a full sentence at timeout. This issue is more severe for larger bags of words, and makes the algorithms practically less useful. We study the effect of an alternative learning and search framework for the linearization prob113 Human Language Technologies: The 2015 Annual Conference of the North"
N15-1012,P14-1021,1,0.82756,"cy trees (He et al., 2009; Zhang, 2013). These methods mostly use greedy or best-first algorithms to order each tree node. Our work is different by performing word ordering using a transition process. Besides dependency grammar, linearization with other syntactic grammars, such as CFG and CCG (White and Rajkumar, 2009; Zhang and Clark, 2011b), has also been studied. In this paper, we adopt the dependency grammar for transition-based linearization. However, since transition-based parsing algorithms has been successfully applied to different grammars, including CFG (Sagae et al., 2005) and CCG (Xu et al., 2014), our linearization method can be applied to these grammars. 6 Conclusion We studied transition-based syntactic linearization as an extension to transition-based parsing. Compared with best-first systems, the advantage of our transition-based algorithm includes bounded time complexity, and the guarantee to yield full sentences when given a bag of words. Experimental results show that our algorithm achieves improved accuracies, with significantly faster decoding speed compared with a state-of-the-art best-first baseline. We publicly release our code at http: //sourceforge.net/projects/zgen/. Fo"
N15-1012,J11-1005,1,0.277872,"ng et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by most previous work, and the best results have been achieved by a time-constrained best-first search framework (White, 2004a; White and Rajkumar, 2009; Zhang and Clark, 2011b; Song et al., 2014). Though empirically highly accurate, one drawback of this approach is that there is no asymptotic upper bound on the time complexity of finding the first full sentence. As a result, it can take 5–10 seconds to process a sentence, and sometimes fail to yield a full sentence at timeout. This issue is more severe for larger bags of words, and makes the algorithms practically less useful. We study the effect of an alternative learning and search framework for the linearization prob113 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the"
N15-1012,D11-1106,1,0.31725,"ng et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by most previous work, and the best results have been achieved by a time-constrained best-first search framework (White, 2004a; White and Rajkumar, 2009; Zhang and Clark, 2011b; Song et al., 2014). Though empirically highly accurate, one drawback of this approach is that there is no asymptotic upper bound on the time complexity of finding the first full sentence. As a result, it can take 5–10 seconds to process a sentence, and sometimes fail to yield a full sentence at timeout. This issue is more severe for larger bags of words, and makes the algorithms practically less useful. We study the effect of an alternative learning and search framework for the linearization prob113 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the"
N15-1012,P11-2033,1,0.955294,"cy tree. lem, which has a theoretical upper bound on the time complexity, and always yields a full sentence in quadratic time. Our method is inspired by the connection between syntactic linearization and syntactic parsing: both build a syntactic tree over a sentence, with the former performing word ordering in addition to derivation construction. As a result, syntactic linearization can be treated as a generalized form of parsing, for which there is no input word order, and therefore extensions to parsing algorithms can be used to perform linearization. For syntactic parsing, the algorithm of Zhang and Nivre (2011) gives competitive accuracies under linear complexity. Compared with parsers that use dynamic programming (McDonald and Pereira, 2006; Koo and Collins, 2010), the efficient beam-search system is more suitable for the NP-hard linearization task. We extend the parser of Zhang and Nivre (2011), so that word ordering is performed in addition to syntactic tree construction. Experimental results show that the transition-based linearization system runs an order of magnitude faster than a state-ofthe-art best-first baseline, with improved accuracies in standard evaluation. Our linearizer is publicly a"
N15-1012,E12-1075,1,0.781136,"n], ∅) Final State ([ ], [ ], A) Induction Rules: S HIFT L EFTA RC R IGHTA RC (σ, [i|β], A) ([σ |i], β, A) ([σ|j i], β, A) ([σ|i], β, A ∪ {j ← i}) ([σ|j i], β, A) ([σ|j], β, A ∪ {j → i}) Figure 1: The arc-standard parsing algorithm. Introduction Linearization is the task of ordering a bag of words into a grammatical and fluent sentence. Syntaxbased linearization algorithms generate a sentence along with its syntactic structure. Depending on how much syntactic information is available as inputs, recent work on syntactic linearization can be classified into free word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), which orders a bag of words without syntactic constraints, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by"
N16-1045,P11-1040,0,0.0941034,"ted Work In terms of scope, our work falls into the area of information extraction from social media (Guo et al., 2013; Li et al., 2015). The proposed event mention extraction system is domain-specific, similar to works that aim at detecting categorized events such as disaster outbreak (Sakaki et al., 2010; Neubig et al., 2011; Li and Cardie, 2013) and cybersecurity events (Ritter et al., 2015). Such work typically trains semi-supervised classifiers to determine events of interest due to the limitation of annotated data. On the other hand, a few studies devote to open domain event extraction (Benson et al., 2011; Ritter et al., 2012; Petrovi´c et al., 2010; Diao et al., 2012; Chierichetti et al., 2014; Li et al., 2014; Qiu and Zhang, 2014), in which an event category is not predefined, and clustering models are applied to automatically induce event types. In terms of method, the proposed model is in line with recent methods on deep learning for neural feature representations, which have seen success in some NLP tasks (Collobert and Weston, 2008; Collobert et al., 2011; Chen and Manning, 2014). 401 Competitive results have been obtained in sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014; Soch"
N16-1045,D14-1082,0,0.0161906,"due to the limitation of annotated data. On the other hand, a few studies devote to open domain event extraction (Benson et al., 2011; Ritter et al., 2012; Petrovi´c et al., 2010; Diao et al., 2012; Chierichetti et al., 2014; Li et al., 2014; Qiu and Zhang, 2014), in which an event category is not predefined, and clustering models are applied to automatically induce event types. In terms of method, the proposed model is in line with recent methods on deep learning for neural feature representations, which have seen success in some NLP tasks (Collobert and Weston, 2008; Collobert et al., 2011; Chen and Manning, 2014). 401 Competitive results have been obtained in sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013b), semantic relation classification (Hashimoto et al., 2013; Liu et al., 2015), and question answering (Dong et al., 2015; Iyyer et al., 2014). In addition, deep learning models have shown promising results on syntactic parsing (Dyer et al., 2015; Zhou et al., 2015) and machine translation (Cho et al., 2014). Compared to syntactic problems, semantic tasks see relatively larger improvements by using neural architectures, possible because of the capability of neural featu"
N16-1045,D14-1179,0,0.0231076,"Missing"
N16-1045,N10-1138,0,0.0279819,"resenting events as (ENTITY, DATE RANGE) tuples. The ENTITY in our seed events is defined as a name entity that appears in either the assailant or victim role of an attack event labeled by framesemantic parsing, and the DATE RANGE is a date window around the news publication date. We use a date window rather than a definite news publication date because news articles are not always published on the day a DDoS attack happened. Some examples are given in Figure 1. We parse DDoS attack news collected from http://www.ddosattacks.net2 with a state-of-the-art frame-semantic parsing system (SEMAFOR; Das et al. (2010)). Tweets are gathered using the Twitter Streaming API3 with a case-insensitive track keyword ddos. Name entities are extracted from both news articles and tweets using a Twitter-tuned NLP pipeline (Ritter et al., 2011).4 Table 2 shows two example DDoS attack news, where the ENTITY values are included in the victim roles, RBS, Ulster Bank, GovCERT and FBI in the first news, and Essex in the second. It is worth noting that the DDoS attack on RBS, Ulster Bank and Natwest was actually on 2015 July 31. The correlation between tweet mentions and news reports are shown in Figure 1, where each bar in"
N16-1045,P12-1056,0,0.0362544,"ation extraction from social media (Guo et al., 2013; Li et al., 2015). The proposed event mention extraction system is domain-specific, similar to works that aim at detecting categorized events such as disaster outbreak (Sakaki et al., 2010; Neubig et al., 2011; Li and Cardie, 2013) and cybersecurity events (Ritter et al., 2015). Such work typically trains semi-supervised classifiers to determine events of interest due to the limitation of annotated data. On the other hand, a few studies devote to open domain event extraction (Benson et al., 2011; Ritter et al., 2012; Petrovi´c et al., 2010; Diao et al., 2012; Chierichetti et al., 2014; Li et al., 2014; Qiu and Zhang, 2014), in which an event category is not predefined, and clustering models are applied to automatically induce event types. In terms of method, the proposed model is in line with recent methods on deep learning for neural feature representations, which have seen success in some NLP tasks (Collobert and Weston, 2008; Collobert et al., 2011; Chen and Manning, 2014). 401 Competitive results have been obtained in sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013b), semantic relation classification (Hashimoto e"
N16-1045,P15-1026,0,0.0243854,"ang, 2014), in which an event category is not predefined, and clustering models are applied to automatically induce event types. In terms of method, the proposed model is in line with recent methods on deep learning for neural feature representations, which have seen success in some NLP tasks (Collobert and Weston, 2008; Collobert et al., 2011; Chen and Manning, 2014). 401 Competitive results have been obtained in sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013b), semantic relation classification (Hashimoto et al., 2013; Liu et al., 2015), and question answering (Dong et al., 2015; Iyyer et al., 2014). In addition, deep learning models have shown promising results on syntactic parsing (Dyer et al., 2015; Zhou et al., 2015) and machine translation (Cho et al., 2014). Compared to syntactic problems, semantic tasks see relatively larger improvements by using neural architectures, possible because of the capability of neural features in better representing semantic information, which is relatively more difficult to capture by discrete indicator features. We consider event mention extraction as a semantic-heavy task and demonstrate that it can benefit significantly from neu"
N16-1045,P15-1033,0,0.016585,"es. In terms of method, the proposed model is in line with recent methods on deep learning for neural feature representations, which have seen success in some NLP tasks (Collobert and Weston, 2008; Collobert et al., 2011; Chen and Manning, 2014). 401 Competitive results have been obtained in sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013b), semantic relation classification (Hashimoto et al., 2013; Liu et al., 2015), and question answering (Dong et al., 2015; Iyyer et al., 2014). In addition, deep learning models have shown promising results on syntactic parsing (Dyer et al., 2015; Zhou et al., 2015) and machine translation (Cho et al., 2014). Compared to syntactic problems, semantic tasks see relatively larger improvements by using neural architectures, possible because of the capability of neural features in better representing semantic information, which is relatively more difficult to capture by discrete indicator features. We consider event mention extraction as a semantic-heavy task and demonstrate that it can benefit significantly from neural feature representations. 3 Baseline We take the method of Ritter et al. (2015) as a baseline. Given a tweet containing th"
N16-1045,P13-1024,0,0.0860558,"ic representation learning, which can effectively encodes syntactic and information about words, phrases and sentences in low-dimensional dense vectors. In this paper we exploit a deep neural model for event mention extraction, using word embeddings and a novel LSTM-based neural network structure to automatically obtain features for a tweet. Results on two human-annotated datasets show that the proposed LSTM-based representation yields significant improvements over Ritter et al. (2015). 2 Related Work In terms of scope, our work falls into the area of information extraction from social media (Guo et al., 2013; Li et al., 2015). The proposed event mention extraction system is domain-specific, similar to works that aim at detecting categorized events such as disaster outbreak (Sakaki et al., 2010; Neubig et al., 2011; Li and Cardie, 2013) and cybersecurity events (Ritter et al., 2015). Such work typically trains semi-supervised classifiers to determine events of interest due to the limitation of annotated data. On the other hand, a few studies devote to open domain event extraction (Benson et al., 2011; Ritter et al., 2012; Petrovi´c et al., 2010; Diao et al., 2012; Chierichetti et al., 2014; Li et"
N16-1045,D13-1137,0,0.0242804,"t al., 2012; Chierichetti et al., 2014; Li et al., 2014; Qiu and Zhang, 2014), in which an event category is not predefined, and clustering models are applied to automatically induce event types. In terms of method, the proposed model is in line with recent methods on deep learning for neural feature representations, which have seen success in some NLP tasks (Collobert and Weston, 2008; Collobert et al., 2011; Chen and Manning, 2014). 401 Competitive results have been obtained in sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013b), semantic relation classification (Hashimoto et al., 2013; Liu et al., 2015), and question answering (Dong et al., 2015; Iyyer et al., 2014). In addition, deep learning models have shown promising results on syntactic parsing (Dyer et al., 2015; Zhou et al., 2015) and machine translation (Cho et al., 2014). Compared to syntactic problems, semantic tasks see relatively larger improvements by using neural architectures, possible because of the capability of neural features in better representing semantic information, which is relatively more difficult to capture by discrete indicator features. We consider event mention extraction as a semantic-heavy t"
N16-1045,D14-1070,0,0.042829,"Missing"
N16-1045,P14-1062,0,0.0751279,"s 400–410, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics tion method was effective on classifying unbalanced datasets. Ritter et al. use manually-defined discrete features. However, the event mention extraction task is highly semantic-driven, and simple textual patterns may suffer limitations in representing subtle semantic differences between true event mentions and false cases with similar word patterns. Recently, deep learning received increasing research attention in the NLP community (Bengio, 2009; Mikolov et al., 2013; Pennington et al., 2014; Kalchbrenner et al., 2014; Vo and Zhang, 2015). One important advantage of deep learning is automatic representation learning, which can effectively encodes syntactic and information about words, phrases and sentences in low-dimensional dense vectors. In this paper we exploit a deep neural model for event mention extraction, using word embeddings and a novel LSTM-based neural network structure to automatically obtain features for a tweet. Results on two human-annotated datasets show that the proposed LSTM-based representation yields significant improvements over Ritter et al. (2015). 2 Related Work In terms of scope,"
N16-1045,D14-1181,0,0.00631574,"on et al., 2011; Ritter et al., 2012; Petrovi´c et al., 2010; Diao et al., 2012; Chierichetti et al., 2014; Li et al., 2014; Qiu and Zhang, 2014), in which an event category is not predefined, and clustering models are applied to automatically induce event types. In terms of method, the proposed model is in line with recent methods on deep learning for neural feature representations, which have seen success in some NLP tasks (Collobert and Weston, 2008; Collobert et al., 2011; Chen and Manning, 2014). 401 Competitive results have been obtained in sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013b), semantic relation classification (Hashimoto et al., 2013; Liu et al., 2015), and question answering (Dong et al., 2015; Iyyer et al., 2014). In addition, deep learning models have shown promising results on syntactic parsing (Dyer et al., 2015; Zhou et al., 2015) and machine translation (Cho et al., 2014). Compared to syntactic problems, semantic tasks see relatively larger improvements by using neural architectures, possible because of the capability of neural features in better representing semantic information, which is relatively more difficult to capture by discre"
N16-1045,D14-1214,0,0.0123756,", 2013; Li et al., 2015). The proposed event mention extraction system is domain-specific, similar to works that aim at detecting categorized events such as disaster outbreak (Sakaki et al., 2010; Neubig et al., 2011; Li and Cardie, 2013) and cybersecurity events (Ritter et al., 2015). Such work typically trains semi-supervised classifiers to determine events of interest due to the limitation of annotated data. On the other hand, a few studies devote to open domain event extraction (Benson et al., 2011; Ritter et al., 2012; Petrovi´c et al., 2010; Diao et al., 2012; Chierichetti et al., 2014; Li et al., 2014; Qiu and Zhang, 2014), in which an event category is not predefined, and clustering models are applied to automatically induce event types. In terms of method, the proposed model is in line with recent methods on deep learning for neural feature representations, which have seen success in some NLP tasks (Collobert and Weston, 2008; Collobert et al., 2011; Chen and Manning, 2014). 401 Competitive results have been obtained in sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013b), semantic relation classification (Hashimoto et al., 2013; Liu et al., 2015), and question"
N16-1045,P15-2047,0,0.0178706,"i et al., 2014; Li et al., 2014; Qiu and Zhang, 2014), in which an event category is not predefined, and clustering models are applied to automatically induce event types. In terms of method, the proposed model is in line with recent methods on deep learning for neural feature representations, which have seen success in some NLP tasks (Collobert and Weston, 2008; Collobert et al., 2011; Chen and Manning, 2014). 401 Competitive results have been obtained in sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013b), semantic relation classification (Hashimoto et al., 2013; Liu et al., 2015), and question answering (Dong et al., 2015; Iyyer et al., 2014). In addition, deep learning models have shown promising results on syntactic parsing (Dyer et al., 2015; Zhou et al., 2015) and machine translation (Cho et al., 2014). Compared to syntactic problems, semantic tasks see relatively larger improvements by using neural architectures, possible because of the capability of neural features in better representing semantic information, which is relatively more difficult to capture by discrete indicator features. We consider event mention extraction as a semantic-heavy task and demonstrate"
N16-1045,I11-1108,0,0.141047,"mention extraction, using word embeddings and a novel LSTM-based neural network structure to automatically obtain features for a tweet. Results on two human-annotated datasets show that the proposed LSTM-based representation yields significant improvements over Ritter et al. (2015). 2 Related Work In terms of scope, our work falls into the area of information extraction from social media (Guo et al., 2013; Li et al., 2015). The proposed event mention extraction system is domain-specific, similar to works that aim at detecting categorized events such as disaster outbreak (Sakaki et al., 2010; Neubig et al., 2011; Li and Cardie, 2013) and cybersecurity events (Ritter et al., 2015). Such work typically trains semi-supervised classifiers to determine events of interest due to the limitation of annotated data. On the other hand, a few studies devote to open domain event extraction (Benson et al., 2011; Ritter et al., 2012; Petrovi´c et al., 2010; Diao et al., 2012; Chierichetti et al., 2014; Li et al., 2014; Qiu and Zhang, 2014), in which an event category is not predefined, and clustering models are applied to automatically induce event types. In terms of method, the proposed model is in line with recen"
N16-1045,N13-1039,0,0.0334039,"Missing"
N16-1045,D14-1162,0,0.079714,"s of NAACL-HLT 2016, pages 400–410, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics tion method was effective on classifying unbalanced datasets. Ritter et al. use manually-defined discrete features. However, the event mention extraction task is highly semantic-driven, and simple textual patterns may suffer limitations in representing subtle semantic differences between true event mentions and false cases with similar word patterns. Recently, deep learning received increasing research attention in the NLP community (Bengio, 2009; Mikolov et al., 2013; Pennington et al., 2014; Kalchbrenner et al., 2014; Vo and Zhang, 2015). One important advantage of deep learning is automatic representation learning, which can effectively encodes syntactic and information about words, phrases and sentences in low-dimensional dense vectors. In this paper we exploit a deep neural model for event mention extraction, using word embeddings and a novel LSTM-based neural network structure to automatically obtain features for a tweet. Results on two human-annotated datasets show that the proposed LSTM-based representation yields significant improvements over Ritter et al. (2015). 2 Relat"
N16-1045,N10-1021,0,0.323074,"Missing"
N16-1045,D14-1201,1,0.823254,", 2015). The proposed event mention extraction system is domain-specific, similar to works that aim at detecting categorized events such as disaster outbreak (Sakaki et al., 2010; Neubig et al., 2011; Li and Cardie, 2013) and cybersecurity events (Ritter et al., 2015). Such work typically trains semi-supervised classifiers to determine events of interest due to the limitation of annotated data. On the other hand, a few studies devote to open domain event extraction (Benson et al., 2011; Ritter et al., 2012; Petrovi´c et al., 2010; Diao et al., 2012; Chierichetti et al., 2014; Li et al., 2014; Qiu and Zhang, 2014), in which an event category is not predefined, and clustering models are applied to automatically induce event types. In terms of method, the proposed model is in line with recent methods on deep learning for neural feature representations, which have seen success in some NLP tasks (Collobert and Weston, 2008; Collobert et al., 2011; Chen and Manning, 2014). 401 Competitive results have been obtained in sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013b), semantic relation classification (Hashimoto et al., 2013; Liu et al., 2015), and question answering (Dong et al"
N16-1045,D11-1141,0,0.0421826,"the DATE RANGE is a date window around the news publication date. We use a date window rather than a definite news publication date because news articles are not always published on the day a DDoS attack happened. Some examples are given in Figure 1. We parse DDoS attack news collected from http://www.ddosattacks.net2 with a state-of-the-art frame-semantic parsing system (SEMAFOR; Das et al. (2010)). Tweets are gathered using the Twitter Streaming API3 with a case-insensitive track keyword ddos. Name entities are extracted from both news articles and tweets using a Twitter-tuned NLP pipeline (Ritter et al., 2011).4 Table 2 shows two example DDoS attack news, where the ENTITY values are included in the victim roles, RBS, Ulster Bank, GovCERT and FBI in the first news, and Essex in the second. It is worth noting that the DDoS attack on RBS, Ulster Bank and Natwest was actually on 2015 July 31. The correlation between tweet mentions and news reports are shown in Figure 1, where each bar indicates the 2 Most of the articles are about DDoS attack events, while a smaller number discusses the nature of DDoS attacks and related issues. 3 https://dev.twitter.com/streaming/overview 4 https://github.com/aritter/"
N16-1045,D13-1170,0,0.0194983,"2011; Ritter et al., 2012; Petrovi´c et al., 2010; Diao et al., 2012; Chierichetti et al., 2014; Li et al., 2014; Qiu and Zhang, 2014), in which an event category is not predefined, and clustering models are applied to automatically induce event types. In terms of method, the proposed model is in line with recent methods on deep learning for neural feature representations, which have seen success in some NLP tasks (Collobert and Weston, 2008; Collobert et al., 2011; Chen and Manning, 2014). 401 Competitive results have been obtained in sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013b), semantic relation classification (Hashimoto et al., 2013; Liu et al., 2015), and question answering (Dong et al., 2015; Iyyer et al., 2014). In addition, deep learning models have shown promising results on syntactic parsing (Dyer et al., 2015; Zhou et al., 2015) and machine translation (Cho et al., 2014). Compared to syntactic problems, semantic tasks see relatively larger improvements by using neural architectures, possible because of the capability of neural features in better representing semantic information, which is relatively more difficult to capture by discrete indicator features"
N16-1045,P15-1117,1,0.814447,"hod, the proposed model is in line with recent methods on deep learning for neural feature representations, which have seen success in some NLP tasks (Collobert and Weston, 2008; Collobert et al., 2011; Chen and Manning, 2014). 401 Competitive results have been obtained in sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014; Socher et al., 2013b), semantic relation classification (Hashimoto et al., 2013; Liu et al., 2015), and question answering (Dong et al., 2015; Iyyer et al., 2014). In addition, deep learning models have shown promising results on syntactic parsing (Dyer et al., 2015; Zhou et al., 2015) and machine translation (Cho et al., 2014). Compared to syntactic problems, semantic tasks see relatively larger improvements by using neural architectures, possible because of the capability of neural features in better representing semantic information, which is relatively more difficult to capture by discrete indicator features. We consider event mention extraction as a semantic-heavy task and demonstrate that it can benefit significantly from neural feature representations. 3 Baseline We take the method of Ritter et al. (2015) as a baseline. Given a tweet containing the keyword ddos, the"
N16-1058,W11-2832,0,0.621016,"Missing"
N16-1058,C10-1009,0,0.0716788,"Missing"
N16-1058,C10-1012,0,0.487455,"k is the sparsity of S HIFT transition actions rather than heavy pruning. To address this issue, we propose a modification to the standard transition-based feature structure, which reduces feature sparsity and allows lookahead features at a small cost to decoding efficiency. Our model gives the best reported accuracies on all benchmarks, yet still being over 30 times faster compared with best-first-search. 1 Introduction Word ordering is the abstract language modeling task of making a grammatical sentence by ordering a bag of words (White, 2004; Zhang and Clark, 2015; De Gispert et al., 2014; Bohnet et al., 2010; Filippova and Strube, 2007; He et al., 2009), which is practically relevant to text-to-text applications such as summarization (Wann et al., 2009) and machine translation (Blackwood et al., 2010). Zhang (2013) built a discriminative word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, and ∗ Part of the work was done when the author was a visiting student at Singapore University of Technology and Design. One limitation of Zhang (2013) is relatively low time efficiency, due to the use of time-constrained best-first-search ("
N16-1058,D14-1082,0,0.0749744,"significantly better accuracies. It gives the best results for all standard benchmarks, being over thirty times faster than Zhang (2013). The new feature structures can be applied to other transition-based systems also. 2 Transition-based linearization Liu et al. (2015) uses a transition-based model for word ordering, building output sentences using a sequence of state transitions. Instead of scoring output syntax trees directly, it scores the transition action sequence for structural disambiguation. Liu et al.’s transition system extends from transition-based parsers (Nivre and Scholz, 2004; Chen and Manning, 2014), where a state consists of a stack to hold partially built outputs. Transition-based parsers use a queue to maintain input word sequences. However, for word ordering, the input is a set without order. Accordingly, Liu et al. uses a set to maintain the input. The transition actions are: • S HIFT-Word-POS, which removes Word from the set, assigns POS to it and pushes it onto the stack as the top word S0 ; • L EFTA RC-LABEL, which removes the second top of stack S1 and builds a dependency arc LABEL S1 ←−−−−− S0 ; • R IGHTA RC-LABEL, which removes the top of stack S0 and builds a dependency arc L"
N16-1058,E14-1028,0,0.222646,"Missing"
N16-1058,P07-1041,0,0.0805036,"S HIFT transition actions rather than heavy pruning. To address this issue, we propose a modification to the standard transition-based feature structure, which reduces feature sparsity and allows lookahead features at a small cost to decoding efficiency. Our model gives the best reported accuracies on all benchmarks, yet still being over 30 times faster compared with best-first-search. 1 Introduction Word ordering is the abstract language modeling task of making a grammatical sentence by ordering a bag of words (White, 2004; Zhang and Clark, 2015; De Gispert et al., 2014; Bohnet et al., 2010; Filippova and Strube, 2007; He et al., 2009), which is practically relevant to text-to-text applications such as summarization (Wann et al., 2009) and machine translation (Blackwood et al., 2010). Zhang (2013) built a discriminative word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, and ∗ Part of the work was done when the author was a visiting student at Singapore University of Technology and Design. One limitation of Zhang (2013) is relatively low time efficiency, due to the use of time-constrained best-first-search (White and Rajkumar, 2009) fo"
N16-1058,P09-1091,0,0.193752,"ther than heavy pruning. To address this issue, we propose a modification to the standard transition-based feature structure, which reduces feature sparsity and allows lookahead features at a small cost to decoding efficiency. Our model gives the best reported accuracies on all benchmarks, yet still being over 30 times faster compared with best-first-search. 1 Introduction Word ordering is the abstract language modeling task of making a grammatical sentence by ordering a bag of words (White, 2004; Zhang and Clark, 2015; De Gispert et al., 2014; Bohnet et al., 2010; Filippova and Strube, 2007; He et al., 2009), which is practically relevant to text-to-text applications such as summarization (Wann et al., 2009) and machine translation (Blackwood et al., 2010). Zhang (2013) built a discriminative word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, and ∗ Part of the work was done when the author was a visiting student at Singapore University of Technology and Design. One limitation of Zhang (2013) is relatively low time efficiency, due to the use of time-constrained best-first-search (White and Rajkumar, 2009) for decoding. In pra"
N16-1058,D15-1043,1,0.905874,"Missing"
N16-1058,N15-1012,1,0.745927,"translation (Blackwood et al., 2010). Zhang (2013) built a discriminative word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, and ∗ Part of the work was done when the author was a visiting student at Singapore University of Technology and Design. One limitation of Zhang (2013) is relatively low time efficiency, due to the use of time-constrained best-first-search (White and Rajkumar, 2009) for decoding. In practice, the system can take seconds to order a bag of words in order to obtain reasonable output quality. Recently, Liu et al. (2015) proposed a transition-based model to address this issue, which uses a sequence of state transitions to build the output. The system of Liu et al. (2015) achieves significant speed improvements without sacrificing accuracies when working with unlabeled dependency trees. With labeled dependency trees as input constraints, however, the system of Liu et al. (2015) gives much lower accuracies compared with Zhang (2013). While the low accuracy can be attributed to heavy pruning, we show that it can be mitigated by modifying the feature structure of the standard transitionbased framework, which scor"
N16-1058,C04-1010,0,0.108144,"l. (2015), but achieves significantly better accuracies. It gives the best results for all standard benchmarks, being over thirty times faster than Zhang (2013). The new feature structures can be applied to other transition-based systems also. 2 Transition-based linearization Liu et al. (2015) uses a transition-based model for word ordering, building output sentences using a sequence of state transitions. Instead of scoring output syntax trees directly, it scores the transition action sequence for structural disambiguation. Liu et al.’s transition system extends from transition-based parsers (Nivre and Scholz, 2004; Chen and Manning, 2014), where a state consists of a stack to hold partially built outputs. Transition-based parsers use a queue to maintain input word sequences. However, for word ordering, the input is a set without order. Accordingly, Liu et al. uses a set to maintain the input. The transition actions are: • S HIFT-Word-POS, which removes Word from the set, assigns POS to it and pushes it onto the stack as the top word S0 ; • L EFTA RC-LABEL, which removes the second top of stack S1 and builds a dependency arc LABEL S1 ←−−−−− S0 ; • R IGHTA RC-LABEL, which removes the top of stack S0 and"
N16-1058,E09-1097,0,0.0746629,"-based feature structure, which reduces feature sparsity and allows lookahead features at a small cost to decoding efficiency. Our model gives the best reported accuracies on all benchmarks, yet still being over 30 times faster compared with best-first-search. 1 Introduction Word ordering is the abstract language modeling task of making a grammatical sentence by ordering a bag of words (White, 2004; Zhang and Clark, 2015; De Gispert et al., 2014; Bohnet et al., 2010; Filippova and Strube, 2007; He et al., 2009), which is practically relevant to text-to-text applications such as summarization (Wann et al., 2009) and machine translation (Blackwood et al., 2010). Zhang (2013) built a discriminative word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, and ∗ Part of the work was done when the author was a visiting student at Singapore University of Technology and Design. One limitation of Zhang (2013) is relatively low time efficiency, due to the use of time-constrained best-first-search (White and Rajkumar, 2009) for decoding. In practice, the system can take seconds to order a bag of words in order to obtain reasonable output qualit"
N16-1058,D09-1043,0,0.0574509,"; Filippova and Strube, 2007; He et al., 2009), which is practically relevant to text-to-text applications such as summarization (Wann et al., 2009) and machine translation (Blackwood et al., 2010). Zhang (2013) built a discriminative word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, and ∗ Part of the work was done when the author was a visiting student at Singapore University of Technology and Design. One limitation of Zhang (2013) is relatively low time efficiency, due to the use of time-constrained best-first-search (White and Rajkumar, 2009) for decoding. In practice, the system can take seconds to order a bag of words in order to obtain reasonable output quality. Recently, Liu et al. (2015) proposed a transition-based model to address this issue, which uses a sequence of state transitions to build the output. The system of Liu et al. (2015) achieves significant speed improvements without sacrificing accuracies when working with unlabeled dependency trees. With labeled dependency trees as input constraints, however, the system of Liu et al. (2015) gives much lower accuracies compared with Zhang (2013). While the low accuracy can"
N16-1058,J11-1005,1,0.936958,"ed dependency trees. With labeled dependency trees as input constraints, however, the system of Liu et al. (2015) gives much lower accuracies compared with Zhang (2013). While the low accuracy can be attributed to heavy pruning, we show that it can be mitigated by modifying the feature structure of the standard transitionbased framework, which scores the output transition sequence by summing the scores of each transition action. Transition actions are treated as an atomic output component in each feature instance. This works effectively for most structured prediction tasks, including parsing (Zhang and Clark, 2011a). For word ordering, however, transition actions are significantly more complex and sparse compared 488 Proceedings of NAACL-HLT 2016, pages 488–493, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics with parsing, which limits the power of the traditional feature model. We instead break down complex actions into smaller components, merging some components into configuration features which reduces sparsity in the output action and allows flexible lookahead features to be defined according to the next action to be applied. On the other hand, this change"
N16-1058,D11-1106,1,0.954058,"ed dependency trees. With labeled dependency trees as input constraints, however, the system of Liu et al. (2015) gives much lower accuracies compared with Zhang (2013). While the low accuracy can be attributed to heavy pruning, we show that it can be mitigated by modifying the feature structure of the standard transitionbased framework, which scores the output transition sequence by summing the scores of each transition action. Transition actions are treated as an atomic output component in each feature instance. This works effectively for most structured prediction tasks, including parsing (Zhang and Clark, 2011a). For word ordering, however, transition actions are significantly more complex and sparse compared 488 Proceedings of NAACL-HLT 2016, pages 488–493, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics with parsing, which limits the power of the traditional feature model. We instead break down complex actions into smaller components, merging some components into configuration features which reduces sparsity in the output action and allows flexible lookahead features to be defined according to the next action to be applied. On the other hand, this change"
N16-1058,J15-3005,1,0.868168,"hat the main cause for the performance bottleneck is the sparsity of S HIFT transition actions rather than heavy pruning. To address this issue, we propose a modification to the standard transition-based feature structure, which reduces feature sparsity and allows lookahead features at a small cost to decoding efficiency. Our model gives the best reported accuracies on all benchmarks, yet still being over 30 times faster compared with best-first-search. 1 Introduction Word ordering is the abstract language modeling task of making a grammatical sentence by ordering a bag of words (White, 2004; Zhang and Clark, 2015; De Gispert et al., 2014; Bohnet et al., 2010; Filippova and Strube, 2007; He et al., 2009), which is practically relevant to text-to-text applications such as summarization (Wann et al., 2009) and machine translation (Blackwood et al., 2010). Zhang (2013) built a discriminative word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, and ∗ Part of the work was done when the author was a visiting student at Singapore University of Technology and Design. One limitation of Zhang (2013) is relatively low time efficiency, due to t"
N16-1058,E12-1075,1,0.932157,"Missing"
N16-1058,D14-1021,1,0.885954,"rmation Technology, Hyderabad (IIIT Hyderabad) ‡Singapore University of Technology and Design ratish.surendran@research.iiit.ac.in yue zhang@sutd.edu.sg m.shrivastava@iiit.ac.in Abstract yields a sentence together with its dependency parse tree that conforms to input syntactic constraints. The system is flexible with respect to input constraints, performing abstract word ordering when no constraints are given, but gives increasingly confined outputs when more POS and dependency relations are specified. It has been applied to syntactic linearization (Song et al., 2014) and machine translation (Zhang et al., 2014). It has been shown that transition-based methods can be used for syntactic word ordering and tree linearization, achieving significantly faster speed compared with traditional best-first methods. State-of-the-art transitionbased models give competitive results on abstract word ordering and unlabeled tree linearization, but significantly worse results on labeled tree linearization. We demonstrate that the main cause for the performance bottleneck is the sparsity of S HIFT transition actions rather than heavy pruning. To address this issue, we propose a modification to the standard transition-b"
N18-1050,C16-1038,0,0.0203923,"ances regularize task-specific parameters. The feature extraction for MTRL follows (Blitzer et al., 2007). We use this baseline to demonstrate the effectiveness of dense features generated by neural models. MDA (Chen et al., 2012) is a cross-domain baseline, which utilizes marginalized de-noising auto-encoders to learn a shared hidden representation by reconstructing pivot features from corrupted inputs. FEMA (Yang and Eisenstein, 2015) is a crossdomain baseline, which utilizes techniques from neural language models to directly learn feature embeddings and is more robust to domain shift. NDA (Kim et al., 2016) is a cross-domain baseline, which uses m + 1 LSTMs, where one LSTM captures global information across all m domains and the remaining m LSTM capture domainspecific information. We set the size of word embeddings K to 300, which are initialized using the word2vec model6 on news. To obtain the best performance, the parameters are set using grid search based on development results. The dropout ratio is chosen from [0.3, , 1]. Learning rate is chosen from (8) Equations 7 and 8 are performed iteratively to generate domain-invariant representations. We name this method DSR-at. 5 Experiments We eval"
N18-1050,P07-1056,0,0.836182,", while fixing θdg and θds . X Li (fi (sij , di ), yji ) − λLat (fat (sij ), di ) max θdc Instance 3770 1907 1572 10662 5.2 Baselines and Hyperparameters Di In addition to the Mix baseline, the Multi baseline (Liu et al., 2016) and our domain-aware models, DSR, DSR-sa, DSR-ctx, DSR-at, we also experiment with the following baselines: MTRL (Zhang and Yeung, 2012) is a state-ofthe-art multi-task learning method with discrete features. The method models covariances between task classifiers, and in turn the covariances regularize task-specific parameters. The feature extraction for MTRL follows (Blitzer et al., 2007). We use this baseline to demonstrate the effectiveness of dense features generated by neural models. MDA (Chen et al., 2012) is a cross-domain baseline, which utilizes marginalized de-noising auto-encoders to learn a shared hidden representation by reconstructing pivot features from corrupted inputs. FEMA (Yang and Eisenstein, 2015) is a crossdomain baseline, which utilizes techniques from neural language models to directly learn feature embeddings and is more robust to domain shift. NDA (Kim et al., 2016) is a cross-domain baseline, which uses m + 1 LSTMs, where one LSTM captures global info"
N18-1050,P11-1014,0,0.0248473,"y network stores the domain-specific training instances for obtaining context knowledge. 6 Related Work Domain Adaptation (Blitzer et al., 2007; Titov, 2011; Yu and Jiang, 2015) adapts classifiers trained on a source domain to an unseen target domain. One stream of work focuses on learning a general representation for different domains based on the co-occurrences of domain-specific and domain-independent features (Blitzer et al., 2007; Pan et al., 2011; Yu and Jiang, 2015; Yang et al., 2017). Another stream of work tries to identify domain-specific words to improve crossdomain classification (Bollegala et al., 2011; Li et al., 2012; Zhang et al., 2014; Qiu and Zhang, 2015). Different from previous work, we utilize multiple source domains for cross-domain validation, which makes our method more general and domain-aware. Multi-domain Learning jointly learn multiple domains to improve generalization. One strand of work (Dredze and Crammer, 2008; Saha et al., 2011; Zhang and Yeung, 2012) uses covari7 Conclusion We investigated domain representations in multitask learning for multi-domain sentiment analysis, showing that leveraging domain descriptors, examples and adversarial training to learn domain represe"
N18-1050,P12-1043,0,0.0207011,"ain-specific training instances for obtaining context knowledge. 6 Related Work Domain Adaptation (Blitzer et al., 2007; Titov, 2011; Yu and Jiang, 2015) adapts classifiers trained on a source domain to an unseen target domain. One stream of work focuses on learning a general representation for different domains based on the co-occurrences of domain-specific and domain-independent features (Blitzer et al., 2007; Pan et al., 2011; Yu and Jiang, 2015; Yang et al., 2017). Another stream of work tries to identify domain-specific words to improve crossdomain classification (Bollegala et al., 2011; Li et al., 2012; Zhang et al., 2014; Qiu and Zhang, 2015). Different from previous work, we utilize multiple source domains for cross-domain validation, which makes our method more general and domain-aware. Multi-domain Learning jointly learn multiple domains to improve generalization. One strand of work (Dredze and Crammer, 2008; Saha et al., 2011; Zhang and Yeung, 2012) uses covari7 Conclusion We investigated domain representations in multitask learning for multi-domain sentiment analysis, showing that leveraging domain descriptors, examples and adversarial training to learn domain representations give sig"
N18-1050,P17-1001,0,0.0666444,"Missing"
N18-1050,D08-1083,0,0.0378398,"hich is used to map adversarially trained domaingeneral Bi-LSTM input representations into domain-specific representations. Based on this model, we further expand the input representation with exemplary domain knowledge, collected by attending over a memory network of domain training data. Results show that our model outperforms existing methods on multidomain sentiment analysis significantly, giving the best accuracies on two different benchmarks. 1 Introduction Sentiment analysis has received constant research attention due to its importance to business (Pang et al., 2002; Hu and Liu, 2004; Choi and Cardie, 2008; Socher et al., 2012; Vo and Zhang, 2015; Tang et al., 2014). For multiple domains, such as movies, restaurants and digital products, manually annotated datasets have been made available. A useful research question is how to leverage resources available across all domains to improve sentiment classification on a certain domain. One naive domain-agnostic baseline is to combine all training data, ignoring domain differences. However, domain knowledge is one valuable source of information available. To utilize this, there has been recent work on domain-aware models via multi-task learning (Liu e"
N18-1050,D08-1072,0,0.0319289,"omains based on the co-occurrences of domain-specific and domain-independent features (Blitzer et al., 2007; Pan et al., 2011; Yu and Jiang, 2015; Yang et al., 2017). Another stream of work tries to identify domain-specific words to improve crossdomain classification (Bollegala et al., 2011; Li et al., 2012; Zhang et al., 2014; Qiu and Zhang, 2015). Different from previous work, we utilize multiple source domains for cross-domain validation, which makes our method more general and domain-aware. Multi-domain Learning jointly learn multiple domains to improve generalization. One strand of work (Dredze and Crammer, 2008; Saha et al., 2011; Zhang and Yeung, 2012) uses covari7 Conclusion We investigated domain representations in multitask learning for multi-domain sentiment analysis, showing that leveraging domain descriptors, examples and adversarial training to learn domain representations give significant improve548 ments compared with strong multi-task learning baselines. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in neural information processing systems. pages 2672–2680. Ack"
N18-1050,P04-1035,0,0.0171252,"main. 5.1 Experimental Settings We conduct experiments on two benchmark datasets. The datasets are balanced, so we use accuracy as the evaluation metric in the experiments. The dataset 1 contains four domains. The statistics are shown in Table 1 , which also shows the accuracies using baseline method Mix trained and tested on each domain. Camera2 consists of reviews with respect to digital products such as cameras and MP3 players (Hu and Liu, 2004). Laptop and Restaurant are laptop and restaurant reviews, respectively, obtained from SemEval 2015 Task 123 . Movie4 are movie reviews provided by Pang and Lee (2004). The dataset 2 is Blitzer’s multi-domain sentiment dataset (Blitzer et al., 2007), which contains 2 http://www.cs.uic.edu/˜liub/FBS/ sentiment-analysis.html 3 Since the original dataset targets aspect-level sentiment analysis, we remove the sentences with opposite polarities on different aspects. The remaining sentences are labeled with the unambiguous polarity. 4 https://www.cs.cornell.edu/people/ pabo/movie-review-data/ 5 https://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/ 6 https://code.google.com/archive/p/ word2vec/ 545 effectiveness of domain-specific input representations in multi-dom"
N18-1050,W02-1011,0,0.0242005,"arned for representing each domain, which is used to map adversarially trained domaingeneral Bi-LSTM input representations into domain-specific representations. Based on this model, we further expand the input representation with exemplary domain knowledge, collected by attending over a memory network of domain training data. Results show that our model outperforms existing methods on multidomain sentiment analysis significantly, giving the best accuracies on two different benchmarks. 1 Introduction Sentiment analysis has received constant research attention due to its importance to business (Pang et al., 2002; Hu and Liu, 2004; Choi and Cardie, 2008; Socher et al., 2012; Vo and Zhang, 2015; Tang et al., 2014). For multiple domains, such as movies, restaurants and digital products, manually annotated datasets have been made available. A useful research question is how to leverage resources available across all domains to improve sentiment classification on a certain domain. One naive domain-agnostic baseline is to combine all training data, ignoring domain differences. However, domain knowledge is one valuable source of information available. To utilize this, there has been recent work on domain-aw"
N18-1050,D12-1110,0,0.0463167,"ersarially trained domaingeneral Bi-LSTM input representations into domain-specific representations. Based on this model, we further expand the input representation with exemplary domain knowledge, collected by attending over a memory network of domain training data. Results show that our model outperforms existing methods on multidomain sentiment analysis significantly, giving the best accuracies on two different benchmarks. 1 Introduction Sentiment analysis has received constant research attention due to its importance to business (Pang et al., 2002; Hu and Liu, 2004; Choi and Cardie, 2008; Socher et al., 2012; Vo and Zhang, 2015; Tang et al., 2014). For multiple domains, such as movies, restaurants and digital products, manually annotated datasets have been made available. A useful research question is how to leverage resources available across all domains to improve sentiment classification on a certain domain. One naive domain-agnostic baseline is to combine all training data, ignoring domain differences. However, domain knowledge is one valuable source of information available. To utilize this, there has been recent work on domain-aware models via multi-task learning (Liu et al., 2016; Nam and"
N18-1050,P14-1146,0,0.0277521,"input representations into domain-specific representations. Based on this model, we further expand the input representation with exemplary domain knowledge, collected by attending over a memory network of domain training data. Results show that our model outperforms existing methods on multidomain sentiment analysis significantly, giving the best accuracies on two different benchmarks. 1 Introduction Sentiment analysis has received constant research attention due to its importance to business (Pang et al., 2002; Hu and Liu, 2004; Choi and Cardie, 2008; Socher et al., 2012; Vo and Zhang, 2015; Tang et al., 2014). For multiple domains, such as movies, restaurants and digital products, manually annotated datasets have been made available. A useful research question is how to leverage resources available across all domains to improve sentiment classification on a certain domain. One naive domain-agnostic baseline is to combine all training data, ignoring domain differences. However, domain knowledge is one valuable source of information available. To utilize this, there has been recent work on domain-aware models via multi-task learning (Liu et al., 2016; Nam and Han, 2016), building an output layer for"
N18-1050,P11-1007,0,0.022354,"oposed by Sukhbaatar et al. (2015) by storing embeddings of input sequences, which requires much less supervision compared to Weston et al. (2014). Kumar et al. (2016) introduces a general dynamic memory network, which iteratively attends over episodic memories to generate answers. Xiong et al. (2016) extends Kumar et al. (2016) by introducing a new architecture to cater image inputs and better capture input dependencies. In similar spirits, our memory network stores the domain-specific training instances for obtaining context knowledge. 6 Related Work Domain Adaptation (Blitzer et al., 2007; Titov, 2011; Yu and Jiang, 2015) adapts classifiers trained on a source domain to an unseen target domain. One stream of work focuses on learning a general representation for different domains based on the co-occurrences of domain-specific and domain-independent features (Blitzer et al., 2007; Pan et al., 2011; Yu and Jiang, 2015; Yang et al., 2017). Another stream of work tries to identify domain-specific words to improve crossdomain classification (Bollegala et al., 2011; Li et al., 2012; Zhang et al., 2014; Qiu and Zhang, 2015). Different from previous work, we utilize multiple source domains for cros"
N18-1050,N15-1069,0,0.0173759,"lines: MTRL (Zhang and Yeung, 2012) is a state-ofthe-art multi-task learning method with discrete features. The method models covariances between task classifiers, and in turn the covariances regularize task-specific parameters. The feature extraction for MTRL follows (Blitzer et al., 2007). We use this baseline to demonstrate the effectiveness of dense features generated by neural models. MDA (Chen et al., 2012) is a cross-domain baseline, which utilizes marginalized de-noising auto-encoders to learn a shared hidden representation by reconstructing pivot features from corrupted inputs. FEMA (Yang and Eisenstein, 2015) is a crossdomain baseline, which utilizes techniques from neural language models to directly learn feature embeddings and is more robust to domain shift. NDA (Kim et al., 2016) is a cross-domain baseline, which uses m + 1 LSTMs, where one LSTM captures global information across all m domains and the remaining m LSTM capture domainspecific information. We set the size of word embeddings K to 300, which are initialized using the word2vec model6 on news. To obtain the best performance, the parameters are set using grid search based on development results. The dropout ratio is chosen from [0.3, ,"
N18-1050,E14-1062,1,0.809161,"ning instances for obtaining context knowledge. 6 Related Work Domain Adaptation (Blitzer et al., 2007; Titov, 2011; Yu and Jiang, 2015) adapts classifiers trained on a source domain to an unseen target domain. One stream of work focuses on learning a general representation for different domains based on the co-occurrences of domain-specific and domain-independent features (Blitzer et al., 2007; Pan et al., 2011; Yu and Jiang, 2015; Yang et al., 2017). Another stream of work tries to identify domain-specific words to improve crossdomain classification (Bollegala et al., 2011; Li et al., 2012; Zhang et al., 2014; Qiu and Zhang, 2015). Different from previous work, we utilize multiple source domains for cross-domain validation, which makes our method more general and domain-aware. Multi-domain Learning jointly learn multiple domains to improve generalization. One strand of work (Dredze and Crammer, 2008; Saha et al., 2011; Zhang and Yeung, 2012) uses covari7 Conclusion We investigated domain representations in multitask learning for multi-domain sentiment analysis, showing that leveraging domain descriptors, examples and adversarial training to learn domain representations give significant improve548"
N18-1191,P16-1035,0,0.284567,"tion. These methods requires massive annotated data, which is expensive to obtain for concept stock recommendation. Query Expansion: One line of work (Cao 1 https://github.com/leuchine/ concept-stock-recommendation et al., 2008; Preston and Colman, 2000) utilizes a feedback-based relevance model to expand queries. Another line applies language modeling to estimate conditional probabilities of concepts given a query, and expands the query with the most probable concepts (Bai et al., 2005; Carpineto and Romano, 2012). Recently, word embeddings are adopted for query expansion (Kuzi et al., 2016; Diaz et al., 2016). Our framework belongs to this line of work with a difference that we use reinforcement learning to dynamically expand queries instead of following handcrafted rules such as using k-nearest neighbors. Reinforcement Learning: Our work aligns with existing work using reinforcement learning to collect evidences. Narasimhan et al. (2016) utilize external evidence to improve information extraction. While the work requires handcrafted features, our model uses dense embedding features. Athukorala et al. (2016) devise an interactive search engine balancing exploration and exploitation. Their work rel"
N18-1191,D15-1001,0,0.0141806,"n. 6.3 Learning We adopt Q-learning (Sutton and Barto, 1998) to optimize the neural agent, which uses a function Q(z, a) to represent Q-values and the recursive Bellman equation to perform Q-value iteration, when observing a new sample (z, a, z 0 , r). Since the state space Z can be extremely large in practice, we represent the Q-value function Q(z, a) with a neural agent shown in Figure 2 (b) named the action network parametrized by θ (Mnih et al., 2015). The deep Q-learning method has the ability to capture nonlinear features and achieve better performance compared with traditional methods (Narasimhan et al., 2015). Formally, Q(z, a) = Q(z, a; θ) (10) To improve learning stability, sample reward estimates are obtained from a separate target network with the same architecture as the action network (Mnih et al., 2015), parametrized by θtarget . Formally, the sample reward estimate of We construct two datasets from the Chinese websites, Jinrongjie2 and Tonghuashun3 , respectively, which are two mass medias for China stock markets. These two websites periodically publish their concept stock lists, which are manually collected and analyzed by their financial professionals. We observe high quote change correl"
N18-1191,D17-1061,0,0.046474,"Missing"
N18-1191,D14-1162,0,0.0855801,". Embedding Bi-LSTM Concept c Reward Bi-LSTM . . . . . . o Evidence Document {F1c,o} Data Source S1 Bi-LSTM Bi-LSTM . . . [c, [ce]] . . . E1 Stock-concept Pair (c, o) Bi-LSTM Eiv1 Evidence Document {Fnc,o} . . . Data Source Sn Query . . . . . . . . . Eno Global Embedding E Envn Evidences Documents {F1context} Evidences Documents{Fncontext} ([c, [ce]], vi) (a) Ranking Baseline Data Source S1 . . . Data Source Sn (b) Reinforcement Learning for Query Expansion Figure 2: Concept Stock Recommendation Models 4 Representation Motivated by the success of embedding-based models (Mikolov et al., 2013; Pennington et al., 2014) in capturing semantic regularities, we use embeddings to represent concepts, stocks and documents. In particular, we adopt Chinese word segmentation (Yang et al., 2017) to obtain words from documents. Doc2Vec (Le and Mikolov, 2014) is then used on the documents of each data source Si to obtain a local word embedding matrix E i and a local document embedding matrix F i , where each column of E i (F i ) corresponds to a word (document) vector representation of Si . In particular, we use embeddings, Eci and Eoi as the local concept representation of c and the local stock representation of o in d"
N18-1191,P17-1078,1,0.787911,"Eiv1 Evidence Document {Fnc,o} . . . Data Source Sn Query . . . . . . . . . Eno Global Embedding E Envn Evidences Documents {F1context} Evidences Documents{Fncontext} ([c, [ce]], vi) (a) Ranking Baseline Data Source S1 . . . Data Source Sn (b) Reinforcement Learning for Query Expansion Figure 2: Concept Stock Recommendation Models 4 Representation Motivated by the success of embedding-based models (Mikolov et al., 2013; Pennington et al., 2014) in capturing semantic regularities, we use embeddings to represent concepts, stocks and documents. In particular, we adopt Chinese word segmentation (Yang et al., 2017) to obtain words from documents. Doc2Vec (Le and Mikolov, 2014) is then used on the documents of each data source Si to obtain a local word embedding matrix E i and a local document embedding matrix F i , where each column of E i (F i ) corresponds to a word (document) vector representation of Si . In particular, we use embeddings, Eci and Eoi as the local concept representation of c and the local stock representation of o in data source Si , respectively. Furthermore, we obtain a global word embedding matrix E by averaging the local embedding matrices, E 1 ...E n , where Ec and Eo are regarde"
N18-2090,P16-1014,0,0.046067,"wer, which encodes all words and the word order. Attentive-matching synthesizes a vector by computing a weighted sum of all answer states against the passage state, then compares the vector with the passage state. It also considers all words in the answer but without word order. Finally, max-attentive-matching only considers the most relevant answer state to the passage state. Pvocab = softmax(V1 [st ; ct ] + b1 ), where V1 and b1 are model parameters, and the number of rows in V1 is the size of the vocabulary. Since many passage words also appear in the question, we adopt the copy mechanism (Gulcehre et al., 2016; Gu et al., 2016), which integrates the attention over input words into the final vocabulary distribution. The probability distribution is defined as the interpolation: Pf inal = gt Pvocab + (1 − gt )Pattn , where gt is the switch for controlling generating a word from the vocabulary or directly copying it from the passage. Pvocab is the vocabulary probability distribution as defined above, and Pattn is calculated based on the current attention distribution by merging probabilities of duplicated words. Finally, gt is defined as: Multi-perspective matching These strategies require a function f"
N18-2090,D16-1264,0,0.134063,"ng et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target answer and the passage, so that contextual information can be better considered by the encoder. In particular, matching is used between the target answer and the passage for collecting relevant contextual information. We adopt the multiperspective context matching (MPCM) algorithm (Wang et al., 2017b), which takes two texts as input before producing a vector of numbers, representing similarity under different perspectives. Results on SQuAD (Rajpurkar et al., 2016) show that our model gives better BLEU scores than the state of the art. Furthermore, the questions generated by our model help to improve a strong extractive QA system. Our code is available at https://github.com/freesunshine0316/MPQG. Work done during an internship at IBM. 569 Proceedings of NAACL-HLT 2018, pages 569–574 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Baseline: sequence-to-sequence Our baseline is a sequence-to-sequence model (Bahdanau et al., 2015) with the copy mechanism (Gulcehre et al., 2016; Gu et al., 2016). It uses an LSTM"
N18-2090,N10-1086,0,0.479507,"born. This can be easily determined by leveraging the contextual information of “10 july 1856 – 7 january 1943”, while it is relatively hard when only the answer position information is adopted. Introduction The task of natural question generation (NQG) is to generate a fluent and relevant question given a passage and a target answer. Recently NQG has received increasing attention from both the industrial and academic communities because of its values for improving QA systems by automatically increasing the training data. It can also be used for educational purposes such as language learning (Heilman and Smith, 2010). One example is shown in Table 1, where a question “when was nikola tesla born ?” is generated given a passage and a fact “1856”. Existing work for NQG uses a sequence-to-sequence model (Sutskever et al., 2014), which takes a passage as input for generating a question. They either entirely ignore the target answer (Du et al., 2017), or directly hard-code answer positions (Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target ans"
N18-2090,D17-1090,0,0.106253,"Missing"
N18-2090,P17-1123,0,0.132216,"tly NQG has received increasing attention from both the industrial and academic communities because of its values for improving QA systems by automatically increasing the training data. It can also be used for educational purposes such as language learning (Heilman and Smith, 2010). One example is shown in Table 1, where a question “when was nikola tesla born ?” is generated given a passage and a fact “1856”. Existing work for NQG uses a sequence-to-sequence model (Sutskever et al., 2014), which takes a passage as input for generating a question. They either entirely ignore the target answer (Du et al., 2017), or directly hard-code answer positions (Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target answer and the passage, so that contextual information can be better considered by the encoder. In particular, matching is used between the target answer and the passage for collecting relevant contextual information. We adopt the multiperspective context matching (MPCM) algorithm (Wang et al., 2017b), which takes two texts as input be"
N18-2090,P16-1154,0,0.116662,"words and the word order. Attentive-matching synthesizes a vector by computing a weighted sum of all answer states against the passage state, then compares the vector with the passage state. It also considers all words in the answer but without word order. Finally, max-attentive-matching only considers the most relevant answer state to the passage state. Pvocab = softmax(V1 [st ; ct ] + b1 ), where V1 and b1 are model parameters, and the number of rows in V1 is the size of the vocabulary. Since many passage words also appear in the question, we adopt the copy mechanism (Gulcehre et al., 2016; Gu et al., 2016), which integrates the attention over input words into the final vocabulary distribution. The probability distribution is defined as the interpolation: Pf inal = gt Pvocab + (1 − gt )Pattn , where gt is the switch for controlling generating a word from the vocabulary or directly copying it from the passage. Pvocab is the vocabulary probability distribution as defined above, and Pattn is calculated based on the current attention distribution by merging probabilities of duplicated words. Finally, gt is defined as: Multi-perspective matching These strategies require a function fm to match two vec"
N18-2090,P17-1096,0,0.0263172,"ic communities because of its values for improving QA systems by automatically increasing the training data. It can also be used for educational purposes such as language learning (Heilman and Smith, 2010). One example is shown in Table 1, where a question “when was nikola tesla born ?” is generated given a passage and a fact “1856”. Existing work for NQG uses a sequence-to-sequence model (Sutskever et al., 2014), which takes a passage as input for generating a question. They either entirely ignore the target answer (Du et al., 2017), or directly hard-code answer positions (Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target answer and the passage, so that contextual information can be better considered by the encoder. In particular, matching is used between the target answer and the passage for collecting relevant contextual information. We adopt the multiperspective context matching (MPCM) algorithm (Wang et al., 2017b), which takes two texts as input before producing a vector of numbers, representing similarity under different pe"
N18-2090,W17-2603,0,0.0998808,"asing the training data. It can also be used for educational purposes such as language learning (Heilman and Smith, 2010). One example is shown in Table 1, where a question “when was nikola tesla born ?” is generated given a passage and a fact “1856”. Existing work for NQG uses a sequence-to-sequence model (Sutskever et al., 2014), which takes a passage as input for generating a question. They either entirely ignore the target answer (Du et al., 2017), or directly hard-code answer positions (Zhou et al., 2017; Yang et al., 2017; Subramanian et al., 2017; Tang et al., 2017; Wang et al., 2017a; Yuan et al., 2017). These methods can neglect rich potential ∗ We investigate explicit interaction between the target answer and the passage, so that contextual information can be better considered by the encoder. In particular, matching is used between the target answer and the passage for collecting relevant contextual information. We adopt the multiperspective context matching (MPCM) algorithm (Wang et al., 2017b), which takes two texts as input before producing a vector of numbers, representing similarity under different perspectives. Results on SQuAD (Rajpurkar et al., 2016) show that our model gives bette"
N19-1044,P16-5005,0,0.38354,"anslations and their corresponding source words during decoding, and thus can hurt translation fidelity (Hasler et al., 2018). There is not a mechanism that allows the model to learn constraint translations during training, which the placeholder method allows. We investigate a novel method based on data augmentation, which combines the advantages of both methods above. The idea is to construct synthetic parallel sentences from the original paralIntroduction One important research question in domainspecific machine translation (Luong and Manning, 2015) is how to impose translation constraints (Crego et al., 2016; Hokamp and Liu, 2017; Post and Vilar, 2018). As shown in Figure 1 (a), the word “breadboard” can be translated into “切面 包板 (a wooden board that is used to cut bread on)” in the food domain, but “电 路 板 (a construction base for prototyping of electronics)” in the electronic domain. To enhance translation quality, a lexicon can be leveraged for domainspecific or user-provided words (Arthur et al., 2016; Hasler et al., 2018). We investigate the method of leveraging pre-specified translation for NMT using such a lexicon. For leveraging pre-specified translation, one existing approach uses placeho"
N19-1044,W17-4715,0,0.0270094,"modify NMT models by integrating translation lexicons. In addition, our data augmentation method is more flexible, because it is model-free. Alkhouli et al. (2018) simulate a dictionaryguided translation task to evaluate NMT’s alignment extraction. A one-to-one word translation dictionary is used to guide NMT decoding. In their method, a dictionary entry is limited to only one word on both the source and target sides. In addition, a pre-specified translation can come into effect only if the corresponding source-side word is successfully aligned during decoding. On translating named entities, Currey et al. (2017) augment the training data by copying target-side sentences to the source-side, resulting in augmented training corpora where the source and the target sides contain identical sentences. The augmented data is shown to improve translation performance, especially for proper nouns and other words that are identical in the source and target languages. 3 Given a bilingual training corpus, we sample augmented sentence pairs by leveraging a SMT phrase table, which can be trained over the same bilingual corpus or a different large corpus. We extract source-target phrase pairs2 from the phrase table, r"
N19-1044,J82-2005,0,0.667768,"Missing"
N19-1044,2015.iwslt-evaluation.11,0,0.0200356,"oes not explicitly explore the correlation between pre-specified translations and their corresponding source words during decoding, and thus can hurt translation fidelity (Hasler et al., 2018). There is not a mechanism that allows the model to learn constraint translations during training, which the placeholder method allows. We investigate a novel method based on data augmentation, which combines the advantages of both methods above. The idea is to construct synthetic parallel sentences from the original paralIntroduction One important research question in domainspecific machine translation (Luong and Manning, 2015) is how to impose translation constraints (Crego et al., 2016; Hokamp and Liu, 2017; Post and Vilar, 2018). As shown in Figure 1 (a), the word “breadboard” can be translated into “切面 包板 (a wooden board that is used to cut bread on)” in the food domain, but “电 路 板 (a construction base for prototyping of electronics)” in the electronic domain. To enhance translation quality, a lexicon can be leveraged for domainspecific or user-provided words (Arthur et al., 2016; Hasler et al., 2018). We investigate the method of leveraging pre-specified translation for NMT using such a lexicon. For leveraging"
N19-1044,N13-1073,0,0.19482,"Missing"
N19-1044,P02-1040,0,0.103818,"e systems, the vocabulary size is set to 50K on both sides. For “Data augmentation”, to allow the source-side dictionary to cover target-side words, the target- and source-side vocabularies are merged for a new source vocabulary. For “Shared embeddings”, the source vocabulary remains the same as the baselines, where the source-side target words use embeddings from target-side vocabulary. Experiments We compare our method with strong baselines on large-scale En-Ru and Ch-En tasks on various test sets across different domains, using a strongly optimized Transformer (Vaswani et al., 2017). BLEU (Papineni et al., 2002) is used for evaluation. 5.1 Data Our training corpora are taken from the WMT2018 news translation task. En-Ru. We use 13.88M sentences as baseline training data, containing both a real bilingual corpus and a synthetic back-translation corpus (Sennrich et al., 2015a). The synthetic corpus is translated from “NewsCommonCrawl”, which can be obtained from the WMT task. The news domain contains four different test sets published by WMT2018 over the recent years, namely “news2015”, “news2016”, “news2017”, and “news2018”, respectively, each having one reference. The e-commerce domain contains four f"
N19-1044,P16-1154,0,0.0234543,"is learned because such information is available both in training and decoding. As a data augmentation method, it can be used on any NMT architecture. In addition, our method enables the model to translate code-switched source sentences, and preserve its strength in translating un-replaced sentences. To further strengthen copying, we propose two model-level adjustments: First, we share targetside embeddings with source-side target words, so that target vocabulary words have a unique embedding in the NMT system. Second, we integrate pointer network (Vinyals et al., 2015; Gulcehre et al., 2016; Gu et al., 2016; See et al., 2017) into the decoder. The copy mechanism was firstly proposed to copy source words. In our method, it is further used to copy source-side target words. Results on large scale English-to-Russian (EnRu) and Chinese-to-English (Ch-En) tasks show that our method outperforms both placeholder and lexical constraint methods over a state-of-the-art Transformer (Vaswani et al., 2017) model on various test sets across different domains. We also show that shared embedding and pointer network can lead to more successful applications of the copying mechanism. We release four high-quality En"
N19-1044,N18-1119,0,0.462311,"ides during training, so that a model can translate such words by learning to translate placeholder tags. For example, the i-th named entity in the source sentence is replaced with “tagi ”, as well as its corresponding translation in the target side. Placeholder tags in the output are replaced with pre-specified translation as a post-processing step. One disadvantage of this approach, however, is that the meaning of the original words in the pre-specified translation is not fully retained, which can be harmful to both adequacy and fluency of the output. Another approach (Hokamp and Liu, 2017; Post and Vilar, 2018) imposes pre-specified translation via lexical constraints, making sure such constraints are satisfied by modifying NMT decoding. This method ensures that pre-specified translations appear in the output. A problem of this method is that it does not explicitly explore the correlation between pre-specified translations and their corresponding source words during decoding, and thus can hurt translation fidelity (Hasler et al., 2018). There is not a mechanism that allows the model to learn constraint translations during training, which the placeholder method allows. We investigate a novel method b"
N19-1044,P16-1014,0,0.135791,"especified translation is learned because such information is available both in training and decoding. As a data augmentation method, it can be used on any NMT architecture. In addition, our method enables the model to translate code-switched source sentences, and preserve its strength in translating un-replaced sentences. To further strengthen copying, we propose two model-level adjustments: First, we share targetside embeddings with source-side target words, so that target vocabulary words have a unique embedding in the NMT system. Second, we integrate pointer network (Vinyals et al., 2015; Gulcehre et al., 2016; Gu et al., 2016; See et al., 2017) into the decoder. The copy mechanism was firstly proposed to copy source words. In our method, it is further used to copy source-side target words. Results on large scale English-to-Russian (EnRu) and Chinese-to-English (Ch-En) tasks show that our method outperforms both placeholder and lexical constraint methods over a state-of-the-art Transformer (Vaswani et al., 2017) model on various test sets across different domains. We also show that shared embedding and pointer network can lead to more successful applications of the copying mechanism. We release fou"
N19-1044,N18-2081,0,0.0896831,"Missing"
N19-1044,P17-1099,0,0.0240029,"e such information is available both in training and decoding. As a data augmentation method, it can be used on any NMT architecture. In addition, our method enables the model to translate code-switched source sentences, and preserve its strength in translating un-replaced sentences. To further strengthen copying, we propose two model-level adjustments: First, we share targetside embeddings with source-side target words, so that target vocabulary words have a unique embedding in the NMT system. Second, we integrate pointer network (Vinyals et al., 2015; Gulcehre et al., 2016; Gu et al., 2016; See et al., 2017) into the decoder. The copy mechanism was firstly proposed to copy source words. In our method, it is further used to copy source-side target words. Results on large scale English-to-Russian (EnRu) and Chinese-to-English (Ch-En) tasks show that our method outperforms both placeholder and lexical constraint methods over a state-of-the-art Transformer (Vaswani et al., 2017) model on various test sets across different domains. We also show that shared embedding and pointer network can lead to more successful applications of the copying mechanism. We release four high-quality En-Ru e-commerce test"
N19-1044,P17-1141,0,0.344153,"he source and target sides during training, so that a model can translate such words by learning to translate placeholder tags. For example, the i-th named entity in the source sentence is replaced with “tagi ”, as well as its corresponding translation in the target side. Placeholder tags in the output are replaced with pre-specified translation as a post-processing step. One disadvantage of this approach, however, is that the meaning of the original words in the pre-specified translation is not fully retained, which can be harmful to both adequacy and fluency of the output. Another approach (Hokamp and Liu, 2017; Post and Vilar, 2018) imposes pre-specified translation via lexical constraints, making sure such constraints are satisfied by modifying NMT decoding. This method ensures that pre-specified translations appear in the output. A problem of this method is that it does not explicitly explore the correlation between pre-specified translations and their corresponding source words during decoding, and thus can hurt translation fidelity (Hasler et al., 2018). There is not a mechanism that allows the model to learn constraint translations during training, which the placeholder method allows. We inves"
N19-1044,W16-2316,0,0.0204654,"ted as ct = i=1 αt,i ∗ hi,n , where αt,i is attention weight mentioned earlier. {h1,n , h2,n , ..., hm,n } are the source-side hidden states of the encoder’s last layer. 5 5.2 Experimental Settings We use six self-attention layers for both the encoder and the decoder. The embedding size and the hidden size are set to 512. Eight heads are used for self-attention. A feed-forward layer with 2048 cells and Swish (Ramachandran et al., 2018) is used as the activation function. Adam (Kingma and Ba, 2014) is used for training; warmup step is 16000; the learning rate is 0.0003. We use label smoothing (Junczys-Dowmunt et al., 2016) with a confidence score of 0.9, and all the drop-out (Gal and Ghahramani, 2016) probabilities are set to 0.1. We extract a SMT phrase table on the bilingual training corpus by using moses (Koehn et al., 2007) with default setting, which is used for matching sentence pairs to generate augmented training data. We apply count-based pruning (Zens et al., 2012) to the phrase table, the threshold is set to 10. During decoding, similar to Hasler et al. (2018), Alkhouli et al. (2018) and Post and Vilar (2018), we make use of references to obtain gold constraints. Following previous work, prespecified"
N19-1044,P18-4020,0,0.0242973,"Missing"
N19-1044,W17-4742,0,0.0670706,"Missing"
N19-1044,D12-1089,0,0.0182688,"d-forward layer with 2048 cells and Swish (Ramachandran et al., 2018) is used as the activation function. Adam (Kingma and Ba, 2014) is used for training; warmup step is 16000; the learning rate is 0.0003. We use label smoothing (Junczys-Dowmunt et al., 2016) with a confidence score of 0.9, and all the drop-out (Gal and Ghahramani, 2016) probabilities are set to 0.1. We extract a SMT phrase table on the bilingual training corpus by using moses (Koehn et al., 2007) with default setting, which is used for matching sentence pairs to generate augmented training data. We apply count-based pruning (Zens et al., 2012) to the phrase table, the threshold is set to 10. During decoding, similar to Hasler et al. (2018), Alkhouli et al. (2018) and Post and Vilar (2018), we make use of references to obtain gold constraints. Following previous work, prespecified translations for each source sentence are sampled from references and used by all systems for fair comparison. In all the baseline systems, the vocabulary size is set to 50K on both sides. For “Data augmentation”, to allow the source-side dictionary to cover target-side words, the target- and source-side vocabularies are merged for a new source vocabulary."
N19-1044,J93-2003,0,\N,Missing
N19-1044,P07-2045,0,\N,Missing
N19-1044,D16-1162,0,\N,Missing
N19-1044,P16-1162,0,\N,Missing
N19-1044,D17-1098,0,\N,Missing
N19-1044,W18-6318,0,\N,Missing
N19-1278,P15-1168,0,0.0958439,"1 中 Center 国 Nation 中国 Matched Words/Subwords: 科 Science Chinese 学 Academy 科学 Science 院 Institute 院 Institute 学院 Academy 士 Fellow 院士 Fellow 科学院 Academy of Science Gold word Sequence: 中国 Chinese 科学院 Academy of Science 院士 Fellow Figure 1: Segmentation with ambiguous words. Introduction Chinese word segmentation (CWS) is a traditional NLP task (Sproat et al., 1996), the features for which have been a central research topic. Statistical methods consider characters (Xue et al., 2003), subwords (Zhang et al., 2006), and words (Zhang and Clark, 2007) as input features. Among these, both characters (Chen et al., 2015a) and words (Zhang et al., 2016; Cai and Zhao, 2016; Yang et al., 2017) have also shown useful in recent neural models. However, how to utilize the subword features in neural networks has not been investigated yet. In this paper, we fill this gap by proposing a subword-based neural word segmentor, by integrating two strands of works: the byte pair encoding (BPE) algorithm (Gage, 1994) and the lattice LSTM structure (Zhang and Yang, 2018). The BPE algorithm constructs a subword list from raw data and lattice LSTM introduces subwords into character LSTM representation. In particular, our baseli"
N19-1278,D15-1141,0,0.248494,"Missing"
N19-1278,I05-3017,0,0.688525,"utput h i includes both the character sequence history information and all the matched subsequence information. 3.3 Decoding and Training We use a standard CRF layer for inference (details in Appendix). Viterbi (1967) is used to find the highest scored label sequence over the input. During training, we choose sentence-level loglikelihood as the loss function. Loss = N X log(P (yi |si )), (5) i=1 4 4.1 Experiments Experimental Settings Data. We evaluate our model on four standard Chinese word segmentation datasets: CTB6, PKU, MSR, and Weibo. PKU and MSR are taken from the SIGHAN 2005 bake-off (Emerson, 2005) and Weibo dataset is the NLPCC 2016 shared task (Qiu et al., 2016), standard split are used. We take CTB6 as the main dataset and split the train/dev/test following Zhang et al. (2016). The statistics of the datasets are listed in Appendix. 5 10 Iteration 15 20 Figure 3: F1-value against training iterations. Hyperparameters. We keep the hyperparameters the same among all datasets. Standard gradient descent (SGD) with a learning rate decay is used as the optimizer. The embedding sizes of character unigram/bigram and subword are all 50. Dropout (Srivastava et al., 2014) is used on both the char"
N19-1278,L18-1473,0,0.0320452,"dient descent (SGD) with a learning rate decay is used as the optimizer. The embedding sizes of character unigram/bigram and subword are all 50. Dropout (Srivastava et al., 2014) is used on both the character input and the subword input to prevent overfitting. Details are listed in Table 1. Embeddings. We take the same character unigram and bigram embeddings as Zhang et al. (2016), who pretrain embeddings using word2vec (Mikolov et al., 2013) on Chinese Gigaword3 . The vocabulary of subword is constructed with 200000 merge operations and the subword embeddings are also trained using word2vec (Heinzerling and Strube, 2018). Trie (Fredkin, 1960) is used to accelerate lattice building. All the embeddings are fine-tuned during training. 4.2 where yi is the gold labels of sentence si . Baseline_Unigram Baseline_Bigram LaLSTM+Subword_Unigram LaLSTM+Subword_Bigram Development Experiments We perform experiments on the CTB6 development dataset to investigate the contribution of character bigram information and the subword information. Figure 3 shows the iteration curve of F-scores against different numbers of training iterations with different character representations. “ Bigram” represents the model using both charact"
N19-1278,D18-1529,0,0.235597,"4: F1-value against the sentence length. Table 2: Main results (F1). Model Baseline Random Emb Pretrain Emb LaLSTM+Subword Baseline Results Table 2 shows the main results and the recent stateof-the-art neural CWS models. Zhang et al. (2016) integrated both discrete features and neural features in a transition-based framework. Xu and Sun (2016) proposed the dependency-based gated recursive neural network to utilize long distance dependencies. Yang et al. (2017)† utilized pretrained character representations from multitasks. We examine their non-pretrained model performance for fair comparison. Ma et al. (2018) built a bidirectional LSTM model with carefully hyperparameter selection. These methods are orthogonal to and can be integrated into our lattice structure. As shown in Table 2, the subword lattice LSTM gives significant improvements on all evaluated datasets. In the PKU dataset, our model is slightly behind Xu and Sun (2016) which preprocesses the dataset by replacing all the Chinese idioms, lead4.4 Analysis Lexicon and Embeddings. To distinguish the contribution of subword lexicon and their pretrained embeddings, we conduct a set of experiments by using the same subword lexicon with randomly"
N19-1278,P15-1167,0,0.0315626,"Missing"
N19-1278,P16-2092,0,0.0481474,"improvements in the lattice LSTM. This is likely because character bigrams are informative but ambiguous. They can provide more useful character disambiguation evidence in segmentation than in NER where lattice LSTM works well in disambiguating characters. 4.3 30 Figure 4: F1-value against the sentence length. Table 2: Main results (F1). Model Baseline Random Emb Pretrain Emb LaLSTM+Subword Baseline Results Table 2 shows the main results and the recent stateof-the-art neural CWS models. Zhang et al. (2016) integrated both discrete features and neural features in a transition-based framework. Xu and Sun (2016) proposed the dependency-based gated recursive neural network to utilize long distance dependencies. Yang et al. (2017)† utilized pretrained character representations from multitasks. We examine their non-pretrained model performance for fair comparison. Ma et al. (2018) built a bidirectional LSTM model with carefully hyperparameter selection. These methods are orthogonal to and can be integrated into our lattice structure. As shown in Table 2, the subword lattice LSTM gives significant improvements on all evaluated datasets. In the PKU dataset, our model is slightly behind Xu and Sun (2016) w"
N19-1278,O03-4002,0,0.90277,"Missing"
N19-1278,P18-4013,1,0.814023,"tworks has not been investigated yet. In this paper, we fill this gap by proposing a subword-based neural word segmentor, by integrating two strands of works: the byte pair encoding (BPE) algorithm (Gage, 1994) and the lattice LSTM structure (Zhang and Yang, 2018). The BPE algorithm constructs a subword list from raw data and lattice LSTM introduces subwords into character LSTM representation. In particular, our baseline is a BiLSTM-CRF segmentor (Chen et al., 2015b) and we replace LSTM with lattice LSTM using subwords to encode character composition information. Our code1 is based on NCRF++ (Yang and Zhang, 2018). 1 Our code is released at https://github.com/ jiesutd/SubwordEncoding-CWS. Compared with character-based neural segmentors, our model can utilize abundant character combination (subword) information, which is effective to disambiguate characters. For example, in Figure 1, the subword “学 院(Academy)” ensures that the character “学” means “Academy(noun)” rather than “study(verb)”. Compared with the word-based neural models (Zhang et al., 2016; Cai and Zhao, 2016), ambiguous subwords in a context can provide additional information for disambiguation. For instance, the subword “科学院(Academy of Scie"
N19-1278,P17-1078,1,0.879633,"demy 科学 Science 院 Institute 院 Institute 学院 Academy 士 Fellow 院士 Fellow 科学院 Academy of Science Gold word Sequence: 中国 Chinese 科学院 Academy of Science 院士 Fellow Figure 1: Segmentation with ambiguous words. Introduction Chinese word segmentation (CWS) is a traditional NLP task (Sproat et al., 1996), the features for which have been a central research topic. Statistical methods consider characters (Xue et al., 2003), subwords (Zhang et al., 2006), and words (Zhang and Clark, 2007) as input features. Among these, both characters (Chen et al., 2015a) and words (Zhang et al., 2016; Cai and Zhao, 2016; Yang et al., 2017) have also shown useful in recent neural models. However, how to utilize the subword features in neural networks has not been investigated yet. In this paper, we fill this gap by proposing a subword-based neural word segmentor, by integrating two strands of works: the byte pair encoding (BPE) algorithm (Gage, 1994) and the lattice LSTM structure (Zhang and Yang, 2018). The BPE algorithm constructs a subword list from raw data and lattice LSTM introduces subwords into character LSTM representation. In particular, our baseline is a BiLSTM-CRF segmentor (Chen et al., 2015b) and we replace LSTM wi"
N19-1278,P14-1028,0,0.181006,"Missing"
N19-1278,P16-1040,1,0.930583,"Words/Subwords: 科 Science Chinese 学 Academy 科学 Science 院 Institute 院 Institute 学院 Academy 士 Fellow 院士 Fellow 科学院 Academy of Science Gold word Sequence: 中国 Chinese 科学院 Academy of Science 院士 Fellow Figure 1: Segmentation with ambiguous words. Introduction Chinese word segmentation (CWS) is a traditional NLP task (Sproat et al., 1996), the features for which have been a central research topic. Statistical methods consider characters (Xue et al., 2003), subwords (Zhang et al., 2006), and words (Zhang and Clark, 2007) as input features. Among these, both characters (Chen et al., 2015a) and words (Zhang et al., 2016; Cai and Zhao, 2016; Yang et al., 2017) have also shown useful in recent neural models. However, how to utilize the subword features in neural networks has not been investigated yet. In this paper, we fill this gap by proposing a subword-based neural word segmentor, by integrating two strands of works: the byte pair encoding (BPE) algorithm (Gage, 1994) and the lattice LSTM structure (Zhang and Yang, 2018). The BPE algorithm constructs a subword list from raw data and lattice LSTM introduces subwords into character LSTM representation. In particular, our baseline is a BiLSTM-CRF segmentor (Ch"
N19-1278,C04-1081,0,0.605946,"院(Academy of Sciences)” and “学 院(Academy)” can be useful in determining the correct segmentation, which is “科学院/(Academy of Sciences/)”. To our knowledge, we are the first to use subwords in a neural network segmentor. We investigate the contributions of subword lexicons and their pretrained embeddings through controlled experiments. Results on four benchmarks show that the proposed model can give comparable results with state-of-the-art models. 2 Related Work State-of-the-art statistical segmentors use either sequence labeling methods e.g. CRF (Lafferty et al., 2001) with character features (Peng et al., 2004; Zhao et al., 2006) or the transition-based models with word features (Zhang and Clark, 2007; Sun, 2010). Neural segmentors (Chen et al., 2720 Proceedings of NAACL-HLT 2019, pages 2720–2725 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics CRF Layer B E B M E B E LSTM Layer LSTM LSTM LSTM LSTM LSTM LSTM LSTM Cell Cell 中 Unichar emb Bichar emb 国 中国 Word emb 国科 中国 Cell 科 学 科学 Cell Cell 院 学院 科学 院院 学院 院 士 院士 士</E&gt; 院士 科学院 Figure 2: Models. Only forward LSTM is illustrated here. 2015a; Cai and Zhao, 2016) generally take the same framework except using n"
N19-1278,N06-2049,0,0.0571227,"our knowledge, this is the first research on the effectiveness of subwords on neural word segmentation. 1 中 Center 国 Nation 中国 Matched Words/Subwords: 科 Science Chinese 学 Academy 科学 Science 院 Institute 院 Institute 学院 Academy 士 Fellow 院士 Fellow 科学院 Academy of Science Gold word Sequence: 中国 Chinese 科学院 Academy of Science 院士 Fellow Figure 1: Segmentation with ambiguous words. Introduction Chinese word segmentation (CWS) is a traditional NLP task (Sproat et al., 1996), the features for which have been a central research topic. Statistical methods consider characters (Xue et al., 2003), subwords (Zhang et al., 2006), and words (Zhang and Clark, 2007) as input features. Among these, both characters (Chen et al., 2015a) and words (Zhang et al., 2016; Cai and Zhao, 2016; Yang et al., 2017) have also shown useful in recent neural models. However, how to utilize the subword features in neural networks has not been investigated yet. In this paper, we fill this gap by proposing a subword-based neural word segmentor, by integrating two strands of works: the byte pair encoding (BPE) algorithm (Gage, 1994) and the lattice LSTM structure (Zhang and Yang, 2018). The BPE algorithm constructs a subword list from raw d"
N19-1278,P07-1106,1,0.814526,"t research on the effectiveness of subwords on neural word segmentation. 1 中 Center 国 Nation 中国 Matched Words/Subwords: 科 Science Chinese 学 Academy 科学 Science 院 Institute 院 Institute 学院 Academy 士 Fellow 院士 Fellow 科学院 Academy of Science Gold word Sequence: 中国 Chinese 科学院 Academy of Science 院士 Fellow Figure 1: Segmentation with ambiguous words. Introduction Chinese word segmentation (CWS) is a traditional NLP task (Sproat et al., 1996), the features for which have been a central research topic. Statistical methods consider characters (Xue et al., 2003), subwords (Zhang et al., 2006), and words (Zhang and Clark, 2007) as input features. Among these, both characters (Chen et al., 2015a) and words (Zhang et al., 2016; Cai and Zhao, 2016; Yang et al., 2017) have also shown useful in recent neural models. However, how to utilize the subword features in neural networks has not been investigated yet. In this paper, we fill this gap by proposing a subword-based neural word segmentor, by integrating two strands of works: the byte pair encoding (BPE) algorithm (Gage, 1994) and the lattice LSTM structure (Zhang and Yang, 2018). The BPE algorithm constructs a subword list from raw data and lattice LSTM introduces sub"
N19-1278,P16-1162,0,0.0735797,"aracter sequence features and all lexicon word embeddings that match a character subsequence in the input into a sequence labeling model. Zhu et al. (2016) proposed a DAG-structured LSTM structure which is similar to the lattice LSTM model but binarizing the paths in the merging process. Chen et al. (2017) also built a DAG-LSTM structure for word segmentation but without memory cells. Our model consistently gives better performance. BPE is a data compression algorithm (Gage, 1994) which has been used in neural machine translation (NMT) by capturing the most frequent subwords instead of words (Sennrich et al., 2016). Here we use it for collecting subwords in Chinese, similar to the use in Chinese NMT. 3 xi = eci ⊕ eci ci+1 , −−−−→ → − → − → − h 1 , h 2 , . . . , h m = LST M (x1 , x2 , . . . , xm ) ←−−−− ← − ← − ← − h 1 , h 2 , . . . , h m = LST M (x1 , x2 , . . . , xm ), (2) −−−−→ ←−−−− where LST M and LST M represent the forward and backward LSTM, respectively. The detailed equations are listed in Appendix. The hidden vector of character ci is → − ← − hi = h i ⊕ h i Baseline Model As shown in Figure 2, for each input character ci , the corresponding character unigram embeddings (1) where ⊕ represents co"
N19-1278,P18-1144,1,0.914404,"methods consider characters (Xue et al., 2003), subwords (Zhang et al., 2006), and words (Zhang and Clark, 2007) as input features. Among these, both characters (Chen et al., 2015a) and words (Zhang et al., 2016; Cai and Zhao, 2016; Yang et al., 2017) have also shown useful in recent neural models. However, how to utilize the subword features in neural networks has not been investigated yet. In this paper, we fill this gap by proposing a subword-based neural word segmentor, by integrating two strands of works: the byte pair encoding (BPE) algorithm (Gage, 1994) and the lattice LSTM structure (Zhang and Yang, 2018). The BPE algorithm constructs a subword list from raw data and lattice LSTM introduces subwords into character LSTM representation. In particular, our baseline is a BiLSTM-CRF segmentor (Chen et al., 2015b) and we replace LSTM with lattice LSTM using subwords to encode character composition information. Our code1 is based on NCRF++ (Yang and Zhang, 2018). 1 Our code is released at https://github.com/ jiesutd/SubwordEncoding-CWS. Compared with character-based neural segmentors, our model can utilize abundant character combination (subword) information, which is effective to disambiguate charac"
N19-1278,J96-3004,0,0.406728,"sequence. Experiments on standard benchmark show that subword information brings significant gains over strong character-based segmentation models. To our knowledge, this is the first research on the effectiveness of subwords on neural word segmentation. 1 中 Center 国 Nation 中国 Matched Words/Subwords: 科 Science Chinese 学 Academy 科学 Science 院 Institute 院 Institute 学院 Academy 士 Fellow 院士 Fellow 科学院 Academy of Science Gold word Sequence: 中国 Chinese 科学院 Academy of Science 院士 Fellow Figure 1: Segmentation with ambiguous words. Introduction Chinese word segmentation (CWS) is a traditional NLP task (Sproat et al., 1996), the features for which have been a central research topic. Statistical methods consider characters (Xue et al., 2003), subwords (Zhang et al., 2006), and words (Zhang and Clark, 2007) as input features. Among these, both characters (Chen et al., 2015a) and words (Zhang et al., 2016; Cai and Zhao, 2016; Yang et al., 2017) have also shown useful in recent neural models. However, how to utilize the subword features in neural networks has not been investigated yet. In this paper, we fill this gap by proposing a subword-based neural word segmentor, by integrating two strands of works: the byte pa"
N19-1278,Y06-1012,0,0.0340083,"es)” and “学 院(Academy)” can be useful in determining the correct segmentation, which is “科学院/(Academy of Sciences/)”. To our knowledge, we are the first to use subwords in a neural network segmentor. We investigate the contributions of subword lexicons and their pretrained embeddings through controlled experiments. Results on four benchmarks show that the proposed model can give comparable results with state-of-the-art models. 2 Related Work State-of-the-art statistical segmentors use either sequence labeling methods e.g. CRF (Lafferty et al., 2001) with character features (Peng et al., 2004; Zhao et al., 2006) or the transition-based models with word features (Zhang and Clark, 2007; Sun, 2010). Neural segmentors (Chen et al., 2720 Proceedings of NAACL-HLT 2019, pages 2720–2725 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics CRF Layer B E B M E B E LSTM Layer LSTM LSTM LSTM LSTM LSTM LSTM LSTM Cell Cell 中 Unichar emb Bichar emb 国 中国 Word emb 国科 中国 Cell 科 学 科学 Cell Cell 院 学院 科学 院院 学院 院 士 院士 士</E&gt; 院士 科学院 Figure 2: Models. Only forward LSTM is illustrated here. 2015a; Cai and Zhao, 2016) generally take the same framework except using neural networks as au"
N19-1278,D13-1061,0,0.611459,"Missing"
N19-1278,C10-2139,0,0.0253893,"(Academy of Sciences/)”. To our knowledge, we are the first to use subwords in a neural network segmentor. We investigate the contributions of subword lexicons and their pretrained embeddings through controlled experiments. Results on four benchmarks show that the proposed model can give comparable results with state-of-the-art models. 2 Related Work State-of-the-art statistical segmentors use either sequence labeling methods e.g. CRF (Lafferty et al., 2001) with character features (Peng et al., 2004; Zhao et al., 2006) or the transition-based models with word features (Zhang and Clark, 2007; Sun, 2010). Neural segmentors (Chen et al., 2720 Proceedings of NAACL-HLT 2019, pages 2720–2725 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics CRF Layer B E B M E B E LSTM Layer LSTM LSTM LSTM LSTM LSTM LSTM LSTM Cell Cell 中 Unichar emb Bichar emb 国 中国 Word emb 国科 中国 Cell 科 学 科学 Cell Cell 院 学院 科学 院院 学院 院 士 院士 士</E&gt; 院士 科学院 Figure 2: Models. Only forward LSTM is illustrated here. 2015a; Cai and Zhao, 2016) generally take the same framework except using neural networks as automatic feature extractor. Lattice LSTM was proposed by Zhang and Yang (2018) for Chi"
N19-1278,D17-1079,1,0.783094,"ci+1 , −−−−→ → − → − → − h 1 , h 2 , . . . , h m = LST M (x1 , x2 , . . . , xm ) ←−−−− ← − ← − ← − h 1 , h 2 , . . . , h m = LST M (x1 , x2 , . . . , xm ), (2) −−−−→ ←−−−− where LST M and LST M represent the forward and backward LSTM, respectively. The detailed equations are listed in Appendix. The hidden vector of character ci is → − ← − hi = h i ⊕ h i Baseline Model As shown in Figure 2, for each input character ci , the corresponding character unigram embeddings (1) where ⊕ represents concatenate operation. Unlike Zhang et al. (2016) which uses a window to strengthen the local features, or Zhou et al. (2017) which adds a non-linear layer before the LSTM layer, we feed {x1 , x2 , . . . , xm } into a bidirectional LSTM: Models We take the state-of-the-art LSTM-CRF framework as our baseline. For an input sentence with m characters s = c1 , c2 , . . . , cm , where ci denotes the ith character, the segmentor is to assign each character ci with a label li . Figure 2 shows the segmentor framework on input character sequence “中国科学院院士 (Fellow of the Chinese Academy of Sciences)”, where the black part represents the baseline LSTM-CRF model and the red part shows the lattice structure. 3.1 and character big"
N19-1278,N16-1106,0,0.0312425,"B M E B E LSTM Layer LSTM LSTM LSTM LSTM LSTM LSTM LSTM Cell Cell 中 Unichar emb Bichar emb 国 中国 Word emb 国科 中国 Cell 科 学 科学 Cell Cell 院 学院 科学 院院 学院 院 士 院士 士</E&gt; 院士 科学院 Figure 2: Models. Only forward LSTM is illustrated here. 2015a; Cai and Zhao, 2016) generally take the same framework except using neural networks as automatic feature extractor. Lattice LSTM was proposed by Zhang and Yang (2018) for Chinese named entity recognition (NER). It integrates the character sequence features and all lexicon word embeddings that match a character subsequence in the input into a sequence labeling model. Zhu et al. (2016) proposed a DAG-structured LSTM structure which is similar to the lattice LSTM model but binarizing the paths in the merging process. Chen et al. (2017) also built a DAG-LSTM structure for word segmentation but without memory cells. Our model consistently gives better performance. BPE is a data compression algorithm (Gage, 1994) which has been used in neural machine translation (NMT) by capturing the most frequent subwords instead of words (Sennrich et al., 2016). Here we use it for collecting subwords in Chinese, similar to the use in Chinese NMT. 3 xi = eci ⊕ eci ci+1 , −−−−→ → − → − → − h 1"
N19-1279,D14-1093,1,0.911974,"Missing"
N19-1279,P16-1039,0,0.105957,"egmentation of full novels for three Chinese novel datasets, their statistics are Sentence (K) Full 40 148 59 32 17 Eval 1 1 1 1 1 Token (K) Full 1,982 5,004 2,131 709 556 Eval 32 17 21 17 34 Character (K) Full 2,867 7,126 3,006 1,150 903 Eval 47 25 31 30 57 Table 1: Statistics of full and evaluation datasets. based on the segmentation given by the baseline segmenter. 4.1.2 Pre-Processing In some studies, pre-processing is applied in order to improve the performance of CWS models, including substituting consecutive digits and English letters, Chinese idioms and long words with unique symbols (Cai and Zhao, 2016; Cai et al., 2017; Chen et al., 2015). However, we do not deploy such techniques for fair comparison, focusing only on the possible improvements brought by word embeddings. The only pre-processing adopted in our model is to first split a sentence with a set of pre-defined delimiters: characters that are not Chinese characters, English letters or digits. Those fragments of a sentence are then fed into the segmenter, and a complete segmented sentence is returned by reassembling the segmented fragments and delimiters in the original order. 4.1.3 Experiments 4.1 Dataset Hyperparameters Hyperparam"
N19-1279,P17-2096,0,0.561107,"novels for three Chinese novel datasets, their statistics are Sentence (K) Full 40 148 59 32 17 Eval 1 1 1 1 1 Token (K) Full 1,982 5,004 2,131 709 556 Eval 32 17 21 17 34 Character (K) Full 2,867 7,126 3,006 1,150 903 Eval 47 25 31 30 57 Table 1: Statistics of full and evaluation datasets. based on the segmentation given by the baseline segmenter. 4.1.2 Pre-Processing In some studies, pre-processing is applied in order to improve the performance of CWS models, including substituting consecutive digits and English letters, Chinese idioms and long words with unique symbols (Cai and Zhao, 2016; Cai et al., 2017; Chen et al., 2015). However, we do not deploy such techniques for fair comparison, focusing only on the possible improvements brought by word embeddings. The only pre-processing adopted in our model is to first split a sentence with a set of pre-defined delimiters: characters that are not Chinese characters, English letters or digits. Those fragments of a sentence are then fed into the segmenter, and a complete segmented sentence is returned by reassembling the segmented fragments and delimiters in the original order. 4.1.3 Experiments 4.1 Dataset Hyperparameters Hyperparameters used in our"
N19-1279,D15-1141,0,0.262548,"hinese novel datasets, their statistics are Sentence (K) Full 40 148 59 32 17 Eval 1 1 1 1 1 Token (K) Full 1,982 5,004 2,131 709 556 Eval 32 17 21 17 34 Character (K) Full 2,867 7,126 3,006 1,150 903 Eval 47 25 31 30 57 Table 1: Statistics of full and evaluation datasets. based on the segmentation given by the baseline segmenter. 4.1.2 Pre-Processing In some studies, pre-processing is applied in order to improve the performance of CWS models, including substituting consecutive digits and English letters, Chinese idioms and long words with unique symbols (Cai and Zhao, 2016; Cai et al., 2017; Chen et al., 2015). However, we do not deploy such techniques for fair comparison, focusing only on the possible improvements brought by word embeddings. The only pre-processing adopted in our model is to first split a sentence with a set of pre-defined delimiters: characters that are not Chinese characters, English letters or digits. Those fragments of a sentence are then fed into the segmenter, and a complete segmented sentence is returned by reassembling the segmented fragments and delimiters in the original order. 4.1.3 Experiments 4.1 Dataset Hyperparameters Hyperparameters used in our WEB-CWS model are ex"
N19-1279,P17-1110,0,0.221235,"arch is used for decoding. Cai and Zhao (2016) use Gated Combination Neural Networks and LSTM to present both character sequences and partially segmented word sequences, combining word scores and link scores for segmentation. Our work is in line with their work in directly using word information for CWS. In contrast, our method is conceptually simpler by directly using word embeddings. In addition, our work aims at domain-adaptation, rather than training from scratch. 2.2 Cross-Domain CWS Supervised, semi-supervised and unsupervised approaches have been proposed for domain adaptation for CWS. Chen et al. (2017) use an Adversarial Network to learn shared knowledge for different segmentation criteria and domains. This approach requires annotated data in the target domain. However, one challenge for cross-domain CWS is the lack of such annotated data. Liu and Zhang (2012) propose an unsupervised model, in which they use features derived from character clustering, together with a self-training algorithm to jointly model CWS and POS-tagging. This approach is highly time-consuming (Qiu and Zhang, 2015). Another challenge is the segmentation of domain-specific noun entities. In a task of segmenting Chinese"
N19-1279,W02-1001,0,0.085792,"Missing"
N19-1279,I05-3017,0,0.345313,"cessing techniques. Value 10−5  threshold for overall subsampling µ threshold for multi-character word subsampling 0.5 Pn noise distribution for general negative sampling uniform n number of general negative samples per word d dimension of word embeddings 100 η smoothing factor for class weights 0.2 f window size m initial maximum word length 5 k initial beam-size 10 e number of epochs in Skip-gram training 1 1 4 Table 2: Hyperparameters used in WEB-CWS. 4.1.4 5.1 Evaluation For consistency, all segmentation results are automatically calculated with the script provided in the SIGHAN Bakeoff (Emerson, 2005) and are reported as word F-measures. 4.2 Baseline Segmenter Two state-of-the-art CWS models trained on a People’s Daily corpus in 2000 January are tested. One is a joint word segmentation and POS-tagging model (Zhang and Clark, 2010), and the other is a word-based neural CWS model (Cai et al., 2017). When training both models, default settings are used, except that the maximum word length in Cai et al.’s model is set to 5, which is in line with the setting of WEB-CWS. On the evaluation set of PKU (Emerson, 2005), both models yield comparable results, but on the evaluation set of DL, Zhang and"
N19-1279,P15-1167,0,0.0193671,"c noun entities. In a task of segmenting Chinese novels, Qiu and Zhang (2015) design a double-propagation algorithm with complex feature templates to iteratively extract noun entities and their context, to improve segmentation 2727 1 https://github.com/vatile/CWS-NAACL2019 performance. This approach still relies heavily on feature templates. Similarly, our model does not require any annotated target data. In contrast to their work, our model is efficient and feature-free. Raw corpus: T Baseline segmenter Segmented corpus: T ’ 2.3 CWS Using Embeddings There are CWS models deploying embeddings. Ma and Hinrichs (2015) and Deng and Sun (2018) propose embedding matching CWS models, in which embeddings of characters in a sequence are compared with high dimensional representations of CWS-specific actions (e.g., separation and combination) or CWS-specific labels (e.g., B/M/E/S). Then each character is labeled according to the similarity between its embedding and the high dimensional representation. Particularly, Zhou et al. (2017) propose to use character embeddings trained on a word-based context to improve the performance of existing neural CWS models, which is similar to our approach in terms of making use o"
N19-1279,C04-1081,0,0.823965,"Missing"
N19-1279,W10-4127,0,0.64875,"Missing"
N19-1279,I05-3027,0,0.241096,"Missing"
N19-1279,C12-2073,1,0.891873,"in directly using word information for CWS. In contrast, our method is conceptually simpler by directly using word embeddings. In addition, our work aims at domain-adaptation, rather than training from scratch. 2.2 Cross-Domain CWS Supervised, semi-supervised and unsupervised approaches have been proposed for domain adaptation for CWS. Chen et al. (2017) use an Adversarial Network to learn shared knowledge for different segmentation criteria and domains. This approach requires annotated data in the target domain. However, one challenge for cross-domain CWS is the lack of such annotated data. Liu and Zhang (2012) propose an unsupervised model, in which they use features derived from character clustering, together with a self-training algorithm to jointly model CWS and POS-tagging. This approach is highly time-consuming (Qiu and Zhang, 2015). Another challenge is the segmentation of domain-specific noun entities. In a task of segmenting Chinese novels, Qiu and Zhang (2015) design a double-propagation algorithm with complex feature templates to iteratively extract noun entities and their context, to improve segmentation 2727 1 https://github.com/vatile/CWS-NAACL2019 performance. This approach still reli"
N19-1279,W14-6816,0,0.0340928,"Missing"
N19-1279,P07-1106,1,0.797196,"Missing"
N19-1279,D10-1082,1,0.736261,"d dimension of word embeddings 100 η smoothing factor for class weights 0.2 f window size m initial maximum word length 5 k initial beam-size 10 e number of epochs in Skip-gram training 1 1 4 Table 2: Hyperparameters used in WEB-CWS. 4.1.4 5.1 Evaluation For consistency, all segmentation results are automatically calculated with the script provided in the SIGHAN Bakeoff (Emerson, 2005) and are reported as word F-measures. 4.2 Baseline Segmenter Two state-of-the-art CWS models trained on a People’s Daily corpus in 2000 January are tested. One is a joint word segmentation and POS-tagging model (Zhang and Clark, 2010), and the other is a word-based neural CWS model (Cai et al., 2017). When training both models, default settings are used, except that the maximum word length in Cai et al.’s model is set to 5, which is in line with the setting of WEB-CWS. On the evaluation set of PKU (Emerson, 2005), both models yield comparable results, but on the evaluation set of DL, Zhang and Clark’s model (Fmeasure = 0.905) performs better than Cai et al.’s model (F-measure = 0.849). It is very possible that Zhang and Clark’s model can handle crossdomain CWS more effectively. As a result, we choose Zhang and Clark’s mode"
N19-1279,D17-1079,1,0.791371,"to their work, our model is efficient and feature-free. Raw corpus: T Baseline segmenter Segmented corpus: T ’ 2.3 CWS Using Embeddings There are CWS models deploying embeddings. Ma and Hinrichs (2015) and Deng and Sun (2018) propose embedding matching CWS models, in which embeddings of characters in a sequence are compared with high dimensional representations of CWS-specific actions (e.g., separation and combination) or CWS-specific labels (e.g., B/M/E/S). Then each character is labeled according to the similarity between its embedding and the high dimensional representation. Particularly, Zhou et al. (2017) propose to use character embeddings trained on a word-based context to improve the performance of existing neural CWS models, which is similar to our approach in terms of making use of CWS-oriented word embeddings derived with automatically segmented raw corpus. However, in their work, when doing cross-domain CWS, word embeddings are fed into the baseline neural model trained on a large annotated general corpus, with annotated special domain data as the development set. In our model, on the contrary, word embeddings are used directly for CWS with a non-parametric decoder, which does not requi"
P07-1106,P04-1015,0,0.119517,"ocal window. However, the correct decision can be made by comparison of the two three-word windows containing this character. In order to explore the potential of word-based models, we adapt the perceptron discriminative learning algorithm to the CWS problem. Collins (2002) proposed the perceptron as an alternative to the CRF method for HMM-style taggers. However, our model does not map the segmentation problem to a tag sequence learning problem, but defines features on segmented sentences directly. Hence we use a beam-search decoder during training and testing; our idea is similar to that of Collins and Roark (2004) who used a beam-search decoder as part of a perceptron parsing model. Our work can also be seen as part of the recent move towards search-based learning methods which do not rely on dynamic programming and are thus able to exploit larger parts of the context for making decisions (Daume III, 2006). We study several factors that influence the performance of the perceptron word segmentor, including the averaged perceptron method, the size of the 841 -ý ý  -ý  ¡  v beam and the importance of word-based features. We compare the accuracy of our final system to the state-of-the-art CWS sys"
P07-1106,W02-1001,0,0.878774,"especially useful is information about surrounding words. Consider the sentence “ ”, which can be from “ (among which) (foreign) (companies)”, or “ (in China) (foreign companies) (business)”. Note that the five-character window surrounding “ ” is the same in both cases, making the tagging decision for that character difficult given the local window. However, the correct decision can be made by comparison of the two three-word windows containing this character. In order to explore the potential of word-based models, we adapt the perceptron discriminative learning algorithm to the CWS problem. Collins (2002) proposed the perceptron as an alternative to the CRF method for HMM-style taggers. However, our model does not map the segmentation problem to a tag sequence learning problem, but defines features on segmented sentences directly. Hence we use a beam-search decoder during training and testing; our idea is similar to that of Collins and Roark (2004) who used a beam-search decoder as part of a perceptron parsing model. Our work can also be seen as part of the recent move towards search-based learning methods which do not rely on dynamic programming and are thus able to exploit larger parts of th"
P07-1106,I05-3017,0,0.737479,"Missing"
P07-1106,I05-3023,0,0.0194707,"vector according to these templates. There are 356, 337 features with non-zero values after 6 training iterations using the development data. For this particular feature set, the longest range features are word bigrams. Therefore, among partial candidates ending with the same bigram, the best one will also be in the best final candidate. The decoder can be optimized accordingly: when an incoming character is combined with candidate items as a new word, only the best candidate is kept among those having the same last word. 5 Comparison with Previous Work Among the character-tagging CWS models, Li et al. (2005) uses an uneven margin alteration of the traditional perceptron classifier (Li et al., 2002). Each character is classified independently, using information in the neighboring five-character window. Liang (2005) uses the discriminative perceptron algorithm (Collins, 2002) to score whole character tag sequences, finding the best candidate by the global score. It can be seen as an alternative to the ME and CRF models (Xue, 2003; Peng et al., 2004), which 1 2 3 4 5 6 7 8 9 10 11 12 13 14 word w word bigram w1 w2 single-character word w a word starting with character c and having length l a word en"
P07-1106,C04-1081,0,0.829011,"Missing"
P07-1106,W03-1719,0,0.0169565,"Missing"
P07-1106,J96-3004,0,0.0625506,"Missing"
P07-1106,W06-0121,0,0.0468868,"bigram w1 w2 single-character word w a word starting with character c and having length l a word ending with character c and having length l space-separated characters c1 and c2 character bigram c1 c2 in any word the first and last characters c1 and c2 of any word word w immediately before character c character c immediately before word w the starting characters c1 and c2 of two consecutive words the ending characters c1 and c2 of two consecutive words a word of length l and the previous word w a word of length l and the next word w Table 1: feature templates do not involve word information. Wang et al. (2006) incorporates an N-gram language model in ME tagging, making use of word information to improve the character tagging model. The key difference between our model and the above models is the wordbased nature of our system. One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model. Unlike the character-tagging models, the CRF submodel assigns tags to subwords, which include single-character words and the most frequent multiple-character words from the training corpus. Thus it can be seen as a step towards a word-based model. However, su"
P07-1106,O03-4002,0,0.835363,"Missing"
P07-1106,N06-2049,0,0.0974195,"er c character c immediately before word w the starting characters c1 and c2 of two consecutive words the ending characters c1 and c2 of two consecutive words a word of length l and the previous word w a word of length l and the next word w Table 1: feature templates do not involve word information. Wang et al. (2006) incorporates an N-gram language model in ME tagging, making use of word information to improve the character tagging model. The key difference between our model and the above models is the wordbased nature of our system. One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model. Unlike the character-tagging models, the CRF submodel assigns tags to subwords, which include single-character words and the most frequent multiple-character words from the training corpus. Thus it can be seen as a step towards a word-based model. However, sub-words do not necessarily contain full word information. Moreover, sub-word extraction is performed separately from feature extraction. Another difference from our model is the rule-based submodel, which uses a dictionary-based forward maximum match method described by Sproat et al. (1996). 6 Exper"
P07-1106,W03-1726,0,\N,Missing
P08-1101,W02-1001,0,0.200367,"nd c2 character bigram c1 c2 in any word the first / last characters c1 / c2 of any word word w immediately before character c character c immediately before word w the starting characters c1 and c2 of two consecutive words the ending characters c1 and c2 of two consecutive words a word of length l with previous word w a word of length l with next word w Table 1: Feature templates for the baseline segmentor 2 The Baseline System We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POS tagger to refer to the Collins tagger which performs POS tagging only (given segmentation). The features used by the baseline segmentor are shown in Table 1. The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to Chinese, are shown in Table 2. The word segmentation featu"
P08-1101,W06-1673,0,0.0345774,"Missing"
P08-1101,P07-2055,0,0.0135112,"entor 0.9 0.89 F-score the large search space by imposing strong restrictions on the form of search candidates. In particular, Ng and Low (2004) used character-based POS tagging, which prevents some important POS tagging features such as word + POS tag; Shi and Wang (2007) used an N -best reranking approach, which limits the influence of POS tagging on segmentation to the N -best list. In comparison, our joint model does not impose any hard limitations on the interaction between segmentation and POS information.4 Fast decoding speed is achieved by using a novel multiple-beam search algorithm. Nakagawa and Uchimoto (2007) proposed a hybrid model for word segmentation and POS tagging using an HMM-based approach. Word information is used to process known-words, and character information is used for unknown words in a similar way to Ng and Low (2004). In comparison, our model handles character and word information simultaneously in a single perceptron model. 0.88 0.87 0.86 1 Experiments 2 3 4 5 6 7 8 9 10 Number of training iterations The Chinese Treebank (CTB) 4 is used for the experiments. It is separated into two parts: CTB 3 (420K characters in 150K words / 10364 sentences) is used for the final 10-fold cross"
P08-1101,W04-3236,0,0.531691,"gmentation is a necessary step before POS tagging can be performed. Typically, a Chinese POS tagger takes segmented inputs, which are produced by a separate word segmentor. This two-step approach, however, has an obvious flaw of error propagation, since word segmentation errors cannot be corrected by the POS tagger. A better approach would be to utilize POS inRecent research on Chinese POS tagging has started to investigate joint segmentation and tagging, reporting accuracy improvements over the pipeline approach. Various decoding approaches have been used to reduce the combined search space. Ng and Low (2004) mapped the joint segmentation and POS tagging task into a single character sequence tagging problem. Two types of tags are assigned to each character to represent its segmentation and POS. For example, the tag “b NN” indicates a character at the beginning of a noun. Using this method, POS features are allowed to interact with segmentation. 888 Proceedings of ACL-08: HLT, pages 888–896, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Since tagging is restricted to characters, the search space is reduced to O((4T )n ), and beam search decoding is effective with"
P08-1101,W96-0213,0,0.781993,"linear model. Denoting the global feature vector for the tagged sentence y with Φ(y), we have: Score(y) = Φ(y) · w ~ Templates 15 and 16 in Table 2 are inspired by the CTBMorph feature templates in Tseng et al. (2005), which gave the most accuracy improvement in their experiments. Here the category of a character is the set of tags seen on the character during training. Other morphological features from Tseng et al. (2005) are not used because they require extra web corpora besides the training data. During training, the baseline POS tagger stores special word-tag pairs into a tag dictionary (Ratnaparkhi, 1996). Such information is used by the decoder to prune unlikely tags. For each word occurring more than N times in the training data, the decoder can only assign a tag the word has been seen with in the training data. This method led to improvement in the decoding speed as well as the output accuracy for English POS tagging (Ratnaparkhi, 1996). Besides tags for frequent words, our baseline POS tagger also uses the tag dictionary to store closed-set tags (Xia, 2000) – those associated only with a limited number of Chinese words. 890 where w ~ is the parameter vector in the model. Each element in w"
P08-1101,P06-2098,0,0.0341201,"Missing"
P08-1101,I05-3005,0,0.0149716,"rticipates in word segmentation. 3.1 Formulation of the joint model We formulate joint word segmentation and POS tagging as a single problem, which maps a raw Chinese sentence to a segmented and POS tagged output. Given an input sentence x, the output F (x) satisfies: F (x) = arg max Score(y) y∈GEN(x) where GEN(x) represents the set of possible outputs for x. Score(y) is computed by a feature-based linear model. Denoting the global feature vector for the tagged sentence y with Φ(y), we have: Score(y) = Φ(y) · w ~ Templates 15 and 16 in Table 2 are inspired by the CTBMorph feature templates in Tseng et al. (2005), which gave the most accuracy improvement in their experiments. Here the category of a character is the set of tags seen on the character during training. Other morphological features from Tseng et al. (2005) are not used because they require extra web corpora besides the training data. During training, the baseline POS tagger stores special word-tag pairs into a tag dictionary (Ratnaparkhi, 1996). Such information is used by the decoder to prune unlikely tags. For each word occurring more than N times in the training data, the decoder can only assign a tag the word has been seen with in the"
P08-1101,P07-1106,1,0.914671,"length l with ending character c space-separated characters c1 and c2 character bigram c1 c2 in any word the first / last characters c1 / c2 of any word word w immediately before character c character c immediately before word w the starting characters c1 and c2 of two consecutive words the ending characters c1 and c2 of two consecutive words a word of length l with previous word w a word of length l with next word w Table 1: Feature templates for the baseline segmentor 2 The Baseline System We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POS tagger to refer to the Collins tagger which performs POS tagging only (given segmentation). The features used by the baseline segmentor are shown in Table 1. The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to C"
P11-1069,C04-1180,1,0.665729,"gives competitive accuracies compared to C&C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalised theory of grammar which has been successfully applied to a range of problems in NLP, including treebank creation (Hockenmaier and Steedman, 2007), syntactic parsing (Hockenmaier, 2003; Clark and Curran, 2007), logical form construction (Bos et al., 2004) and surface realization (White and Rajkumar, 2009). From a parsing perspective, the C&C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrase683 Stephen Clark University of Cambridge Computer Laboratory stephen.clark@cl.cam.ac.uk structure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is naturally compatible with bottom-up parsing a"
P11-1069,J07-4004,1,0.750723,"e two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalised theory of grammar which has been successfully applied to a range of problems in NLP, including treebank creation (Hockenmaier and Steedman, 2007), syntactic parsing (Hockenmaier, 2003; Clark and Curran, 2007), logical form construction (Bos et al., 2004) and surface realization (White and Rajkumar, 2009). From a parsing perspective, the C&C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrase683 Stephen Clark University of Cambridge Computer Laboratory stephen.clark@cl.cam.ac.uk structure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is"
P11-1069,P09-2014,1,0.884833,"Missing"
P11-1069,P04-1015,0,0.0376825,"t are put on the agenda. Then the list is cleared and the parser moves on to the next step. This process repeats until the agenda is empty (which means that no new items have been generated in the previous step), and the candidate output is the final derivation. Pseudocode for the algorithm is shown in Figure 3. 687 5 Model and Training We use a global linear model to score candidate items, trained discriminatively with the averaged perceptron (Collins, 2002). Features for a (finished or partial) candidate are extracted from each action that have been applied to build the candidate. Following Collins and Roark (2004), we apply the “early update” strategy to perceptron training: at any step during decoding, if neither the candidate output nor any item in the agenda is correct, decoding is stopped and the parameters are updated using the current highest scored item in the agenda or the candidate output, whichever has the higher score. Table 1 shows the feature templates used by the parser. The symbols S0 , S1 , S2 and S3 in the table represent the top four nodes on the stack (if existent), and Q0 , Q1 , Q2 and Q3 represent the front four words in the incoming queue (if existent). S0 H and S1 H represent the"
P11-1069,W02-1001,0,0.0167382,"y generated partial candidates. After all candidate items from the agenda have been processed, the agenda is cleared and the N -best items from the list are put on the agenda. Then the list is cleared and the parser moves on to the next step. This process repeats until the agenda is empty (which means that no new items have been generated in the previous step), and the candidate output is the final derivation. Pseudocode for the algorithm is shown in Figure 3. 687 5 Model and Training We use a global linear model to score candidate items, trained discriminatively with the averaged perceptron (Collins, 2002). Features for a (finished or partial) candidate are extracted from each action that have been applied to build the candidate. Following Collins and Roark (2004), we apply the “early update” strategy to perceptron training: at any step during decoding, if neither the candidate output nor any item in the agenda is correct, decoding is stopped and the parameters are updated using the current highest scored item in the agenda or the candidate output, whichever has the higher score. Table 1 shows the feature templates used by the parser. The symbols S0 , S1 , S2 and S3 in the table represent the t"
P11-1069,P08-1109,0,0.0309352,"Missing"
P11-1069,P10-1035,0,0.0929355,"theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrase683 Stephen Clark University of Cambridge Computer Laboratory stephen.clark@cl.cam.ac.uk structure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is naturally compatible with bottom-up parsing algorithms such as shift-reduce and CKY (Ades and Steedman, 1982; Steedman, 2000). However, the parsing work by Clark and Curran (2007), and also Hockenmaier (2003) and Fowler and Penn (2010), has only considered chart-parsing. In this paper we fill a gap in the CCG literature by developing a shiftreduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). One advantage of shift-reduce parsers is that the scoring model can be defined over actions, allowing highly efficient parsing by using a greedy algorithm in which the highest scoring action (or a small number of possible actions) is taken at each step. In addition, high accuracy can be maintained by using a model"
P11-1069,J07-3004,0,0.122469,"s with the chart-based C&C parser. We study different errors made by the two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalised theory of grammar which has been successfully applied to a range of problems in NLP, including treebank creation (Hockenmaier and Steedman, 2007), syntactic parsing (Hockenmaier, 2003; Clark and Curran, 2007), logical form construction (Bos et al., 2004) and surface realization (White and Rajkumar, 2009). From a parsing perspective, the C&C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrase683 Stephen Clark University of Cambridge Computer Laboratory stephen.clark@cl.cam.ac.uk structure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et"
P11-1069,D09-1127,0,0.0122833,"Missing"
P11-1069,D07-1123,0,0.0178413,"shifted as an NP. Then “bought” is combined with its object “Lotus” resulting in a verb phrase looking for its subject on the left (S[dcl]NP). Finally, the resulting verb phrase is combined with its subject, resulting in a declarative sentence (S[dcl]). A key difference with previous work on shiftreduce dependency (Nivre et al., 2006) and CFG 686 Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, F i, where S represents the stack with partial derivations that have been built, Q represents the queue of incoming words that have not been processed, and F is a boolean value that represents whether the candidate item has been finished. A candidate item is finished if and only if the FINISH action has been applied to it, and no more acti"
P11-1069,D07-1013,0,0.0286029,"Missing"
P11-1069,P05-1012,0,0.0343332,"l. (2009) (and Ninomiya et al. (2010)) describe a greedy shift-reduce parser for HPSG , in which a single action is chosen at each parsing step, allowing the possibility of highly efficient parsing. Since the HPSG grammar has relatively tight constraints, similar to CCG, the possibility arises that a spanning analysis cannot be found for some sentences. Our approach to this problem was to allow the parser to return a fragmentary analysis; Ninomiya et al. (2009) adopt a different approach based on default unification. Finally, our work is similar to the comparison of the chart-based MSTParser (McDonald et al., 2005) and shift-reduce MaltParser (Nivre et al., 2006) for dependency parsing. MSTParser can perform exhaustive search, given certain feature restrictions, because the complexity of the parsing task is lower than for constituent parsing. C&C can perform exhaustive search because the supertagger has already reduced the search space. We also found that approximate heuristic search for shift-reduce parsing, utilising a rich feature space, can match the performance of the optimal chart-based parser, as well as similar error profiles for the two CCG parsers compared to the two dependency parsers. 8 Conc"
P11-1069,P05-1011,0,0.0344085,"Missing"
P11-1069,E09-1069,0,0.051403,"search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. Matsuzaki et al. (2007) describes similar work to ours but using an automatically-extracted HPSG, rather than CCG, grammar. They also use the generalised perceptron to train a disambiguation model. One difference is that Matsuzaki et al. (2007) use an approximating CFG, in addition to the supertagger, to improve the efficiency of the parser. Ninomiya et al. (2009) (and Ninomiya et al. (2010)) describe a greedy shift-reduce parser for HPSG , in which a single action is chosen at each parsing step, allowing the possibility of highly efficient parsing. Since the HPSG grammar has relatively tight constraints, similar to CCG, the possibility arises that a spanning analysis cannot be found for some sentences. Our approach to this problem was to allow the parser to return a fragmentary analysis; Ninomiya et al. (2009) adopt a different approach based on default unification. Finally, our work is similar to the comparison of the chart-based MSTParser (McDonald"
P11-1069,C04-1010,0,0.187938,"ed onto the stack as an NP; then “bought” is shifted as a transitive verb looking for its object NP on the right and subject NP on the left ((S[dcl]NP)/NP); and then “Lotus” is shifted as an NP. Then “bought” is combined with its object “Lotus” resulting in a verb phrase looking for its subject on the left (S[dcl]NP). Finally, the resulting verb phrase is combined with its subject, resulting in a declarative sentence (S[dcl]). A key difference with previous work on shiftreduce dependency (Nivre et al., 2006) and CFG 686 Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, F i, where S represents the stack with partial derivations that have been built, Q represents the queue of incoming words that have not been processed, and F is"
P11-1069,W06-2933,0,0.0134265,"Missing"
P11-1069,N07-1051,0,0.0312963,"Missing"
P11-1069,P02-1035,0,0.0292003,"the derivations, by extracting combinatory rule instances from the local trees consisting of a parent category and one or two child categories, and applying only those instances during parsing. (These rule instances also include rules to deal with punctuation and unary type-changing rules, in addition to instances of the combinatory rule schemas.) This is the method used by Hockenmaier (2003) and is the method we adopt in this paper. Fowler and Penn (2010) demonstrate that the second extraction method results in a context-free approximation to the grammar resulting from the first 2 1 See e.g. Riezler et al. (2002) and Zhang et al. (2007) for chartbased parsers which can produce fragmentary analyses. 684 Although the C&C default mode applies a restriction for efficiency reasons in which only rule instances seen in CCGbank can be applied, making the grammar of the second type. method, which has the potential to produce a mildlycontext sensitive grammar (given the existence of certain combinatory rules) (Weir, 1988). However, it is important to note that the advantages of CCG, in particular the tight relationship between syntax and semantic interpretation, are still maintained with the second approach, as"
P11-1069,D09-1085,1,0.891416,"Missing"
P11-1069,W05-1513,0,0.183074,"the word “IBM” is shifted onto the stack as an NP; then “bought” is shifted as a transitive verb looking for its object NP on the right and subject NP on the left ((S[dcl]NP)/NP); and then “Lotus” is shifted as an NP. Then “bought” is combined with its object “Lotus” resulting in a verb phrase looking for its subject on the left (S[dcl]NP). Finally, the resulting verb phrase is combined with its subject, resulting in a declarative sentence (S[dcl]). A key difference with previous work on shiftreduce dependency (Nivre et al., 2006) and CFG 686 Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, F i, where S represents the stack with partial derivations that have been built, Q represents the queue of incoming words that have not"
P11-1069,P06-2089,0,0.0157233,"d is not directly comparable with ours, especially considering that their test set is smaller and potentially slightly easier. The final comparison is parser speed. The shiftreduce parser is linear-time (in both sentence length and beam size), and can analyse over 10 sentences per second on a 2GHz CPU, with a beam of 16, which compares very well with other constituency parsers. However, this is no faster than the chart690 based C&C parser, although speed comparisons are difficult because of implementation differences (C&C uses heavily engineered C++ with a focus on efficiency). 7 Related Work Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. Matsuzaki et al. (2007) describes similar work to ours but using an automatically-extracted HPSG, rather than CCG, gra"
P11-1069,N06-2033,0,0.0165163,"d is not directly comparable with ours, especially considering that their test set is smaller and potentially slightly easier. The final comparison is parser speed. The shiftreduce parser is linear-time (in both sentence length and beam size), and can analyse over 10 sentences per second on a 2GHz CPU, with a beam of 16, which compares very well with other constituency parsers. However, this is no faster than the chart690 based C&C parser, although speed comparisons are difficult because of implementation differences (C&C uses heavily engineered C++ with a focus on efficiency). 7 Related Work Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. Matsuzaki et al. (2007) describes similar work to ours but using an automatically-extracted HPSG, rather than CCG, gra"
P11-1069,D09-1043,0,0.00927653,"&C. Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result. 1 Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is a lexicalised theory of grammar which has been successfully applied to a range of problems in NLP, including treebank creation (Hockenmaier and Steedman, 2007), syntactic parsing (Hockenmaier, 2003; Clark and Curran, 2007), logical form construction (Bos et al., 2004) and surface realization (White and Rajkumar, 2009). From a parsing perspective, the C&C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrase683 Stephen Clark University of Cambridge Computer Laboratory stephen.clark@cl.cam.ac.uk structure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al., 2009). The binary branching nature of CCG means that it is naturally compatible with bottom-up parsing algorithms such as shift-reduce and CKY (Ades and St"
P11-1069,W03-3023,0,0.162666,"e “IBM bought Lotus”. First the word “IBM” is shifted onto the stack as an NP; then “bought” is shifted as a transitive verb looking for its object NP on the right and subject NP on the left ((S[dcl]NP)/NP); and then “Lotus” is shifted as an NP. Then “bought” is combined with its object “Lotus” resulting in a verb phrase looking for its subject on the left (S[dcl]NP). Finally, the resulting verb phrase is combined with its subject, resulting in a declarative sentence (S[dcl]). A key difference with previous work on shiftreduce dependency (Nivre et al., 2006) and CFG 686 Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, F i, where S represents the stack with partial derivations that have been built, Q represents the queue of incomi"
P11-1069,D08-1059,1,0.111383,"ght” is combined with its object “Lotus” resulting in a verb phrase looking for its subject on the left (S[dcl]NP). Finally, the resulting verb phrase is combined with its subject, resulting in a declarative sentence (S[dcl]). A key difference with previous work on shiftreduce dependency (Nivre et al., 2006) and CFG 686 Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, F i, where S represents the stack with partial derivations that have been built, Q represents the queue of incoming words that have not been processed, and F is a boolean value that represents whether the candidate item has been finished. A candidate item is finished if and only if the FINISH action has been applied to it, and no more actions can be applied to a"
P11-1069,W09-3825,1,0.737168,"ts object “Lotus” resulting in a verb phrase looking for its subject on the left (S[dcl]NP). Finally, the resulting verb phrase is combined with its subject, resulting in a declarative sentence (S[dcl]). A key difference with previous work on shiftreduce dependency (Nivre et al., 2006) and CFG 686 Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). Both greedy local search and beam-search have linear time complexity. We use beam-search in our CCG parser. To formulate the decoding algorithm, we define a candidate item as a tuple hS, Q, F i, where S represents the stack with partial derivations that have been built, Q represents the queue of incoming words that have not been processed, and F is a boolean value that represents whether the candidate item has been finished. A candidate item is finished if and only if the FINISH action has been applied to it, and no more actions can be applied to a candidate item after i"
P11-1069,W07-1217,0,0.0179799,"ting combinatory rule instances from the local trees consisting of a parent category and one or two child categories, and applying only those instances during parsing. (These rule instances also include rules to deal with punctuation and unary type-changing rules, in addition to instances of the combinatory rule schemas.) This is the method used by Hockenmaier (2003) and is the method we adopt in this paper. Fowler and Penn (2010) demonstrate that the second extraction method results in a context-free approximation to the grammar resulting from the first 2 1 See e.g. Riezler et al. (2002) and Zhang et al. (2007) for chartbased parsers which can produce fragmentary analyses. 684 Although the C&C default mode applies a restriction for efficiency reasons in which only rule instances seen in CCGbank can be applied, making the grammar of the second type. method, which has the potential to produce a mildlycontext sensitive grammar (given the existence of certain combinatory rules) (Weir, 1988). However, it is important to note that the advantages of CCG, in particular the tight relationship between syntax and semantic interpretation, are still maintained with the second approach, as Fowler and Penn (2010)"
P11-2033,D07-1101,0,0.496386,"ers we are aware of through the combination of their POS-tag with information from S0 and N0 . Such use is exemplified by 190 the feature templates “from three words” in Table 1. We further use their word and POS-tag information as “unigram” features in Table 2. Moreover, we include the dependency label information in the unigram features, represented by l in the table. Unigram label information has been used in MaltParser (Nivre et al., 2006a; Nivre, 2006). Third-order features of S 0 and N0 Higher-order context features have been used by graph-based dependency parsers to improve accuracies (Carreras, 2007; Koo and Collins, 2010). We include information of third order dependency arcs in our new feature templates, when available. In Table 2, S0h2 , S0l2 , S0r2 and N0l2 refer to the head of S0h , the second leftmost modifier and the second rightmost modifier of S0 , and the second leftmost modifier of N0 , respectively. The new templates include unigram word, POS-tag and dependency labels of S0h2 , S0l2 , S0r2 and N0l2 , as well as POS-tag combinations with S0 and N0 . Set of dependency labels with S0 and N0 As a more global feature, we include the set of unique dependency labels from the modifie"
P11-2033,cer-etal-2010-parsing,0,0.0840264,"Missing"
P11-2033,W02-1001,0,0.651947,"rc-standard (Yamada and Matsumoto, 2003; Huang and Sagae, 2010) process. We adopt the arc-eager system1 , for which the actions are: • Shift, which removes the front of the queue and pushes it onto the top of the stack; • Reduce, which pops the top item off the stack; • LeftArc, which pops the top item off the stack, and adds it as a modifier to the front of the queue; • RightArc, which removes the front of the queue, pushes it onto the stack and adds it as a modifier to the top of the stack. Further, we follow Zhang and Clark (2008) and Huang et al. (2009) and use the generalized perceptron (Collins, 2002) for global learning and beamsearch for decoding. Unlike both earlier globallearning parsers, which only perform unlabeled parsing, we perform labeled parsing by augmenting the LeftArc and RightArc actions with the set of dependency labels. Hence our work is in line with Titov and Henderson (2007) in using labeled transitions with global learning. Moreover, we will see that label information can actually improve link accuracy. 3 Feature Templates At each step during a parsing process, the parser configuration can be represented by a tuple hS, N, Ai, where S is the stack, N is the queue of inco"
P11-2033,de-marneffe-etal-2006-generating,0,0.273267,"Missing"
P11-2033,P10-1110,0,0.878958,"inguistics:shortpapers, pages 188–193, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 The Transition-based Parsing Algorithm In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack. A set of shiftreduce actions are defined, which consume words from the queue and build the output parse. Recent research have focused on action sets that build projective dependency trees in an arc-eager (Nivre et al., 2006b; Zhang and Clark, 2008) or arc-standard (Yamada and Matsumoto, 2003; Huang and Sagae, 2010) process. We adopt the arc-eager system1 , for which the actions are: • Shift, which removes the front of the queue and pushes it onto the top of the stack; • Reduce, which pops the top item off the stack; • LeftArc, which pops the top item off the stack, and adds it as a modifier to the front of the queue; • RightArc, which removes the front of the queue, pushes it onto the stack and adds it as a modifier to the top of the stack. Further, we follow Zhang and Clark (2008) and Huang et al. (2009) and use the generalized perceptron (Collins, 2002) for global learning and beamsearch for decoding."
P11-2033,D09-1127,0,0.319641,"Missing"
P11-2033,D07-1123,0,0.0191833,"ndency parsing (Yamada and Matsumoto, 2003; Nivre et al., 2006b; Zhang and Clark, 2008; Huang and Sagae, 2010) utilize a deterministic shift-reduce process for making structural predictions. Compared to graph-based dependency parsing, it typically offers linear time complexity and the comparative freedom to define non-local features, as exemplified by the comparison between MaltParser and MSTParser (Nivre et al., 2006b; McDonald et al., 2005; McDonald and Nivre, 2007). Recent research has addressed two potential disadvantages of systems like MaltParser. In the aspect of decoding, beam-search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2009) and partial dynamic-programming (Huang and Sagae, 2010) have been applied to improve upon 188 Joakim Nivre Uppsala University Department of Linguistics and Philology joakim.nivre@lingfil.uu.se greedy one-best search, and positive results were reported. In the aspect of training, global structural learning has been used to replace local learning on each decision (Zhang and Clark, 2008; Huang et al., 2009), although the effect of global learning has not been separated out and studied alone. In this short paper, we study a third aspect in a statistical"
P11-2033,P10-1001,0,0.752146,"s been used to replace local learning on each decision (Zhang and Clark, 2008; Huang et al., 2009), although the effect of global learning has not been separated out and studied alone. In this short paper, we study a third aspect in a statistical system: feature definition. Representing the type of information a statistical system uses to make predictions, feature templates can be one of the most important factors determining parsing accuracy. Various recent attempts have been made to include non-local features into graph-based dependency parsing (Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Transitionbased parsing, by contrast, can easily accommodate arbitrarily complex representations involving nonlocal features. Complex non-local features, such as bracket matching and rhythmic patterns, are used in transition-based constituency parsing (Zhang and Clark, 2009; Wang et al., 2006), and most transitionbased dependency parsers incorporate some nonlocal features, but current practice is nevertheless to use a rather restricted set of features, as exemplified by the default feature models in MaltParser (Nivre et al., 2006a). We explore considerably richer feature representations and"
P11-2033,P08-1068,0,0.724975,"Missing"
P11-2033,P09-1039,0,0.246008,"structural learning has been used to replace local learning on each decision (Zhang and Clark, 2008; Huang et al., 2009), although the effect of global learning has not been separated out and studied alone. In this short paper, we study a third aspect in a statistical system: feature definition. Representing the type of information a statistical system uses to make predictions, feature templates can be one of the most important factors determining parsing accuracy. Various recent attempts have been made to include non-local features into graph-based dependency parsing (Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Transitionbased parsing, by contrast, can easily accommodate arbitrarily complex representations involving nonlocal features. Complex non-local features, such as bracket matching and rhythmic patterns, are used in transition-based constituency parsing (Zhang and Clark, 2009; Wang et al., 2006), and most transitionbased dependency parsers incorporate some nonlocal features, but current practice is nevertheless to use a rather restricted set of features, as exemplified by the default feature models in MaltParser (Nivre et al., 2006a). We explore considerably richer feat"
P11-2033,D07-1013,1,0.745444,"hey give a signficant improvement of the state of the art. An open source release of our parser is freely available. 1 Introduction Transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre et al., 2006b; Zhang and Clark, 2008; Huang and Sagae, 2010) utilize a deterministic shift-reduce process for making structural predictions. Compared to graph-based dependency parsing, it typically offers linear time complexity and the comparative freedom to define non-local features, as exemplified by the comparison between MaltParser and MSTParser (Nivre et al., 2006b; McDonald et al., 2005; McDonald and Nivre, 2007). Recent research has addressed two potential disadvantages of systems like MaltParser. In the aspect of decoding, beam-search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2009) and partial dynamic-programming (Huang and Sagae, 2010) have been applied to improve upon 188 Joakim Nivre Uppsala University Department of Linguistics and Philology joakim.nivre@lingfil.uu.se greedy one-best search, and positive results were reported. In the aspect of training, global structural learning has been used to replace local learning on each decision (Zhang and Clark, 2008; Huang et al.,"
P11-2033,E06-1011,0,0.816632,"Missing"
P11-2033,P05-1012,0,0.9789,"the Chinese Treebank, they give a signficant improvement of the state of the art. An open source release of our parser is freely available. 1 Introduction Transition-based dependency parsing (Yamada and Matsumoto, 2003; Nivre et al., 2006b; Zhang and Clark, 2008; Huang and Sagae, 2010) utilize a deterministic shift-reduce process for making structural predictions. Compared to graph-based dependency parsing, it typically offers linear time complexity and the comparative freedom to define non-local features, as exemplified by the comparison between MaltParser and MSTParser (Nivre et al., 2006b; McDonald et al., 2005; McDonald and Nivre, 2007). Recent research has addressed two potential disadvantages of systems like MaltParser. In the aspect of decoding, beam-search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2009) and partial dynamic-programming (Huang and Sagae, 2010) have been applied to improve upon 188 Joakim Nivre Uppsala University Department of Linguistics and Philology joakim.nivre@lingfil.uu.se greedy one-best search, and positive results were reported. In the aspect of training, global structural learning has been used to replace local learning on each decision (Zhang and"
P11-2033,P81-1022,0,0.736428,"Missing"
P11-2033,W06-2933,1,0.798448,"Missing"
P11-2033,D07-1111,0,0.0803254,"ature set by combining it with the word and POS-tag of S0 and N0 , as shown in Table 2. It is worth noticing that the use of distance information in our transition-based model is different from that in a typical graph-based parser such as MSTParser. The distance between S0 and N0 will correspond to the distance between a pair of head and modifier when an LeftArc action is taken, for example, but not when a Shift action is taken. Valency of S0 and N0 The number of modifiers to a given head is used by the graph-based submodel of Zhang and Clark (2008) and the models of Martins et al. (2009) and Sagae and Tsujii (2007). We include similar information in our model. In particular, we calculate the number of left and right modifiers separately, calling them left valency and right valency, respectively. Left and right valencies are represented by vl and vr in Table 2, respectively. They are combined with the word and POS-tag of S0 and N0 to form new feature templates. Again, the use of valency information in our transition-based parser is different from the aforementioned graph-based models. In our case, valency information is put into the context of the shift-reduce process, and used together with each action"
P11-2033,D08-1016,0,0.00728383,"ect of training, global structural learning has been used to replace local learning on each decision (Zhang and Clark, 2008; Huang et al., 2009), although the effect of global learning has not been separated out and studied alone. In this short paper, we study a third aspect in a statistical system: feature definition. Representing the type of information a statistical system uses to make predictions, feature templates can be one of the most important factors determining parsing accuracy. Various recent attempts have been made to include non-local features into graph-based dependency parsing (Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Transitionbased parsing, by contrast, can easily accommodate arbitrarily complex representations involving nonlocal features. Complex non-local features, such as bracket matching and rhythmic patterns, are used in transition-based constituency parsing (Zhang and Clark, 2009; Wang et al., 2006), and most transitionbased dependency parsers incorporate some nonlocal features, but current practice is nevertheless to use a rather restricted set of features, as exemplified by the default feature models in MaltParser (Nivre et al., 2006a). We explore co"
P11-2033,W07-2218,0,0.236678,"pops the top item off the stack, and adds it as a modifier to the front of the queue; • RightArc, which removes the front of the queue, pushes it onto the stack and adds it as a modifier to the top of the stack. Further, we follow Zhang and Clark (2008) and Huang et al. (2009) and use the generalized perceptron (Collins, 2002) for global learning and beamsearch for decoding. Unlike both earlier globallearning parsers, which only perform unlabeled parsing, we perform labeled parsing by augmenting the LeftArc and RightArc actions with the set of dependency labels. Hence our work is in line with Titov and Henderson (2007) in using labeled transitions with global learning. Moreover, we will see that label information can actually improve link accuracy. 3 Feature Templates At each step during a parsing process, the parser configuration can be represented by a tuple hS, N, Ai, where S is the stack, N is the queue of incoming words, and A is the set of dependency arcs that have been built. Denoting the top of stack 1 It is very likely that the type of features explored in this paper would be beneficial also for the arc-standard system, although the exact same feature templates would not be applicable because of di"
P11-2033,W06-0121,0,0.0414168,"Missing"
P11-2033,W03-3023,0,0.942541,"ociation for Computational Linguistics:shortpapers, pages 188–193, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 The Transition-based Parsing Algorithm In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack. A set of shiftreduce actions are defined, which consume words from the queue and build the output parse. Recent research have focused on action sets that build projective dependency trees in an arc-eager (Nivre et al., 2006b; Zhang and Clark, 2008) or arc-standard (Yamada and Matsumoto, 2003; Huang and Sagae, 2010) process. We adopt the arc-eager system1 , for which the actions are: • Shift, which removes the front of the queue and pushes it onto the top of the stack; • Reduce, which pops the top item off the stack; • LeftArc, which pops the top item off the stack, and adds it as a modifier to the front of the queue; • RightArc, which removes the front of the queue, pushes it onto the stack and adds it as a modifier to the top of the stack. Further, we follow Zhang and Clark (2008) and Huang et al. (2009) and use the generalized perceptron (Collins, 2002) for global learning and"
P11-2033,D08-1059,1,0.926236,"gs of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 188–193, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 The Transition-based Parsing Algorithm In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack. A set of shiftreduce actions are defined, which consume words from the queue and build the output parse. Recent research have focused on action sets that build projective dependency trees in an arc-eager (Nivre et al., 2006b; Zhang and Clark, 2008) or arc-standard (Yamada and Matsumoto, 2003; Huang and Sagae, 2010) process. We adopt the arc-eager system1 , for which the actions are: • Shift, which removes the front of the queue and pushes it onto the top of the stack; • Reduce, which pops the top item off the stack; • LeftArc, which pops the top item off the stack, and adds it as a modifier to the front of the queue; • RightArc, which removes the front of the queue, pushes it onto the stack and adds it as a modifier to the top of the stack. Further, we follow Zhang and Clark (2008) and Huang et al. (2009) and use the generalized percept"
P11-2033,W09-3825,1,0.743729,"Missing"
P11-2033,nivre-etal-2006-maltparser,1,\N,Missing
P11-2033,N10-1115,0,\N,Missing
P11-2033,L10-1000,0,\N,Missing
P13-1013,P12-1110,0,0.289433,"syntax trees (Figure 1(b)). With richer information than word-level trees, this form of parse trees can be useful for all the aforementioned Chinese NLP applications. (d) modifier-noun. Figure 2: Inner word structures of “库存 (repertory)”,“考古 (archaeology)”, “科技 (science and technology)” and “败类 (degenerate)”. art joint segmenter and POS tagger, and our baseline word-based parser. Our word annotations lead to further improvements to the joint system, especially for phrase-structure parsing accuracy. Our parser work falls in line with recent work of joint segmentation, POS tagging and parsing (Hatori et al., 2012; Li and Zhou, 2012; Qian and Liu, 2012). Compared with related work, our model gives the best published results for joint segmentation and POS tagging, as well as joint phrase-structure parsing on standard CTB5 evaluations. With linear-time complexity, our parser is highly efficient, processing over 30 sentences per second with a beam size of 16. An open release of the parser is freely available at http://sourceforge.net/projects/zpar/, version 0.6. With regard to task of parsing itself, an important advantage of the character-level syntax trees is that they allow word segmentation, part-of-s"
P13-1013,P09-1058,0,0.179797,"Missing"
P13-1013,D12-1132,0,0.68711,"1(b)). With richer information than word-level trees, this form of parse trees can be useful for all the aforementioned Chinese NLP applications. (d) modifier-noun. Figure 2: Inner word structures of “库存 (repertory)”,“考古 (archaeology)”, “科技 (science and technology)” and “败类 (degenerate)”. art joint segmenter and POS tagger, and our baseline word-based parser. Our word annotations lead to further improvements to the joint system, especially for phrase-structure parsing accuracy. Our parser work falls in line with recent work of joint segmentation, POS tagging and parsing (Hatori et al., 2012; Li and Zhou, 2012; Qian and Liu, 2012). Compared with related work, our model gives the best published results for joint segmentation and POS tagging, as well as joint phrase-structure parsing on standard CTB5 evaluations. With linear-time complexity, our parser is highly efficient, processing over 30 sentences per second with a beam size of 16. An open release of the parser is freely available at http://sourceforge.net/projects/zpar/, version 0.6. With regard to task of parsing itself, an important advantage of the character-level syntax trees is that they allow word segmentation, part-of-speech (POS) tagging"
P13-1013,P11-1141,0,0.689122,"c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (constituent) trees, adding recursive structures of characters for words. We manually annotate the structures of 37,382 words, which cover the entire CTB5. Using these annotations, we transform CTB-style constituent trees into character-level trees (Figure 1(b)). Our word structure corpus, together with a set of tools to transform CTB-style trees into character-level trees, is released at https://github.com/zhangmeishan/wordstructures. Our annotation work is in line with the work of Vadas and Curran (2007) and Li (2011), which provide extended annotations of Penn Treebank (PTB) noun phrases and CTB words (on the morphological level), respectively. NN-rNN-r NN-r NN-r NN-bNN-b NN-iNN-i NN-b NN-b NN-i NN-i NN-lNN-l NN-l NN-l VV-bVV-b VV-iVV-i VV-b VV-b VV-i VV-i 库 库 存 存 考 考 古 古 (repository) (saving) (investigate) (ancient) 库 存 考 古 (repository) (saving) (investigate) (ancient) 库 存 考 古 (repository) (saving) (saving) (investigate) (investigate)(ancient) (ancient) (repository) (a) subject-predicate. (b) verb-object. NN-rNN-r NN-cNN-c NNc NN-r NN-r NN-c NN-bNN-b NN-iNN-i NN-bNN-b NN-iNN-i NN-b NN-b NN-b NN-i NN-i NN"
P13-1013,W03-1025,0,0.0140119,"for joint segmentation and POS tagging, as well as joint phrase-structure parsing on standard CTB5 evaluations. With linear-time complexity, our parser is highly efficient, processing over 30 sentences per second with a beam size of 16. An open release of the parser is freely available at http://sourceforge.net/projects/zpar/, version 0.6. With regard to task of parsing itself, an important advantage of the character-level syntax trees is that they allow word segmentation, part-of-speech (POS) tagging and parsing to be performed jointly, using an efficient CKY-style or shift-reduce algorithm. Luo (2003) exploited this advantage by adding flat word structures without manually annotation to CTB trees, and building a generative character-based parser. Compared to a pipeline system, the advantages of a joint system include reduction of error propagation, and the integration of segmentation, POS tagging and syntax features. With hierarchical structures and head character information, our annotated words are more informative than flat word structures, and hence can bring further improvements to phrase-structure parsing. 2 Word Structures and Syntax Trees The Chinese language is a character-based l"
P13-1013,W12-6304,0,0.0342964,"ore informative than flat word structures, and hence can bring further improvements to phrase-structure parsing. 2 Word Structures and Syntax Trees The Chinese language is a character-based language. Unlike alphabetical languages, Chinese characters convey meanings, and the meaning of most Chinese words takes roots in their character. For example, the word “计算机 (computer)” is composed of the characters “计 (count)”, “算 (calculate)” and “机 (machine)”. An informal name of “computer” is “电脑”, which is composed of “电 (electronic)” and “脑 (brain)”. Chinese words have internal structures (Xue, 2001; Ma et al., 2012). The way characters interact within words can be similar to the way words interact within phrases. Figure 2 shows the structures of the four words “库存 (repertory)”, “ 考古 To analyze word structures in addition to phrase structures, our character-based parser naturally performs joint word segmentation, POS tagging and parsing jointly. Our model is based on the discriminative shift-reduce parser of Zhang and Clark (2009; 2011), which is a state-of-the-art word-based phrase-structure parser for Chinese. We extend their shift-reduce framework, adding more transition actions for word segmentation a"
P13-1013,W04-3236,0,0.0796187,"and morphological-level word structures. 朋友 (friend) 们 (plural) 教育 (education) 界 (field) structures. For each word or subword, we specify its POS and head direction. We use “l”, “r” and “c” to indicate the “left”, “right” and “coordination” head directions, respectively. The “coordination” direction is mostly used in coordination structures, while a very small number of transliteration words, such as “奥巴马 (Obama)” and “洛 杉矶 (Los Angeles)”, have flat structures, and we use “coordination” for their left binarization. For leaf characters, we follow previous work on word segmentation (Xue, 2003; Ng and Low, 2004), and use “b” and “i” to indicate the beginning and nonbeginning characters of a word, respectively. The vast majority of words do not have structural ambiguities. However, the structures of some words may vary according to different POS. For example, “制 服” means “dominate” when it is tagged as a verb, of which the head is the left character; the same word means “uniform dress” when tagged as a noun, of which the head is the right character. Thus the input of the word structure annotation is a word together with its POS. The annotation work was conducted by three persons, with one person annot"
P13-1013,P04-1015,0,0.337377,"erform word segmentation, POS tagging and phrase-structure parsing. To our knowledge, this is the first work to develop a transition-based system that jointly performs the above three tasks. Trained using annotated word structures, our parser also analyzes the internal structures of Chinese words. Our character-based Chinese parsing model is based on the work of Zhang and Clark (2009), which is a transition-based model for lexicalized constituent parsing. They use a beam-search decoder so that the transition action sequence can be globally optimized. The averaged perceptron with early-update (Collins and Roark, 2004) is used to train the model parameters. Their transition system contains four kinds of actions: (1) SHIFT, (2) REDUCE-UNARY, (3) REDUCE-BINARY and (4) TERMINATE. The system can provide binarzied CFG trees in Chomsky Norm Form, and they present a reversible conversion procedure to map arbitrary CFG trees into binarized trees. In this work, we remain consistent with their work, using the head-finding rules of Zhang and Clark (2008), and the same binarization algorithm.1 We apply the same beam-search algorithm for decoding, and employ the averaged perceptron with early-update to train our model."
P13-1013,D10-1082,1,0.605411,"a word is in a tag dictionary, which is collected by extracting all multi-character subwords that occur more than five times in the training corpus. For string features, c0 , c−1 and c−2 represent the current character and its previous two characters, respectively; w−1 and w−2 represent the previous two words to the current character, respectively; t0 , t−1 and t−2 represent the POS tags of the current word and the previous two words, respectively. The string features are used for word segmentation and POS tagging, and are adapted from a state-of-the-art joint segmentation and tagging model (Zhang and Clark, 2010). In summary, our character-based parser contains the word-based features of constituent parser presented in Zhang and Clark (2009), the wordbased and shallow character-based features of joint word segmentation and POS tagging presented in Zhang and Clark (2010), and additionally the deep character-based features that encode word structure information, which are the first presented by this paper. 4 4.1 95 90 90 80 85 70 80 64b 16b 4b 1b 75 70 60 64b 16b 4b 1b 50 40 65 30 0 10 20 30 40 (a) Joint segmentation and POS tagging F-scores. 0 10 20 30 40 (b) Joint constituent parsing F-scores. Figure"
P13-1013,D12-1046,0,0.0993978,"information than word-level trees, this form of parse trees can be useful for all the aforementioned Chinese NLP applications. (d) modifier-noun. Figure 2: Inner word structures of “库存 (repertory)”,“考古 (archaeology)”, “科技 (science and technology)” and “败类 (degenerate)”. art joint segmenter and POS tagger, and our baseline word-based parser. Our word annotations lead to further improvements to the joint system, especially for phrase-structure parsing accuracy. Our parser work falls in line with recent work of joint segmentation, POS tagging and parsing (Hatori et al., 2012; Li and Zhou, 2012; Qian and Liu, 2012). Compared with related work, our model gives the best published results for joint segmentation and POS tagging, as well as joint phrase-structure parsing on standard CTB5 evaluations. With linear-time complexity, our parser is highly efficient, processing over 30 sentences per second with a beam size of 16. An open release of the parser is freely available at http://sourceforge.net/projects/zpar/, version 0.6. With regard to task of parsing itself, an important advantage of the character-level syntax trees is that they allow word segmentation, part-of-speech (POS) tagging and parsing to be pe"
P13-1013,J11-1005,1,0.212902,"Missing"
P13-1013,P11-1139,0,0.0424243,"Missing"
P13-1013,E09-1100,0,0.443661,"Missing"
P13-1013,P07-1031,0,0.0309607,"Linguistics, pages 125–134, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (constituent) trees, adding recursive structures of characters for words. We manually annotate the structures of 37,382 words, which cover the entire CTB5. Using these annotations, we transform CTB-style constituent trees into character-level trees (Figure 1(b)). Our word structure corpus, together with a set of tools to transform CTB-style trees into character-level trees, is released at https://github.com/zhangmeishan/wordstructures. Our annotation work is in line with the work of Vadas and Curran (2007) and Li (2011), which provide extended annotations of Penn Treebank (PTB) noun phrases and CTB words (on the morphological level), respectively. NN-rNN-r NN-r NN-r NN-bNN-b NN-iNN-i NN-b NN-b NN-i NN-i NN-lNN-l NN-l NN-l VV-bVV-b VV-iVV-i VV-b VV-b VV-i VV-i 库 库 存 存 考 考 古 古 (repository) (saving) (investigate) (ancient) 库 存 考 古 (repository) (saving) (investigate) (ancient) 库 存 考 古 (repository) (saving) (saving) (investigate) (investigate)(ancient) (ancient) (repository) (a) subject-predicate. (b) verb-object. NN-rNN-r NN-cNN-c NNc NN-r NN-r NN-c NN-bNN-b NN-iNN-i NN-bNN-b NN-iNN-i NN-b NN-b NN-"
P13-1013,I11-1035,0,0.0680725,"Missing"
P13-1013,O03-4002,0,0.0938348,"f NN-f NN-f and morphological-level word structures. 朋友 (friend) 们 (plural) 教育 (education) 界 (field) structures. For each word or subword, we specify its POS and head direction. We use “l”, “r” and “c” to indicate the “left”, “right” and “coordination” head directions, respectively. The “coordination” direction is mostly used in coordination structures, while a very small number of transliteration words, such as “奥巴马 (Obama)” and “洛 杉矶 (Los Angeles)”, have flat structures, and we use “coordination” for their left binarization. For leaf characters, we follow previous work on word segmentation (Xue, 2003; Ng and Low, 2004), and use “b” and “i” to indicate the beginning and nonbeginning characters of a word, respectively. The vast majority of words do not have structural ambiguities. However, the structures of some words may vary according to different POS. For example, “制 服” means “dominate” when it is tagged as a verb, of which the head is the left character; the same word means “uniform dress” when tagged as a noun, of which the head is the right character. Thus the input of the word structure annotation is a word together with its POS. The annotation work was conducted by three persons, wi"
P13-1013,D08-1059,1,0.600383,"zed constituent parsing. They use a beam-search decoder so that the transition action sequence can be globally optimized. The averaged perceptron with early-update (Collins and Roark, 2004) is used to train the model parameters. Their transition system contains four kinds of actions: (1) SHIFT, (2) REDUCE-UNARY, (3) REDUCE-BINARY and (4) TERMINATE. The system can provide binarzied CFG trees in Chomsky Norm Form, and they present a reversible conversion procedure to map arbitrary CFG trees into binarized trees. In this work, we remain consistent with their work, using the head-finding rules of Zhang and Clark (2008), and the same binarization algorithm.1 We apply the same beam-search algorithm for decoding, and employ the averaged perceptron with early-update to train our model. We make two extensions to their work to enable joint segmentation, POS tagging and phrasestructure parsing from the character level. First, we modify the actions of the transition system for • SHIFT-SEPARATE(t): remove the head character cj from Q, pushing a subword node S0 2 0 cj onto S, assigning S .t = t. Note that the parse tree S0 must correspond to a full-word or a phrase node, and the character cj is the first character of"
P13-1013,W09-3825,1,0.954764,"”, “算 (calculate)” and “机 (machine)”. An informal name of “computer” is “电脑”, which is composed of “电 (electronic)” and “脑 (brain)”. Chinese words have internal structures (Xue, 2001; Ma et al., 2012). The way characters interact within words can be similar to the way words interact within phrases. Figure 2 shows the structures of the four words “库存 (repertory)”, “ 考古 To analyze word structures in addition to phrase structures, our character-based parser naturally performs joint word segmentation, POS tagging and parsing jointly. Our model is based on the discriminative shift-reduce parser of Zhang and Clark (2009; 2011), which is a state-of-the-art word-based phrase-structure parser for Chinese. We extend their shift-reduce framework, adding more transition actions for word segmentation and POS tagging, and defining novel features that capture character information. Even when trained using character-level syntax trees with flat word structures, our joint parser outperforms a strong pipelined baseline that consists of a state-of-the126 VV-lV VV-l VV-bVV-b VV-b VV-b 烧 烧 烧 (burn) 烧(burn) (burn)(burn) AD-lA AD-l AD-bAD-b AD-b AD-b 徒 徒 徒 (vain) 徒(vain) (vain)(vain) V-l VV-b 横 (fiercely) 横 (fiercely) VV-l V"
P13-1013,N07-1051,0,\N,Missing
P13-1043,P05-1022,0,0.0285913,"Missing"
P13-1043,A00-2018,0,0.0580873,"oduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics smallest one. This turns out to have a significant empirical impact on parsing with beam-search. We propose an extension to the shift-reduce process to address this problem, which gives significant improvements to the parsing accuracies. Our method is conceptually simple, requiring only one additional transition action to eliminate size differences between different candidate outp"
P13-1043,D09-1060,1,0.318907,"ed, and output the best state item in the agenda. With this new transition process, we experimented with several extended features,and found that the templates in Table 2 are useful to improve the accuracies further. Here si ll denotes the left child of si ’s left child. Other notations can be explained in a similar way. 4.2 Dependency Relations: Lexical Dependencies Lexical dependencies represent linguistic relations between words: whether a word modifies another word. The idea of exploiting lexical dependency information from auto-parsed data has been explored before for dependency parsing (Chen et al., 2009) and constituent parsing (Zhu et al., 2012). To extract lexical dependencies, we first run the baseline parser on unlabeled data. To simplify the extraction process, we can convert auto-parsed constituency trees into dependency trees by using Penn2Malt. 2 From the dependency trees, we extract bigram lexical dependencies hw1 , w2 , L/Ri where the symbol L (R) means that w1 (w2 ) is the head of w2 (w1 ). We also extract trigram lexical 4 Semi-supervised Parsing with Large Data This section discusses how to extract information from unlabeled data or auto-parsed data to further improve shift-reduc"
P13-1043,P12-1023,1,0.715706,"Missing"
P13-1043,P04-1015,0,0.364497,"cq0 w, s0 cq0 t, q0 wq1 w, q0 wq1 t, q0 tq1 w, q0 tq1 t, s1 wq0 w, s1 wq0 t, s1 cq0 w, s1 cq0 t s0 cs1 cs2 c, s0 ws1 cs2 c, s0 cs1 wq0 t s0 cs1 cs2 w, s0 cs1 cq0 t, s0 ws1 cq0 t s0 cs1 wq0 t, s0 cs1 cq0 w NN address i=1 Φ(ai ) · θ~ Here Φ(ai ) represents the feature vector for the ith action ai in state item α. It is computed by applying the feature templates in Table 1 to the context of α. N is the total number of actions in α. The model parameter ~ θ is trained with the averaged perceptron algorithm, applied to state items (sequence of actions) globally. We apply the early update strategy (Collins and Roark, 2004), stopping parsing for parameter updates when the goldstandard state item falls off the agenda. 2.3 NP address NNS issues issues items for the same sentence can have different numbers of unary actions. Take the phrase “address issues” for example, two possible parses are shown in Figure 2 (a) and (b), respectively. The first parse corresponds to the action sequence [SHIFT, SHIFT, REDUCE-R-NP, FINISH], while the second parse corresponds to the action sequence [SHIFT, SHIFT, UNARY-NP, REDUCE-LVP, FINISH], which consists of one more action than the first case. In practice, variances between state"
P13-1043,P97-1003,0,0.0366356,"d as the 1 Introduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics smallest one. This turns out to have a significant empirical impact on parsing with beam-search. We propose an extension to the shift-reduce process to address this problem, which gives significant improvements to the parsing accuracies. Our method is conceptually simple, requiring only one additional transition action to eliminate size differences between differen"
P13-1043,D12-1133,0,0.0157449,"2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). In addition, processing tens of sentences per second (Zhang and Nivre, 2011), these transition-based parsers can be a favorable choice for dependency parsing. Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global trai"
P13-1043,W08-2102,0,0.0934831,"Missing"
P13-1043,D09-1087,0,0.0306782,"Missing"
P13-1043,P10-1110,0,0.023533,"ang Chen∗ , Min Zhang∗ and Jingbo Zhu† † Natural Language Processing Lab., Northeastern University, China ‡ Singapore University of Technology and Design, Singapore ∗ Soochow University, China and Institute for Infocomm Research, Singapore zhumuhua@gmail.com yue zhang@sutd.edu.sg chenwenliang@gmail.com mzhang@i2r.a-star.edu.sg zhujingbo@mail.neu.edu.cn Abstract Various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search and global discriminative training have been shown effective for dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). In"
P13-1043,D10-1002,0,0.0607489,"Missing"
P13-1043,P08-1067,0,0.0401316,"Missing"
P13-1043,N06-2033,0,0.0133712,"eam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser. The above global-learning and beam-search framework can be applied to transition-based phrase-structure (constituent) parsing also (Zhang and Clark, 2009), maintaining all the aforementioned benefits. However, the effects were not as significant as for transition-based dependency parsing. The best reported accuracies of transition-based constituent parsers still lag behind the state-of-the-art (Sagae and Lavie, 2006; Zhang and Clark, 2009). One difference between phrasestructure parsing and dependency parsing is that for the former, parse trees with different numbers of unary rules require different numbers of actions to build. Hence the scoring model needs to disambiguate between transitions sequences with different sizes. For the same sentence, the largest output can take twice as many as actions to build as the 1 Introduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local d"
P13-1043,P08-1066,0,0.0170814,"Missing"
P13-1043,P12-1026,0,0.0106504,"tions are captured by word clustering, lexical dependencies, and a dependency language model, respectively. Based on the information, we propose a set of novel features specifically designed for shift-reduce constituent parsing. [S|s0 , i, false, k, c] [S|X, i, false, k + 1, c + cu ] [S, n, false, k, c] [S, n, true, k + 1, c + cf ] [S, n, true, k, c] [S, n, true, k + 1, c + ci ] 4.1 Paradigmatic Relations: Word Clustering Figure 3: Deductive system of the extended transition system. Word clusters are regarded as lexical intermediaries for dependency parsing (Koo et al., 2008) and POS tagging (Sun and Uszkoreit, 2012). We employ the Brown clustering algorithm (Liang, 2005) on unannotated data (word segmentation is performed if necessary). In the initial state of clustering, each word in the input corpus is regarded as a cluster, then the algorithm repeatedly merges pairs of clusters that cause the least decrease in the likelihood of the input corpus. The clustering results are a binary tree with words appearing as leaves. Each cluster is represented as a bit-string from the root to the tree node that represents the cluster. We define a function CLU(w) to return the cluster ID (a bit string) of an input wor"
P13-1043,P08-1068,0,0.00967274,"nd structural relations. These relations are captured by word clustering, lexical dependencies, and a dependency language model, respectively. Based on the information, we propose a set of novel features specifically designed for shift-reduce constituent parsing. [S|s0 , i, false, k, c] [S|X, i, false, k + 1, c + cu ] [S, n, false, k, c] [S, n, true, k + 1, c + cf ] [S, n, true, k, c] [S, n, true, k + 1, c + ci ] 4.1 Paradigmatic Relations: Word Clustering Figure 3: Deductive system of the extended transition system. Word clusters are regarded as lexical intermediaries for dependency parsing (Koo et al., 2008) and POS tagging (Sun and Uszkoreit, 2012). We employ the Brown clustering algorithm (Liang, 2005) on unannotated data (word segmentation is performed if necessary). In the initial state of clustering, each word in the input corpus is regarded as a cluster, then the algorithm repeatedly merges pairs of clusters that cause the least decrease in the likelihood of the input corpus. The clustering results are a binary tree with words appearing as leaves. Each cluster is represented as a bit-string from the root to the tree node that represents the cluster. We define a function CLU(w) to return the"
P13-1043,W03-3023,0,0.0272721,"y rules require different numbers of actions to build. Hence the scoring model needs to disambiguate between transitions sequences with different sizes. For the same sentence, the largest output can take twice as many as actions to build as the 1 Introduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics smallest one. This turns out to have a significant empirical impact on parsing with beam-search. We propose an extension to the shift-reduce pro"
P13-1043,P08-1101,1,0.41006,"u† , Yue Zhang‡ , Wenliang Chen∗ , Min Zhang∗ and Jingbo Zhu† † Natural Language Processing Lab., Northeastern University, China ‡ Singapore University of Technology and Design, Singapore ∗ Soochow University, China and Institute for Infocomm Research, Singapore zhumuhua@gmail.com yue zhang@sutd.edu.sg chenwenliang@gmail.com mzhang@i2r.a-star.edu.sg zhujingbo@mail.neu.edu.cn Abstract Various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search and global discriminative training have been shown effective for dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohn"
P13-1043,W09-3825,1,0.736189,"outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser. The above global-learning and beam-search framework can be applied to transition-based phrase-structure (constituent) parsing also (Zhang and Clark, 2009), maintaining all the aforementioned benefits. However, the effects were not as significant as for transition-based dependency parsing. The best reported accuracies of transition-based constituent parsers still lag behind the state-of-the-art (Sagae and Lavie, 2006; Zhang and Clark, 2009). One difference between phrasestructure parsing and dependency parsing is that for the former, parse trees with different numbers of unary rules require different numbers of actions to build. Hence the scoring model needs to disambiguate between transitions sequences with different sizes. For the same sentenc"
P13-1043,J93-2004,0,0.0616461,"Missing"
P13-1043,N06-1020,0,0.0167397,"Missing"
P13-1043,P05-1012,0,0.0147001,"ion-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics smallest one. This turns out to have a significant empirical impact on parsing with beam-search. We propose an extension to the shift-reduce process to address this problem, which gives significant improvements to the parsing accuracies. Our method is conceptually simple, requiring only one additional transition action to eliminate size differences between different candidate outputs. On standard evaluat"
P13-1043,nivre-etal-2006-maltparser,0,0.0175741,"Missing"
P13-1043,N07-1051,0,0.0287651,"Missing"
P13-1043,W97-0301,0,0.268373,"Missing"
P13-1043,W05-1513,0,0.352838,"needs to disambiguate between transitions sequences with different sizes. For the same sentence, the largest output can take twice as many as actions to build as the 1 Introduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics smallest one. This turns out to have a significant empirical impact on parsing with beam-search. We propose an extension to the shift-reduce process to address this problem, which gives significant improvements to the p"
P13-1043,P11-2033,1,0.692073,"sing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). In addition, processing tens of sentences per second (Zhang and Nivre, 2011), these transition-based parsers can be a favorable choice for dependency parsing. Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the"
P13-1043,C12-1194,1,0.77715,"enda. With this new transition process, we experimented with several extended features,and found that the templates in Table 2 are useful to improve the accuracies further. Here si ll denotes the left child of si ’s left child. Other notations can be explained in a similar way. 4.2 Dependency Relations: Lexical Dependencies Lexical dependencies represent linguistic relations between words: whether a word modifies another word. The idea of exploiting lexical dependency information from auto-parsed data has been explored before for dependency parsing (Chen et al., 2009) and constituent parsing (Zhu et al., 2012). To extract lexical dependencies, we first run the baseline parser on unlabeled data. To simplify the extraction process, we can convert auto-parsed constituency trees into dependency trees by using Penn2Malt. 2 From the dependency trees, we extract bigram lexical dependencies hw1 , w2 , L/Ri where the symbol L (R) means that w1 (w2 ) is the head of w2 (w1 ). We also extract trigram lexical 4 Semi-supervised Parsing with Large Data This section discusses how to extract information from unlabeled data or auto-parsed data to further improve shift-reduce parsing accuracies. We consider three typ"
P13-1043,J03-4003,0,\N,Missing
P13-2063,P00-1056,0,0.149182,"(a) time vs. BLEU (b) hypo count vs. BLEU Figure 4: Translation quality comparison with the cube pruning baseline. 4 4.1 Experiments sign POS tags for both our training and test data. Setup 4.2 A Moses (Koehn et al., 2007) string-to-tree system is used as our baseline. The training corpus consists of the English-German sections of the Europarl (Koehn, 2005) and the News Commentary corpus. Discarding pairs without targetside parses, the final training data has 2M sentence pairs, with 54M and 52M words on the English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolkit (Stolcke and others, 2002) is used. The development and test sets are the 2008 WMT newstest (2,051 sentences) and 2009 WMT newstest (2,525 sentences) respectively. Feature weights are tuned with MERT (Och, 2003) on the development set and output is evaluated using case-sensitive BLEU (Papineni et al., 2002). For both rule extraction and decoding, up to seven terminal/non-terminal symbols on"
P13-2063,P96-1041,0,0.108328,"d test data. Setup 4.2 A Moses (Koehn et al., 2007) string-to-tree system is used as our baseline. The training corpus consists of the English-German sections of the Europarl (Koehn, 2005) and the News Commentary corpus. Discarding pairs without targetside parses, the final training data has 2M sentence pairs, with 54M and 52M words on the English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolkit (Stolcke and others, 2002) is used. The development and test sets are the 2008 WMT newstest (2,051 sentences) and 2009 WMT newstest (2,525 sentences) respectively. Feature weights are tuned with MERT (Och, 2003) on the development set and output is evaluated using case-sensitive BLEU (Papineni et al., 2002). For both rule extraction and decoding, up to seven terminal/non-terminal symbols on the source-side are allowed. For decoding, the maximum spanlength is restricted to 15, and the grammar is prefiltered to match the entire test set for both the baseline system"
P13-2063,P03-1021,0,0.0159682,"de parses, the final training data has 2M sentence pairs, with 54M and 52M words on the English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolkit (Stolcke and others, 2002) is used. The development and test sets are the 2008 WMT newstest (2,051 sentences) and 2009 WMT newstest (2,525 sentences) respectively. Feature weights are tuned with MERT (Och, 2003) on the development set and output is evaluated using case-sensitive BLEU (Papineni et al., 2002). For both rule extraction and decoding, up to seven terminal/non-terminal symbols on the source-side are allowed. For decoding, the maximum spanlength is restricted to 15, and the grammar is prefiltered to match the entire test set for both the baseline system and the chart pruning decoder. We use two labellers to perform b- and e-tag labelling independently prior to decoding. Training of the labelling models is able to complete in under 2.5 hours and the whole test set is labelled in under 2 seco"
P13-2063,J07-2003,0,0.0696048,"ranslate the same English sentence into Japanese (Figure 2a); unlike the English to German example, the English phrase “the products” will be a valid phrase that has a Japanese translation under a target constituent, since it is syntactically aligned to “製品” (Figure 2b). The key question to consider is how to inject target syntax and word alignment information into our labelling models, so that pruning decisions can be based on the source alone, we address this in the following two sections. The Baseline String-to-Tree Model Our baseline translation model uses the rule extraction algorithm of Chiang (2007) adapted to a string-to-tree grammar. After extracting phrasal pairs using the standard approach of Koehn et al. (2003), all pairs whose target phrases are not exhaustively dominated by a constituent of the parse tree are removed and each remaining pair, hf , ei, together with its constituent label, C, forms a lexical grammar rule: C → hf , ei. The rules r1 , r2 , and r3 in Figure 1 are lexical rules. Non-lexical rules are generated by eliminating one or more pairs of terminal substrings from an existing rule and substituting non-terminals. This process produces the example rules r4 and r5 . O"
P13-2063,P02-1040,0,0.0868433,"he English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolkit (Stolcke and others, 2002) is used. The development and test sets are the 2008 WMT newstest (2,051 sentences) and 2009 WMT newstest (2,525 sentences) respectively. Feature weights are tuned with MERT (Och, 2003) on the development set and output is evaluated using case-sensitive BLEU (Papineni et al., 2002). For both rule extraction and decoding, up to seven terminal/non-terminal symbols on the source-side are allowed. For decoding, the maximum spanlength is restricted to 15, and the grammar is prefiltered to match the entire test set for both the baseline system and the chart pruning decoder. We use two labellers to perform b- and e-tag labelling independently prior to decoding. Training of the labelling models is able to complete in under 2.5 hours and the whole test set is labelled in under 2 seconds. A standard perceptron POS tagger (Collins, 2002) trained on Wall Street Journal sections 2-2"
P13-2063,C04-1041,0,0.0318675,"ls will be pruned. The pruning effects of the two types of tags are illustrated in Figure 3. In general, 0-valued b-tags prune a whole column of chart cells and 0-valued e-tags prune a whole diagonal of cells; and the chart cells on the first row and the top-most cell are always kept so that complete translations can always be found. We build a separate labeller for each tag type using gold-standard b- and e-tags, respectively. We train the labellers with maximum-entropy models (Curran and Clark, 2003; Ratnaparkhi, 1996), using features similar to those used for suppertagging for CCG parsing (Clark and Curran, 2004). In each case, features for a pruning tag consist of word and POS uni-grams extracted from the 5word window with the current word in the middle, POS trigrams ending with the current word, as well as two previous tags as a bigram and two separate uni-grams. Our pruning labellers are highly efficient, run in linear time and add little overhead to decoding. During testing, in order to prevent overpruning, a probability cutoff value θ is used. A tag value of 0 is assigned to a word only if its marginal probability is greater than θ. 3.3 procedure C ONSISTENT(i, j, i0 , j 0 ) 12: t ← {A[k] |k ∈ [i"
P13-2063,D08-1012,0,0.605446,"Missing"
P13-2063,W02-1001,0,0.0304112,"valuated using case-sensitive BLEU (Papineni et al., 2002). For both rule extraction and decoding, up to seven terminal/non-terminal symbols on the source-side are allowed. For decoding, the maximum spanlength is restricted to 15, and the grammar is prefiltered to match the entire test set for both the baseline system and the chart pruning decoder. We use two labellers to perform b- and e-tag labelling independently prior to decoding. Training of the labelling models is able to complete in under 2.5 hours and the whole test set is labelled in under 2 seconds. A standard perceptron POS tagger (Collins, 2002) trained on Wall Street Journal sections 2-21 of the Penn Treebank is used to asResults Figures 4a and 4b compare CSP with the cube pruning baseline in terms of BLEU. Decoding speed is measured by the average decoding time and average number of hypotheses generated per sentence. We first run the baseline decoder under various beam settings (b = 100 - 2500) until no further increase in BLEU is observed. We then run the CSP decoder with a range of θ values (θ = 0.91 − 0.99), at the default beam size of 1000 of the baseline decoder. The CSP decoder, which considers far fewer chart cells and gener"
P13-2063,W96-0213,0,0.197145,"hrase. If either the b-tag or the e-tag of an input phrase is 0, the corresponding chart cells will be pruned. The pruning effects of the two types of tags are illustrated in Figure 3. In general, 0-valued b-tags prune a whole column of chart cells and 0-valued e-tags prune a whole diagonal of cells; and the chart cells on the first row and the top-most cell are always kept so that complete translations can always be found. We build a separate labeller for each tag type using gold-standard b- and e-tags, respectively. We train the labellers with maximum-entropy models (Curran and Clark, 2003; Ratnaparkhi, 1996), using features similar to those used for suppertagging for CCG parsing (Clark and Curran, 2004). In each case, features for a pruning tag consist of word and POS uni-grams extracted from the 5word window with the current word in the middle, POS trigrams ending with the current word, as well as two previous tags as a bigram and two separate uni-grams. Our pruning labellers are highly efficient, run in linear time and add little overhead to decoding. During testing, in order to prevent overpruning, a probability cutoff value θ is used. A tag value of 0 is assigned to a word only if its margina"
P13-2063,E03-1071,0,0.0161754,"rd cannot end a source phrase. If either the b-tag or the e-tag of an input phrase is 0, the corresponding chart cells will be pruned. The pruning effects of the two types of tags are illustrated in Figure 3. In general, 0-valued b-tags prune a whole column of chart cells and 0-valued e-tags prune a whole diagonal of cells; and the chart cells on the first row and the top-most cell are always kept so that complete translations can always be found. We build a separate labeller for each tag type using gold-standard b- and e-tags, respectively. We train the labellers with maximum-entropy models (Curran and Clark, 2003; Ratnaparkhi, 1996), using features similar to those used for suppertagging for CCG parsing (Clark and Curran, 2004). In each case, features for a pruning tag consist of word and POS uni-grams extracted from the 5word window with the current word in the middle, POS trigrams ending with the current word, as well as two previous tags as a bigram and two separate uni-grams. Our pruning labellers are highly efficient, run in linear time and add little overhead to decoding. During testing, in order to prevent overpruning, a probability cutoff value θ is used. A tag value of 0 is assigned to a word"
P13-2063,C08-1094,0,0.560688,"ses (i.e. any sequence of consecutive words) that are unlikely to have any consistently aligned target counterparts according to the source context and grammar constraints. We show that by using highly-efficient sequence labelling models learned from the bitext used for translation model training, such phrases can be effectively identified prior to MT decoding, and corresponding chart cells can be excluded for decoding without affecting translation quality. We call our method context-sensitive pruning (CSP); it can be viewed as a bilingual adaptation of similar methods in monolingual parsing (Roark and Hollingshead, 2008; Zhang et al., 2010) which improve parsing efficiency by “closing” chart cells using binary classifiers. Our contribution is that we demonstrate such methods can be applied to synchronous-grammar parsing by labelling the source-side alone. This is achieved through a novel training scheme where the labelling models are trained over the word-aligned bitext and gold-standard pruning labels are obtained by projecting target-side constituents to the source words. To our knowledge, this is the first work to apply this technique to MT decoding. The proposed method is easy to implement and effective"
P13-2063,N09-1026,0,0.086088,"nline n-gram language model integration and high grammar complexity. Various efforts have been devoted to improving decoding efficiency, including hypergraph rescoring (Heafield et al., 2013; Huang and Chiang, 2007), coarse-to-fine processing (Petrov et al., 2008; Zhang and Gildea, 2008) and grammar transformations (Zhang et al., 2006). For more expressive, linguistically-motivated syntactic MT models (Galley et al., 2004; Galley et al., 2006), the grammar complexity has grown considerably over hierarchical phrase-based models (Chiang, 2007), and decoding still suffers from efficiency issues (DeNero et al., 2009). In this paper, we study a chart pruning method for CKY-style MT decoding that is orthogonal to 352 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 352–357, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics TOP KON NP-OA denn PDAT PUNC. VVFIN NN NP-SB NN der NN NN Produkte 製品 (a) en-de but we need that reform process . KON NP - SB NP - OA → → → r4 r5 TOP S - TOP → → h but, denn i h we, wir i h that reform process, diesen Reformprozeß i h X1 . , S - TOP1 . i h but X1 need X2 , denn NP - OA2 brauchen NP - SB1 i 3.1"
P13-2063,N09-1073,0,0.0389563,"Missing"
P13-2063,N04-1035,0,0.137642,"Missing"
P13-2063,P06-1121,0,0.118792,"Missing"
P13-2063,P08-1025,0,0.0495614,"Missing"
P13-2063,N06-1033,0,0.0606546,"Missing"
P13-2063,N13-1116,1,0.859545,"Missing"
P13-2063,C10-2168,1,0.889886,"Missing"
P13-2063,D10-1063,0,0.0154531,"are not exhaustively dominated by a constituent of the parse tree are removed and each remaining pair, hf , ei, together with its constituent label, C, forms a lexical grammar rule: C → hf , ei. The rules r1 , r2 , and r3 in Figure 1 are lexical rules. Non-lexical rules are generated by eliminating one or more pairs of terminal substrings from an existing rule and substituting non-terminals. This process produces the example rules r4 and r5 . Our decoding algorithm is a variant of CKY and is similar to other algorithms tailored for specific syntactic translation grammars (DeNero et al., 2009; Hopkins and Langmead, 2010). By taking the source-side of each rule, projecting onto it the non-terminal labels from the target-side, and weighting the grammar according to the model’s local scoring features, decoding is a straightforward extension of monolingual weighted chart parsing. Non-local features, such as n-gram language model scores, are incorporated through cube pruning (Chiang, 2007). 3 の Figure 2: Two example alignments. In (a) “the products” does not have a consistent alignment on the target side, while it does in (b). Figure 1: A selection of grammar rules extractable from an example word-aligned sentence"
P13-2063,P07-1019,0,0.0456905,"Missing"
P13-2063,N03-1017,1,0.0756894,"hrase “the products” will be a valid phrase that has a Japanese translation under a target constituent, since it is syntactically aligned to “製品” (Figure 2b). The key question to consider is how to inject target syntax and word alignment information into our labelling models, so that pruning decisions can be based on the source alone, we address this in the following two sections. The Baseline String-to-Tree Model Our baseline translation model uses the rule extraction algorithm of Chiang (2007) adapted to a string-to-tree grammar. After extracting phrasal pairs using the standard approach of Koehn et al. (2003), all pairs whose target phrases are not exhaustively dominated by a constituent of the parse tree are removed and each remaining pair, hf , ei, together with its constituent label, C, forms a lexical grammar rule: C → hf , ei. The rules r1 , r2 , and r3 in Figure 1 are lexical rules. Non-lexical rules are generated by eliminating one or more pairs of terminal substrings from an existing rule and substituting non-terminals. This process produces the example rules r4 and r5 . Our decoding algorithm is a variant of CKY and is similar to other algorithms tailored for specific syntactic translatio"
P13-2063,P07-2045,1,0.0119064,"traction constraints, which is crucial for the success of our method. For each training sentence pair, gold-standard b-tags and e-tags are assigned separately to the 354 0.149 0.1485 0.1485 0.148 0.148 BLEU BLEU 0.149 0.1475 0.1475 0.147 0.147 0.1465 0.1465 csp cube pruning 0.146 0 2 4 6 csp cube pruning 0.146 8 10 12 14 16 18 20 0 0.5 1 1.5 2 2.5 6 CPU seconds/sentence Hypothesis Count (x10 ) (a) time vs. BLEU (b) hypo count vs. BLEU Figure 4: Translation quality comparison with the cube pruning baseline. 4 4.1 Experiments sign POS tags for both our training and test data. Setup 4.2 A Moses (Koehn et al., 2007) string-to-tree system is used as our baseline. The training corpus consists of the English-German sections of the Europarl (Koehn, 2005) and the News Commentary corpus. Discarding pairs without targetside parses, the final training data has 2M sentence pairs, with 54M and 52M words on the English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolki"
P13-2063,2005.mtsummit-papers.11,1,0.0850506,"ned separately to the 354 0.149 0.1485 0.1485 0.148 0.148 BLEU BLEU 0.149 0.1475 0.1475 0.147 0.147 0.1465 0.1465 csp cube pruning 0.146 0 2 4 6 csp cube pruning 0.146 8 10 12 14 16 18 20 0 0.5 1 1.5 2 2.5 6 CPU seconds/sentence Hypothesis Count (x10 ) (a) time vs. BLEU (b) hypo count vs. BLEU Figure 4: Translation quality comparison with the cube pruning baseline. 4 4.1 Experiments sign POS tags for both our training and test data. Setup 4.2 A Moses (Koehn et al., 2007) string-to-tree system is used as our baseline. The training corpus consists of the English-German sections of the Europarl (Koehn, 2005) and the News Commentary corpus. Discarding pairs without targetside parses, the final training data has 2M sentence pairs, with 54M and 52M words on the English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolkit (Stolcke and others, 2002) is used. The development and test sets are the 2008 WMT newstest (2,051 sentences) and 2009 WMT newstest (2,"
P13-2063,D08-1022,0,0.0468076,"Missing"
P14-1014,P07-1034,0,0.046321,"eeping the learned representations unchanged yields better performance compared with further optimizing them on the source domain data. We release our tools at https://github.com/majineu/TWeb. For future work, we would like to investigate the two-phase approach to more challenging tasks, such as web domain syntactic parsing. We believe that high-accuracy web domain taggers and parsers would benefit a wide range of downstream tasks such as machine translation2 . Besides learning representations, another line of research addresses domain-adaptation by instance re-weighting (Bickel et al., 2007; Jiang and Zhai, 2007) or feature re-weighting (Satpal and Sarawagi, 2007). Those methods assume that each example x that has a non-zero probability on the source domain must have a non-zero probability on the target domain, and vice-versa. As pointed out by Titov (2011), such an assumption is likely to be too restrictive since most NLP tasks adopt word-based or lexicon-based features that vary significantly across different domains. 8 Acknowledgements We would like to thank Hugo Larochelle for his advices on re-implementing WRRBM. We also thank Nan Yang, Shujie Liu and Tong Xiao for the fruitful discussions, and t"
P14-1014,W06-1615,0,0.663678,"classification, which might be of thousands of dimensions. Such high dimensional input gives rise to high computational cost and it is not clear whether those approaches can be applied to large scale unlabelled data, with hundreds of millions of training examples. Our method learns representations from only word ngrams with n ranging from 3 to 5, which can be easily applied to large scale-data. In addition, while Titov (2011) and Glorot et al. (2011) use the learned representation to improve cross-domain classification tasks, we are the first to apply it to cross-domain structured prediction. Blitzer et al. (2006) propose to induce shared representations for domain adaptation, which is based on the alternating structure optimization representations mainly comes from better accuracy of out-of-vocabulary (oov) words. By contrast, using n-gram representations improves the performance on both oov and non-oov. 5.4 Effect of Unlabelled Domain Data In some circumstances, we may know beforehand that the target domain data belongs to a certain sub-domain, such as the email domain. In such cases, it might be desirable to train WRRBM using data only on that domain. We conduct experiments to test whether using the"
P14-1014,W02-1001,0,0.0341955,"text, in our case). Our approach consists of two phrases. In the pre-training phase, we learn an encoder that con144 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 144–154, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics set, higher than those given by ensembled syntactic parsers. Our code will be publicly available at https://github.com/majineu/TWeb. 2 2.2 Most of the indicative features for POS disambiguation can be found from the words and word combinations within a local context (Ratnaparkhi, 1996; Collins, 2002). Inspired by this observation, we apply the RBM to learn feature representations from word n-grams. More specifically, given the ith word wi of a sentence, we apply RBMs to model the joint distribution of the n-gram (wi−l , · · · , wi+r ), where l and r denote the left and right window, respectively. Note that the visible units of RBMs are binary. While in our case, each visible variable corresponds to a word, which may take on tens-of-thousands of different values. Therefore, the RBM need to be re-factorized to make inference tractable. We utilize the Word Representation RBM (WRRBM) factoriz"
P14-1014,P13-2020,1,0.845517,"ction layer of web-feature module with the projection matrix of the learned WRRBM, or ngram-level representation, which corresponds to initializing both the projection and sigmoid layers of the webfeature module by the learned WRRBM. In each case, there can be two different training strategies depending on whether the learned representations are further adjusted or kept unchanged during the fine-turning phrase. Experimental results under the 4 combined settings on the development sets are illustrated in Figure 2, 3 and 4, where the Baseline We reimplemented the greedy easy-first POS tagger of Ma et al. (2013), which is used for all the experiments. While the tagger of Ma et al. (2013) utilizes a linear scorer, our tagger adopts the neural network as its scorer. The neural network of our baseline tagger only contains the sparse-feature module. We use this baseline to examine the performance of a tagger trained purely on the source domain. Feature templates are shown in Table 3, 149 method baseline word-adjust word-fix ngram-adjust ngram-fix WSJ word-fixed word-adjust ngram-fixed ngram-adjust Accuracy 96.9 96.8 all 89.81 +0.09 +0.11 +0.53 +0.69 non-oov 92.42 −0.05 +0.13 +0.52 +0.60 oov 65.64 +1.38 +"
P14-1014,N10-1115,0,0.0143344,"back-propagation updates can be used to update the model’s parameters (line 8 ∼ line 11). For the special case where w ˆ and w do refer to the same word w, it can be easily verified that the two separate back-propagation updates equal to the standard back-propagation with a loss 1 + nn(w, t) − nn(w, tˆ) on the input hwi. The algorithm proposed here belongs to a general framework named guided learning, where search and learning interact with each other. The algorithm learns not only a local classifier, but also the inference order. While previous work (Shen et al., 2007; Zhang and Clark, 2011; Goldberg and Elhadad, 2010) apply guided learning to train a linear classifier by using variants of the perceptron algorithm, we are the first to combine guided learning with a neural network, by using a margin loss and a modified back-propagation algorithm. 5 5.1 about labelled and unlabelled data are summarized in Table 1 and Table 2, respectively. The raw web domain data contains much noise, including spelling error, emotions and inconsistent capitalization. Following some participants (Le Roux et al., 2012), we conduct simple preprocessing steps to the input of the development and the test sets1 • Neutral quotes are"
P14-1014,P06-1055,0,0.0670542,"Missing"
P14-1014,E09-3005,0,0.0237759,"t shows accuracies of the official baseline and that of the top 2 participants. training set likelihood, while our approach is to minimize the margin loss using guided learning. (ASO) method of Ando and Zhang (2005). The idea is to project the original feature representations into low dimensional representations, which yields a high-accuracy classifier on the target domain. The new representations are induced based on the auxiliary tasks defined on unlabelled data together with a dimensionality reduction technique. Such auxiliary tasks can be specific to the supervised task. As pointed out by Plank (2009), for many NLP tasks, defining the auxiliary tasks is a non-trivial engineering problem. Compared with Blitzer et al. (2006), the advantage of using RBMs is that it learns representations in a pure unsupervised manner, which is much simpler. 7 Conclusion We built a web-domain POS tagger using a two-phase approach. We used a WRRBM to learn the representation of the web text and incorporate the representation in a neural network, which is trained using guided learning for easy-first POS tagging. Experiment showed that our approach achieved significant improvement in tagging the web domain text."
P14-1014,D11-1106,1,0.756249,"this way, two separate back-propagation updates can be used to update the model’s parameters (line 8 ∼ line 11). For the special case where w ˆ and w do refer to the same word w, it can be easily verified that the two separate back-propagation updates equal to the standard back-propagation with a loss 1 + nn(w, t) − nn(w, tˆ) on the input hwi. The algorithm proposed here belongs to a general framework named guided learning, where search and learning interact with each other. The algorithm learns not only a local classifier, but also the inference order. While previous work (Shen et al., 2007; Zhang and Clark, 2011; Goldberg and Elhadad, 2010) apply guided learning to train a linear classifier by using variants of the perceptron algorithm, we are the first to combine guided learning with a neural network, by using a margin loss and a modified back-propagation algorithm. 5 5.1 about labelled and unlabelled data are summarized in Table 1 and Table 2, respectively. The raw web domain data contains much noise, including spelling error, emotions and inconsistent capitalization. Following some participants (Le Roux et al., 2012), we conduct simple preprocessing steps to the input of the development and the te"
P14-1014,W96-0213,0,0.875293,"aw input data (web text, in our case). Our approach consists of two phrases. In the pre-training phase, we learn an encoder that con144 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 144–154, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics set, higher than those given by ensembled syntactic parsers. Our code will be publicly available at https://github.com/majineu/TWeb. 2 2.2 Most of the indicative features for POS disambiguation can be found from the words and word combinations within a local context (Ratnaparkhi, 1996; Collins, 2002). Inspired by this observation, we apply the RBM to learn feature representations from word n-grams. More specifically, given the ith word wi of a sentence, we apply RBMs to model the joint distribution of the n-gram (wi−l , · · · , wi+r ), where l and r denote the left and right window, respectively. Note that the visible units of RBMs are binary. While in our case, each visible variable corresponds to a word, which may take on tens-of-thousands of different values. Therefore, the RBM need to be re-factorized to make inference tractable. We utilize the Word Representation RBM"
P14-1014,D13-1061,0,0.0358404,"resources in data cleaning. Table 5: Effect of unlabelled data. “+acc” denotes improvement in tagging accuracy and “cov” denotes the lexicon coverages. 6 Learning representations has been intensively studied in computer vision tasks (Bengio et al., 2007; Lee et al., 2009a). In NLP, there is also much work along this line. In particular, Collobert et al. (2011) and Turian et al. (2010) learn word embeddings to improve the performance of in-domain POS tagging, named entity recognition, chunking and semantic role labelling. Yang et al. (2013) induce bi-lingual word embeddings for word alignment. Zheng et al. (2013) investigate Chinese character embeddings for joint word segmentation and POS tagging. While those approaches mainly explore token-level representations (word or character embeddings), using WRRBM is able to utilize both word and n-gram representations. Titov (2011) and Glorot et al. (2011) propose to learn representations from the mixture of both source and target domain unlabelled data to improve cross-domain sentiment classification. Titov (2011) also propose a regularizer to constrain the inter-domain variability. In particular, their regularizer aims to minimize the Kullback-Leibler (KL)"
P14-1014,P07-1096,0,0.0826462,"niversity, China ‡ Singapore University of Technology and Design majineu@gmail.com yue zhang@sutd.edu.sg zhujingbo@mail.neu.edu.cn Abstract verts the web text into an intermediate representation, which acts as useful features for prediction tasks. We integrate the learned encoder with a set of well-established features for POS tagging (Ratnaparkhi, 1996; Collins, 2002) in a single neural network, which is applied as a scorer to an easyfirst POS tagger. We choose the easy-first tagging approach since it has been demonstrated to give higher accuracies than the standard left-to-right POS tagger (Shen et al., 2007; Ma et al., 2013). In the fine-tuning phase, the parameters of the network are optimized on a set of labelled training data using guided learning. The learned model preserves the property of preferring to tag easy words first. To our knowledge, we are the first to investigate guided learning for neural networks. The idea of learning representations from unlabelled data and then fine-tuning a model with such representations according to some supervised criterion has been studied before (Turian et al., 2010; Collobert et al., 2011; Glorot et al., 2011). While most previous work focus on in-doma"
P14-1014,P11-1007,0,0.0804275,". In NLP, there is also much work along this line. In particular, Collobert et al. (2011) and Turian et al. (2010) learn word embeddings to improve the performance of in-domain POS tagging, named entity recognition, chunking and semantic role labelling. Yang et al. (2013) induce bi-lingual word embeddings for word alignment. Zheng et al. (2013) investigate Chinese character embeddings for joint word segmentation and POS tagging. While those approaches mainly explore token-level representations (word or character embeddings), using WRRBM is able to utilize both word and n-gram representations. Titov (2011) and Glorot et al. (2011) propose to learn representations from the mixture of both source and target domain unlabelled data to improve cross-domain sentiment classification. Titov (2011) also propose a regularizer to constrain the inter-domain variability. In particular, their regularizer aims to minimize the Kullback-Leibler (KL) distance between the marginal distributions of the learned representations on the source and target domains. Their work differs from ours in that their approaches learn representations from the feature vectors for sentiment classification, which might be of thousand"
P14-1014,P10-1040,0,0.653482,"as been demonstrated to give higher accuracies than the standard left-to-right POS tagger (Shen et al., 2007; Ma et al., 2013). In the fine-tuning phase, the parameters of the network are optimized on a set of labelled training data using guided learning. The learned model preserves the property of preferring to tag easy words first. To our knowledge, we are the first to investigate guided learning for neural networks. The idea of learning representations from unlabelled data and then fine-tuning a model with such representations according to some supervised criterion has been studied before (Turian et al., 2010; Collobert et al., 2011; Glorot et al., 2011). While most previous work focus on in-domain sequential labelling or cross-domain classification tasks, we are the first to learn representations for web-domain structured prediction. Previous work treats the learned representations either as model parameters that are further optimized in supervised fine-tuning (Collobert et al., 2011) or as fixed features that are kept unchanged (Turian et al., 2010; Glorot et al., 2011). In this work, we investigate both strategies and give empirical comparisons in the cross-domain setting. Our results suggest t"
P14-1014,I13-1183,0,0.109622,"ing (Hinton et al., 2006; Bengio et al., 2007), DBNs are capable of modelling higher order non-linear relations between the input, and has been demonstrated to improve performance for many computer vision tasks (Hinton et al., 2006; Bengio et al., 2007; Lee et al., 2009a). However, in this work we do not observe further improvement by employing DBNs. This may partly be due to the fact that unlike computer vision tasks, the input structure of POS tagging or other sequential labelling tasks is relatively simple, and a single non-linear layer is enough to model the interactions within the input (Wang and Manning, 2013). 3 Figure 1: The proposed neural network. The webfeature module (lower left) and sparse-feature module (lower right) are combined by a shared output layer (upper). is identical to the training data of the pre-trained WRRBM. The first layer is a linear projection layer, where each word in the input is projected into a Ddimensional real value vector using the projection operation described in Section 2.2. The output of this layer o1w is the concatenation of the projections of wi−l , . . . , wi+r :  1  Mw wi−l   .. o1w =  (8)  . 1 Mw wi+r Neural Network for POS Disambiguation We integrate"
P14-1014,N07-1051,0,\N,Missing
P14-1014,P05-1001,0,\N,Missing
P14-1014,P13-1017,0,\N,Missing
P14-1021,P11-1048,0,0.432956,"Missing"
P14-1021,P06-2006,0,0.0360593,"provides an elegant solution to the spurious ambiguity problem (Clark and Curran, 2007). Second, obtaining training data for dependencies is likely to be easier than for syntactic derivations, especially for incomplete data (Schneider et al., 2013). Clark and Curran (2006) show how the dependency model from Clark and Curran (2007) extends naturally to the partialtraining case, and also how to obtain dependency data cheaply from gold-standard lexical category sequences alone. And third, it has been argued that dependencies are an ideal representation for parser evaluation, especially for CCG (Briscoe and Carroll, 2006; Clark and Hockenmaier, 2002), and so optimizing for dependency recovery makes sense from an evaluation perspective. In this paper, we fill a gap in the literature by developing the first dependency model for a shiftreduce CCG parser. Shift-reduce parsing applies naturally to CCG (Zhang and Clark, 2011), and the left-to-right, incremental nature of the decoding fits with CCG’s cognitive claims. The discriminative model is global and trained with the structured perceptron. The decoder is based on beam-search This paper presents the first dependency model for a shift-reduce CCG parser. Modellin"
P14-1021,N06-1019,1,0.718846,"plying constraints on grammaticality, is the semantic interpretation. The early dependency model of Clark et al. (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. More generally, dependency models are desirable for a number of reasons. First, modelling dependencies provides an elegant solution to the spurious ambiguity problem (Clark and Curran, 2007). Second, obtaining training data for dependencies is likely to be easier than for syntactic derivations, especially for incomplete data (Schneider et al., 2013). Clark and Curran (2006) show how the dependency model from Clark and Curran (2007) extends naturally to the partialtraining case, and also how to obtain dependency data cheaply from gold-standard lexical category sequences alone. And third, it has been argued that dependencies are an ideal representation for parser evaluation, especially for CCG (Briscoe and Carroll, 2006; Clark and Hockenmaier, 2002), and so optimizing for dependency recovery makes sense from an evaluation perspective. In this paper, we fill a gap in the literature by developing the first dependency model for a shiftreduce CCG parser. Shift-reduce"
P14-1021,N12-1015,0,0.0171276,"e arises from the potentially exponential number of derivations leading to a gold-standard dependency structure, which the oracle needs to keep track of. Our solution is an integration of a packed parse forest, which efficiently stores all the derivations, with the beam-search decoder at training time. The derivations are not explicitly part of the data, since the forest is built from the gold-standard dependencies. We also show how perceptron learning with beam-search (Collins and Roark, 2004) can be extended to handle the additional ambiguity, by adapting the “violationfixing” perceptron of Huang et al. (2012). Results on the standard CCGBank tests show that our parser achieves absolute labeled F-score gains of up to 0.5 over the shift-reduce parser of Zhang and Clark (2011); and up to 1.05 and 0.64 over the normal-form and hybrid models of Clark and Curran (2007), respectively. 2 0 1 2 3 4 5 6 7 8 9 stack (sn , ..., s1 , s0 ) N /N N /N N N NP NP (S [dcl]NP)/NP NP (S [dcl]NP)/NP N NP (S [dcl]NP)/NP NP NP S [dcl]NP S [dcl] queue (q0 , q1 , ..., qm ) Mr. President visited Paris President visited Paris visited Paris visited Paris visited Paris Paris action SHIFT SHIFT REDUCE UNARY SHIFT SHIFT UNAR"
P14-1021,J07-4004,1,0.0940301,"ry of CCG, in which Steedman (2000) argues that the derivation is merely a “trace” of the underlying syntactic process, and that the structure which is built, and predicated over when applying constraints on grammaticality, is the semantic interpretation. The early dependency model of Clark et al. (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. More generally, dependency models are desirable for a number of reasons. First, modelling dependencies provides an elegant solution to the spurious ambiguity problem (Clark and Curran, 2007). Second, obtaining training data for dependencies is likely to be easier than for syntactic derivations, especially for incomplete data (Schneider et al., 2013). Clark and Curran (2006) show how the dependency model from Clark and Curran (2007) extends naturally to the partialtraining case, and also how to obtain dependency data cheaply from gold-standard lexical category sequences alone. And third, it has been argued that dependencies are an ideal representation for parser evaluation, especially for CCG (Briscoe and Carroll, 2006; Clark and Hockenmaier, 2002), and so optimizing for dependenc"
P14-1021,P08-1108,0,0.0223216,"en tracing out a single, correct derivation; then we describe how a model of normal-form derivations — or, more accurately, a sequence of shift-reduce actions leading to a normal-form derivation — can be used with beam-search to develop a nondeterministic parser which selects the highest scoring sequence of actions. Note this section only describes a normal-form derivation model for shiftreduce parsing. Section 3 explains how we extend the approach to dependency models. The shift-reduce algorithm adapted to CCG is similar to that of shift-reduce dependency parsing (Yamada and Matsumoto, 2003; Nivre and McDonald, 2008; Zhang and Clark, 2008; Huang and Sagae, 2010). Following Zhang and Clark (2011), we define each item in the parser as a pair hs, qi, where q is a queue of remaining input, consisting of words and a set of possible lexical categories for each word (with q0 being the front word), and s is the stack that holds subtrees s0 , s1 , ... (with s0 at the top). Subtrees on the stack are partial deriva1 See Hockenmaier (2003) and Clark and Curran (2007) for a description of CCG rules. 219 Mr. President visited Paris N /N N (S [dcl ]NP )/NP NP N NP &gt; S [dcl ]NP &gt;TC Mr. N /N President visited Paris N ("
P14-1021,P02-1042,1,0.258244,"Computer Laboratory sc609@cam.ac.uk Abstract and Curran, 2007) is to model derivations directly, restricting the gold-standard to be the normal-form derivations (Eisner, 1996) from CCGBank (Hockenmaier and Steedman, 2007). Modelling dependencies, as a proxy for the semantic interpretation, fits well with the theory of CCG, in which Steedman (2000) argues that the derivation is merely a “trace” of the underlying syntactic process, and that the structure which is built, and predicated over when applying constraints on grammaticality, is the semantic interpretation. The early dependency model of Clark et al. (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. More generally, dependency models are desirable for a number of reasons. First, modelling dependencies provides an elegant solution to the spurious ambiguity problem (Clark and Curran, 2007). Second, obtaining training data for dependencies is likely to be easier than for syntactic derivations, especially for incomplete data (Schneider et al., 2013). Clark and Curran (2006) show how the dependency model from Clark and Curran (2007) extends naturally to the partialtrai"
P14-1021,C04-1010,0,0.0382713,"nitial item hs, qi0 (row 0), which has an empty stack and a full queue, a total of nine actions are applied to produce the complete derivation. Applying beam-search to a statistical shiftreduce parser is a straightforward extension to the deterministic example. At each step, a beam is used to store the top-k highest-scoring items, resulting from expanding all items in the previous beam. An item becomes a candidate output once it has an empty queue, and the parser keeps track of the highest scored candidate output and returns the best one as the final output. Compared with greedy local-search (Nivre and Scholz, 2004), the use of a beam allows the parser to explore a larger search space and delay difficult ambiguity-resolving decisions by considering multiple items in parallel. We refer to the shift-reduce model of Zhang and Clark (2011) as the normal-form model, where the oracle for each sentence specifies a unique sequence of gold-standard actions which produces the corresponding normal-form derivation. No dependency structures are involved at training and test time, except for evaluation. In the next section, we describe a dependency oracle which considers all sequences of actions producing a goldstanda"
P14-1021,P04-1015,0,0.624603,"paper is a novel technique for training the parser using a dependency oracle, in which all derivations are hidden. A challenge arises from the potentially exponential number of derivations leading to a gold-standard dependency structure, which the oracle needs to keep track of. Our solution is an integration of a packed parse forest, which efficiently stores all the derivations, with the beam-search decoder at training time. The derivations are not explicitly part of the data, since the forest is built from the gold-standard dependencies. We also show how perceptron learning with beam-search (Collins and Roark, 2004) can be extended to handle the additional ambiguity, by adapting the “violationfixing” perceptron of Huang et al. (2012). Results on the standard CCGBank tests show that our parser achieves absolute labeled F-score gains of up to 0.5 over the shift-reduce parser of Zhang and Clark (2011); and up to 1.05 and 0.64 over the normal-form and hybrid models of Clark and Curran (2007), respectively. 2 0 1 2 3 4 5 6 7 8 9 stack (sn , ..., s1 , s0 ) N /N N /N N N NP NP (S [dcl]NP)/NP NP (S [dcl]NP)/NP N NP (S [dcl]NP)/NP NP NP S [dcl]NP S [dcl] queue (q0 , q1 , ..., qm ) Mr. President visited Paris"
P14-1021,C10-1094,0,0.00918542,"over three existing, competitive CCG parsing models. 1 Yue Zhang Singapore University of Technology and Design yue zhang@sutd.edu.sg Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is able to derive typed dependency structures (Hockenmaier, 2003; Clark and Curran, 2007), providing a useful approximation to the underlying predicate-argument relations of “who did what to whom”. To date, CCG remains the most competitive formalism for recovering “deep” dependencies arising from many linguistic phenomena such as raising, control, extraction and coordination (Rimell et al., 2009; Nivre et al., 2010). To achieve its expressiveness, CCG exhibits so-called “spurious” ambiguity, permitting many non-standard surface derivations which ease the recovery of certain dependencies, especially those arising from type-raising and composition. But this raises the question of what is the most suitable model for CCG: should we model the derivations, the dependencies, or both? The choice for some existing parsers (Hockenmaier, 2003; Clark 218 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 218–227, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Associatio"
P14-1021,W02-1001,0,0.422492,"if there is a disjunctive node d ∈ D(c) s.t. clex0 ∈ γ(d). With this implementation, the complexity of checking each valid SHIFT action is then O(|pL (cs0 )|). Definition 7. Let hs, qi be an item and let hs0 , q 0 i = hs, qi ◦ (x, c). We define the shared ancestor set R(cs01 , cs00 ) of cs00 , after applying action (x, c), as: • {c0 |c0 ∈ pL (cs0 ) ∩ A(c)}, if s is frontier and x = 3.3 SHIFT 0 0 • {c |c ∈ pL (cs0 ) ∩ A(c) and there is some c ∈ R(cs1 , cs0 ) s.t. c00 ∈ A(c0 )}, if s is non-frontier and x = SHIFT • {c0 |c0 ∈ R(cs2 , cs1 ) ∩ A(c)}, if x = Training We use the averaged perceptron (Collins, 2002) to train a global linear model and score each action. The normal-form model of Zhang and Clark (2011) uses an early update mechanism (Collins and Roark, 2004), where decoding is stopped to update model weights whenever the single gold action falls outside the beam. In our parser, there can be multiple gold items in a beam. One option would be to apply early update whenever at least 00 REDUCE • {c0 |c0 ∈ R(cs1 , cs0 ) ∩ A(c)}, if s is non-frontier and x = UNARY • R(, c0s0 ) = ∅ where c0s0 is the conjunctive node corresponding to the gold-standard lexical category of the 223 Algorithm 3 Depend"
P14-1021,D09-1085,1,0.84161,"F-score improvements over three existing, competitive CCG parsing models. 1 Yue Zhang Singapore University of Technology and Design yue zhang@sutd.edu.sg Introduction Combinatory Categorial Grammar (CCG; Steedman (2000)) is able to derive typed dependency structures (Hockenmaier, 2003; Clark and Curran, 2007), providing a useful approximation to the underlying predicate-argument relations of “who did what to whom”. To date, CCG remains the most competitive formalism for recovering “deep” dependencies arising from many linguistic phenomena such as raising, control, extraction and coordination (Rimell et al., 2009; Nivre et al., 2010). To achieve its expressiveness, CCG exhibits so-called “spurious” ambiguity, permitting many non-standard surface derivations which ease the recovery of certain dependencies, especially those arising from type-raising and composition. But this raises the question of what is the most suitable model for CCG: should we model the derivations, the dependencies, or both? The choice for some existing parsers (Hockenmaier, 2003; Clark 218 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 218–227, c Baltimore, Maryland, USA, June 23-25"
P14-1021,P96-1011,0,0.436747,"Missing"
P14-1021,W13-2307,0,0.038113,"Missing"
P14-1021,P10-1035,0,0.130723,"t our shift-reduce parser on top of the core C & C code base (Clark and Curran, 2007) and evaluate it against the shift-reduce parser of Zhang and Clark (2011) (henceforth Z & C) and the chartbased normal-form and hybrid models of Clark and Curran (2007). For all experiments, we use CCGBank with the standard split: sections 2-21 for training (39,604 sentences), section 00 for development (1,913 sentences) and section 23 (2,407 sentences) for testing. The way that the CCG grammar is implemented in C & C has some implications for our parser. First, unlike Z & C, which uses a context-free cover (Fowler and Penn, 2010) and hence is able to use all sentences in the training data, we are only able to use 36,036 sentences. The reason is that the grammar in C & C does not have complete coverage of CCGBank, due to the fact that e.g. not all rules in CCGBank conform to the combinatory rules of CCG. Second, our parser uses the unification mechanism from C & C to output dependencies directly, and hence does not need a separate postprocessing step to convert derivations into CCG dependencies, as required by Z & C. The feature templates of our model consist of all of those in Z & C, except the ones which require lexi"
P14-1021,W03-3023,0,0.090749,"ich a parser would follow when tracing out a single, correct derivation; then we describe how a model of normal-form derivations — or, more accurately, a sequence of shift-reduce actions leading to a normal-form derivation — can be used with beam-search to develop a nondeterministic parser which selects the highest scoring sequence of actions. Note this section only describes a normal-form derivation model for shiftreduce parsing. Section 3 explains how we extend the approach to dependency models. The shift-reduce algorithm adapted to CCG is similar to that of shift-reduce dependency parsing (Yamada and Matsumoto, 2003; Nivre and McDonald, 2008; Zhang and Clark, 2008; Huang and Sagae, 2010). Following Zhang and Clark (2011), we define each item in the parser as a pair hs, qi, where q is a queue of remaining input, consisting of words and a set of possible lexical categories for each word (with q0 being the front word), and s is the stack that holds subtrees s0 , s1 , ... (with s0 at the top). Subtrees on the stack are partial deriva1 See Hockenmaier (2003) and Clark and Curran (2007) for a description of CCG rules. 219 Mr. President visited Paris N /N N (S [dcl ]NP )/NP NP N NP &gt; S [dcl ]NP &gt;TC Mr. N /N P"
P14-1021,C12-1059,0,0.0227654,"ts all derivations as hidden, and defines a probabilistic model for a dependency structure by summing probabilities of all derivations leading to a particular structure. Features are defined over both derivations and CCG predicate-argument dependencies. We follow a similar approach, but rather than define a probabilistic model (which requires summing), we define a linear model over sequences of shiftreduce actions, as for the normal-form shift-reduce model. However, the difference compared to the normal-form model is that we do not assume a single gold-standard sequence of actions. Similar to Goldberg and Nivre (2012), we define an oracle which determines, for a goldstandard dependency structure, G, what the valid transition sequences are (i.e. those sequences corresponding to derivations leading to G). More specifically, the oracle can determine, given G and an item hs, qi, what the valid actions are for that item (i.e. what actions can potentially lead to G, starting with hs, qi and the dependencies already built on s). However, there can be exponentially many valid action sequences for G, which we represent efficiently using a packed parse forest. We show how the forest can be used, during beamsearch de"
P14-1021,D13-1112,0,0.0389206,"Missing"
P14-1021,P13-2111,0,0.0183734,"d surface derivations which ease the recovery of certain dependencies, especially those arising from type-raising and composition. But this raises the question of what is the most suitable model for CCG: should we model the derivations, the dependencies, or both? The choice for some existing parsers (Hockenmaier, 2003; Clark 218 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 218–227, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics step (Zhang and Clark, 2008) with the advantage of linear-time decoding (Goldberg et al., 2013). A main contribution of the paper is a novel technique for training the parser using a dependency oracle, in which all derivations are hidden. A challenge arises from the potentially exponential number of derivations leading to a gold-standard dependency structure, which the oracle needs to keep track of. Our solution is an integration of a packed parse forest, which efficiently stores all the derivations, with the beam-search decoder at training time. The derivations are not explicitly part of the data, since the forest is built from the gold-standard dependencies. We also show how perceptro"
P14-1021,D08-1059,1,0.142872,"xhibits so-called “spurious” ambiguity, permitting many non-standard surface derivations which ease the recovery of certain dependencies, especially those arising from type-raising and composition. But this raises the question of what is the most suitable model for CCG: should we model the derivations, the dependencies, or both? The choice for some existing parsers (Hockenmaier, 2003; Clark 218 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 218–227, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics step (Zhang and Clark, 2008) with the advantage of linear-time decoding (Goldberg et al., 2013). A main contribution of the paper is a novel technique for training the parser using a dependency oracle, in which all derivations are hidden. A challenge arises from the potentially exponential number of derivations leading to a gold-standard dependency structure, which the oracle needs to keep track of. Our solution is an integration of a packed parse forest, which efficiently stores all the derivations, with the beam-search decoder at training time. The derivations are not explicitly part of the data, since the forest is bu"
P14-1021,J07-3004,0,0.14906,"Missing"
P14-1021,P11-1069,1,0.337827,"m Clark and Curran (2007) extends naturally to the partialtraining case, and also how to obtain dependency data cheaply from gold-standard lexical category sequences alone. And third, it has been argued that dependencies are an ideal representation for parser evaluation, especially for CCG (Briscoe and Carroll, 2006; Clark and Hockenmaier, 2002), and so optimizing for dependency recovery makes sense from an evaluation perspective. In this paper, we fill a gap in the literature by developing the first dependency model for a shiftreduce CCG parser. Shift-reduce parsing applies naturally to CCG (Zhang and Clark, 2011), and the left-to-right, incremental nature of the decoding fits with CCG’s cognitive claims. The discriminative model is global and trained with the structured perceptron. The decoder is based on beam-search This paper presents the first dependency model for a shift-reduce CCG parser. Modelling dependencies is desirable for a number of reasons, including handling the “spurious” ambiguity of CCG; fitting well with the theory of CCG; and optimizing for structures which are evaluated at test time. We develop a novel training technique using a dependency oracle, in which all derivations are hidde"
P14-1021,W05-1506,0,0.0337318,"N /N , sh N , re N , un NP , sh (S [dcl ]NP )/NP , sh NP , re S [dcl ]NP , re S [dcl ].3 The order of traversal is left-child, right-child and parent. For a single parse, the corresponding shift-reduce action sequence is unique, and for a given item this canonical order restricts the possible derivations that can be formed using further actions. We now extend this observation to the more general case of an oracle forest, where there may be more than one gold-standard action for a given item. Definition 1. Given a gold-standard dependency 2 Under the hypergraph framework (Gallo et al., 1993; Huang and Chiang, 2005), a conjunctive node corresponds to a hyperedge and a disjunctive node corresponds to the head of a hyperedge or hyperedge bundle. 3 The derivation is “upside down”, following the convention used for CCG, where the root is S [dcl ]. We use sh, re and un to denote the three types of shift-reduce action. 221 Mr. President visited Paris N/N N (S [dcl ]NP )/NP NP Mr. President visited Paris N /N N (S [dcl ]NP )/NP NP N &gt; S[dcl]NP &gt; S[dcl]NP (a) &gt; (b) Figure 3: Example subtrees on two stacks, with two subtrees in (a) and three in (b); roots of subtrees are in bold. It is trivial to determine th"
P14-1021,P10-1110,0,\N,Missing
P14-1125,D12-1133,0,0.10455,"d of “大法官 (chief lawyer)” is “法官 (lawyer)”, and the smallest right subword 1327 大 big 法 law 官 officer (a) smallest left subword 合 agree with 法 law 化 ize (b) smallest right subword Figure 2: An example to illustrate the innermost left/right subwords. of “合法化 (legalize)” is “合法 (legal)”. 3 Character-Level Dependency Parsing A transition-based framework with global learning and beam search decoding (Zhang and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhan"
P14-1125,W06-2925,0,0.0254782,"Missing"
P14-1125,W08-0336,0,0.0274826,"flexible segmentation standards using this formalism. In Figure 1(c), for example, “副局长 (deputy 1326 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326–1336, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics director)” can be segmented as both “副 (deputy) |局 长 (director)” and “副 局 长 (deputy director)”, but not “副 (deputy) 局 (office) |长 (manager)”, by dependency coherence. Chinese language processing tasks, such as machine translation, can benefit from flexible segmentation standards (Zhang et al., 2008; Chang et al., 2008). Second, word internal structures can also be useful for syntactic parsing. Zhang et al. (2013) have shown the usefulness of word structures in Chinese constituent parsing. Their results on the Chinese Treebank (CTB) showed that characterlevel constituent parsing can bring increased performances even with the pseudo word structures. They further showed that better performances can be achieved when manually annotated word structures are used instead of pseudo structures. In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al. (2013)’s annotation"
P14-1125,W09-2307,0,0.0174117,"ting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependency tree Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 ("
P14-1125,D09-1127,0,0.015966,"Missing"
P14-1125,P10-1001,0,0.0106105,"ake speech (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows"
P14-1125,D12-1132,0,0.15249,"his work, we extend their formulation, making use of largescale annotations of Zhang et al. (2013), so that the syntactic word-level dependencies can be parsed together with intra-word dependencies. Hatori et al. (2012) proposed a joint model for Chinese word segmentation, POS-tagging and dependency parsing, studying the influence of joint model and character features for parsing, Their model is extended from the arc-standard transition-based model, and can be regarded as an alternative to the arc-standard model of our work when pseudo intra-word dependencies are used. Similar work is done by Li and Zhou (2012). Our proposed arc-standard model is more concise while obtaining better performance than Hatori et al. (2012)’s work. With respect to word structures, real intra-word dependencies are often more complicated, while pseudo word structures cannot be used to correctly guide segmentation. Zhao (2009), Hatori et al. (2012) and our work all study character-level dependency parsing. While Zhao (2009) focus on word internal structures using pseudo inter-word dependencies, Hatori et al. (2012) investigate a joint model using pseudo intra-word dependencies. We use manual dependencies for both inner- and"
P14-1125,P12-1071,1,0.880086,"(c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependency tree Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 (The deputy directo"
P14-1125,P11-1141,0,0.0156388,"d increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependency tree Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 (The deputy director of forestry administration make a speech in the meeting)”. a character-level dependency tree, where the leaf nodes are Chinese characters. Character-level dependency parsing is interesting in at least two aspects. First, character-level t"
P14-1125,P13-1104,0,0.0118312,"Missing"
P14-1125,P05-1012,0,0.0232684,"make a speech 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on"
P14-1125,P04-1015,0,0.0960443,"011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhang and Clark, 2008) or the arc-standard (Huang et al., 2009) transition systems. When the internal structures of words are annotated, character-level dependency parsing can be treated as a special case of word-level dependency parsing, with “words” being “characters”. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to well-established features fo"
P14-1125,P05-1013,0,0.00967053,"长 会 上 发 言 woods industry office deputy office manager meeting in make speech (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, bui"
P14-1125,W02-1001,0,0.090003,"ng and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhang and Clark, 2008) or the arc-standard (Huang et al., 2009) transition systems. When the internal structures of words are annotated, character-level dependency parsing can be treated as a special case of word-level dependency parsing, with “words” being “characters”. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to we"
P14-1125,J08-4003,0,0.437215,"ee by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author."
P14-1125,tsarfaty-goldberg-2008-word,0,0.311687,"el dependency parsing. While Zhao (2009) focus on word internal structures using pseudo inter-word dependencies, Hatori et al. (2012) investigate a joint model using pseudo intra-word dependencies. We use manual dependencies for both inner- and inter-word structures, studying their influences on each other. Zhang et al. (2013) was the first to perform Chinese syntactic parsing over characters. They extended word-level constituent trees by annotated word structures, and proposed a transition-based approach to parse intra-word structures and wordlevel constituent structures jointly. For Hebrew, Tsarfaty and Goldberg (2008) investigated joint segmentation and parsing over characters using a graph-based method. Our work is similar in exploiting character-level syntax. We study the dependency grammar, another popular syntactic representation, and propose two novel transition systems for character-level dependency parsing. Nivre (2008) gave a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing. We extend both algorithms to character-level joint word segmentation, POS-tagging and dependency parsing. To our knowl"
P14-1125,I05-3017,0,0.0666397,"ee Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 (The deputy director of forestry administration make a speech in the meeting)”. a character-level dependency tree, where the leaf nodes are Chinese characters. Character-level dependency parsing is interesting in at least two aspects. First, character-level trees circumvent the issue that no universal standard exists for Chinese word segmentation. In the well-known Chinese word segmentation bakeoff tasks, for example, different segmentation standards have been used by different data sets (Emerson, 2005). On the other hand, most disagreement on segmentation standards boils down to disagreement on segmentation granularity. As demonstrated by Zhao (2009), one can extract both finegrained and coarse-grained words from characterlevel dependency trees, and hence can adapt to flexible segmentation standards using this formalism. In Figure 1(c), for example, “副局长 (deputy 1326 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326–1336, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics director)” can be segmented a"
P14-1125,I11-1035,0,0.0124878,"a-word dependencies and pseudo inter-word dependencies; same as those of the character-level arc-standard model, shown in Table 1. 4 • STD (pseudo, real): the arc-standard model with pseudo intra-word dependencies and real inter-word dependencies; Experiments 4.1 Experimental Settings We use the Chinese Penn Treebank 5.0, 6.0 and 7.0 to conduct the experiments, splitting the corpora into training, development and test sets according to previous work. Three different splitting methods are used, namely CTB50 by Zhang and Clark (2010), CTB60 by the official documentation of CTB 6.0, and CTB70 by Wang et al. (2011). The dataset statistics are shown in Table 2. We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. The intra-word dependencies are extracted from the annotations of Zhang et al. (2013)2 . The standard measures of word-level precision, recall and F1 score are used to evaluate word segmentation, POS-tagging and dependency parsing, following Hatori et al. (2012). In addition, we use the same measures to evaluate intra-word dependencies, which indicate the performance of predicting word structures. A word’s structure is correct only if all the i"
P14-1125,P12-1110,0,0.264853,"ency parsing, with “words” being “characters”. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to well-established features for word segmentation, POS-tagging and syntactic parsing. In this section, we introduce novel extensions to the arc-standard and the arc-eager transition systems, so that word-based and characterbased features can be used simultaneously for character-level dependency parsing. 3.1 The Arc-Standard Model The arc-standard model has been applied to joint segmentation, POS-tagging and dependency parsing (Hatori et al., 2012), but with pseudo word structures. For unified processing of annotated word structures and fair comparison between character-level arc-eager and arc-standard systems, we define a different arc-standard transition system, consistent with our character-level arceager system. In the word-based arc-standard model, the transition state includes a stack and a queue, where the stack contains a sequence of partially-parsed dependency trees, and the queue consists of unprocessed input words. Four actions are defined for state transition, including arc-left (AL, which creates a left arc between the top"
P14-1125,D08-1059,1,0.940802,"ter-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Correspo"
P14-1125,P10-1110,0,0.021232,"he smallest left subword of “大法官 (chief lawyer)” is “法官 (lawyer)”, and the smallest right subword 1327 大 big 法 law 官 officer (a) smallest left subword 合 agree with 法 law 化 ize (b) smallest right subword Figure 2: An example to illustrate the innermost left/right subwords. of “合法化 (legalize)” is “合法 (legal)”. 3 Character-Level Dependency Parsing A transition-based framework with global learning and beam search decoding (Zhang and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be ei"
P14-1125,D10-1082,1,0.927638,"ows an example, where the smallest left subword of “大法官 (chief lawyer)” is “法官 (lawyer)”, and the smallest right subword 1327 大 big 法 law 官 officer (a) smallest left subword 合 agree with 法 law 化 ize (b) smallest right subword Figure 2: An example to illustrate the innermost left/right subwords. of “合法化 (legalize)” is “合法 (legal)”. 3 Character-Level Dependency Parsing A transition-based framework with global learning and beam search decoding (Zhang and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the s"
P14-1125,J11-1005,1,0.727336,"for syntactic parsing. Zhang et al. (2013) have shown the usefulness of word structures in Chinese constituent parsing. Their results on the Chinese Treebank (CTB) showed that characterlevel constituent parsing can bring increased performances even with the pseudo word structures. They further showed that better performances can be achieved when manually annotated word structures are used instead of pseudo structures. In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al. (2013)’s annotations and based on a transition-based parsing framework (Zhang and Clark, 2011). There are two dominant transitionbased dependency parsing systems, namely the arc-standard and the arc-eager parsers (Nivre, 2008). We study both algorithms for characterlevel dependency parsing in order to make a comprehensive investigation. For direct comparison with word-based parsers, we incorporate the traditional word segmentation, POS-tagging and dependency parsing stages in our joint parsing models. We make changes to the original transition systems, and arrive at two novel transition-based character-level parsers. We conduct experiments on three data sets, including CTB 5.0, CTB 6.0"
P14-1125,P11-2033,1,0.953288,"intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependenc"
P14-1125,W08-0335,0,0.0212139,"hence can adapt to flexible segmentation standards using this formalism. In Figure 1(c), for example, “副局长 (deputy 1326 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326–1336, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics director)” can be segmented as both “副 (deputy) |局 长 (director)” and “副 局 长 (deputy director)”, but not “副 (deputy) 局 (office) |长 (manager)”, by dependency coherence. Chinese language processing tasks, such as machine translation, can benefit from flexible segmentation standards (Zhang et al., 2008; Chang et al., 2008). Second, word internal structures can also be useful for syntactic parsing. Zhang et al. (2013) have shown the usefulness of word structures in Chinese constituent parsing. Their results on the Chinese Treebank (CTB) showed that characterlevel constituent parsing can bring increased performances even with the pseudo word structures. They further showed that better performances can be achieved when manually annotated word structures are used instead of pseudo structures. In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al"
P14-1125,P13-1013,1,0.528047,"ng interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependency tree Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 (The deputy director of forestry administration make a speech in the meeting)”. a character-level dependency tree, where the leaf nodes are Chinese characters. Character-level dependency parsing is interesting in at least two aspects. First, character-level trees circumvent the i"
P14-1125,E09-1100,0,0.097413,"over characters. Character-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving wordlevel dependency parsing accuracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods. 1 林业局 forestry administration 上 in 发言 make a speech 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohn"
P14-1125,P13-1043,1,0.251531,"ion is performed by the SHw action. The actions for intra-word dependencies include intra-word arc-left (ALc ), intra-word arcright (ARc ), pop-word (PW) and inter-word shift (SHc ). The definitions of ALc , ARc and SHc are the same as the word-based arc-standard model, while PW changes the top element on the stack into a full-word node, which can only take interword dependencies. One thing to note is that, due to variable word sizes in character-level parsing, the number of actions can vary between different sequences of actions corresponding to different analyses. We use the padding method (Zhu et al., 2013), adding an IDLE action to finished transition action sequences, for better alignments between states in the beam. 1328 In the character-level arc-standard transition step 0 1 2 3 4 5 6 7 ··· 12 13 ··· step 0 1 2 3 4 5 6 7 ··· 13 14 ··· action SHw (NR) SHc ALc SHc ALc PW SHw (NN) ··· PW ALw ··· (a) action SHc (NR) ALc SHc ALc SHc PW SHw ··· PW ALw ··· stack queue dependencies φ 林 业 ··· φ 林/NR 业 局 ··· φ 林/NR 业/NR 局 副 ··· φ x 业/NR 局 副 ··· A1 = {林 业} 业/NR 局/NR 副 局 ··· A1 S x 局/NR 副 局 ··· A2 = A1 {业 局} 林业局/NR 副 局 ··· A2 林业局/NR 副/NN 局 长 ··· A2 ··· ··· ··· 林业局/NR 副局长/NN 会 上 · · · Ai S x 副局长/NN 会 上 ·"
P14-2106,P08-1068,0,0.258192,"nversion. 1 Introduction This work presents a set of experiments to investigate the use of lexical semantic information in dependency parsing of English. Whether semantics improve parsing is one interesting research topic both on parsing and lexical semantics. Broadly speaking, we can classify the methods to incorporate semantic information into parsers in two: systems using static lexical semantic repositories, such as WordNet or similar ontologies (Agirre et al., 2008; Agirre et al., 2011; Fujita et al., 2010), and systems using dynamic semantic clusters automatically acquired from corpora (Koo et al., 2008; Suzuki et al., 2009). Our main objective will be to determine whether static semantic knowledge can help parsing. We will apply different types of semantic information to three dependency parsers. Specifically, we will test the following questions: • Does semantic information in WordNet help dependency parsing? Agirre et al. (2011) found improvements in dependency parsing 2 Related work Broadly speaking, we can classify the attempts to add external knowledge to a parser in two sets: using large semantic repositories such as WordNet and approaches that use information automatically acquired f"
P14-2106,P08-1037,1,0.914971,"ombinations, showing that the semantically enhanced parsers yield a small significant gain only on the more semantically oriented LTH treebank conversion. 1 Introduction This work presents a set of experiments to investigate the use of lexical semantic information in dependency parsing of English. Whether semantics improve parsing is one interesting research topic both on parsing and lexical semantics. Broadly speaking, we can classify the methods to incorporate semantic information into parsers in two: systems using static lexical semantic repositories, such as WordNet or similar ontologies (Agirre et al., 2008; Agirre et al., 2011; Fujita et al., 2010), and systems using dynamic semantic clusters automatically acquired from corpora (Koo et al., 2008; Suzuki et al., 2009). Our main objective will be to determine whether static semantic knowledge can help parsing. We will apply different types of semantic information to three dependency parsers. Specifically, we will test the following questions: • Does semantic information in WordNet help dependency parsing? Agirre et al. (2011) found improvements in dependency parsing 2 Related work Broadly speaking, we can classify the attempts to add external kno"
P14-2106,S12-1031,0,0.0345955,"Missing"
P14-2106,P11-2123,1,0.834298,"r introducing related work in section 2, section 3 describes the treebank conversions, parsers and semantic features. Section 4 presents the results and section 5 draws the main conclusions. This paper presents experiments with WordNet semantic classes to improve dependency parsing. We study the effect of semantic classes in three dependency parsers, using two types of constituencyto-dependency conversions of the English Penn Treebank. Overall, we can say that the improvements are small and not significant using automatic POS tags, contrary to previously published results using gold POS tags (Agirre et al., 2011). In addition, we explore parser combinations, showing that the semantically enhanced parsers yield a small significant gain only on the more semantically oriented LTH treebank conversion. 1 Introduction This work presents a set of experiments to investigate the use of lexical semantic information in dependency parsing of English. Whether semantics improve parsing is one interesting research topic both on parsing and lexical semantics. Broadly speaking, we can classify the methods to incorporate semantic information into parsers in two: systems using static lexical semantic repositories, such"
P14-2106,P04-1036,0,0.0398379,"UMENT singleton synset, and also in the ARTIFACT SF along with thousands of words including cutter. These are the two extremes of semantic granularity in WordNet. For each semantic representation, we need to determine the semantics of each occurrence of a target word. Agirre et al. (2011) used i) gold-standard annotations from SemCor, a subset of the PTB, to give an upper bound performance of the semantic representation, ii) first sense, where all instances of a word were tagged with their most frequent sense, and iii) automatic sense ranking, predicting the most frequent sense for each word (McCarthy et al., 2004). As we will make use of the full PTB, we only have access to the first sense information. Clusters. Koo et al. (2008) describe a semi4 Results In all the experiments we employed a baseline feature set using word forms and parts of speech, and an enriched feature set (WordNet or clusters). We firstly tested the addition of each individual semantic feature to each parser, evaluating its contribution to the parser’s performance. For the combinations, instead of feature-engineering each parser with the wide array of different possibilities for features, as in Agirre et al. (2011), we adopted the"
P14-2106,J04-4004,0,0.106253,"Missing"
P14-2106,W10-1409,0,0.0387869,"Missing"
P14-2106,P05-1012,0,0.0747214,"output are lower than those for Penn2Malt conversions. 3.2 We have made use of three parsers representative of successful paradigms in dependency parsing. MaltParser (Nivre et al., 2007) is a deterministic transition-based dependency parser that obtains a dependency tree in linear-time in a single pass over the input using a stack of partially analyzed items and the remaining input sequence, by means of history-based feature models. We added two features that inspect the semantic feature at the top of the stack and the next input token. MST 3 represents global, exhaustive graphbased parsing (McDonald et al., 2005; McDonald et al., 2006) that finds the highest scoring directed spanning tree in a graph. The learning procedure is global since model parameters are set relative to classifying the entire dependency graph, in contrast to the local but richer contexts used by transition-based parsers. The system can be trained using first or second order models. The second order projective algorithm performed best on both conversions, and we used it in the rest of the evaluations. We modified the system in order to add semantic features, combining them with wordforms and POS tags, on the parent and child node"
P14-2106,W06-2932,0,0.0292674,"those for Penn2Malt conversions. 3.2 We have made use of three parsers representative of successful paradigms in dependency parsing. MaltParser (Nivre et al., 2007) is a deterministic transition-based dependency parser that obtains a dependency tree in linear-time in a single pass over the input using a stack of partially analyzed items and the remaining input sequence, by means of history-based feature models. We added two features that inspect the semantic feature at the top of the stack and the next input token. MST 3 represents global, exhaustive graphbased parsing (McDonald et al., 2005; McDonald et al., 2006) that finds the highest scoring directed spanning tree in a graph. The learning procedure is global since model parameters are set relative to classifying the entire dependency graph, in contrast to the local but richer contexts used by transition-based parsers. The system can be trained using first or second order models. The second order projective algorithm performed best on both conversions, and we used it in the rest of the evaluations. We modified the system in order to add semantic features, combining them with wordforms and POS tags, on the parent and child nodes of each arc. ZPar4 (Zh"
P14-2106,A00-2018,0,0.262203,"Missing"
P14-2106,W02-1001,0,0.126521,"Missing"
P14-2106,W09-3829,0,0.0385685,"Missing"
P14-2106,J03-4003,0,0.120933,"Missing"
P14-2106,N06-2033,0,0.0327289,"the best performing system was evaluated on the test set (section 23). 651 Parsers Best baseline (ZPar) Best single parser (ZPar + Clusters) Best combination (3 baseline parsers) Best combination of 3 parsers: 3 baselines + 3 SF extensions Best combination of 3 parsers: 3 baselines + 3 SS extensions Best combination of 3 parsers: 3 baselines + 3 cluster extensions LAS 91.52 91.74 (+0.22) 91.90 (+0.38) UAS 92.57 92.63 93.01 91.93 (+0.41) 92.95 91.87 (+0.35) 92.92 91.90 (+0.38) 92.90 4.2 Subsection 4.1 presented the results of the base algorithms and their extensions based on semantic features. Sagae and Lavie (2006) report improvements over the best single parser when combining three transition-based models and one graph-based model. The same technique was also used by the winning team of the CoNLL 2007 Shared Task (Hall et al., 2007), combining six transition-based parsers. We used MaltBlender5 , a tool for merging the output of several dependency parsers, using the Chu-Liu/Edmonds directed MST algorithm. After several tests we noticed that weighted voting by each parser’s labeled accuracy gave good results, using it in the rest of the experiments. We trained different types of combination: • Base algor"
P14-2106,D07-1097,1,0.82722,"F extensions Best combination of 3 parsers: 3 baselines + 3 SS extensions Best combination of 3 parsers: 3 baselines + 3 cluster extensions LAS 91.52 91.74 (+0.22) 91.90 (+0.38) UAS 92.57 92.63 93.01 91.93 (+0.41) 92.95 91.87 (+0.35) 92.92 91.90 (+0.38) 92.90 4.2 Subsection 4.1 presented the results of the base algorithms and their extensions based on semantic features. Sagae and Lavie (2006) report improvements over the best single parser when combining three transition-based models and one graph-based model. The same technique was also used by the winning team of the CoNLL 2007 Shared Task (Hall et al., 2007), combining six transition-based parsers. We used MaltBlender5 , a tool for merging the output of several dependency parsers, using the Chu-Liu/Edmonds directed MST algorithm. After several tests we noticed that weighted voting by each parser’s labeled accuracy gave good results, using it in the rest of the experiments. We trained different types of combination: • Base algorithms. This set includes the 3 baseline algorithms, MaltParser, MST, and ZPar. • Extended parsers, adding semantic information to the baselines. We include the three base algorithms and their semantic extensions (SF, SS, an"
P14-2106,N10-1091,0,0.0577312,"Missing"
P14-2106,W07-2416,0,0.0729244,"Missing"
P14-2106,D09-1058,0,0.0187841,"uction This work presents a set of experiments to investigate the use of lexical semantic information in dependency parsing of English. Whether semantics improve parsing is one interesting research topic both on parsing and lexical semantics. Broadly speaking, we can classify the methods to incorporate semantic information into parsers in two: systems using static lexical semantic repositories, such as WordNet or similar ontologies (Agirre et al., 2008; Agirre et al., 2011; Fujita et al., 2010), and systems using dynamic semantic clusters automatically acquired from corpora (Koo et al., 2008; Suzuki et al., 2009). Our main objective will be to determine whether static semantic knowledge can help parsing. We will apply different types of semantic information to three dependency parsers. Specifically, we will test the following questions: • Does semantic information in WordNet help dependency parsing? Agirre et al. (2011) found improvements in dependency parsing 2 Related work Broadly speaking, we can classify the attempts to add external knowledge to a parser in two sets: using large semantic repositories such as WordNet and approaches that use information automatically acquired from corpora. In the fi"
P14-2106,P10-1001,0,0.0300867,"Missing"
P14-2106,N12-1052,0,0.0566838,"Missing"
P14-2106,W03-3023,0,0.153063,"Missing"
P14-2106,D08-1059,1,0.816794,"6) that finds the highest scoring directed spanning tree in a graph. The learning procedure is global since model parameters are set relative to classifying the entire dependency graph, in contrast to the local but richer contexts used by transition-based parsers. The system can be trained using first or second order models. The second order projective algorithm performed best on both conversions, and we used it in the rest of the evaluations. We modified the system in order to add semantic features, combining them with wordforms and POS tags, on the parent and child nodes of each arc. ZPar4 (Zhang and Clark, 2008; Zhang and Nivre, 2011) performs transition-based dependency parsing with a stack of partial analysis and a queue of remaining inputs. In contrast to MaltParser (local model and greedy deterministic search) ZPar applies global discriminative learning and beam search. We extend the feature set of ZPar to include semantic features. Each set of semantic information is represented by two atomic Experimental Framework In this section we will briefly describe the PTBbased datasets (subsection 3.1), followed by the data-driven parsers used for the experiments (subsection 3.2). Finally, we will descr"
P14-2106,P11-2033,1,0.836904,"st scoring directed spanning tree in a graph. The learning procedure is global since model parameters are set relative to classifying the entire dependency graph, in contrast to the local but richer contexts used by transition-based parsers. The system can be trained using first or second order models. The second order projective algorithm performed best on both conversions, and we used it in the rest of the evaluations. We modified the system in order to add semantic features, combining them with wordforms and POS tags, on the parent and child nodes of each arc. ZPar4 (Zhang and Clark, 2008; Zhang and Nivre, 2011) performs transition-based dependency parsing with a stack of partial analysis and a queue of remaining inputs. In contrast to MaltParser (local model and greedy deterministic search) ZPar applies global discriminative learning and beam search. We extend the feature set of ZPar to include semantic features. Each set of semantic information is represented by two atomic Experimental Framework In this section we will briefly describe the PTBbased datasets (subsection 3.1), followed by the data-driven parsers used for the experiments (subsection 3.2). Finally, we will describe the different types"
P14-2106,P07-1122,1,\N,Missing
P14-2106,W07-1204,0,\N,Missing
P14-2128,D12-1133,0,0.0896542,"Missing"
P14-2128,J08-4003,0,0.533264,"5 88.15 0 ∼ 15 91.84 90.81 92.96 92.45 21 − 40 15 ∼ 30 91.82 90.15 92.63 93.11 &gt; 30 83.87 75.00 76.61 77.42 0 ∼ 15 89.83 88.06 90.78 90.89 41 − 60 15 ∼ 30 88.01 88.89 88.76 89.77 &gt; 30 − − − − Table 2: Parsing accuracies vs punctuation ratios, on the development set System Dev UAS Test UAS Dev UAS-p Test UAS-p Dev− UAS Test− UAS E-F 91.83 91.75 83.20 84.67 90.64 90.40 A-S 90.71 90.34 79.69 79.64 89.55 89.33 A-S-64 93.02 92.84 84.80 87.80 91.87 91.75 MST 92.56 92.10 84.42 85.67 90.11 89.82 Parser1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transitionbased parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011) 2 , and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al., 2013). Table 1: Parsing accuracies. “E-F” and “MST” denote easy-first parser and MSTparser, respectively. “A-S” and “A-S 64” denote our arc-standard parser with beam width 1 and 64, respectively. “UAS” and “UAS-p” denote word and punctuation unlabelled attachment score, respectively. “− ” denotes the data set with punctuations removed. 2.2"
P14-2128,P13-1104,0,0.117611,"Missing"
P14-2128,W11-0303,0,0.14348,"Missing"
P14-2128,W02-1001,0,0.320993,"ithub.com/majineu/Parser/Punc/A-STD. 2 Influence of Punctuations on Parsing In this section, we conduct a set of experiments to show the influence of punctuations on dependency parsing accuracies. 2.1 Punctuations and Parsing Accuracy Setup We use the Wall Street Journal portion of the Penn Treebank with the standard splits: sections 02-21 are used as the training set; section 22 and section 23 are used as the development and test set, respectively. Penn2Malt is used to convert bracketed structures into dependencies. We use our own implementation of the Part-Of-Speech (POS) tagger proposed by Collins (2002) to tag the development and test sets. Training set POS tags are generated using 10-fold jack-knifing. Parsing accuracy is evaluated using unlabelled attachment score (UAS), which is the percentage of words that are assigned the correct lexical heads. To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MST1 We trained a second order labelled parser with all the configurations set to the default value. The code is publicly available at h"
P14-2128,D08-1059,1,0.88905,"63 93.11 &gt; 30 83.87 75.00 76.61 77.42 0 ∼ 15 89.83 88.06 90.78 90.89 41 − 60 15 ∼ 30 88.01 88.89 88.76 89.77 &gt; 30 − − − − Table 2: Parsing accuracies vs punctuation ratios, on the development set System Dev UAS Test UAS Dev UAS-p Test UAS-p Dev− UAS Test− UAS E-F 91.83 91.75 83.20 84.67 90.64 90.40 A-S 90.71 90.34 79.69 79.64 89.55 89.33 A-S-64 93.02 92.84 84.80 87.80 91.87 91.75 MST 92.56 92.10 84.42 85.67 90.11 89.82 Parser1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transitionbased parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011) 2 , and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al., 2013). Table 1: Parsing accuracies. “E-F” and “MST” denote easy-first parser and MSTparser, respectively. “A-S” and “A-S 64” denote our arc-standard parser with beam width 1 and 64, respectively. “UAS” and “UAS-p” denote word and punctuation unlabelled attachment score, respectively. “− ” denotes the data set with punctuations removed. 2.2 Our first experiment is to show that, compared with words, punctuations are more"
P14-2128,N10-1115,0,0.035385,"Parsing accuracies vs punctuation ratios, on the development set System Dev UAS Test UAS Dev UAS-p Test UAS-p Dev− UAS Test− UAS E-F 91.83 91.75 83.20 84.67 90.64 90.40 A-S 90.71 90.34 79.69 79.64 89.55 89.33 A-S-64 93.02 92.84 84.80 87.80 91.87 91.75 MST 92.56 92.10 84.42 85.67 90.11 89.82 Parser1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transitionbased parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011) 2 , and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al., 2013). Table 1: Parsing accuracies. “E-F” and “MST” denote easy-first parser and MSTparser, respectively. “A-S” and “A-S 64” denote our arc-standard parser with beam width 1 and 64, respectively. “UAS” and “UAS-p” denote word and punctuation unlabelled attachment score, respectively. “− ” denotes the data set with punctuations removed. 2.2 Our first experiment is to show that, compared with words, punctuations are more difficult to parse and to learn. To see this, we evaluate the parsing accuracies of the selected parsers on words and punctuations, sep"
P14-2128,P11-2033,1,0.959265,"o Company, 358 Wener Rd., Hangzhou, China, 310012 majineu@gmail.com yue zhang@sutd.edu.sg zhujingbo@mail.neu.edu.cn Abstract Moreover, experimental results showed that parsing accuracy of content words drops on sentences which contain higher ratios of punctuations. One reason for this result is that projective dependency parsers satisfy the “no crossing links” constraint, and errors in punctuations may prevent correct word-word dependencies from being created (see section 2). In addition, punctuations cause certain type of features inaccurate. Take valency features for example, previous work (Zhang and Nivre, 2011) has shown that such features are important to parsing accuracy, e.g., it may inform the parser that a verb already has two objects attached to it. However, such information might be inaccurate when the verb’s modifiers contain punctuations. Ultimately, it is the dependencies between words that provide useful information for real world applications. Take machine translation or information extraction for example, most systems take advantage of the head-modifier relationships between word pairs rather than word-punctuation pairs to make better predictions. The fact that most previous work evalua"
P14-2128,P10-1110,0,0.0976296,"Missing"
P14-2128,C12-2136,1,0.917822,"Missing"
P14-2128,P13-2020,1,0.805497,"nt set System Dev UAS Test UAS Dev UAS-p Test UAS-p Dev− UAS Test− UAS E-F 91.83 91.75 83.20 84.67 90.64 90.40 A-S 90.71 90.34 79.69 79.64 89.55 89.33 A-S-64 93.02 92.84 84.80 87.80 91.87 91.75 MST 92.56 92.10 84.42 85.67 90.11 89.82 Parser1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transitionbased parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011) 2 , and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al., 2013). Table 1: Parsing accuracies. “E-F” and “MST” denote easy-first parser and MSTparser, respectively. “A-S” and “A-S 64” denote our arc-standard parser with beam width 1 and 64, respectively. “UAS” and “UAS-p” denote word and punctuation unlabelled attachment score, respectively. “− ” denotes the data set with punctuations removed. 2.2 Our first experiment is to show that, compared with words, punctuations are more difficult to parse and to learn. To see this, we evaluate the parsing accuracies of the selected parsers on words and punctuations, separately. Results are listed in Table 1, where r"
P14-2128,E06-1011,0,0.0431084,"64 MST 0 ∼ 15 94.56 93.87 95.28 94.90 1 ∼ 20 15 ∼ 30 92.88 92.00 94.43 93.55 &gt; 30 87.67 90.05 88.15 88.15 0 ∼ 15 91.84 90.81 92.96 92.45 21 − 40 15 ∼ 30 91.82 90.15 92.63 93.11 &gt; 30 83.87 75.00 76.61 77.42 0 ∼ 15 89.83 88.06 90.78 90.89 41 − 60 15 ∼ 30 88.01 88.89 88.76 89.77 &gt; 30 − − − − Table 2: Parsing accuracies vs punctuation ratios, on the development set System Dev UAS Test UAS Dev UAS-p Test UAS-p Dev− UAS Test− UAS E-F 91.83 91.75 83.20 84.67 90.64 90.40 A-S 90.71 90.34 79.69 79.64 89.55 89.33 A-S-64 93.02 92.84 84.80 87.80 91.87 91.75 MST 92.56 92.10 84.42 85.67 90.11 89.82 Parser1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transitionbased parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011) 2 , and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al., 2013). Table 1: Parsing accuracies. “E-F” and “MST” denote easy-first parser and MSTparser, respectively. “A-S” and “A-S 64” denote our arc-standard parser with beam width 1 and 64, respectively. “UAS” and “UAS-p” denote word and punctuation unlabelled attach"
P14-2128,P05-1012,0,0.216525,"Missing"
P14-6008,P14-1021,1,0.829043,"troduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentation, POS-tagging and parsing (Zhang et al., 2013; Zhang et al., 2014). In addition to the aforementioned tasks,"
P14-6008,P07-1106,1,0.91772,"l discriminative learning and beam-search decoding. The method has been applied to a wide range of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010),"
P14-6008,P08-1101,1,0.827347,"coding. The method has been applied to a wide range of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing ("
P14-6008,D08-1059,1,0.822846,"coding. The method has been applied to a wide range of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing ("
P14-6008,W09-3825,1,0.756058,"tion to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentatio"
P14-6008,D10-1082,1,0.847244,"(Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentation, POS-tagging and parsing (Zhang et al., 2013; Zhang et al., 2014). In addition to the aforementioned tasks, the 13 framework can be applied to all structural prediction tasks for which the output can be constructed using an incremental process. The advantage of this framework is two-fold. First, beamsearch enables highly efficient decoding, which typically has linear time complexity, depending on"
P14-6008,P11-1069,1,0.847476,"g, tliu}@ir.hit.edu.cn † Abstract This tutorial discusses a framework for incremental left-to-right structured predication, which makes use of global discriminative learning and beam-search decoding. The method has been applied to a wide range of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has a"
P14-6008,J11-1005,1,0.848919,"g, tliu}@ir.hit.edu.cn † Abstract This tutorial discusses a framework for incremental left-to-right structured predication, which makes use of global discriminative learning and beam-search decoding. The method has been applied to a wide range of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has a"
P14-6008,P11-2033,1,0.838007,"ange of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), j"
P14-6008,C12-2136,1,0.812736,"ch the output can be constructed using an incremental process. The advantage of this framework is two-fold. First, beamsearch enables highly efficient decoding, which typically has linear time complexity, depending on the incremental process. Second, free from DPstyle constraints and Markov-style independence assumptions, the framework allows arbitrary features to be defined to capture structural patterns. In addition to feature advantages, the high accuracies of this framework are also enabled by direct interactions between learning and search (Daum´e III and Marcu, 2005; Huang et al., 2012; Zhang and Nivre, 2012). 2 Tutorial Overview In this tutorial, we make an introduction to the framework, illustrating how it can be applied to a range of NLP problems, giving theoretical discussions and demonstrating a software implementation. We start with a detailed introduction of the framework, describing the averaged perceptron algorithm (Collins, 2002) and its efficient implementation issues (Zhang and Clark, 2007), as well as beam-search and the early-update strategy (Collins and Roark, 2004). We then illustrate how the framework can be applied to NLP tasks, including word segmentation, joint segmentation & P"
P14-6008,P13-1013,1,0.83742,"tional categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentation, POS-tagging and parsing (Zhang et al., 2013; Zhang et al., 2014). In addition to the aforementioned tasks, the 13 framework can be applied to all structural prediction tasks for which the output can be constructed using an incremental process. The advantage of this framework is two-fold. First, beamsearch enables highly efficient decoding, which typically has linear time complexity, depending on the incremental process. Second, free from DPstyle constraints and Markov-style independence assumptions, the framework allows arbitrary features to be defined to capture structural patterns. In addition to feature advantages, the high accuraci"
P14-6008,P14-1125,1,0.795356,"ammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentation, POS-tagging and parsing (Zhang et al., 2013; Zhang et al., 2014). In addition to the aforementioned tasks, the 13 framework can be applied to all structural prediction tasks for which the output can be constructed using an incremental process. The advantage of this framework is two-fold. First, beamsearch enables highly efficient decoding, which typically has linear time complexity, depending on the incremental process. Second, free from DPstyle constraints and Markov-style independence assumptions, the framework allows arbitrary features to be defined to capture structural patterns. In addition to feature advantages, the high accuracies of this framework"
P14-6008,P13-1043,1,0.847813,"and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentation, POS-tagging and"
P14-6008,E12-1009,0,\N,Missing
P14-6008,W02-1001,0,\N,Missing
P14-6008,P04-1015,0,\N,Missing
P14-6008,P10-1110,0,\N,Missing
P14-6008,P13-1001,0,\N,Missing
P14-6008,I11-1136,0,\N,Missing
P14-6008,N12-1015,0,\N,Missing
P15-1045,C10-1037,0,0.205831,"w(ei ) = salext (s) (2) where salabs (·) denotes the word salience score of an abstractive model, salext (·) denotes the sentence salience score of an extractive model, and Sen(ei ) denotes the sentence set where ei is extracted from. We exploit our baseline sentence ranking method, SentRank, to obtain the sentence salience score, and use our baseline phrase ranking method, PhraseRank, to obtain the phrase salience score. 3.2 Headline Generation We use a graph-based multi-sentence compression (MSC) model to generate the final title for the proposed event-driven model. The model is inspired by Filippova (2010). First, a weighted directed acyclic word graph is built, with a start node and an end node in the graph. A headline can be obtained by any path from the start node to the end node. We measure each candidate path by a scoring function. Based on the measurement, we exploit a beam-search algorithm to find the optimum path. (1) i=1 (lj ,ei )∈Gbi X s∈Sen(ei ) rij × sal(lj ) rij × sal(ei ) Events w(lj ) · w(ei ) A 3.2.1 Word-Graph Construction Given a set of candidate events CE, we extract all the sentences that contain the events. In particular, we add two artificial words, hSi and hEi, to the sta"
P15-1045,P13-1122,0,0.38366,"e #K Multi-Sentence Compression Headline Headline Generation Figure 1: System framework. Zajic et al., 2005). Abstractive models choose a set of informative phrases for candidate extraction, and then exploit sentence synthesis techniques for headline generation (Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010). Extractive HG and abstractive HG have their respective advantages and disadvantages. Extractive models can generate more readable headlines, because the final title is derived by tailoring human-written sentences. However, extractive models give less informative titles (Alfonseca et al., 2013), because sentences are very sparse, making high-recall candidate extraction difficult. In contrast, abstractive models use phrases as the basic processing units, which are much less sparse. However, it is more difficult for abstractive HG to ensure the grammaticality of the generated titles, given that sentence synthesis is still very inaccurate based on a set of phrases with little grammatical information (Zhang, 2013). In this paper, we propose an event-driven model for headline generation, which alleviates the Introduction Headline generation (HG) is a text summarization task, which aims t"
P15-1045,N10-1131,0,0.0313243,"rade-off between sentences and phrases, avoiding sparsity and structureless problems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single document using an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced"
P15-1045,W97-0703,0,0.110622,"extracting event arguments. Event arguments that have the same predicate are merged into one event, represented by tuple (Subject, Predicate, Object). For example, given the sentence, “the Keenans could demand the Aryan Nations’ assets”, Figure 2 present its partial parsing tree. Based on the parsing results, two event arguments are obtained: nsubj(demand, Keenans) and dobj(demand, assets). The two event arguments are merged into one event: (Keenans, demand, assets). 3.1.2 Extracting Lexical Chains Lexical chains are used to link semanticallyrelated words and phrases (Morris and Hirst, 1991; Barzilay and Elhadad, 1997). A lexical chain is analogous to a semantic synset. Compared with words, lexical chains are less sparse for event ranking. Given a text, we follow Boudin and Morin (2013) to construct lexical chains based on the following principles: 1. All words that are identical after stemming are treated as one word; 2. All NPs with the same head word fall into one lexical chain;2 3.1.1 Extracting Events We apply an open-domain event extraction approach. Different from traditional event extraction, for which types and arguments are predefined, open event extraction does not have a closed set of entities a"
P15-1045,J95-2003,0,0.490343,"Missing"
P15-1045,J05-3002,0,0.0528138,"of EventRank is better, capturing the major event in the reference title. 469 a word graph are treated equally, and the edges in the graph are constructed according to the adjacent order in original sentence. Our MSC model is also inspired by Filippova (2010). Our approach is more aggressive than their approach, generating compressions with arbitrary length by using a different edge construction strategy. In addition, our search algorithm is also different from theirs. Our graph-based MSC model is also similar in spirit to sentence fusion, which has been used for multi-document summarization (Barzilay and McKeown, 2005; Elsner and Santhanam, 2011). abstractive models have the problem of poor grammaticality. The event-driven mothod can alleviate both issues since event offer a trade-off between sentence and phrase. 5 Related Work Our event-driven model is different from traditional extractive (Dorr et al., 2003; Erkan and Radev, 2004; Alfonseca et al., 2013) and abstractive models (Zajic et al., 2005; Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010) in that events are used as the basic processing units instead of sentences and phrases. As mentioned above, events are a trade-off between senten"
P15-1045,D13-1036,0,0.142968,"Missing"
P15-1045,N13-1030,0,0.043263,"Missing"
P15-1045,D14-1076,0,0.0195508,"lems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single document using an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013;"
P15-1045,J10-3005,0,0.05359,"Missing"
P15-1045,W04-1013,0,0.0153886,"ne sentence. 6 466 http://www.speech.sri.com/projects/srilm/ Input: G ← (V, E), LM, B Output: best candidates ← { {hSi} } loop do beam ← { } for each candidate in candidates if candidate endwith hEi A DD T O B EAM(beam, candidate) continue for each Vi in V candidate ← A DDV ERTEX(candidate, Vi ) C OMPUTE S CORE(candidate, LM) A DD T O B EAM(beam, candidate) end for end for candidates ← T OP -K(beam, B) if candidates all endwith hEi : break end loop best ← B EST(candidates) conducted using the Stanford NLP tools (Marneffe and Manning, 2008). The MRP iteration number is set to 10. We use ROUGE (Lin, 2004) to automatically measure the model performance, which has been widely used in summarization tasks (Wang et al., 2013; Ng et al., 2014). We focus on Rouge1 and Rouge2 scores, following Xu et al. (2010). In addition, we conduct human evaluations, using the same method as Woodsend et al. (2010). Four participants are asked to rate the generated headlines by three criteria: informativeness (how much important information in the article does the headline describe?), fluency (is it fluent to read?) and coherence (does it capture the topic of article?). Each headline is given a subjective score from"
P15-1045,P04-1015,0,0.038794,"m language model is trained using SRILM6 on English Gigaword (LDC2011T07). e∈CE where sal(e) is the salience score of an event from the candidate extraction step, Vi .w denotes the word of vertex Vi , and dist(w, e) denotes the distance from the word w to the event e, which are defined by the minimum distance from w to all the related words of e in a sentence by the dependency path5 between them. Intuitively, equation 3 demonstrates that a vertex is salient when its corresponding word is close to salient 3.2.3 Beam Search Beam search has been widely used aiming to find the sub optimum result (Collins and Roark, 2004; Zhang and Clark, 2011), when exact inference is extremely difficult. Assuming our word graph has a vertex size of n, the worst computation complexity is O(n4 ) when using a trigram language model, which is time consuming. 5 The distance is +∞ when e and w are not in one sentence. 6 466 http://www.speech.sri.com/projects/srilm/ Input: G ← (V, E), LM, B Output: best candidates ← { {hSi} } loop do beam ← { } for each candidate in candidates if candidate endwith hEi A DD T O B EAM(beam, candidate) continue for each Vi in V candidate ← A DDV ERTEX(candidate, Vi ) C OMPUTE S CORE(candidate, LM) A"
P15-1045,W09-1801,0,0.0753187,"Missing"
P15-1045,D14-1148,1,0.812388,"POS NNS the Keenans could demand the Aryan Nations ’ assets Figure 2: Dependency tree for the sentence “the Keenans could demand the Aryan Nations’ assets”. Candidate Extraction We exploit events as the basic units for candidate extraction. Here an event is a tuple (S, P, O), where S is the subject, P is the predicate and O is the object. For example, for the sentence “Ukraine Delays Announcement of New Government”, the event is (Ukraine, Delays, Announcement). This type of event structures has been used in open information extraction (Fader et al., 2011), and has a range of NLP applications (Ding et al., 2014; Ng et al., 2014). A sentence is a well-formed structure with complete syntactic information, but can contain redundant information for text summarization, which makes sentences very sparse. Phrases can be used to avoid the sparsity problem, but with little syntactic information between phrases, fluent headline generation is difficult. Events can be regarded as a trade-off between sentences and phrases. They are meaningful structures without redundant components, less sparse than sentences and containing more syntactic information than phrases. In our system, candidate event extraction is per"
P15-1045,W13-2117,0,0.0238691,"2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013; Mehdad et al. (2014) introduce the MSC based on word graph into meeting summarization. Tzouridis et al. (2014) cast multi-sentence compression as a structured predication problem. They use a largemargin approach to adapt parameterised edge weights to the data in order to acquire the shortest path. In their work, the sentences introduced to 6 Conclusion and Future Work We proposed an event-driven model headline generation, introducing a graph-based MSC model to generate the final title, based on a set of events. Our event-driven model can incorporate sentence and phrase salience, which has be"
P15-1045,W03-0501,0,0.414777,"nd McKeown, 2004). This task is challenging in not only informativeness and readability, which are challenges to common summarization tasks, but also the length reduction, which is unique for headline generation. Previous headline generation models fall into two main categories, namely extractive HG and abstractive HG (Woodsend et al., 2010; Alfonseca et al., 2013). Both consist of two steps: candidate extraction and headline generation. Extractive models choose a set of salient sentences in candidate extraction, and then exploit sentence compression techniques to achieve headline generation (Dorr et al., 2003; 462 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 462–472, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics sentence ranking method. In this paper, we use SentRank to denote this method. disadvantages of both extractive and abstractive HG. The framework of the proposed model is shown in Figure 1. In particular, we use events as the basic processing units for candidate extraction. We use structured tuples to represent the subject, predica"
P15-1045,P14-1115,0,0.0355339,"Missing"
P15-1045,W11-1607,0,0.0240977,"uring the major event in the reference title. 469 a word graph are treated equally, and the edges in the graph are constructed according to the adjacent order in original sentence. Our MSC model is also inspired by Filippova (2010). Our approach is more aggressive than their approach, generating compressions with arbitrary length by using a different edge construction strategy. In addition, our search algorithm is also different from theirs. Our graph-based MSC model is also similar in spirit to sentence fusion, which has been used for multi-document summarization (Barzilay and McKeown, 2005; Elsner and Santhanam, 2011). abstractive models have the problem of poor grammaticality. The event-driven mothod can alleviate both issues since event offer a trade-off between sentence and phrase. 5 Related Work Our event-driven model is different from traditional extractive (Dorr et al., 2003; Erkan and Radev, 2004; Alfonseca et al., 2013) and abstractive models (Zajic et al., 2005; Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010) in that events are used as the basic processing units instead of sentences and phrases. As mentioned above, events are a trade-off between sentences and phrases, avoiding spa"
P15-1045,J91-1002,0,0.655589,"ons, nsubj and dobj, for extracting event arguments. Event arguments that have the same predicate are merged into one event, represented by tuple (Subject, Predicate, Object). For example, given the sentence, “the Keenans could demand the Aryan Nations’ assets”, Figure 2 present its partial parsing tree. Based on the parsing results, two event arguments are obtained: nsubj(demand, Keenans) and dobj(demand, assets). The two event arguments are merged into one event: (Keenans, demand, assets). 3.1.2 Extracting Lexical Chains Lexical chains are used to link semanticallyrelated words and phrases (Morris and Hirst, 1991; Barzilay and Elhadad, 1997). A lexical chain is analogous to a semantic synset. Compared with words, lexical chains are less sparse for event ranking. Given a text, we follow Boudin and Morin (2013) to construct lexical chains based on the following principles: 1. All words that are identical after stemming are treated as one word; 2. All NPs with the same head word fall into one lexical chain;2 3.1.1 Extracting Events We apply an open-domain event extraction approach. Different from traditional event extraction, for which types and arguments are predefined, open event extraction does not ha"
P15-1045,W08-1301,0,0.0796257,": 1. All words that are identical after stemming are treated as one word; 2. All NPs with the same head word fall into one lexical chain;2 3.1.1 Extracting Events We apply an open-domain event extraction approach. Different from traditional event extraction, for which types and arguments are predefined, open event extraction does not have a closed set of entities and relations (Fader et al., 2011). We follow Hu’s work (Hu et al., 2013) to extract events. Given a text, we first use the Stanford dependency parser1 to obtain the Stanford typed dependency structures of the sentences (Marneffe and Manning, 2008). Then we focus on 1 poss aux 3. A pronoun is added to the corresponding lexical chain if it refers to a word in the chain (The coreference resolution is performed using the Stanford Coreference Resolution system);3 4. Lexical chains are merged if their main words are in the same synset of WordNet.4 2 NPs are extracted according to the dependency relations nn and amod. As shown in Figure 2, we can extract the noun phrase Aryan Nations according to the dependency relation nn(Nations, Aryan). 3 http://nlp.stanford.edu/software/dcoref.shtml 4 http://wordnet.princeton.edu/ http://nlp.stanford.edu/"
P15-1045,P14-1087,0,0.148337,"could demand the Aryan Nations ’ assets Figure 2: Dependency tree for the sentence “the Keenans could demand the Aryan Nations’ assets”. Candidate Extraction We exploit events as the basic units for candidate extraction. Here an event is a tuple (S, P, O), where S is the subject, P is the predicate and O is the object. For example, for the sentence “Ukraine Delays Announcement of New Government”, the event is (Ukraine, Delays, Announcement). This type of event structures has been used in open information extraction (Fader et al., 2011), and has a range of NLP applications (Ding et al., 2014; Ng et al., 2014). A sentence is a well-formed structure with complete syntactic information, but can contain redundant information for text summarization, which makes sentences very sparse. Phrases can be used to avoid the sparsity problem, but with little syntactic information between phrases, fluent headline generation is difficult. Events can be regarded as a trade-off between sentences and phrases. They are meaningful structures without redundant components, less sparse than sentences and containing more syntactic information than phrases. In our system, candidate event extraction is performed on a bipart"
P15-1045,D11-1142,0,0.474229,"ternational Joint Conference on Natural Language Processing, pages 462–472, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics sentence ranking method. In this paper, we use SentRank to denote this method. disadvantages of both extractive and abstractive HG. The framework of the proposed model is shown in Figure 1. In particular, we use events as the basic processing units for candidate extraction. We use structured tuples to represent the subject, predicate and object of an event. This form of event representation is widely used in open information extraction (Fader et al., 2011; Qiu and Zhang, 2014). Intuitively, events can be regarded as a trade-off between sentences and phrases. Events are meaningful structures, containing necessary grammatical information, and yet are much less sparse than sentences. We use salience measures of both sentences and phrases for event extraction, and thus our model can be regarded as a combination of extractive and abstractive HG. During the headline generation step, A graphbased multi-sentence compression (MSC) model is proposed to generate a final title, given multiple events. First a directed acyclic word graph is constructed base"
P15-1045,D14-1201,1,0.841711,"nference on Natural Language Processing, pages 462–472, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics sentence ranking method. In this paper, we use SentRank to denote this method. disadvantages of both extractive and abstractive HG. The framework of the proposed model is shown in Figure 1. In particular, we use events as the basic processing units for candidate extraction. We use structured tuples to represent the subject, predicate and object of an event. This form of event representation is widely used in open information extraction (Fader et al., 2011; Qiu and Zhang, 2014). Intuitively, events can be regarded as a trade-off between sentences and phrases. Events are meaningful structures, containing necessary grammatical information, and yet are much less sparse than sentences. We use salience measures of both sentences and phrases for event extraction, and thus our model can be regarded as a combination of extractive and abstractive HG. During the headline generation step, A graphbased multi-sentence compression (MSC) model is proposed to generate a final title, given multiple events. First a directed acyclic word graph is constructed based on the extracted eve"
P15-1045,J11-1005,1,0.849206,"ed using SRILM6 on English Gigaword (LDC2011T07). e∈CE where sal(e) is the salience score of an event from the candidate extraction step, Vi .w denotes the word of vertex Vi , and dist(w, e) denotes the distance from the word w to the event e, which are defined by the minimum distance from w to all the related words of e in a sentence by the dependency path5 between them. Intuitively, equation 3 demonstrates that a vertex is salient when its corresponding word is close to salient 3.2.3 Beam Search Beam search has been widely used aiming to find the sub optimum result (Collins and Roark, 2004; Zhang and Clark, 2011), when exact inference is extremely difficult. Assuming our word graph has a vertex size of n, the worst computation complexity is O(n4 ) when using a trigram language model, which is time consuming. 5 The distance is +∞ when e and w are not in one sentence. 6 466 http://www.speech.sri.com/projects/srilm/ Input: G ← (V, E), LM, B Output: best candidates ← { {hSi} } loop do beam ← { } for each candidate in candidates if candidate endwith hEi A DD T O B EAM(beam, candidate) continue for each Vi in V candidate ← A DDV ERTEX(candidate, Vi ) C OMPUTE S CORE(candidate, LM) A DD T O B EAM(beam, candi"
P15-1045,P14-1117,0,0.0129353,"ar, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single document using an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013; Mehdad et al. (2"
P15-1045,C14-1155,0,0.0185322,"multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013; Mehdad et al. (2014) introduce the MSC based on word graph into meeting summarization. Tzouridis et al. (2014) cast multi-sentence compression as a structured predication problem. They use a largemargin approach to adapt parameterised edge weights to the data in order to acquire the shortest path. In their work, the sentences introduced to 6 Conclusion and Future Work We proposed an event-driven model headline generation, introducing a graph-based MSC model to generate the final title, based on a set of events. Our event-driven model can incorporate sentence and phrase salience, which has been used in extractive and abstractive HG models. The proposed graph-based MSC model is not limited to our event-"
P15-1045,C04-1079,0,0.160236,"ce realization preferences jointly. Previous extractive and abstractive models take two main steps, namely candidate extraction and headline generation. Here, we introduce these two types of models according to the two steps. 2.1 Abstractive Headline Generation Extractive Headline Generation Candidate Extraction. Extractive models exploit sentences as the basic processing units in this step. Sentences are ranked by their salience according to specific strategies (Dorr et al., 2003; Erkan and Radev, 2004; Zajic et al., 2005). One of the stateof-the-art approaches is the work of Erkan and Radev (2004), which exploits centroid, position and length features to compute sentence salience. We re-implemented this method as our baseline 463 3 Our Model dobj Similar to extractive and abstractive models, the proposed event-driven model consists of two steps, namely candidate extraction and headline generation. 3.1 nsubj det nn DT NNPS MD VB DT NNP NNP POS NNS the Keenans could demand the Aryan Nations ’ assets Figure 2: Dependency tree for the sentence “the Keenans could demand the Aryan Nations’ assets”. Candidate Extraction We exploit events as the basic units for candidate extraction. Here an ev"
P15-1045,P13-1136,0,0.317702,"also from the proposed graph-based MSC model. Both our candidate extraction and headline generation methods outperform competitive baseline methods, and our model achieves the best results compared with previous state-of-the-art systems. 2 Headline Generation. Given a set of sentences, extractive models exploit sentence compression techniques to generate a final title. Most previous work exploits single-sentence compression (SSC) techniques. Dorr et al. (2003) proposed the Hedge Trimmer algorithm to compress a sentence by making use of handcrafted linguistically-based rules. Alfonseca et al. (2013) introduce a multi-sentence compression (MSC) model into headline generation, using it as a baseline in their work. They indicated that the most important information is distributed across several sentences in the text. 2.2 Candidate Extraction. Different from extractive models, abstractive models exploit phrases as the basic processing units. A set of salient phrases are selected according to specific principles during candidate extraction (Schwartz, 01; Soricut and Marcu, 2007; Xu et al., 2010; Woodsend et al., 2010). Xu et al. (2010) propose to rank phrases using background knowledge extrac"
P15-1045,D10-1050,0,0.138899,"adline generation, combining the advantages of both methods using event structures. Standard evaluation shows that our model achieves the best performance compared with previous state-of-the-art systems. 1 Candidate Extraction Events Phrases Sentences Candidate Ranking Candidate #1 ... Candidate #i ... Candidate #K Multi-Sentence Compression Headline Headline Generation Figure 1: System framework. Zajic et al., 2005). Abstractive models choose a set of informative phrases for candidate extraction, and then exploit sentence synthesis techniques for headline generation (Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010). Extractive HG and abstractive HG have their respective advantages and disadvantages. Extractive models can generate more readable headlines, because the final title is derived by tailoring human-written sentences. However, extractive models give less informative titles (Alfonseca et al., 2013), because sentences are very sparse, making high-recall candidate extraction difficult. In contrast, abstractive models use phrases as the basic processing units, which are much less sparse. However, it is more difficult for abstractive HG to ensure the grammaticality of the generated"
P15-1045,P12-2068,0,0.0198855,"ses, avoiding sparsity and structureless problems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single document using an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path"
P15-1045,P14-2054,0,\N,Missing
P15-1045,R13-1033,0,\N,Missing
P15-1117,D12-1133,0,0.0490078,"h size n, parsing stops after performing exactly 2n − 1 actions. MaltParser uses an SVM classifier for deterministic arc-standard parsing. At each step, MaltParser generates a set of successor states according to the current state, and deterministically selects the highest-scored one as the next state. 2.2 Global Learning and Beam Search The drawback of deterministic parsing is error propagation. An incorrect action will have a negative influence to its subsequent actions, leading to an incorrect output parse tree. To address this issue, global learning and beam search (Zhang and Clark, 2011; Bohnet and Nivre, 2012; Choi and McCallum, 2013) are used. Given an input x, the goal of decoding is to find the highest-scored action sequence globally. y = arg max score(y ′ ) y ′ ∈GEN(x) (1) Where GEN(x) denotes all possible action sequences on x, which correspond to all possible parse trees. The score of an action sequence y is: ∑ score(y) = θ · Φ(a) (2) a∈y Here a is an action in the action sequence y, Φ is a feature function for a, and θ is the parameter vector of the linear model. The score of an action sequence is the linear sum of the scores of each action. During training, action sequence scores are globa"
P15-1117,D14-1082,0,0.123397,"ompetitive greedy neural parser baseline, giving performance comparable to the best linear parser. 1 Introduction Transition-based methods have given competitive accuracies and efficiencies for dependency parsing (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Clark, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011; Goldberg and Nivre, 2013). These parsers construct dependency trees by using a sequence of transition actions, such as SHIFT and REDUCE, over input sentences. High accuracies are achieved by using a linear model and millions of binary indicator features. Recently, Chen and Manning (2014) propose an alternative dependency parser using a neural network, which represents atomic features as dense vectors, and obtains feature combination automatically other than devising high-order features manually. The greedy neural parser of Chen and Manning (2014) gives higher accuracies compared to ∗ Work done while the first author was visiting SUTD. the greedy linear MaltParser (Nivre and Scholz, 2004), but lags behind state-of-the-art linear systems with sparse features (Zhang and Nivre, 2011), which adopt global learning and beam search decoding (Zhang and Nivre, 2012). The key difference"
P15-1117,C14-1078,1,0.675229,"Missing"
P15-1117,P13-1104,0,0.0175535,"Missing"
P15-1117,P04-1015,0,0.959028,"12). The key difference is that Chen and Manning (2014) is a local classifier that greedily optimizes each action. In contrast, Zhang and Nivre (2011) leverage a structured-prediction model to optimize whole sequences of actions, which correspond to tree structures. In this paper, we propose a novel framework for structured neural probabilistic dependency parsing, which maximizes the likelihood of action sequences instead of individual actions. Following Zhang and Clark (2011), beam search is applied to decoding, and global structured learning is integrated with beam search using earlyupdate (Collins and Roark, 2004). Designing such a framework is challenging for two main reasons: First, applying global structured learning to transition-based neural parsing is non-trivial. A direct adaptation of the framework of Zhang and Clark (2011) under the neural probabilistic model setting does not yield good results. The main reason is that the parameter space of a neural network is much denser compared to that of a linear model such as the structured perceptron (Collins, 2002). Due to the dense parameter space, for neural models, the scores of actions in a sequence are relatively more dependent than that in the li"
P15-1117,W02-1001,0,0.857367,"lark (2011), beam search is applied to decoding, and global structured learning is integrated with beam search using earlyupdate (Collins and Roark, 2004). Designing such a framework is challenging for two main reasons: First, applying global structured learning to transition-based neural parsing is non-trivial. A direct adaptation of the framework of Zhang and Clark (2011) under the neural probabilistic model setting does not yield good results. The main reason is that the parameter space of a neural network is much denser compared to that of a linear model such as the structured perceptron (Collins, 2002). Due to the dense parameter space, for neural models, the scores of actions in a sequence are relatively more dependent than that in the linear models. As a result, the log probability of an action sequence can not be modeled just as the sum of log probabilities of each action in the sequence, which is the case of structured linear model. We address the challenge by using a softmax function to directly model the distribution of action sequences. Second, for the structured model above, maximum-likelihood training is computationally intractable, requiring summing over all possible action sequen"
P15-1117,P15-1033,0,0.561306,"Missing"
P15-1117,Q13-1033,0,0.0538833,"Missing"
P15-1117,P04-1013,0,0.426907,"Missing"
P15-1117,P10-1110,0,0.0250292,"word embeddings in supervised training, the embeddings of in-vocabulary words become systematically different from these of out-of-vocabulary words after training, and the effect of pre-trained out-ofvocabulary embeddings become uncertain. In this sense, our model can also be regarded as an almost fully supervised model. The same applies to the models of Chen and Manning (2014). We also compare the speed of the structured neural parser on an Intel Core i7 3.40GHz CPU with 16GB RAM. The structured neural parser runs about as fast as Zhang and Nivre (Zhang and Nivre, 2011) and Huang and Sagae (Huang and Sagae, 2010). The results show that our parser combines the benefits of structured models and neural probabilistic models, offering high accuracies, fast speed and slim model size. 5 Related Work Parsing with neural networks. A line of work has been proposed to explore the effect of neural network models for constituent parsing (Henderson, 2004; Mayberry III and Miikkulainen, 2005; Collobert, 2011; Socher et al., 2013; Legrand and Collobert, 2014). Performances of most of these methods are still well below the state-of-the-art, except for Socher et al.(2013), who propose a neural reranker based on a PCFG"
P15-1117,P14-2128,1,0.864478,"Missing"
P15-1117,J93-2004,0,0.0521026,"sequence as a positive example for parameter update, using the training algorithm of Section 3.3. AdaGrad algorithm (Duchi et al., 2011) with mini-batch is adopted for optimization. In this way, the distribution of ot only full action sequences (i.e. complete parse trees), but also partial action sequences (i.e. partial outputs) are modeled, which makes training more challenging. The advantage of early update is that training is used to guide search, minimizing search errors. 1217 4 Description Baseline Experiments 4.1 Set-up Our experiments are performed using the English Penn Treebank (PTB; Marcus et al., (1993)). We follow the standard splits of PTB3, using sections 2-21 for training, section 22 for development testing and section 23 for final testing. For comparison with previous work, we use Penn2Malt1 to convert constituent trees to dependency trees. We use the POS-tagger of Collins (2002) to assign POS automatically. 10-fold jackknifing is performed for tagging the training data. We follow Chen and Manning (2014), and use the set of pre-trained word embeddings2 from Collobert et al. (2011) with a dictionary size of 13,000. The word embeddings were trained on the entire English Wikipedia, which c"
P15-1117,C04-1010,0,0.155583,"Missing"
P15-1117,J08-4003,0,0.367493,"Missing"
P15-1117,P13-1045,0,0.110444,"Missing"
P15-1117,D09-1058,0,0.019485,"Missing"
P15-1117,P15-1032,0,0.244197,"Missing"
P15-1117,W03-3023,0,0.372233,"Missing"
P15-1117,D08-1059,1,0.847959,"Missing"
P15-1117,J11-1005,1,0.774205,"he-art linear systems with sparse features (Zhang and Nivre, 2011), which adopt global learning and beam search decoding (Zhang and Nivre, 2012). The key difference is that Chen and Manning (2014) is a local classifier that greedily optimizes each action. In contrast, Zhang and Nivre (2011) leverage a structured-prediction model to optimize whole sequences of actions, which correspond to tree structures. In this paper, we propose a novel framework for structured neural probabilistic dependency parsing, which maximizes the likelihood of action sequences instead of individual actions. Following Zhang and Clark (2011), beam search is applied to decoding, and global structured learning is integrated with beam search using earlyupdate (Collins and Roark, 2004). Designing such a framework is challenging for two main reasons: First, applying global structured learning to transition-based neural parsing is non-trivial. A direct adaptation of the framework of Zhang and Clark (2011) under the neural probabilistic model setting does not yield good results. The main reason is that the parameter space of a neural network is much denser compared to that of a linear model such as the structured perceptron (Collins, 20"
P15-1117,P11-2033,1,0.638376,"sed. Given an input x, the goal of decoding is to find the highest-scored action sequence globally. y = arg max score(y ′ ) y ′ ∈GEN(x) (1) Where GEN(x) denotes all possible action sequences on x, which correspond to all possible parse trees. The score of an action sequence y is: ∑ score(y) = θ · Φ(a) (2) a∈y Here a is an action in the action sequence y, Φ is a feature function for a, and θ is the parameter vector of the linear model. The score of an action sequence is the linear sum of the scores of each action. During training, action sequence scores are globally learned. 1214 The parser of Zhang and Nivre (2011) is developed using this framework. The structured perceptron (Collins, 2002) with early update (Collins and Roark, 2004) is applied for training. By utilizing rich manual features, it gives state-of-the-art accuracies in standard Penn Treebank evaluation. We take this method as one baseline. Fw Ft Fl 2.3 Greedy Neural Network Model Chen and Manning (2014) build a greedy neural arc-standard parser. The model can be regarded as an alternative implementation of MaltParser, using a feedforward neural network to replace the SVM classifier for deterministic parsing. 2.3.1 Model The greedy neural mo"
P15-1117,C12-2136,1,0.792212,"eatures. Recently, Chen and Manning (2014) propose an alternative dependency parser using a neural network, which represents atomic features as dense vectors, and obtains feature combination automatically other than devising high-order features manually. The greedy neural parser of Chen and Manning (2014) gives higher accuracies compared to ∗ Work done while the first author was visiting SUTD. the greedy linear MaltParser (Nivre and Scholz, 2004), but lags behind state-of-the-art linear systems with sparse features (Zhang and Nivre, 2011), which adopt global learning and beam search decoding (Zhang and Nivre, 2012). The key difference is that Chen and Manning (2014) is a local classifier that greedily optimizes each action. In contrast, Zhang and Nivre (2011) leverage a structured-prediction model to optimize whole sequences of actions, which correspond to tree structures. In this paper, we propose a novel framework for structured neural probabilistic dependency parsing, which maximizes the likelihood of action sequences instead of individual actions. Following Zhang and Clark (2011), beam search is applied to decoding, and global structured learning is integrated with beam search using earlyupdate (Col"
P15-1117,P08-1068,0,\N,Missing
P16-1033,D14-1108,0,0.0232974,"nnotated sentence, where only the heads of “saw” and “with” are decided. upsurge of web data (e.g., tweets, blogs, and product comments) imposes great challenges to existing parsing techniques. Meanwhile, previous research on out-of-domain dependency parsing gains little success (Dredze et al., 2007; Petrov and McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial"
P16-1033,C10-1011,0,0.131693,"Missing"
P16-1033,W09-2307,0,0.0617144,"Missing"
P16-1033,P10-1001,0,0.0312588,"lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences provide more flexibility in choosing potentially informative structures to annotate. Beyond previous work, this paper endeavors to more thoroughly study this issue, and has made substantial progress from the following perspectives. In this work, we for the first time apply a probabilistic CRF-based pa"
P16-1033,D07-1015,0,0.0416854,"Missing"
P16-1033,N06-1019,0,0.0701527,"Missing"
P16-1033,W08-1301,0,0.158099,"Missing"
P16-1033,C12-2067,0,0.0659251,"Missing"
P16-1033,C14-1075,1,0.946592,"certain metric for AL for sequence labeling problems. In the case of dependency parsing, the marginal probability of a dependency is the sum of probabilities of all legal trees that contain the dependency. ∑ p(h ↷ m|x; w) = p(d|x; w) (4) d∈Y(x):h↷m∈d Intuitively, marginal probability is a more principled metric for measuring reliability of a dependency since it considers all legal parses in the search space, compared to previous methods based on scores of local classifiers (Sassano and Kurohashi, 2010; Flannery and Mori, 2015) or votes of n-best parses (Mirroshandel and Nasr, 2011). Moreover, Li et al. (2014) find strong correlation between marginal probability and correctness of a dependency in cross-lingual syntax projection. Score(x, d∗ ) (5) n1.5 Normalized tree probability. The CRF-based parser allows us, for the first time in AL for dependency parsing, to directly use tree probabilities for uncertainty measurement. Unlike previous approximate methods based on k-best parses (Mirroshandel and Nasr, 2011), tree probabilities globally consider all parse trees in the search space, and thus are intuitively more consistent and proper for measuring the reliability of a tree. Our initial assumption i"
P16-1033,U12-1005,0,0.0219821,"2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They f"
P16-1033,P14-1126,0,0.0479024,"Missing"
P16-1033,W15-2202,0,0.24441,"notation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting"
P16-1033,I11-1087,0,0.0377813,"Missing"
P16-1033,W13-5711,0,0.102764,"arsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences pro"
P16-1033,D14-1097,0,0.0830651,"Missing"
P16-1033,I11-1100,0,0.0281049,"Missing"
P16-1033,P15-1119,1,0.88885,"Missing"
P16-1033,E06-1011,0,0.0810264,"4th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences provide more flexibility in choosing potentially informative structures to annotate. Beyond previous work, this paper endeavors to more thoroughly study this issue, and has made substantial progress from the following perspectives. In this work, we for the first time apply a probabilistic CRF-based parsing model to AL for dependency parsing. We adopt the second-order graphbased model of McDonald and Pereira (2006), which casts the problem as finding an optimal tree from a fully-connect directed graph and factors the score of a dependency tree into scores of pairs of sibling dependencies. (1) This is the first work that applies a stateof-the-art probabilistic parsing model to AL for dependency parsing. The CRF-based dependency parser on the one hand allows us to use probabilities of trees or marginal probabilities of single dependencies for uncertainty measurement, and on the other hand can directly learn parameters from partially annotated trees. Using probabilistic models may be ubiquitous in AL for r"
P16-1033,P99-1010,0,0.894454,"Missing"
P16-1033,W07-2216,0,0.0829738,"Missing"
P16-1033,J04-3001,0,0.119634,"d McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and"
P16-1033,N12-1053,0,0.0241059,"omain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing perf"
P16-1033,C08-1113,0,0.0347094,"Missing"
P16-1033,P15-1134,0,0.075254,"Missing"
P16-1033,D14-1122,0,0.0228308,"where only the heads of “saw” and “with” are decided. upsurge of web data (e.g., tweets, blogs, and product comments) imposes great challenges to existing parsing techniques. Meanwhile, previous research on out-of-domain dependency parsing gains little success (Dredze et al., 2007; Petrov and McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which"
P16-1033,W11-2917,0,0.448477,"es full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units r"
P16-1033,P11-2033,1,0.841112,"ng trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that smaller units rather than sentences provide more flexibility in choosing potentially informative structures to annotate. Beyond previous work, this paper endeavors to more thoroughly study this issue, and has made substantial progress from the following perspectives. In this work, we for the first time apply a probabilistic CRF-based parsing model to AL for de"
P16-1033,P92-1017,0,0.8556,"Missing"
P16-1033,D15-1039,0,0.0453421,"Missing"
P16-1033,P02-1035,0,0.254628,"e, and may be asked to annotate another selected word in the same sentence in next AL iteration. Obviously, frequently switching sentences incurs great waste of cognitive effort, 3.4 Learning from PA A major challenge for AL with PA is how to learn from partially labeled sentences, as depicted in Figure 1. Li et al. (2014) show that a probabilistic CRF-based parser can naturally and effectively learn from PA. The basic idea is converting a partial tree into a forest as shown in Figure 2, 347 and using the forest as the gold-standard reference during training, also known as ambiguous labeling (Riezler et al., 2002; T¨ackstr¨om et al., 2013). For each remaining word without head, we add all dependencies linking to it as long as the new dependency does not violate the existing dependencies. We denote the resulting forest as Fj, whose probability is naturally the sum of probabilities of each tree d in F. ∑ p(F|x; w) = p(d|x; w) d∈F ∑ eScore(x,d;w) = ∑ d∈F Score(x,d′ ;w) d′ ∈Y(x) e Train Chinese English ∑N i=1 log p(Fi |xi ; w) #Sentences 14,304 803 1,910 #Tokens 318,408 20,454 50,319 #Sentences 39,115 1,700 2,416 #Tokens 908,154 40,117 56,684 are selected and annotated at each iteration. In the case of si"
P16-1033,P10-1037,0,0.419473,"09). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; Flannery and Mori, 2015). They find Introduction During the past decade, supervised dependency parsing has gained extensive progress in boosting parsing performance on canonical texts, especially on texts from domains or genres similar to existing manually labeled treebanks (Koo and Collins, 2010; Zhang and Nivre, 2011). However, the ∗ Correspondence author. 344 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 344–354, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Lin"
P16-1033,D08-1112,0,0.139903,"Missing"
P16-1033,D07-1014,0,0.220985,"Missing"
P16-1033,N13-1126,0,0.0792451,"Missing"
P16-1033,P02-1016,0,0.0777853,"l., 2007; Petrov and McDonald, 2012). A more feasible way for open-domain parsing is to manually annotate a certain amount of texts from the target domain or genre. Recently, several small-scale treebanks on web texts have been built for study and evaluation (Foster et al., 2011; Petrov and McDonald, 2012; Kong et al., 2014; Wang et al., 2014). Meanwhile, active learning (AL) aims to reduce annotation effort by choosing and manually annotating unlabeled instances that are most valuable for training statistical models (Olsson, 2009). Traditionally, AL utilizes full annotation (FA) for parsing (Tang et al., 2002; Hwa, 2004; Lynn et al., 2012), where a whole syntactic tree is annotated for a given sentence at a time. However, as commented by Mejer and Crammer (2012), the annotation process is complex, slow, and prone to mistakes when FA is required. Particularly, annotators waste a lot of effort on labeling trivial dependencies which can be well handled by current statistical models (Flannery and Mori, 2015). Recently, researchers report promising results with AL based on partial annotation (PA) for dependency parsing (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Majidi and Crane, 2013; F"
P16-1040,P16-1039,0,0.329018,"with previous work. Finally, we conducted several comparisons to study the differences between our word-based model with character-based neural models, showing that they have different error characteristics. For word-based segmentation, Andrew (2006) used a semi-CRF model to integrate word features, Zhang and Clark (2007) used a perceptron algorithm with inexact search, and Sun et al. (2009) used a discriminative latent variable model to make use of word features. Recently, there have been several neural-based models using word-level embedding features (Morita et al., 2015; Liu et al., 2016; Cai and Zhao, 2016), which are different from our work in the basic framework. For instance, Liu et al. (2016) follow Andrew (2006) using a semi-CRF for structured inference. We followed the global learning and beamsearch framework of Zhang and Clark (2011) in building a word-based neural segmentor. The main difference between our model and that of Zhang and Clark (2011) is that we use a neural network to induce feature combinations directly from character and word embeddings. In addition, the use of a bi-directional LSTM allows us to leverage non-local information from the word sequence, and look-ahead informat"
P16-1040,P15-1167,0,0.371099,"Missing"
P16-1040,P15-1168,0,0.789778,"ubsequent work explores different label sets (Zhao et al., 2006), feature sets (Shi and Wang, 2007) and semi-supervised learning (Sun and Xu, 2011), reporting state-of-the-art accuracies. Recently, neural network models have been investigated for the character tagging approach. The main idea is to replace manual discrete features with automatic real-valued features, which are derived automatically from distributed character representations using neural networks. In particular, convolution neural network1 (Zheng et al., 2013), tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a) and longshort-term-memory (LSTM) (Chen et al., 2015b) have been used to derive neural feature representations from input word sequences, which are fed into a CRF inference layer. In this paper, we investigate the effectiveness of word embedding features for neural network segmentation using transition-based models. Since it is challenging to integrate word features to the CRF inference framework of the existing Introduction Statistical word segmentation methods can be categorized character-based (Xue, 2003; Tseng et al., 2005) and word-based (Andrew, 2006; Zhang and Clark, 2007) approaches."
P16-1040,D15-1141,0,0.273242,"Missing"
P16-1040,I13-1181,0,0.0491099,"s and date/time characters are also differentiated for extracting features. Zheng et al. (2013) built a neural network segmentor, which essentially substitutes the manual discrete features of Peng et al. (2004), with dense real-valued features induced automatically from character embeddings, using a deep neural network structure (Collobert et al., 2011). A tag transition matrix is used for inference, which makes the model effectively. Most subsequent work on neural segmentation followed this method, improving the extraction of emission features by using more complex neural network structures. Mansur et al. (2013) experimented with embeddings of richer features, and in particular characFigure 8: F-measure against character length. we examine the error distribution on individual sentences. Figure 7 shows the F-measure values of each test sentence by word- and characterbased neural models, respectively, where the xaxis value denotes the F-measure value of the word-based neural model, and the y-axis value denotes its performance of the character-based neural model. We can see that the majority scatter points are off the diagonal line, demonstrating strong differences between the two models. This results f"
P16-1040,P04-1015,0,0.0488927,"of “中 国 (Chinese) 外 企 (foreign company) 业 务 (business) 发展 (develop) 迅速 (quickly)”. • Separate (SEP), which moves the first character of the queue onto the buffer as a new (sub) word. character-based methods, we take inspiration from word-based discrete segmentation instead. In particular, we follow Zhang and Clark (2007), using the transition-based framework to decode a sentence from left-to-right incrementally, scoring partially segmented results using both character-level and word-level features. Beam-search is applied to reduce error propagation and large-margin training with early-update (Collins and Roark, 2004) is used for learning from inexact search. We replace the discrete word and character features of Zhang and Clark (2007) with word and character embeddings, respectively, and change their linear model into a deep neural network. Following Zheng et al. (2013) and Chen et al. (2015b), we use convolution neural networks to achieve local feature combination and LSTM to learn global sentence-level features, respectively. The resulting model is a word-based neural segmenter that can leverage rich embedding features. Its correlation with existing work on Chinese segmentation is shown in Figure 1. Res"
P16-1040,D15-1276,0,0.0243707,"that achieved top performances compared with previous work. Finally, we conducted several comparisons to study the differences between our word-based model with character-based neural models, showing that they have different error characteristics. For word-based segmentation, Andrew (2006) used a semi-CRF model to integrate word features, Zhang and Clark (2007) used a perceptron algorithm with inexact search, and Sun et al. (2009) used a discriminative latent variable model to make use of word features. Recently, there have been several neural-based models using word-level embedding features (Morita et al., 2015; Liu et al., 2016; Cai and Zhao, 2016), which are different from our work in the basic framework. For instance, Liu et al. (2016) follow Andrew (2006) using a semi-CRF for structured inference. We followed the global learning and beamsearch framework of Zhang and Clark (2011) in building a word-based neural segmentor. The main difference between our model and that of Zhang and Clark (2011) is that we use a neural network to induce feature combinations directly from character and word embeddings. In addition, the use of a bi-directional LSTM allows us to leverage non-local information from the"
P16-1040,P14-1028,0,0.835364,"ntext window and a twolabel history window. Subsequent work explores different label sets (Zhao et al., 2006), feature sets (Shi and Wang, 2007) and semi-supervised learning (Sun and Xu, 2011), reporting state-of-the-art accuracies. Recently, neural network models have been investigated for the character tagging approach. The main idea is to replace manual discrete features with automatic real-valued features, which are derived automatically from distributed character representations using neural networks. In particular, convolution neural network1 (Zheng et al., 2013), tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a) and longshort-term-memory (LSTM) (Chen et al., 2015b) have been used to derive neural feature representations from input word sequences, which are fed into a CRF inference layer. In this paper, we investigate the effectiveness of word embedding features for neural network segmentation using transition-based models. Since it is challenging to integrate word features to the CRF inference framework of the existing Introduction Statistical word segmentation methods can be categorized character-based (Xue, 2003; Tseng et al., 2005) and word-based (And"
P16-1040,C04-1081,0,0.936237,"is challenging to integrate word features to the CRF inference framework of the existing Introduction Statistical word segmentation methods can be categorized character-based (Xue, 2003; Tseng et al., 2005) and word-based (Andrew, 2006; Zhang and Clark, 2007) approaches. The former casts word segmentation as a sequence labeling problem, using segmentation tags on characters to mark their relative positions inside words. The latter, in contrast, ranks candidate segmented outputs directly, extracting both character and full-word features. An influential character-based word segmentation model (Peng et al., 2004; Tseng et al., 2005) uses B/I/E/S labels to mark a character as the beginning, internal (neither beginning nor end), end and only-character (both beginning and end) of a 1 The term in this paper is used to denote the neural network structure with convolutional layers, which is different from the typical convolution neural network that has a pooling layer upon convolutional layers (Krizhevsky et al., 2012). 421 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 421–431, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguist"
P16-1040,P15-1030,0,0.0204482,"Missing"
P16-1040,P15-1033,0,0.0236858,"ural(-tune) (c) neural(+tune) 0.96 neural Figure 5: Accuracies against the training epoch using beam sizes 1, 2, 4, 8 and 16, respectively. 5.2.1 P 95.21 91.81 94.89 94.93 95.00 Embeddings and beam size 0.92 0.88 0.84 We study the influence of beam size on the baseline and neural models. Our neural model has two choices of using pre-trained word embeddings. We can either fine-tune or fix the embeddings during training. In case of fine-tuning, only words in the training data can be learned, while embeddings of out-of-vocabulary (OOV) words could not be used effectively.5 In addition, following Dyer et al. (2015) we randomly set words with frequency 1 in the training data as the OOV words in order to learn the OOV embedding, while avoiding overfitting. If the pretrained word embeddings are not fine-tuned, we can utilize all word embeddings. Figure 5 shows the development results, where the training curve of the discrete baseline is shown in Figure 5(a) and the curve of the neural model without and with fine tuning are shown in 5(b) and 5(c), respectively. The performance increases with a larger beam size in all settings. When the beam increases into 16, the gains levels out. The results of the discret"
P16-1040,D11-1090,0,0.0881911,"word features lead to comparable performances to the best systems in the literature, and a further combination of discrete and neural features gives top accuracies. 1 Figure 1: Word segmentation methods. word, respectively, employing conditional random field (CRF) to model the correspondence between the input character sequence and output label sequence. For each character, features are extracted from a five-character context window and a twolabel history window. Subsequent work explores different label sets (Zhao et al., 2006), feature sets (Shi and Wang, 2007) and semi-supervised learning (Sun and Xu, 2011), reporting state-of-the-art accuracies. Recently, neural network models have been investigated for the character tagging approach. The main idea is to replace manual discrete features with automatic real-valued features, which are derived automatically from distributed character representations using neural networks. In particular, convolution neural network1 (Zheng et al., 2013), tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a) and longshort-term-memory (LSTM) (Chen et al., 2015b) have been used to derive neural feature representations from input word s"
P16-1040,I05-3017,0,0.513427,"Missing"
P16-1040,N09-1007,0,0.853958,"changes. 426 Models word-based models discrete neural combined character-based models discrete neural combined other models Zhang et al. (2014) Wang et al. (2011) Zhang and Clark (2011) P R F 95.29 95.34 96.11 95.26 94.69 95.79 95.28 95.01 95.95 95.38 94.59 95.63 95.12 94.92 95.60 95.25 94.76 95.61 N/A 95.83 95.46 N/A 95.75 94.78 95.71 95.79 95.13 Models our word-based models discrete neural combined character-based models discrete neural combined other models Cai and Zhao (2016) Ma and Hinrichs (2015) Pei et al. (2014) Zhang et al. (2013a) Sun et al. (2012) Zhang and Clark (2011) Sun (2010) Sun et al. (2009) Table 5: Main results on CTB60 test dataset. 2013; Durrett and Klein, 2015; Zhang and Zhang, 2015). We investigate the usefulness of such integration to our word-based segmentor on the development dataset. We study it by two ways. First, we compare the error distributions between the discrete and the neural models. Intuitively, different error distributions are necessary for improvements by integration. We draw a scatter graph to show their differences, with the (x, y) values of each point denoting the F-measure scores of the two models with respect to sentences, respectively. As shown in Fig"
P16-1040,P12-1027,0,0.0598188,"gth, where the boxes with red dots denote the performances of word-based neural model, and the boxes with blue slant lines denote character-based neural model. 1 word word 1 45 50+ 6 Related Work Xue (2003) was the first to propose a charactertagging method to Chinese word segmentation, using a maximum entropy model to assign B/I/E/S tags to each character in the input sentence separately. Peng et al. (2004) showed that better results can be achieved by global learning using a CRF model. This method has been followed by most subsequent models in the literature (Tseng et al., 2005; Zhao, 2009; Sun et al., 2012). The most effective features have been character unigrams, bigrams and trigrams within a five-character window, and a bigram tag window. Special characters such as alphabets, numbers and date/time characters are also differentiated for extracting features. Zheng et al. (2013) built a neural network segmentor, which essentially substitutes the manual discrete features of Peng et al. (2004), with dense real-valued features induced automatically from character embeddings, using a deep neural network structure (Collobert et al., 2011). A tag transition matrix is used for inference, which makes th"
P16-1040,C10-2139,0,0.601041,"s in little changes. 426 Models word-based models discrete neural combined character-based models discrete neural combined other models Zhang et al. (2014) Wang et al. (2011) Zhang and Clark (2011) P R F 95.29 95.34 96.11 95.26 94.69 95.79 95.28 95.01 95.95 95.38 94.59 95.63 95.12 94.92 95.60 95.25 94.76 95.61 N/A 95.83 95.46 N/A 95.75 94.78 95.71 95.79 95.13 Models our word-based models discrete neural combined character-based models discrete neural combined other models Cai and Zhao (2016) Ma and Hinrichs (2015) Pei et al. (2014) Zhang et al. (2013a) Sun et al. (2012) Zhang and Clark (2011) Sun (2010) Sun et al. (2009) Table 5: Main results on CTB60 test dataset. 2013; Durrett and Klein, 2015; Zhang and Zhang, 2015). We investigate the usefulness of such integration to our word-based segmentor on the development dataset. We study it by two ways. First, we compare the error distributions between the discrete and the neural models. Intuitively, different error distributions are necessary for improvements by integration. We draw a scatter graph to show their differences, with the (x, y) values of each point denoting the F-measure scores of the two models with respect to sentences, respectivel"
P16-1040,I05-3027,0,0.638401,"ensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a) and longshort-term-memory (LSTM) (Chen et al., 2015b) have been used to derive neural feature representations from input word sequences, which are fed into a CRF inference layer. In this paper, we investigate the effectiveness of word embedding features for neural network segmentation using transition-based models. Since it is challenging to integrate word features to the CRF inference framework of the existing Introduction Statistical word segmentation methods can be categorized character-based (Xue, 2003; Tseng et al., 2005) and word-based (Andrew, 2006; Zhang and Clark, 2007) approaches. The former casts word segmentation as a sequence labeling problem, using segmentation tags on characters to mark their relative positions inside words. The latter, in contrast, ranks candidate segmented outputs directly, extracting both character and full-word features. An influential character-based word segmentation model (Peng et al., 2004; Tseng et al., 2005) uses B/I/E/S labels to mark a character as the beginning, internal (neither beginning nor end), end and only-character (both beginning and end) of a 1 The term in this"
P16-1040,P14-1038,0,0.0132384,"s from the differences in feature sources. Second, we study the F-measure distribution of the two neural models with respect to sentence lengths. We divide the test sentences into ten bins, with bin i denoting sentence lengths in [5 ∗ (i − 1), 5 ∗ i]. Figure 8 shows the results. According to the figure, we observe that word-based neural model is relatively weaker for sentences with length in [5, 10], while can better tackle long sentences. Third, we compare the two neural models by their capabilities of modeling words with different lengths. Figure 9 shows the results. The perfor428 traction (Li and Ji, 2014) and the work of joint models (Zhang et al., 2013b; Zhang et al., 2014). Recently, the effectiveness of neural features has been studied for this framework. In the natural language parsing community, it has achieved great success. Representative work includes Zhou et al. (2015), Weiss et al. (2015), Watanabe and Sumita (2015) and Andor et al. (2016). In this work, we apply the transition-based neural framework to Chinese segmentation, in order to exploit wordlevel neural features such as word embeddings. ter bigrams. Pei et al. (2014) used a tensor neural network to achieve extensive feature c"
P16-1040,P10-1040,0,0.0272047,"Missing"
P16-1040,D13-1031,0,0.773659,"ram embeddings, because fine-tuning of these embeddings results in little changes. 426 Models word-based models discrete neural combined character-based models discrete neural combined other models Zhang et al. (2014) Wang et al. (2011) Zhang and Clark (2011) P R F 95.29 95.34 96.11 95.26 94.69 95.79 95.28 95.01 95.95 95.38 94.59 95.63 95.12 94.92 95.60 95.25 94.76 95.61 N/A 95.83 95.46 N/A 95.75 94.78 95.71 95.79 95.13 Models our word-based models discrete neural combined character-based models discrete neural combined other models Cai and Zhao (2016) Ma and Hinrichs (2015) Pei et al. (2014) Zhang et al. (2013a) Sun et al. (2012) Zhang and Clark (2011) Sun (2010) Sun et al. (2009) Table 5: Main results on CTB60 test dataset. 2013; Durrett and Klein, 2015; Zhang and Zhang, 2015). We investigate the usefulness of such integration to our word-based segmentor on the development dataset. We study it by two ways. First, we compare the error distributions between the discrete and the neural models. Intuitively, different error distributions are necessary for improvements by integration. We draw a scatter graph to show their differences, with the (x, y) values of each point denoting the F-measure scores of"
P16-1040,I13-1183,0,0.0195275,"Missing"
P16-1040,P13-1013,1,0.902048,"ram embeddings, because fine-tuning of these embeddings results in little changes. 426 Models word-based models discrete neural combined character-based models discrete neural combined other models Zhang et al. (2014) Wang et al. (2011) Zhang and Clark (2011) P R F 95.29 95.34 96.11 95.26 94.69 95.79 95.28 95.01 95.95 95.38 94.59 95.63 95.12 94.92 95.60 95.25 94.76 95.61 N/A 95.83 95.46 N/A 95.75 94.78 95.71 95.79 95.13 Models our word-based models discrete neural combined character-based models discrete neural combined other models Cai and Zhao (2016) Ma and Hinrichs (2015) Pei et al. (2014) Zhang et al. (2013a) Sun et al. (2012) Zhang and Clark (2011) Sun (2010) Sun et al. (2009) Table 5: Main results on CTB60 test dataset. 2013; Durrett and Klein, 2015; Zhang and Zhang, 2015). We investigate the usefulness of such integration to our word-based segmentor on the development dataset. We study it by two ways. First, we compare the error distributions between the discrete and the neural models. Intuitively, different error distributions are necessary for improvements by integration. We draw a scatter graph to show their differences, with the (x, y) values of each point denoting the F-measure scores of"
P16-1040,P14-1125,1,0.884053,"egrating discrete features Prior work has shown the effectiveness of integrating discrete and neural features for several NLP tasks (Turian et al., 2010; Wang and Manning, 5 6 We perform experiments using random initialized word embeddings as well when fine-tune is used, which is a fully supervised model. The performance is slightly lower. In all our experiments, we fix the character unigram and bigram embeddings, because fine-tuning of these embeddings results in little changes. 426 Models word-based models discrete neural combined character-based models discrete neural combined other models Zhang et al. (2014) Wang et al. (2011) Zhang and Clark (2011) P R F 95.29 95.34 96.11 95.26 94.69 95.79 95.28 95.01 95.95 95.38 94.59 95.63 95.12 94.92 95.60 95.25 94.76 95.61 N/A 95.83 95.46 N/A 95.75 94.78 95.71 95.79 95.13 Models our word-based models discrete neural combined character-based models discrete neural combined other models Cai and Zhao (2016) Ma and Hinrichs (2015) Pei et al. (2014) Zhang et al. (2013a) Sun et al. (2012) Zhang and Clark (2011) Sun (2010) Sun et al. (2009) Table 5: Main results on CTB60 test dataset. 2013; Durrett and Klein, 2015; Zhang and Zhang, 2015). We investigate the usefuln"
P16-1040,I11-1035,0,0.0492711,"atures Prior work has shown the effectiveness of integrating discrete and neural features for several NLP tasks (Turian et al., 2010; Wang and Manning, 5 6 We perform experiments using random initialized word embeddings as well when fine-tune is used, which is a fully supervised model. The performance is slightly lower. In all our experiments, we fix the character unigram and bigram embeddings, because fine-tuning of these embeddings results in little changes. 426 Models word-based models discrete neural combined character-based models discrete neural combined other models Zhang et al. (2014) Wang et al. (2011) Zhang and Clark (2011) P R F 95.29 95.34 96.11 95.26 94.69 95.79 95.28 95.01 95.95 95.38 94.59 95.63 95.12 94.92 95.60 95.25 94.76 95.61 N/A 95.83 95.46 N/A 95.75 94.78 95.71 95.79 95.13 Models our word-based models discrete neural combined character-based models discrete neural combined other models Cai and Zhao (2016) Ma and Hinrichs (2015) Pei et al. (2014) Zhang et al. (2013a) Sun et al. (2012) Zhang and Clark (2011) Sun (2010) Sun et al. (2009) Table 5: Main results on CTB60 test dataset. 2013; Durrett and Klein, 2015; Zhang and Zhang, 2015). We investigate the usefulness of such integra"
P16-1040,Y06-1012,0,0.241425,"atures in a word-based segmentation framework. Experimental results demonstrate that word features lead to comparable performances to the best systems in the literature, and a further combination of discrete and neural features gives top accuracies. 1 Figure 1: Word segmentation methods. word, respectively, employing conditional random field (CRF) to model the correspondence between the input character sequence and output label sequence. For each character, features are extracted from a five-character context window and a twolabel history window. Subsequent work explores different label sets (Zhao et al., 2006), feature sets (Shi and Wang, 2007) and semi-supervised learning (Sun and Xu, 2011), reporting state-of-the-art accuracies. Recently, neural network models have been investigated for the character tagging approach. The main idea is to replace manual discrete features with automatic real-valued features, which are derived automatically from distributed character representations using neural networks. In particular, convolution neural network1 (Zheng et al., 2013), tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a) and longshort-term-memory (LSTM) (Chen et al"
P16-1040,P14-2032,0,0.196418,"ain difference between our model and that of Zhang and Clark (2011) is that we use a neural network to induce feature combinations directly from character and word embeddings. In addition, the use of a bi-directional LSTM allows us to leverage non-local information from the word sequence, and look-ahead information from the incoming character sequence. The automatic neural features are complementary to the manual discrete features of Zhang and Clark (2011). We show that our model can accommodate the integration of both types of features. This is similar in spirit to the work of Sun (2010) and Wang et al. (2014), who integrated features of character-based and word-based segmentors. Acknowledgments We thank the anonymous reviewers, Yijia Liu and Hai Zhao for their constructive comments, which help to improve the final paper. This work is supported by National Natural Science Foundation of China (NSFC) under grant 61170148, Natural Science Foundation of Heilongjiang Province (China) under grant No.F2016036, the Singapore Ministry of Education (MOE) AcRF Tier 2 grant T2MOE201301 and SRG ISTD 2012 038 from Singapore University of Technology and Design. Yue Zhang is the corresponding author. References Da"
P16-1040,E09-1100,0,0.0124881,"nst word length, where the boxes with red dots denote the performances of word-based neural model, and the boxes with blue slant lines denote character-based neural model. 1 word word 1 45 50+ 6 Related Work Xue (2003) was the first to propose a charactertagging method to Chinese word segmentation, using a maximum entropy model to assign B/I/E/S tags to each character in the input sentence separately. Peng et al. (2004) showed that better results can be achieved by global learning using a CRF model. This method has been followed by most subsequent models in the literature (Tseng et al., 2005; Zhao, 2009; Sun et al., 2012). The most effective features have been character unigrams, bigrams and trigrams within a five-character window, and a bigram tag window. Special characters such as alphabets, numbers and date/time characters are also differentiated for extracting features. Zheng et al. (2013) built a neural network segmentor, which essentially substitutes the manual discrete features of Peng et al. (2004), with dense real-valued features induced automatically from character embeddings, using a deep neural network structure (Collobert et al., 2011). A tag transition matrix is used for infere"
P16-1040,D13-1061,0,0.36697,"tures are extracted from a five-character context window and a twolabel history window. Subsequent work explores different label sets (Zhao et al., 2006), feature sets (Shi and Wang, 2007) and semi-supervised learning (Sun and Xu, 2011), reporting state-of-the-art accuracies. Recently, neural network models have been investigated for the character tagging approach. The main idea is to replace manual discrete features with automatic real-valued features, which are derived automatically from distributed character representations using neural networks. In particular, convolution neural network1 (Zheng et al., 2013), tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a) and longshort-term-memory (LSTM) (Chen et al., 2015b) have been used to derive neural feature representations from input word sequences, which are fed into a CRF inference layer. In this paper, we investigate the effectiveness of word embedding features for neural network segmentation using transition-based models. Since it is challenging to integrate word features to the CRF inference framework of the existing Introduction Statistical word segmentation methods can be categorized character-based (Xue, 200"
P16-1040,P15-1113,0,0.0210521,"hat word-based neural model is relatively weaker for sentences with length in [5, 10], while can better tackle long sentences. Third, we compare the two neural models by their capabilities of modeling words with different lengths. Figure 9 shows the results. The perfor428 traction (Li and Ji, 2014) and the work of joint models (Zhang et al., 2013b; Zhang et al., 2014). Recently, the effectiveness of neural features has been studied for this framework. In the natural language parsing community, it has achieved great success. Representative work includes Zhou et al. (2015), Weiss et al. (2015), Watanabe and Sumita (2015) and Andor et al. (2016). In this work, we apply the transition-based neural framework to Chinese segmentation, in order to exploit wordlevel neural features such as word embeddings. ter bigrams. Pei et al. (2014) used a tensor neural network to achieve extensive feature combinations, capturing the interaction between characters and tags. Chen et al. (2015a) used a recursive network structure to the same end, extracting more combined features to model complicated character combinations in a five-character window. Chen et al. (2015b) used a LSTM model to capture long-range dependencies between"
P16-1040,P15-1032,0,0.0189251,"figure, we observe that word-based neural model is relatively weaker for sentences with length in [5, 10], while can better tackle long sentences. Third, we compare the two neural models by their capabilities of modeling words with different lengths. Figure 9 shows the results. The perfor428 traction (Li and Ji, 2014) and the work of joint models (Zhang et al., 2013b; Zhang et al., 2014). Recently, the effectiveness of neural features has been studied for this framework. In the natural language parsing community, it has achieved great success. Representative work includes Zhou et al. (2015), Weiss et al. (2015), Watanabe and Sumita (2015) and Andor et al. (2016). In this work, we apply the transition-based neural framework to Chinese segmentation, in order to exploit wordlevel neural features such as word embeddings. ter bigrams. Pei et al. (2014) used a tensor neural network to achieve extensive feature combinations, capturing the interaction between characters and tags. Chen et al. (2015a) used a recursive network structure to the same end, extracting more combined features to model complicated character combinations in a five-character window. Chen et al. (2015b) used a LSTM model to capture long"
P16-1040,P15-1117,1,0.444949,"rization parameter and η is used to tune the loss margins. For the discrete models, f (·) denotes the features extracted according to the feature templates in Table 1. For the neural models, f (·) denotes the corresponding hsep and happ . Thus only the output layer is updated, and we further use backpropagation to learn the parameters of the other layers (LeCun et al., 2012). We use online Ada5.2 Development Results To better understand the word-based neural models, we perform several development experiments. All the experiments in this section are conducted on the CTB6 development dataset. 3 Zhou et al. (2015) find that max-margin training did not yield reasonable results for neural transition-based parsing, which is different from our findings. One likely reason is that when the number of labels is small max-margin is effective. 4 425 http://word2vec.googlecode.com/ b16 b8 b4 b2 Model neural -word -character unigram -character bigram -action +discrete features (combined) b1 96 94 92 90 88 R 95.69 92.00 95.56 95.53 95.31 F 95.45 91.90 95.22 95.23 95.17 96.38 96.22 96.30 Table 4: Feature experiments. 86 5 10 15 (a) discrete 20 5 10 15 20 5 10 15 20 1 (b) neural(-tune) (c) neural(+tune) 0.96 neural F"
P16-1040,P16-2092,0,0.353963,"In this work, we apply the transition-based neural framework to Chinese segmentation, in order to exploit wordlevel neural features such as word embeddings. ter bigrams. Pei et al. (2014) used a tensor neural network to achieve extensive feature combinations, capturing the interaction between characters and tags. Chen et al. (2015a) used a recursive network structure to the same end, extracting more combined features to model complicated character combinations in a five-character window. Chen et al. (2015b) used a LSTM model to capture long-range dependencies between characters in a sentence. Xu and Sun (2016) proposed a dependency-based gated recursive neural network to efficiently integrate local and long-distance features. The above methods are all character-based models, making no use of full word information. In contrast, we leverage both character embeddings and word embeddings for better accuracies. 7 Conclusion We proposed a word-based neural model for Chinese segmentation, which exploits not only character embeddings as previous work does, but also word embeddings pre-trained from large scale corpus. The model achieved comparable performances compared with a discrete word-based baseline, a"
P16-1040,P13-1043,1,0.903254,"Missing"
P16-1040,O03-4002,0,0.954327,"., 2013), tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a) and longshort-term-memory (LSTM) (Chen et al., 2015b) have been used to derive neural feature representations from input word sequences, which are fed into a CRF inference layer. In this paper, we investigate the effectiveness of word embedding features for neural network segmentation using transition-based models. Since it is challenging to integrate word features to the CRF inference framework of the existing Introduction Statistical word segmentation methods can be categorized character-based (Xue, 2003; Tseng et al., 2005) and word-based (Andrew, 2006; Zhang and Clark, 2007) approaches. The former casts word segmentation as a sequence labeling problem, using segmentation tags on characters to mark their relative positions inside words. The latter, in contrast, ranks candidate segmented outputs directly, extracting both character and full-word features. An influential character-based word segmentation model (Peng et al., 2004; Tseng et al., 2005) uses B/I/E/S labels to mark a character as the beginning, internal (neither beginning nor end), end and only-character (both beginning and end) of"
P16-1040,P07-1106,1,0.729273,"neural network (Chen et al., 2015a) and longshort-term-memory (LSTM) (Chen et al., 2015b) have been used to derive neural feature representations from input word sequences, which are fed into a CRF inference layer. In this paper, we investigate the effectiveness of word embedding features for neural network segmentation using transition-based models. Since it is challenging to integrate word features to the CRF inference framework of the existing Introduction Statistical word segmentation methods can be categorized character-based (Xue, 2003; Tseng et al., 2005) and word-based (Andrew, 2006; Zhang and Clark, 2007) approaches. The former casts word segmentation as a sequence labeling problem, using segmentation tags on characters to mark their relative positions inside words. The latter, in contrast, ranks candidate segmented outputs directly, extracting both character and full-word features. An influential character-based word segmentation model (Peng et al., 2004; Tseng et al., 2005) uses B/I/E/S labels to mark a character as the beginning, internal (neither beginning nor end), end and only-character (both beginning and end) of a 1 The term in this paper is used to denote the neural network structure"
P16-1040,J11-1005,1,0.863676,"sequence “SEP APP SEP APP SEP APP SEP APP SEP APP”, as shown in Figure 2. Search. Based on the transition system, the decoder searches for an optimal action sequence for a given sentence. Denote an action sequence as A = a1 · · · an . We define the score of A as the total score of all actions in the sequence, which is computed by: X X score(A) = score(a) = w · f (s, q, a), a∈A where w is the model parameters, f is a feature extraction function, s and q are the buffer and queue of a certain state before the action a is applied. The feature templates are shown in Table 1, which are the same as Zhang and Clark (2011). These base features include three main source of information. First, characters in the front of the queue and the end of the buffer are used for scoring both separate and append actions (e.g. c0 ). Second, words that are identified are used to guide separate actions (e.g. w0 ). Third, relevant information of identified words, such as their lengths and first/last characters are utilized for additional features (e.g. len(w−1 )). We follow Zhang and Clark (2011) in using beam-search for decoding, shown in Algorith 1, where Θ is the set of model parameters. Initially the beam contains only the i"
P16-1040,P11-2033,1,0.479981,"Missing"
P16-1040,D15-1153,1,0.831268,"Missing"
P16-1040,P16-1231,0,\N,Missing
P16-1132,D15-1041,0,0.0171462,"speed. As a local and greedy neural baseline, it does not outperform the best discrete-feature parsers, but nevertheless demonstrates strong potentials for neural network models in transition-based dependency parsing. Subsequent work aimed to improve the model of Chen and Manning (2014) in two main directions. First, global optimization learning and beam search inference have been exploited to reduce error propagation (Weiss et al., 2015; Zhou et al., 2015). Second, recurrent neural network models have been used to extend the range of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been use"
P16-1132,P05-1022,0,0.125056,"of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the"
P16-1132,D14-1082,0,0.694626,"s0 ], j is the head of the queue (i.e. [ q0 = wj , q1 = wj+1 · · · ]), and L is a set of dependency arcs that has been built. At each step, the parser chooses one of the following actions: • S HIFT (S): move the front word wj from the queue onto the stacks. • L EFT-l (L): add an arc with label l between the top two trees on the stack (s1 ← s0 ), and remove s1 from the stack. • R IGHT-l (R): add an arc with label l between the top two trees on the stack (s1 → s0 ), and remove s0 from the stack. Given the sentence “John loves Mary”, the gold standard action sequence is S, S, L, S, R. 2.1 Model Chen and Manning (2014) proposed a deterministic neural dependency parser, which rely on dense embeddings to predict the optimal actions at each step. We propose a variation of Chen and Manning 1394 (2014), which splits the output layer into two hierarchical layers: the action layer and dependency label layer. The hierarchical parser determines a action in two steps, first deciding the action type, and then the dependency label (Figure 2). At each step of deterministic parsing, the neural model extracts n atomic features from the parsing state. We adopt the feature templates of Chen and Manning (2014). Every atomic"
P16-1132,D15-1215,0,0.0311275,"Missing"
P16-1132,W02-1001,0,0.0460083,"ee with the best heuristic reranking score yˆi0 . J(Θh ) = 1 |Dh | X (xi ,yi0 ,ˆ yi0 )∈Dh ri (Θh ) = ri (Θh ) + λ ||Θh || 2 max(0, st (xi , yˆi0 , Θh )) − st (xi , yi0 , Θh ) (20) (21) The detailed training algorithm is given by Algorithm 1. AdaGrad (Duchi et al., 2011) updating with subgradient (Ratliff et al., 2007) and minibatch is adopted for optimization. 5 5.1 section 23 for final testing. Following prior work on reranking, we use Penn2Malt1 to convert constituent trees to dependency trees. Ten-fold POS jackknifing is used in the training of the baseline parser. We use the POS-tagger of Collins (2002) to assign POS automatically. Because our reranking model is a dynamic reranking model, which generates training instances during search, we train 10 baseline parsing models on the 10-fold jackknifing data, and load the baseline parser model dynamically for reranking training . We follow Chen and Manning (2014), using the set of pre-trained word embeddings with a dictionary size of 13,0002 from Collobert et al. (2011). The word embeddings were trained on the entire English Wikipedia, which contains about 631 million words. 5.2 There are two different networks in our system, namely a hierarchic"
P16-1132,P15-1033,0,0.0578806,"curacies and faster speed. As a local and greedy neural baseline, it does not outperform the best discrete-feature parsers, but nevertheless demonstrates strong potentials for neural network models in transition-based dependency parsing. Subsequent work aimed to improve the model of Chen and Manning (2014) in two main directions. First, global optimization learning and beam search inference have been exploited to reduce error propagation (Weiss et al., 2015; Zhou et al., 2015). Second, recurrent neural network models have been used to extend the range of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural n"
P16-1132,P13-2111,0,0.0549975,"Missing"
P16-1132,J08-4003,0,0.0982509,"Missing"
P16-1132,D11-1137,0,0.0521305,"Missing"
P16-1132,Q13-1012,0,0.383871,"the best tree output. Denote b(i) as the beam at i-th step of search, k-best candidates in the beam of i + 1 step is: b(i + 1) = arg K (st (x, c, Θh ) + sb (x, c)), (14) c∈c(i) where c(i) denotes the set of newly constructed trees by revising trees in b(i), sb (x, c) is the baseline model score and arg K leaves the k best candidate trees to the next beam. Finally, the output tree yi of reranking is selected from all searched trees C in the revising process yi = arg max(st (x, c, Θc ) + sb (x, c)) c∈C (15) Interpolated Reranker In testing, we also adopt the popular mixture reranking strategy (Hayashi et al., 2013; Le and Mikolov, 2014), which obtains better reranking performance by a linear combination of the reranking score and the baseline model score. yi = arg max (β(st (xi , y, Θc ) + st (x, y, Θh )) y∈τ (xi ) + (1 − β)sb (xi , y)) (16) Here yi is the final output tree for a sentence xi ; τ (xi ) returns all the trees candidates of the dynamic reranking; β ∈[0, 1] is a hyper-parameter. 4.4 Training As k-best neural rerankers (Socher et al., 2013; Zhu et al., 2015), we use the max-margin criterion to train our model in a stage-wise manner (Doppa et al., 2013). Given training data Dc = (xi , yi , yˆ"
P16-1132,W05-1506,0,0.0964815,"rerankers are capable of capturing global syntax features across the tree. In contrast, the most non-local neural parser with LSTM (Dyer et al., 2015) cannot exploit global features. Different to previous neural rerankers, our work in this paper contributes on integrating search and learning for reranking, instead of proposing a new neural model. Forest Reranking Forest reranking (Huang, 2008; Hayashi et al., 2013) offers a different way to extend the coverage of reranking candidates, with computing the reranking score in the trees forests by decomposing non-local features with cube-pruning (Huang and Chiang, 2005). In contrast, the neural reranking score encodes the whole dependency tree, which cannot be decomposed for forest reranking efficiently and accurately. HC-Search Doppa et al. (2013) proposed a structured prediction model with HC-Search strategy and imitation learning, which is closely related to our work in spirit. They used the complete space search (Doppa et al., 2012) for sequence labeling tasks, and the whole search process halts after a specific time bound. Different from them, we propose a dynamic parsing reranking model based on the action revising process, which is a multi-step proces"
P16-1132,P08-1067,0,0.601351,"local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the Association fo"
P16-1132,D14-1081,0,0.0457333,"literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1393–1402, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics have rat"
P16-1132,P15-1031,0,0.0450277,"Missing"
P16-1132,P13-1045,0,0.206611,"ve accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1393–1402, c Berlin, Germany, August 7-12, 2016."
P16-1132,P15-1032,0,0.0169178,"by replacing the SVM classifier at the transition-based MaltParser (Nivre et al., 2007) with a feed-forward neural network, achieving significantly higher accuracies and faster speed. As a local and greedy neural baseline, it does not outperform the best discrete-feature parsers, but nevertheless demonstrates strong potentials for neural network models in transition-based dependency parsing. Subsequent work aimed to improve the model of Chen and Manning (2014) in two main directions. First, global optimization learning and beam search inference have been exploited to reduce error propagation (Weiss et al., 2015; Zhou et al., 2015). Second, recurrent neural network models have been used to extend the range of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively"
P16-1132,P15-1117,1,0.854947,"classifier at the transition-based MaltParser (Nivre et al., 2007) with a feed-forward neural network, achieving significantly higher accuracies and faster speed. As a local and greedy neural baseline, it does not outperform the best discrete-feature parsers, but nevertheless demonstrates strong potentials for neural network models in transition-based dependency parsing. Subsequent work aimed to improve the model of Chen and Manning (2014) in two main directions. First, global optimization learning and beam search inference have been exploited to reduce error propagation (Weiss et al., 2015; Zhou et al., 2015). Second, recurrent neural network models have been used to extend the range of neural features beyond a local window (Dyer et al., 2015; Ballesteros et al., 2015). These methods give accuracies that are competitive to the best results in the literature. S S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another"
P16-1132,P15-1112,0,0.250311,"S 0.5 R Revise 2 1.0 S 0.3 L 1.0 S 0.5 L L 0.9 L Revise 1 Base Tree 1 L 1.0 S 0.4 S 0.5 (d) 2-step action revising process for sentence “John loves Mary”. Numbers before actions are the probabilities for that action. Figure 1: Example action revising process. S, L, R stand for the SHIFT, LEFT, RIGHT actions, respectively (Section 2). Another direction to extend a baseline parser is reranking (Collins and Koo, 2000; Charniak and Johnson, 2005; Huang, 2008). Recently, neural network models have been used to constituent (Socher et al., 2013; Le et al., 2013) and dependency (Le and Zuidema, 2014; Zhu et al., 2015) parsing reranking. Compared with rerankers that rely on discrete manual features, neural network rerankers can potentially capture more global information over whole parse trees. Traditional rerankers are based on chart parsers, which can yield exact k-best lists and forests. For reranking, this is infeasible for the transitionbased neural parser and neural reranker, which 1393 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1393–1402, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics have rather weak feature lo"
P16-1132,J93-2004,0,0.0549301,"Missing"
P16-2036,W11-0705,0,0.0192151,"accianella et al., 2010; Mohammad et al., 2013). Such methods leverage knowledge resources (Bravo-Marquez et al., 2015) or labeled sentiment data (Tang et al., 2014a), giving significantly better coverage compared to manual lexicons. Among the automatic methods, Mohammad et al. (2013) proposed to use tweets with emoticons or hashtags as training data. The main advantage is that such training data are abundant, and manual annotation can be avoided. Despite that emoticons or hashtags can be noisy in indicating the sentiment of a tweet, existing research (Go et al., 2009; Pak and Paroubek, 2010; Agarwal et al., 2011; Kalchbrenner et al., 2014; Ren et al., 2016b) has shown that effectiveness of such data when used to supervise sentiment classifiers. Mohammad et al. (2013) collect sentiment lexicons by calculating pointwise mutual information (PMI) between words and emoticons. The resulting lexicons give the best results in a SemEval13 benchmark (Nakov et al., 2013). In this paper, we show that a better lexicon can be learned by directly optimizing the prediction accuracy, taking the lexicon as input and emoticon as the output. The correlation between our method and the method of Mohammad et al. (2013) is"
P16-2036,S13-2053,0,0.163108,"rning Sentiment Lexicons for Short Text Duy Tin Vo and Yue Zhang Singapore University of Technology and Design 08 Somapah Road, Singapore 487372 duytin vo@mymail.sutd.edu.sg and yue zhang@sutd.edu.sg Abstract tise. Recently, statistical methods have been exploited to learn sentiment lexicons automatically (Esuli and Sebastiani, 2006; Baccianella et al., 2010; Mohammad et al., 2013). Such methods leverage knowledge resources (Bravo-Marquez et al., 2015) or labeled sentiment data (Tang et al., 2014a), giving significantly better coverage compared to manual lexicons. Among the automatic methods, Mohammad et al. (2013) proposed to use tweets with emoticons or hashtags as training data. The main advantage is that such training data are abundant, and manual annotation can be avoided. Despite that emoticons or hashtags can be noisy in indicating the sentiment of a tweet, existing research (Go et al., 2009; Pak and Paroubek, 2010; Agarwal et al., 2011; Kalchbrenner et al., 2014; Ren et al., 2016b) has shown that effectiveness of such data when used to supervise sentiment classifiers. Mohammad et al. (2013) collect sentiment lexicons by calculating pointwise mutual information (PMI) between words and emoticons."
P16-2036,baccianella-etal-2010-sentiwordnet,0,0.194506,"val13 by leveraging emoticons in large tweets, using the PMI between words and tweet sentiments to define the sentiment attributes of words. We show that better lexicons can be learned by using them to predict the tweet sentiment labels. By using a very simple neural network, our method is fast and can take advantage of the same data volume as the NRC method. Experiments show that our lexicons give significantly better accuracies on multiple languages compared to the current best methods. 1 Introduction Sentiment lexicons contain the sentiment polarity and/or the strength of words or phrases (Baccianella et al., 2010; Taboada et al., 2011; Tang et al., 2014a; Ren et al., 2016a). They have been used for both rule-based (Taboada et al., 2011) and unsupervised (Turney, 2002; Hu and Liu, 2004; Kiritchenko et al., 2014) or supervised (Mohammad et al., 2013; Tang et al., 2014b; Vo and Zhang, 2015) machine-learning-based sentiment analysis. As a result, constructing sentiment lexicons is one important research task in sentiment analysis. Many approaches have been proposed to construct sentiment lexicons. Traditional methods manually label the sentiment attributes of words (Hu and Liu, 2004; Wilson et al., 2005;"
P16-2036,P14-1023,0,0.0315074,"iment classifiers. Mohammad et al. (2013) collect sentiment lexicons by calculating pointwise mutual information (PMI) between words and emoticons. The resulting lexicons give the best results in a SemEval13 benchmark (Nakov et al., 2013). In this paper, we show that a better lexicon can be learned by directly optimizing the prediction accuracy, taking the lexicon as input and emoticon as the output. The correlation between our method and the method of Mohammad et al. (2013) is analogous to the “predicting” vs “counting” correlation between distributional and distributed word representations (Baroni et al., 2014). We follow Esuli and Sebastiani (2006) in using two simple attributes to represent each sentiment word, and take inspiration from Mikolov et al. (2013) in using a very simple neural network for sentiment prediction. The method can leverage the same data as Mohammad et al. (2013) and therefore benefits from both scale and annotation independence. Experiments show We describe an efficient neural network method to automatically learn sentiment lexicons without relying on any manual resources. The method takes inspiration from the NRC method, which gives the best results in SemEval13 by leveragin"
P16-2036,D15-1299,0,0.00769741,"Missing"
P16-2036,S13-2052,0,0.04396,"e main advantage is that such training data are abundant, and manual annotation can be avoided. Despite that emoticons or hashtags can be noisy in indicating the sentiment of a tweet, existing research (Go et al., 2009; Pak and Paroubek, 2010; Agarwal et al., 2011; Kalchbrenner et al., 2014; Ren et al., 2016b) has shown that effectiveness of such data when used to supervise sentiment classifiers. Mohammad et al. (2013) collect sentiment lexicons by calculating pointwise mutual information (PMI) between words and emoticons. The resulting lexicons give the best results in a SemEval13 benchmark (Nakov et al., 2013). In this paper, we show that a better lexicon can be learned by directly optimizing the prediction accuracy, taking the lexicon as input and emoticon as the output. The correlation between our method and the method of Mohammad et al. (2013) is analogous to the “predicting” vs “counting” correlation between distributional and distributed word representations (Baroni et al., 2014). We follow Esuli and Sebastiani (2006) in using two simple attributes to represent each sentiment word, and take inspiration from Mikolov et al. (2013) in using a very simple neural network for sentiment prediction. T"
P16-2036,esuli-sebastiani-2006-sentiwordnet,0,0.174592,"l. (2013) collect sentiment lexicons by calculating pointwise mutual information (PMI) between words and emoticons. The resulting lexicons give the best results in a SemEval13 benchmark (Nakov et al., 2013). In this paper, we show that a better lexicon can be learned by directly optimizing the prediction accuracy, taking the lexicon as input and emoticon as the output. The correlation between our method and the method of Mohammad et al. (2013) is analogous to the “predicting” vs “counting” correlation between distributional and distributed word representations (Baroni et al., 2014). We follow Esuli and Sebastiani (2006) in using two simple attributes to represent each sentiment word, and take inspiration from Mikolov et al. (2013) in using a very simple neural network for sentiment prediction. The method can leverage the same data as Mohammad et al. (2013) and therefore benefits from both scale and annotation independence. Experiments show We describe an efficient neural network method to automatically learn sentiment lexicons without relying on any manual resources. The method takes inspiration from the NRC method, which gives the best results in SemEval13 by leveraging emoticons in large tweets, using the"
P16-2036,S10-1097,0,0.0135654,"and Sebastiani, 2006; Baccianella et al., 2010; Mohammad et al., 2013). Such methods leverage knowledge resources (Bravo-Marquez et al., 2015) or labeled sentiment data (Tang et al., 2014a), giving significantly better coverage compared to manual lexicons. Among the automatic methods, Mohammad et al. (2013) proposed to use tweets with emoticons or hashtags as training data. The main advantage is that such training data are abundant, and manual annotation can be avoided. Despite that emoticons or hashtags can be noisy in indicating the sentiment of a tweet, existing research (Go et al., 2009; Pak and Paroubek, 2010; Agarwal et al., 2011; Kalchbrenner et al., 2014; Ren et al., 2016b) has shown that effectiveness of such data when used to supervise sentiment classifiers. Mohammad et al. (2013) collect sentiment lexicons by calculating pointwise mutual information (PMI) between words and emoticons. The resulting lexicons give the best results in a SemEval13 benchmark (Nakov et al., 2013). In this paper, we show that a better lexicon can be learned by directly optimizing the prediction accuracy, taking the lexicon as input and emoticon as the output. The correlation between our method and the method of Moha"
P16-2036,P11-2008,0,0.0191162,"Missing"
P16-2036,N15-1078,0,0.0273469,"by classifying words using manual features. These methods are also limited to domains and languages with manual resources. The third line of methods constructs lexicons from scratch by accumulating statistical information over large data. Turney (2002) proposes to estimate the sentiment polarity of words by calculating PMI between seed words and search hits. Mohammad et al. (2013) improve the method by computing sentiment scores using distance-supervised data from emoticon-baring tweets instead of seed words. This approach can be used to automatically extract multilingual sentiment lexicons (Salameh et al., 2015; Mohammad et al., 2015) without using manual resources, which makes it more flexible compared to the first two methods. We consider it as our baseline. We use the same data source as Mohammad et al. (2013) to train lexicons. However, rather than relying on PMI, we take a machine-learning method in optimizing the prediction accuracy of emoticons using the lexicons. To leverage large data, we use a very simple neural network to train the lexicons. Figure 1: Our model. 3 Baseline Mohammad et al. (2013) employ emoticons and relevant hashtags contained in a tweet as the sentiment label of the twee"
P16-2036,J11-2001,0,0.334851,"ons in large tweets, using the PMI between words and tweet sentiments to define the sentiment attributes of words. We show that better lexicons can be learned by using them to predict the tweet sentiment labels. By using a very simple neural network, our method is fast and can take advantage of the same data volume as the NRC method. Experiments show that our lexicons give significantly better accuracies on multiple languages compared to the current best methods. 1 Introduction Sentiment lexicons contain the sentiment polarity and/or the strength of words or phrases (Baccianella et al., 2010; Taboada et al., 2011; Tang et al., 2014a; Ren et al., 2016a). They have been used for both rule-based (Taboada et al., 2011) and unsupervised (Turney, 2002; Hu and Liu, 2004; Kiritchenko et al., 2014) or supervised (Mohammad et al., 2013; Tang et al., 2014b; Vo and Zhang, 2015) machine-learning-based sentiment analysis. As a result, constructing sentiment lexicons is one important research task in sentiment analysis. Many approaches have been proposed to construct sentiment lexicons. Traditional methods manually label the sentiment attributes of words (Hu and Liu, 2004; Wilson et al., 2005; Taboada et al., 2011)."
P16-2036,C14-1018,0,0.166706,"sing the PMI between words and tweet sentiments to define the sentiment attributes of words. We show that better lexicons can be learned by using them to predict the tweet sentiment labels. By using a very simple neural network, our method is fast and can take advantage of the same data volume as the NRC method. Experiments show that our lexicons give significantly better accuracies on multiple languages compared to the current best methods. 1 Introduction Sentiment lexicons contain the sentiment polarity and/or the strength of words or phrases (Baccianella et al., 2010; Taboada et al., 2011; Tang et al., 2014a; Ren et al., 2016a). They have been used for both rule-based (Taboada et al., 2011) and unsupervised (Turney, 2002; Hu and Liu, 2004; Kiritchenko et al., 2014) or supervised (Mohammad et al., 2013; Tang et al., 2014b; Vo and Zhang, 2015) machine-learning-based sentiment analysis. As a result, constructing sentiment lexicons is one important research task in sentiment analysis. Many approaches have been proposed to construct sentiment lexicons. Traditional methods manually label the sentiment attributes of words (Hu and Liu, 2004; Wilson et al., 2005; Taboada et al., 2011). One benefit of suc"
P16-2036,P14-1146,0,0.510521,"sing the PMI between words and tweet sentiments to define the sentiment attributes of words. We show that better lexicons can be learned by using them to predict the tweet sentiment labels. By using a very simple neural network, our method is fast and can take advantage of the same data volume as the NRC method. Experiments show that our lexicons give significantly better accuracies on multiple languages compared to the current best methods. 1 Introduction Sentiment lexicons contain the sentiment polarity and/or the strength of words or phrases (Baccianella et al., 2010; Taboada et al., 2011; Tang et al., 2014a; Ren et al., 2016a). They have been used for both rule-based (Taboada et al., 2011) and unsupervised (Turney, 2002; Hu and Liu, 2004; Kiritchenko et al., 2014) or supervised (Mohammad et al., 2013; Tang et al., 2014b; Vo and Zhang, 2015) machine-learning-based sentiment analysis. As a result, constructing sentiment lexicons is one important research task in sentiment analysis. Many approaches have been proposed to construct sentiment lexicons. Traditional methods manually label the sentiment attributes of words (Hu and Liu, 2004; Wilson et al., 2005; Taboada et al., 2011). One benefit of suc"
P16-2036,P02-1053,0,0.0927749,"can be learned by using them to predict the tweet sentiment labels. By using a very simple neural network, our method is fast and can take advantage of the same data volume as the NRC method. Experiments show that our lexicons give significantly better accuracies on multiple languages compared to the current best methods. 1 Introduction Sentiment lexicons contain the sentiment polarity and/or the strength of words or phrases (Baccianella et al., 2010; Taboada et al., 2011; Tang et al., 2014a; Ren et al., 2016a). They have been used for both rule-based (Taboada et al., 2011) and unsupervised (Turney, 2002; Hu and Liu, 2004; Kiritchenko et al., 2014) or supervised (Mohammad et al., 2013; Tang et al., 2014b; Vo and Zhang, 2015) machine-learning-based sentiment analysis. As a result, constructing sentiment lexicons is one important research task in sentiment analysis. Many approaches have been proposed to construct sentiment lexicons. Traditional methods manually label the sentiment attributes of words (Hu and Liu, 2004; Wilson et al., 2005; Taboada et al., 2011). One benefit of such lexicons is high quality. On the other hand, the methods are timeconsuming, requiring language and domain exper219"
P16-2036,H05-1044,0,0.0777969,"cianella et al., 2010; Taboada et al., 2011; Tang et al., 2014a; Ren et al., 2016a). They have been used for both rule-based (Taboada et al., 2011) and unsupervised (Turney, 2002; Hu and Liu, 2004; Kiritchenko et al., 2014) or supervised (Mohammad et al., 2013; Tang et al., 2014b; Vo and Zhang, 2015) machine-learning-based sentiment analysis. As a result, constructing sentiment lexicons is one important research task in sentiment analysis. Many approaches have been proposed to construct sentiment lexicons. Traditional methods manually label the sentiment attributes of words (Hu and Liu, 2004; Wilson et al., 2005; Taboada et al., 2011). One benefit of such lexicons is high quality. On the other hand, the methods are timeconsuming, requiring language and domain exper219 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 219–224, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics that the neural model gives the best results on standard benchmarks across multiple languages. Our code and lexicons are publicly available at https://github.com/duytinvo/acl2016. 2 Related work Existing methods for automatically learning sentiment le"
P16-2036,S14-2077,0,0.0606648,"Missing"
P16-2036,P14-1062,0,\N,Missing
P17-1078,J09-4006,0,0.20222,"has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (Andor et al., 2016). Pretraining can be regarded as one way of leveraging external resources to improve accuracies, which is practically highly useful and has become a standard practice in neural NLP. On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (Li and Sun, 2009; Sun and Xu, 2011), and making use of selfpredictions (Wang et al., 2011; Liu and Zhang, 2012). It has also utilised heterogenous annotations such as POS (Ng and Low, 2004; Zhang and Clark, 2008) and segmentation under different Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sour"
P17-1078,P16-1231,0,0.105453,"ll as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016b). For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al., 2016b). Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (Andor et al., 2016). Pretraining can be regarded as one way of leveraging external resources to improve accuracies, which is practically highly useful and has become a standard practice in neural NLP. On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (Li and Sun, 2009; Sun and Xu, 2011), and making use of selfpredictions (Wang et al., 2011; Liu and Zhang, 2012). It has also utilised heterogenous annotations such as POS (Ng and Low, 2004; Zhang and Clark, 2008) and"
P17-1078,C12-2073,1,0.95031,"beddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (Andor et al., 2016). Pretraining can be regarded as one way of leveraging external resources to improve accuracies, which is practically highly useful and has become a standard practice in neural NLP. On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (Li and Sun, 2009; Sun and Xu, 2011), and making use of selfpredictions (Wang et al., 2011; Liu and Zhang, 2012). It has also utilised heterogenous annotations such as POS (Ng and Low, 2004; Zhang and Clark, 2008) and segmentation under different Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most"
P17-1078,I13-1181,0,0.117067,"utd.edu.sg yue zhang@sutd.edu.sg Abstract 角(corner)” (Zheng et al., 2013), which is infeasible by using sparse one-hot character features. In addition to character embeddings, distributed representations of character bigrams (Mansur et al., 2013; Pei et al., 2014) and words (Morita et al., 2015; Zhang et al., 2016b) have also been shown to improve segmentation accuracies. With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016b). For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al., 2016b). Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with"
P17-1078,P16-1039,0,0.156126,"r embeddings, distributed representations of character bigrams (Mansur et al., 2013; Pei et al., 2014) and words (Morita et al., 2015; Zhang et al., 2016b) have also been shown to improve segmentation accuracies. With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016b). For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al., 2016b). Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (Andor et al., 2016). Pretraining can be regarded as one way of leveraging external resources to improve accuracies,"
P17-1078,D15-1276,0,0.367817,"addition to character embeddings, distributed representations of character bigrams (Mansur et al., 2013; Pei et al., 2014) and words (Morita et al., 2015; Zhang et al., 2016b) have also been shown to improve segmentation accuracies. With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016b). For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al., 2016b). Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (Andor et al., 2016). Pretraining can be regarded as one way of leveraging external resources to"
P17-1078,P15-1168,0,0.714752,"ct 角(corner)” (Zheng et al., 2013), which is infeasible by using sparse one-hot character features. In addition to character embeddings, distributed representations of character bigrams (Mansur et al., 2013; Pei et al., 2014) and words (Morita et al., 2015; Zhang et al., 2016b) have also been shown to improve segmentation accuracies. With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016b). For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al., 2016b). Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as"
P17-1078,W04-3236,0,0.255956,"tasks, such as parsing (Andor et al., 2016). Pretraining can be regarded as one way of leveraging external resources to improve accuracies, which is practically highly useful and has become a standard practice in neural NLP. On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (Li and Sun, 2009; Sun and Xu, 2011), and making use of selfpredictions (Wang et al., 2011; Liu and Zhang, 2012). It has also utilised heterogenous annotations such as POS (Ng and Low, 2004; Zhang and Clark, 2008) and segmentation under different Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pret"
P17-1078,D15-1141,0,0.406525,"Missing"
P17-1078,P14-1028,0,0.803743,"sutd.edu.sg Abstract 角(corner)” (Zheng et al., 2013), which is infeasible by using sparse one-hot character features. In addition to character embeddings, distributed representations of character bigrams (Mansur et al., 2013; Pei et al., 2014) and words (Morita et al., 2015; Zhang et al., 2016b) have also been shown to improve segmentation accuracies. With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016b). For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al., 2016b). Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other"
P17-1078,P04-1015,0,0.0234281,"istics are listed in Table 3. Evaluation. The standard word precision, recall and F1 measure (Emerson, 2005) are used to evaluate segmentation performances. Hyper-parameter Values. We adopt commonly used values for most hyperparameters, but tuned the sizes of hidden layers on the development set. The values are summarized in Table 2. a positive example. The loss function is l(ˆ yj , yji ) = max((score(ˆ yj ) + η · δ(ˆ yj , yji ) − score(yji )), 0), (10) where δ(ˆ yj , yji ) is the number of incorrect local decisions in yˆj , and η controls the score margin. The strategy above is early-update (Collins and Roark, 2004). On the other hand, if the goldstandard hypothesis does not fall out of the agenda until the full sentence has been segmented, a final update is made between the highest scored hypothesis yˆ (non-gold standard) in the agenda and the gold-standard y i , using exactly the same loss function. Pseudocode for the online learning algorithm is shown in Algorithm 1. We use Adagrad (Duchi et al., 2011) to optimize model parameters, with an initial learning rate α. L2 regularization and dropout (Srivastava et al., 2014) on input are used to reduce overfitting, with a L2 weight λ and a dropout rate p. A"
P17-1078,C04-1081,0,0.890317,"Missing"
P17-1078,P10-1130,0,0.0155451,"studied for statistical word segmentation, but not for neural network segmentors. Raw Text. Although raw texts do not contain explicit word boundary information, statistics such as mutual information between consecutive characters can be useful features for guiding segmentation (Sun and Xu, 2011). For neural segmentation, these distributional statistics can be implicitly learned by pretraining character embeddings. We therefore consider a more explicit clue for pretraining our character window network, namely punctuations (Li and Sun, 2009). Punctuation can serve as a type of explicit markup (Spitkovsky et al., 2010), indicating that the two characters on its left and right belong to two different words. We leverage this source of information by extracting character five-grams excluding punctuation from raw sentences, using them as inputs to classify whether there is punctuation before middle character. Denoting the resulting five character window as [c−2 , c−1 , c0 , c1 , c2 ], the MLP in Figure 3 is used to derive its representation DC , which is then fed to a softmax layer for binary classification: XP = [ec (P [0]); ec (P [−1]); el (L EN(P ))] (4) Word. Similar to the character case, we investigate tw"
P17-1078,I05-3017,0,0.790241,"and Clark, 2007), the statictics of which are shown in Table 3. For pretraining character representations, we extract punctuation classification data from the Gigaword corpus, and use the word-based ZPar and a standard character-based CRF model (Tseng et al., 2005) to obtain automatic segmentation results. We compare pretraining using ZPar results only and using results that both segmentors agree on. For heterogenous segmentation corpus and POS data, we use a People’s Daily corpus of 5 months5 . Statistics are listed in Table 3. Evaluation. The standard word precision, recall and F1 measure (Emerson, 2005) are used to evaluate segmentation performances. Hyper-parameter Values. We adopt commonly used values for most hyperparameters, but tuned the sizes of hidden layers on the development set. The values are summarized in Table 2. a positive example. The loss function is l(ˆ yj , yji ) = max((score(ˆ yj ) + η · δ(ˆ yj , yji ) − score(yji )), 0), (10) where δ(ˆ yj , yji ) is the number of incorrect local decisions in yˆj , and η controls the score margin. The strategy above is early-update (Collins and Roark, 2004). On the other hand, if the goldstandard hypothesis does not fall out of the agenda"
P17-1078,J96-3004,0,0.536473,"context representations, such as character windows (Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) and LSTMs (Chen et al., 2015b; Xu and Sun, 2016). Similar to Zhang et al. (2016b) and Cai and Zhao (2016), we use word context on top of character context. However, words play a relatively less important role in our model, and we find that word LSTM, which has been used by all previous neural segmentation work, is unnecessary for our model. Our model is conceptually simpler and more modularised compared with Related Work Work on statistical word segmentation dates back to the 1990s (Sproat et al., 1996). State-of-the-art approaches include character sequence labeling models (Xue et al., 2003) using CRFs (Peng et al., 1 https://github.com/SUTDNLP/LibN3L 840 S A output ... h Axiom: Goal: S = h[ ], φ, Ci, V = 0 S = hW, φ, [ ]i, V = Vf inal S EP: S = hW, P, c0 |Ci, V 0 0 S = hW |P, c0 , Ci, V = V + Score(S, S EP) A PP: S = hW, P, c0 |Ci, V 0 0 S = hW, P ⊕ c0 , Ci, V = V + Score(S, A PP) hidden layer XW ... XP w-k 我 之前 ... ... XC Partial word Recognized words Incoming chars w-2 w-1 P c0 c1 ... cm 去 过 火 车 站 那 边 Figure 2: Deduction system, where ⊕ denotes string concatenation. our scoring network i"
P17-1078,P09-1059,0,0.17738,"ciation for Computational Linguistics, pages 839–849 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1078 State state0 state1 state2 state3 state4 state5 state6 state7 state8 state9 Recognized words [] [] [我] [我,去] [我,去,过] [我,去,过] [我,去,过] [我,去,过,火车站] [我,去,过,火车站] [我,去,过,火车站,那边] Partial word φ 我 去 过 火 火车 火车站 那 那边 φ Incoming chars [我去过火车站那边] [去过火车站那边] [过火车站那边] [火车站那边] [车站那边] [站那边] [那边] [边] [] [] Next Action S EP S EP S EP S EP A PP A PP S EP A PP F IN -- Table 1: A transition based word segmentation example. standards (Jiang et al., 2009). To our knowledge, such rich external information has not been systematically investigated for neural segmentation. We fill this gap by investigating rich external pretraining for neural segmentation. Following Cai and Zhao (2016) and Zhang et al. (2016b), we adopt a globally optimised beam-search framework for neural structured prediction (Andor et al., 2016; Zhou et al., 2015; Wiseman and Rush, 2016), which allows word information to be modelled explicitly. Different from previous work, we make our model conceptually simple and modular, so that the most important sub module, namely a five-c"
P17-1078,C10-2139,0,0.164053,"Missing"
P17-1078,D13-1031,0,0.141893,"mentation models consistently, giving the best reported results on 5 datasets in different domains and genres. Our implementation is based on LibN3L1 (Zhang et al., 2016a). Code and models can be downloaded from http://gitHub. com/jiesutd/RichWordSegmentor 2 2004; Zhao et al., 2006) and max-margin structured models leveraging word features (Zhang and Clark, 2007; Sun et al., 2009; Sun, 2010). Semisupervised methods have been applied to both character-based and word-based models, exploring external training data for better segmentation (Sun and Xu, 2011; Wang et al., 2011; Liu and Zhang, 2012; Zhang et al., 2013). Our work belongs to recent neural word segmentation. To our knowledge, there has been no work in the literature systematically investigating rich external resources for neural word segmentation training. Closest in spirit to our work, Sun and Xu (2011) empirically studied the use of various external resources for enhancing a statistical segmentor, including character mutual information, access variety information, punctuation and other statistical information. Their baseline is similar to ours in the sense that both character and word contexts are considered. On the other hand, their model i"
P17-1078,D11-1090,0,0.452355,"mentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (Andor et al., 2016). Pretraining can be regarded as one way of leveraging external resources to improve accuracies, which is practically highly useful and has become a standard practice in neural NLP. On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (Li and Sun, 2009; Sun and Xu, 2011), and making use of selfpredictions (Wang et al., 2011; Liu and Zhang, 2012). It has also utilised heterogenous annotations such as POS (Ng and Low, 2004; Zhang and Clark, 2008) and segmentation under different Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word"
P17-1078,P12-1027,0,0.0574264,"Missing"
P17-1078,L16-1034,1,0.0747522,"buted representations of character bigrams (Mansur et al., 2013; Pei et al., 2014) and words (Morita et al., 2015; Zhang et al., 2016b) have also been shown to improve segmentation accuracies. With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016b). For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al., 2016b). Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (Andor et al., 2016). Pretraining can be regarded as one way of leveraging external resources to improve accuracies, which is practicall"
P17-1078,N09-1007,0,0.10169,"er window network. After pretraining, the character window network is used to initialize the corresponding module in our segmentor. Results on 6 different benchmarks show that our method outperforms the best statistical and neural segmentation models consistently, giving the best reported results on 5 datasets in different domains and genres. Our implementation is based on LibN3L1 (Zhang et al., 2016a). Code and models can be downloaded from http://gitHub. com/jiesutd/RichWordSegmentor 2 2004; Zhao et al., 2006) and max-margin structured models leveraging word features (Zhang and Clark, 2007; Sun et al., 2009; Sun, 2010). Semisupervised methods have been applied to both character-based and word-based models, exploring external training data for better segmentation (Sun and Xu, 2011; Wang et al., 2011; Liu and Zhang, 2012; Zhang et al., 2013). Our work belongs to recent neural word segmentation. To our knowledge, there has been no work in the literature systematically investigating rich external resources for neural word segmentation training. Closest in spirit to our work, Sun and Xu (2011) empirically studied the use of various external resources for enhancing a statistical segmentor, including c"
P17-1078,P14-1125,1,0.839706,"Figure 1, respectively. All the experiments in this section are performed using a beam size of 8. Character Context. We fix the word representation XW to a 2-word window and compare different character context representations. The results are shown in Table 4, where “no char” represents our model without XC , “5-char window” represents a five-character window context, “char LSTM” represents character LSTM context and Experiments Experimental Settings Data. We use Chinese Treebank 6.0 (CTB6) (Xue et al., 2005) as our main dataset. Training, development and test set splits follow previous work (Zhang et al., 2014). In order to verify the robustness of our model, we additionally use SIGHAN 2005 bake-off (Emerson, 2005) and NLPCC 2016 shared task for Weibo segmentation (Qiu et al., 2016) as test datasets, where the standard splits are used. For pretraining embedding of 4 5 844 https://catalog.ldc.upenn.edu/LDC2011T13 http://www.icl.pku.edu.cn/icl res Character No char 5-char window char LSTM 5-char window+LSTM -char emb -bichar emb P 82.19 95.33 95.21 95.77 95.20 93.87 R 87.20 95.50 95.82 95.95 95.19 94.67 F 84.62 95.41 95.51 95.86 95.20 94.27 Word No word 1-word window 2-word window 3-word window word L"
P17-1078,P16-1040,1,0.11258,"buted representations of character bigrams (Mansur et al., 2013; Pei et al., 2014) and words (Morita et al., 2015; Zhang et al., 2016b) have also been shown to improve segmentation accuracies. With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016b). For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al., 2016b). Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (Andor et al., 2016). Pretraining can be regarded as one way of leveraging external resources to improve accuracies, which is practicall"
P17-1078,I05-3027,0,0.191097,"8 200 Paramater size(ec ) size(eb ) size(ew ) size(el ) size(XC ) size(XP ) size(XW ) Value 50 50 50 20 150 50 100 Raw data Auto seg Hete. POS #Words – 238.6m 6.17m 6.17m #Sents – 12.04m 104k 104k words, characters and character bigrams, we use Chinese Gigaword (simplified Chinese sections)4 , automatically segmented using ZPar 0.6 off-theshelf (Zhang and Clark, 2007), the statictics of which are shown in Table 3. For pretraining character representations, we extract punctuation classification data from the Gigaword corpus, and use the word-based ZPar and a standard character-based CRF model (Tseng et al., 2005) to obtain automatic segmentation results. We compare pretraining using ZPar results only and using results that both segmentors agree on. For heterogenous segmentation corpus and POS data, we use a People’s Daily corpus of 5 months5 . Statistics are listed in Table 3. Evaluation. The standard word precision, recall and F1 measure (Emerson, 2005) are used to evaluate segmentation performances. Hyper-parameter Values. We adopt commonly used values for most hyperparameters, but tuned the sizes of hidden layers on the development set. The values are summarized in Table 2. a positive example. The"
P17-1078,N06-2049,0,0.154578,"Missing"
P17-1078,P14-2032,0,0.0353973,"Missing"
P17-1078,P07-1106,1,0.942823,"sharing a five-character window network. After pretraining, the character window network is used to initialize the corresponding module in our segmentor. Results on 6 different benchmarks show that our method outperforms the best statistical and neural segmentation models consistently, giving the best reported results on 5 datasets in different domains and genres. Our implementation is based on LibN3L1 (Zhang et al., 2016a). Code and models can be downloaded from http://gitHub. com/jiesutd/RichWordSegmentor 2 2004; Zhao et al., 2006) and max-margin structured models leveraging word features (Zhang and Clark, 2007; Sun et al., 2009; Sun, 2010). Semisupervised methods have been applied to both character-based and word-based models, exploring external training data for better segmentation (Sun and Xu, 2011; Wang et al., 2011; Liu and Zhang, 2012; Zhang et al., 2013). Our work belongs to recent neural word segmentation. To our knowledge, there has been no work in the literature systematically investigating rich external resources for neural word segmentation training. Closest in spirit to our work, Sun and Xu (2011) empirically studied the use of various external resources for enhancing a statistical segm"
P17-1078,P08-1101,1,0.913994,"rsing (Andor et al., 2016). Pretraining can be regarded as one way of leveraging external resources to improve accuracies, which is practically highly useful and has become a standard practice in neural NLP. On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (Li and Sun, 2009; Sun and Xu, 2011), and making use of selfpredictions (Wang et al., 2011; Liu and Zhang, 2012). It has also utilised heterogenous annotations such as POS (Ng and Low, 2004; Zhang and Clark, 2008) and segmentation under different Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly im"
P17-1078,D16-1137,0,0.023308,"chars [我去过火车站那边] [去过火车站那边] [过火车站那边] [火车站那边] [车站那边] [站那边] [那边] [边] [] [] Next Action S EP S EP S EP S EP A PP A PP S EP A PP F IN -- Table 1: A transition based word segmentation example. standards (Jiang et al., 2009). To our knowledge, such rich external information has not been systematically investigated for neural segmentation. We fill this gap by investigating rich external pretraining for neural segmentation. Following Cai and Zhao (2016) and Zhang et al. (2016b), we adopt a globally optimised beam-search framework for neural structured prediction (Andor et al., 2016; Zhou et al., 2015; Wiseman and Rush, 2016), which allows word information to be modelled explicitly. Different from previous work, we make our model conceptually simple and modular, so that the most important sub module, namely a five-character window context, can be pretrained using external data. We adopt a multi-task learning strategy (Collobert et al., 2011), casting each external source of information as a auxiliary classification task, sharing a five-character window network. After pretraining, the character window network is used to initialize the corresponding module in our segmentor. Results on 6 different benchmarks show tha"
P17-1078,J11-1005,1,0.827987,"(8) POS Data. Previous research has shown that POS information is closely related to segmentation (Ng and Low, 2004; Zhang and Clark, 2008). We verify the utility of POS information for our segmentor by pretraining a classifier that predicts the POS on each character, according to the character window representation DC . In particular, given [c−2 , c−1 , c0 , c1 , c2 ], the POS of the word that c0 belongs to is used as the output. P (pos) = softmax(Wpos · DC + bpos ) 4 Decoding and Training To train the main segmentor, we adopt the global transition-based learning and beam-search strategy of Zhang and Clark (2011). For decoding, standard beam search is used, where the B best partial output hypotheses at each step are maintained in an agenda. Initially, the agenda contains only the start state. At each step, all hypotheses in the agenda are expanded, by applying all possible actions and B highest scored resulting hypotheses are used as the agenda for the next step. For training, the same decoding process is applied to each training example (xi , y i ). At step j, if the gold-standard sequence of transition actions yji falls out of the agenda, max-margin update is performed by taking the current best hyp"
P17-1078,Y06-1012,0,0.281066,"asting each external source of information as a auxiliary classification task, sharing a five-character window network. After pretraining, the character window network is used to initialize the corresponding module in our segmentor. Results on 6 different benchmarks show that our method outperforms the best statistical and neural segmentation models consistently, giving the best reported results on 5 datasets in different domains and genres. Our implementation is based on LibN3L1 (Zhang et al., 2016a). Code and models can be downloaded from http://gitHub. com/jiesutd/RichWordSegmentor 2 2004; Zhao et al., 2006) and max-margin structured models leveraging word features (Zhang and Clark, 2007; Sun et al., 2009; Sun, 2010). Semisupervised methods have been applied to both character-based and word-based models, exploring external training data for better segmentation (Sun and Xu, 2011; Wang et al., 2011; Liu and Zhang, 2012; Zhang et al., 2013). Our work belongs to recent neural word segmentation. To our knowledge, there has been no work in the literature systematically investigating rich external resources for neural word segmentation training. Closest in spirit to our work, Sun and Xu (2011) empirical"
P17-1078,P16-2092,0,0.163928,"ne-hot character features. In addition to character embeddings, distributed representations of character bigrams (Mansur et al., 2013; Pei et al., 2014) and words (Morita et al., 2015; Zhang et al., 2016b) have also been shown to improve segmentation accuracies. With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016b). For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al., 2016b). Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (Andor et al., 2016). Pretraining can be regarded as one way of l"
P17-1078,D13-1061,0,0.79824,", fei dong}@mymail.sutd.edu.sg yue zhang@sutd.edu.sg Abstract 角(corner)” (Zheng et al., 2013), which is infeasible by using sparse one-hot character features. In addition to character embeddings, distributed representations of character bigrams (Mansur et al., 2013; Pei et al., 2014) and words (Morita et al., 2015; Zhang et al., 2016b) have also been shown to improve segmentation accuracies. With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016b). For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al., 2016b). Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, whic"
P17-1078,P15-1117,1,0.738905,"车站 那 那边 φ Incoming chars [我去过火车站那边] [去过火车站那边] [过火车站那边] [火车站那边] [车站那边] [站那边] [那边] [边] [] [] Next Action S EP S EP S EP S EP A PP A PP S EP A PP F IN -- Table 1: A transition based word segmentation example. standards (Jiang et al., 2009). To our knowledge, such rich external information has not been systematically investigated for neural segmentation. We fill this gap by investigating rich external pretraining for neural segmentation. Following Cai and Zhao (2016) and Zhang et al. (2016b), we adopt a globally optimised beam-search framework for neural structured prediction (Andor et al., 2016; Zhou et al., 2015; Wiseman and Rush, 2016), which allows word information to be modelled explicitly. Different from previous work, we make our model conceptually simple and modular, so that the most important sub module, namely a five-character window context, can be pretrained using external data. We adopt a multi-task learning strategy (Collobert et al., 2011), casting each external source of information as a auxiliary classification task, sharing a five-character window network. After pretraining, the character window network is used to initialize the corresponding module in our segmentor. Results on 6 diff"
P17-1078,O03-4002,0,0.905369,"Missing"
P17-1078,E14-1062,1,\N,Missing
P17-1159,D11-1006,0,0.0942936,"Missing"
P17-1159,W11-2148,0,0.0124621,"uial Singaporean English (Singlish) (MianLian and Platt, 1993), an English-based creole. While the majority of the natural language processing (NLP) research attention has been focused on the major languages, little work has been done on adapting the components to creoles. One notable body of work originated from the featured translation task of the EMNLP 2011 Workshop on Statistical Machine Translation (WMT11) to translate Haitian Creole SMS messages sent during the 2010 Haitian earthquake. This work highlights the importance of NLP tools on creoles in crisis situations for emergency relief (Hu et al., 2011; Hewavitharana et al., 2011). Singlish is one of the major languages in Singapore, with borrowed vocabulary and grammars1 from a number of languages including Malay, Tamil, and Chinese dialects such as Hokkien, Cantonese and Teochew (Leimgruber, 2009, 2011), and it has been increasingly used in written forms on web media. Fluent English speakers unfamiliar with Singlish would find the creole hard to comprehend (Harada, 2009). Correspondingly, fundamental English NLP components such as POS taggers and dependency parsers perform poorly on such Singlish texts as shown in Table 2 and 4. For examp"
P17-1159,Q16-1023,0,0.116831,"Missing"
P17-1159,D15-1278,0,0.042925,"Missing"
P17-1159,P16-1105,0,0.0222997,"nd Teochew (Leimgruber, 2009, 2011), and it has been increasingly used in written forms on web media. Fluent English speakers unfamiliar with Singlish would find the creole hard to comprehend (Harada, 2009). Correspondingly, fundamental English NLP components such as POS taggers and dependency parsers perform poorly on such Singlish texts as shown in Table 2 and 4. For example, Seah et al. (2015) adapted the Socher et al. (2013) sentiment analysis engine to the Singlish vocabulary, but failed to adapt the parser. Since dependency parsers are important for tasks such as information extraction (Miwa and Bansal, 2016) and discourse parsing (Li et al., 2015), this hinders the development of such downstream applications for Singlish in written forms and thus makes it crucial to build a dependency parser that can perform well natively on Singlish. To address this issue, we start with investigating the linguistic characteristics of Singlish and specifically the causes of difficulties for understanding Singlish with English syntax. We found that, despite the obvious attribute of inheriting a large portion of basic vocabularies and grammars from English, Singlish not only imports terms from regional languages an"
P17-1159,P12-1066,0,0.0318295,"arsing, which leverages English resources in Universal Dependencies to improve the parsing accuracies of low-resource languages (Hwa et al., 2005; Cohen and Smith, 2009; Ganchev et al., 2009). Seminal work employed statistical models. McDonald et al. (2011) investigated delexicalized transfer, where word-based features are removed from a statistical model for English, so that POS and dependency label knowledge can be utilized for training a model for lowresource language. Subsequent work considered syntactic similarities between languages for better feature transfer (T¨ackstr¨om et al., 2012; Naseem et al., 2012; Zhang and Barzilay, 2015). Recently, a line of work leverages neural network models for multi-lingual parsing (Guo et al., 2015; Duong et al., 2015; Ammar et al., 2016). The basic idea is to map the word embedding spaces between different languages into the same vector space, by using sentence-aligned bilingual data. This gives consistency in tokens, POS and dependency labels thanks to the availability of Universal Dependencies (Nivre et al., 2016). Our work is similar to these methods in using a neural network model for knowledge sharing between different languages. However, ours is differe"
P17-1159,D10-1120,0,0.0148669,"f our knowledge, we are the first to employ it on cross-lingual 1733 feature transfer from resource-rich languages to improve dependency parsing for low-resource languages. Besides these three dimensions in dealing with heterogeneous text data, another popular area of research is on the topic of domain adaption, which is commonly associated with crosslingual problems (Nivre et al., 2007). While this large strand of work is remotely related to ours, we do not describe them in details. Unsupervised rule-based approaches also offer an competitive alternative for cross-lingual dependency parsing (Naseem et al., 2010; Gillenwater et al., 2010; Gelling et al., 2012; Søgaard, 2012a,b; Mart´ınez Alonso et al., 2017), and recently been benchmarked for the Universal Dependencies formalism by exploiting the linguistic constraints in the Universal Dependencies to improve the robustness against error propagation and domain adaption (Mart´ınez Alonso et al., 2017). However, we choose a data-driven supervised approach given the relatively higher parsing accuracy owing to the availability of resourceful treebanks from the Universal Dependencies project. 3 3.1 Singlish Dependency Treebank Universal Dependencies for S"
P17-1159,D14-1162,0,0.0904724,"Missing"
P17-1159,P16-2067,0,0.02341,"Missing"
P17-1159,D13-1170,0,0.00274895,"lish is one of the major languages in Singapore, with borrowed vocabulary and grammars1 from a number of languages including Malay, Tamil, and Chinese dialects such as Hokkien, Cantonese and Teochew (Leimgruber, 2009, 2011), and it has been increasingly used in written forms on web media. Fluent English speakers unfamiliar with Singlish would find the creole hard to comprehend (Harada, 2009). Correspondingly, fundamental English NLP components such as POS taggers and dependency parsers perform poorly on such Singlish texts as shown in Table 2 and 4. For example, Seah et al. (2015) adapted the Socher et al. (2013) sentiment analysis engine to the Singlish vocabulary, but failed to adapt the parser. Since dependency parsers are important for tasks such as information extraction (Miwa and Bansal, 2016) and discourse parsing (Li et al., 2015), this hinders the development of such downstream applications for Singlish in written forms and thus makes it crucial to build a dependency parser that can perform well natively on Singlish. To address this issue, we start with investigating the linguistic characteristics of Singlish and specifically the causes of difficulties for understanding Singlish with English"
P17-1159,W12-1910,0,0.0141493,"feature transfer from resource-rich languages to improve dependency parsing for low-resource languages. Besides these three dimensions in dealing with heterogeneous text data, another popular area of research is on the topic of domain adaption, which is commonly associated with crosslingual problems (Nivre et al., 2007). While this large strand of work is remotely related to ours, we do not describe them in details. Unsupervised rule-based approaches also offer an competitive alternative for cross-lingual dependency parsing (Naseem et al., 2010; Gillenwater et al., 2010; Gelling et al., 2012; Søgaard, 2012a,b; Mart´ınez Alonso et al., 2017), and recently been benchmarked for the Universal Dependencies formalism by exploiting the linguistic constraints in the Universal Dependencies to improve the robustness against error propagation and domain adaption (Mart´ınez Alonso et al., 2017). However, we choose a data-driven supervised approach given the relatively higher parsing accuracy owing to the availability of resourceful treebanks from the Universal Dependencies project. 3 3.1 Singlish Dependency Treebank Universal Dependencies for Singlish Since English is the major genesis of Singlish, we choo"
P17-1159,N12-1052,0,0.109571,"Missing"
P17-1159,P15-1032,0,0.0602407,"Missing"
P17-1159,D15-1213,0,0.0269391,"es English resources in Universal Dependencies to improve the parsing accuracies of low-resource languages (Hwa et al., 2005; Cohen and Smith, 2009; Ganchev et al., 2009). Seminal work employed statistical models. McDonald et al. (2011) investigated delexicalized transfer, where word-based features are removed from a statistical model for English, so that POS and dependency label knowledge can be utilized for training a model for lowresource language. Subsequent work considered syntactic similarities between languages for better feature transfer (T¨ackstr¨om et al., 2012; Naseem et al., 2012; Zhang and Barzilay, 2015). Recently, a line of work leverages neural network models for multi-lingual parsing (Guo et al., 2015; Duong et al., 2015; Ammar et al., 2016). The basic idea is to map the word embedding spaces between different languages into the same vector space, by using sentence-aligned bilingual data. This gives consistency in tokens, POS and dependency labels thanks to the availability of Universal Dependencies (Nivre et al., 2016). Our work is similar to these methods in using a neural network model for knowledge sharing between different languages. However, ours is different in the use of a neural s"
P17-1159,P16-1147,0,0.106121,"Missing"
P17-1159,P15-1117,1,0.918879,"Missing"
P17-1159,N09-1009,0,\N,Missing
P17-1159,P09-1042,0,\N,Missing
P17-1159,D14-1082,0,\N,Missing
P17-1159,D15-1040,0,\N,Missing
P17-1159,L16-1262,0,\N,Missing
P17-1159,D16-1070,1,\N,Missing
P17-2002,2006.amta-papers.8,0,0.0358799,"istinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ,"
P17-2002,D15-1198,0,0.0124762,"s. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3#"
P17-2002,C12-1083,0,0.0608432,"Missing"
P17-2002,W13-2322,0,0.227385,"nto a sentence as a traveling salesman problem, using local features and a language model to rank candidate sentences. However, their method does not learn hierarchical structural correspondences between AMR graphs and strings. We propose to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules. As shown in Figure 1, we learn a synchronous node replacement grammar (NRG) from a corpus of aligned AMR and sentence pairs. At test time, we apply a graph transducer to collapse input Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as “boy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syn"
P17-2002,N03-1017,0,0.0322761,"rule list R (Lines 5 and 9), which will be further normalized to obtain the final induced rule set. 2.3 t where g denotes the input AMR, fi (·, ·) and wi represent a feature and the corresponding weight, respectively. The feature set that we adopt includes phrase-to-graph and graph-to-phrase translation probabilities and their corresponding lexicalized translation probabilities (section 3.1), language model score, word count, rule count, reordering model score (section 3.2) and moving distance (section 3.3). The language model score, word count and phrase count features are adopted from SMT (Koehn et al., 2003; Chiang, 2005). We perform bottom-up search to transduce input AMRs to surface strings. Each hypothesis contains the current AMR graph, translations of collapsed subgraphs, the feature vector and the current model score. Beam search is adopted, where hypotheses with the same number of collapsed edges and nodes are put into the same beam. Concept Rules and Glue Rules In addition to induced rules, we adopt concept rules (Song et al., 2016) and graph glue rules to ensure existence of derivations. For a concept rule, F is a single node in the input AMR graph, and E is a morphological string of th"
P17-2002,P05-1033,0,0.0721117,"ubscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node"
P17-2002,W15-4502,0,0.0386307,"to collapse input Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as “boy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the"
P17-2002,P06-1077,0,0.016854,"non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ, E is a correspondi"
P17-2002,D13-1108,1,0.889096,"Missing"
P17-2002,N16-1087,0,0.26532,"versity of Technology and Design Abstract This paper addresses the task of AMR-totext generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on a standard benchmark, our method gives the state-of-the-art result. 1 #X3# #X3# ARG1 #X2# go-01 #X2# ARG0 ARG0 #X1# ARG0 boy want-01 ARG0 want-01 ARG0 ARG1 #X1# go-01 ARG1 go-01 the boy wants to go Figure 1: Graph-to-string derivation. Introduction Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer. Their method leverages existing machine translation techniques, capturing hierarchical correspondences between the spanning tree and the surface string. However, it suffers from error propagation since the output is constrained given a spanning tree due to the projective correspondence between them. Information loss in the graph-to-tree transformation step cannot be recovered. Song et al. (2016) directly generate sentences using graphfragment-to-string rules. They cast the"
P17-2002,P14-1134,0,0.0267127,"oy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2#"
P17-2002,P03-1021,0,0.0915657,"want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy wants to go} Figure 2: Example deduction procedure ID. (a) (b) (c) (d) F (b / boy) (w / want-01 :ARG0 (X / #X#)) (X / #X# :ARG1 (g / go-01 :ARG0 X)) (w / want-01 :ARG0 (b / boy)) E the boy 1 #X# wants 2 3 #X# to go 4 5 6 the boy wants 7 8 Table 1: Example rule set 9 10 11 AMR graphs and generate output strings according to the learned grammar. Our system makes use of a log-linear model with real-valued features, tuned using MERT (Och, 2003), and beam search decoding. It gives a BLEU score of 25.62 on LDC2015E86, which is the state-of-the-art on this dataset. 2 2.1 12 13 Data: training corpus C Result: rule instances R R ← []; for (Sent, AM R, ∼) in C do Rcur ← F RAGMENT E XTRACT(Sent,AM R,∼); for ri in Rcur do R.APPEND(ri ) ; for rj in Rcur /{ri } do if ri .C ONTAINS(rj ) then rij ← ri .COLLAPSE(rj ); R.APPEND(rij ) ; end end end end Algorithm 1: Rule extraction (2005), we use only one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process"
P17-2002,P03-1011,1,0.658507,"o S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels o"
P17-2002,P02-1040,0,0.101213,"connected by edge l. 3.3 Dev 21.12 23.00 25.24 16.75 23.99 23.48 25.09 Experiments Setup We use LDC2015E86 as our experimental dataset, which contains 16833 training, 1368 dev and 1371 test instances. Each instance contains a sentence, an AMR graph and the alignment generated by a heuristic aligner. Rules are extracted from the training data, and model parameters are tuned on the dev set. For tuning and testing, we filter out sentences with more than 30 words, resulting in 1103 dev instances and 1055 test instances. We train a 4-gram language model (LM) on gigaword (LDC2011T07), and use BLEU (Papineni et al., 2002) as the evaluation metric. MERT is used (Och, 2003) to tune model parameters on k-best outputs on the devset, where k is set 50. We investigate the effectiveness of rules and features by ablation tests: “NoInducedRule” does not adopt induced rules, “NoConceptRule” does not adopt concept rules, “NoMovingDistance” does not adopt the moving distance feature, and “NoReorderModel” disables the reordering model. Given an AMR graph, if NoConceptRule cannot produce a legal derivation, we concatenate 4.3 Grammar analysis We have shown the effectiveness of our synchronous node replacement grammar (SNRG)"
P17-2002,P16-1001,0,0.0154203,"aph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy want"
P17-2002,K15-1004,1,0.839012,"ges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3#"
P17-2002,P15-1143,0,0.0161628,"antic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go"
P17-2002,E17-1035,1,0.853477,"s such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy wants to go} Figure 2: Example deduction pr"
P17-2002,W16-6603,0,0.200621,"), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy wants to go} Figure 2: Example deduction procedure ID. (a) (b) (c) (d) F (b / boy) (w / want-01 :ARG0 (X / #X#)) (X / #X# :ARG1 (g / go-01 :ARG0 X)) (w / want-01 :ARG0"
P17-2002,D16-1065,0,0.0438864,"useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1 (b) go-01 ARG0 {#X3# wants to go} ARG0 boy go-01 ARG0 {the boy wants to go} Figure 2:"
P17-2002,D15-1136,0,0.0142351,"ons between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0 ARG0 want-01 ARG1"
P17-2002,P08-1066,0,0.00990465,"ances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ, E is a corresponding target string ov"
P17-2002,D16-1224,1,0.913448,"o-01 the boy wants to go Figure 1: Graph-to-string derivation. Introduction Flanigan et al. (2016) transform a given AMR graph into a spanning tree, before translating it to a sentence using a tree-to-string transducer. Their method leverages existing machine translation techniques, capturing hierarchical correspondences between the spanning tree and the surface string. However, it suffers from error propagation since the output is constrained given a spanning tree due to the projective correspondence between them. Information loss in the graph-to-tree transformation step cannot be recovered. Song et al. (2016) directly generate sentences using graphfragment-to-string rules. They cast the task of finding a sequence of disjoint rules to transduce an AMR graph into a sentence as a traveling salesman problem, using local features and a language model to rank candidate sentences. However, their method does not learn hierarchical structural correspondences between AMR graphs and strings. We propose to leverage the advantages of hierarchical rules without suffering from graph-to-tree errors by directly learning graph-to-string rules. As shown in Figure 1, we learn a synchronous node replacement grammar (N"
P17-2002,D16-1112,0,0.0090019,"At test time, we apply a graph transducer to collapse input Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism encoding the meaning of a sentence as a rooted, directed graph. AMR uses a graph to represent meaning, where nodes (such as “boy”, “want-01”) represent concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedi"
P17-2002,W15-3504,0,0.363273,"Missing"
P17-2002,N15-3006,0,0.0208597,", “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String: {#S#} {#X1#} #X3# {#X2# to go} ARG1 (a) ARG0"
P17-2002,N15-1040,0,0.0145351,"nt concepts, and edges (such as “ARG0”, “ARG1”) represent relations between concepts. Encoding many semantic phenomena into a graph structure, AMR is useful for NLP tasks such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). AMR-to-text generation is challenging as function words and syntactic structures are abstracted away, making an AMR graph correspond to multiple realizations. Despite much literature so far on text-to-AMR parsing (Flanigan et al., 2014; Wang et al., 2015; Peng et al., 2015; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015; Groschwitz et al., 2015; Goodman et al., 2016; Zhou et al., 2016; Peng et al., 2017), there has been little work on AMR-to-text generation (Flanigan et al., 2016; Song et al., 2016; Pourdamghani et al., 2016). 7 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7–13 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2002 ARG1 (c) AMR: #S# want-01 (root) #X2# #X1# go-01 String:"
P17-2002,J97-3002,0,0.0460913,"nly one nonterminal X in addition to S, and use subscripts to distinguish different non-terminal instances. Figure 2 shows an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, co"
P17-2002,D11-1020,0,0.0167829,"ws an example derivation process for the sentence “the boy wants to go” given the rule set in Table 1. Given the start symbol S, which is first replaced with X1 , rule (c) is applied to generate “X2 to go” and its AMR counterpart. Then rule (b) is used to generate “X3 wants” and its AMR counterpart from X2 . Finally, rule (a) is used to generate “the boy” and its AMR counterpart from X3 . Our graph-to-string rules are inspired by synchronous grammars for machine translation (Wu, 1997; Yamada and Knight, 2002; Gildea, 2003; Chiang, 2005; Huang et al., 2006; Liu et al., 2006; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Synchronous Node Replacement Grammar Grammar Definition A synchronous node replacement grammar (NRG) is a rewriting formalism: G = hN, Σ, ∆, P, Si, where N is a finite set of nonterminals, Σ and ∆ are finite sets of terminal symbols for the source and target sides, respectively. S ∈ N is the start symbol, and P is a finite set of productions. Each instance of P takes the form Xi → (hF, Ei, ∼), where Xi ∈ N is a nonterminal node, F is a rooted, connected AMR fragment with edge labels over Σ and node labels over N ∪ Σ, E is a corresponding target string over N ∪ ∆ and ∼ den"
P17-2002,P02-1039,0,\N,Missing
P17-2002,P13-2131,0,\N,Missing
P18-1030,N16-1030,0,0.121858,"Missing"
P18-1030,D17-1209,0,0.0649709,"ention and stacked CNN in this respect, incrementally refining sentence representations. However, S-LSTM models hierarchical encoding of sentence structure as a recurrent state transition process. In nature, our work belongs to the family of LSTM sentence representations. S-LSTM is inspired by message passing over graphs (Murphy et al., 1999; Scarselli et al., 2009). Graph-structure neural models have been used for computer program verification (Li et al., 2016) and image object detection (Liang et al., 2016). The closest previous work in NLP includes the use of convolutional neural networks (Bastings et al., 2017; Marcheggiani and Titov, 2017) and DAG LSTMs (Peng et al., 2017) for modelling syntactic structures. Compared to our work, their motivations and network structures are highly different. In particular, the DAG LSTM of Peng et al. (2017) is a natural extension of tree LSTM (Tai et al., 2015), and is sequential rather than parallel in nature. To our knowledge, we are the first to investigate a graph RNN for encoding sentences, proposing parallel graph states for integrating word-level and sentence-level information. In this perspective, our contribution is similar to that of Kim (2014) and Bahda"
P18-1030,P15-1033,0,0.0235531,"on and sequence labelling show that S-LSTM gives better accuracies compared to BiLSTM using the same number of parameters, while being faster. We release our code and models at https://github.com/ leuchine/S-LSTM, which include all baselines and the final model. 2 Related Work LSTM (Graves and Schmidhuber, 2005) showed its early potentials in NLP when a neural machine translation system that leverages LSTM source encoding gave highly competitive results compared to the best SMT models (Bahdanau et al., 2015). LSTM encoders have since been explored for other tasks, including syntactic parsing (Dyer et al., 2015), text classification (Yang et al., 2016) and machine reading (Hermann et al., 2015). Bidirectional extensions have become a standard configuration for achieving state-of-the-art accuracies among various tasks (Wen et al., 2015; Ma and Hovy, 2016; Dozat and Manning, 2017). SLSTMs are similar to BiLSTMs in their recurrent bi-directional message flow between words, but different in the design of state transition. CNNs (Krizhevsky et al., 2012) also allow better parallelisation compared to LSTMs for sentence encoding (Kim, 2014), thanks to parallelism among convolution filters. On the other hand,"
P18-1030,P17-1001,0,0.0290537,"Missing"
P18-1030,D15-1104,0,0.087169,"Missing"
P18-1030,P16-1101,0,0.19955,"he final model. 2 Related Work LSTM (Graves and Schmidhuber, 2005) showed its early potentials in NLP when a neural machine translation system that leverages LSTM source encoding gave highly competitive results compared to the best SMT models (Bahdanau et al., 2015). LSTM encoders have since been explored for other tasks, including syntactic parsing (Dyer et al., 2015), text classification (Yang et al., 2016) and machine reading (Hermann et al., 2015). Bidirectional extensions have become a standard configuration for achieving state-of-the-art accuracies among various tasks (Wen et al., 2015; Ma and Hovy, 2016; Dozat and Manning, 2017). SLSTMs are similar to BiLSTMs in their recurrent bi-directional message flow between words, but different in the design of state transition. CNNs (Krizhevsky et al., 2012) also allow better parallelisation compared to LSTMs for sentence encoding (Kim, 2014), thanks to parallelism among convolution filters. On the other hand, convolution features embody only fix-sized local ngram information, whereas sentence-level feature aggregation via pooling can lead to loss of information (Sabour et al., 2017). In contrast, S-LSTM uses a global sentence-level node to assemble a"
P18-1030,D17-1159,0,0.0554216,"in this respect, incrementally refining sentence representations. However, S-LSTM models hierarchical encoding of sentence structure as a recurrent state transition process. In nature, our work belongs to the family of LSTM sentence representations. S-LSTM is inspired by message passing over graphs (Murphy et al., 1999; Scarselli et al., 2009). Graph-structure neural models have been used for computer program verification (Li et al., 2016) and image object detection (Liang et al., 2016). The closest previous work in NLP includes the use of convolutional neural networks (Bastings et al., 2017; Marcheggiani and Titov, 2017) and DAG LSTMs (Peng et al., 2017) for modelling syntactic structures. Compared to our work, their motivations and network structures are highly different. In particular, the DAG LSTM of Peng et al. (2017) is a natural extension of tree LSTM (Tai et al., 2015), and is sequential rather than parallel in nature. To our knowledge, we are the first to investigate a graph RNN for encoding sentences, proposing parallel graph states for integrating word-level and sentence-level information. In this perspective, our contribution is similar to that of Kim (2014) and Bahdanau et al. (2015) in introducin"
P18-1030,J93-2004,0,0.0610428,"Missing"
P18-1030,P14-1062,0,0.0717103,"Association for Computational Linguistics Attention (Bahdanau et al., 2015) has recently been explored as a standalone method for sentence encoding, giving competitive results compared to Bi-LSTM encoders for neural machine translation (Vaswani et al., 2017). The attention mechanism allows parallelisation, and can play a similar role to the sentence-level state in S-LSTMs, which uses neural gates to integrate word-level information compared to hierarchical attention. S-LSTM further allows local communication between neighbouring words. Hierarchical stacking of CNN layers (LeCun et al., 1995; Kalchbrenner et al., 2014; Papandreou et al., 2015; Dauphin et al., 2017) allows better interaction between non-local components in a sentence via incremental levels of abstraction. S-LSTM is similar to hierarchical attention and stacked CNN in this respect, incrementally refining sentence representations. However, S-LSTM models hierarchical encoding of sentence structure as a recurrent state transition process. In nature, our work belongs to the family of LSTM sentence representations. S-LSTM is inspired by message passing over graphs (Murphy et al., 1999; Scarselli et al., 2009). Graph-structure neural models have b"
P18-1030,D14-1181,0,0.0498126,"astings et al., 2017; Marcheggiani and Titov, 2017) and DAG LSTMs (Peng et al., 2017) for modelling syntactic structures. Compared to our work, their motivations and network structures are highly different. In particular, the DAG LSTM of Peng et al. (2017) is a natural extension of tree LSTM (Tai et al., 2015), and is sequential rather than parallel in nature. To our knowledge, we are the first to investigate a graph RNN for encoding sentences, proposing parallel graph states for integrating word-level and sentence-level information. In this perspective, our contribution is similar to that of Kim (2014) and Bahdanau et al. (2015) in introducing a neural representation to the NLP literature. At each recurrent step, information exchange is conducted between consecutive words in the sentence, and between the sentence-level state and each word. In particular, each word receives information from its predecessor and successor simultaneously. From an initial state without information exchange, each word-level state can obtain 3-gram, 5-gram and 7-gram information after 1, 2 and 3 recurrent steps, respectively. Being connected with every word, the sentence-level state vector serves to exchange non-l"
P18-1030,W17-3204,0,0.0216347,"g benchmarks show that the proposed model has strong representation power, giving highly competitive performances compared to stacked BiLSTM models with similar parameter numbers. 1 0 1 ... tim e ... ... ... ... ... t-1 ... ... ... t Figure 1: Sentence-State LSTM dustry. In addition, local ngrams, which have been shown a highly useful source of contextual information for NLP, are not explicitly modelled (Wang et al., 2016). Finally, sequential information flow leads to relatively weaker power in capturing longrange dependencies, which results in lower performance in encoding longer sentences (Koehn and Knowles, 2017). Introduction Neural models have become the dominant approach in the NLP literature. Compared to handcrafted indicator features, neural sentence representations are less sparse, and more flexible in encoding intricate syntactic and semantic information. Among various neural networks for encoding sentences, bi-directional LSTMs (BiLSTM) (Hochreiter and Schmidhuber, 1997) have been a dominant method, giving state-of-the-art results in language modelling (Sundermeyer et al., 2012), machine translation (Bahdanau et al., 2015), syntactic parsing (Dozat and Manning, 2017) and question answering (Ta"
P18-1030,W14-1609,0,0.0386609,"Missing"
P18-1030,Q17-1008,0,0.0295337,"nce representations. However, S-LSTM models hierarchical encoding of sentence structure as a recurrent state transition process. In nature, our work belongs to the family of LSTM sentence representations. S-LSTM is inspired by message passing over graphs (Murphy et al., 1999; Scarselli et al., 2009). Graph-structure neural models have been used for computer program verification (Li et al., 2016) and image object detection (Liang et al., 2016). The closest previous work in NLP includes the use of convolutional neural networks (Bastings et al., 2017; Marcheggiani and Titov, 2017) and DAG LSTMs (Peng et al., 2017) for modelling syntactic structures. Compared to our work, their motivations and network structures are highly different. In particular, the DAG LSTM of Peng et al. (2017) is a natural extension of tree LSTM (Tai et al., 2015), and is sequential rather than parallel in nature. To our knowledge, we are the first to investigate a graph RNN for encoding sentences, proposing parallel graph states for integrating word-level and sentence-level information. In this perspective, our contribution is similar to that of Kim (2014) and Bahdanau et al. (2015) in introducing a neural representation to the N"
P18-1030,D14-1162,0,0.0860755,"Missing"
P18-1030,P15-1150,0,0.348017,"Missing"
P18-1030,P17-1161,0,0.0318105,"Missing"
P18-1030,W09-1119,0,0.150791,"Missing"
P18-1030,C16-1229,0,0.0546673,"Missing"
P18-1030,P17-1194,0,0.0315892,"Missing"
P18-1030,D15-1199,0,0.0609081,"Missing"
P18-1030,W03-0419,0,0.191843,"Missing"
P18-1030,N16-1174,0,0.475386,"M gives better accuracies compared to BiLSTM using the same number of parameters, while being faster. We release our code and models at https://github.com/ leuchine/S-LSTM, which include all baselines and the final model. 2 Related Work LSTM (Graves and Schmidhuber, 2005) showed its early potentials in NLP when a neural machine translation system that leverages LSTM source encoding gave highly competitive results compared to the best SMT models (Bahdanau et al., 2015). LSTM encoders have since been explored for other tasks, including syntactic parsing (Dyer et al., 2015), text classification (Yang et al., 2016) and machine reading (Hermann et al., 2015). Bidirectional extensions have become a standard configuration for achieving state-of-the-art accuracies among various tasks (Wen et al., 2015; Ma and Hovy, 2016; Dozat and Manning, 2017). SLSTMs are similar to BiLSTMs in their recurrent bi-directional message flow between words, but different in the design of state transition. CNNs (Krizhevsky et al., 2012) also allow better parallelisation compared to LSTMs for sentence encoding (Kim, 2014), thanks to parallelism among convolution filters. On the other hand, convolution features embody only fix-siz"
P18-1030,D12-1110,0,0.173199,"Missing"
P18-1030,D11-1014,0,0.115743,"Missing"
P18-1030,P11-2009,0,0.0376289,"Missing"
P18-1144,N13-1006,0,0.466304,"Missing"
P18-1144,W06-0130,0,0.725376,"ert et al. (2011) used a CNN-CRF structure, obtaining competitive results to the best statistical models. dos Santos et al. (2015) used character CNN to augment a CNN-CRF model. Most recent work leverages an LSTM-CRF architecture. Huang et al. (2015) uses hand-crafted spelling features; Ma and Hovy (2016) and Chiu and Nichols (2016) use a character CNN to represent spelling characteristics; Lample et al. (2016) use a character LSTM instead. Our baseline word-based system takes a similar structure to this line of work. Character sequence labeling has been the dominant approach for Chinese NER (Chen et al., 2006b; Lu et al., 2016; Dong et al., 2016). There have been explicit discussions comparing statistical word-based and character-based methods for the task, showing that the latter is empirically a superior choice (He and Wang, 2008; Liu et al., 2010; Li et al., 2014). We find that with proper How to better leverage word information for Chinese NER has received continued research attention (Gao et al., 2005), where segmentation information has been used as soft features for NER (Zhao and Kit, 2008; Peng and Dredze, 2015; He and Sun, 2017a), and joint segmentation and NER has been investigated using"
P18-1144,W06-0116,0,0.111341,"ert et al. (2011) used a CNN-CRF structure, obtaining competitive results to the best statistical models. dos Santos et al. (2015) used character CNN to augment a CNN-CRF model. Most recent work leverages an LSTM-CRF architecture. Huang et al. (2015) uses hand-crafted spelling features; Ma and Hovy (2016) and Chiu and Nichols (2016) use a character CNN to represent spelling characteristics; Lample et al. (2016) use a character LSTM instead. Our baseline word-based system takes a similar structure to this line of work. Character sequence labeling has been the dominant approach for Chinese NER (Chen et al., 2006b; Lu et al., 2016; Dong et al., 2016). There have been explicit discussions comparing statistical word-based and character-based methods for the task, showing that the latter is empirically a superior choice (He and Wang, 2008; Liu et al., 2010; Li et al., 2014). We find that with proper How to better leverage word information for Chinese NER has received continued research attention (Gao et al., 2005), where segmentation information has been used as soft features for NER (Zhao and Kit, 2008; Peng and Dredze, 2015; He and Sun, 2017a), and joint segmentation and NER has been investigated using"
P18-1144,I17-1019,0,0.0309682,"y of performing Chinese NER is to perform word segmentation first, before applying word sequence labeling. The segmentation → NER pipeline, however, can suffer the potential issue of error propagation, since NEs are an important source of OOV Equal contribution. 市 City 市长 南京 Nanjing Introduction ∗ 京 Capital in segmentation, and incorrectly segmented entity boundaries lead to NER errors. This problem can be severe in the open domain since crossdomain word segmentation remains an unsolved problem (Liu and Zhang, 2012; Jiang et al., 2013; Liu et al., 2014; Qiu and Zhang, 2015; Chen et al., 2017; Huang et al., 2017). It has been shown that character-based methods outperform word-based methods for Chinese NER (He and Wang, 2008; Liu et al., 2010; Li et al., 2014). One drawback of character-based NER, however, is that explicit word and word sequence information is not fully exploited, which can be potentially useful. To address this issue, we integrate latent word information into characterbased LSTM-CRF by representing lexicon words from the sentence using a lattice structure LSTM. As shown in Figure 1, we construct a wordcharacter lattice by matching a sentence with a large automatically-obtained lexicon"
P18-1144,D15-1141,0,0.0459976,"dding lookup table. A bidirectional LSTM (same structurally as Eq. 11) is applied to x1 , x2 , . . . , xm to obtain → −c → − → − ← − ← − ← − h 1 , h c2 , . . . , h cm and h c1 , h c2 , . . . , h cm in the left-to-right and right-to-left directions, respectively, with two distinct sets of parameters. The hidden vector representation of each character is: xcj = [ec (cj ); eb (cj , cj+1 )], 京 South 南京 A standard CRF model (Eq. 17) is used on c h1 , hc2 , . . . , hcm for sequence labelling. • Char + bichar. Character bigrams have been shown useful for representing characters in word segmentation (Chen et al., 2015; Yang et al., 2017a). We augment the character-based model with bigram information by concatenating bigram embeddings with character embeddings: B-‐LOC ?#&quot; The character-based model is shown in Figure 3(a). It uses an LSTM-CRF model on the character sequence c1 , c2 , . . . , cm . Each character cj is represented using → − ← − hcj = [ h cj ; h cj ] E-‐LOC ?#( Character-Based Model xcj = ec (cj ) I-‐LOC Figure 3: Models.1 We augment the character representation with segmentation information by concatenating segmentation label embeddings to character embeddings: xcj = [ec (cj ); es (seg(cj )"
P18-1144,P17-1110,0,0.0233353,"s. One intuitive way of performing Chinese NER is to perform word segmentation first, before applying word sequence labeling. The segmentation → NER pipeline, however, can suffer the potential issue of error propagation, since NEs are an important source of OOV Equal contribution. 市 City 市长 南京 Nanjing Introduction ∗ 京 Capital in segmentation, and incorrectly segmented entity boundaries lead to NER errors. This problem can be severe in the open domain since crossdomain word segmentation remains an unsolved problem (Liu and Zhang, 2012; Jiang et al., 2013; Liu et al., 2014; Qiu and Zhang, 2015; Chen et al., 2017; Huang et al., 2017). It has been shown that character-based methods outperform word-based methods for Chinese NER (He and Wang, 2008; Liu et al., 2010; Li et al., 2014). One drawback of character-based NER, however, is that explicit word and word sequence information is not fully exploited, which can be potentially useful. To address this issue, we integrate latent word information into characterbased LSTM-CRF by representing lexicon words from the sentence using a lattice structure LSTM. As shown in Figure 1, we construct a wordcharacter lattice by matching a sentence with a large automatic"
P18-1144,P13-1075,0,0.052087,"Missing"
P18-1144,N16-1030,0,0.870489,"rms both word-based and character-based LSTM baselines, achieving the best results. 1 South mayor 南京市 Nanjing City 江 长 River Long 桥 大 Bridge Big 大桥 长江 Bridge Yangtze River 长江大桥 Yangtze River Bridge Figure 1: Word character lattice. As a fundamental task in information extraction, named entity recognition (NER) has received constant research attention over the recent years. The task has traditionally been solved as a sequence labeling problem, where entity boundary and category labels are jointly predicted. The current stateof-the-art for English NER has been achieved by using LSTM-CRF models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018) with character information being integrated into word representations. Chinese NER is correlated with word segmentation. In particular, named entity boundaries are also word boundaries. One intuitive way of performing Chinese NER is to perform word segmentation first, before applying word sequence labeling. The segmentation → NER pipeline, however, can suffer the potential issue of error propagation, since NEs are an important source of OOV Equal contribution. 市 City 市长 南京 Nanjing Introduction ∗ 京 Capital in segmentation, and incor"
P18-1144,W15-3904,0,0.122186,"Missing"
P18-1144,J05-4005,0,0.0485387,". (2016) use a character LSTM instead. Our baseline word-based system takes a similar structure to this line of work. Character sequence labeling has been the dominant approach for Chinese NER (Chen et al., 2006b; Lu et al., 2016; Dong et al., 2016). There have been explicit discussions comparing statistical word-based and character-based methods for the task, showing that the latter is empirically a superior choice (He and Wang, 2008; Liu et al., 2010; Li et al., 2014). We find that with proper How to better leverage word information for Chinese NER has received continued research attention (Gao et al., 2005), where segmentation information has been used as soft features for NER (Zhao and Kit, 2008; Peng and Dredze, 2015; He and Sun, 2017a), and joint segmentation and NER has been investigated using dual decomposition (Xu et al., 2014), multi-task learning (Peng and Dredze, 2016), etc. Our work is in line, focusing on neural representation learning. While the above methods can be affected by segmented training data and segmentation errors, our method does not require a word segmentor. The model is conceptually simpler by not considering multi-task settings. External sources of information has been"
P18-1144,W03-0426,0,0.176848,"nce. Compared with characterbased and word-based NER methods, our model has the advantage of leveraging explicit word information over character sequence labeling without suffering from segmentation error. Results show that our model significantly outperforms both character sequence labeling models and word sequence labeling models using LSTMCRF, giving the best results over a variety of Chinese NER datasets across different domains. Our code and data are released at https:// github.com/jiesutd/LatticeLSTM. 2 Related Work Our work is in line with existing methods using neural network for NER. Hammerton (2003) attempted to solve the problem using a unidirectional LSTM, which was among the first neural models for NER. Collobert et al. (2011) used a CNN-CRF structure, obtaining competitive results to the best statistical models. dos Santos et al. (2015) used character CNN to augment a CNN-CRF model. Most recent work leverages an LSTM-CRF architecture. Huang et al. (2015) uses hand-crafted spelling features; Ma and Hovy (2016) and Chiu and Nichols (2016) use a character CNN to represent spelling characteristics; Lample et al. (2016) use a character LSTM instead. Our baseline word-based system takes a"
P18-1144,E17-2113,0,0.320551,"nce labeling has been the dominant approach for Chinese NER (Chen et al., 2006b; Lu et al., 2016; Dong et al., 2016). There have been explicit discussions comparing statistical word-based and character-based methods for the task, showing that the latter is empirically a superior choice (He and Wang, 2008; Liu et al., 2010; Li et al., 2014). We find that with proper How to better leverage word information for Chinese NER has received continued research attention (Gao et al., 2005), where segmentation information has been used as soft features for NER (Zhao and Kit, 2008; Peng and Dredze, 2015; He and Sun, 2017a), and joint segmentation and NER has been investigated using dual decomposition (Xu et al., 2014), multi-task learning (Peng and Dredze, 2016), etc. Our work is in line, focusing on neural representation learning. While the above methods can be affected by segmented training data and segmentation errors, our method does not require a word segmentor. The model is conceptually simpler by not considering multi-task settings. External sources of information has been leveraged for NER. In particular, lexicon features have been widely used (Collobert et al., 2011; Passos et al., 2014; Huang et al."
P18-1144,I08-4022,0,0.707363,"entation → NER pipeline, however, can suffer the potential issue of error propagation, since NEs are an important source of OOV Equal contribution. 市 City 市长 南京 Nanjing Introduction ∗ 京 Capital in segmentation, and incorrectly segmented entity boundaries lead to NER errors. This problem can be severe in the open domain since crossdomain word segmentation remains an unsolved problem (Liu and Zhang, 2012; Jiang et al., 2013; Liu et al., 2014; Qiu and Zhang, 2015; Chen et al., 2017; Huang et al., 2017). It has been shown that character-based methods outperform word-based methods for Chinese NER (He and Wang, 2008; Liu et al., 2010; Li et al., 2014). One drawback of character-based NER, however, is that explicit word and word sequence information is not fully exploited, which can be potentially useful. To address this issue, we integrate latent word information into characterbased LSTM-CRF by representing lexicon words from the sentence using a lattice structure LSTM. As shown in Figure 1, we construct a wordcharacter lattice by matching a sentence with a large automatically-obtained lexicon. As a result, word sequences such as “长江大桥 (Yangtze River Bridge)”, “长江 (Yangtze River)” and “大 桥 (Bridge)” can"
P18-1144,li-etal-2014-comparison,0,0.465221,"Missing"
P18-1144,C12-2073,1,0.839849,"ord segmentation. In particular, named entity boundaries are also word boundaries. One intuitive way of performing Chinese NER is to perform word segmentation first, before applying word sequence labeling. The segmentation → NER pipeline, however, can suffer the potential issue of error propagation, since NEs are an important source of OOV Equal contribution. 市 City 市长 南京 Nanjing Introduction ∗ 京 Capital in segmentation, and incorrectly segmented entity boundaries lead to NER errors. This problem can be severe in the open domain since crossdomain word segmentation remains an unsolved problem (Liu and Zhang, 2012; Jiang et al., 2013; Liu et al., 2014; Qiu and Zhang, 2015; Chen et al., 2017; Huang et al., 2017). It has been shown that character-based methods outperform word-based methods for Chinese NER (He and Wang, 2008; Liu et al., 2010; Li et al., 2014). One drawback of character-based NER, however, is that explicit word and word sequence information is not fully exploited, which can be potentially useful. To address this issue, we integrate latent word information into characterbased LSTM-CRF by representing lexicon words from the sentence using a lattice structure LSTM. As shown in Figure 1, we c"
P18-1144,D14-1093,1,0.873098,"tity boundaries are also word boundaries. One intuitive way of performing Chinese NER is to perform word segmentation first, before applying word sequence labeling. The segmentation → NER pipeline, however, can suffer the potential issue of error propagation, since NEs are an important source of OOV Equal contribution. 市 City 市长 南京 Nanjing Introduction ∗ 京 Capital in segmentation, and incorrectly segmented entity boundaries lead to NER errors. This problem can be severe in the open domain since crossdomain word segmentation remains an unsolved problem (Liu and Zhang, 2012; Jiang et al., 2013; Liu et al., 2014; Qiu and Zhang, 2015; Chen et al., 2017; Huang et al., 2017). It has been shown that character-based methods outperform word-based methods for Chinese NER (He and Wang, 2008; Liu et al., 2010; Li et al., 2014). One drawback of character-based NER, however, is that explicit word and word sequence information is not fully exploited, which can be potentially useful. To address this issue, we integrate latent word information into characterbased LSTM-CRF by representing lexicon words from the sentence using a lattice structure LSTM. As shown in Figure 1, we construct a wordcharacter lattice by ma"
P18-1144,L16-1138,1,0.626571,"ed a CNN-CRF structure, obtaining competitive results to the best statistical models. dos Santos et al. (2015) used character CNN to augment a CNN-CRF model. Most recent work leverages an LSTM-CRF architecture. Huang et al. (2015) uses hand-crafted spelling features; Ma and Hovy (2016) and Chiu and Nichols (2016) use a character CNN to represent spelling characteristics; Lample et al. (2016) use a character LSTM instead. Our baseline word-based system takes a similar structure to this line of work. Character sequence labeling has been the dominant approach for Chinese NER (Chen et al., 2006b; Lu et al., 2016; Dong et al., 2016). There have been explicit discussions comparing statistical word-based and character-based methods for the task, showing that the latter is empirically a superior choice (He and Wang, 2008; Liu et al., 2010; Li et al., 2014). We find that with proper How to better leverage word information for Chinese NER has received continued research attention (Gao et al., 2005), where segmentation information has been used as soft features for NER (Zhao and Kit, 2008; Peng and Dredze, 2015; He and Sun, 2017a), and joint segmentation and NER has been investigated using dual decompositio"
P18-1144,D15-1104,0,0.0263337,"joint segmentation and NER has been investigated using dual decomposition (Xu et al., 2014), multi-task learning (Peng and Dredze, 2016), etc. Our work is in line, focusing on neural representation learning. While the above methods can be affected by segmented training data and segmentation errors, our method does not require a word segmentor. The model is conceptually simpler by not considering multi-task settings. External sources of information has been leveraged for NER. In particular, lexicon features have been widely used (Collobert et al., 2011; Passos et al., 2014; Huang et al., 2015; Luo et al., 2015). Rei (2017) uses a word-level language modeling objective to augment NER training, performing multi-task learning over large raw text. Peters et al. (2017) pretrain a character language model to enhance word representations. Yang et al. (2017b) exploit cross-domain and cross-lingual knowledge via multi-task learning. We leverage external data by pretraining word embedding lexicon over large automatically-segmented texts, while semisupervised techniques such as language modeling are orthogonal to and can also be used for our lattice LSTM model. Lattice structured RNNs can be viewed as a natura"
P18-1144,P16-1101,0,0.7211,"nd character-based LSTM baselines, achieving the best results. 1 South mayor 南京市 Nanjing City 江 长 River Long 桥 大 Bridge Big 大桥 长江 Bridge Yangtze River 长江大桥 Yangtze River Bridge Figure 1: Word character lattice. As a fundamental task in information extraction, named entity recognition (NER) has received constant research attention over the recent years. The task has traditionally been solved as a sequence labeling problem, where entity boundary and category labels are jointly predicted. The current stateof-the-art for English NER has been achieved by using LSTM-CRF models (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Liu et al., 2018) with character information being integrated into word representations. Chinese NER is correlated with word segmentation. In particular, named entity boundaries are also word boundaries. One intuitive way of performing Chinese NER is to perform word segmentation first, before applying word sequence labeling. The segmentation → NER pipeline, however, can suffer the potential issue of error propagation, since NEs are an important source of OOV Equal contribution. 市 City 市长 南京 Nanjing Introduction ∗ 京 Capital in segmentation, and incorrectly segmented en"
P18-1144,W14-1609,0,0.0171979,"nd Dredze, 2015; He and Sun, 2017a), and joint segmentation and NER has been investigated using dual decomposition (Xu et al., 2014), multi-task learning (Peng and Dredze, 2016), etc. Our work is in line, focusing on neural representation learning. While the above methods can be affected by segmented training data and segmentation errors, our method does not require a word segmentor. The model is conceptually simpler by not considering multi-task settings. External sources of information has been leveraged for NER. In particular, lexicon features have been widely used (Collobert et al., 2011; Passos et al., 2014; Huang et al., 2015; Luo et al., 2015). Rei (2017) uses a word-level language modeling objective to augment NER training, performing multi-task learning over large raw text. Peters et al. (2017) pretrain a character language model to enhance word representations. Yang et al. (2017b) exploit cross-domain and cross-lingual knowledge via multi-task learning. We leverage external data by pretraining word embedding lexicon over large automatically-segmented texts, while semisupervised techniques such as language modeling are orthogonal to and can also be used for our lattice LSTM model. Lattice st"
P18-1144,D15-1064,0,0.178735,"f work. Character sequence labeling has been the dominant approach for Chinese NER (Chen et al., 2006b; Lu et al., 2016; Dong et al., 2016). There have been explicit discussions comparing statistical word-based and character-based methods for the task, showing that the latter is empirically a superior choice (He and Wang, 2008; Liu et al., 2010; Li et al., 2014). We find that with proper How to better leverage word information for Chinese NER has received continued research attention (Gao et al., 2005), where segmentation information has been used as soft features for NER (Zhao and Kit, 2008; Peng and Dredze, 2015; He and Sun, 2017a), and joint segmentation and NER has been investigated using dual decomposition (Xu et al., 2014), multi-task learning (Peng and Dredze, 2016), etc. Our work is in line, focusing on neural representation learning. While the above methods can be affected by segmented training data and segmentation errors, our method does not require a word segmentor. The model is conceptually simpler by not considering multi-task settings. External sources of information has been leveraged for NER. In particular, lexicon features have been widely used (Collobert et al., 2011; Passos et al.,"
P18-1144,P16-2025,0,0.506937,"icit discussions comparing statistical word-based and character-based methods for the task, showing that the latter is empirically a superior choice (He and Wang, 2008; Liu et al., 2010; Li et al., 2014). We find that with proper How to better leverage word information for Chinese NER has received continued research attention (Gao et al., 2005), where segmentation information has been used as soft features for NER (Zhao and Kit, 2008; Peng and Dredze, 2015; He and Sun, 2017a), and joint segmentation and NER has been investigated using dual decomposition (Xu et al., 2014), multi-task learning (Peng and Dredze, 2016), etc. Our work is in line, focusing on neural representation learning. While the above methods can be affected by segmented training data and segmentation errors, our method does not require a word segmentor. The model is conceptually simpler by not considering multi-task settings. External sources of information has been leveraged for NER. In particular, lexicon features have been widely used (Collobert et al., 2011; Passos et al., 2014; Huang et al., 2015; Luo et al., 2015). Rei (2017) uses a word-level language modeling objective to augment NER training, performing multi-task learning over"
P18-1144,Q17-1008,0,0.024907,") pretrain a character language model to enhance word representations. Yang et al. (2017b) exploit cross-domain and cross-lingual knowledge via multi-task learning. We leverage external data by pretraining word embedding lexicon over large automatically-segmented texts, while semisupervised techniques such as language modeling are orthogonal to and can also be used for our lattice LSTM model. Lattice structured RNNs can be viewed as a natural extension of tree-structured RNNs (Tai et al., 2015) to DAGs. They have been used to model motion dynamics (Sun et al., 2017), dependencydiscourse DAGs (Peng et al., 2017), as well as speech tokenization lattice (Sperber et al., 2017) and multi-granularity segmentation outputs (Su et al., 2017) for NMT encoders. Compared with existing work, our lattice LSTM is different in both motivation and structure. For example, being designed for character-centric lattice-LSTMCRF sequence labeling, it has recurrent cells but not hidden vectors for words. To our knowledge, we are the first to design a novel lattice LSTM representation for mixed characters and lexicon words, and the first to use a word-character lattice for segmentation-free Chinese NER. 1555 3 B-‐LOC Model"
P18-1144,P17-1161,0,0.0217225,"s in line, focusing on neural representation learning. While the above methods can be affected by segmented training data and segmentation errors, our method does not require a word segmentor. The model is conceptually simpler by not considering multi-task settings. External sources of information has been leveraged for NER. In particular, lexicon features have been widely used (Collobert et al., 2011; Passos et al., 2014; Huang et al., 2015; Luo et al., 2015). Rei (2017) uses a word-level language modeling objective to augment NER training, performing multi-task learning over large raw text. Peters et al. (2017) pretrain a character language model to enhance word representations. Yang et al. (2017b) exploit cross-domain and cross-lingual knowledge via multi-task learning. We leverage external data by pretraining word embedding lexicon over large automatically-segmented texts, while semisupervised techniques such as language modeling are orthogonal to and can also be used for our lattice LSTM model. Lattice structured RNNs can be viewed as a natural extension of tree-structured RNNs (Tai et al., 2015) to DAGs. They have been used to model motion dynamics (Sun et al., 2017), dependencydiscourse DAGs (P"
P18-1144,W09-1119,0,0.157559,"ovy, 2016; Lample et al., 2016), using LSTM-CRF as the main network structure. Formally, denote an input sentence as s = c1 , c2 , . . . , cm , where cj denotes the jth character. s can further be seen as a word sequence s = w1 , w2 , . . . , wn , where wi denotes the ith word in the sentence, obtained using a Chinese segmentor. We use t(i, k) to denote the index j for the kth character in the ith word in the sentence. Take the sentence in Figure 1 for example. If the segmentation is “南京市 长江大桥”, and indices are from 1, then t(2, 1) = 4 (长) and t(1, 3) = 3 (市). We use the BIOES tagging scheme (Ratinov and Roth, 2009) for both wordbased and character-based NER tagging. 3.1 ?#& ?#' ?#( ?#&quot; ?#& ?#' ?#( ?#&quot; ?#& ?#' 南 B-‐LOC Long B-‐LOC E-‐LOC ?# &quot; ?# & ?# ' ?# ( ?# &quot; ?# & ?# ' ?# ( ?# &quot; ?# & ?# ' ?# ( 市 City 长江 大桥 Yangtze River Bridge (b) Word-based model. B-‐LOC I-‐LOC E-‐LOC ?#' ?#&quot; ?#& ?#' ?#&quot; ?#& ?#' ?#&quot; 南 South ?) ',& 京 Capital ?#& ?) ',& 市 City 南京市 Nanjing City (c) Lattice model. (2) where eb denotes a charater bigram lookup table. • Char + softword. It has been shown that using segmentation as soft features for character-based NER models can lead to improved performance (Zhao and Kit, 2008; Peng"
P18-1144,P17-1194,0,0.014855,"and NER has been investigated using dual decomposition (Xu et al., 2014), multi-task learning (Peng and Dredze, 2016), etc. Our work is in line, focusing on neural representation learning. While the above methods can be affected by segmented training data and segmentation errors, our method does not require a word segmentor. The model is conceptually simpler by not considering multi-task settings. External sources of information has been leveraged for NER. In particular, lexicon features have been widely used (Collobert et al., 2011; Passos et al., 2014; Huang et al., 2015; Luo et al., 2015). Rei (2017) uses a word-level language modeling objective to augment NER training, performing multi-task learning over large raw text. Peters et al. (2017) pretrain a character language model to enhance word representations. Yang et al. (2017b) exploit cross-domain and cross-lingual knowledge via multi-task learning. We leverage external data by pretraining word embedding lexicon over large automatically-segmented texts, while semisupervised techniques such as language modeling are orthogonal to and can also be used for our lattice LSTM model. Lattice structured RNNs can be viewed as a natural extension"
P18-1144,D17-1145,0,0.0419394,"entations. Yang et al. (2017b) exploit cross-domain and cross-lingual knowledge via multi-task learning. We leverage external data by pretraining word embedding lexicon over large automatically-segmented texts, while semisupervised techniques such as language modeling are orthogonal to and can also be used for our lattice LSTM model. Lattice structured RNNs can be viewed as a natural extension of tree-structured RNNs (Tai et al., 2015) to DAGs. They have been used to model motion dynamics (Sun et al., 2017), dependencydiscourse DAGs (Peng et al., 2017), as well as speech tokenization lattice (Sperber et al., 2017) and multi-granularity segmentation outputs (Su et al., 2017) for NMT encoders. Compared with existing work, our lattice LSTM is different in both motivation and structure. For example, being designed for character-centric lattice-LSTMCRF sequence labeling, it has recurrent cells but not hidden vectors for words. To our knowledge, we are the first to design a novel lattice LSTM representation for mixed characters and lexicon words, and the first to use a word-character lattice for segmentation-free Chinese NER. 1555 3 B-‐LOC Model We follow the best English NER model (Huang et al., 2015; Ma a"
P18-1144,P15-1150,0,0.108549,"Missing"
P18-1144,O03-4002,0,0.172304,"cj ] E-‐LOC ?#( Character-Based Model xcj = ec (cj ) I-‐LOC Figure 3: Models.1 We augment the character representation with segmentation information by concatenating segmentation label embeddings to character embeddings: xcj = [ec (cj ); es (seg(cj ))], (4) where es represents a segmentation label embedding lookup table. seg(cj ) denotes the segmentation label on the character cj given by a word segmentor. We use the BMES scheme for repre1 To keep the figure concise, we (i) do not show gate cells, which uses ht−1 for calculating ct ; (ii) only show one direction. 1556 senting segmentation (Xue, 2003). − → − w ← w hw i = [hi ; hi ] (5) Similar to the character-based case, a standard w w CRF model (Eq. 17) is used on hw 1 , h2 , . . . , hm for sequence labelling. 3.2 • Word + char CNN. A standard CNN (LeCun et al., 1989) structure is used on the character sequence of each word to obtain its character representation xci . Denoting the embedding of character cj as ec (cj ), the vector xci is given by:  xci = Word-Based Model max t(i,1)≤j≤t(i,len(i))  (W&gt; C NN  ec (cj− ke−1 ) 2 ... ec (cj+ ke−1 )    + bC NN ), 2 The word-based model is shown in Figure 3(b). It takes the word embedding ew"
P18-1144,P17-1078,1,0.888694,"cted by segmented training data and segmentation errors, our method does not require a word segmentor. The model is conceptually simpler by not considering multi-task settings. External sources of information has been leveraged for NER. In particular, lexicon features have been widely used (Collobert et al., 2011; Passos et al., 2014; Huang et al., 2015; Luo et al., 2015). Rei (2017) uses a word-level language modeling objective to augment NER training, performing multi-task learning over large raw text. Peters et al. (2017) pretrain a character language model to enhance word representations. Yang et al. (2017b) exploit cross-domain and cross-lingual knowledge via multi-task learning. We leverage external data by pretraining word embedding lexicon over large automatically-segmented texts, while semisupervised techniques such as language modeling are orthogonal to and can also be used for our lattice LSTM model. Lattice structured RNNs can be viewed as a natural extension of tree-structured RNNs (Tai et al., 2015) to DAGs. They have been used to model motion dynamics (Sun et al., 2017), dependencydiscourse DAGs (Peng et al., 2017), as well as speech tokenization lattice (Sperber et al., 2017) and mu"
P18-1144,W06-0126,0,0.727928,"Missing"
P18-1144,I08-4017,0,0.394747,"cture to this line of work. Character sequence labeling has been the dominant approach for Chinese NER (Chen et al., 2006b; Lu et al., 2016; Dong et al., 2016). There have been explicit discussions comparing statistical word-based and character-based methods for the task, showing that the latter is empirically a superior choice (He and Wang, 2008; Liu et al., 2010; Li et al., 2014). We find that with proper How to better leverage word information for Chinese NER has received continued research attention (Gao et al., 2005), where segmentation information has been used as soft features for NER (Zhao and Kit, 2008; Peng and Dredze, 2015; He and Sun, 2017a), and joint segmentation and NER has been investigated using dual decomposition (Xu et al., 2014), multi-task learning (Peng and Dredze, 2016), etc. Our work is in line, focusing on neural representation learning. While the above methods can be affected by segmented training data and segmentation errors, our method does not require a word segmentor. The model is conceptually simpler by not considering multi-task settings. External sources of information has been leveraged for NER. In particular, lexicon features have been widely used (Collobert et al."
P18-1144,W06-0115,0,\N,Missing
P18-1144,P18-4006,1,\N,Missing
P18-1144,Q16-1026,0,\N,Missing
P18-1150,W13-2322,0,0.343839,"le to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus faces challenges with large graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature. 1 :ARG0 :ARG1 :ARG2 person :name name genius :op1 ""Ryan"" Figure 1: An example of AMR graph meaning “Ryan’s description of himself: a genius.” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1 shows an AMR graph in which the nodes (such as “describe-01” and “person”) represent the concepts, and edges (such as “:ARG0” and “:name”) represent the relations between concepts they connect. AMR has been proven helpful on other NLP tasks, such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the"
P18-1150,D17-1209,0,0.0587751,"to provide research and institutes in the center . G2S: the agreement provides the staff of research centers and funding . G2S+CP: the agreement provides the staff of the research center and the funding . Table 3: Example system outputs. graphs by breadth-first traversal, and then use a phrase-based machine translation system2 to generate results by translating linearized sequences. Prior work using graph neural networks for NLP include the use graph convolutional networks (GCN) (Kipf and Welling, 2017) for semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017). Both GCN and the graph LSTM update node states by exchanging information between neighboring nodes within each iteration. However, our graph state LSTM adopts gated operations for making updates, while GCN uses a linear transformation. Intuitively, the former has better learning power than the later. Another major difference is that our graph state LSTM keeps a cell vector for each node to remember all history. The contrast 1623 2 http://www.statmt.org/moses/ between our model with GCN is reminiscent of the contrast between RNN and CNN. We leave empirical comparison of their effectiveness to"
P18-1150,S16-1186,0,0.439676,"al, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closel"
P18-1150,N16-1087,0,0.285942,"al, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closel"
P18-1150,S17-2159,0,0.0529915,"n is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and siblings can be far away after serialization. It can be difficult for a l"
P18-1150,P16-1154,0,0.593108,"tional Linguistics (Long Papers), pages 1616–1626 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics To capture non-local information, the encoder performs graph state transition by information exchange between connected nodes, with a graph state consisting of all node states. Multiple recurrent transition steps are taken so that information can propagate non-locally, and LSTM (Hochreiter and Schmidhuber, 1997) is used to avoid gradient diminishing and bursting in the recurrent process. The decoder is an attention-based LSTM model with a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016), which helps copy sparse tokens (such as numbers and named entities) from the input. Trained on a standard dataset (LDC2015E86), our model surpasses a strong sequence-tosequence baseline by 2.3 BLEU points, demonstrating the advantage of graph-to-sequence models for AMR-to-text generation compared to sequence-to-sequence models. Our final model achieves a BLEU score of 23.3 on the test set, which is 1.3 points higher than the existing state of the art (Konstas et al., 2017) trained on the same dataset. When using gigaword sentences as additional training data, our mode"
P18-1150,P16-1014,0,0.452163,"s (Long Papers), pages 1616–1626 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics To capture non-local information, the encoder performs graph state transition by information exchange between connected nodes, with a graph state consisting of all node states. Multiple recurrent transition steps are taken so that information can propagate non-locally, and LSTM (Hochreiter and Schmidhuber, 1997) is used to avoid gradient diminishing and bursting in the recurrent process. The decoder is an attention-based LSTM model with a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016), which helps copy sparse tokens (such as numbers and named entities) from the input. Trained on a standard dataset (LDC2015E86), our model surpasses a strong sequence-tosequence baseline by 2.3 BLEU points, demonstrating the advantage of graph-to-sequence models for AMR-to-text generation compared to sequence-to-sequence models. Our final model achieves a BLEU score of 23.3 on the test set, which is 1.3 points higher than the existing state of the art (Konstas et al., 2017) trained on the same dataset. When using gigaword sentences as additional training data, our model is consistently better"
P18-1150,C12-1083,0,0.109094,"Missing"
P18-1150,N03-1017,0,0.00861135,"We perform a similar experiment for the Seq2seq+copy baseline by only executing singledirectional LSTM for the encoder. We observe BLEU scores of 11.8 and 12.7 using only forward or backward LSTM, respectively. This is consistent with our graph model in that execution using only one direction leads to a huge performance drop. The contrast is also reminiscent of using the normal input versus the reversed input in neural machine translation (Sutskever et al., 2014). 5.5 model trained with the anonymized data. PBMT (Pourdamghani et al., 2016) adopts a phrase-based model for machine translation (Koehn et al., 2003) on the input of linearized AMR graph, SNRG (Song et al., 2017) uses synchronous node replacement grammar for parsing the AMR graph while generating the text, and Tree2Str (Flanigan et al., 2016b) converts AMR graphs into trees by splitting the re-entrances before using a tree transducer to generate the results. Graph2seq+charLSTM+copy achieves a BLEU score of 23.3, which is 1.3 points better than MSeq2seq+Anon trained on the same AMR corpus. In addition, our model without character LSTM is still 0.7 BLEU points higher than MSeq2seq+Anon. Note that MSeq2seq+Anon relies on anonymization, which"
P18-1150,P17-1014,0,0.218578,"s can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and siblings can be far away after serialization. It can be difficult for a linear recurrent neural network to automatically induce their original connections from bracketed string forms. To address this issue, we introduce a novel graph-to-sequence model, where a graph-state LSTM is used to encode AMR structure"
P18-1150,S17-2096,0,0.214613,"Missing"
P18-1150,W15-4502,0,0.0367213,"nius.” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1 shows an AMR graph in which the nodes (such as “describe-01” and “person”) represent the concepts, and edges (such as “:ARG0” and “:name”) represent the relations between concepts they connect. AMR has been proven helpful on other NLP tasks, such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and"
P18-1150,D17-1159,0,0.0610451,"rovide staff and funding for the research center . S2S: agreed to provide research and institutes in the center . G2S: the agreement provides the staff of research centers and funding . G2S+CP: the agreement provides the staff of the research center and the funding . Table 3: Example system outputs. graphs by breadth-first traversal, and then use a phrase-based machine translation system2 to generate results by translating linearized sequences. Prior work using graph neural networks for NLP include the use graph convolutional networks (GCN) (Kipf and Welling, 2017) for semantic role labeling (Marcheggiani and Titov, 2017) and neural machine translation (Bastings et al., 2017). Both GCN and the graph LSTM update node states by exchanging information between neighboring nodes within each iteration. However, our graph state LSTM adopts gated operations for making updates, while GCN uses a linear transformation. Intuitively, the former has better learning power than the later. Another major difference is that our graph state LSTM keeps a cell vector for each node to remember all history. The contrast 1623 2 http://www.statmt.org/moses/ between our model with GCN is reminiscent of the contrast between RNN and CNN."
P18-1150,P05-1012,0,0.0220157,"y incoming or outgoing edges are used. From the results, we can see that there is a huge drop when state transition is performed only with incoming or outgoing edges. Using edges of one direction, the node states only contain information of ancestors or descendants. On the other hand, node states contain information of ancestors, descendants, and siblings if edges of both directions are used. From the results, we can conclude that not only the ancestors and descendants, but also the siblings are important for modeling the AMR graphs. This is similar to observations on syntactic parsing tasks (McDonald et al., 2005), where sibling features are adopted. We perform a similar experiment for the Seq2seq+copy baseline by only executing singledirectional LSTM for the encoder. We observe BLEU scores of 11.8 and 12.7 using only forward or backward LSTM, respectively. This is consistent with our graph model in that execution using only one direction leads to a huge performance drop. The contrast is also reminiscent of using the normal input versus the reversed input in neural machine translation (Sutskever et al., 2014). 5.5 model trained with the anonymized data. PBMT (Pourdamghani et al., 2016) adopts a phrase-"
P18-1150,S17-2158,0,0.0361727,"MR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and siblings can be far away after serialization. It"
P18-1150,P02-1040,0,0.103679,"onstas et al. (2017) only works on the anonymized data. For training on both sampled data and LDC2015E86, we also follow the method of Konstas et al. (2017), which is fine-tuning the model on the AMR corpus after every epoch of pretraining on the gigaword data. 5.2 Settings We extract a vocabulary from the training set, which is shared by both the encoder and the decoder. The word embeddings are initialized from Glove pretrained word embeddings (Pennington et al., 2014) on Common Crawl, and are not updated during training. Following existing work, we evaluate the results with the BLEU metric (Papineni et al., 2002). For model hyperparameters, we set the graph state transition number as 9 according to development experiments. Each node takes information from at most 10 neighbors. The hidden vector sizes for both encoder and decoder are set to 300 (They are set to 600 for experiments using largescale automatic data). Both character embeddings and hidden layer sizes for character LSTMs are set 100, and at most 20 characters are taken for each graph node or linearized token. Data We use a standard AMR corpus (LDC2015E86) as our experimental dataset, which contains 16,833 instances for training, 1368 for dev"
P18-1150,Q17-1008,0,0.0361108,"te LSTM adopts gated operations for making updates, while GCN uses a linear transformation. Intuitively, the former has better learning power than the later. Another major difference is that our graph state LSTM keeps a cell vector for each node to remember all history. The contrast 1623 2 http://www.statmt.org/moses/ between our model with GCN is reminiscent of the contrast between RNN and CNN. We leave empirical comparison of their effectiveness to future work. In this work our main goal is to show that graph LSTM encoding of AMR is superior compared with sequence LSTM. Closest to our work, Peng et al. (2017) modeled syntactic and discourse structures using DAG LSTM, which can be viewed as extensions to tree LSTMs (Tai et al., 2015). The state update follows the sentence order for each node, and has sequential nature. Our state update is in parallel. In addition, Peng et al. (2017) split input graphs into separate DAGs before their method can be used. To our knowledge, we are the first to apply an LSTM structure to encode AMR graphs. The recurrent information exchange mechanism in our state transition process is remotely related to the idea of loopy belief propagation (LBP) (Murphy et al., 1999)."
P18-1150,D14-1162,0,0.0936009,"19.9 20.6 20.4 22.2 22.1 22.8 Time 35.4s 37.4s 39.7s 11.2s 11.1s 9.2s 16.3s Table 1: D EV BLEU scores and decoding times. AMRs, as the AMR parser of Konstas et al. (2017) only works on the anonymized data. For training on both sampled data and LDC2015E86, we also follow the method of Konstas et al. (2017), which is fine-tuning the model on the AMR corpus after every epoch of pretraining on the gigaword data. 5.2 Settings We extract a vocabulary from the training set, which is shared by both the encoder and the decoder. The word embeddings are initialized from Glove pretrained word embeddings (Pennington et al., 2014) on Common Crawl, and are not updated during training. Following existing work, we evaluate the results with the BLEU metric (Papineni et al., 2002). For model hyperparameters, we set the graph state transition number as 9 according to development experiments. Each node takes information from at most 10 neighbors. The hidden vector sizes for both encoder and decoder are set to 300 (They are set to 600 for experiments using largescale automatic data). Both character embeddings and hidden layer sizes for character LSTMs are set 100, and at most 20 characters are taken for each graph node or line"
P18-1150,W16-6603,0,0.403474,"(Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as par"
P18-1150,P17-1099,0,0.0409756,"tely related to the idea of loopy belief propagation (LBP) (Murphy et al., 1999). However, there are two major differences. First, messages between LSTM states are gated neural node values, rather than probabilities in LBP. Second, while the goal of LBP is to estimate marginal probabilities, the goal of information exchange between graph states in our LSTM is to find neural representation features, which are directly optimized by a task objective. In addition to NMT (Gulcehre et al., 2016), the copy mechanism has been shown effective on tasks such as dialogue (Gu et al., 2016), summarization (See et al., 2017) and question generation (Song et al., 2018). We investigate the copy mechanism on AMR-to-text generation. 7 Conclusion We introduced a novel graph-to-sequence model for AMR-to-text generation. Compared to sequence-to-sequence models, which require linearization of AMR before decoding, a graph LSTM is leveraged to directly model full AMR structure. Allowing high parallelization, the graph encoder is more efficient than the sequence encoder. In our experiments, the graph model outperforms a strong sequence-to-sequence model, achieving the best performance. Acknowledgments We thank the anonymize"
P18-1150,P17-2002,1,0.902838,"event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2016; Song et al., 2017; Lampouras and Vlachos, 2017; Mille et al., 2017; Gruzitis et al., 2017), recent research has demonstrated the success of deep learning, and in particular the sequence-to-sequence model (Sutskever et al., 2014), which has achieved the state-of-the-art results on AMR-to-text generation (Konstas et al., 2017). One limitation of sequence-to-sequence models, however, is that they require serialization of input AMR graphs, which adds to the challenge of representing graph structure information, especially when the graph is large. In particular, closely-related nodes, such as parents, children and"
P18-1150,N18-2090,1,0.825928,"opagation (LBP) (Murphy et al., 1999). However, there are two major differences. First, messages between LSTM states are gated neural node values, rather than probabilities in LBP. Second, while the goal of LBP is to estimate marginal probabilities, the goal of information exchange between graph states in our LSTM is to find neural representation features, which are directly optimized by a task objective. In addition to NMT (Gulcehre et al., 2016), the copy mechanism has been shown effective on tasks such as dialogue (Gu et al., 2016), summarization (See et al., 2017) and question generation (Song et al., 2018). We investigate the copy mechanism on AMR-to-text generation. 7 Conclusion We introduced a novel graph-to-sequence model for AMR-to-text generation. Compared to sequence-to-sequence models, which require linearization of AMR before decoding, a graph LSTM is leveraged to directly model full AMR structure. Allowing high parallelization, the graph encoder is more efficient than the sequence encoder. In our experiments, the graph model outperforms a strong sequence-to-sequence model, achieving the best performance. Acknowledgments We thank the anonymized reviewers for the insightful comments, and"
P18-1150,P15-1150,0,0.208715,"Missing"
P18-1150,D16-1112,0,0.0316661,"aning “Ryan’s description of himself: a genius.” Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1 shows an AMR graph in which the nodes (such as “describe-01” and “person”) represent the concepts, and edges (such as “:ARG0” and “:name”) represent the relations between concepts they connect. AMR has been proven helpful on other NLP tasks, such as machine translation (Jones et al., 2012; Tamchyna et al., 2015), question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016) and event detection (Li et al., 2015). The task of AMR-to-text generation is to produce a text with the same meaning as a given input AMR graph. The task is challenging as word tenses and function words are abstracted away when constructing AMR graphs from texts. The translation from AMR nodes to text phrases can be far from literal. For example, shown in Figure 1, “Ryan” is represented as “(p / person :name (n / name :op1 “Ryan”))”, and “description of” is represented as “(d / describe-01 :ARG1 )”. While initial work used statistical approaches (Flanigan et al., 2016b; Pourdamghani et al., 2"
P18-1150,W15-3504,0,0.0515809,"Missing"
P18-1150,P16-1008,0,0.0232013,". ; aN ] (4) where N is the number of input tokens. The decoder yields an output sequence w1 , w2 , . . . , wM by calculating a sequence of hidden states s1 , s2 . . . , sM recurrently. While generating the t-th word, the decoder considers five factors: (1) the attention memory A; (2) the previous hidden state of the LSTM model st 1 ; (3) the embedding of the current input (previously generated word) et ; (4) the previous context vector µt 1 , which is calculated with attention from A; and (5) the previous coverage vector γt 1 , which is the accumulation of all attention distributions so far (Tu et al., 2016). When t = 1, we initialize µ0 and γ0 as zero vectors, set e1 to the embedding of the start token “<s>”, and s0 as the average of all encoder states. For each time-step t, the decoder feeds the concatenation of the embedding of the current input et and the previous context vector µt 1 into the 1617 In order to capture non-local interaction between nodes, we allow information exchange between nodes through a sequence of state transitions, leading to a sequence of states g0 , g1 , . . . , gt , . . . , where gt = {hjt }|vj ∈V . The initial state g0 consists of a set of initial node states hj0 = h"
P18-2088,N09-1003,0,0.172175,"Missing"
P18-2088,P14-1023,0,0.0478418,"ille University 4 ISTD, Singapore University of Technology and Design Abstract ing the semantic content of a word. Such a limitation led to the introduction of alternative metrics based on feature ranking, which have been reported to outperform vector cosine in several similarity tasks (Santus et al., 2016a,b). Recently, the focus of the research on word representations has been shifting onto the so-called word embeddings (WE), which are dense vectors obtained by means of neural network training that achieved significant improvements in several similarity-related tasks (Mikolov et al., 2013a; Baroni et al., 2014). Although the representation type of the embeddings was helpful for reducing the sparsity of traditional count vectors, their nature does not sensibly differ (Levy et al., 2015). Most research works involving WE still adopt vector cosine for similarity estimation, yet little experimentation has been done on alternative metrics for comparing dense representations (exceptions include Camacho-Collados et al. (2015)). Some attempts to directly transfer rank-based measures from traditional DSMs to WE have faced difficulties (see, for example, Jebbara et al. (2018)). In this paper, we suggest a pos"
P18-2088,W16-2502,0,0.02045,"rity about them), as opposed to ‘genuine’ semantic similarity (i.e. the relation holding between concepts such as coffee and tea) (Agirre et al., 2009; Hill et al., 2015; Gladkova and Drozd, 2016). Therefore, when testing a DSM, it is important to pay attention to what type of semantic relation is actually modeled by the evaluation dataset. Moreover, researchers pointed out that similarity estimation alone does not constitute a strong benchmark, as the inter-annotator agreement is relatively low in all datasets and the performance of several automated systems is already above the upper bound (Batchkarov et al., 2016). As a consequence, workshops such as RepEval have been organized with the explicit purpose of finding alternative evaluation tasks for DSMs. A recent proposal is the challenging outlier detection task (Camacho-Collados and Navigli, 2016; Blair et al., 2016), which consists in the recognition of cluster membership, as well as of a relative degree of semantic dissimilarity. The task is described as follows: given a group of words, identify the outlier, namely the word that does not belong to the group (i.e. the one that is less similar to the others). On top of its potential applications (e.g."
P18-2088,D14-1162,0,0.0879466,"it on the similarity subset of WordSim dataset, obtaining the optimal value of p = 0.1, which has been successfully used in all evaluations, under all settings (i.e. embedding types and training corpora). Such regularity allows us to consider p = 0.1 as a constant, therefore dropping p. Since in WE we can drop also the parameter N by defining N = |f |, AP SynP can be not parametrized at all. 4 4.1 Datasets Evaluation Settings Embeddings For our experiments, we used two popular word embeddings architectures: the Skip-Gram with negative sampling (Mikolov et al., 2013a,b) and the GloVe vectors (Pennington et al., 2014) (standard hyperparameter settings: 300 dimensions, 4 We also performed experiments with CBOW embeddings (Mikolov et al., 2013b), but results were irregular and inconsistent. We leave therefore their analysis to future work. 5 Dump of Nov. 2014, approx. 1.7 billion words. 554 Cosine AP Syn AP SynP Skip-Gram WordSim-353 MEN Simlex-999 0.736 0.758 0.364 0.599 0.643 0.343 0.710 0.737 0.369 WordSim-353 0.511 0.356 0.607 GloVe MEN 0.640 0.393 0.670 Simlex-999 0.311 0.174 0.335 Table 1: Similarity Estimation, Spearman Correlation by Setting. Embeddings trained on Wikipedia. CC − Cos AP Syn AP SynP P"
P18-2088,D17-1068,1,0.848381,"y are likely to be useful for setting the outliers apart. In fact, a cohesive cluster should be mostly characterized by the same ‘salient’ dimensions, and thus, basing word comparisons on such dimensions should lead to more reliable estimates of cluster membership. In our contribution, we propose to adapt AP Syn, a metric originally proposed by Santus et al. (2016a,b), to dense word embeddings representations.3 AP Syn was shown to perform well on both synonymy detection and similarity estimation tasks, and it was recently adapted to achieve state-of-the-art results in thematic fit estimation (Santus et al., 2017). The original AP Syn formula is shown in equation 1. i=N AP Syn(wx , wy ) = X i=0 1 AV G(rsx (fi ), rssy (fi )) (1) For every feature fi in the intersection between the top N features of two vectors wx and wy , we 3 The number of dimensions in word embeddings is in the scale of hundreds, and thus the dimensionality is way lower than in the original DSMs used by Santus and colleagues. 553 add the inverse of the average rank of such feature, rsx (fx ) and rsy (fy ), in the two decreasingly value-sorted vectors sx and sy (in traditional vectors, often the parameter N ≥ 1000, but in WE N = |f |)."
P18-2088,Y16-2021,1,0.893749,"d Embeddings Enrico Santus1 , Hongmin Wang2 , Emmanuele Chersoni3 and Yue Zhang4 esantus@mit.edu hongmin wang@cs.ucsb.edu emmanuelechersoni@gmail.com yue zhang@sutd.edu.sg 1 Computer Science and Artificial Intelligence Lab, MIT 2 Department of Computer Science, University of California Santa Barbara 3 Aix-Marseille University 4 ISTD, Singapore University of Technology and Design Abstract ing the semantic content of a word. Such a limitation led to the introduction of alternative metrics based on feature ranking, which have been reported to outperform vector cosine in several similarity tasks (Santus et al., 2016a,b). Recently, the focus of the research on word representations has been shifting onto the so-called word embeddings (WE), which are dense vectors obtained by means of neural network training that achieved significant improvements in several similarity-related tasks (Mikolov et al., 2013a; Baroni et al., 2014). Although the representation type of the embeddings was helpful for reducing the sparsity of traditional count vectors, their nature does not sensibly differ (Levy et al., 2015). Most research works involving WE still adopt vector cosine for similarity estimation, yet little experiment"
P18-2088,W16-2508,0,0.275329,", it is important to pay attention to what type of semantic relation is actually modeled by the evaluation dataset. Moreover, researchers pointed out that similarity estimation alone does not constitute a strong benchmark, as the inter-annotator agreement is relatively low in all datasets and the performance of several automated systems is already above the upper bound (Batchkarov et al., 2016). As a consequence, workshops such as RepEval have been organized with the explicit purpose of finding alternative evaluation tasks for DSMs. A recent proposal is the challenging outlier detection task (Camacho-Collados and Navigli, 2016; Blair et al., 2016), which consists in the recognition of cluster membership, as well as of a relative degree of semantic dissimilarity. The task is described as follows: given a group of words, identify the outlier, namely the word that does not belong to the group (i.e. the one that is less similar to the others). On top of its potential applications (e.g. ontology learning), detecting outliers in clusters is a goal that poses a more strict quality requirement on the distributional representations compared to tests based simply on pairwise comparisons, as it is required that similar words"
P18-2088,L16-1723,1,0.844757,"d Embeddings Enrico Santus1 , Hongmin Wang2 , Emmanuele Chersoni3 and Yue Zhang4 esantus@mit.edu hongmin wang@cs.ucsb.edu emmanuelechersoni@gmail.com yue zhang@sutd.edu.sg 1 Computer Science and Artificial Intelligence Lab, MIT 2 Department of Computer Science, University of California Santa Barbara 3 Aix-Marseille University 4 ISTD, Singapore University of Technology and Design Abstract ing the semantic content of a word. Such a limitation led to the introduction of alternative metrics based on feature ranking, which have been reported to outperform vector cosine in several similarity tasks (Santus et al., 2016a,b). Recently, the focus of the research on word representations has been shifting onto the so-called word embeddings (WE), which are dense vectors obtained by means of neural network training that achieved significant improvements in several similarity-related tasks (Mikolov et al., 2013a; Baroni et al., 2014). Although the representation type of the embeddings was helpful for reducing the sparsity of traditional count vectors, their nature does not sensibly differ (Levy et al., 2015). Most research works involving WE still adopt vector cosine for similarity estimation, yet little experiment"
P18-2088,N15-1059,0,0.147072,"led word embeddings (WE), which are dense vectors obtained by means of neural network training that achieved significant improvements in several similarity-related tasks (Mikolov et al., 2013a; Baroni et al., 2014). Although the representation type of the embeddings was helpful for reducing the sparsity of traditional count vectors, their nature does not sensibly differ (Levy et al., 2015). Most research works involving WE still adopt vector cosine for similarity estimation, yet little experimentation has been done on alternative metrics for comparing dense representations (exceptions include Camacho-Collados et al. (2015)). Some attempts to directly transfer rank-based measures from traditional DSMs to WE have faced difficulties (see, for example, Jebbara et al. (2018)). In this paper, we suggest a possible solution to this problem by adapting AP Syn, a rank-based similarity metric originally proposed for sparse vectors (Santus et al., 2016b,a), to low-dimensional word embeddings. This goal is achieved by removing the parameter N (the extent of the feature overlap to be taken into account) and adding a smoothing parameter that is proven to be constant under multiple settings, therefore making the measure unsup"
P18-2088,Y14-1018,1,0.862126,"ne as a baseline and we test an adaptation of a rank-based measure to the dense features of the word embeddings. Vector cosine computes the correlation between all the vector dimensions, independently of their relevance for a given word pair or for a semantic cluster, and this could be a limitation for discerning different degrees of dissimilarity. The alternative rank-based measure is based on the hypothesis that similarity consists of sharing many relevant features, whereas dissimilarity can be described as either the non-sharing of relevant features or the sharing of non-relevant features (Santus et al., 2014, 2016b). This hypothesis could turn out to be very helpful for a task like the outlier detection, where prominent features might be the key to improve clustering quality: semantic dimensions that are shared by many of the cluster elements should be weighted more, as they are likely to be useful for setting the outliers apart. In fact, a cohesive cluster should be mostly characterized by the same ‘salient’ dimensions, and thus, basing word comparisons on such dimensions should lead to more reliable estimates of cluster membership. In our contribution, we propose to adapt AP Syn, a metric origi"
P18-2088,W16-2507,0,0.0195833,"es assigned to the pairs by the subjects and the cosines of the corresponding vectors (similary estimation task). Similarity as modeled by DSMs has been under debate, as its definition is underspecified. It in fact includes an ambiguity with the more generic notion of semantic relatedness, which is present also in many popular datasets (i.e. the concepts of coffee and cup are certainly related, but there is very little similarity about them), as opposed to ‘genuine’ semantic similarity (i.e. the relation holding between concepts such as coffee and tea) (Agirre et al., 2009; Hill et al., 2015; Gladkova and Drozd, 2016). Therefore, when testing a DSM, it is important to pay attention to what type of semantic relation is actually modeled by the evaluation dataset. Moreover, researchers pointed out that similarity estimation alone does not constitute a strong benchmark, as the inter-annotator agreement is relatively low in all datasets and the performance of several automated systems is already above the upper bound (Batchkarov et al., 2016). As a consequence, workshops such as RepEval have been organized with the explicit purpose of finding alternative evaluation tasks for DSMs. A recent proposal is the chall"
P18-2088,S13-1005,0,0.0282831,"nstant value ranging between 0 and 1 (excluded), as shown in equation 2, such that now the number of ranks contributing to the final score is widen to all features (see the smoother curve of AP SynP ower in Figure 1). We name this variant AP SynP ower or, shortly, AP SynP . i=|f | AP SynP (wx , wy ) = X i=0 Figure 1: Comparison of weight per feature rank in AP Syn and AP SynP (p = 0.1) across feature ranks ranging from 1 to 300. context size of 10 and negative sampling).4 For comparison with Camacho-Collados and Navigli (2016) on outlier detection, we used the same training corpora: the UMBC (Han et al., 2013) and the English Wikipedia.5 4.2 As for the similarity estimation task, we evaluate the Spearman correlation between systemgenerated scores and human judgments. We used three popular benchmark datasets: WordSim353 (Finkelstein et al., 2001), MEN (Bruni et al., 2014) and SimLex-999 (Hill et al., 2015). It is important to point out that SimLex-999 is the only one specifically built for targeting genuine semantic similarity, while the others tend to mix similarity and relatedness scores. As for outlier detection, we evaluate our DSMs on the 8-8-8 dataset (Camacho-Collados and Navigli, 2016). The"
P18-2088,J15-4004,0,0.349061,"en the average scores assigned to the pairs by the subjects and the cosines of the corresponding vectors (similary estimation task). Similarity as modeled by DSMs has been under debate, as its definition is underspecified. It in fact includes an ambiguity with the more generic notion of semantic relatedness, which is present also in many popular datasets (i.e. the concepts of coffee and cup are certainly related, but there is very little similarity about them), as opposed to ‘genuine’ semantic similarity (i.e. the relation holding between concepts such as coffee and tea) (Agirre et al., 2009; Hill et al., 2015; Gladkova and Drozd, 2016). Therefore, when testing a DSM, it is important to pay attention to what type of semantic relation is actually modeled by the evaluation dataset. Moreover, researchers pointed out that similarity estimation alone does not constitute a strong benchmark, as the inter-annotator agreement is relatively low in all datasets and the performance of several automated systems is already above the upper bound (Batchkarov et al., 2016). As a consequence, workshops such as RepEval have been organized with the explicit purpose of finding alternative evaluation tasks for DSMs. A r"
P18-4006,J93-2004,0,0.0614908,"igure 2. Here text span boundaries are selected and assigned with a label, which can be useful for Named Entity Recognition (NER) (Tjong Kim Sang and De Meulder, 2003), word segmentation (Sproat and Emerson, 2003), chunking (Tjong Kim Sang and Buchholz, 2000) ,etc. To keep annotation efficient and accurate, Y EDDA provides systematic solutions across the whole annotation process, which includes the shortcut annotation, batch annotation with a command line, intelligent recommendation, format exporting and Introduction Natural Language Processing (NLP) systems rely on large-scale training data (Marcus et al., 1993) for supervised training. However, manual annotation can be time-consuming and expensive. Despite detailed annotation standards and rules, interannotator disagreement is inevitable because of human mistakes, language phenomena which are not covered by the annotation rules and the ambiguity of language itself (Plank et al., 2014). Existing annotation tools (Cunningham et al., 2002; Morton and LaCivita, 2003; Chen and Styler, 2013; Druskat et al., 2014) mainly focus on providing a visual interface for user annotation 1 Code is available jiesutd/YEDDA. at https://github.com/ 31 Proceedings of the"
P18-4006,N06-4006,0,0.437707,"ements only for each sentence and shows the comparison within the interface, while our system can generate a detailed disagreement report in .pdf file through the whole annotated content. Besides, those webbased annotation tools need to build a server through complex configurations and some of the servers cannot be deployed on Windows systems. Related Work There exists a range of text span annotation tools which focus on different aspects of the annotation process. Stanford manual annotation tool2 is a lightweight tool but does not support result analysis and system recommendation. Knowtator (Ogren, 2006) is a general-task annotation tool which links to a biomedical onto ontology to 2 http://nlp.stanford.edu/software/ 3 G ATE is a general NLP tool which includes annotation stanford-manual-annotation-tool-2004-05-16. tar.gz function. 32 Figure 2: Annotator Interface. The differences between Y EDDA and related work are summarised in Table 14 . Here “Self Consistency” represents whether the tool works independently or it relies on pre-installed packages. Compared to these tools, Y EDDA provides a lighter but more systematic choice with more flexibility, efficiency and less dependence on system en"
P18-4006,P14-2083,0,0.012686,"atic solutions across the whole annotation process, which includes the shortcut annotation, batch annotation with a command line, intelligent recommendation, format exporting and Introduction Natural Language Processing (NLP) systems rely on large-scale training data (Marcus et al., 1993) for supervised training. However, manual annotation can be time-consuming and expensive. Despite detailed annotation standards and rules, interannotator disagreement is inevitable because of human mistakes, language phenomena which are not covered by the annotation rules and the ambiguity of language itself (Plank et al., 2014). Existing annotation tools (Cunningham et al., 2002; Morton and LaCivita, 2003; Chen and Styler, 2013; Druskat et al., 2014) mainly focus on providing a visual interface for user annotation 1 Code is available jiesutd/YEDDA. at https://github.com/ 31 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 31–36 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Tool WordFreak G ATE Knowtator Stanford Atomic WebAnno Anafora B RAT Y EDDA Operating System MacOS Linux Win √ √ √ √ √ √ √ √ √ √ √ √ √"
P18-4006,W09-1119,0,0.0247122,"during the whole annotation process, which learns the up-to-date and in-domain annotation information. The recommending system is designed as “pluggable” which ensures that the recommending algorithm can be easily extended into other sequence labeling mod3.1.5 Export Annotated Text As the annotated file is saved in .ann format, Y EDDA provides the “Export” function which exports the annotated text as standard format (ended with .anns). Each line includes one word/character and its label, sentences are separated by an empty line. The exported label can be chosen in either BIO or BIOES format (Ratinov and Roth, 2009). 3.2 Administrator Toolkits For the administrator, it is important and necessary to evaluate the quality of annotated files and analyze the detailed disagreements of different annotators. Shown in Figure 3, Y EDDA provides a 7 Those sequence labeling models work well on big training data. For limited training data, the maximum matching algorithm gives better performance. 34 Annotation Comparison Report 4000 SUTDNLP Group 3000 1 Time (s) Singapore University of Technology and Design Overall Statistics File1 color: Blue ; Dir: ~/demotext/EnglishUserA.txt.ann Stanford BRAT GATE WebAnno YEDDA YED"
P18-4006,W03-1719,0,0.0149978,"ing relatively less addressed in existing annotation tools (Ogren, 2006; Stenetorp et al., 2012). Besides, many tools (Ogren, 2006; Chen and Styler, 2013) require a complex system configuration on either local device or server, which is not friendly to new users. To address the challenges above, we propose Y EDDA1 , a lightweight and efficient annotation tool for text span annotation. A snapshot is shown in Figure 2. Here text span boundaries are selected and assigned with a label, which can be useful for Named Entity Recognition (NER) (Tjong Kim Sang and De Meulder, 2003), word segmentation (Sproat and Emerson, 2003), chunking (Tjong Kim Sang and Buchholz, 2000) ,etc. To keep annotation efficient and accurate, Y EDDA provides systematic solutions across the whole annotation process, which includes the shortcut annotation, batch annotation with a command line, intelligent recommendation, format exporting and Introduction Natural Language Processing (NLP) systems rely on large-scale training data (Marcus et al., 1993) for supervised training. However, manual annotation can be time-consuming and expensive. Despite detailed annotation standards and rules, interannotator disagreement is inevitable because of h"
P18-4006,N13-3004,0,0.733044,"through intelligent recommendation. 1 Multi-Annotation Analysis Results Annotator 1 Annotator 2 Annotator Interface Admin Toolkits Administrator Detailed Pairwise Annotation Report Annotator n Feedback Figure 1: Framework of Y EDDA. process but rarely consider the post-annotation quality analysis, which is necessary due to the inter-annotator disagreement. In addition to the annotation quality, efficiency is also critical in large-scale annotation task, while being relatively less addressed in existing annotation tools (Ogren, 2006; Stenetorp et al., 2012). Besides, many tools (Ogren, 2006; Chen and Styler, 2013) require a complex system configuration on either local device or server, which is not friendly to new users. To address the challenges above, we propose Y EDDA1 , a lightweight and efficient annotation tool for text span annotation. A snapshot is shown in Figure 2. Here text span boundaries are selected and assigned with a label, which can be useful for Named Entity Recognition (NER) (Tjong Kim Sang and De Meulder, 2003), word segmentation (Sproat and Emerson, 2003), chunking (Tjong Kim Sang and Buchholz, 2000) ,etc. To keep annotation efficient and accurate, Y EDDA provides systematic soluti"
P18-4006,P02-1022,0,0.164696,"ss, which includes the shortcut annotation, batch annotation with a command line, intelligent recommendation, format exporting and Introduction Natural Language Processing (NLP) systems rely on large-scale training data (Marcus et al., 1993) for supervised training. However, manual annotation can be time-consuming and expensive. Despite detailed annotation standards and rules, interannotator disagreement is inevitable because of human mistakes, language phenomena which are not covered by the annotation rules and the ambiguity of language itself (Plank et al., 2014). Existing annotation tools (Cunningham et al., 2002; Morton and LaCivita, 2003; Chen and Styler, 2013; Druskat et al., 2014) mainly focus on providing a visual interface for user annotation 1 Code is available jiesutd/YEDDA. at https://github.com/ 31 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 31–36 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Tool WordFreak G ATE Knowtator Stanford Atomic WebAnno Anafora B RAT Y EDDA Operating System MacOS Linux Win √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ √ × √ √ × √ √ √ Self Consistency √ √ Co"
P18-4006,W00-0726,0,0.525472,"Missing"
P18-4006,W16-4011,0,0.13132,"Missing"
P18-4006,W03-0419,0,0.0558752,"Missing"
P18-4006,P13-4001,0,0.114707,"Missing"
P18-4013,P17-1161,0,0.136812,"xible neural feature design and utilization. Built on PyTorch1 , the core operations are calculated in batch, making the toolkit efficient with the acceleration of GPU. It also includes the implementations of most state-of-the-art neural sequence labeling models such as LSTMCRF, facilitating reproducing and refinement on those methods. 1 Figure 1: Configuration file segment with distributed word representations. Similar to discrete models, a CRF layer is used in many state-of-the-art neural sequence labeling models for capturing label dependencies (Collobert et al., 2011; Lample et al., 2016; Peters et al., 2017). There exist several open-source statistical CRF sequence labeling toolkits, such as CRF++2 , CRFSuite (Okazaki, 2007) and FlexCRFs (Phan et al., 2004), which provide users with flexible means of feature extraction, various training settings and decoding formats, facilitating quick implementation and extension on state-of-the-art models. On the other hand, there is limited choice for neural sequence labeling toolkits. Although many authors released their code along with their sequence labeling papers (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018), the implementations are mostly fo"
P18-4013,D17-1035,0,0.016891,".20 91.35 90.94 91.21 91.20 90.87 chunking F1-value 94.23 94.76 94.77 94.49 95.00 95.06 – – 94.66 95.00 Features WLSTM+CRF +POS Human Feature +Cap +POS+Cap +CLSTM Auto Feature +CCNN POS Acc 96.99 97.38 97.33 97.20 97.49 97.46 97.51 97.55 97.55 – Baseline P 80.44 90.61 90.74 90.92 91.22 91.66 R 87.88 89.28 90.43 90.27 91.17 91.04 F 89.15 89.94 90.58 90.59 91.20 91.35 Table 2: Results using different features. and De Meulder, 2003) with the standard split is used. For the chunking task, we perform experiments on CoNLL 2000 shared task (Tjong Kim Sang and Buchholz, 2000), data split is following Reimers and Gurevych (2017). For POS tagging, we use the same data and split with Ma and Hovy (2016). We test different combinations of character representations and word sequence representations on these three benchmarks. Hyperparameters are mostly following Ma and Hovy (2016) and almost keep the same in all these experiments5 . Standard SGD with a decaying learning rate is used as the optimizer. Table 1: Results on three benchmarks. dev dir, test dir, raw dir, pretrained character or word embedding (char emb dim or word emb dim), and decode file directory (decode dir). • Training includes the loss function (loss funct"
P18-4013,N03-1028,0,0.288613,"regarded as the neural version of CRF++, with both take the CoNLL data format as input and can add handIntroduction Sequence labeling is one of the most fundamental NLP models, which is used for many tasks such as named entity recognition (NER), chunking, word segmentation and part-of-speech (POS) tagging. It has been traditionally investigated using statistical approaches (Lafferty et al., 2001; Ratinov and Roth, 2009), where conditional random fields (CRF) (Lafferty et al., 2001) has been proven as an effective framework, by taking discrete features as the representation of input sequence (Sha and Pereira, 2003; Keerthi and Sundararajan, 2007). With the advances of deep learning, neural sequence labeling models have achieved state-ofthe-art for many tasks (Ling et al., 2015; Ma and Hovy, 2016; Peters et al., 2017). Features are extracted automatically through network structures including long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and convolution neural network (CNN) (LeCun et al., 1989), 1 2 https://taku910.github.io/crfpp/ Code is available at https://github.com/ jiesutd/NCRFpp. 3 http://pytorch.org/ 74 Proceedings of the 56th Annual Meeting of the Association for Computationa"
P18-4013,D17-1283,0,0.0185508,"the training and the decoding speed can be significantly accelerated through a large batch size. The decoding speed reaches saturation at batch size 100, while the training speed keeps growing. The decoding speed and training speed of NCRF++ are over 2000 sentences/second and 1000 sentences/second, respectively, demonstrating the efficiency of our implementation. We also investigate the influence of different features on system performance. Table 2 shows the results on the NER task. POS tag and capital indicator are two common features on NER tasks (Collobert et al., 2011; Huang et al., 2015; Strubell et al., 2017). In our implementation, each POS tag or capital indicator feature is mapped as 10dimension feature embeddings through randomly initialized feature lookup table 6 . The feature embeddings are concatenated with the word embeddings as the representation of the corresponding word. Results show that both human features [POS] and [Cap] can contribute the NER system, this is consistent with previous observations (Collobert et al., 2011; Chiu and Nichols, 2016). By utilizing LSTM or CNN to encode character sequence automatically, the system can achieve better performance on NER task. 3.4 Speed with B"
P18-4013,W00-0726,0,0.746252,"Missing"
P18-4013,N16-1030,0,0.633599,"uration file with flexible neural feature design and utilization. Built on PyTorch1 , the core operations are calculated in batch, making the toolkit efficient with the acceleration of GPU. It also includes the implementations of most state-of-the-art neural sequence labeling models such as LSTMCRF, facilitating reproducing and refinement on those methods. 1 Figure 1: Configuration file segment with distributed word representations. Similar to discrete models, a CRF layer is used in many state-of-the-art neural sequence labeling models for capturing label dependencies (Collobert et al., 2011; Lample et al., 2016; Peters et al., 2017). There exist several open-source statistical CRF sequence labeling toolkits, such as CRF++2 , CRFSuite (Okazaki, 2007) and FlexCRFs (Phan et al., 2004), which provide users with flexible means of feature extraction, various training settings and decoding formats, facilitating quick implementation and extension on state-of-the-art models. On the other hand, there is limited choice for neural sequence labeling toolkits. Although many authors released their code along with their sequence labeling papers (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018), the impleme"
P18-4013,W03-0419,0,0.641113,"Missing"
P18-4013,D15-1176,0,0.1113,"models, which is used for many tasks such as named entity recognition (NER), chunking, word segmentation and part-of-speech (POS) tagging. It has been traditionally investigated using statistical approaches (Lafferty et al., 2001; Ratinov and Roth, 2009), where conditional random fields (CRF) (Lafferty et al., 2001) has been proven as an effective framework, by taking discrete features as the representation of input sequence (Sha and Pereira, 2003; Keerthi and Sundararajan, 2007). With the advances of deep learning, neural sequence labeling models have achieved state-ofthe-art for many tasks (Ling et al., 2015; Ma and Hovy, 2016; Peters et al., 2017). Features are extracted automatically through network structures including long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and convolution neural network (CNN) (LeCun et al., 1989), 1 2 https://taku910.github.io/crfpp/ Code is available at https://github.com/ jiesutd/NCRFpp. 3 http://pytorch.org/ 74 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 74–79 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics I l o B r v e u c e L e e"
P18-4013,C18-1327,1,0.771292,"Missing"
P19-1236,E17-2026,0,0.077391,"Missing"
P19-1236,Q16-1026,0,0.0225765,"in LM tasks and the first to work on NER transfer learning between domains with completely different entity types (i.e. news vs. biomedical). We released our data and code at https://github.com/ jiachenwestlake/Cross-Domain_NER. 2 Related Work NER. Recently, neural networks have been used for NER and achieved state-of-the-art results. Hammerton (2003) use a unidirectional LSTM with a Softmax classifer. Collobert et al. (2011) use a CNN-CRF architecture. Santos and Guimar˜aes (2015) extend the model by using character CNN. Most recent work uses LSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Yang et al., 2018). We choose BiLSTM-CRF as our method since it gives stateof-the-art resutls on standard benchmarks. Cross-domain NER. Most existing work on cross-domain NER investigates the supervised setting, where both source and target domains have labeled data. Daum´e III (2009) maps entity label space between the source and target domains. Kim et al. (2015) and Obeidat et al. (2016) use label embeddings instead of entities themselves as the features for cross-domain transfer. Wang et al. (2018) perform label-aware feature representation transfer based on text representation learned by"
P19-1236,D08-1071,0,0.0977508,"Missing"
P19-1236,W03-0426,0,0.125177,"tically. Results on three different cross-domain datasets show that our method outperforms naive multitask learning and a wide range of domain adaptation methods. To our knowledge, we are the first to consider unsupervised domain adaptation for NER via cross-domain LM tasks and the first to work on NER transfer learning between domains with completely different entity types (i.e. news vs. biomedical). We released our data and code at https://github.com/ jiachenwestlake/Cross-Domain_NER. 2 Related Work NER. Recently, neural networks have been used for NER and achieved state-of-the-art results. Hammerton (2003) use a unidirectional LSTM with a Softmax classifer. Collobert et al. (2011) use a CNN-CRF architecture. Santos and Guimar˜aes (2015) extend the model by using character CNN. Most recent work uses LSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Yang et al., 2018). We choose BiLSTM-CRF as our method since it gives stateof-the-art resutls on standard benchmarks. Cross-domain NER. Most existing work on cross-domain NER investigates the supervised setting, where both source and target domains have labeled data. Daum´e III (2009) maps entity label space between the source"
P19-1236,P15-1046,0,0.235924,"llowing unsupervised domain adaptation while also giving state-ofthe-art results among supervised domain adaptation methods. W ITlm Horizontal Transfer D I src D I tgt Figure 1: Overview of the proposed model. Named entity recognition (NER) is a fundamental task in information extraction and text understanding. Due to large variations in entity names and flexibility in entity mentions, NER has been a challenging task in NLP. Cross-domain NER adds to the difficulty of modeling due to the difference in text genre and entity names. Existing methods make use of feature transfer (Daum´e III, 2009; Kim et al., 2015; Obeidat et al., 2016; Wang et al., 2018) and parameters sharing (Lee et al., 2017; Sachan et al., 2018; Yang et al., 2017; Lin and Lu, 2018) for supervised NER domain adaptation. Language modeling (LM) has been shown useful for NER, both via multi-task learning (Rei, 2017) and via pre-training (Peters et al., 2018). Intuitively, both noun entities and context patterns can be captured during LM training, which benefits the recognition of named entities. A natural question that arises is whether cross-domain Work done when visiting Westlake University.  tgt ,lm  src ,lm Vertical Transfer LM"
P19-1236,N16-1030,0,0.0561717,"domain adaptation for NER via cross-domain LM tasks and the first to work on NER transfer learning between domains with completely different entity types (i.e. news vs. biomedical). We released our data and code at https://github.com/ jiachenwestlake/Cross-Domain_NER. 2 Related Work NER. Recently, neural networks have been used for NER and achieved state-of-the-art results. Hammerton (2003) use a unidirectional LSTM with a Softmax classifer. Collobert et al. (2011) use a CNN-CRF architecture. Santos and Guimar˜aes (2015) extend the model by using character CNN. Most recent work uses LSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Yang et al., 2018). We choose BiLSTM-CRF as our method since it gives stateof-the-art resutls on standard benchmarks. Cross-domain NER. Most existing work on cross-domain NER investigates the supervised setting, where both source and target domains have labeled data. Daum´e III (2009) maps entity label space between the source and target domains. Kim et al. (2015) and Obeidat et al. (2016) use label embeddings instead of entities themselves as the features for cross-domain transfer. Wang et al. (2018) perform label-aware feature representation trans"
P19-1236,D18-1226,0,0.206565,"l Transfer D I src D I tgt Figure 1: Overview of the proposed model. Named entity recognition (NER) is a fundamental task in information extraction and text understanding. Due to large variations in entity names and flexibility in entity mentions, NER has been a challenging task in NLP. Cross-domain NER adds to the difficulty of modeling due to the difference in text genre and entity names. Existing methods make use of feature transfer (Daum´e III, 2009; Kim et al., 2015; Obeidat et al., 2016; Wang et al., 2018) and parameters sharing (Lee et al., 2017; Sachan et al., 2018; Yang et al., 2017; Lin and Lu, 2018) for supervised NER domain adaptation. Language modeling (LM) has been shown useful for NER, both via multi-task learning (Rei, 2017) and via pre-training (Peters et al., 2018). Intuitively, both noun entities and context patterns can be captured during LM training, which benefits the recognition of named entities. A natural question that arises is whether cross-domain Work done when visiting Westlake University.  tgt ,lm  src ,lm Vertical Transfer LM Task Introduction ∗ I Tner  tgt ,ner  src ,ner NER Task Vertical Transfer 1 Target Domain LM training can benefit cross-domain NER. Figure 1"
P19-1236,N18-1050,1,0.83816,"raw data for both domains. In addition, our method can deal with a zero-shot learning setting for unsupervised NER domain adaptation, which no existing work considers. Learning task embedding vectors. There has been related work using task vector representations for multi-task learning. Ammar et al. (2016) learn language embeddings for multi-lingual parsing. Stymne et al. (2018) learn treebank embeddings for cross-annotation-style parsing. These methods use “task” embeddings to augment word embedding inputs, distilling “task” characteristics into these vectors for preserving word embeddings. Liu et al. (2018) learn domain embeddings for multi-domain sentiment classification. They combine domain vectors with domainindependent representation of the input sentences to obtain a domain-specific input representation. A salient difference between our work and the methods above is that we use domain and task embeddings to obtain domain and task-specific parameters, rather than input representations. Closer in spirit to our work, Platanios et al. (2018) learn language vectors, using them to generate parameters for multi-lingual machine translation. While one of their main motivation is to save the paramete"
P19-1236,P16-1101,0,0.340007,"NER via cross-domain LM tasks and the first to work on NER transfer learning between domains with completely different entity types (i.e. news vs. biomedical). We released our data and code at https://github.com/ jiachenwestlake/Cross-Domain_NER. 2 Related Work NER. Recently, neural networks have been used for NER and achieved state-of-the-art results. Hammerton (2003) use a unidirectional LSTM with a Softmax classifer. Collobert et al. (2011) use a CNN-CRF architecture. Santos and Guimar˜aes (2015) extend the model by using character CNN. Most recent work uses LSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Yang et al., 2018). We choose BiLSTM-CRF as our method since it gives stateof-the-art resutls on standard benchmarks. Cross-domain NER. Most existing work on cross-domain NER investigates the supervised setting, where both source and target domains have labeled data. Daum´e III (2009) maps entity label space between the source and target domains. Kim et al. (2015) and Obeidat et al. (2016) use label embeddings instead of entities themselves as the features for cross-domain transfer. Wang et al. (2018) perform label-aware feature representation transfer based on text r"
P19-1236,D16-1046,0,0.0746975,"Missing"
P19-1236,D14-1162,0,0.0794284,"1,617 1,660 L OC 7,140 1,837 1,668 629 O RG 6,321 1,341 1,661 1,352 M ISC 3,438 922 702 497 Table 2: Entity numbers of the CoNLL dataset and the CBS SciTech News dataset. Mul t i Tas k Tar get Figure 3: Development results on 13CG. gle Task Model (S TM -TARGET) for the strongest baseline according to development experiments, while the multi-task models use SGD with a learning rate of 0.015 as (Yang et al., 2018). We use domain embeddings and task embeddings of size 8 to fit the model in one GPU of 8GB memory. The word embeddings for all models are initialized with GloVe 100-dimension vectors (Pennington et al., 2014) and fine-tuned during training. Character embeddings are randomly initialized. 4.2 Development Experiments We report a set of development experiments on the biomedical datasets 13PC and 13CG. Learning curves. Figure 3 shows the F1-scores against the number of training iterations on the 13CG development set. S TM -TARGET is our single task model trained on the target-domain training set Tner ; F INE T UNE is a model pre-trained 2468 Methods Crichton et al. (2017) S TM -TARGET M ULTI TASK ( NER + LM ) M ULTI TASK ( NER ) F INE T UNE S TM +E LMO C O -L M C O -N ER M IX -DATA F INAL Figure 4: Joi"
P19-1236,N18-1202,0,0.573265,"ue to large variations in entity names and flexibility in entity mentions, NER has been a challenging task in NLP. Cross-domain NER adds to the difficulty of modeling due to the difference in text genre and entity names. Existing methods make use of feature transfer (Daum´e III, 2009; Kim et al., 2015; Obeidat et al., 2016; Wang et al., 2018) and parameters sharing (Lee et al., 2017; Sachan et al., 2018; Yang et al., 2017; Lin and Lu, 2018) for supervised NER domain adaptation. Language modeling (LM) has been shown useful for NER, both via multi-task learning (Rei, 2017) and via pre-training (Peters et al., 2018). Intuitively, both noun entities and context patterns can be captured during LM training, which benefits the recognition of named entities. A natural question that arises is whether cross-domain Work done when visiting Westlake University.  tgt ,lm  src ,lm Vertical Transfer LM Task Introduction ∗ I Tner  tgt ,ner  src ,ner NER Task Vertical Transfer 1 Target Domain LM training can benefit cross-domain NER. Figure 1 shows one example, where there are relatively large training data in the news domain but no data or a small amount of data in a target domain. We are interested in transferrin"
P19-1236,D18-1039,0,0.0494446,"rsing. These methods use “task” embeddings to augment word embedding inputs, distilling “task” characteristics into these vectors for preserving word embeddings. Liu et al. (2018) learn domain embeddings for multi-domain sentiment classification. They combine domain vectors with domainindependent representation of the input sentences to obtain a domain-specific input representation. A salient difference between our work and the methods above is that we use domain and task embeddings to obtain domain and task-specific parameters, rather than input representations. Closer in spirit to our work, Platanios et al. (2018) learn language vectors, using them to generate parameters for multi-lingual machine translation. While one of their main motivation is to save the parameter space when the number of langauges grows, our main goal is to investigate the modularization of transferable knowledge in a cross-domain and cross-task setting. To our knowledge, we are the first to study “task” embeddings in a multi-dimensional parameter decomposition setting (e.g. domain + task). 3 Methods The overall structure of our proposed model is shown in Figure 2. The bottom shows the com2465 of BiLSTM using a Parameter Generatio"
P19-1236,P17-1194,0,0.0275814,"raction and text understanding. Due to large variations in entity names and flexibility in entity mentions, NER has been a challenging task in NLP. Cross-domain NER adds to the difficulty of modeling due to the difference in text genre and entity names. Existing methods make use of feature transfer (Daum´e III, 2009; Kim et al., 2015; Obeidat et al., 2016; Wang et al., 2018) and parameters sharing (Lee et al., 2017; Sachan et al., 2018; Yang et al., 2017; Lin and Lu, 2018) for supervised NER domain adaptation. Language modeling (LM) has been shown useful for NER, both via multi-task learning (Rei, 2017) and via pre-training (Peters et al., 2018). Intuitively, both noun entities and context patterns can be captured during LM training, which benefits the recognition of named entities. A natural question that arises is whether cross-domain Work done when visiting Westlake University.  tgt ,lm  src ,lm Vertical Transfer LM Task Introduction ∗ I Tner  tgt ,ner  src ,ner NER Task Vertical Transfer 1 Target Domain LM training can benefit cross-domain NER. Figure 1 shows one example, where there are relatively large training data in the news domain but no data or a small amount of data in a targ"
P19-1236,W03-0419,0,0.192619,"Missing"
P19-1236,W15-3904,0,0.0768277,"Missing"
P19-1236,P18-2098,0,0.0283116,"Missing"
P19-1236,N18-1001,0,0.286077,"hile also giving state-ofthe-art results among supervised domain adaptation methods. W ITlm Horizontal Transfer D I src D I tgt Figure 1: Overview of the proposed model. Named entity recognition (NER) is a fundamental task in information extraction and text understanding. Due to large variations in entity names and flexibility in entity mentions, NER has been a challenging task in NLP. Cross-domain NER adds to the difficulty of modeling due to the difference in text genre and entity names. Existing methods make use of feature transfer (Daum´e III, 2009; Kim et al., 2015; Obeidat et al., 2016; Wang et al., 2018) and parameters sharing (Lee et al., 2017; Sachan et al., 2018; Yang et al., 2017; Lin and Lu, 2018) for supervised NER domain adaptation. Language modeling (LM) has been shown useful for NER, both via multi-task learning (Rei, 2017) and via pre-training (Peters et al., 2018). Intuitively, both noun entities and context patterns can be captured during LM training, which benefits the recognition of named entities. A natural question that arises is whether cross-domain Work done when visiting Westlake University.  tgt ,lm  src ,lm Vertical Transfer LM Task Introduction ∗ I Tner  tgt ,ner  sr"
P19-1236,C18-1327,1,0.902787,"t to work on NER transfer learning between domains with completely different entity types (i.e. news vs. biomedical). We released our data and code at https://github.com/ jiachenwestlake/Cross-Domain_NER. 2 Related Work NER. Recently, neural networks have been used for NER and achieved state-of-the-art results. Hammerton (2003) use a unidirectional LSTM with a Softmax classifer. Collobert et al. (2011) use a CNN-CRF architecture. Santos and Guimar˜aes (2015) extend the model by using character CNN. Most recent work uses LSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016; Chiu and Nichols, 2016; Yang et al., 2018). We choose BiLSTM-CRF as our method since it gives stateof-the-art resutls on standard benchmarks. Cross-domain NER. Most existing work on cross-domain NER investigates the supervised setting, where both source and target domains have labeled data. Daum´e III (2009) maps entity label space between the source and target domains. Kim et al. (2015) and Obeidat et al. (2016) use label embeddings instead of entities themselves as the features for cross-domain transfer. Wang et al. (2018) perform label-aware feature representation transfer based on text representation learned by BiLSTM networks. Re"
P19-1236,P18-4013,1,0.857436,"ifference is that a great number of entities in the CBS News dataset are closely related to the domain of science and technology. In particular, for the M ISC category, more technology terms such as Space X, bitcoin and IP are included, as compared with the CoNLL data set. Lack of such entities in the CoNLL training set and the difference of text genre cause the main difficulty in domain transfer. To address this difference, 398,990 unlabeled sentences from CBS SciTech News are used for LM training. We released this dataset as one contribution of this paper. Hyperparameters. We choose NCRF++ (Yang and Zhang, 2018) for developing the models. Our hyperparameter settings largly follow (Yang et al., 2018), with the following exceptions: (1) The batch size is set to 30 instead of 10 for shorter training time in multi-task learning; (2) RMSprop with a learning rate of 0.001 is used for our Sin1 https://github.com/cambridgeltl/MTL-Bioinformatics2016 2 We tried to use a larger number of raw data from the PubMed, but this did not improve the performances. 3 https://www.cbsnews.com/ CoNLL BioNLP13PC BioNLP13CG CBS News Type Sentence Entity Sentence Entity Sentence Entity Sentence Entity Train 15.0K 23.5K 2.5K 7."
P19-1276,W17-2710,0,0.0609551,"th various slot combinations. Event Schema Induction seminal work studies patterns (Shinyama and Sekine, 2006; Filatova et al., 2006; Qiu et al., 2008) and event chains (Chambers and Jurafsky, 2011) for template induction. For MUC 4, the current dominant methods include probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 201"
P19-1276,P11-1040,0,0.0371388,"lso been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsupervised methods have been used. The former (Sakaki et al., 2010; Benson et al., 2011) are typically designed to monitor certain event types, while the latter cluster features according to their burstiness (Becker et al., 2011; Cui et al., 2012; Li et al., 2012; Ritter et al., 2012; Qin et al., 2013; Ifrim et al., 2014; McMinn and Jose, 2015; Qin et al., 2017). This line of work is similar to our work in using information redundancy, but different because we focus on formal news texts and induce structural event schemas. First Story Detection (FSD) systems aim to identify news articles that discuss events not reported before. Most work on FSD detects first stories by finding th"
P19-1276,D13-1185,0,0.788263,"not been considered in traditional methods. First, more than one event can be extracted from a news cluster, where events can be flexible in having varying numbers of slots in the open domain, and slots can be flexible without identical distributions regardless of the event type, which has been assumed by previous work on schema induction. Second, mentions of the same entities from different reports in a news cluster should be taken into account for improved performance. We build an unsupervised generative model to address these challenges. While previous work on generative schema induction (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) relies on hand-crafted indicator features, we introduce latent variables produced by neural networks for better representation power. A novel graph model 2860 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2860–2871 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics is designed, with a latent event type vector for each news cluster from a global parameterized normal distribution, and textual redundancy features for entities. Our model takes advantage of contextua"
P19-1276,P11-1098,0,0.523624,"r slots - Perpetrator, Instrument, Target and Victim - are defined. We compare the task settings of MUC 4 and ODEE in Figure 1. For MUC 4, the inputs are single news documents, and the output belongs to four types of events with schemas consisting of fixed slots. For ODEE, in contrast, the inputs are news clusters rather than the individual news, and the output is unconstrained types of open domain events and unique schemas with various slot combinations. Event Schema Induction seminal work studies patterns (Shinyama and Sekine, 2006; Filatova et al., 2006; Qiu et al., 2008) and event chains (Chambers and Jurafsky, 2011) for template induction. For MUC 4, the current dominant methods include probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using n"
P19-1276,P15-1017,0,0.173081,"not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsupervised methods have been used. The former (Sakaki et al., 2010; Benson et al., 2011) are typically designed to monitor certain event types, while the latter cluster features according to their"
P19-1276,D18-1158,0,0.0573605,"ooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsupervised methods have been used. The former (Sakaki et al., 2010; Benson et al., 2011) are typically designed to monitor certain event types, while the latter cluster fe"
P19-1276,N13-1104,0,0.475598,"ered in traditional methods. First, more than one event can be extracted from a news cluster, where events can be flexible in having varying numbers of slots in the open domain, and slots can be flexible without identical distributions regardless of the event type, which has been assumed by previous work on schema induction. Second, mentions of the same entities from different reports in a news cluster should be taken into account for improved performance. We build an unsupervised generative model to address these challenges. While previous work on generative schema induction (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) relies on hand-crafted indicator features, we introduce latent variables produced by neural networks for better representation power. A novel graph model 2860 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2860–2871 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics is designed, with a latent event type vector for each news cluster from a global parameterized normal distribution, and textual redundancy features for entities. Our model takes advantage of contextualized pre-trained lan"
P19-1276,P98-1013,0,0.235638,"ferent sources. In each news cluster, there are no more than five news reports. For each news report, we obtain the title, publish timestamp, download timestamp, source URL and full text. In total, we obtain 55,618 business news reports with 13,047 news clusters in 288 batches from Oct. 17, 2018, to Jan. 22, 2019. The crawler is executed about three times per day. The full text corpus is released as GNBusinessFull-Text. For this paper, we trim the news reports in each news cluster by keeping the title and first paragraph, releasing as GNBusiness-All. Inspired by the general slots in FrameNet (Baker et al., 1998), we design reference event schemas for open domain event types, which include eight possible slots: Agent, Patient, Time, Place, Aim, Old Value, New Value and Variation. Agent and Patient are the semantic agent and patient of the trigger, respectively; Aim is the target or reason for the event. If the event involves value changes, Old Value serves the old value, New Value serves the new value and Variation is the variation between New Value and Old Value. Note that the roles that we define are more thematic and less specific to detailed events as some of the existing event extraction datasets"
P19-1276,P03-1054,0,0.00900482,"or qω and the prior pα . Due 2864 pβ 0 ,θ,λ (s|e, t) ∝ pβ 0 ,θ,λ (s, h, f 0 , t) = pθ (s|t) × pλ (h|s) × pβ 0 (f 0 |s) (5) Name Slots number S Feature Dimension n Fully connected layer size MLP layer number Activation function Learning rate Momentum Dropout rate Batch size Value 30 256 100 1 softplus 0.002 0.99 0.2 200 Table 3: Hyper-parameters setting. 4.5 Assembling Events for Output To assemble the events in a news cluster c for final output, we need to find the predicate for each entity, which now has a slot value. We use POStags and parse trees produced by the Stanford dependency parser (Klein and Manning, 2003) to extract the predicate for the head word of each entity mention. The following rules are applied: (1) if the governor of a head word is VB, or (2) if the governor of a head word is NN and belongs to the noun.ACT or noun.EVENT category of WordNet, then it is regarded as a predicate. We merge the predicates of entity mentions in the same coreference chain as a predicate set. For each predicate v in these sets, we find the entities whose predicate set contains v, treating the entities as arguments of the event triggered by v. Finally, by ranking the numbers of arguments, we obtain top-N open-d"
P19-1276,H05-1016,0,0.0761238,"tain event types, while the latter cluster features according to their burstiness (Becker et al., 2011; Cui et al., 2012; Li et al., 2012; Ritter et al., 2012; Qin et al., 2013; Ifrim et al., 2014; McMinn and Jose, 2015; Qin et al., 2017). This line of work is similar to our work in using information redundancy, but different because we focus on formal news texts and induce structural event schemas. First Story Detection (FSD) systems aim to identify news articles that discuss events not reported before. Most work on FSD detects first stories by finding the nearest neighbors of new documents (Kumaran and Allan, 2005; Moran et al., 2016; Panagiotou et al., 2016; Vuurens and de Vries, 2016). This line of work exploits textual redundancy in massive streams predicting whether or not a document contains a new event as a clas2861 Split #C #R #S #W Test 574 2,433 5,830 96,745 Dev 106 414 991 16,839 Unlabelled 12,305 52,464 127,416 2,101,558 All 12,985 55,311 134,237 2,215,142 Full-Text 12,985 55,311 1,450,336 31,103,698 sification task. In contrast, we study the event schemas and extract detailed events. 3 Task and Data Task Definition. In ODEE, the input consists of news clusters, each containing reports about"
P19-1276,P16-2011,0,0.132178,"News Cluster Document News News Report News Report Report Introduction Extracting events from news text has received much research attention. The task typically consists of two subtasks, namely schema induction, which is to extract event templates that specify argument slots for given event types (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018), and event extraction, which is to identify events with filled slots from a piece of news (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018, 2015; Feng et al., 2016; Nguyen and Grishman, 2016; Liu et al., 2018b). Previous work focuses on extracting events from single news documents according to a set of pre-specified event types, such as arson, attack or earthquakes. While useful for tracking highly specific types of events from news, the above setting can be relatively less useful for decision making in security and financial markets, which can require comprehensive knowledge on broad-coverage, finegrained and dynamically-evolving event categories. In addition, given the fact that different news agencies can report the same events, redundancy can be lev"
P19-1276,P06-2027,0,0.0604128,"ent types - Arson, Attack, Bombing and Kidnapping - and four slots - Perpetrator, Instrument, Target and Victim - are defined. We compare the task settings of MUC 4 and ODEE in Figure 1. For MUC 4, the inputs are single news documents, and the output belongs to four types of events with schemas consisting of fixed slots. For ODEE, in contrast, the inputs are news clusters rather than the individual news, and the output is unconstrained types of open domain events and unique schemas with various slot combinations. Event Schema Induction seminal work studies patterns (Shinyama and Sekine, 2006; Filatova et al., 2006; Qiu et al., 2008) and event chains (Chambers and Jurafsky, 2011) for template induction. For MUC 4, the current dominant methods include probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy."
P19-1276,P11-1113,0,0.0308881,"re are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsupervised methods have b"
P19-1276,P16-1025,0,0.0905794,"nd unique schemas with various slot combinations. Event Schema Induction seminal work studies patterns (Shinyama and Sekine, 2006; Filatova et al., 2006; Qiu et al., 2008) and event chains (Chambers and Jurafsky, 2011) for template induction. For MUC 4, the current dominant methods include probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li"
P19-1276,E12-1029,0,0.0252816,"E. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsupervised methods have been used. The former (Sakaki et al., 2010; Benson et al., 2011) are typically designed to monitor certain event types, while the latter cluster features according to their burstiness (Becker et al., 2011; Cui et al., 2012; Li et al., 2012; Ritter et al., 2012; Qin et al., 2013; Ifrim et al., 2014; McMinn and Jose, 2015; Qin et al., 2017). This line of work is similar to our work in using informatio"
P19-1276,E14-1056,0,0.219679,"guistics is designed, with a latent event type vector for each news cluster from a global parameterized normal distribution, and textual redundancy features for entities. Our model takes advantage of contextualized pre-trained language model (ELMo, Peters et al. (2018)) and scalable neural variational inference (Srivastava and Sutton, 2017). To evaluate model performance, we collect and annotate a large-scale dataset from Google Business News1 with diverse event types and explainable event schemas. In addition to the standard metrics for schema matching, we adapt slot coherence based on NPMI (Lau et al., 2014) for quantitatively measuring the intrinsic qualities of slots and schemas, which are inherently clusters. Results show that our neural latent variable model outperforms state-of-the-art event schema induction methods. In addition, redundancy is highly useful for improving open domain event extraction. Visualizations of learned parameters show that our model can give reasonable latent event types. To our knowledge, we are the first to use neural latent variable model for inducing event schemas and extracting events. We release our code and dataset at https://github.com/ lx865712528/ACL2019-ODE"
P19-1276,P13-1008,0,0.303462,"016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting."
P19-1276,C10-1077,0,0.0369674,"s without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both"
P19-1276,P10-1081,0,0.126776,"s without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both"
P19-1276,P16-1201,0,0.0176097,"uan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery i"
P19-1276,P08-1030,0,0.432239,"textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsuperv"
P19-1276,D18-1156,1,0.90501,"Missing"
P19-1276,P11-1163,0,0.1642,"Missing"
P19-1276,W14-1606,0,0.0125304,". For MUC 4, the current dominant methods include probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b"
P19-1276,N10-1012,0,0.117149,"Missing"
P19-1276,P15-1019,0,0.186122,"Missing"
P19-1276,L16-1307,0,0.0436377,"Missing"
P19-1276,N16-1034,0,0.6201,"Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsupervised methods have been used. The former (Sakaki et al., 2010; Benson et al., 2011) are typically designed t"
P19-1276,D16-1085,0,0.0977141,"nt News News Report News Report Report Introduction Extracting events from news text has received much research attention. The task typically consists of two subtasks, namely schema induction, which is to extract event templates that specify argument slots for given event types (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015; Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018), and event extraction, which is to identify events with filled slots from a piece of news (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018, 2015; Feng et al., 2016; Nguyen and Grishman, 2016; Liu et al., 2018b). Previous work focuses on extracting events from single news documents according to a set of pre-specified event types, such as arson, attack or earthquakes. While useful for tracking highly specific types of events from news, the above setting can be relatively less useful for decision making in security and financial markets, which can require comprehensive knowledge on broad-coverage, finegrained and dynamically-evolving event categories. In addition, given the fact that different news agencies can report the same events, redundancy can be leveraged for better event ext"
P19-1276,N18-1202,0,0.0745603,"15) relies on hand-crafted indicator features, we introduce latent variables produced by neural networks for better representation power. A novel graph model 2860 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2860–2871 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics is designed, with a latent event type vector for each news cluster from a global parameterized normal distribution, and textual redundancy features for entities. Our model takes advantage of contextualized pre-trained language model (ELMo, Peters et al. (2018)) and scalable neural variational inference (Srivastava and Sutton, 2017). To evaluate model performance, we collect and annotate a large-scale dataset from Google Business News1 with diverse event types and explainable event schemas. In addition to the standard metrics for schema matching, we adapt slot coherence based on NPMI (Lau et al., 2014) for quantitatively measuring the intrinsic qualities of slots and schemas, which are inherently clusters. Results show that our neural latent variable model outperforms state-of-the-art event schema induction methods. In addition, redundancy is highly"
P19-1276,P16-1027,0,0.0203117,"lude probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al.,"
P19-1276,I13-1035,1,0.837493,"sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsupervised methods have been used. The former (Sakaki et al., 2010; Benson et al., 2011) are typically designed to monitor certain event types, while the latter cluster features according to their burstiness (Becker et al., 2011; Cui et al., 2012; Li et al., 2012; Ritter et al., 2012; Qin et al., 2013; Ifrim et al., 2014; McMinn and Jose, 2015; Qin et al., 2017). This line of work is similar to our work in using information redundancy, but different because we focus on formal news texts and induce structural event schemas. First Story Detection (FSD) systems aim to identify news articles that discuss events not reported before. Most work on FSD detects first stories by finding the nearest neighbors of new documents (Kumaran and Allan, 2005; Moran et al., 2016; Panagiotou et al., 2016; Vuurens and de Vries, 2016). This line of work exploits textual redundancy in massive streams predicting w"
P19-1276,I08-1021,0,0.0431016,"ck, Bombing and Kidnapping - and four slots - Perpetrator, Instrument, Target and Victim - are defined. We compare the task settings of MUC 4 and ODEE in Figure 1. For MUC 4, the inputs are single news documents, and the output belongs to four types of events with schemas consisting of fixed slots. For ODEE, in contrast, the inputs are news clusters rather than the individual news, and the output is unconstrained types of open domain events and unique schemas with various slot combinations. Event Schema Induction seminal work studies patterns (Shinyama and Sekine, 2006; Filatova et al., 2006; Qiu et al., 2008) and event chains (Chambers and Jurafsky, 2011) for template induction. For MUC 4, the current dominant methods include probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also work"
P19-1276,N12-1008,0,0.0313509,"n modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts news-worthy clusters of words, segments and frames. Both supervised and unsupervised methods have been used. The former (Sakaki e"
P19-1276,D15-1195,0,0.0473595,"Missing"
P19-1276,N16-1049,0,0.272957,"en domain events and unique schemas with various slot combinations. Event Schema Induction seminal work studies patterns (Shinyama and Sekine, 2006; Filatova et al., 2006; Qiu et al., 2008) and event chains (Chambers and Jurafsky, 2011) for template induction. For MUC 4, the current dominant methods include probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClos"
P19-1276,N06-1039,0,0.0720711,"is MUC 4, in which four event types - Arson, Attack, Bombing and Kidnapping - and four slots - Perpetrator, Instrument, Target and Victim - are defined. We compare the task settings of MUC 4 and ODEE in Figure 1. For MUC 4, the inputs are single news documents, and the output belongs to four types of events with schemas consisting of fixed slots. For ODEE, in contrast, the inputs are news clusters rather than the individual news, and the output is unconstrained types of open domain events and unique schemas with various slot combinations. Event Schema Induction seminal work studies patterns (Shinyama and Sekine, 2006; Filatova et al., 2006; Qiu et al., 2008) and event chains (Chambers and Jurafsky, 2011) for template induction. For MUC 4, the current dominant methods include probabilistic generative methods (Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015) that jointly model predicate and ar1 https://news.google.com/?hl=en-US&gl= US&ceid=US:en, crawled from Oct. 2018 to Jan. 2019. gument assignment, and ad-hoc clustering algorithms for inducing slots (Sha et al., 2016; Huang et al., 2016; Ahn, 2017; Yuan et al., 2018). These methods all rely on hand-crafted discrete features without fully model t"
P19-1276,M92-1001,0,0.505227,"hich can require comprehensive knowledge on broad-coverage, finegrained and dynamically-evolving event categories. In addition, given the fact that different news agencies can report the same events, redundancy can be leveraged for better event extraction. In this paper, we investigate open domain ∗ Corresponding author. Figure 1: Comparison between MUC 4 and ODEE. event extraction (ODEE), which is to extract unconstraint types of events and induce universal event schemas from clusters of news reports. As shown in Figure 1, compared with traditional event extraction task exemplified by MUC 4 (Sundheim, 1992), the task of ODEE poses additional challenges to modeling, which have not been considered in traditional methods. First, more than one event can be extracted from a news cluster, where events can be flexible in having varying numbers of slots in the open domain, and slots can be flexible without identical distributions regardless of the event type, which has been assumed by previous work on schema induction. Second, mentions of the same entities from different reports in a news cluster should be taken into account for improved performance. We build an unsupervised generative model to address"
P19-1276,N16-1033,0,0.112869,"These methods all rely on hand-crafted discrete features without fully model the textual redundancy. There are also works on modeling event schemas and scripts using neural language models (Modi and Titov, 2014; Rudinger et al., 2015; Pichotta and Mooney, 2016), but they do not explore neural latent variables and redundancy. Event Extraction work typically assumes that event schemas are given, recognizing event triggers and their corresponding arguments. This can be regarded as a subtask of ODEE. Existing work exploits sentence-level (McClosky et al., 2011; Li et al., 2013; Liu et al., 2016; Yang and Mitchell, 2016) and document-level statistics (Liao and Grishman, 2010b; Ji and Grishman, 2008; Hong et al., 2011; Reichart and Barzilay, 2012). There has also been work using RNNs (Nguyen et al., 2016b; Sha et al., 2018; Liu et al., 2018a; Chen et al., 2018), CNNs (Chen et al., 2015; Feng et al., 2016; Nguyen and Grishman, 2016) and GCNs (Liu et al., 2018b) to represent sentences of events. Event extraction has been treated as a supervised or semi-supervised (Liao and Grishman, 2010a; Huang and Riloff, 2012) task. In contrast, ODEE is a fully unsupervised setting. Event Discovery in Tweet Streams extracts n"
P19-1295,D15-1112,0,0.0438657,"Missing"
P19-1295,P18-1249,0,0.0331647,"nvenient for quantifying their contributions. Experiments on various neural machine translation tasks demonstrate the effectiveness of the proposed method. The extensive analyses verify that the two types of contexts are complementary to each other, and our method gives highly effective improvements in their integration. 1 Introduction Self-attention networks (SANs) (Parikh et al., 2016; Lin et al., 2017) have shown promising results for a range of NLP tasks, including machine translation (Vaswani et al., 2017), contextualized word embedding learning (Devlin et al., 2019), dependency parsing (Kitaev and Klein, 2018) and semantic role labeling (Tan et al., 2018). They learn hidden representations of a sequence by letting each word attend to all words in the sentence regardless of their distances. Such a fully connected structure endows SANs with the appealing strength of collecting the global information (Yu et al., 2018; Shen et al., 2018; Chen et al., 2018; Zhang et al., 2017a; Yang et al., 2019a). However, some recent researches observe that a fully connected SANs may overlook the important ∗ Corresponding author neighboring information (Luong et al., 2015; Sperber et al., 2018; Yang et al., 2019a). Th"
P19-1295,W04-3250,0,0.326169,"R (Sperber et al., 2018) and L OCAL H (Luong et al., 2015) apply Gaussian biases to regularize the conventional attention distribution with a learnable window size and a predicable central position, respectively. L O CAL S (Yang et al., 2018) is the combination of these two approaches. “Param.” denotes the model size. Model T RANSFORMER + L OCAL PATTERN + H YBRID (Gate) En-De 27.67 28.13 28.31⇑ Ja-En 28.10 28.23 28.66⇑ Table 2: Experimental results on WMT17 En⇒De and WAT17 Ja⇒En test sets. “⇑”: significant over the vanilla self-attention counterpart (p < 0.05), tested by bootstrap resampling (Koehn, 2004). Comparison to Existing Approaches We reimplement and compare several existing methods (Sperber et al., 2018; Luong et al., 2015; Yang et al., 2018, 2019b) upon T RANSFORMER. Table 1 reports the results on the En-De test set. Clearly, all the models improve translation quality, reconfirming the necessity of modeling locality for SANs. By leveraging the local and global properties, our models outperform all the related works with fewer additional parameters. Performance across Languages We further conduct experiments on WAT17 Ja-En task, which is a distant language pair (Isozaki et al., 2010)."
P19-1295,N19-1359,1,0.821084,"ating the linguistic properties. We category 10 probing tasks into three groups (“Surf.”: surface, “Sync.”: syntax and “Semc.”: semantics) following the setting in Conneau et al. (2018). For simplistic, we merely reported the average score on each group. and 2) how different representations learn the locality and globality. 5.1 Linguistic Properties Although the proposed model improves the translation performance dramatically, we still lack of understanding on which linguistic perspectives are exactly improved by the two sources of information. To this end, we follow Conneau et al. (2018) and Li et al. (2019) to conduct 10 classification tasks to study what linguistic properties are enhanced by our model. Experiment Setting These tasks are divided into three categories (Conneau et al., 2018): tasks in “Surf.” focus on the surface properties learned in the sentence embedding; “Sync.” are the tasks which designed to evaluate the capabilities of the encoder on capturing the syntactic information; and “Semc.” tasks assess the ability of a model to understanding the denotation of a sentence. For the model setting, we replace the decoder of our translation model to a MLP classifier and keep the encoder"
P19-1295,D15-1166,0,0.850603,"Devlin et al., 2019), dependency parsing (Kitaev and Klein, 2018) and semantic role labeling (Tan et al., 2018). They learn hidden representations of a sequence by letting each word attend to all words in the sentence regardless of their distances. Such a fully connected structure endows SANs with the appealing strength of collecting the global information (Yu et al., 2018; Shen et al., 2018; Chen et al., 2018; Zhang et al., 2017a; Yang et al., 2019a). However, some recent researches observe that a fully connected SANs may overlook the important ∗ Corresponding author neighboring information (Luong et al., 2015; Sperber et al., 2018; Yang et al., 2019a). They find that SANs can be empirically enhanced by restricting the attention scope to a local area. One interesting question arises: how the local and global patterns quantitatively affect the SANs. To this end, we make empirical investigations with a hybrid attention mechanism, which integrates a local and a global attentive representation via a gating scalar. Empirical results on English-to-German and Japanese-to-English tasks demonstrate the effectiveness of using both the local and global information, which are shown complementary with each othe"
P19-1295,P11-2093,0,0.026638,") where σ(.) denotes the logistic sigmoid function. As seen, gating scalar offers the model a possibility to explicitly quantify the contribution of the local and global representations. 4 Experiments We evaluate the effectiveness of the proposed approach on widely used WMT 14 English-toGerman (En-De) and WAT17 Japanese-to-English (Ja-En) translation tasks. For the WAT17 benchmark, we follow (Morishita et al., 2017) to use the 3070 first two sections of WAT17 dataset as the training data, which contains 2M sentences. The Japanese sentences are segmented by the word segmentation toolkit KeTea (Neubig et al., 2011). To alleviate the problem of Out-of-Vocabulary, all the data are segmented into subword units using bytepair encoding (Sennrich et al., 2016) with 32K merge operations. We incorporate the proposed model 1 into the widely used SAN-based framework – T RANSFORMER (Vaswani et al., 2017) and following their network configuration. We refer readers to Appendix A.1 for the details of our data and experimental settings. Prior studies reveal that modeling locality in lower layers can achieve better performance (Shen et al., 2018; Yu et al., 2018; Yang et al., 2018). Therefore, we merely apply the local"
P19-1295,P02-1040,0,0.104604,"he proposed model 1 into the widely used SAN-based framework – T RANSFORMER (Vaswani et al., 2017) and following their network configuration. We refer readers to Appendix A.1 for the details of our data and experimental settings. Prior studies reveal that modeling locality in lower layers can achieve better performance (Shen et al., 2018; Yu et al., 2018; Yang et al., 2018). Therefore, we merely apply the locality model at the lowest two layers of the encoder. According to our empirical results (Section 5.2), we set the window size to 3 (i.e. m = 1). The 4-gram case-sensitive NIST BLEU score (Papineni et al., 2002) is used as the evaluation metric. 4.1 Results In this section, we give the ablation study of the proposed model and compare several existing works upon the same architecture. Effectiveness of Hybrid Attention Mechanism To make the evaluation convincing, we reproduced the reported results in Vaswani et al. (2017) on the same data as the baseline. We first investigate the effect of the local pattern without the global information. As shown in Table 1, restricting the attention scope to a local part is able to improve the performance of translation task, showing the effectiveness of localness mo"
P19-1295,D16-1244,0,0.0962473,"Missing"
P19-1295,D18-1179,0,0.0183767,"and semantic roles in the sentence. Both these results show that different words indeed have distinct requirements of the local and global information. Therefore, modeling locality and globality in a flexible fashion is necessary for SANs on sentence modeling. Gating Scalar across Layers As visualized in Figure 3, the requirements of the local information are reduced with the stacking of layers. This is consistent with the prior findings that the lower layers tend to learn more word- and phrase-level properties than the higher layers, while the top layers of SANs seek more global information (Peters et al., 2018; Yang et al., 2018; Devlin et al., 2019). Moreover, the local information is less than the global information even in the first layer, verifying our hypothesis that both the local and global patterns are necessary for SANs. 6 Gating Scalar across POS We further explore how different types of words learn the local information. In response to this problem, we categorize different words in validation set using the Universal Part-of-Speech tagset.2 Figure 4 shows the averaged factors learned for different types of words at the first layer. As seen, contrary to the content words (e.g, “NOUN”,“VERB"
P19-1295,P16-1162,0,0.0933449,"ution of the local and global representations. 4 Experiments We evaluate the effectiveness of the proposed approach on widely used WMT 14 English-toGerman (En-De) and WAT17 Japanese-to-English (Ja-En) translation tasks. For the WAT17 benchmark, we follow (Morishita et al., 2017) to use the 3070 first two sections of WAT17 dataset as the training data, which contains 2M sentences. The Japanese sentences are segmented by the word segmentation toolkit KeTea (Neubig et al., 2011). To alleviate the problem of Out-of-Vocabulary, all the data are segmented into subword units using bytepair encoding (Sennrich et al., 2016) with 32K merge operations. We incorporate the proposed model 1 into the widely used SAN-based framework – T RANSFORMER (Vaswani et al., 2017) and following their network configuration. We refer readers to Appendix A.1 for the details of our data and experimental settings. Prior studies reveal that modeling locality in lower layers can achieve better performance (Shen et al., 2018; Yu et al., 2018; Yang et al., 2018). Therefore, we merely apply the locality model at the lowest two layers of the encoder. According to our empirical results (Section 5.2), we set the window size to 3 (i.e. m = 1)."
P19-1295,D18-1475,1,0.563184,"is beneficial to the extraction of syntactic features, integrating with the global information further improves the performance on semantic probing tasks. The quantification analysis of gating scalar also indicates that different types of words have different requirements for the local and global information. 2 Related Works Previous work has shown that modeling locality benefits SANs for certain tasks. Luong et al. (2015) proposed a Gaussian-based local attention with a predictable position; Sperber et al. (2018) differently applied a local method with variable window size for acoustic task; Yang et al. (2018) investigated the affect of the dynamical local Gaussian bias by combining these two approaches for the translation task. Different from these methods using a learnable local scope, Yang et al. (2019b) and Wu et al. (2019) restricted the attention area with fixed size by borrowing the concept of convolution into SANs. Although both these methods yield considerable improvements, 3069 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3069–3075 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics they to some exte"
P19-1295,N19-1407,1,0.906131,"et al., 2017) have shown promising results for a range of NLP tasks, including machine translation (Vaswani et al., 2017), contextualized word embedding learning (Devlin et al., 2019), dependency parsing (Kitaev and Klein, 2018) and semantic role labeling (Tan et al., 2018). They learn hidden representations of a sequence by letting each word attend to all words in the sentence regardless of their distances. Such a fully connected structure endows SANs with the appealing strength of collecting the global information (Yu et al., 2018; Shen et al., 2018; Chen et al., 2018; Zhang et al., 2017a; Yang et al., 2019a). However, some recent researches observe that a fully connected SANs may overlook the important ∗ Corresponding author neighboring information (Luong et al., 2015; Sperber et al., 2018; Yang et al., 2019a). They find that SANs can be empirically enhanced by restricting the attention scope to a local area. One interesting question arises: how the local and global patterns quantitatively affect the SANs. To this end, we make empirical investigations with a hybrid attention mechanism, which integrates a local and a global attentive representation via a gating scalar. Empirical results on Engli"
P19-1342,D17-1209,0,0.0594362,"Missing"
P19-1342,I17-1034,0,0.01666,"rase in its constituent tree. There have been methods that classify each phrase independently (Li et al., 2015; McCann et al., 2017). However, sentiments over hierarchical phrases can have dependencies. For example, in Figure 1, both sentences have a phrase “an awesome day”, but the polarities of which are different according to their sentence level contexts. To better represent such sentiment dependencies, one can encode a constituency tree holistically using a neural encoder. To this end, treestructured LSTMs have been investigated as a dominant approach (Tai et al., 2015; Zhu et al., 2015; Gan and Gong, 2017; Yu et al., 2017; Liu et al., 2016). Such methods work by encoding hierarchical phrases bottom-up, so that sub constituents can be used as inputs for representing a constituent. However, they cannot pass information from a constituent node to its children, which can be necessary for cases similar to Figure 1. In this example, sentence level information from toplevel nodes is useful for disambiguating “an awesome day”. Bi-directional tree LSTMs provide a solution, using a separate top-down LSTM to augment a tree-LSTM (Teng and Zhang, 2017). This method has achieved highly competitive accuracie"
P19-1342,S15-1002,0,0.0427304,"Missing"
P19-1342,P18-1026,0,0.109356,"ng, 2017). This method has achieved highly competitive accuracies, at the cost of doubling the runtime. Intuitively, information exchange between tree nodes can happen beyond bottom-up and topdown directions. For example, direct communication between sibling nodes, such as (“an awesome day”, “winning the game”) and (“an awesome day”, “experiencing the tsunami”) can also bring benefits to tree representation. Recent advances of graph neural networks, such as graph convolutional neural network (GCN) (Kipf and Welling, 2016; Marcheggiani and Titov, 2017) and graph recurrent neural network (GRN) (Beck et al., 2018; Zhang et al., 2018b; Song et al., 2018) offer rich node communication patterns over graphs. For relation extraction, for example, GCNs have 3518 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3518–3527 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics been shown superior to tree LSTMs for encoding a dependency tree (Zhang et al., 2018c) We investigate both GCNs and GRNs as tree communication models for tree sentiment classification. In particular, initialized with a vanilla tree LSTM representation, eac"
P19-1342,D15-1278,0,0.0171542,"ts. 1 * 0 2 * 2 2 0 2 2 I had an -2 (2) -1 0 0 0 awesome day * 0 2 winning * -2 0 -2 -2 0 -2 had an game * -2 I 0 the -1 0 0 awesome day -1 0 experiencing the tsunami Figure 1: Examples of tree-based sentiment. Introduction There has been increasing research interest investigating sentiment classification over hierarchical phrases (Tai et al., 2015; Zhu et al., 2015; Looks et al., 2017; Teng and Zhang, 2017). As shown in Figure 1, the goal is to predict the sentiment class over a sentence and each phrase in its constituent tree. There have been methods that classify each phrase independently (Li et al., 2015; McCann et al., 2017). However, sentiments over hierarchical phrases can have dependencies. For example, in Figure 1, both sentences have a phrase “an awesome day”, but the polarities of which are different according to their sentence level contexts. To better represent such sentiment dependencies, one can encode a constituency tree holistically using a neural encoder. To this end, treestructured LSTMs have been investigated as a dominant approach (Tai et al., 2015; Zhu et al., 2015; Gan and Gong, 2017; Yu et al., 2017; Liu et al., 2016). Such methods work by encoding hierarchical phrases bot"
P19-1342,D16-1012,0,0.0135111,"ave been methods that classify each phrase independently (Li et al., 2015; McCann et al., 2017). However, sentiments over hierarchical phrases can have dependencies. For example, in Figure 1, both sentences have a phrase “an awesome day”, but the polarities of which are different according to their sentence level contexts. To better represent such sentiment dependencies, one can encode a constituency tree holistically using a neural encoder. To this end, treestructured LSTMs have been investigated as a dominant approach (Tai et al., 2015; Zhu et al., 2015; Gan and Gong, 2017; Yu et al., 2017; Liu et al., 2016). Such methods work by encoding hierarchical phrases bottom-up, so that sub constituents can be used as inputs for representing a constituent. However, they cannot pass information from a constituent node to its children, which can be necessary for cases similar to Figure 1. In this example, sentence level information from toplevel nodes is useful for disambiguating “an awesome day”. Bi-directional tree LSTMs provide a solution, using a separate top-down LSTM to augment a tree-LSTM (Teng and Zhang, 2017). This method has achieved highly competitive accuracies, at the cost of doubling the runti"
P19-1342,D17-1159,0,0.166727,"ion, using a separate top-down LSTM to augment a tree-LSTM (Teng and Zhang, 2017). This method has achieved highly competitive accuracies, at the cost of doubling the runtime. Intuitively, information exchange between tree nodes can happen beyond bottom-up and topdown directions. For example, direct communication between sibling nodes, such as (“an awesome day”, “winning the game”) and (“an awesome day”, “experiencing the tsunami”) can also bring benefits to tree representation. Recent advances of graph neural networks, such as graph convolutional neural network (GCN) (Kipf and Welling, 2016; Marcheggiani and Titov, 2017) and graph recurrent neural network (GRN) (Beck et al., 2018; Zhang et al., 2018b; Song et al., 2018) offer rich node communication patterns over graphs. For relation extraction, for example, GCNs have 3518 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3518–3527 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics been shown superior to tree LSTMs for encoding a dependency tree (Zhang et al., 2018c) We investigate both GCNs and GRNs as tree communication models for tree sentiment classification. In particul"
P19-1342,P16-1105,0,0.0217261,"d a right child node, respectively, in a recurrent state transition process. Formally, a treeLSTM calculates a cell state through an input gate, an output gate and two forget gates at each time step. In particular, at time step t, the input gate it and the output gate ot are calculated respectively as follows: R R L L ht−1 it = σ Whi ht−1 + Whi  R R + WciL cL t−1 + Wci ct−1 + bi , L L R R ot = σ Who ht−1 + Who ht−1 Related Work Bi-directional Tree-LSTM Paulus et al. (2014) capture bidirectional information over a binary tree by propagating global belief down from the tree root to leaf nodes. Miwa and Bansal (2016) adopt a bidirectional dependency treeLSTM model by introducing a top-down LSTM path. Teng and Zhang (2017) propose a first bidirectional tree-LSTM for constituent structures, by building a top-down tree-LSTM with estimations of head lexicons. Compared with their work, we achieve information interaction using an asymptotically more efficient algorithm, which performs node communication simultaneously across a whole tree. Graph Neural Network Scarselli et al. (2009) propose graph neural network (GNN) for encoding an arbitrary graph structure. Kipf and Welling (2016) use graph convolutional netw"
P19-1342,E17-1002,0,0.0412711,"Missing"
P19-1342,P05-1015,0,0.0681351,"he hidden states obtained after different recurrent steps: h = T −1 X wt hjt . t=0 5 SST-5 5 11,855 442,629 227,242 SST-2 2 9,613 137,988 185,621 Table 1: Data statistics. ), ept,2k+1 = cos(t/100002k/demb ), j Corpus Classes Sentences Phrases Tokens Decoding and Training Following Looks et al. (2017) and Teng and Zhang (2017), we perform softmax classification on each node according to the last hidden state: o = softmax(M h + b) where M and b are model parameters. For training, negative log-likelihood loss is computed over each o locally, and accumulated over the tree. reviews originally from Pang and Lee (2005) annotated at both the clause level and the sentence level. Following Zhu et al. (2015) and Teng and Zhang (2017), we perform both fine-grained sentiment classification and binary classification. For the former, the dataset was annotated for 5 levels of sentiment: strong negative, negative, neutral, positive, and strong positive. For the latter, the data was labeled with positive sentiment and negative sentiment. We adopt a standard dataset split following Tai et al. (2015); Teng and Zhang (2017). Table 1 lists the data statistics. 6.2 Experimental Settings Hyper-parameters We initialize word"
P19-1342,D14-1162,0,0.0820731,"lause level and the sentence level. Following Zhu et al. (2015) and Teng and Zhang (2017), we perform both fine-grained sentiment classification and binary classification. For the former, the dataset was annotated for 5 levels of sentiment: strong negative, negative, neutral, positive, and strong positive. For the latter, the data was labeled with positive sentiment and negative sentiment. We adopt a standard dataset split following Tai et al. (2015); Teng and Zhang (2017). Table 1 lists the data statistics. 6.2 Experimental Settings Hyper-parameters We initialize word embeddings using GloVe (Pennington et al., 2014) 300dimensional embeddings. Embeddings are finetuned during training. The size of LSTM hidden states are set to 300. We thus fix the number to 9. Training In order to obtain a good representation for an initial constituent state, we first train an independent bottom-up tree-LSTM, over which we train our tree communication models. To avoid over-fitting, we adopt dropout on the embedding layer, with a rate of 0.5. Training is done on minibatches through Adagrad (Duchi et al., 2011) with a learning rate of 0.05. We adopt gradient clipping with a threshold of 1.0. The L2 regularization parameter i"
P19-1342,N18-1202,0,0.019979,"works compared to a top-down LSTM for tree communication. Our model gives the state-of-the-art accuracies on phrase-level settings. Note that we do not leverage character representation or external resources such as sentiment lexicons and large-scale corpuses. There has also been work using large-scale external datasets to improve performance. McCann et al. (2017) pretrain their model on large parallel bilingual datasets and exploit character ngram features. They report an accuracy of 53.7 on sentence-level SST-5 and an accuracy of 90.3 on sentence-level SST-2, which are lower than our model. Peters et al. (2018) pretrain a language model with character convolutions on a large-scale corpus and report an accuracy of 54.7 on sentencelevel SST-5, which is slightly higher than our model. Large-scale pretraining is orthogonal to our method. For a fair comparison, we do not list their results on Table 3. We further analyze the performance with re84.0 93.5 83.5 93.0 83.0 92.5 82.5 92.0 91.5 82.0 5 10 15 Length 20 25 30 Figure 4: Sentiment classification accuracies against the sentence length. The accuracy for each length l is calculated on the test set sentences length in the bin [l, l + 5]. spect to differe"
P19-1342,D13-1170,0,0.133979,"n particular, initialized with a vanilla tree LSTM representation, each node repeatedly exchanges information with its neighbours using graph neural networks. Such multi-pass information exchange can allow each node to be more informed about its sentence-level context through rich communication patterns. In addition, the number of time steps does not scale with the height of the tree. To allow better interaction, we further propose a novel time-wise attention mechanism over GRN, which summarizes the representation after each communication step. Experiments on Stanford Sentiment Treebank (SST; Socher et al. 2013) show that our model outperforms standard bottom-up tree-LSTM (Zhu et al., 2015; Looks et al., 2017) and also recent work on bidirectional tree-LSTM (Teng and Zhang, 2017). In addition, our model allows a more holistic prediction of phase-level sentiments over the tree with a high degree of node sentiment consistency. To our knowledge, we are the first to investigate graph NNs for tree sentiment classification, and the first to discuss phrase level sentiment consistency over a constituent tree for SST. We release our code and models at https://github.com/fred2008/TCMSA. 2 giani and Titov (2017"
P19-1342,P18-1150,1,0.714669,"ly competitive accuracies, at the cost of doubling the runtime. Intuitively, information exchange between tree nodes can happen beyond bottom-up and topdown directions. For example, direct communication between sibling nodes, such as (“an awesome day”, “winning the game”) and (“an awesome day”, “experiencing the tsunami”) can also bring benefits to tree representation. Recent advances of graph neural networks, such as graph convolutional neural network (GCN) (Kipf and Welling, 2016; Marcheggiani and Titov, 2017) and graph recurrent neural network (GRN) (Beck et al., 2018; Zhang et al., 2018b; Song et al., 2018) offer rich node communication patterns over graphs. For relation extraction, for example, GCNs have 3518 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3518–3527 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics been shown superior to tree LSTMs for encoding a dependency tree (Zhang et al., 2018c) We investigate both GCNs and GRNs as tree communication models for tree sentiment classification. In particular, initialized with a vanilla tree LSTM representation, each node repeatedly exchanges information w"
P19-1342,P15-1150,0,0.560837,"al network, which allows rich information exchange between phrases constituent tree. Experiments show that our model outperforms existing work on bidirectional tree-LSTMs in both accuracy and efficiency, providing more consistent predictions on phrase-level sentiments. 1 * 0 2 * 2 2 0 2 2 I had an -2 (2) -1 0 0 0 awesome day * 0 2 winning * -2 0 -2 -2 0 -2 had an game * -2 I 0 the -1 0 0 awesome day -1 0 experiencing the tsunami Figure 1: Examples of tree-based sentiment. Introduction There has been increasing research interest investigating sentiment classification over hierarchical phrases (Tai et al., 2015; Zhu et al., 2015; Looks et al., 2017; Teng and Zhang, 2017). As shown in Figure 1, the goal is to predict the sentiment class over a sentence and each phrase in its constituent tree. There have been methods that classify each phrase independently (Li et al., 2015; McCann et al., 2017). However, sentiments over hierarchical phrases can have dependencies. For example, in Figure 1, both sentences have a phrase “an awesome day”, but the polarities of which are different according to their sentence level contexts. To better represent such sentiment dependencies, one can encode a constituency tree"
P19-1342,Q17-1012,1,0.24436,"en phrases constituent tree. Experiments show that our model outperforms existing work on bidirectional tree-LSTMs in both accuracy and efficiency, providing more consistent predictions on phrase-level sentiments. 1 * 0 2 * 2 2 0 2 2 I had an -2 (2) -1 0 0 0 awesome day * 0 2 winning * -2 0 -2 -2 0 -2 had an game * -2 I 0 the -1 0 0 awesome day -1 0 experiencing the tsunami Figure 1: Examples of tree-based sentiment. Introduction There has been increasing research interest investigating sentiment classification over hierarchical phrases (Tai et al., 2015; Zhu et al., 2015; Looks et al., 2017; Teng and Zhang, 2017). As shown in Figure 1, the goal is to predict the sentiment class over a sentence and each phrase in its constituent tree. There have been methods that classify each phrase independently (Li et al., 2015; McCann et al., 2017). However, sentiments over hierarchical phrases can have dependencies. For example, in Figure 1, both sentences have a phrase “an awesome day”, but the polarities of which are different according to their sentence level contexts. To better represent such sentiment dependencies, one can encode a constituency tree holistically using a neural encoder. To this end, treestruct"
P19-1342,D17-1056,0,0.017408,"ent tree. There have been methods that classify each phrase independently (Li et al., 2015; McCann et al., 2017). However, sentiments over hierarchical phrases can have dependencies. For example, in Figure 1, both sentences have a phrase “an awesome day”, but the polarities of which are different according to their sentence level contexts. To better represent such sentiment dependencies, one can encode a constituency tree holistically using a neural encoder. To this end, treestructured LSTMs have been investigated as a dominant approach (Tai et al., 2015; Zhu et al., 2015; Gan and Gong, 2017; Yu et al., 2017; Liu et al., 2016). Such methods work by encoding hierarchical phrases bottom-up, so that sub constituents can be used as inputs for representing a constituent. However, they cannot pass information from a constituent node to its children, which can be necessary for cases similar to Figure 1. In this example, sentence level information from toplevel nodes is useful for disambiguating “an awesome day”. Bi-directional tree LSTMs provide a solution, using a separate top-down LSTM to augment a tree-LSTM (Teng and Zhang, 2017). This method has achieved highly competitive accuracies, at the cost of"
P19-1342,P18-1030,1,0.834731,"hod has achieved highly competitive accuracies, at the cost of doubling the runtime. Intuitively, information exchange between tree nodes can happen beyond bottom-up and topdown directions. For example, direct communication between sibling nodes, such as (“an awesome day”, “winning the game”) and (“an awesome day”, “experiencing the tsunami”) can also bring benefits to tree representation. Recent advances of graph neural networks, such as graph convolutional neural network (GCN) (Kipf and Welling, 2016; Marcheggiani and Titov, 2017) and graph recurrent neural network (GRN) (Beck et al., 2018; Zhang et al., 2018b; Song et al., 2018) offer rich node communication patterns over graphs. For relation extraction, for example, GCNs have 3518 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3518–3527 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics been shown superior to tree LSTMs for encoding a dependency tree (Zhang et al., 2018c) We investigate both GCNs and GRNs as tree communication models for tree sentiment classification. In particular, initialized with a vanilla tree LSTM representation, each node repeatedly ex"
P19-1342,D18-1244,0,0.101582,"hod has achieved highly competitive accuracies, at the cost of doubling the runtime. Intuitively, information exchange between tree nodes can happen beyond bottom-up and topdown directions. For example, direct communication between sibling nodes, such as (“an awesome day”, “winning the game”) and (“an awesome day”, “experiencing the tsunami”) can also bring benefits to tree representation. Recent advances of graph neural networks, such as graph convolutional neural network (GCN) (Kipf and Welling, 2016; Marcheggiani and Titov, 2017) and graph recurrent neural network (GRN) (Beck et al., 2018; Zhang et al., 2018b; Song et al., 2018) offer rich node communication patterns over graphs. For relation extraction, for example, GCNs have 3518 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3518–3527 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics been shown superior to tree LSTMs for encoding a dependency tree (Zhang et al., 2018c) We investigate both GCNs and GRNs as tree communication models for tree sentiment classification. In particular, initialized with a vanilla tree LSTM representation, each node repeatedly ex"
P19-1393,N16-1098,0,0.132545,"asoning, but it can be non-trivial for a system to tell the difference. Arguably, commonsense reasoning should be a central capability in a practical NLU system (Davis, 2017); it is, therefore, important to be able to evaluate how well a model can do for sense making. Existing datasets test common sense indirectly through tasks that require extra knowledge, such Figure 1: Samples of our dataset as co-reference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011; Zhang et al., 2017; Zellers et al., 2018), or reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b). They verify whether a system is equipped with common sense by testing whether it can give a correct answer where the input does not contain such knowledge. However, there are two limitations to such benchmarks. First, they do not give a direct metric to quantitatively measure sense making capability. Second, they do not explicitly identify the key factors required in a sense making process. We address these issues by creating a testset for direct sense making. As shown in Figure 1, the 4020 Proceedings of the 57th Annual Meeting of the Association for Computational"
P19-1393,L18-1564,0,0.195852,"trivial for a system to tell the difference. Arguably, commonsense reasoning should be a central capability in a practical NLU system (Davis, 2017); it is, therefore, important to be able to evaluate how well a model can do for sense making. Existing datasets test common sense indirectly through tasks that require extra knowledge, such Figure 1: Samples of our dataset as co-reference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011; Zhang et al., 2017; Zellers et al., 2018), or reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b). They verify whether a system is equipped with common sense by testing whether it can give a correct answer where the input does not contain such knowledge. However, there are two limitations to such benchmarks. First, they do not give a direct metric to quantitatively measure sense making capability. Second, they do not explicitly identify the key factors required in a sense making process. We address these issues by creating a testset for direct sense making. As shown in Figure 1, the 4020 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4020–"
P19-1393,S18-1119,0,0.259284,"trivial for a system to tell the difference. Arguably, commonsense reasoning should be a central capability in a practical NLU system (Davis, 2017); it is, therefore, important to be able to evaluate how well a model can do for sense making. Existing datasets test common sense indirectly through tasks that require extra knowledge, such Figure 1: Samples of our dataset as co-reference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011; Zhang et al., 2017; Zellers et al., 2018), or reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b). They verify whether a system is equipped with common sense by testing whether it can give a correct answer where the input does not contain such knowledge. However, there are two limitations to such benchmarks. First, they do not give a direct metric to quantitatively measure sense making capability. Second, they do not explicitly identify the key factors required in a sense making process. We address these issues by creating a testset for direct sense making. As shown in Figure 1, the 4020 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4020–"
P19-1393,N18-1202,0,0.0519661,"In this paper, we release a benchmark to directly test whether a system can differentiate natural language statements that make sense from those that do not make sense. In addition, a system is asked to identify the most crucial reason why a statement does not make sense. We evaluate models trained over large-scale language modeling tasks as well as human performance, showing that there are different challenges for system sense making. 1 Introduction Natural Language Understanding (NLU) has received increasing research attention in recent years. With language models trained on large corpora (Peters et al., 2018; Devlin et al., 2018), algorithms show better performance than humans on some benchmarks (Group, 2017; Devlin et al., 2018). Compared to humans, however, most endto-end trained systems are rather weak on common sense. For example, it is straightforward for a human to understand that someone can put a turkey into a fridge but he can never put an elephant into a fridge with basic commonsense reasoning, but it can be non-trivial for a system to tell the difference. Arguably, commonsense reasoning should be a central capability in a practical NLU system (Davis, 2017); it is, therefore, important"
P19-1393,P17-1025,0,0.0428834,"chmark. Results show that there is still a large gap behind human performance despite that the models are trained over 100 million natural language sentences. Detailed examination shows that inference remains a challenge for such systems. To our knowledge, our dataset has the most direct decision-making process in commonsense reasoning and is the first one asking reasons behind the decision making process. Note that there has been dataset which focus on non-linguistic world knowledge plausibility (Wang et al., 2018) or only limited attributes or actions of physical knowledge like verbphysics (Forbes and Choi, 2017). They are related to our dataset but serve robotic research mainly. Our dataset is the first benchmark for direct linguistic sense making and explanation. We hope this benchmark can promote commonsense reasoning by the NLP community, and further applied on other applications such as machine translation and dialogue. Besides, we also expect that this work could be instructive on enhancing interpretability on commonsense reasoning research and other NLP tasks and on combining explanation with language generation. Our dataset is released at https://github.com/wangcunxiang/SenMaking-and-Explanati"
P19-1393,D16-1264,0,0.136017,"Missing"
P19-1393,P18-1043,0,0.0582548,"ing at the alternative endings ignoring the stories (Schwartz et al., 2017). Last, we control the length of each In contrast, to our work, the tasks above do not directly estimate common sense or ask the logical reasons behind the correct answers and questions. In recent years, some large-scale commonsense inference knowledge resources have been released, which may be helpful in commonsense reasoning tasks. Atomic (Sap et al., 2018) presents a huge everyday commonsense reasoning knowledge graph, which has nine if-then relations with variables, including causes, effects, and so on. Event2Mind (Rashkin et al., 2018) proposes a new corpus and task, aiming to find out mentioned/unmentioned people’s intents and reactions under various daily circumstances. These datasets are not directly useful for our benchmark since they focus only on a small domain. ConceptNet is a prestigious knowledge graph that has been upgraded over a long time (Liu and Singh, 2004; Havasi et al., 2007; Speer and Havasi, 2013; Speer et al., 2017). ConceptNet constructs triples using labeled edges as relations and various words and/or phrases as entities. It also has the sentences describing the corresponding triples. Thus we consider"
P19-1393,D18-1260,0,0.0378594,"4021 want as long as they will help in the task. 3 Some other datasets are evolved from QA problems and care more about factual commonsense knowledge. SQUABU (Davis, 2016) provides a small hand-constructed test of commonsense and scientific questions. CommonsenseQA (Talmor et al., 2018) asks crowd workers to create questions from ConceptNet (Speer et al., 2017), which is a large knowledge graph of commonsense knowledge, where each question discriminates its answer candidates between three target concepts that all share the same relationship to a single source drawn from ConceptNet OpenBookQA (Mihaylov et al., 2018) provides questions and answer candidates, as well as thousands of diverse facts about elementary level science that are related to the questions. The AI2 Reasoning Challenge (ARC) (Clark et al., 2018) gives thousands of questions with different knowledge types, as well as a relevant 14M-sentence corpus, mixed with science facts and other narrative sentences. Those questions are not easy to answer without specializing certain knowledge while our questions are easy for both adults and children. Task. Formally, each instance in our dataset is composed of 5 sentences: {s1 , s2 , r1 , r2 , r3 }. s"
P19-1393,K17-1004,0,0.0254525,"s explicitly. Next, the three option reasons should be only related to the incorrect sentence rather the correct sentence. Because we want further studies to be able to estimate against-common-sense statements without those correct statements. Furthermore, the confusing sentences should be correct themselves. Otherwise, the models may simply ignore the incorrect options without considering the casual relations between them. This worry was raised from the fact that models can achieve high performance in the ROC Story Cloze Task when only looking at the alternative endings ignoring the stories (Schwartz et al., 2017). Last, we control the length of each In contrast, to our work, the tasks above do not directly estimate common sense or ask the logical reasons behind the correct answers and questions. In recent years, some large-scale commonsense inference knowledge resources have been released, which may be helpful in commonsense reasoning tasks. Atomic (Sap et al., 2018) presents a huge everyday commonsense reasoning knowledge graph, which has nine if-then relations with variables, including causes, effects, and so on. Event2Mind (Rashkin et al., 2018) proposes a new corpus and task, aiming to find out me"
P19-1393,P18-2119,0,0.0378546,"to 1 (impossible) of human response after a certain situation. Situations with Adversarial Generations (SWAG) (Zellers et al., 2018) requests to choose the most likely-to-happen alternative after a specific situation. Those datasets put emphasis on the presituations and/or the after-situations of certain situations, but not on the reasons why they occur or lead. Some datasets are inspired by reading comprehension, providing some textual materials and questions, asking to find suitable answers from the provided materials. The Story Cloze Test and ROCStories Corpora (Mostafazadeh et al., 2016; Sharma et al., 2018) require to figure out the right ending from two candidate sentences after a foursentence story. For a narrative text, MCScript (Ostermann et al., 2018a) give various types of questions and pairs of answer candidates for each question. Most questions require knowledge beyond the facts mentioned in the text. Compared to those reading comprehension tasks, our benchmark encourages people to use any external resources they 4021 want as long as they will help in the task. 3 Some other datasets are evolved from QA problems and care more about factual commonsense knowledge. SQUABU (Davis, 2016) provi"
P19-1393,N18-2049,0,0.0321309,"aluate contextualized representations trained over large-scale language modeling tasks on our benchmark. Results show that there is still a large gap behind human performance despite that the models are trained over 100 million natural language sentences. Detailed examination shows that inference remains a challenge for such systems. To our knowledge, our dataset has the most direct decision-making process in commonsense reasoning and is the first one asking reasons behind the decision making process. Note that there has been dataset which focus on non-linguistic world knowledge plausibility (Wang et al., 2018) or only limited attributes or actions of physical knowledge like verbphysics (Forbes and Choi, 2017). They are related to our dataset but serve robotic research mainly. Our dataset is the first benchmark for direct linguistic sense making and explanation. We hope this benchmark can promote commonsense reasoning by the NLP community, and further applied on other applications such as machine translation and dialogue. Besides, we also expect that this work could be instructive on enhancing interpretability on commonsense reasoning research and other NLP tasks and on combining explanation with la"
P19-1393,D18-1009,0,0.354095,"elephant into a fridge with basic commonsense reasoning, but it can be non-trivial for a system to tell the difference. Arguably, commonsense reasoning should be a central capability in a practical NLU system (Davis, 2017); it is, therefore, important to be able to evaluate how well a model can do for sense making. Existing datasets test common sense indirectly through tasks that require extra knowledge, such Figure 1: Samples of our dataset as co-reference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011; Zhang et al., 2017; Zellers et al., 2018), or reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b). They verify whether a system is equipped with common sense by testing whether it can give a correct answer where the input does not contain such knowledge. However, there are two limitations to such benchmarks. First, they do not give a direct metric to quantitatively measure sense making capability. Second, they do not explicitly identify the key factors required in a sense making process. We address these issues by creating a testset for direct sense making. As shown in Figure 1, the 4020 Proceedings of the 57th"
P19-1457,D08-1083,0,0.0432787,"Tree-LSTM model that adds a top-down component after Tree-LSTM encoding. These models handle sentiment composition implicitly and predict sentiment polarities only based on embeddings of current nodes. In contrast, we model sentiment explicitly. Sentiment composition Moilanen and Pulman (2007) introduced a seminal model for sentiment composition (Montague, 1974), composed positive, negative and neutral (+1/-1/0) singles hierarchically. Taboada et al. (2011) proposed a lexiconbased method for addressing sentence level contextual valence shifting phenomena such as negation and intensification. Choi and Cardie (2008) used a structured linear model to learn semantic compositionality relying on a set of manual features. Dong et al. (2015) developed a statistical parser to learn the sentiment structure of a sentence. Our method is similar in that grammars are used to model semantic compositionality. But we consider neural methods instead of statistical methods for sentiment composition. Teng et al. (2016) proposed a simple weighted-sum model of introducing sentiment lexicon features to LSTM for sentiment analysis. They used -2 to 2 represent sentiment polarities. In contrast, we model sentiment subtypes with"
P19-1457,J15-2004,0,0.0519991,"al., 2017) have been exploited for modeling each phrase independently. Recently, tree structured models (Zhu et al., 2015; Tai et al., 2015; Teng and Zhang, 2017) were leveraged for learning phrase compositions in sentence representation given the syntactic structure. Such models classify the sentiment over each constituent node according to its hidden vector through tree structure encoding. Though effective, existing neural methods do not consider explicit sentiment compositionality (Montague, 1974). Take the sentence “The movie is not very good, but I still like it” in Figure 1 as example (Dong et al., 2015), over the constituent tree, sentiment signals can be propagated from leaf nodes to the root, going through negation, intensification and contrast according to the context. Modeling such signal channels can intuitively lead to more ∗ Work was done when the first author was visiting Westlake University. The third author is the corresponding author. interpretable and reliable results. To model sentiment composition, direct encoding of sentiment signals (e.g., +1/-1 or more fine-grained forms) is necessary. To this end, we consider a neural network grammar with latent variables. In particular, we"
P19-1457,S15-1002,0,0.0184592,"ults than coarsegrained models. Using a bi-attentive classification network (Peters et al., 2018) as the encoder, out final model gives the best results on SST. To our knowledge, we are the first to consider neural network grammars with latent variables for sentiment composition. Our code will be released at https://github.com/Ehaschia/bi-tree-lstm-crf. 2 Related Work Phrase-level sentiment analysis Li et al. (2015) and McCann et al. (2017) proposed sequence structured models that predict the sentiment polarities of the individual phrases in a sentence independently. Zhu et al. (2015), Le and Zuidema (2015), Tai et al. (2015) and Gupta and Zhang (2018) proposed Tree-LSTM models to capture bottom-up dependencies between constituents for sentiment analysis. In order to support information flow bidirectionally over trees, Teng and Zhang (2017) introduced a Bi-directional Tree-LSTM model that adds a top-down component after Tree-LSTM encoding. These models handle sentiment composition implicitly and predict sentiment polarities only based on embeddings of current nodes. In contrast, we model sentiment explicitly. Sentiment composition Moilanen and Pulman (2007) introduced a seminal model for sentime"
P19-1457,D15-1278,0,0.299164,"presentations that capture sentiment subtype expressions by latent variables and Gaussian mixture vectors, respectively. Experiments on Stanford Sentiment Treebank (SST) show the effectiveness of sentiment grammar over vanilla neural encoders. Using ELMo embeddings, our method gives the best results on this benchmark. 1 -1 -1 0 0 The movie is -1 -1 not 0 1 but I still like it , 1 0 1 very good Figure 1: Example of sentiment composition Introduction Determining the sentiment polarity at or below the sentence level is an important task in natural language processing. Sequence structured models (Li et al., 2015; McCann et al., 2017) have been exploited for modeling each phrase independently. Recently, tree structured models (Zhu et al., 2015; Tai et al., 2015; Teng and Zhang, 2017) were leveraged for learning phrase compositions in sentence representation given the syntactic structure. Such models classify the sentiment over each constituent node according to its hidden vector through tree structure encoding. Though effective, existing neural methods do not consider explicit sentiment compositionality (Montague, 1974). Take the sentence “The movie is not very good, but I still like it” in Figure 1 a"
P19-1457,P05-1010,0,0.205135,"To model sentiment composition, direct encoding of sentiment signals (e.g., +1/-1 or more fine-grained forms) is necessary. To this end, we consider a neural network grammar with latent variables. In particular, we employ a grammar as the backbone of our approach in which nonterminals represent sentiment signals and grammar rules specify sentiment compositions. In the simplest version of our approach, nonterminals are sentiment labels from SST directly, resulting in 1 a weighted grammar. To model more fine-grained emotions (Ortony and Turner, 1990), we consider a latent variable grammar (LVG, Matsuzaki et al. (2005), Petrov et al. (2006)), which splits each nonterminal into subtypes to represent subtle sentiment signals and uses a discrete latent variable to denote the sentiment subtype of a phrase. Finally, inspired by the fact that sentiment can be modeled with a low dimensional continuous space (Mehrabian, 1980), we introduce a Gaussian mixture latent vector grammar (GM-LVeG, Zhao et al. (2018)), which 4642 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4642–4651 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics"
P19-1457,D14-1162,0,0.082102,"and very positive, respectively. The root label represents the sentiment label of the whole sentence. The constituent node label represents the sentiment label of the phrase it spans. We perform both binary classification (-1, 1) and fine-grained classification (0-4), called SST-2 and SST-5, respectively. Following previous work, we use the labels of all phrases and gold-standard tree structures for training and testing. For binary classification, we merge all positive labels and negative labels. 5.2 Experimental Settings Hyper-parameters For ConTree, word vectors are initialized using Glove (Pennington et al., 2014) 300-dimensional embeddings and are updated together with other parameters. We set the hidden size of hidden units is 300. Adam (Kingma and Ba, 2014) is used to optimize the parameters with learning rate is 0.001. We adopt Dropout after the Embedding layer with a probability of 0.5. The sentence level mini-batch size is 32. For BCN experiment, we follow the model setting in McCann et al. (2017) except the sentence level mini-batch is set to 8. 5.3 Development Experiments We use the SST development dataset to investigate different configurations of our latent variables and Gaussian mixtures. Th"
P19-1457,N18-1202,0,0.496416,"018)), which 4642 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4642–4651 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics associates each sentiment signal with a continuous vector instead of a discrete variable. Experiments on SST show that explicit modeling of sentiment composition leads to significantly improved performance over standard tree encoding, and models that learn subtle emotions as hidden variables give better results than coarsegrained models. Using a bi-attentive classification network (Peters et al., 2018) as the encoder, out final model gives the best results on SST. To our knowledge, we are the first to consider neural network grammars with latent variables for sentiment composition. Our code will be released at https://github.com/Ehaschia/bi-tree-lstm-crf. 2 Related Work Phrase-level sentiment analysis Li et al. (2015) and McCann et al. (2017) proposed sequence structured models that predict the sentiment polarities of the individual phrases in a sentence independently. Zhu et al. (2015), Le and Zuidema (2015), Tai et al. (2015) and Gupta and Zhang (2018) proposed Tree-LSTM models to capture"
P19-1457,P06-1055,0,0.496334,"ition, direct encoding of sentiment signals (e.g., +1/-1 or more fine-grained forms) is necessary. To this end, we consider a neural network grammar with latent variables. In particular, we employ a grammar as the backbone of our approach in which nonterminals represent sentiment signals and grammar rules specify sentiment compositions. In the simplest version of our approach, nonterminals are sentiment labels from SST directly, resulting in 1 a weighted grammar. To model more fine-grained emotions (Ortony and Turner, 1990), we consider a latent variable grammar (LVG, Matsuzaki et al. (2005), Petrov et al. (2006)), which splits each nonterminal into subtypes to represent subtle sentiment signals and uses a discrete latent variable to denote the sentiment subtype of a phrase. Finally, inspired by the fact that sentiment can be modeled with a low dimensional continuous space (Mehrabian, 1980), we introduce a Gaussian mixture latent vector grammar (GM-LVeG, Zhao et al. (2018)), which 4642 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4642–4651 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics associates each sentim"
P19-1457,N07-1051,0,0.480552,"stead of statistical methods for sentiment composition. Teng et al. (2016) proposed a simple weighted-sum model of introducing sentiment lexicon features to LSTM for sentiment analysis. They used -2 to 2 represent sentiment polarities. In contrast, we model sentiment subtypes with latent variables and combine the strength of neural encoder and hierarchical sentiment composition. Latent Variable Grammar There has been a line of work using discrete latent variables to enrich coarse-grained constituent labels in phrasestructure parsing (Johnson, 1998; Matsuzaki et al., 2005; Petrov et al., 2006; Petrov and Klein, 2007). Our work is similar in that discrete latent variables are used to model sentiment polarities. To our knowledge, we are the first to consider modeling fine-grained sentiment signals by investigating different types of latent variables. Recently, there has been work using continuous latent vectors for modeling syntactic categories (Zhao et al., 2018). We consider their grammar also in modeling sentiment polarities. 3 Baseline We take the constituent Tree-LSTM as our baseline, which extends sequential LSTM to tree-structured network topologies. Formally, our model computes a parent representati"
P19-1457,D13-1170,0,0.026665,"sion rules. 5 Experiments To investigate the effectiveness of modeling sentiment composition explicitly and using discrete variables or continuous vectors to model sentiment subtypes, we compare standard constituent TreeLSTM (ConTree) with our models ConTree+WG, ConTree+LVG and ConTree+LVeG, respectively. To show the universality of our approaches, we also experiment with the combination of a state-of-theart sequence structured model, bi-attentive classification network (BCN, Peters et al. (2018)), with our model: BCN+WG, BCN+LVG and BCN+LVeG. 5.1 Data We use Stanford Sentiment TreeBank (SST, Socher et al. (2013)) for our experiments. Each constituent node in a phrase-structured tree is manually assigned an integer sentiment polarity from 0 to 4, which correspond to five sentiment classes: very negative, negative, neutral, positive and very positive, respectively. The root label represents the sentiment label of the whole sentence. The constituent node label represents the sentiment label of the phrase it spans. We perform both binary classification (-1, 1) and fine-grained classification (0-4), called SST-2 and SST-5, respectively. Following previous work, we use the labels of all phrases and gold-st"
P19-1457,J11-2001,0,0.0432387,"between constituents for sentiment analysis. In order to support information flow bidirectionally over trees, Teng and Zhang (2017) introduced a Bi-directional Tree-LSTM model that adds a top-down component after Tree-LSTM encoding. These models handle sentiment composition implicitly and predict sentiment polarities only based on embeddings of current nodes. In contrast, we model sentiment explicitly. Sentiment composition Moilanen and Pulman (2007) introduced a seminal model for sentiment composition (Montague, 1974), composed positive, negative and neutral (+1/-1/0) singles hierarchically. Taboada et al. (2011) proposed a lexiconbased method for addressing sentence level contextual valence shifting phenomena such as negation and intensification. Choi and Cardie (2008) used a structured linear model to learn semantic compositionality relying on a set of manual features. Dong et al. (2015) developed a statistical parser to learn the sentiment structure of a sentence. Our method is similar in that grammars are used to model semantic compositionality. But we consider neural methods instead of statistical methods for sentiment composition. Teng et al. (2016) proposed a simple weighted-sum model of introd"
P19-1457,P15-1150,0,0.183571,"Missing"
P19-1457,D16-1169,1,0.895889,"Missing"
P19-1457,Q17-1012,1,0.925999,") show the effectiveness of sentiment grammar over vanilla neural encoders. Using ELMo embeddings, our method gives the best results on this benchmark. 1 -1 -1 0 0 The movie is -1 -1 not 0 1 but I still like it , 1 0 1 very good Figure 1: Example of sentiment composition Introduction Determining the sentiment polarity at or below the sentence level is an important task in natural language processing. Sequence structured models (Li et al., 2015; McCann et al., 2017) have been exploited for modeling each phrase independently. Recently, tree structured models (Zhu et al., 2015; Tai et al., 2015; Teng and Zhang, 2017) were leveraged for learning phrase compositions in sentence representation given the syntactic structure. Such models classify the sentiment over each constituent node according to its hidden vector through tree structure encoding. Though effective, existing neural methods do not consider explicit sentiment compositionality (Montague, 1974). Take the sentence “The movie is not very good, but I still like it” in Figure 1 as example (Dong et al., 2015), over the constituent tree, sentiment signals can be propagated from leaf nodes to the root, going through negation, intensification and contras"
P19-1457,P18-1109,1,0.889839,"Missing"
P19-1457,J98-4004,0,\N,Missing
P19-1457,P18-1197,0,\N,Missing
Q17-1004,D15-1041,0,0.0154143,"layer. The input layer represents each word using its characters and token information; the encoder hidden layer uses a 49 bidirectional recurrent neural network structure to learn global features from the sentence; and the decoder layer predicts constituent hierarchies according to the encoder layer features, by using the attention mechanism (Bahdanau et al., 2015) to compute the contribution of each hidden unit of the encoder. 4.1 Input Layer The input layer generates a dense vector representation of each input word. We use character embeddings to alleviate OOV problems in word embeddings (Ballesteros et al., 2015; Santos and Zadrozny, 2014; Kim et al., 2016), concatenating characterembeddings of a word with its word embedding. Formally, the input representation xi of the word wi is computed by: xi = [xwi ; ci att ] X ci att = αij c0ij , j where xwi is a word embedding vector of the word wi according to a embedding lookup table, ci att is a character embedding form of the word wi , cij is the embedding of the jth character in wi , c0ij is the character window representation centered at cij , and αij is the contribution of the c0ij to ci att , which is computed by: 0 ef (xwi ,cij ) αij = P f (x ,c0 ) wi"
Q17-1004,J99-2004,0,0.127267,"erarchy is defined to describe how words can start or end hierarchical constituents (it can be empty if the word cannot start or end constituents). Second, spines are extracted from gold trees and used to prune the search space of parsing as hard constraints. In contrast, we use constituent hierarchies as soft features. Third, Carreras et al. (2008) use spines to prune chart parsing, while we use constituent hierarchies to improve a linear shift-reduce parser. For lexicalized grammars, supertags can benefit parsing significantly since they contain rich syntactic information as almost parsing (Bangalore and Joshi, 1999). Recently, there has been a line of work on better supertagging. Zhang et al. (2010a) proposed efficient methods to obtain supertags for HPSG parsing using dependency information. Xu et al. (2015) and Vaswani et al. (2016) leverage recursive neural networks for supertagging for CCG parsing. In contrast, our models predict the constituent hierarchy instead of a single supertag for each word in the input sentence. Our constituent hierarchy predictor is also related to sequence-to-sequence learning (Sutskever et al., 2014), which has been successfully used in neural machine translation (Bahdanau"
Q17-1004,W08-2102,0,0.570528,"t the original architecture achieves the highest performance on constituent hierarchy prediction, compared to the two baselines. The baseline only without character embeddings has relatively small influence on constituent hierarchy prediction. On the other hand, the baseline only without input word windows has relatively smaller influence on constituent hierarchy prediction. Nevertheless, both of these two ablation architectures lead to lower pars52 Parser Fully-supervised Ratnaparkhi (1997) Charniak (2000) Collins (2003) Sagae and Lavie (2005)† Sagae and Lavie (2006)† Petrov and Klein (2007) Carreras et al. (2008) Shindo et al. (2012) Zhu et al. (2013)† Socher et al. (2013)* Vinyals et al. (2015)* Cross and Huang (2016)*† Dyer et al. (2016)*† This work Ensemble Shindo et al. (2012) Vinyals et al. (2015)* Rerank Charniak and Johnson (2005) Huang (2008) Dyer et al. (2016)*† Semi-supervised McClosky et al. (2006) Huang and Harper (2009) Huang et al. (2010) Zhu et al. (2013)† Durrett and Klein (2015)* LR LP F1 86.3 89.5 88.1 86.1 87.8 90.1 90.7 N/A 90.2 N/A N/A N/A N/A 91.3 87.5 89.9 88.3 86.0 88.1 90.2 91.4 N/A 90.7 N/A N/A N/A N/A 92.1 86.9 89.5 88.2 86.0 87.9 90.1 91.1 91.1 90.4 90.4 88.3 91.3 91.2 91.7"
Q17-1004,P05-1022,0,0.191484,"Missing"
Q17-1004,A00-2018,0,0.840333,"Missing"
Q17-1004,D14-1082,0,0.0253981,"(2015), who predict a full parse tree from input, our predictors tackle a much simpler task, by predicting the constituent hierarchies of each word separately. In addition, the outputs of the predictors are used for soft lookahead features in bottom-up parsing, rather than 55 being taken as output structures directly. By integrating a neural constituent hierarchy predictor, our parser is related to neural network models for parsing, which has given competitive accuracies for both constituency parsing (Dyer et al., 2016; Cross and Huang, 2016; Watanabe and Sumita, 2015) and dependency parsing (Chen and Manning, 2014; Zhou et al., 2015; Dyer et al., 2015). In particular, our parser is more closely related to neural models that integrate discrete manual features (Socher et al., 2013; Durrett and Klein, 2015). Socher et al. (2013) use neural features to rerank a sparse baseline parser; Durrett and Klein directly integrate sparse features into neural layers in a chart parser. In contrast, we integrate neural information into sparse features in the form of lookahead features. There has also been work on lookahead features for parsing. Tsuruoka et al. (2011) run a baseline parser for a few future steps, and us"
Q17-1004,C04-1041,0,0.0724868,"tuent hierarchy is identified for each word. Third, the results are added into a transition-based parser as soft features, rather then being used as hard constraints to a chart parser. Our concept of constituent hierarchies is similar to supertags in the sense that both are shallow parses. For lexicalized grammars such as Combinatory Categorial Grammar (CCG), Tree-Adjoining Grammar (TAG) and Head-Driven Phrase Structure Grammar (HPSG), each word in the input sentence is assigned one or more supertags, which are used to identify the syntactic role of the word to constrain parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008; Fale´nska et al., 2015). For a lexicalized grammar, supertagging can benefit the parsing in both accuracy and efficiency by offering almostparsing information. In particular, Carreras et al. (2008) used the concept of spine for TAG (Schabes, 1992; Vijay-Shanker and Joshi, 1988), which is similar to our constituent hierarchy. However, there are three differences. First, the spine is defined to describe the main syntactic tree structure with a series baseline with lookahead feature improvement constituent hierarchy s-type e-typ"
Q17-1004,W02-2203,0,0.10392,"so the constituent hierarchy is identified for each word. Third, the results are added into a transition-based parser as soft features, rather then being used as hard constraints to a chart parser. Our concept of constituent hierarchies is similar to supertags in the sense that both are shallow parses. For lexicalized grammars such as Combinatory Categorial Grammar (CCG), Tree-Adjoining Grammar (TAG) and Head-Driven Phrase Structure Grammar (HPSG), each word in the input sentence is assigned one or more supertags, which are used to identify the syntactic role of the word to constrain parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008; Fale´nska et al., 2015). For a lexicalized grammar, supertagging can benefit the parsing in both accuracy and efficiency by offering almostparsing information. In particular, Carreras et al. (2008) used the concept of spine for TAG (Schabes, 1992; Vijay-Shanker and Joshi, 1988), which is similar to our constituent hierarchy. However, there are three differences. First, the spine is defined to describe the main syntactic tree structure with a series baseline with lookahead feature improvement constituen"
Q17-1004,P04-1015,0,0.0700004,"symbol c denotes the constituent label of an item; the symbol t is the POS of a lexical head; u denotes unary child; si ll denotes the left child of si ’s left child. empty, and the best completed state is taken as output. The score of a state is the total score of the transition actions that have been applied to build it: C(α) = N X i=1 Φ(αi ) · θ~ (1) Here Φ(αi ) represents the feature vector for the ith action αi in the state item α. N is the total number of actions in α. The model parameter vector θ~ is trained online using the averaged perceptron algorithm with the early-update strategy (Collins and Roark, 2004). 2.3 Baseline Features Our baseline features are taken from Zhu et al. (2013). As shown in Table 1, they include the U N IGRAM , B IGRAM , T RIGRAM features of Zhang and Clark (2009) and the extended features of Zhu et al. (2013). Templates s0 gs , s0 ge , s1 gs , s1 ge q0 gs , q0 ge , q1 gs , q1 ge Formally, a constituent hierarchy is defined as Table 2: Lookahead feature templates, where si represents the ith item on the top of the stack and qi denotes the ith item in the front end of the buffer. The symbol gs and ge denote the next level constituent in the s-type hierarchy and e-type hiera"
Q17-1004,J03-4003,0,0.606881,"Missing"
Q17-1004,D16-1001,0,0.531246,"es (Vinyals et al., 2015; Luong et al., 2015). Compared to Vinyals et al. (2015), who predict a full parse tree from input, our predictors tackle a much simpler task, by predicting the constituent hierarchies of each word separately. In addition, the outputs of the predictors are used for soft lookahead features in bottom-up parsing, rather than 55 being taken as output structures directly. By integrating a neural constituent hierarchy predictor, our parser is related to neural network models for parsing, which has given competitive accuracies for both constituency parsing (Dyer et al., 2016; Cross and Huang, 2016; Watanabe and Sumita, 2015) and dependency parsing (Chen and Manning, 2014; Zhou et al., 2015; Dyer et al., 2015). In particular, our parser is more closely related to neural models that integrate discrete manual features (Socher et al., 2013; Durrett and Klein, 2015). Socher et al. (2013) use neural features to rerank a sparse baseline parser; Durrett and Klein directly integrate sparse features into neural layers in a chart parser. In contrast, we integrate neural information into sparse features in the form of lookahead features. There has also been work on lookahead features for parsing."
Q17-1004,P08-1070,0,0.0262778,"ded into a transition-based parser as soft features, rather then being used as hard constraints to a chart parser. Our concept of constituent hierarchies is similar to supertags in the sense that both are shallow parses. For lexicalized grammars such as Combinatory Categorial Grammar (CCG), Tree-Adjoining Grammar (TAG) and Head-Driven Phrase Structure Grammar (HPSG), each word in the input sentence is assigned one or more supertags, which are used to identify the syntactic role of the word to constrain parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008; Fale´nska et al., 2015). For a lexicalized grammar, supertagging can benefit the parsing in both accuracy and efficiency by offering almostparsing information. In particular, Carreras et al. (2008) used the concept of spine for TAG (Schabes, 1992; Vijay-Shanker and Joshi, 1988), which is similar to our constituent hierarchy. However, there are three differences. First, the spine is defined to describe the main syntactic tree structure with a series baseline with lookahead feature improvement constituent hierarchy s-type e-type NP 92.06 93.10 +1.04 95.18 91.98 VP 90.63 92.45 +1.82 97.51 76.82"
Q17-1004,P15-1033,0,0.00626407,"input, our predictors tackle a much simpler task, by predicting the constituent hierarchies of each word separately. In addition, the outputs of the predictors are used for soft lookahead features in bottom-up parsing, rather than 55 being taken as output structures directly. By integrating a neural constituent hierarchy predictor, our parser is related to neural network models for parsing, which has given competitive accuracies for both constituency parsing (Dyer et al., 2016; Cross and Huang, 2016; Watanabe and Sumita, 2015) and dependency parsing (Chen and Manning, 2014; Zhou et al., 2015; Dyer et al., 2015). In particular, our parser is more closely related to neural models that integrate discrete manual features (Socher et al., 2013; Durrett and Klein, 2015). Socher et al. (2013) use neural features to rerank a sparse baseline parser; Durrett and Klein directly integrate sparse features into neural layers in a chart parser. In contrast, we integrate neural information into sparse features in the form of lookahead features. There has also been work on lookahead features for parsing. Tsuruoka et al. (2011) run a baseline parser for a few future steps, and use the output actions to guide the curre"
Q17-1004,N16-1024,0,0.226025,"s given raw sentences (Vinyals et al., 2015; Luong et al., 2015). Compared to Vinyals et al. (2015), who predict a full parse tree from input, our predictors tackle a much simpler task, by predicting the constituent hierarchies of each word separately. In addition, the outputs of the predictors are used for soft lookahead features in bottom-up parsing, rather than 55 being taken as output structures directly. By integrating a neural constituent hierarchy predictor, our parser is related to neural network models for parsing, which has given competitive accuracies for both constituency parsing (Dyer et al., 2016; Cross and Huang, 2016; Watanabe and Sumita, 2015) and dependency parsing (Chen and Manning, 2014; Zhou et al., 2015; Dyer et al., 2015). In particular, our parser is more closely related to neural models that integrate discrete manual features (Socher et al., 2013; Durrett and Klein, 2015). Socher et al. (2013) use neural features to rerank a sparse baseline parser; Durrett and Klein directly integrate sparse features into neural layers in a chart parser. In contrast, we integrate neural information into sparse features in the form of lookahead features. There has also been work on lookahead"
Q17-1004,W15-2215,0,0.0384772,"Missing"
Q17-1004,D09-1087,0,0.0735452,"Missing"
Q17-1004,D10-1002,0,0.0873257,"Missing"
Q17-1004,P08-1067,0,0.20113,"Missing"
Q17-1004,P10-1036,0,0.0252692,"st, we integrate neural information into sparse features in the form of lookahead features. There has also been work on lookahead features for parsing. Tsuruoka et al. (2011) run a baseline parser for a few future steps, and use the output actions to guide the current action. In contrast to their model, our model leverages full sentential information, yet is significantly faster. Previous work investigated more efficient parsing without loss of accuracy, which is required by real time applications, such as web parsing. Zhang et al. (2010b) introduced a chart pruner to accelerate a CCG parser. Kummerfeld et al. (2010) proposed a self-training method focusing on increasing the speed of a CCG parser rather than its accuracy. 8 Conclusion We proposed a novel constituent hierarchy predictor based on recurrent neural networks, aiming to capture global sentential information. The resulting constituent hierarchies are fed to a baseline shiftreduce parser as lookahead features, addressing limitations of shift-reduce parsers in not leveraging right-hand side syntax for local decisions, yet maintaining the same model size and speed. The resulting fully-supervised parser outperforms the state-of-theart baseline parse"
Q17-1004,J93-2004,0,0.0968251,"ised parsing. 1 Introduction Transition-based constituent parsers are fast and accurate, performing incremental parsing using a sequence of state transitions in linear time. Pioneering models rely on a classifier to make local decisions, searching greedily for local transitions to build a parse tree (Sagae and Lavie, 2005). Zhu et al. (2013) use a beam search framework, which preserves linear time complexity of greedy search, while alleviating the disadvantage of error propagation. The model gives state-of-the-art accuracies at a speed of 89 sentences per second on the standard WSJ benchmark (Marcus et al., 1993). Zhu et al. (2013) exploit rich features by extracting history information from a parser stack, which consists of a sequence of non-local constituents. However, due to the incremental nature of shiftreduce parsing, the right-hand side constituents of the current word cannot be used to guide the action at each step. Such lookahead features (Tsuruoka et al., 2011) correspond to the outside scores in chart parsing (Goodman, 1998), which has been effective for obtaining improved accuracies. To leverage such information for improving shiftreduce parsing, we propose a novel neural model to predict"
Q17-1004,N06-1020,0,0.0296755,"Missing"
Q17-1004,W06-1619,0,0.0412264,"ird, the results are added into a transition-based parser as soft features, rather then being used as hard constraints to a chart parser. Our concept of constituent hierarchies is similar to supertags in the sense that both are shallow parses. For lexicalized grammars such as Combinatory Categorial Grammar (CCG), Tree-Adjoining Grammar (TAG) and Head-Driven Phrase Structure Grammar (HPSG), each word in the input sentence is assigned one or more supertags, which are used to identify the syntactic role of the word to constrain parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008; Fale´nska et al., 2015). For a lexicalized grammar, supertagging can benefit the parsing in both accuracy and efficiency by offering almostparsing information. In particular, Carreras et al. (2008) used the concept of spine for TAG (Schabes, 1992; Vijay-Shanker and Joshi, 1988), which is similar to our constituent hierarchy. However, there are three differences. First, the spine is defined to describe the main syntactic tree structure with a series baseline with lookahead feature improvement constituent hierarchy s-type e-type NP 92.06 93.10 +1.04 95.18 91.98 VP 90.63 92"
Q17-1004,N07-1051,0,0.150906,"Missing"
Q17-1004,W97-0301,0,0.432475,"Missing"
Q17-1004,N09-1073,0,0.408668,"ich consists of a sequence of non-local constituents. However, due to the incremental nature of shiftreduce parsing, the right-hand side constituents of the current word cannot be used to guide the action at each step. Such lookahead features (Tsuruoka et al., 2011) correspond to the outside scores in chart parsing (Goodman, 1998), which has been effective for obtaining improved accuracies. To leverage such information for improving shiftreduce parsing, we propose a novel neural model to predict the constituent hierarchy related to each word before parsing. Our idea is inspired by the work of Roark and Hollingshead (2009) and Zhang et al. (2010b), which shows that shallow syntactic information gathered over the word sequence can be utilized for pruning chart parsers, improving chart parsing speed without sacrificing accuracies. For example, Roark and Hollingshead (2009) predict constituent boundary information on words as a preprocessing step, and use such information to prune the chart. Since such information is much lighterweight compared to full parsing, it can be predicted relatively accurately using sequence labellers. Different from Roark and Hollingshead (2009), we collect lookahead constituent informat"
Q17-1004,W05-1513,0,0.742943,"S|s0 ,i,f alse,k] [S|X,i,f alse,k+1] F INISH [S,n,f alse,k] [S,n,true,k+1] I DLE [S,n,true,k] [S,n,true,k+1] Figure 2: Deduction system for the baseline shiftreduce parsing process. baseline of Zhu et al. (2013), resulting in a accuracy of 91.7 F1 for English and 85.5 F1 for Chinese, which are the best for fully-supervised models in the literature. We release our code, based on ZPar (Zhang and Clark, 2011; Zhu et al., 2013), at https://github.com/SUTDNLP/LookAheadConparser. 2 Baseline System We adopt the parser of Zhu et al. (2013) for a baseline, which is based on the shift-reduce process of Sagae and Lavie (2005) and the beam search strategy of Zhang and Clark (2011) with global perceptron training. 2.1 The Shift-Reduce System Description Templates Shift-reduce parsers process an input sentence incrementally from left to right. A stack is used to maintain partial phrase-structures, while the incoming words are ordered in a buffer. At each step, a transition action is applied to consume an input word or construct a new phrase-structure. The set of transition actions are U NIGRAM • S HIFT: pop the front word off the buffer, and push it onto the stack. T RIGRAM s0 tc, s0 wc, s1 tc, s1 wc, s2 tc s2 wc, s3"
Q17-1004,N06-2033,0,0.0205472,"P. The constituent hierarchy predictor has relatively better performance on s-type labels for the constituents VP, WHNP and PP, which are prone to errors by the baseline system. The constituent hierarchy can give guidance to the constituent parser for tackling the issue. Compared to the s-type constituent hierarchy, the e-type constituent hierarchy 5 5.5 Comparison of Speed Table 8 shows the running times of various parsers on test sets on a Intel 2.2 GHz processor with 16G memory. Our parsers are much faster than the related parser with the same shift-reduce framework (Sagae and Lavie, 2005; Sagae and Lavie, 2006). Compared to the baseline parser, our parser gives 53 Phrase Type The constituent hierarchy prediction is excluded, which processes an average of 150 sentences per second on a single CPU. The cost of this step is far less than the cost of parsing, and can be essentially eliminated by pipelining the constituent hierarchy prediction and the shift-reduce decoder, by launching the constituent hierarchy predictor first, and then starting parsing in parallel as soon as the lookahead output is available for the first sentence, since the lookahead will outpace the parsing from that point forward. 95"
Q17-1004,H92-1027,0,0.338941,"Combinatory Categorial Grammar (CCG), Tree-Adjoining Grammar (TAG) and Head-Driven Phrase Structure Grammar (HPSG), each word in the input sentence is assigned one or more supertags, which are used to identify the syntactic role of the word to constrain parsing (Clark, 2002; Clark and Curran, 2004; Carreras et al., 2008; Ninomiya et al., 2006; Dridan et al., 2008; Fale´nska et al., 2015). For a lexicalized grammar, supertagging can benefit the parsing in both accuracy and efficiency by offering almostparsing information. In particular, Carreras et al. (2008) used the concept of spine for TAG (Schabes, 1992; Vijay-Shanker and Joshi, 1988), which is similar to our constituent hierarchy. However, there are three differences. First, the spine is defined to describe the main syntactic tree structure with a series baseline with lookahead feature improvement constituent hierarchy s-type e-type NP 92.06 93.10 +1.04 95.18 91.98 VP 90.63 92.45 +1.82 97.51 76.82 S 90.28 91.78 +1.50 93.37 80.72 PP 87.93 88.84 +0.91 98.01 84.80 SBAR 86.93 88.59 +1.66 92.14 66.82 ADVP 84.83 85.64 +0.81 88.94 85.01 ADJP 74.12 74.50 +0.38 79.88 71.16 WHNP 95.03 96.18 +1.15 96.18 95.13 QP 89.32 89.63 +0.31 91.70 91.02 Table 9:"
Q17-1004,P12-1046,0,0.537939,"Missing"
Q17-1004,P13-1045,0,0.16961,"constituent hierarchy prediction, compared to the two baselines. The baseline only without character embeddings has relatively small influence on constituent hierarchy prediction. On the other hand, the baseline only without input word windows has relatively smaller influence on constituent hierarchy prediction. Nevertheless, both of these two ablation architectures lead to lower pars52 Parser Fully-supervised Ratnaparkhi (1997) Charniak (2000) Collins (2003) Sagae and Lavie (2005)† Sagae and Lavie (2006)† Petrov and Klein (2007) Carreras et al. (2008) Shindo et al. (2012) Zhu et al. (2013)† Socher et al. (2013)* Vinyals et al. (2015)* Cross and Huang (2016)*† Dyer et al. (2016)*† This work Ensemble Shindo et al. (2012) Vinyals et al. (2015)* Rerank Charniak and Johnson (2005) Huang (2008) Dyer et al. (2016)*† Semi-supervised McClosky et al. (2006) Huang and Harper (2009) Huang et al. (2010) Zhu et al. (2013)† Durrett and Klein (2015)* LR LP F1 86.3 89.5 88.1 86.1 87.8 90.1 90.7 N/A 90.2 N/A N/A N/A N/A 91.3 87.5 89.9 88.3 86.0 88.1 90.2 91.4 N/A 90.7 N/A N/A N/A N/A 92.1 86.9 89.5 88.2 86.0 87.9 90.1 91.1 91.1 90.4 90.4 88.3 91.3 91.2 91.7 N/A N/A N/A N/A 92.4 90.5 91.2 92.2 N/A 91.8 91.2 N/A 91.5 9"
Q17-1004,W11-0328,0,0.120615,"ramework, which preserves linear time complexity of greedy search, while alleviating the disadvantage of error propagation. The model gives state-of-the-art accuracies at a speed of 89 sentences per second on the standard WSJ benchmark (Marcus et al., 1993). Zhu et al. (2013) exploit rich features by extracting history information from a parser stack, which consists of a sequence of non-local constituents. However, due to the incremental nature of shiftreduce parsing, the right-hand side constituents of the current word cannot be used to guide the action at each step. Such lookahead features (Tsuruoka et al., 2011) correspond to the outside scores in chart parsing (Goodman, 1998), which has been effective for obtaining improved accuracies. To leverage such information for improving shiftreduce parsing, we propose a novel neural model to predict the constituent hierarchy related to each word before parsing. Our idea is inspired by the work of Roark and Hollingshead (2009) and Zhang et al. (2010b), which shows that shallow syntactic information gathered over the word sequence can be utilized for pruning chart parsers, improving chart parsing speed without sacrificing accuracies. For example, Roark and Hol"
Q17-1004,N16-1027,0,0.00545439,"of parsing as hard constraints. In contrast, we use constituent hierarchies as soft features. Third, Carreras et al. (2008) use spines to prune chart parsing, while we use constituent hierarchies to improve a linear shift-reduce parser. For lexicalized grammars, supertags can benefit parsing significantly since they contain rich syntactic information as almost parsing (Bangalore and Joshi, 1999). Recently, there has been a line of work on better supertagging. Zhang et al. (2010a) proposed efficient methods to obtain supertags for HPSG parsing using dependency information. Xu et al. (2015) and Vaswani et al. (2016) leverage recursive neural networks for supertagging for CCG parsing. In contrast, our models predict the constituent hierarchy instead of a single supertag for each word in the input sentence. Our constituent hierarchy predictor is also related to sequence-to-sequence learning (Sutskever et al., 2014), which has been successfully used in neural machine translation (Bahdanau et al., 2015). The neural model encodes the source-side sentence into dense vectors, and then uses them to generate targetside word by word. There has also been work that directly applies sequence-to-sequence models for co"
Q17-1004,P14-1069,0,0.274019,"Missing"
Q17-1004,P15-1110,0,0.633622,"Missing"
Q17-1004,P15-1113,0,0.382701,"5; Luong et al., 2015). Compared to Vinyals et al. (2015), who predict a full parse tree from input, our predictors tackle a much simpler task, by predicting the constituent hierarchies of each word separately. In addition, the outputs of the predictors are used for soft lookahead features in bottom-up parsing, rather than 55 being taken as output structures directly. By integrating a neural constituent hierarchy predictor, our parser is related to neural network models for parsing, which has given competitive accuracies for both constituency parsing (Dyer et al., 2016; Cross and Huang, 2016; Watanabe and Sumita, 2015) and dependency parsing (Chen and Manning, 2014; Zhou et al., 2015; Dyer et al., 2015). In particular, our parser is more closely related to neural models that integrate discrete manual features (Socher et al., 2013; Durrett and Klein, 2015). Socher et al. (2013) use neural features to rerank a sparse baseline parser; Durrett and Klein directly integrate sparse features into neural layers in a chart parser. In contrast, we integrate neural information into sparse features in the form of lookahead features. There has also been work on lookahead features for parsing. Tsuruoka et al. (2011) run a"
Q17-1004,P15-2041,0,0.00653847,"une the search space of parsing as hard constraints. In contrast, we use constituent hierarchies as soft features. Third, Carreras et al. (2008) use spines to prune chart parsing, while we use constituent hierarchies to improve a linear shift-reduce parser. For lexicalized grammars, supertags can benefit parsing significantly since they contain rich syntactic information as almost parsing (Bangalore and Joshi, 1999). Recently, there has been a line of work on better supertagging. Zhang et al. (2010a) proposed efficient methods to obtain supertags for HPSG parsing using dependency information. Xu et al. (2015) and Vaswani et al. (2016) leverage recursive neural networks for supertagging for CCG parsing. In contrast, our models predict the constituent hierarchy instead of a single supertag for each word in the input sentence. Our constituent hierarchy predictor is also related to sequence-to-sequence learning (Sutskever et al., 2014), which has been successfully used in neural machine translation (Bahdanau et al., 2015). The neural model encodes the source-side sentence into dense vectors, and then uses them to generate targetside word by word. There has also been work that directly applies sequence"
Q17-1004,W09-3825,1,0.381818,"t completed state is taken as output. The score of a state is the total score of the transition actions that have been applied to build it: C(α) = N X i=1 Φ(αi ) · θ~ (1) Here Φ(αi ) represents the feature vector for the ith action αi in the state item α. N is the total number of actions in α. The model parameter vector θ~ is trained online using the averaged perceptron algorithm with the early-update strategy (Collins and Roark, 2004). 2.3 Baseline Features Our baseline features are taken from Zhu et al. (2013). As shown in Table 1, they include the U N IGRAM , B IGRAM , T RIGRAM features of Zhang and Clark (2009) and the extended features of Zhu et al. (2013). Templates s0 gs , s0 ge , s1 gs , s1 ge q0 gs , q0 ge , q1 gs , q1 ge Formally, a constituent hierarchy is defined as Table 2: Lookahead feature templates, where si represents the ith item on the top of the stack and qi denotes the ith item in the front end of the buffer. The symbol gs and ge denote the next level constituent in the s-type hierarchy and e-type hierarchy, respectively. where c is a constituent label (e.g. NP), “→” represents the top-down hierarchy, and type can be s or e, denoting that the current word starts or ends the constitu"
Q17-1004,J11-1005,1,0.396687,"e 46 Initial State Final State S HIFT [φ, 0, f alse, 0] [S, n, true, m : 2n &lt;= m &lt;= 4n] Induction Rules: [S,i,f alse,k] [S|w,i+1,f alse,k+1] R EDUCE - L / R - X [S|s1 s0 ,i,f alse,k] [S|X,i,f alse,k+1] U NARY- X [S|s0 ,i,f alse,k] [S|X,i,f alse,k+1] F INISH [S,n,f alse,k] [S,n,true,k+1] I DLE [S,n,true,k] [S,n,true,k+1] Figure 2: Deduction system for the baseline shiftreduce parsing process. baseline of Zhu et al. (2013), resulting in a accuracy of 91.7 F1 for English and 85.5 F1 for Chinese, which are the best for fully-supervised models in the literature. We release our code, based on ZPar (Zhang and Clark, 2011; Zhu et al., 2013), at https://github.com/SUTDNLP/LookAheadConparser. 2 Baseline System We adopt the parser of Zhu et al. (2013) for a baseline, which is based on the shift-reduce process of Sagae and Lavie (2005) and the beam search strategy of Zhang and Clark (2011) with global perceptron training. 2.1 The Shift-Reduce System Description Templates Shift-reduce parsers process an input sentence incrementally from left to right. A stack is used to maintain partial phrase-structures, while the incoming words are ordered in a buffer. At each step, a transition action is applied to consume an in"
Q17-1004,N10-1090,0,0.376433,"local constituents. However, due to the incremental nature of shiftreduce parsing, the right-hand side constituents of the current word cannot be used to guide the action at each step. Such lookahead features (Tsuruoka et al., 2011) correspond to the outside scores in chart parsing (Goodman, 1998), which has been effective for obtaining improved accuracies. To leverage such information for improving shiftreduce parsing, we propose a novel neural model to predict the constituent hierarchy related to each word before parsing. Our idea is inspired by the work of Roark and Hollingshead (2009) and Zhang et al. (2010b), which shows that shallow syntactic information gathered over the word sequence can be utilized for pruning chart parsers, improving chart parsing speed without sacrificing accuracies. For example, Roark and Hollingshead (2009) predict constituent boundary information on words as a preprocessing step, and use such information to prune the chart. Since such information is much lighterweight compared to full parsing, it can be predicted relatively accurately using sequence labellers. Different from Roark and Hollingshead (2009), we collect lookahead constituent information for shift-reduce pa"
Q17-1004,C10-2168,1,0.870942,"Missing"
Q17-1004,P15-1117,1,0.0597029,"ll parse tree from input, our predictors tackle a much simpler task, by predicting the constituent hierarchies of each word separately. In addition, the outputs of the predictors are used for soft lookahead features in bottom-up parsing, rather than 55 being taken as output structures directly. By integrating a neural constituent hierarchy predictor, our parser is related to neural network models for parsing, which has given competitive accuracies for both constituency parsing (Dyer et al., 2016; Cross and Huang, 2016; Watanabe and Sumita, 2015) and dependency parsing (Chen and Manning, 2014; Zhou et al., 2015; Dyer et al., 2015). In particular, our parser is more closely related to neural models that integrate discrete manual features (Socher et al., 2013; Durrett and Klein, 2015). Socher et al. (2013) use neural features to rerank a sparse baseline parser; Durrett and Klein directly integrate sparse features into neural layers in a chart parser. In contrast, we integrate neural information into sparse features in the form of lookahead features. There has also been work on lookahead features for parsing. Tsuruoka et al. (2011) run a baseline parser for a few future steps, and use the output action"
Q17-1004,P13-1043,1,0.857261,"embeddings as part of the model parameters. In the standard WSJ (Marcus et al., 1993) and CTB 5.1 tests (Xue et al., 2005), our parser gives 1.3 F1 and 2.3 F1 improvement, respectively, over the 46 Initial State Final State S HIFT [φ, 0, f alse, 0] [S, n, true, m : 2n &lt;= m &lt;= 4n] Induction Rules: [S,i,f alse,k] [S|w,i+1,f alse,k+1] R EDUCE - L / R - X [S|s1 s0 ,i,f alse,k] [S|X,i,f alse,k+1] U NARY- X [S|s0 ,i,f alse,k] [S|X,i,f alse,k+1] F INISH [S,n,f alse,k] [S,n,true,k+1] I DLE [S,n,true,k] [S,n,true,k+1] Figure 2: Deduction system for the baseline shiftreduce parsing process. baseline of Zhu et al. (2013), resulting in a accuracy of 91.7 F1 for English and 85.5 F1 for Chinese, which are the best for fully-supervised models in the literature. We release our code, based on ZPar (Zhang and Clark, 2011; Zhu et al., 2013), at https://github.com/SUTDNLP/LookAheadConparser. 2 Baseline System We adopt the parser of Zhu et al. (2013) for a baseline, which is based on the shift-reduce process of Sagae and Lavie (2005) and the beam search strategy of Zhang and Clark (2011) with global perceptron training. 2.1 The Shift-Reduce System Description Templates Shift-reduce parsers process an input sentence inc"
Q17-1012,P05-1022,0,0.0825344,"number of incorrect nodes in the oracle tree: ∆(yi , yˆi ) = X node∈ˆ yi κ1{node ∈ / yi }. (23) κ is a scalar. With this loss function, we require the score of the oracle tree to be higher than the other candidates by a score margin. Intuitively, the score of the yi will increase and the score of yˆi will decrease during training. Results. We experiment on the WSJ portion of the Penn Treebank, following the standard split (Collins, 2003). Sections 2-21 are used for training, Section 24 and Section 23 are the development 173 set and test set, respectively. The Charniak parser (Charniak, 2000; Charniak and Johnson, 2005) is adopted for our baseline by following the settings of Choe and Charniak (2016). To obtain N-best lists on the development set and test set, we first train a baseline parser on the training set. To obtain N-best lists on the training data, we split the training data into 20 folds and trained 20 parsers. Each parser was trained on 19 folds data and used to produce the n-best list of the remaining fold. For the neural reranking model, we use the pretrained word vectors from Collobert et al. (2011). The input dimension is 50. The dimension of state vectors in Tree-LSTM model is 60. These param"
Q17-1012,A00-2018,0,0.740548,"by counting the number of incorrect nodes in the oracle tree: ∆(yi , yˆi ) = X node∈ˆ yi κ1{node ∈ / yi }. (23) κ is a scalar. With this loss function, we require the score of the oracle tree to be higher than the other candidates by a score margin. Intuitively, the score of the yi will increase and the score of yˆi will decrease during training. Results. We experiment on the WSJ portion of the Penn Treebank, following the standard split (Collins, 2003). Sections 2-21 are used for training, Section 24 and Section 23 are the development 173 set and test set, respectively. The Charniak parser (Charniak, 2000; Charniak and Johnson, 2005) is adopted for our baseline by following the settings of Choe and Charniak (2016). To obtain N-best lists on the development set and test set, we first train a baseline parser on the training set. To obtain N-best lists on the training data, we split the training data into 20 folds and trained 20 parsers. Each parser was trained on 19 folds data and used to produce the n-best list of the remaining fold. For the neural reranking model, we use the pretrained word vectors from Collobert et al. (2011). The input dimension is 50. The dimension of state vectors in Tree-"
Q17-1012,D15-1092,0,0.0481706,"discussed when Tree LSTMs are necessary. In addition, Li et al. (2016) employed graph gated units to model graph-based structures. Tree LSTM The idea of extending linear recurrent structures to tree recurrent structures is reminiscent of extending Recurrent Neural Network to Recursive Neural Network (ReNN) (Socher et al., 2013b; Le and Zuidema, 2014) to support information flow over trees. In addition to Tai et al. (2015), Zhu et al. (2015) and Le and Zuidema (2015), who 165 explicitly named their models as Tree LSTMs, Cho et al. (2014) designed gated recurrent units over tree structures, and Chen et al. (2015) introduced gate mechanisms to recursive neural networks. These can also be regarded as variants of Tree LSTMs. Both Zhu et al. (2015) and Le and Zuidema (2015) proposed Binary Tree LSTM models, which can be applied to situations where there are exactly two children of each internal node in a tree. The difference between Zhu et al. (2015) and Le and Zuidema (2015) is that besides using two forget gates, Le and Zuidema (2015) also make use of two input gates to let a node know its sibling. Tai et al. (2015) introduced Child-Sum Tree LSTM and Nary Tree LSTM. Child-Sum Tree LSTMs can support mult"
Q17-1012,W14-4012,0,0.107252,"Missing"
Q17-1012,D16-1257,0,0.105869,"yi }. (23) κ is a scalar. With this loss function, we require the score of the oracle tree to be higher than the other candidates by a score margin. Intuitively, the score of the yi will increase and the score of yˆi will decrease during training. Results. We experiment on the WSJ portion of the Penn Treebank, following the standard split (Collins, 2003). Sections 2-21 are used for training, Section 24 and Section 23 are the development 173 set and test set, respectively. The Charniak parser (Charniak, 2000; Charniak and Johnson, 2005) is adopted for our baseline by following the settings of Choe and Charniak (2016). To obtain N-best lists on the development set and test set, we first train a baseline parser on the training set. To obtain N-best lists on the training data, we split the training data into 20 folds and trained 20 parsers. Each parser was trained on 19 folds data and used to produce the n-best list of the remaining fold. For the neural reranking model, we use the pretrained word vectors from Collobert et al. (2011). The input dimension is 50. The dimension of state vectors in Tree-LSTM model is 60. These parameters are trained with ADAM (Kingma and Ba, 2015) with a batch size of 20. We set"
Q17-1012,P04-1014,0,0.0601436,"missing in forming a cell (Figure 1(b)). We fill this gap by proposing an extension to the tree LSTM model, injecting lexical information into every node in the tree. Our method takes inspiration from work on head-lexicalization, which shows that each node in a constituent tree structure is governed by a head word. As shown in Figure 2, the head word for the verb phrase “visited Mary” is “visited”, and the head word of the adverb phrase “this afternoon” is “afternoon”. Research has shown that head word information can significantly improve the performance of syntactic parsing (Collins, 2003; Clark and Curran, 2004). Correspondingly, we use the head lexical information of each constituent word as the input node x for calculating the corresponding cell c in Figure 1(b). 164 Figure 2: Head-Lexicalized Constituent Tree. Traditional head-lexicalization relies on specific rules (Collins, 2003; Zhang and Clark, 2009), typically extracting heads from constituent treebanks according to certain grammar formalisms. For better generalization, we use a neural attention mechanism to derive head lexical information automatically, rather than relying on linguistic head rules to find the head lexicon of each constituent"
Q17-1012,J03-4003,0,0.859337,"on (i.e. words) missing in forming a cell (Figure 1(b)). We fill this gap by proposing an extension to the tree LSTM model, injecting lexical information into every node in the tree. Our method takes inspiration from work on head-lexicalization, which shows that each node in a constituent tree structure is governed by a head word. As shown in Figure 2, the head word for the verb phrase “visited Mary” is “visited”, and the head word of the adverb phrase “this afternoon” is “afternoon”. Research has shown that head word information can significantly improve the performance of syntactic parsing (Collins, 2003; Clark and Curran, 2004). Correspondingly, we use the head lexical information of each constituent word as the input node x for calculating the corresponding cell c in Figure 1(b). 164 Figure 2: Head-Lexicalized Constituent Tree. Traditional head-lexicalization relies on specific rules (Collins, 2003; Zhang and Clark, 2009), typically extracting heads from constituent treebanks according to certain grammar formalisms. For better generalization, we use a neural attention mechanism to derive head lexical information automatically, rather than relying on linguistic head rules to find the head le"
Q17-1012,P15-1033,0,0.0232616,"oning (Vinyals et al., 2015b). There are many variants of sequential LSTMs, such as simple Gated Recurrent Neural Networks (Cho et al., 2014). Greff et al. (2017) compared various architectures of LSTM. In this paper, we take the standard LSTM with peephole connections (Gers and Schmidhuber, 2000) as a baseline. Structured LSTM There has been a line of research that extends the standard sequential LSTM in order to model more complex structures. Kalchbrenner et al. (2016) proposed Grid LSTMs to process multi-dimensional data. Theis and Bethge (2015) proposed Spatial LSTMs to handle image data. Dyer et al. (2015) designed Stack LSTMs by adding a top pointer to sequential LSTMs to deal with push and pop sequences of a stack. Tai et al. (2015), Zhu et al. (2015) and Le and Zuidema (2015) extended sequential LSTMs to Tree-Structured LSTMs (Tree LSTMs) by adding branching factors. Experiments demonstrated that Tree LSTMs can outperform competitive LSTM baselines on several tasks, such as semantic relatedness prediction and sentiment classification. Li et al. (2015) further investigated the effectiveness of Tree LSTMs on various tasks and discussed when Tree LSTMs are necessary. In addition, Li et al. (201"
Q17-1012,N16-1024,0,0.023412,"ne Y (xi ) to be the 10-best parsing trees of xi . Table 7 shows the reranking results on WSJ test set. The baseline F1 score is 89.7. Our ConTree improves the baseline model to 90.6. Using ConTree+Lex model can further improve the performance (90.6 → 90.9). This suggests that automatic heads can also be useful for a syntactic task. Among neural rerankers, our model outperforms Socher et al. (2013a), but underperforms current state-of-theart models, including sequence-to-sequence based LSTM language models (Vinyals et al., 2015a; Choe and Charniak, 2016) and recurrent neural network grammars (Dyer et al., 2016). This is likely due to our simple reranking configurations and settings5 . Nevertheless, it serves our goal of contrasting the tree LSTM models. 8.2 Language Modeling Kuncoro et al. (2017) investigate composition functions in recurrent neural network grammars (RNNG) (Dyer et al., 2016), finding that syntactic head information can be automatically learned. Their observa5 Dyer et al. (2016) employs 2-layerd LSTMs with input and hidden dimensions of size 256 and 128. Choe and Charniak (2016) use 3-layered LSTMs with both the input and hidden dimensions of size 1500. In addition, we only use the"
Q17-1012,P03-1054,0,0.0614625,"esents the head lexicon of its right child constituent. The gate R zt is calculated based on xL t−1 and xt−1 , L L R R zt = σ(Wzx xt−1 + Wzx xt−1 + bz ) (9) L , W R and b are model parameters. Here Wzx z zx 4.2 Lexicalized Tree LSTM Given head lexicon vectors for nodes, the Tree LSTM of Zhu et al. (2015) can be extended by leveraging xt in calculating the corresponding ct . In particular, xt is used to estimate the input (it ), output 1 In this paper, we work on binary trees only, which is a common form for CKY and shift-reduce parsing. Typical binarization methods, such as head binarization (Klein and Manning, 2003) , also rely on specific head-finding rules. 167  = σ Wxf xt +  X N N N N (Whf h + Wcf c ) + bfl l t−1 l t−1 N ∈{L,R} ftR  = σ Wxf xt +  X N N N N (Whf h + W c ) + b f t−1 cf t−1 r r r (10) N ∈{L,R}  ot = σ Wxo xt +  X N N Who ht−1 + Wco ct + bo N ∈{L,R} In addition, xt is also used in computing gt ,   X N N Whg ht−1 + bg gt = tanh Wxg xt + (11) N ∈{L,R} With the new definition of it , ftR , ftL and gt , the computing of ct remains the same as the baseline Tree LSTM model as shown in Equation 4. Similarly, ht remains the Hadamard product of ct and the new ot as shown in Equation 1. In"
Q17-1012,E17-1117,0,0.153395,"ing ConTree+Lex model can further improve the performance (90.6 → 90.9). This suggests that automatic heads can also be useful for a syntactic task. Among neural rerankers, our model outperforms Socher et al. (2013a), but underperforms current state-of-theart models, including sequence-to-sequence based LSTM language models (Vinyals et al., 2015a; Choe and Charniak, 2016) and recurrent neural network grammars (Dyer et al., 2016). This is likely due to our simple reranking configurations and settings5 . Nevertheless, it serves our goal of contrasting the tree LSTM models. 8.2 Language Modeling Kuncoro et al. (2017) investigate composition functions in recurrent neural network grammars (RNNG) (Dyer et al., 2016), finding that syntactic head information can be automatically learned. Their observa5 Dyer et al. (2016) employs 2-layerd LSTMs with input and hidden dimensions of size 256 and 128. Choe and Charniak (2016) use 3-layered LSTMs with both the input and hidden dimensions of size 1500. In addition, we only use the tree LSTM for scoring candidate parses in order to isolate the effect of tree LSTMs. In contrast, the previous works use the complex feature combinations in order to achieve high accuracies"
Q17-1012,D14-1081,0,0.043613,"Experiments demonstrated that Tree LSTMs can outperform competitive LSTM baselines on several tasks, such as semantic relatedness prediction and sentiment classification. Li et al. (2015) further investigated the effectiveness of Tree LSTMs on various tasks and discussed when Tree LSTMs are necessary. In addition, Li et al. (2016) employed graph gated units to model graph-based structures. Tree LSTM The idea of extending linear recurrent structures to tree recurrent structures is reminiscent of extending Recurrent Neural Network to Recursive Neural Network (ReNN) (Socher et al., 2013b; Le and Zuidema, 2014) to support information flow over trees. In addition to Tai et al. (2015), Zhu et al. (2015) and Le and Zuidema (2015), who 165 explicitly named their models as Tree LSTMs, Cho et al. (2014) designed gated recurrent units over tree structures, and Chen et al. (2015) introduced gate mechanisms to recursive neural networks. These can also be regarded as variants of Tree LSTMs. Both Zhu et al. (2015) and Le and Zuidema (2015) proposed Binary Tree LSTM models, which can be applied to situations where there are exactly two children of each internal node in a tree. The difference between Zhu et al."
Q17-1012,S15-1002,0,0.0889742,". 1 Depending on the node type, typical tree structures in NLP can be categorized to constituent trees and dependency trees. A salient difference between the two types of tree structures is in the node. While dependency tree nodes are input words themselves, constituent tree nodes represent syntactic constituents. Only leaf nodes in constituent trees correspond to words. Though LSTM structures have been developed for both types of trees above, we investigate constituent trees in this paper. There are three existing methods for constituent tree LSTMs (Zhu et al., 2015; Tai et al., 2015; Le and Zuidema, 2015), which make essentially the same extension from sequence structured LSTMs. We take the method of Zhu et al. (2015) as our baseline. Introduction Both sequence structured and tree structured neural models have been applied to NLP problems. Seminal work uses convolutional neural networks (Collobert and Weston, 2008), recurrent neural networks (Elman, 1990; Mikolov et al., 2010) and recursive neural networks (Socher et al., 2011) for sequence and tree modeling. Long short-term memory (LSTM) networks have significantly improved accuracies in a variety of sequence tasks (Sutskever et al., 2014; Ba"
Q17-1012,C02-1150,0,0.0630203,"very negative, negative, neutral, positive and very positive, respectively. The root label represents the sentiment label of the whole sentence. We perform both binary classification and finegrained classification. Following previous work, we use labels of all phrases for training. Gold-standard 3 tree structures are used for training and testing (Le and Zuidema, 2015; Li et al., 2015; Zhu et al., 2015; Tai et al., 2015). Accuracies are evaluated for both the sentence root labels and phrase labels. Question Type Classification. For the question type classification task, we use the TREC data (Li and Roth, 2002). Each training sample in this dataset contains a question sentence and its corresponding question type. We work on the sixway coarse classification task, where the six question types are ENTY, HUM, LOC, DESC, NUM and ABBR, corresponding to ENTITY, HUMAN, LOCATION, DESCRIPTION, NUMERIC VALUE and ABBREVIATION, respectively. For example, the type for the sentence “What year did the Titanic sink?” is NUM. The training set consists of 5,452 examples and the test set contains 500 examples. Since there is no development set, we follow Zhou et al. (2015), randomly extracting 500 examples from the tra"
Q17-1012,D15-1278,0,0.24231,"tention mechanism to derive head lexical information automatically, rather than relying on linguistic head rules to find the head lexicon of each constituent, which is language- and formalism-dependent. Based on such head lexicalization, we further make a bidirectional extension of the tree structured LSTM, propagating information in the top-down direction as well as the bottom-up direction. This is analogous to the bidirectional extension of sequence structured LSTMs, which are commonly used for NLP tasks such as speech recognition (Graves et al., 2013), sentiment analysis (Tai et al., 2015; Li et al., 2015) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) tasks. Results on a standard sentiment classification benchmark and a question type classification benchmark show that our tree LSTM structure gives significantly better accuracies compared with the method of Zhu et al. (2015). We achieve the best reported results for sentiment classification. Interestingly, the head lexical information that is learned automatically from the sentiment treebank consists of both syntactic head information and key sentiment word information. This shows the advantage of automatic head-finding"
Q17-1012,P16-1105,0,0.439586,"des using two forget gates, Le and Zuidema (2015) also make use of two input gates to let a node know its sibling. Tai et al. (2015) introduced Child-Sum Tree LSTM and Nary Tree LSTM. Child-Sum Tree LSTMs can support multiple children, while N-ary Tree LSTMs work for trees with a branching factor of at most N . In this perspective, Binary Tree LSTM is a special case of N-ary Tree LSTM with N = 2. When a Child-Sum Tree LSTM is applied to a dependency tree, it is referred to as a Dependency Tree LSTM. A Binary Tree LSTM is also referred to as a Constituent Tree LSTM. Based on Tai et al. (2015), Miwa and Bansal (2016) introduced a Tree LSTM model that can handle different types of children. A dependency tree naturally contains lexical information at every node, while only leaf nodes contain lexical information in a constituent tree. None of these methods (Tai et al., 2015; Zhu et al., 2015; Le and Zuidema, 2015) make direct use of lexical input for internal nodes when using constituent Tree LSTMs. Bi-LSTM Another common extension to sequential LSTM is to include bidirectional information (Graves et al., 2013), which can model history both left-to-right and right-to-left. The aforementioned Tree LSTM models"
Q17-1012,P13-1045,0,0.565216,"by adding branching factors. Experiments demonstrated that Tree LSTMs can outperform competitive LSTM baselines on several tasks, such as semantic relatedness prediction and sentiment classification. Li et al. (2015) further investigated the effectiveness of Tree LSTMs on various tasks and discussed when Tree LSTMs are necessary. In addition, Li et al. (2016) employed graph gated units to model graph-based structures. Tree LSTM The idea of extending linear recurrent structures to tree recurrent structures is reminiscent of extending Recurrent Neural Network to Recursive Neural Network (ReNN) (Socher et al., 2013b; Le and Zuidema, 2014) to support information flow over trees. In addition to Tai et al. (2015), Zhu et al. (2015) and Le and Zuidema (2015), who 165 explicitly named their models as Tree LSTMs, Cho et al. (2014) designed gated recurrent units over tree structures, and Chen et al. (2015) introduced gate mechanisms to recursive neural networks. These can also be regarded as variants of Tree LSTMs. Both Zhu et al. (2015) and Le and Zuidema (2015) proposed Binary Tree LSTM models, which can be applied to situations where there are exactly two children of each internal node in a tree. The differ"
Q17-1012,D13-1170,0,0.599326,"by adding branching factors. Experiments demonstrated that Tree LSTMs can outperform competitive LSTM baselines on several tasks, such as semantic relatedness prediction and sentiment classification. Li et al. (2015) further investigated the effectiveness of Tree LSTMs on various tasks and discussed when Tree LSTMs are necessary. In addition, Li et al. (2016) employed graph gated units to model graph-based structures. Tree LSTM The idea of extending linear recurrent structures to tree recurrent structures is reminiscent of extending Recurrent Neural Network to Recursive Neural Network (ReNN) (Socher et al., 2013b; Le and Zuidema, 2014) to support information flow over trees. In addition to Tai et al. (2015), Zhu et al. (2015) and Le and Zuidema (2015), who 165 explicitly named their models as Tree LSTMs, Cho et al. (2014) designed gated recurrent units over tree structures, and Chen et al. (2015) introduced gate mechanisms to recursive neural networks. These can also be regarded as variants of Tree LSTMs. Both Zhu et al. (2015) and Le and Zuidema (2015) proposed Binary Tree LSTM models, which can be applied to situations where there are exactly two children of each internal node in a tree. The differ"
Q17-1012,P15-1150,0,0.100262,"type classification task. 1 Depending on the node type, typical tree structures in NLP can be categorized to constituent trees and dependency trees. A salient difference between the two types of tree structures is in the node. While dependency tree nodes are input words themselves, constituent tree nodes represent syntactic constituents. Only leaf nodes in constituent trees correspond to words. Though LSTM structures have been developed for both types of trees above, we investigate constituent trees in this paper. There are three existing methods for constituent tree LSTMs (Zhu et al., 2015; Tai et al., 2015; Le and Zuidema, 2015), which make essentially the same extension from sequence structured LSTMs. We take the method of Zhu et al. (2015) as our baseline. Introduction Both sequence structured and tree structured neural models have been applied to NLP problems. Seminal work uses convolutional neural networks (Collobert and Weston, 2008), recurrent neural networks (Elman, 1990; Mikolov et al., 2010) and recursive neural networks (Socher et al., 2011) for sequence and tree modeling. Long short-term memory (LSTM) networks have significantly improved accuracies in a variety of sequence tasks (Sut"
Q17-1012,D16-1169,1,0.896395,"Missing"
Q17-1012,W09-3825,1,0.798442,"by a head word. As shown in Figure 2, the head word for the verb phrase “visited Mary” is “visited”, and the head word of the adverb phrase “this afternoon” is “afternoon”. Research has shown that head word information can significantly improve the performance of syntactic parsing (Collins, 2003; Clark and Curran, 2004). Correspondingly, we use the head lexical information of each constituent word as the input node x for calculating the corresponding cell c in Figure 1(b). 164 Figure 2: Head-Lexicalized Constituent Tree. Traditional head-lexicalization relies on specific rules (Collins, 2003; Zhang and Clark, 2009), typically extracting heads from constituent treebanks according to certain grammar formalisms. For better generalization, we use a neural attention mechanism to derive head lexical information automatically, rather than relying on linguistic head rules to find the head lexicon of each constituent, which is language- and formalism-dependent. Based on such head lexicalization, we further make a bidirectional extension of the tree structured LSTM, propagating information in the top-down direction as well as the bottom-up direction. This is analogous to the bidirectional extension of sequence st"
Q17-1012,J11-1005,1,0.737436,", DESC, NUM and ABBR, corresponding to ENTITY, HUMAN, LOCATION, DESCRIPTION, NUMERIC VALUE and ABBREVIATION, respectively. For example, the type for the sentence “What year did the Titanic sink?” is NUM. The training set consists of 5,452 examples and the test set contains 500 examples. Since there is no development set, we follow Zhou et al. (2015), randomly extracting 500 examples from the training set as a development set. Unlike the sentiment treebank, there is no annotated tree for each sentence. Instead, we obtain an automatically parsed tree for each sentence using ZPar4 off-the-shelf (Zhang and Clark, 2011). Another difference between the TREC data and the sentiment treebank is that there is only one label, at the root node, rather than a label for each phrase. 7.2 We consider two models for our baselines. The first is bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013). Our bidirectional constituency Tree LSTM (BiConTree) is compared against BiLSTM to investigate the effectiveness of tree structures. For the sentiment task, following Tai et al. (2015) and Li et al. (2015), we convert the treebank into sequences to allow the bidirectional LSTM model to make use of"
Q17-1012,N16-1035,0,0.0547494,"Missing"
Q17-1029,P16-1231,0,0.135819,"Missing"
Q17-1029,D16-1211,0,0.0541837,"Missing"
Q17-1029,P05-1022,0,0.565732,"Missing"
Q17-1029,A00-2018,0,0.793843,"“The little boy” is first read, and then an NP recognized for the word sequence. After the system reads the verb “likes” and its subsequent NP, a VP is recognized. The full order of recognition for the tree nodes is 3 4 5 2 7 9 10 8 6 11 → → → → → → → → → → 1 When making local decisions, rich information . is available from readily built partial trees (Zhu et al., 2013; Watanabe and Sumita, 2015; Cross and Huang, 2016), which contributes to local disambiguation. However, there is lack of top-down guidance from lookahead information, which can be useful (Johnson, 1998; Roark and Johnson, 1999; Charniak, 2000; Liu and Zhang, 2017). In addition, binarization must be applied to trees, as shown in Figure 1(b), to ensure a constant number of actions (Sagae and Lavie, 2005), and to take advantage of lexical head information (Collins, 2003). However, such binarization requires a set of language-specific rules, which hampers adaptation of parsing to other languages. On the other hand, the process of top-down parsing can be regarded as pre-order traversal over a tree. Given the sentence in Figure 1, a top-down 1 The action sequence is taken on unbinarized trees. 413 Transactions of the Association for Com"
Q17-1029,D14-1082,0,0.112834,"ohnson (1999) apply the strategy into parsing. Typical works investigate the transformation of syntactic trees based on left-corner rules (Roark, 2001; Schuler et al., 2010; Van Schijndel and Schuler, 2013). In contrast, we propose a novel general transition-based in-order constituent parsing system. Neural networks have achieved the state-of-the422 art for parsing under various grammar formalisms, including dependency (Dozat and Manning, 2017), constituent (Dyer et al., 2016; Kuncoro et al., 2017), and CCG parsing (Xu, 2016; Lewis et al., 2016). Seminal work employs transition-based methods (Chen and Manning, 2014). This method has been extended by investigating more complex representations of configurations for constituent parsing (Watanabe and Sumita, 2015; Dyer et al., 2016). Dyer et al. (2016) employ stack-LSTM onto the top-down system, which is the same as our topdown parser. Watanabe and Sumita (2015) employ tree-LSTM to model the complex representation in the stack in bottom-up system. We are the first to investigate in-order traversal by designing a novel transition-based system under the same neural structure model framework. 8 Conclusion We proposed a novel psycho-linguistically motivated cons"
Q17-1029,D16-1238,0,0.0613331,"Missing"
Q17-1029,D16-1257,0,0.450779,"e pretrained English word embeddings generated on the AFP portion of English Gigaword. For Chinese data, we use Version 5.1 of the Penn Chinese Treebank (CTB) (Xue et al., 2005). We use articles 001- 270 and 440-1151 for training, articles 301-325 for system development, and articles 271300 for final performance evaluation. We adopt the pretrained Chinese word embeddings generated on the complete Chinese Gigaword corpus. The POS tags in both the English data and the Chinese data are automatically assigned the same as the work of Dyer et al. (2016), using Stanford tagger. We follow the work of Choe and Charniak (2016) and adopt the AFP portion of the English Gigaword as the extra resources for the semi-supervised reranking. 5.2 λ ||θ||2 , 2 Value 2 32 100 80 12 16 128 128 Settings Hyper-parameters For both English and Chinese experiments, we use the same hyper-parameters as the work of Dyer et al. (2016) without further optimization, as shown in Table 1. Reranking experiments Following the same reranking setting of Dyer et al. (2016) and Choe and Charniak (2016), we obtain 100 samples from our bottom-up, top-down, and in-order model (SecModel Top-down parser Bottom-up parser In-order parser LR 91.59 91.89"
Q17-1029,J03-4003,0,0.31424,"8 6 11 → → → → → → → → → → 1 When making local decisions, rich information . is available from readily built partial trees (Zhu et al., 2013; Watanabe and Sumita, 2015; Cross and Huang, 2016), which contributes to local disambiguation. However, there is lack of top-down guidance from lookahead information, which can be useful (Johnson, 1998; Roark and Johnson, 1999; Charniak, 2000; Liu and Zhang, 2017). In addition, binarization must be applied to trees, as shown in Figure 1(b), to ensure a constant number of actions (Sagae and Lavie, 2005), and to take advantage of lexical head information (Collins, 2003). However, such binarization requires a set of language-specific rules, which hampers adaptation of parsing to other languages. On the other hand, the process of top-down parsing can be regarded as pre-order traversal over a tree. Given the sentence in Figure 1, a top-down 1 The action sequence is taken on unbinarized trees. 413 Transactions of the Association for Computational Linguistics, vol. 5, pp. 413–424, 2017. Action Editor: Brian Roark. Submission batch: 6/2017; Published 11/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. stack [] [The] [T"
Q17-1029,D16-1001,0,0.663899,"r traversal over a constituent tree. For example, given the sentence in Figure 1, a bottom-up shift-reduce parser takes the action sequence in Table 2(a)1 to build the output, where the word sequence “The little boy” is first read, and then an NP recognized for the word sequence. After the system reads the verb “likes” and its subsequent NP, a VP is recognized. The full order of recognition for the tree nodes is 3 4 5 2 7 9 10 8 6 11 → → → → → → → → → → 1 When making local decisions, rich information . is available from readily built partial trees (Zhu et al., 2013; Watanabe and Sumita, 2015; Cross and Huang, 2016), which contributes to local disambiguation. However, there is lack of top-down guidance from lookahead information, which can be useful (Johnson, 1998; Roark and Johnson, 1999; Charniak, 2000; Liu and Zhang, 2017). In addition, binarization must be applied to trees, as shown in Figure 1(b), to ensure a constant number of actions (Sagae and Lavie, 2005), and to take advantage of lexical head information (Collins, 2003). However, such binarization requires a set of language-specific rules, which hampers adaptation of parsing to other languages. On the other hand, the process of top-down parsing"
Q17-1029,P15-1033,0,0.0197281,"little boy a1 xj-2 xj-1 buffer xj NP The little boy NP (a) Unbinarized composition action a0 NP-r* little boy Figure 4: parsers. Framework of our transition-based NP boy little NP (b) Binarized composition 4 Neural parsing model We employ the stack-LSTM parsing model of Dyer et al. (2016) for the three types of transition-based parsing systems in Section 2.1, 2.2 and 3, respectively, where a stack-LSTM is used to represent the stack, a stack-LSTM is used to represent the buffer, and a vanilla LSTM is used to represent the action history, as shown in Figure 4. 4.1 Word representation We follow Dyer et al. (2015), representing each word using three different types of embeddings, including pretrained word embedding, ewi , which is not fine-tuned during the training of the parser, randomly initialized embeddings ewi , which is finetuned, and the randomly initialized part-of-speech embeddings, which is fine-tuned. The three embeddings are concatenated, and then fed to nonlinear layer to derive the final word embedding: xi = f (Winput [epi ; ewi ; ewi ] + binput ), where Winput and binput are model parameters, wi and pi denote the form and the POS tag of the ith input word, respectively, and f is an nonli"
Q17-1029,N16-1024,0,0.169685,"guistically motivated constituent parsing system achieves 91.8 F1 on the WSJ benchmark. Furthermore, the system achieves 93.6 F1 with supervised reranking and 94.2 F1 with semi-supervised reranking, which are the best results on the WSJ benchmark. 1 Introduction Transition-based constituent parsing employs sequences of local transition actions to construct constituent trees over sentences. There are two popular transition-based constituent parsing systems, namely bottom-up parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al., 2016; Kuncoro et al., 2017). The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree. The process of bottom-up parsing can be regarded as post-order traversal over a constituent tree. For example, given the sentence in Figure 1, a bottom-up shift-reduce parser takes the action sequence in Table 2(a)1 to build the output, where the word sequence “The little boy” is first read, and then an NP recognized for the word sequence. After the system reads the verb “likes” and its subsequent NP, a VP is recognized. The full order of recognition for the"
Q17-1029,J98-4004,0,0.243051,"Missing"
Q17-1029,Q16-1023,0,0.100143,"Missing"
Q17-1029,E17-1117,0,0.383413,"ed constituent parsing system achieves 91.8 F1 on the WSJ benchmark. Furthermore, the system achieves 93.6 F1 with supervised reranking and 94.2 F1 with semi-supervised reranking, which are the best results on the WSJ benchmark. 1 Introduction Transition-based constituent parsing employs sequences of local transition actions to construct constituent trees over sentences. There are two popular transition-based constituent parsing systems, namely bottom-up parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al., 2016; Kuncoro et al., 2017). The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree. The process of bottom-up parsing can be regarded as post-order traversal over a constituent tree. For example, given the sentence in Figure 1, a bottom-up shift-reduce parser takes the action sequence in Table 2(a)1 to build the output, where the word sequence “The little boy” is first read, and then an NP recognized for the word sequence. After the system reads the verb “likes” and its subsequent NP, a VP is recognized. The full order of recognition for the tree nodes is 3 4 5 2 7"
Q17-1029,Q17-1004,1,0.730844,"is first read, and then an NP recognized for the word sequence. After the system reads the verb “likes” and its subsequent NP, a VP is recognized. The full order of recognition for the tree nodes is 3 4 5 2 7 9 10 8 6 11 → → → → → → → → → → 1 When making local decisions, rich information . is available from readily built partial trees (Zhu et al., 2013; Watanabe and Sumita, 2015; Cross and Huang, 2016), which contributes to local disambiguation. However, there is lack of top-down guidance from lookahead information, which can be useful (Johnson, 1998; Roark and Johnson, 1999; Charniak, 2000; Liu and Zhang, 2017). In addition, binarization must be applied to trees, as shown in Figure 1(b), to ensure a constant number of actions (Sagae and Lavie, 2005), and to take advantage of lexical head information (Collins, 2003). However, such binarization requires a set of language-specific rules, which hampers adaptation of parsing to other languages. On the other hand, the process of top-down parsing can be regarded as pre-order traversal over a tree. Given the sentence in Figure 1, a top-down 1 The action sequence is taken on unbinarized trees. 413 Transactions of the Association for Computational Linguistics"
Q17-1029,J93-2004,0,0.0604454,"l2 regularization term, defined by L(θ) = − XX i j log paij + Table 1: Hyper-parameters. 5 5.1 where θ is the set of parameters, paij is the probability of the jth action in the ith training example given by the model and λ is a regularization hyperparameter (λ = 10−6 ). We use stochastic gradient descent with a 0.1 initialized learning rate with a 0.05 learning rate decay. 418 Experiments Data We empirically compare our bottom-up, top-down and in-order parsers. The experiments are carried out on both English and Chinese. For English data, we use the standard benchmark of WSJ sections in PTB (Marcus et al., 1993), where the Sections 221 are taken for training data, Section 22 for development data and Section 23 for testing both dependency parsing and constituency parsing. We adopt the pretrained English word embeddings generated on the AFP portion of English Gigaword. For Chinese data, we use Version 5.1 of the Penn Chinese Treebank (CTB) (Xue et al., 2005). We use articles 001- 270 and 440-1151 for training, articles 301-325 for system development, and articles 271300 for final performance evaluation. We adopt the pretrained Chinese word embeddings generated on the complete Chinese Gigaword corpus. T"
Q17-1029,P99-1054,0,0.410596,"where the word sequence “The little boy” is first read, and then an NP recognized for the word sequence. After the system reads the verb “likes” and its subsequent NP, a VP is recognized. The full order of recognition for the tree nodes is 3 4 5 2 7 9 10 8 6 11 → → → → → → → → → → 1 When making local decisions, rich information . is available from readily built partial trees (Zhu et al., 2013; Watanabe and Sumita, 2015; Cross and Huang, 2016), which contributes to local disambiguation. However, there is lack of top-down guidance from lookahead information, which can be useful (Johnson, 1998; Roark and Johnson, 1999; Charniak, 2000; Liu and Zhang, 2017). In addition, binarization must be applied to trees, as shown in Figure 1(b), to ensure a constant number of actions (Sagae and Lavie, 2005), and to take advantage of lexical head information (Collins, 2003). However, such binarization requires a set of language-specific rules, which hampers adaptation of parsing to other languages. On the other hand, the process of top-down parsing can be regarded as pre-order traversal over a tree. Given the sentence in Figure 1, a top-down 1 The action sequence is taken on unbinarized trees. 413 Transactions of the Ass"
Q17-1029,D09-1034,0,0.067107,"Missing"
Q17-1029,P80-1024,0,0.805105,"S, marked in red, at a early stage, leaving no choice but to follow this incorrect non-terminal. The bottom-up parser without lookahead information makes incorrect local decisions. By contrast, the in-order parser reads the word “and” and projects a non-terminal S for coordination after completing “(S investors uneasy)”. On the other hand, the inorder parser is confused by projecting for the word “made” or the word “both” into an VP, which we think could be addressed by using a in-order system variant with k=2 described in Section 3. 7 Related work Our work is related to left corner parsing. Rosenkrantz and Lewis (1970) formalize this in automata theory, which have appeared frequently in the compiler literature. Roark and Johnson (1999) apply the strategy into parsing. Typical works investigate the transformation of syntactic trees based on left-corner rules (Roark, 2001; Schuler et al., 2010; Van Schijndel and Schuler, 2013). In contrast, we propose a novel general transition-based in-order constituent parsing system. Neural networks have achieved the state-of-the422 art for parsing under various grammar formalisms, including dependency (Dozat and Manning, 2017), constituent (Dyer et al., 2016; Kuncoro et a"
Q17-1029,W05-1513,0,0.917895,"between bottom-up constituent information and top-down lookahead information. Based on stack-LSTM, our psycholinguistically motivated constituent parsing system achieves 91.8 F1 on the WSJ benchmark. Furthermore, the system achieves 93.6 F1 with supervised reranking and 94.2 F1 with semi-supervised reranking, which are the best results on the WSJ benchmark. 1 Introduction Transition-based constituent parsing employs sequences of local transition actions to construct constituent trees over sentences. There are two popular transition-based constituent parsing systems, namely bottom-up parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al., 2016; Kuncoro et al., 2017). The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree. The process of bottom-up parsing can be regarded as post-order traversal over a constituent tree. For example, given the sentence in Figure 1, a bottom-up shift-reduce parser takes the action sequence in Table 2(a)1 to build the output, where the word sequence “The little boy” is first read, and then an NP recognized for the word sequence. After the sy"
Q17-1029,J10-1001,0,0.0308028,"er completing “(S investors uneasy)”. On the other hand, the inorder parser is confused by projecting for the word “made” or the word “both” into an VP, which we think could be addressed by using a in-order system variant with k=2 described in Section 3. 7 Related work Our work is related to left corner parsing. Rosenkrantz and Lewis (1970) formalize this in automata theory, which have appeared frequently in the compiler literature. Roark and Johnson (1999) apply the strategy into parsing. Typical works investigate the transformation of syntactic trees based on left-corner rules (Roark, 2001; Schuler et al., 2010; Van Schijndel and Schuler, 2013). In contrast, we propose a novel general transition-based in-order constituent parsing system. Neural networks have achieved the state-of-the422 art for parsing under various grammar formalisms, including dependency (Dozat and Manning, 2017), constituent (Dyer et al., 2016; Kuncoro et al., 2017), and CCG parsing (Xu, 2016; Lewis et al., 2016). Seminal work employs transition-based methods (Chen and Manning, 2014). This method has been extended by investigating more complex representations of configurations for constituent parsing (Watanabe and Sumita, 2015; D"
Q17-1029,P12-1046,0,0.1343,"e top-down system. The inorder system outperforms both the bottom-up and the top-down system. 5.4 Results Table 3 shows the parsing results on the English test dataset. We find that the bottom-up parser and the top-down parser have similar results under the greedy setting, and the in-order parser outperforms both of them. Also, with supervised reranking, the in-order parser achieves the best results. English constituent results We compare our models with previous work, as shown in Table 4. With the fully-supervise setting5 , the inorder parser outperforms the state-of-the-art discrete parser (Shindo et al., 2012; Zhu et al., 2013), the state-of-the-art neural parsers (Cross and Huang, 5 Here, we only consider the work of a single model. 419 Model fully-supervise Socher et al. (2013) Zhu et al. (2013) Vinyals et al. (2015) Watanabe and Sumita (2015) Shindo et al. (2012) Durrett and Klein (2015) Dyer et al. (2016) Cross and Huang (2016) Liu and Zhang (2017) Top-down parser Bottom-up parser In-order parser reranking Huang (2008) Charniak and Johnson (2005) Choe and Charniak (2016) Dyer et al. (2016) Kuncoro et al. (2017) Top-down parser Bottom-up parser In-order parser semi-supervised reranking Choe and"
Q17-1029,P13-1045,0,0.178148,"Missing"
Q17-1029,N13-1010,0,0.0275321,"Missing"
Q17-1029,P14-1069,0,0.0972638,"Missing"
Q17-1029,P15-1110,0,0.529716,"Missing"
Q17-1029,P15-1113,0,0.32937,"d information. Based on stack-LSTM, our psycholinguistically motivated constituent parsing system achieves 91.8 F1 on the WSJ benchmark. Furthermore, the system achieves 93.6 F1 with supervised reranking and 94.2 F1 with semi-supervised reranking, which are the best results on the WSJ benchmark. 1 Introduction Transition-based constituent parsing employs sequences of local transition actions to construct constituent trees over sentences. There are two popular transition-based constituent parsing systems, namely bottom-up parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al., 2016; Kuncoro et al., 2017). The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree. The process of bottom-up parsing can be regarded as post-order traversal over a constituent tree. For example, given the sentence in Figure 1, a bottom-up shift-reduce parser takes the action sequence in Table 2(a)1 to build the output, where the word sequence “The little boy” is first read, and then an NP recognized for the word sequence. After the system reads the verb “likes” and its subsequent NP, a VP is recognized"
Q17-1029,W09-3825,1,0.867101,"tituent information and top-down lookahead information. Based on stack-LSTM, our psycholinguistically motivated constituent parsing system achieves 91.8 F1 on the WSJ benchmark. Furthermore, the system achieves 93.6 F1 with supervised reranking and 94.2 F1 with semi-supervised reranking, which are the best results on the WSJ benchmark. 1 Introduction Transition-based constituent parsing employs sequences of local transition actions to construct constituent trees over sentences. There are two popular transition-based constituent parsing systems, namely bottom-up parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al., 2016; Kuncoro et al., 2017). The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree. The process of bottom-up parsing can be regarded as post-order traversal over a constituent tree. For example, given the sentence in Figure 1, a bottom-up shift-reduce parser takes the action sequence in Table 2(a)1 to build the output, where the word sequence “The little boy” is first read, and then an NP recognized for the word sequence. After the system reads the verb “li"
Q17-1029,P13-1043,1,0.949326,"top-down lookahead information. Based on stack-LSTM, our psycholinguistically motivated constituent parsing system achieves 91.8 F1 on the WSJ benchmark. Furthermore, the system achieves 93.6 F1 with supervised reranking and 94.2 F1 with semi-supervised reranking, which are the best results on the WSJ benchmark. 1 Introduction Transition-based constituent parsing employs sequences of local transition actions to construct constituent trees over sentences. There are two popular transition-based constituent parsing systems, namely bottom-up parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015) and top-down parsing (Dyer et al., 2016; Kuncoro et al., 2017). The parsing strategies differ in terms of the order in which they recognize productions in the derivation tree. The process of bottom-up parsing can be regarded as post-order traversal over a constituent tree. For example, given the sentence in Figure 1, a bottom-up shift-reduce parser takes the action sequence in Table 2(a)1 to build the output, where the word sequence “The little boy” is first read, and then an NP recognized for the word sequence. After the system reads the verb “likes” and its subse"
Q17-1029,P15-1030,0,\N,Missing
Q17-1029,P08-1067,0,\N,Missing
Q17-1029,N16-1026,0,\N,Missing
Q17-1029,D16-1181,0,\N,Missing
Q19-1002,P17-2021,0,0.0274704,"ledge can significantly improve a strong attention-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association"
Q19-1002,D15-1198,0,0.0207633,"MRs capture more relations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experim"
Q19-1002,J12-2006,0,0.0211864,"en training data are not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling,"
Q19-1002,W13-2322,0,0.484662,"ow@gmail.com 4 jssu@xmu.edu.cn Abstract from SRL can improve the performance of an attention-based sequence-to-sequence model by alleviating the ‘‘argument switching’’ problem,1 one frequent and severe issue faced by NMT systems (Isabelle et al., 2017). Figure 1(a) shows one example of semantic role information, which only captures the relations between a predicate (gave) and its arguments (John, wife, and present). Other important information, such as the relation between John and wife, cannot be incorporated. In this paper, we explore the usefulness of abstract meaning representation (AMR) (Banarescu et al., 2013) as a semantic representation for NMT. AMR is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1(b) shows an AMR graph, in which the nodes (such as give-01 and John) represent the concepts and edges (such as :ARG0 and :ARG1) represent the relations between concepts they connect. Comparing with semantic roles, AMRs capture more relations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they"
Q19-1002,S16-1186,0,0.244245,"ohn and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorpo"
Q19-1002,D17-1209,0,0.0409234,"eto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4."
Q19-1002,P14-1134,0,0.0836165,"with semantic roles, AMRs capture more relations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representat"
Q19-1002,P18-1026,0,0.190312,"se layer: − ← − → s 0 = W 1 [ h 0 ; h N ] + b1 , and Transformer (Vaswani et al., 2017) on classification and sequence labeling tasks; Song et al. (2018) build a GRN for encoding AMR graphs for text generation, showing that the representation is superior compared to BiLSTM on serialized AMR. We extend Song et al. (2018) by investigating the usefulness of AMR for neural machine translation. To our knowledge, we are the first to use GRN for machine translation. In addition to GRNs and GCNs, there have been other graph neural networks, such as graph gated neural network (GGNN) (Li et al., 2015b; Beck et al., 2018). Because our main concern is to empirically investigate the effectiveness of AMR for NMT, we leave it to future work to compare GCN, GGNN, and GRN for our task. 3 where W 1 and b1 are model parameters. For each decoding step m, the decoder feeds the concatenation of the embedding of the current input eym and the previous context vector ζ m−1 into the LSTM model to update its hidden state: Baseline: Attention-Based BiLSTM sm = LSTM(sm−1 , [eym ; ζ m−1 ]). We take the attention-based sequence-to-sequence model of Bahdanau et al. (2015) as the baseline, but use LSTM cells (Hochreiter and Schmidh"
Q19-1002,P18-1170,0,0.0223019,"2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 2018) as the AMR encoder. I"
Q19-1002,P17-1112,0,0.0195771,"ed by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional"
Q19-1002,D18-1198,0,0.0365651,") for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 2018) as the AMR encoder. In particular, a ful"
Q19-1002,P17-1177,0,0.0237591,"ntion-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distr"
Q19-1002,P82-1020,0,0.739452,"Missing"
Q19-1002,D17-1263,0,0.0258882,"feng Song,1 Daniel Gildea,1 Yue Zhang,2 Zhiguo Wang,3 and Jinsong Su4 1 Department of Computer Science, University of Rochester, Rochester, NY 14627 2 School of Engineering, Westlake University, China 3 IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 4 Xiamen University, Xiamen, China 1 {lsong10,gildea}@cs.rochester.edu 2 yue.zhang@wias.org.cn 3 zgw.tomorrow@gmail.com 4 jssu@xmu.edu.cn Abstract from SRL can improve the performance of an attention-based sequence-to-sequence model by alleviating the ‘‘argument switching’’ problem,1 one frequent and severe issue faced by NMT systems (Isabelle et al., 2017). Figure 1(a) shows one example of semantic role information, which only captures the relations between a predicate (gave) and its arguments (John, wife, and present). Other important information, such as the relation between John and wife, cannot be incorporated. In this paper, we explore the usefulness of abstract meaning representation (AMR) (Banarescu et al., 2013) as a semantic representation for NMT. AMR is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1(b) shows an AMR graph, in which the nodes (such as give-01 and John) represent the co"
Q19-1002,D14-1179,0,0.0368049,"Missing"
Q19-1002,C12-1083,0,0.0622695,"not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling, 2017), which are laid"
Q19-1002,W14-3348,0,0.0650882,"Missing"
Q19-1002,P14-5010,0,0.00592251,"Missing"
Q19-1002,P17-4012,0,0.114376,"Missing"
Q19-1002,N18-2078,0,0.264809,"s to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. cantly improves a strong attention-based sequenceto-sequence baseline (25.5 vs 23.7 BLEU). When trained with small-scale (226K) data, the improvement increases"
Q19-1002,W04-3250,0,0.44934,"Missing"
Q19-1002,P17-1014,0,0.345035,"n dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional knowledge can signifi"
Q19-1002,P02-1040,0,0.103866,"Missing"
Q19-1002,P17-1064,0,0.0168608,"ove a strong attention-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational"
Q19-1002,K15-1004,1,0.859168,"relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman da"
Q19-1002,W15-4502,0,0.34139,", which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 2018) as the AMR encoder. In particular, a full AMR graph is considered as a single state, with nodes in the graph being its substates. State transitions are performed on the graph recurrently, allowing substates to exchange information through edges. At each r"
Q19-1002,P18-1171,1,0.850218,"a GRN (Song et al., 2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 20"
Q19-1002,C10-1081,1,0.785947,"viate data sparsity when training data are not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layer"
Q19-1002,D15-1136,0,0.0408605,"ations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard"
Q19-1002,P18-1037,0,0.181496,"y capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional knowledge can significantly improve a strong attention-based s"
Q19-1002,P16-1162,0,0.0975416,"ns around 4.5 million sentence pairs for training. In addition, we use a subset of the full dataset (News Commentary v11 [NC-v11], containing around 243,000 sentence pairs) for development and additional experiments. For all experiments, we use newstest2013 and newstest2016 as the development and test sets, respectively. To preprocess the data, the tokenizer from Moses4 is used to tokenize both the English and German sides. The training sentence pairs where either side is longer than 50 words are filtered out after tokenization. To deal with rare and compound words, byte-pair encoding (BPE)5 (Sennrich et al., 2016) is applied to both sides. In particular, 8,000 and 16,000 BPE merges are used on the News Commentary v11 subset and the full training set, respectively. On the other hand, JAMR6 (Flanigan et al., 2016) is adopted to parse the English sentences into AMRs before BPE is applied. The statistics of the training data and vocabularies after preprocessing are shown in Tables 1 and 2, respectively. For the experiments with the full training set, we used the top 40K where el and ei are the embeddings of edge label l and source node vi , and W 4 and b4 are model parameters. 4.2 Training Incorporating AM"
Q19-1002,2006.amta-papers.25,0,0.0803469,"Missing"
Q19-1002,D17-1129,0,0.0841465,"dition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional knowledge can significantly improve a str"
Q19-1002,N06-1056,0,0.151113,"Missing"
Q19-1002,P18-1150,1,0.928432,"ir graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling, 2017), which are laid on top of regular BiRNN or CNN layers. Our work is in line with exploring semantic information, but different in exploiting AMR rather than SRL for NMT. In addition, we leverage a GRN (Song et al., 2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Grosc"
Q19-1002,N09-2004,0,0.0443451,"n from AMR can alleviate data sparsity when training data are not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolution"
Q19-1002,P16-2049,0,0.0266319,"g AMR as additional knowledge can significantly improve a strong attention-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published"
Q19-1002,D16-1112,0,0.0991011,"Missing"
Q19-1002,P18-1030,1,0.718086,"g representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling, 2017), which are laid on top of regular BiRNN or CNN layers. Our work is in line with exploring semantic information, but different in exploiting AMR rather than SRL for NMT. In addition, we leverage a GRN (Song et al., 2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; G"
W09-3825,W00-1201,0,0.0810676,"Missing"
W09-3825,P05-1012,0,0.0755544,"ce. One advantage of this approach is that the parsing can be highly efficient, for example by pursuing a greedy strategy in which a single action is chosen at each decision point. The alternative approach, exemplified by Collins (1997) and Charniak (2000), is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar Stephen Clark Cambridge University Computer Laboratory stephen.clark@cl.cam.ac.uk results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008). For constituent-based parsing using the Chinese Treebank (CTB), Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this pape"
W09-3825,W06-2933,0,0.0384175,"Missing"
W09-3825,J93-1002,0,0.11149,"shift-reduce parsing model on parsing the Chinese Treebank 2 data (Wang et al., 2006). In this paper, we propose a global discriminative model based on the shift-reduce parsing process, combined with a beam-search decoder, obtaining competitive accuracies on CTB2. We also report the performance of the parser on CTB5 data, obtaining the highest scores in the literature for a dependencybased evaluation. 1 Introduction Transition-based statistical parsing associates scores with each decision in the parsing process, selecting the parse which is built by the highest scoring sequence of decisions (Briscoe and Carroll, 1993; Nivre et al., 2006). The parsing algorithm is typically some form of bottom-up shiftreduce algorithm, so that scores are associated with actions such as shift and reduce. One advantage of this approach is that the parsing can be highly efficient, for example by pursuing a greedy strategy in which a single action is chosen at each decision point. The alternative approach, exemplified by Collins (1997) and Charniak (2000), is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest s"
W09-3825,N07-1051,0,0.257214,"Missing"
W09-3825,W08-2102,0,0.0334691,". The main difference is that our parser in this paper is for constituent parsing. In fact, our parser is one of only a few constituent parsers which have successfully applied global discriminative models, certainly without a generative baseline as a feature, whereas global models for dependency parsing have been comparatively easier to develop. 7 Conclusion The contributions of this paper can be summarized as follows. First, we defined a global discriminative model for Chinese constituent-based parsing, continuing recent work in this area which has focused on English (Clark and Curran, 2007; Carreras et al., 2008; Finkel et al., 2008). Second, we showed how such a model can be applied to shiftreduce parsing and combined with beam search, resulting in an accurate linear-time parser. In standard tests using CTB2 data, our parser achieved comparable Parseval F-score to the state-of-theart systems. Moreover, we observed that more training data lead to improvements on both accuracy and stability against feature variations, and reported performance of the parser using CTB5 data. By converting constituent-based output to dependency relations using standard head-finding rules, our parser also obtained the hig"
W09-3825,A00-2018,0,0.0368523,"ed statistical parsing associates scores with each decision in the parsing process, selecting the parse which is built by the highest scoring sequence of decisions (Briscoe and Carroll, 1993; Nivre et al., 2006). The parsing algorithm is typically some form of bottom-up shiftreduce algorithm, so that scores are associated with actions such as shift and reduce. One advantage of this approach is that the parsing can be highly efficient, for example by pursuing a greedy strategy in which a single action is chosen at each decision point. The alternative approach, exemplified by Collins (1997) and Charniak (2000), is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar Stephen Clark Cambridge University Computer Laboratory stephen.clark@cl.cam.ac.uk results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chinese, the best depend"
W09-3825,C02-1126,0,0.0345237,"Missing"
W09-3825,J07-4004,1,0.534681,"ve model and beam search. The main difference is that our parser in this paper is for constituent parsing. In fact, our parser is one of only a few constituent parsers which have successfully applied global discriminative models, certainly without a generative baseline as a feature, whereas global models for dependency parsing have been comparatively easier to develop. 7 Conclusion The contributions of this paper can be summarized as follows. First, we defined a global discriminative model for Chinese constituent-based parsing, continuing recent work in this area which has focused on English (Clark and Curran, 2007; Carreras et al., 2008; Finkel et al., 2008). Second, we showed how such a model can be applied to shiftreduce parsing and combined with beam search, resulting in an accurate linear-time parser. In standard tests using CTB2 data, our parser achieved comparable Parseval F-score to the state-of-theart systems. Moreover, we observed that more training data lead to improvements on both accuracy and stability against feature variations, and reported performance of the parser using CTB5 data. By converting constituent-based output to dependency relations using standard head-finding rules, our parse"
W09-3825,P04-1015,0,0.743677,"ses the current model to decode training examples (the parse function in the pseudo-code). If the output is correct, it passes on to the next example. If the output is incorrect, it adjusts the weight values by adding the feature vector from the goldstandard output and subtracting the feature vector from the parser output. Weight values are updated for each example (making the process online) and the training data is iterated over T times. In order to avoid overfitting we used the now-standard averaged version of this algorithm (Collins, 2002). We also apply the early update modification from Collins and Roark (2004). If the agenda, at any point during the decoding process, does not contain the correct partial parse, it is not possible for the decoder to produce the correct output. In this case, decoding is stopped early and the weight values are updated using the highest scoring partial parse on the agenda. 4.1 Feature set Table 1 shows the set of feature templates for the model. Individual features are generated from 165 Description Unigrams Feature templates S0 tc, S0 wc, S1 tc, S1 wc, S2 tc, S2 wc, S3 tc, S3 wc, N0 wt, N1 wt, N2 wt, N3 wt, S0 lwc, S0 rwc, S0 uwc, S1 lwc, S1 rwc, S1 uwc, Bigrams S0 wS1"
W09-3825,P97-1003,0,0.0181104,"tion Transition-based statistical parsing associates scores with each decision in the parsing process, selecting the parse which is built by the highest scoring sequence of decisions (Briscoe and Carroll, 1993; Nivre et al., 2006). The parsing algorithm is typically some form of bottom-up shiftreduce algorithm, so that scores are associated with actions such as shift and reduce. One advantage of this approach is that the parsing can be highly efficient, for example by pursuing a greedy strategy in which a single action is chosen at each decision point. The alternative approach, exemplified by Collins (1997) and Charniak (2000), is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar Stephen Clark Cambridge University Computer Laboratory stephen.clark@cl.cam.ac.uk results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chin"
W09-3825,W05-1513,0,0.139096,"ative approach, exemplified by Collins (1997) and Charniak (2000), is to use a chart-based algorithm to build the space of possible parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar Stephen Clark Cambridge University Computer Laboratory stephen.clark@cl.cam.ac.uk results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008). For constituent-based parsing using the Chinese Treebank (CTB), Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.’s approach. We apply the same shift-reduce procedure as Wang et al. (2006), but in"
W09-3825,W03-1706,0,0.0223096,"re vector. q represents the count of any separator punctuation between S0 and S1 . Whenever an action is being considered at each point in the beam-search process, templates from Table 1 are matched with the context defined by the parser state and combined with the action to generate features. Negative features, which are the features from incorrect parser outputs but not from any training example, are included in the model. There are around a million features in our experiments with the CTB2 dataset. Wang et al. (2006) used a range of other features, including rhythmic features of S0 and S1 (Sun and Jurafsky, 2003), features from the most recently found node that is to the left or right of S0 and S1 , the number of words and the number of punctuations in S0 and S1 , the distance between S0 and S1 and so on. We did not include these features in our parser, because they did not lead to improved performance during development experiments. 5 Experiments The experiments were performed using the Chinese Treebank 2 and Chinese Treebank 5 data. Standard data preparation was performed before the experiments: empty terminal nodes were removed; any non-terminal nodes with no children were removed; any unary X → X"
W09-3825,P06-1054,0,0.106074,"m to find the highest scoring parse. For English dependency parsing, the two approaches give similar Stephen Clark Cambridge University Computer Laboratory stephen.clark@cl.cam.ac.uk results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008). For constituent-based parsing using the Chinese Treebank (CTB), Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.’s approach. We apply the same shift-reduce procedure as Wang et al. (2006), but instead of using a local classifier for each transition-based action, we train a generalized perceptron model over complete sequences of actions, so that the parameters are learned in the context of complete parses."
W09-3825,I05-1007,0,0.0525411,"Missing"
W09-3825,D08-1059,1,0.770866,"le parses, together with pruning of lowprobability constituents and the Viterbi algorithm to find the highest scoring parse. For English dependency parsing, the two approaches give similar Stephen Clark Cambridge University Computer Laboratory stephen.clark@cl.cam.ac.uk results (McDonald et al., 2005; Nivre et al., 2006). For English constituent-based parsing using the Penn Treebank, the best performing transitionbased parser lags behind the current state-of-theart (Sagae and Lavie, 2005). In contrast, for Chinese, the best dependency parsers are currently transition-based (Duan et al., 2007; Zhang and Clark, 2008). For constituent-based parsing using the Chinese Treebank (CTB), Wang et al. (2006) have shown that a shift-reduce parser can give competitive accuracy scores together with high speeds, by using an SVM to make a single decision at each point in the parsing process. In this paper we describe a global discriminative model for Chinese shift-reduce parsing, and compare it with Wang et al.’s approach. We apply the same shift-reduce procedure as Wang et al. (2006), but instead of using a local classifier for each transition-based action, we train a generalized perceptron model over complete sequenc"
W09-3825,W02-1001,0,0.239499,"Outputs: w ~ Figure 4: the perceptron learning algorithm for state item Y is defined by: Score(Y ) = w ~ · Φ(Y ) = X λi fi (Y ) i where Φ(Y ) is the global feature vector from Y , and w ~ is the weight vector defined by the model. Each element from Φ(Y ) represents the global count of a particular feature from Y . The feature set consists of a large number of features which pick out various configurations from the stack and queue, based on the words and subtrees in the state item. The features are described in Section 4.1. The weight values are set using the generalized perceptron algorithm (Collins, 2002). The perceptron algorithm is shown in Figure 4. It initializes weight values as all zeros, and uses the current model to decode training examples (the parse function in the pseudo-code). If the output is correct, it passes on to the next example. If the output is incorrect, it adjusts the weight values by adding the feature vector from the goldstandard output and subtracting the feature vector from the parser output. Weight values are updated for each example (making the process online) and the training data is iterated over T times. In order to avoid overfitting we used the now-standard aver"
W09-3825,P08-1109,0,0.0724614,"Missing"
W09-3825,P03-1056,0,0.0287834,"Missing"
W17-6315,N16-1024,0,0.360019,"ich uses a sequence of transition-actions to build trees. We empirically investigate the effectiveness of applying the encoder-decoder network to transition-based parsing. On standard benchmarks, our system gives comparable results to the stack LSTM parser for dependency parsing, and significantly better results compared to the aforementioned parser for constituent parsing, which uses bracketed tree formats. 1 Introduction Neural networks have achieved the state-of-theart for parsing under various grammar formalisms, including dependency grammar (Dozat and Manning, 2017), constituent grammar (Dyer et al., 2016) and CCG (Xu et al., 2016). For transitionbased parsing, Chen and Manning (2014) employed a feed-forward neural network with cube activation functions for local action modeling, archiving better results compared to MaltParser (Nivre et al., 2007). Subsequent work extend this method by investigating more complex representations of configurations (Dyer et al., 2015; Ballesteros et al., 2015) and global training with beam search (Zhou et al., 2015; Andor et al., 2016). Borrowing ideas from neural machine translation (NMT) (Bahdanau et al., 2015), a line of work utilizes a bidirectional RNN to en1"
W17-6315,Q17-1029,1,0.853938,"Missing"
W17-6315,P16-1231,0,0.0531642,"f-theart for parsing under various grammar formalisms, including dependency grammar (Dozat and Manning, 2017), constituent grammar (Dyer et al., 2016) and CCG (Xu et al., 2016). For transitionbased parsing, Chen and Manning (2014) employed a feed-forward neural network with cube activation functions for local action modeling, archiving better results compared to MaltParser (Nivre et al., 2007). Subsequent work extend this method by investigating more complex representations of configurations (Dyer et al., 2015; Ballesteros et al., 2015) and global training with beam search (Zhou et al., 2015; Andor et al., 2016). Borrowing ideas from neural machine translation (NMT) (Bahdanau et al., 2015), a line of work utilizes a bidirectional RNN to en105 Proceedings of the 15th International Conference on Parsing Technologies, pages 105–114, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics strings correspond to valid dependency trees. On the other hand, a more commonly used sequential representation of syntactic structures is the transition-action sequences in shift reduce parsers. For both constituent (Sagae and Lavie, 2005; Zhang and Clark, 2009) and dependency (Yamada and M"
W17-6315,Q17-1004,1,0.88452,"Missing"
W17-6315,D15-1166,0,0.0427506,"Missing"
W17-6315,D15-1041,0,0.0211191,"es bracketed tree formats. 1 Introduction Neural networks have achieved the state-of-theart for parsing under various grammar formalisms, including dependency grammar (Dozat and Manning, 2017), constituent grammar (Dyer et al., 2016) and CCG (Xu et al., 2016). For transitionbased parsing, Chen and Manning (2014) employed a feed-forward neural network with cube activation functions for local action modeling, archiving better results compared to MaltParser (Nivre et al., 2007). Subsequent work extend this method by investigating more complex representations of configurations (Dyer et al., 2015; Ballesteros et al., 2015) and global training with beam search (Zhou et al., 2015; Andor et al., 2016). Borrowing ideas from neural machine translation (NMT) (Bahdanau et al., 2015), a line of work utilizes a bidirectional RNN to en105 Proceedings of the 15th International Conference on Parsing Technologies, pages 105–114, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics strings correspond to valid dependency trees. On the other hand, a more commonly used sequential representation of syntactic structures is the transition-action sequences in shift reduce parsers. For both constituen"
W17-6315,J93-2004,0,0.0599967,"Missing"
W17-6315,W03-3017,0,0.334751,"Missing"
W17-6315,D14-1082,0,0.706614,"d Manning (2017) achieves the state-of-the-art results for dependency parsing. The success of the encoder structure can be attributed to the use of multilayer bidirectional LSTMs to induce non-local representations of sentences. Without manual feature engineering, such architecture automatically extracts complex features for syntactic representation. For neural machine translation, such encoder structure has been connected to a corresponding LSTM decoder, giving the state-of-the-art for sequenceto-sequence learning. Compared to carefully designed feature representations, such as the parser of Chen and Manning (2014) and the stack-LSTM structure of Dyer et al. (2015), the encoderdecoder structure is conceptually simpler, and more general, which can be used across different grammar formalisms without redesigning the stack representation. Vinyals et al. (2015) applied the encoder-decoder structure to constituent parsing, generating the bracketed syntactic trees as the output token sequence without model combination. However, their model achieves relatively low accuracies. The advantage of using a decoder LSTM is that it leverages a recurrent structure for capturing full sequence information in the output. U"
W17-6315,D16-1257,0,0.0202285,"n mechanism models is far beyond the manual feature templates, since words even on the Contrast with Vinyals et al. (2015) For constituent parsing, our models outperforms the parser of Vinyals et al. (2015) by differentiating stack and queue and generating transition actions instead. This shows the advantage of shiftreduce actions over bracketed syntactic trees as decoder outputs. Using the settings tuned on the dependency development data directly, our model achieves a F1-score of 90.5, which is comparable to the models of Zhu et al. (2013) and Socher et al. (2013). By using the rerankers of Choe and Charniak (2016) under the same settings, we obtain a 92.7 F1-score with fully-supervised reranking and a 93.4 F1-score with semi-supervised reranking. 111 stack (a) The Dow Jones Model queue industrials closed at 2569.26 . Shift (b) The Dow (c) industrials at closed . 2569.26 Left-Arc Jones (S NP (VP said most of their major institutional … Traders NT(SBAR) (d) … said (SBAR … (NP most (e) … (PP of (NP their major institutional of their major institutional investors … Reduce investors , on the … Shift dency parsing. Their success forms a strong motivation of our work. bottom of the stack can sometimes influen"
W17-6315,W05-1513,0,0.215067,"Missing"
W17-6315,P12-1046,0,0.0663416,"Missing"
W17-6315,P15-1030,0,0.0141567,"of Dyer et al. (2015), the encoderdecoder structure is conceptually simpler, and more general, which can be used across different grammar formalisms without redesigning the stack representation. Vinyals et al. (2015) applied the encoder-decoder structure to constituent parsing, generating the bracketed syntactic trees as the output token sequence without model combination. However, their model achieves relatively low accuracies. The advantage of using a decoder LSTM is that it leverages a recurrent structure for capturing full sequence information in the output. Unlike greedy or CRF decoders (Durrett and Klein, 2015), which capture only local label dependencies, LSTM decoder models global label sequence relations. Vinyals et al. (2015) use bracketed syntactic trees as the output token sequence, which requires strong constraints to ensure that the output Encoder-decoder neural networks have been used for many NLP tasks, such as neural machine translation. They have also been applied to constituent parsing by using bracketed tree structures as a target language, translating input sentences into syntactic trees. A more commonly used method to linearize syntactic trees is the shift-reduce system, which uses a"
W17-6315,P15-1033,0,0.107579,"or dependency parsing. The success of the encoder structure can be attributed to the use of multilayer bidirectional LSTMs to induce non-local representations of sentences. Without manual feature engineering, such architecture automatically extracts complex features for syntactic representation. For neural machine translation, such encoder structure has been connected to a corresponding LSTM decoder, giving the state-of-the-art for sequenceto-sequence learning. Compared to carefully designed feature representations, such as the parser of Chen and Manning (2014) and the stack-LSTM structure of Dyer et al. (2015), the encoderdecoder structure is conceptually simpler, and more general, which can be used across different grammar formalisms without redesigning the stack representation. Vinyals et al. (2015) applied the encoder-decoder structure to constituent parsing, generating the bracketed syntactic trees as the output token sequence without model combination. However, their model achieves relatively low accuracies. The advantage of using a decoder LSTM is that it leverages a recurrent structure for capturing full sequence information in the output. Unlike greedy or CRF decoders (Durrett and Klein, 20"
W17-6315,P13-1045,0,0.0419691,"ddition, the range of features that our attention mechanism models is far beyond the manual feature templates, since words even on the Contrast with Vinyals et al. (2015) For constituent parsing, our models outperforms the parser of Vinyals et al. (2015) by differentiating stack and queue and generating transition actions instead. This shows the advantage of shiftreduce actions over bracketed syntactic trees as decoder outputs. Using the settings tuned on the dependency development data directly, our model achieves a F1-score of 90.5, which is comparable to the models of Zhu et al. (2013) and Socher et al. (2013). By using the rerankers of Choe and Charniak (2016) under the same settings, we obtain a 92.7 F1-score with fully-supervised reranking and a 93.4 F1-score with semi-supervised reranking. 111 stack (a) The Dow Jones Model queue industrials closed at 2569.26 . Shift (b) The Dow (c) industrials at closed . 2569.26 Left-Arc Jones (S NP (VP said most of their major institutional … Traders NT(SBAR) (d) … said (SBAR … (NP most (e) … (PP of (NP their major institutional of their major institutional investors … Reduce investors , on the … Shift dency parsing. Their success forms a strong motivation of"
W17-6315,P15-1150,0,0.161101,"Missing"
W17-6315,P15-1113,0,0.0731565,"apture the stack structure. Table 2 shows the development results on dependency parsing. To verify the effectiveness of attention, we build a baseline using average pooling instead (SQ decoder + average pooling). We additionally build a baseline (SQ decoder + treeLSTM) that is aware of stack structures, by using a tree-LSTM (Tai et al., 2015) to derive head node representations when dependency arcs are built. Attention on the stack sector are applied only on words on the stack, but not for their dependents. This representation is analogous to the stack representation of Dyer et al. (2015) and Watanabe and Sumita (2015). Results show that the explicit construction of stack does not bring significant improvements 2 92 Sentence length The hyper-parameter values are chosen according to the performance of the model on the development data for dependency parsing, and final values are shown in Table 1. For constituent parsing, we use the same hyper-parameters without further optimization. 5.3 94 10 We use the standard WSJ sections in PTB (Marcus et al., 1993), where the sections 2-21 are taken for training data, section 22 for development data and section 23 for test for both dependency parsing and constituent par"
W17-6315,D16-1137,0,0.0130223,"er of Zhang and Clark (2011b), using beam search and early update training. They set a max-likelihood training objective, using probability mass in the beam to approximate partition function of CRF training. Watanabe and Sumita (2015) study constituent parsing by using a largemargin objective, where the negative example is the expected score of all states in the beam for transition-based parsing. Xu et al. (2016) build CCG parsing models with a training objective of maximizing the expected F1 score of all items in the beam when parsing finishes, under a transitionbased system. More relatedly, Wiseman and Rush (2016) use beam search and global max-margin training for the method of Vinyals et al. (2015). In contrast, we use a greedy local model; our method is orthogonal to these techniques. Related work LSTM encoder structures have been used in both transition-based and graph-based parsing. Among transition-based parsers, Kiperwasser and Goldberg (2016) use two-layer encoder to encode input sentence, extracting 11 different features from a given state in order to predict the next transition action, showing that the encoder structure lead to significant accuracy improvements over the baseline parser of Chen"
W17-6315,N16-1025,0,0.0144773,"ition-actions to build trees. We empirically investigate the effectiveness of applying the encoder-decoder network to transition-based parsing. On standard benchmarks, our system gives comparable results to the stack LSTM parser for dependency parsing, and significantly better results compared to the aforementioned parser for constituent parsing, which uses bracketed tree formats. 1 Introduction Neural networks have achieved the state-of-theart for parsing under various grammar formalisms, including dependency grammar (Dozat and Manning, 2017), constituent grammar (Dyer et al., 2016) and CCG (Xu et al., 2016). For transitionbased parsing, Chen and Manning (2014) employed a feed-forward neural network with cube activation functions for local action modeling, archiving better results compared to MaltParser (Nivre et al., 2007). Subsequent work extend this method by investigating more complex representations of configurations (Dyer et al., 2015; Ballesteros et al., 2015) and global training with beam search (Zhou et al., 2015; Andor et al., 2016). Borrowing ideas from neural machine translation (NMT) (Bahdanau et al., 2015), a line of work utilizes a bidirectional RNN to en105 Proceedings of the 15th"
W17-6315,W03-3023,0,0.451195,"Missing"
W17-6315,W09-3825,1,0.898893,"Missing"
W17-6315,P11-1069,1,0.843824,"UCE, S HIFT , R EDUCE, can be used to construct its constituent tree. Induction Rules: S HIFT (S,q0 |Q,n) (S|q0 ,Q,n) NT(X) (S,Q,n) (S|e(x),Q,n+1) R EDUCE (S|e(x)|sj |...|s0 ,Q,n) (S|e(x,sj ,...,s0 ),Q,n−1) 2.3 (b) Top-down constituent parsing. Both transition systems above can be treated as examples of a general sequence-to-sequence task. Formally, given a sentence x1 , x2 , ..., xn where xi is the ith word in the sentence, the goal is to generate a corresponding sequence of actions a1 , a2 , ..., am , which correspond to a syntactic structure. Other shift-reduce parser systems, such as CCG (Zhang and Clark, 2011a), can be regarded as instantiation of this. Figure 2: Deduction systems 2.2 Generalization Constituent parsing We employ the top-down transition system of Dyer et al. (2016) for constituent parsing. Formally, a parsing state is denoted as a tuple (S, Q, n), where S is the stack [..., s2 , s1 , s0 ]. Each element in S can be a open nonterminal1 , a completed constituent, or a terminal, Q is the queue, and n is the number of open nonterminals on the stack. At each step, the parser chooses one of the following actions: 3 Baseline We take two baseline neural parsers, namely the parser of Dyer et"
W17-6315,J11-1005,1,0.826549,"UCE, S HIFT , R EDUCE, can be used to construct its constituent tree. Induction Rules: S HIFT (S,q0 |Q,n) (S|q0 ,Q,n) NT(X) (S,Q,n) (S|e(x),Q,n+1) R EDUCE (S|e(x)|sj |...|s0 ,Q,n) (S|e(x,sj ,...,s0 ),Q,n−1) 2.3 (b) Top-down constituent parsing. Both transition systems above can be treated as examples of a general sequence-to-sequence task. Formally, given a sentence x1 , x2 , ..., xn where xi is the ith word in the sentence, the goal is to generate a corresponding sequence of actions a1 , a2 , ..., am , which correspond to a syntactic structure. Other shift-reduce parser systems, such as CCG (Zhang and Clark, 2011a), can be regarded as instantiation of this. Figure 2: Deduction systems 2.2 Generalization Constituent parsing We employ the top-down transition system of Dyer et al. (2016) for constituent parsing. Formally, a parsing state is denoted as a tuple (S, Q, n), where S is the stack [..., s2 , s1 , s0 ]. Each element in S can be a open nonterminal1 , a completed constituent, or a terminal, Q is the queue, and n is the number of open nonterminals on the stack. At each step, the parser chooses one of the following actions: 3 Baseline We take two baseline neural parsers, namely the parser of Dyer et"
W17-6315,P15-1117,1,0.838091,"chieved the state-of-theart for parsing under various grammar formalisms, including dependency grammar (Dozat and Manning, 2017), constituent grammar (Dyer et al., 2016) and CCG (Xu et al., 2016). For transitionbased parsing, Chen and Manning (2014) employed a feed-forward neural network with cube activation functions for local action modeling, archiving better results compared to MaltParser (Nivre et al., 2007). Subsequent work extend this method by investigating more complex representations of configurations (Dyer et al., 2015; Ballesteros et al., 2015) and global training with beam search (Zhou et al., 2015; Andor et al., 2016). Borrowing ideas from neural machine translation (NMT) (Bahdanau et al., 2015), a line of work utilizes a bidirectional RNN to en105 Proceedings of the 15th International Conference on Parsing Technologies, pages 105–114, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics strings correspond to valid dependency trees. On the other hand, a more commonly used sequential representation of syntactic structures is the transition-action sequences in shift reduce parsers. For both constituent (Sagae and Lavie, 2005; Zhang and Clark, 2009) and dep"
W17-6315,P13-1043,1,0.875615,"Goldberg (2016). In addition, the range of features that our attention mechanism models is far beyond the manual feature templates, since words even on the Contrast with Vinyals et al. (2015) For constituent parsing, our models outperforms the parser of Vinyals et al. (2015) by differentiating stack and queue and generating transition actions instead. This shows the advantage of shiftreduce actions over bracketed syntactic trees as decoder outputs. Using the settings tuned on the dependency development data directly, our model achieves a F1-score of 90.5, which is comparable to the models of Zhu et al. (2013) and Socher et al. (2013). By using the rerankers of Choe and Charniak (2016) under the same settings, we obtain a 92.7 F1-score with fully-supervised reranking and a 93.4 F1-score with semi-supervised reranking. 111 stack (a) The Dow Jones Model queue industrials closed at 2569.26 . Shift (b) The Dow (c) industrials at closed . 2569.26 Left-Arc Jones (S NP (VP said most of their major institutional … Traders NT(SBAR) (d) … said (SBAR … (NP most (e) … (PP of (NP their major institutional of their major institutional investors … Reduce investors , on the … Shift dency parsing. Their success for"
W17-6315,P04-1013,0,\N,Missing
W17-6315,Q16-1023,0,\N,Missing
W18-6553,P09-1091,0,0.489263,"al network, observing significantly better results compared to LSTM language models on this task. 1 Introduction Linearization is the task of finding the grammatical order for a given set of words. Syntactic linearization systems generate output sentences along with their syntactic trees. Depending on how much syntactic information is available during decoding, recent work on syntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Nethe"
W18-6553,D16-1032,0,0.0287327,"yntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics Liu et al., 2015; Liu and Zhang, 2015) on syntactic linearization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as context free grammar (de Gispert et al., 2014) and combinatory categorial grammar (White and Rajkumar, 2009; Zhang an"
W18-6553,D15-1043,1,0.952914,"ial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics Liu et al., 2015; Liu and Zhang, 2015) on syntactic linearization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as context free grammar (de Gispert et al., 2014) and combinatory categorial grammar (White and Rajkumar, 2009; Zhang and Clark, 2011), has also been studied. and the gap can go up to 11 BLEU points by integrating the LSTM language model as features. The integrated system also outperforms the LSTM language model by 1 BLEU point using beam search, which shows that syntactic information is useful for a neural linearization system. 2 Related work"
W18-6553,N15-1012,1,0.926688,"ently, Schmaltz et al. (2016) report new state-of-the-art results by leveraging a neural language model without using syntactic information. In their experiments, the neural language model, which is less sparse and captures long-range dependencies, outperforms previous discrete syntactic systems. A research question that naturally arises from this result is whether syntactic information is helpful for a neural linearization system. We empirically answer this question by comparing a neural transition-based syntactic linearizer with the neural language model of Schmaltz et al. (2016). Following Liu et al. (2015), our linearizer works incrementally given a set of words, using a stack to store partially built dependency trees, and a set to maintain unordered incoming words. At each step, it either shifts a word onto the stack, or reduces the top two partial trees on the stack. We leverage a feed forward neural network, which takes stack features as input and predicts the next action (such as S HIFT, L EFTA RC and R IGHTA RC). Hence our method can be regarded as an extension of the parser of Chen and Manning (2014), adding word ordering functionalities. In addition, we investigate two methods for integr"
W18-6553,P14-2128,1,0.86206,"latility . the debate between the stock and futures markets is prepared for wall street will cause another situation about whether de-linkage crash undoubtedly properly renewed friday . the wall street futures markets undoubtedly will cause renewed debate about whether the stock situation is properly prepared for an other crash between friday and de-linkage . the de-linkage between the stock and futures markets friday will undoubtedly cause renewed debate about whether wall street is prope rly prepared for another crash situation . Table 5: Output samples. 0.8 The results are consistent with (Ma et al., 2014) in that both increasing beam size and using richer features are solutions for error propagation. Synl ×LSTM-512 Synl ×LSTM-1 LSTM-512 LSTM-1 0.7 0.6 S YN×LSTM is better than S YN+LSTM. In fact, S YN×LSTM can be considered as interpolation with α being automatically calculated under different states. Finally, S YNl ×LSTM is better than S YN×LSTM except under greedy search, showing that word-to-word dependency features may be sufficient for this task. BLEU 0.5 0.4 0.3 0.2 0.1 10 As for the decoding times, S YNl ×LSTM shows a moderate time growth along increasing beam size, which is roughly 1.5"
W18-6553,P18-1026,0,0.0565936,"Missing"
W18-6553,C10-1012,0,0.143374,"ving significantly better results compared to LSTM language models on this task. 1 Introduction Linearization is the task of finding the grammatical order for a given set of words. Syntactic linearization systems generate output sentences along with their syntactic trees. Depending on how much syntactic information is available during decoding, recent work on syntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8,"
W18-6553,J08-4003,0,0.0315241,"terpolated probability distribution is chosen, before both systems advancing to a new state using the action. The interpolated conditional probability is: 6 Following Chen and Manning (2014), we set the training objective as maximizing the loglikelihood of each successive action conditioned on the dependency tree, which can be gold or automatically parsed. To train our linearizer, we first generate training examples {(si , ti )}m i=1 from the training sentences and their gold parse trees, where si is a state, and ti ∈ T is the corresponding oracle transition. We use the “arc standard” oracle (Nivre, 2008), which always prefers S HIFT over LEFTA RC. The final training objective is to minimize the cross-entropy loss, plus an L2regularization term: p(a|si , hi ; θ1 , θ2 ) = log p(a|si ; θ1 ) + α log p(a|hi ; θ2 ), (8) where si and θ1 are the state and parameters of the linearizer, hi and θ2 are the state and parameters of the LSTM language model, and α is the interpolation hyper parameter. The action spaces of the two systems are different because the actions of the LSTM language model correspond only to the shift actions of the linearizer. To match the probability distributions, we expand the di"
W18-6553,P02-1040,0,0.10215,"orward neural network, which takes stack features as input and predicts the next action (such as S HIFT, L EFTA RC and R IGHTA RC). Hence our method can be regarded as an extension of the parser of Chen and Manning (2014), adding word ordering functionalities. In addition, we investigate two methods for integrating neural language models: interpolating the log probabilities of both models and integrating the neural language model as a feature. On standard benchmarks, our syntactic linearizer gives results that are higher than the LSTM language model of Schmaltz et al. (2016) by 7 BLEU points (Papineni et al., 2002) using greedy search, The task of linearization is to find a grammatical order given a set of words. Traditional models use statistical methods. Syntactic linearization systems, which generate a sentence along with its syntactic tree, have shown state-of-the-art performance. Recent work shows that a multilayer LSTM language model outperforms competitive statistical syntactic linearization systems without using syntax. In this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better"
W18-6553,D14-1082,0,0.635996,"-based syntactic linearizer with the neural language model of Schmaltz et al. (2016). Following Liu et al. (2015), our linearizer works incrementally given a set of words, using a stack to store partially built dependency trees, and a set to maintain unordered incoming words. At each step, it either shifts a word onto the stack, or reduces the top two partial trees on the stack. We leverage a feed forward neural network, which takes stack features as input and predicts the next action (such as S HIFT, L EFTA RC and R IGHTA RC). Hence our method can be regarded as an extension of the parser of Chen and Manning (2014), adding word ordering functionalities. In addition, we investigate two methods for integrating neural language models: interpolating the log probabilities of both models and integrating the neural language model as a feature. On standard benchmarks, our syntactic linearizer gives results that are higher than the LSTM language model of Schmaltz et al. (2016) by 7 BLEU points (Papineni et al., 2002) using greedy search, The task of linearization is to find a grammatical order given a set of words. Traditional models use statistical methods. Syntactic linearization systems, which generate a sent"
W18-6553,D16-1255,0,0.596497,"with discriminative features. Recently, Schmaltz et al. (2016) report new state-of-the-art results by leveraging a neural language model without using syntactic information. In their experiments, the neural language model, which is less sparse and captures long-range dependencies, outperforms previous discrete syntactic systems. A research question that naturally arises from this result is whether syntactic information is helpful for a neural linearization system. We empirically answer this question by comparing a neural transition-based syntactic linearizer with the neural language model of Schmaltz et al. (2016). Following Liu et al. (2015), our linearizer works incrementally given a set of words, using a stack to store partially built dependency trees, and a set to maintain unordered incoming words. At each step, it either shifts a word onto the stack, or reduces the top two partial trees on the stack. We leverage a feed forward neural network, which takes stack features as input and predicts the next action (such as S HIFT, L EFTA RC and R IGHTA RC). Hence our method can be regarded as an extension of the parser of Chen and Manning (2014), adding word ordering functionalities. In addition, we inves"
W18-6553,E14-1028,0,0.0515525,"Missing"
W18-6553,E12-1075,1,0.923719,"this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better results compared to LSTM language models on this task. 1 Introduction Linearization is the task of finding the grammatical order for a given set of words. Syntactic linearization systems generate output sentences along with their syntactic trees. Depending on how much syntactic information is available during decoding, recent work on syntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et"
W18-6553,P16-1056,0,0.0600418,"Missing"
W18-6553,D11-1106,1,0.961405,"., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics Liu et al., 2015; Liu and Zhang, 2015) on syntactic linearization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as context free grammar (de Gispert et al., 2014) and combinatory categorial grammar (White and Rajkumar, 2009; Zhang and Clark, 2011), has also been studied. and the gap can go up to 11 BLEU points by integrating the LSTM language model as features. The integrated system also outperforms the LSTM language model by 1 BLEU point using beam search, which shows that syntactic information is useful for a neural linearization system. 2 Related work 3 Previous work (White, 2005; White and Rajkumar, 2009; Zhang and Clark, 2011; Zhang, 2013) on syntactic linearization uses best-first search, which adopts a priority queue to store partial hypotheses and a chart to store input words. At each step, it pops the highest-scored hypothesis"
W18-6553,P18-1030,1,0.876063,"Missing"
W18-6553,P11-2033,1,0.776233,"on-like models. Second, we investigate a light version of the system, which only uses word features, while previous works all rely on POS tags and arc labels, limiting their usability on low-resource domains and languages. Schmaltz et al. (2016) are the first to adopt neural networks on this task, while only using surface features. To our knowledge, we are the first to leverage both neural networks and syntactic features. The contrast between our method and the method of Chen and Manning (2014) is reminiscent of the contrast between the method of Liu et al. (2015) and the dependency parser of Zhang and Nivre (2011). Comparing with the dependency parsing task, which assumes that POS tags are available as input, the search space of syntactic linearization is much larger. Recent work (Zhang, 2013; Song et al., 2014; Task Given an input bag-of-words x = {x1 , x2 , ..., xn }, the goal is to output the correct permutation y, which recovers the original sentence, from the set of all possible permutations Y. A linearizer can be seen as a scoring function f over Y, which is trained to output its highest scoring permutation yˆ = argmaxy0 ∈Y f (x, y 0 ) as close as possible to the correct permutation y. 3.1 Baseli"
W18-6553,P18-1150,1,0.887521,"Missing"
W18-6553,D14-1021,1,0.83641,"ir syntactic trees. Depending on how much syntactic information is available during decoding, recent work on syntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics Liu et al., 2015; Liu and Zhang, 2015) on syntactic linearization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as contex"
W18-6553,P13-1043,1,0.818051,"on the training set. we use ten-fold jackknifing to construct WSJ training data with different accuracies. More specifically, the data is first randomly split into ten equalsize subsets, and then each subset is automatically parsed with a constituent parser trained on the other subsets, before the results are finally converted to dependency trees using Penn2Malt. In order to obtain datasets with different parsing accuracies, we randomly sample a small number of sentences from each training subset and choose different training iterations, as shown in Table 4. In our experiments, we use ZPar3 (Zhu et al., 2013) for automatic constituent parsing. Our syntactic linearizer is implemented with Keras.4 We randomly initialize Ew , Et , El , W1 and W2 within (−0.01, 0.01), and use default setting for other parameters. The hyper-parameters and parameters which achieve the best performance on the development set are chosen for final evaluation. Our vocabulary comes from SENNA5 , which has 130,000 words. The activation functions tanh and softmax are added on top of the hidden and output layers, respectively. We use Adagrad (Duchi et al., 2011) with an initial learning rate of 0.01, regularization parameter λ"
W18-6553,E09-1097,0,0.732492,"using syntax. In this paper, we study neural syntactic linearization, building a transition-based syntactic linearizer leveraging a feed forward neural network, observing significantly better results compared to LSTM language models on this task. 1 Introduction Linearization is the task of finding the grammatical order for a given set of words. Syntactic linearization systems generate output sentences along with their syntactic trees. Depending on how much syntactic information is available during decoding, recent work on syntactic linearization can be classified into abstract word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), where no syntactic information is available during decoding, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), where full tree information is available, and partial tree linearization (Zhang, 2013), where partial syntactic information is given as input. Linearization has been adapted to tasks such as machine translation (Zhang et al., 2014), and is potentially helpful for many NLG applications, such as cooking recipe generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question ge"
W18-6553,D15-1199,0,0.0691019,"Missing"
W18-6553,W05-1104,0,0.0491195,"earization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as context free grammar (de Gispert et al., 2014) and combinatory categorial grammar (White and Rajkumar, 2009; Zhang and Clark, 2011), has also been studied. and the gap can go up to 11 BLEU points by integrating the LSTM language model as features. The integrated system also outperforms the LSTM language model by 1 BLEU point using beam search, which shows that syntactic information is useful for a neural linearization system. 2 Related work 3 Previous work (White, 2005; White and Rajkumar, 2009; Zhang and Clark, 2011; Zhang, 2013) on syntactic linearization uses best-first search, which adopts a priority queue to store partial hypotheses and a chart to store input words. At each step, it pops the highest-scored hypothesis from the priority queue, expanding it by combination with the words in the chart, before finally putting all new hypotheses back into the priority queue. As the search space is huge, a timeout threshold is set, beyond which the search terminates and the current best hypothesis is taken as the result. Liu et al. (2015) adapt the transition-"
W18-6553,D09-1043,0,0.03745,"e generation (Kiddon et al., 2016), dialogue response generation (Wen et al., 2015), and question generation (Serban et al., 2016). 431 Proceedings of The 11th International Natural Language Generation Conference, pages 431–440, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics Liu et al., 2015; Liu and Zhang, 2015) on syntactic linearization uses dependency grammar. We follow this line of works. On the other hand, linearization with other syntactic grammars, such as context free grammar (de Gispert et al., 2014) and combinatory categorial grammar (White and Rajkumar, 2009; Zhang and Clark, 2011), has also been studied. and the gap can go up to 11 BLEU points by integrating the LSTM language model as features. The integrated system also outperforms the LSTM language model by 1 BLEU point using beam search, which shows that syntactic information is useful for a neural linearization system. 2 Related work 3 Previous work (White, 2005; White and Rajkumar, 2009; Zhang and Clark, 2011; Zhang, 2013) on syntactic linearization uses best-first search, which adopts a priority queue to store partial hypotheses and a chart to store input words. At each step, it pops the h"
W19-4805,D16-1011,0,0.0782085,"Missing"
W19-4805,N18-1100,0,0.0839713,"their attention weights. However, previous work has several limitations. Lin et al. (2017), for example, take single words as basic units, while meaningful information is usually carried by multi-word phrases. For instance, useful symptoms in Table 1, such as “bleeding after nasogastric tube insertion”, are larger than a single word. Another issue of Lin et al. (2017) is that their attention model is applied on the representation vectors produced by an LSTM. Each LSTM output contains more than just the information of that position, thus the real range for the highlighted position is unclear. Mullenbach et al. (2018) defines all 4-grams of the input text as basic units and uses a convolutional layer to learn their representations, which still suffers from fixed-length highlighting. Thus the explainability of the model is limited. Lei et al. (2016) introduce a regularizer over the selected (single-word) positions to encourage the model to extract larger phrases. However, their method can not tell how much a selected unit contributes to the model’s decision through a weight value. In this paper, we study what the meaningful units to highlight are. We define multi-granular ngrams as basic units, so that all"
W19-4805,D14-1162,0,0.0826462,"coder, all ngrams with the same order can be computed in parallel, and the model needs at most 7 iterative steps along the depth dimension for representing a given text of arbitrary length. 4 ACC #Param. 2.6 4.6 1.4 64.8 64.5 66.2 848,228 147,928 168,228 tokens, and one label out of five categories indicating which disease this document is about. We randomly split the dataset into train/dev/test sets by 8:1:1 for each category, and end up with 11,216/1,442/1,444 instances for each set. Hyperparameters We use the 300-dimensional GloVe word vectors pre-trained from the 840B Common Crawl corpus (Pennington et al., 2014), and set the hidden size as 100 for node embeddings. We apply dropout to every layer with a dropout ratio 0.2, and set the batch size as 50. We minimize the cross-entropy of the training set with the ADAM optimizer (Kingma and Ba, 2014), and set the learning rate is to 0.001. During training, the pre-trained word embeddings are not updated. (5) (6) Eval Time 57.0 92.1 30.3 Table 2: Efficiency evaluation. (4) c=i u+f h +f h Train Time 4.1 Properties of the multi-granular encoder Influence of the n-gram order: For CNN and our LeftForest encoder, we vary the order of ngrams from 1 to 9, and plot"
W19-4805,P15-1150,0,0.13561,"Missing"
yang-etal-2017-neural-reranking,C10-2104,0,\N,Missing
yang-etal-2017-neural-reranking,D07-1073,0,\N,Missing
yang-etal-2017-neural-reranking,W14-1609,0,\N,Missing
yang-etal-2017-neural-reranking,N04-1023,0,\N,Missing
yang-etal-2017-neural-reranking,W09-1119,0,\N,Missing
yang-etal-2017-neural-reranking,W07-1033,0,\N,Missing
yang-etal-2017-neural-reranking,J05-1003,0,\N,Missing
yang-etal-2017-neural-reranking,P02-1060,0,\N,Missing
yang-etal-2017-neural-reranking,P15-1112,0,\N,Missing
yang-etal-2017-neural-reranking,N13-1006,0,\N,Missing
yang-etal-2017-neural-reranking,D15-1073,1,\N,Missing
yang-etal-2017-neural-reranking,N16-1030,0,\N,Missing
yang-etal-2017-neural-reranking,L16-1034,1,\N,Missing
yang-etal-2017-neural-reranking,P16-1101,0,\N,Missing
