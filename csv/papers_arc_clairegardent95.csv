2021.tacl-1.6,Augmenting Transformers with {KNN}-Based Composite Memory for Dialog,2021,-1,-1,2,0.714286,849,angela fan,Transactions of the Association for Computational Linguistics,0,"Various machine learning tasks can benefit from access to external information of different modalities, such as text and images. Recent work has focused on learning architectures with large memories capable of storing this knowledge. We propose augmenting generative Transformer neural networks with KNN-based Information Fetching (KIF) modules. Each KIF module learns a read operation to access fixed external knowledge. We apply these modules to generative dialog modeling, a challenging task where information must be flexibly retrieved and incorporated to maintain the topic and flow of conversation. We demonstrate the effectiveness of our approach by identifying relevant knowledge required for knowledgeable but engaging dialog from Wikipedia, images, and human-written dialog utterances, and show that leveraging this retrieved information improves model performance, measured by automatic and human evaluation."
2021.nlpmc-1.3,"Gathering Information and Engaging the User {C}om{B}ot: A Task-Based, Serendipitous Dialog Model for Patient-Doctor Interactions",2021,-1,-1,4,0,2757,anna liednikova,Proceedings of the Second Workshop on Natural Language Processing for Medical Conversations,0,"We focus on dialog models in the context of clinical studies where the goal is to help gather, in addition to the close information collected based on a questionnaire, serendipitous information that is medically relevant. To promote user engagement and address this dual goal (collecting both a predefined set of data points and more informal information about the state of the patients), we introduce an ensemble model made of three bots: a task-based, a follow-up and a social bot. We introduce a generic method for developing follow-up bots. We compare different ensemble configurations and we show that the combination of the three bots (i) provides a better basis for collecting information than just the information seeking bot and (ii) collects information in a more user-friendly, more efficient manner that an ensemble model combining the information seeking and the social bot."
2021.findings-emnlp.25,Discourse-Based Sentence Splitting,2021,-1,-1,3,0,6442,liam cripwell,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Sentence splitting involves the segmentation of a sentence into two or more shorter sentences. It is a key component of sentence simplification, has been shown to help human comprehension and is a useful preprocessing step for NLP tasks such as summarisation and relation extraction. While several methods and datasets have been proposed for developing sentence splitting models, little attention has been paid to how sentence splitting interacts with discourse structure. In this work, we focus on cases where the input text contains a discourse connective, which we refer to as discourse-based sentence splitting. We create synthetic and organic datasets for discourse-based splitting and explore different ways of combining these datasets using different model architectures. We show that pipeline models which use discourse structure to mediate sentence splitting outperform end-to-end models in learning the various ways of expressing a discourse relation but generate text that is less grammatical; that large scale synthetic data provides a better basis for learning than smaller scale organic data; and that training on discourse-focused, rather than on general sentence splitting data provides a better basis for discourse splitting."
2021.findings-emnlp.132,Entity-Based Semantic Adequacy for Data-to-Text Generation,2021,-1,-1,3,0,6763,juliette faille,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"While powerful pre-trained language models have improved the fluency of text generation models, semantic adequacy -the ability to generate text that is semantically faithful to the input- remains an unsolved issue. In this paper, we introduce a novel automatic evaluation metric, Entity-Based Semantic Adequacy, which can be used to assess to what extent generation models that verbalise RDF (Resource Description Framework) graphs produce text that contains mentions of the entities occurring in the RDF input. This is important as RDF subject and object entities make up 2/3 of the input. We use our metric to compare 25 models from the WebNLG Shared Tasks and we examine correlation with results from human evaluations of semantic adequacy. We show that while our metric correlates with human evaluation scores, this correlation varies with the specifics of the human evaluation setup. This suggests that in order to measure the entity-based adequacy of generated texts, an automatic metric such as the one proposed here might be more reliable, as less subjective and more focused on correct verbalisation of the input, than human evaluation measures."
2020.webnlg-1.3,A General Benchmarking Framework for Text Generation,2020,-1,-1,9,0,14063,diego moussallem,Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+),0,"The RDF-to-text task has recently gained substantial attention due to the continuous growth of RDF knowledge graphs in number and size. Recent studies have focused on systematically comparing RDF-to-text approaches on benchmarking datasets such as WebNLG. Although some evaluation tools have already been proposed for text generation, none of the existing solutions abides by the Findability, Accessibility, Interoperability, and Reusability (FAIR) principles and involves RDF data for the knowledge extraction task. In this paper, we present BENG, a FAIR benchmarking platform for Natural Language Generation (NLG) and Knowledge Extraction systems with focus on RDF data. BENG builds upon the successful benchmarking platform GERBIL, is opensource and is publicly available along with the data it contains."
2020.webnlg-1.7,"The 2020 Bilingual, Bi-Directional {W}eb{NLG}+ Shared Task: Overview and Evaluation Results ({W}eb{NLG}+ 2020)",2020,-1,-1,2,0,5949,thiago ferreira,Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+),0,"WebNLG+ offers two challenges: (i) mapping sets of RDF triples to English or Russian text (generation) and (ii) converting English or Russian text to sets of RDF triples (semantic parsing). Compared to the eponymous WebNLG challenge, WebNLG+ provides an extended dataset that enable the training, evaluation, and comparison of microplanners and semantic parsers. In this paper, we present the results of the generation and semantic parsing task for both English and Russian and provide a brief description of the participating systems."
2020.tacl-1.38,Modeling Global and Local Node Contexts for Text Generation from Knowledge Graphs,2020,29,1,3,1,704,leonardo ribeiro,Transactions of the Association for Computational Linguistics,0,"Recent graph-to-text models generate text from graph-based data using either global or local aggregation to learn node representations. Global node encoding allows explicit communication between two distant nodes, thereby neglecting graph topology as all nodes are directly connected. In contrast, local node encoding considers the relations between neighbor nodes capturing the graph structure, but it can fail to capture long-range relations. In this work, we gather both encoding strategies, proposing novel neural models that encode an input graph combining both global and local node contexts, in order to learn better contextualized node embeddings. In our experiments, we demonstrate that our approaches lead to significant improvements on two graph-to-text datasets achieving BLEU scores of 18.01 on the AGENDA dataset, and 63.69 on the WebNLG dataset for seen categories, outperforming state-of-the-art models by 3.7 and 3.1 points, respectively.1"
2020.nl4xai-1.5,"The Natural Language Pipeline, Neural Text Generation and Explainability",2020,-1,-1,3,0,6763,juliette faille,2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence,0,End-to-end encoder-decoder approaches to data-to-text generation are often black boxes whose predictions are difficult to explain. Breaking up the end-to-end model into sub-modules is a natural way to address this problem. The traditional pre-neural Natural Language Generation (NLG) pipeline provides a framework for breaking up the end-to-end encoder-decoder. We survey recent papers that integrate traditional NLG submodules in neural approaches and analyse their explainability. Our survey is a first step towards building explainable neural NLG models.
2020.emnlp-main.231,Multilingual {AMR}-to-Text Generation,2020,-1,-1,2,0.714286,849,angela fan,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Generating text from structured data is challenging because it requires bridging the gap between (i) structure and natural language (NL) and (ii) semantically underspecified input and fully specified NL output. Multilingual generation brings in an additional challenge: that of generating into languages with varied word order and morphological properties. In this work, we focus on Abstract Meaning Representations (AMRs) as structured input, where previous research has overwhelmingly focused on generating only into English. We leverage advances in cross-lingual embeddings, pretraining, and multilingual models to create multilingual AMR-to-text models that generate in twenty one different languages. Our multilingual models surpass baselines that generate into one language in eighteen languages, based on automatic metrics. We analyze the ability of our multilingual models to accurately capture morphology and word order using human evaluation, and find that native speakers judge our generations to be fluent."
2020.coling-main.55,Learning Health-Bots from Training Data that was Automatically Created using Paraphrase Detection and Expert Knowledge,2020,-1,-1,4,0,2757,anna liednikova,Proceedings of the 28th International Conference on Computational Linguistics,0,"A key bottleneck for developing dialog models is the lack of adequate training data. Due to privacy issues, dialog data is even scarcer in the health domain. We propose a novel method for creating dialog corpora which we apply to create doctor-patient interaction data. We use this data to learn both a generation and a hybrid classification/retrieval model and find that the generation model consistently outperforms the hybrid model. We show that our data creation method has several advantages. Not only does it allow for the semi-automatic creation of large quantities of training data. It also provides a natural way of guiding learning and a novel method for assessing the quality of human-machine interactions."
W19-8614,Generating Text from Anonymised Structures,2019,0,0,2,1,23322,emilie colin,Proceedings of the 12th International Conference on Natural Language Generation,0,"Surface realisation (SR) consists in generating a text from a meaning representations (MR). In this paper, we introduce a new parallel dataset of deep meaning representations (MR) and French sentences and we present a novel method for MR-to-text generation which seeks to generalise by abstracting away from lexical content. Most current work on natural language generation focuses on generating text that matches a reference using BLEU as evaluation criteria. In this paper, we additionally consider the model{'}s ability to reintroduce the function words that are absent from the deep input meaning representations. We show that our approach increases both BLEU score and the scores used to assess function words generation."
W19-8635,Revisiting the Binary Linearization Technique for Surface Realization,2019,0,0,2,0,23339,yevgeniy puzikov,Proceedings of the 12th International Conference on Natural Language Generation,0,"End-to-end neural approaches have achieved state-of-the-art performance in many natural language processing (NLP) tasks. Yet, they often lack transparency of the underlying decision-making process, hindering error analysis and certain model improvements. In this work, we revisit the binary linearization approach to surface realization, which exhibits more interpretable behavior, but was falling short in terms of prediction accuracy. We show how enriching the training data to better capture word order constraints almost doubles the performance of the system. We further demonstrate that encoding both local and global prediction contexts yields another considerable performance boost. With the proposed modifications, the system which ranked low in the latest shared task on multilingual surface realization now achieves best results in five out of ten languages, while being on par with the state-of-the-art approaches in others."
W19-3706,Creating a Corpus for {R}ussian Data-to-Text Generation Using Neural Machine Translation and Post-Editing,2019,0,1,3,1,5963,anastasia shimorina,Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing,0,"In this paper, we propose an approach for semi-automatically creating a data-to-text (D2T) corpus for Russian that can be used to learn a D2T natural language generation model. An error analysis of the output of an English-to-Russian neural machine translation system shows that 80{\%} of the automatically translated sentences contain an error and that 53{\%} of all translation errors bear on named entities (NE). We therefore focus on named entities and introduce two post-editing techniques for correcting wrongly translated NEs."
D19-6312,{LORIA} / Lorraine University at Multilingual Surface Realisation 2019,2019,0,0,2,1,5963,anastasia shimorina,Proceedings of the 2nd Workshop on Multilingual Surface Realisation (MSR 2019),0,"This paper presents the LORIA / Lorraine University submission at the Multilingual Surface Realisation shared task 2019 for the shallow track. We outline our approach and evaluate it on 11 languages covered by the shared task. We provide a separate evaluation of each component of our pipeline, concluding on some difficulties and suggesting directions for future work."
D19-1305,Surface Realisation Using Full Delexicalisation,2019,0,0,2,1,5963,anastasia shimorina,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Surface realisation (SR) maps a meaning representation to a sentence and can be viewed as consisting of three subtasks: word ordering, morphological inflection and contraction generation (e.g., clitic attachment in Portuguese or elision in French). We propose a modular approach to surface realisation which models each of these components separately, and evaluate our approach on the 10 languages covered by the SR{'}18 Surface Realisation Shared Task shallow track. We provide a detailed evaluation of how word order, morphological realisation and contractions are handled by the model and an analysis of the differences in word ordering performance across languages."
D19-1314,Enhancing {AMR}-to-Text Generation with Dual Graph Representations,2019,0,4,2,1,704,leonardo ribeiro,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Generating text from graph-based data, such as Abstract Meaning Representation (AMR), is a challenging task due to the inherent difficulty in how to properly encode the structure of a graph with labeled edges. To address this difficulty, we propose a novel graph-to-sequence model that encodes different but complementary perspectives of the structural information contained in the AMR graph. The model learns parallel top-down and bottom-up representations of nodes capturing contrasting views of the graph. We also investigate the use of different node message passing strategies, employing different state-of-the-art graph encoders to compute node representations based on incoming and outgoing perspectives. In our experiments, we demonstrate that the dual graph representation leads to improvements in AMR-to-text generation, achieving state-of-the-art results on two AMR datasets"
D19-1428,Using Local Knowledge Graph Construction to Scale {S}eq2{S}eq Models to Multi-Document Inputs,2019,0,7,2,0.612245,849,angela fan,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Query-based open-domain NLP tasks require information synthesis from long and diverse web results. Current approaches extractively select portions of web text as input to Sequence-to-Sequence models using methods such as TF-IDF ranking. We propose constructing a local graph structured knowledge base for each query, which compresses the web search information and reduces redundancy. We show that by linearizing the graph into a structured input sequence, models can encode the graph representations within a standard Sequence-to-Sequence setting. For two generative tasks with very long text input, long-form question answering and multi-document summarization, feeding graph representations as input can achieve better performance than using retrieved text portions."
W18-6543,Handling Rare Items in Data-to-Text Generation,2018,0,4,2,1,5963,anastasia shimorina,Proceedings of the 11th International Conference on Natural Language Generation,0,"Neural approaches to data-to-text generation generally handle rare input items using either delexicalisation or a copy mechanism. We investigate the relative impact of these two methods on two datasets (E2E and WebNLG) and using two evaluation settings. We show (i) that rare items strongly impact performance; (ii) that combining delexicalisation and copying yields the strongest improvement; (iii) that copying underperforms for rare and unseen items and (iv) that the impact of these two mechanisms greatly varies depending on how the dataset is constructed and on how it is split into train, dev and test."
N18-6002,Deep Learning Approaches to Text Production,2018,0,0,1,1,850,claire gardent,Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Tutorial Abstracts,0,"Text production is a key component of many NLP applications. In data-driven approaches, it is used for instance, to generate dialogue turns from dialogue moves, to verbalise the content of Knowledge bases or to generate natural English sentences from rich linguistic representations, such as dependency trees or Abstract Meaning Representations. In text-driven methods on the other hand, text production is at work in sentence compression, sentence fusion, paraphrasing, sentence (or text) simplification, text summarisation and end-to-end dialogue systems. Following the success of encoder-decoder models in modeling sequence-rewriting tasks such as machine translation, deep learning models have successfully been applied to the various text production tasks. In this tutorial, we will cover the fundamentals and the state-of-the-art research on neural models for text production. Each text production task raises a slightly different communication goal (e.g, how to take the dialogue context into account when producing a dialogue turn; how to detect and merge relevant information when summarising a text; or how to produce a well-formed text that correctly capture the information contained in some input data in the case of data-to-text generation). We will outline the constraints specific to each subtasks and examine how the existing neural models account for them."
D18-1113,Generating Syntactic Paraphrases,2018,0,1,2,1,23322,emilie colin,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"We study the automatic generation of syntactic paraphrases using four different models for generation: data-to-text generation, text-to-text generation, text reduction and text expansion, We derive training data for each of these tasks from the WebNLG dataset and we show (i) that conditioning generation on syntactic constraints effectively permits the generation of syntactically distinct paraphrases for the same input and (ii) that exploiting different types of input (data, text or data+text) further increases the number of distinct paraphrases that can be generated for a given input."
W17-3518,The {W}eb{NLG} Challenge: Generating Text from {RDF} Data,2017,16,36,1,1,850,claire gardent,Proceedings of the 10th International Conference on Natural Language Generation,0,"The WebNLG challenge consists in mapping sets of RDF triples to text. It provides a common benchmark on which to train, evaluate and compare {``}microplanners{''}, i.e. generation systems that verbalise a given content by making a range of complex interacting choices including referring expression generation, aggregation, lexicalisation, surface realisation and sentence segmentation. In this paper, we introduce the microplanning task, describe data preparation, introduce our evaluation methodology, analyse participant results and provide a brief description of the participating systems."
W17-3537,Analysing Data-To-Text Generation Benchmarks,2017,0,6,2,1,6264,laura perezbeltrachini,Proceedings of the 10th International Conference on Natural Language Generation,0,"A generation system can only be as good as the data it is trained on. In this short paper, we propose a methodology for analysing data-to-text corpora used for training Natural Language Generation (NLG) systems. We apply this methodology to three existing benchmarks. We conclude by eliciting a set of criteria for the creation of a data-to-text benchmark which could help better support the development, evaluation and comparison of linguistically sophisticated data-to-text generators."
P17-1017,Creating Training Corpora for {NLG} Micro-Planners,2017,16,39,1,1,850,claire gardent,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper, we present a novel framework for semi-automatically creating linguistically challenging micro-planning data-to-text corpora from existing Knowledge Bases. Because our method pairs data of varying size and shape with texts ranging from simple clauses to short texts, a dataset created using this framework provides a challenging benchmark for microplanning. Another feature of this framework is that it can be applied to any large scale knowledge base and can therefore be used to train and learn KB verbalisers. We apply our framework to DBpedia data and compare the resulting dataset with Wen et al. 2016{'}s. We show that while Wen et al.{'}s dataset is more than twice larger than ours, it is less diverse both in terms of input and in terms of text. We thus propose our corpus generation framework as a novel method for creating challenging data sets from which NLG models can be learned which are capable of handling the complex interactions occurring during in micro-planning between lexicalisation, aggregation, surface realisation, referring expression generation and sentence segmentation. To encourage researchers to take up this challenge, we made available a dataset of 21,855 data/text pairs created using this framework in the context of the WebNLG shared task."
J17-1001,"A Statistical, Grammar-Based Approach to Microplanning",2017,35,5,1,1,850,claire gardent,Computational Linguistics,0,"Although there has been much work in recent years on data-driven natural language generation, little attention has been paid to the fine-grained interactions that arise during microplanning between aggregation, surface realization, and sentence segmentation. In this article, we propose a hybrid symbolic/statistical approach to jointly model the constraints regulating these interactions. Our approach integrates a small handwritten grammar, a statistical hypertagger, and a surface realization algorithm. It is applied to the verbalization of knowledge base queries and tested on 13 knowledge bases to demonstrate domain independence. We evaluate our approach in several ways. A quantitative analysis shows that the hybrid approach outperforms a purely symbolic approach in terms of both speed and coverage. Results from a human study indicate that users find the output of this hybrid statistic/symbolic system more fluent than both a template-based and a purely symbolic grammar-based approach. Finally, we illustrate by means of examples that our approach can account for various factors impacting aggregation, sentence segmentation, and surface realization."
D17-1064,Split and Rephrase,2017,39,2,2,0.78125,6260,shashi narayan,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"We propose a new sentence simplification task (Split-and-Rephrase) where the aim is to split a complex sentence into a meaning preserving sequence of shorter sentences. Like sentence simplification, splitting-and-rephrasing has the potential of benefiting both natural language processing and societal applications. Because shorter sentences are generally better processed by NLP systems, it could be used as a preprocessing step which facilitates and improves the performance of parsers, semantic role labellers and machine translation systems. It should also be of use for people with reading disabilities because it allows the conversion of longer sentences into shorter ones. This paper makes two contributions towards this new task. First, we create and make available a benchmark consisting of 1,066,115 tuples mapping a single complex sentence to a sequence of sentences expressing the same meaning. Second, we propose five models (vanilla sequence-to-sequence to semantically-motivated models) to understand the difficulty of the proposed task."
W16-6616,Category-Driven Content Selection,2016,0,3,3,0,33323,rania mohammed,Proceedings of the 9th International Natural Language Generation conference,0,None
W16-6620,Unsupervised Sentence Simplification Using Deep Semantics,2016,23,8,2,0.78125,6260,shashi narayan,Proceedings of the 9th International Natural Language Generation conference,0,"We present a novel approach to sentence simplification which departs from previous work in two main ways. First, it requires neither hand written rules nor a training corpus of aligned standard and simplified sentences. Second, sentence splitting operates on deep semantic structure. We show (i) that the unsupervised framework we propose is competitive with four state-of-the-art supervised systems and (ii) that our semantic based approach allows for a principled and effective handling of sentence splitting."
W16-6626,The {W}eb{NLG} Challenge: Generating Text from {DBP}edia Data,2016,14,19,2,1,23322,emilie colin,Proceedings of the 9th International Natural Language Generation conference,0,None
W16-3505,Content Selection through Paraphrase Detection: Capturing different Semantic Realisations of the Same Idea,2016,15,0,2,0,25381,elena lloret,Proceedings of the 2nd International Workshop on Natural Language Generation and the Semantic Web ({W}eb{NLG} 2016),0,None
W16-3506,Aligning Texts and Knowledge Bases with Semantic Sentence Simplification,2016,10,5,4,0,7147,yassine mrabet,Proceedings of the 2nd International Workshop on Natural Language Generation and the Semantic Web ({W}eb{NLG} 2016),0,"Finding the natural language equivalent of structured data is both a challenging and promising task. In particular, an efficient alignment of knowledge bases with texts would benefit many applications, including natural language generation, information retrieval and text simplification. In this paper, we present an approach to build a dataset of triples aligned with equivalent sentences written in natural language. Our approach consists of three main steps. First, target sentences are annotated automatically with knowledge base (KB) concepts and instances. The triples linking these elements in the KB are extracted as candidate facts to be aligned with the annotated sentence. Second, we use textual mentions referring to the subject and object of these facts to semantically simplify the target sentence via crowdsourcing. Third, the sentences provided by different contributors are post-processed to keep only the most relevant simplifications for the alignment with KB facts. We present different filtering methods, and share the constructed datasets in the public domain. These datasets contain 1050 sentences aligned with 1885 triples. They can be used to train natural language generators as well as semantic or contextual text simplifiers."
W16-3508,Content selection as semantic-based ontology exploration,2016,0,0,2,1,6264,laura perezbeltrachini,Proceedings of the 2nd International Workshop on Natural Language Generation and the Semantic Web ({W}eb{NLG} 2016),0,None
W16-3511,Generating Paraphrases from {DBP}edia using Deep Learning,2016,8,5,2,0,33752,amin sleimi,Proceedings of the 2nd International Workshop on Natural Language Generation and the Semantic Web ({W}eb{NLG} 2016),0,None
S16-2019,Orthogonality regularizer for question answering,2016,13,0,4,0,24326,chunyang xiao,Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics,0,"Learning embeddings of words and knowledge base elements is a promising approach for open domain question answering. Based on the remark that relations and entities are distinct object types lying in the same embedding space, we analyze the benefit of adding a regularizer favoring the embeddings of entities to be orthogonal to those of relations. The main motivation comes from the observation that modifying the embeddings using prior knowledge often helps performance. The experiments show that incorporating the regularizer yields better results on a challenging question answering benchmark."
S16-2027,Learning Embeddings to lexicalise {RDF} Properties,2016,12,4,2,1,6264,laura perezbeltrachini,Proceedings of the Fifth Joint Conference on Lexical and Computational Semantics,0,"A difficult task when generating text from knowledge bases (KB) consists in finding appropriate lexicalisations for KB symbols. We present an approach for lexicalis-ing knowledge base relations and apply it to DBPedia data. Our model learns low-dimensional embeddings of words and RDF resources and uses these representations to score RDF properties against candidate lexicalisations. Training our model using (i) pairs of RDF triples and automatically generated verbalisations of these triples and (ii) pairs of paraphrases extracted from various resources, yields competitive results on DBPedia data."
P16-1127,Sequence-based Structured Prediction for Semantic Parsing,2016,36,61,3,0,24326,chunyang xiao,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose an approach for semantic parsing that uses a recurrent neural network to map a natural language question into a logical form representation of a KB query. Building on recent work by (Wang et al., 2015), the interpretable logical forms, which are structured objects obeying certain constraints, are enumerated by an underlying grammar and are paired with their canonical realizations. In order to use sequence prediction, we need to sequentialize these logical forms. We compare three sequentializations: a direct linearization of the logical form, a linearization of the associated canonical realization, and a sequence consisting of derivation steps relative to the underlying grammar. We also show how grammatical constraints on the derivation sequence can easily be integrated inside the RNN-based sequential predictor. Our experiments show important improvements over previous results for the same dataset, and also demonstrate the advantage of incorporating the grammatical constraints."
C16-1141,Building {RDF} Content for Data-to-Text Generation,2016,0,9,3,1,6264,laura perezbeltrachini,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"In Natural Language Generation (NLG), one important limitation is the lack of common benchmarks on which to train, evaluate and compare data-to-text generators. In this paper, we make one step in that direction and introduce a method for automatically creating an arbitrary large repertoire of data units that could serve as input for generation. Using both automated metrics and a human evaluation, we show that the data units produced by our method are both diverse and coherent."
W15-4703,A Domain Agnostic Approach to Verbalizing n-ary Events without Parallel Corpora,2015,29,0,2,1,13648,bikash gyawali,Proceedings of the 15th {E}uropean Workshop on Natural Language Generation ({ENLG}),0,"We present a method for automatically generating descriptions of biological events encoded in the KB Bio 101 Knowledge base. We evaluate our approach on a corpus of 336 event descriptions, provide a qualitative and quantitative analysis of the results obtained and discuss possible directions for further work."
J15-1003,Multiple Adjunction in Feature-Based {T}ree-{A}djoining {G}rammar,2015,26,5,1,1,850,claire gardent,Computational Linguistics,0,"In parsing with Tree Adjoining Grammar TAG, independent derivations have been shown by Schabes and Shieber 1994 to be essential for correctly supporting syntactic analysis, semantic interpretation, and statistical language modeling. However, the parsing algorithm they propose is not directly applicable to Feature-Based TAGs FB-TAG. We provide a recognition algorithm for FB-TAG that supports both dependent and independent derivations. The resulting algorithm combines the benefits of independent derivations with those of Feature-Based grammars. In particular, we show that it accounts for a range of interactions between dependent vs. independent derivation on the one hand, and syntactic constraints, linear ordering, and scopal vs. nonscopal semantic dependencies on the other hand."
P14-1040,Surface Realisation from Knowledge-Bases,2014,47,13,2,1,13648,bikash gyawali,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Natural Language Generation (NLG) is the task of automatically producing natural language text to describe information present in non-linguistic data. It involves three main subtasks: (i) selecting the relevant portion of input data; (ii) determining the words that will be used to verbalise the selected data; and (iii) mapping these words into natural language text. The latter task is known as Surface Realisation (SR). In my thesis, I study the SR task in the context of input data coming from Knowledge Bases (KB). I present two novel approaches to surface realisation from knowledge bases: a supervised approach and a weakly supervised approach. n n In the first, supervised, approach, I present a corpus-based method for inducing a Feature Based Lexicalized Tree Adjoining Grammar (FB-LTAG) from a parallel corpus of text and data. The resulting grammar includes a unification based semantics and can be used by an existing surface realiser to generate sentences from test data. I show that the induced grammar is compact and generalises well over the test data yielding results that are close to those produced by a handcrafted symbolic approach and which outperform an alternative statistical approach.n n In the weakly supervised approach, I explore a method for surface realisation from KB data which uses a supplied lexicon but does not require a parallel corpus. Instead, I build a corpus from heterogeneous sources of domain-related text and use it to identify possible lexicalisations of KB symbols (classes and relations) and their verbalisation patterns (frames). Based on the observations made, I build different probabilistic models which are used for selection of appropriate frames and syntax/semantics linking while verbalising KB inputs. I evaluate the output sentences and analyse the issues relevant to learning from non-parallel corpora.n n In both these approaches, I use the data derived from an existing biomedical ontology as a reference input. The proposed methods are generic and can be easily adapted for input from other ontologies for which a parallel/non-parallel corpora exists."
P14-1041,Hybrid Simplification using Deep Semantics and Machine Translation,2014,25,41,2,1,6260,shashi narayan,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We present a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation to derive simple sentences from complex ones. The approach differs from previous work in two main ways. First, it is semantic based in that it takes as input a deep semantic representation rather than e.g., a sentence or a parse tree. Second, it combines a simplification model for splitting and deletion with a monolingual translation model for phrase substitution and reordering. When compared against current state of the art methods, our model yields significantly simpler output that is both grammatical and meaning preserving."
E14-1020,Incremental Query Generation,2014,15,4,2,1,6264,laura perezbeltrachini,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present a natural language generation system which supports the incremental specification of ontology-based queries in natural language. Our contribution is two fold. First, we introduce a chart based surface realisation algorithm which supports the kind of incremental processing required by ontology-based querying. Crucially, this algorithm avoids confusing the end user by preserving a consistent ordering of the query elements throughout the incremental query formulation process. Second, we show that grammar based surface realisation better supports the generation of fluent, natural sounding queries than previous template-based approaches."
W13-4056,Weakly and Strongly Constrained Dialogues for Language Learning,2013,7,3,1,1,850,claire gardent,Proceedings of the {SIGDIAL} 2013 Conference,0,We present two dialogue systems for language learning which both restrict the dialog to a specific domain thereby promoting robustness and the learning of a given vocabulary. The systems vary in how much they constrain the learnerxe2x80x99s answer : one system places no other constrain on the learner than that provided by the restricted domain and the dialog context ; the other provides the learner with an exercise whose solution is the expected answer. The first system uses supervised learning for simulating a human tutor whilst the second one uses natural language generation techniques to produce grammar exercises which guide the learner toward the expected answer.
W13-2105,Generating Elliptic Coordination,2013,39,3,1,1,850,claire gardent,Proceedings of the 14th {E}uropean Workshop on Natural Language Generation,0,"In this paper, we focus on the task of generating elliptic sentences. We extract from the data provided by the Surface Realisation (SR) Task 2398 input whose corresponding output sentence contain an ellipsis. We show that 9% of the data contains an ellipsis and that both coverage and BLEU score markedly decrease for elliptic input (from 82.3% coverage for non-elliptic sentences to 65.3% for elliptic sentences and from 0.60 BLEU score to 0.47). We argue that elided material should be represented using phonetically empty nodes and we introduce a set of rewrite rules which permits adding these empty categories to the SR data. Finally, we evaluate an existing surface realiser on the resulting dataset. We show that, after rewriting, the generator achieves a coverage of 76% and a BLEU score of 0.74 on the elliptical data."
W13-2111,The {KBG}en Challenge,2013,3,15,2,0.833333,40963,eva banik,Proceedings of the 14th {E}uropean Workshop on Natural Language Generation,0,"Given a preselected set of relations extracted from the AURA knowledge base on biology, the KBGEN Task consisted in generating a sentence verbalising these relations. Three team submitted the results of their systems. The systems were compared using both automatic metrics (BLEU, NIST) and subjective ratings by 12 human users for three dimensions namely, fluency, grammaticality and meaning similarity. In this report, we summarise the KBGen Task, the evaluation methods and the results obtained."
W13-2131,"{LOR}-{KBGEN}, A Hybrid Approach To Generating from the {KBG}en Knowledge-Base",2013,1,2,2,1,13648,bikash gyawali,Proceedings of the 14th {E}uropean Workshop on Natural Language Generation,0,"This abstract describes a contribution to the 2013 KBGen Challenge from CNRS/LORIA and the University of Lorraine. Our contribution focuses on an attempt to automate the extraction of a Feature Based Tree Adjoining Grammar equipped with a unification based compositional semantics which can be used to generate from KBGen data. Introduction Semantic grammars, i.e., grammars which link syntax and semantics, have been shown to be useful for generation and for semantic parsing. This abstract outlines an attempt to automatically extract from the KBGen data, a Feature Based Tree Adjoining Grammar which can be used for generation from the KBGen data. Data The KBGen data consists of sets of triples extracted from the AURA knowledge base which encodes knowledge contained in a college-level biology textbook. Each set of triple was selected to be verbalisable as a simple, possibly complex sentence. For instance, the input shown in Figure 1 can be verbalised as1: (1) The function of a gated channel is to release particles from the endoplasmic reticulum Sketch of the Overall Grammar Extraction and Generation Procedure To generate from the KBGen data, we parsed each input sentence using the Stanford parser; we aligned the semantic input with a substring in the input sentence; we extracted a grammar from the parsed sentences provided with the input triples; and we generated using an existing surface realiser. In addition some of the input were preprocessed to produce a semantics more compatible with the assumption underlying the syntax/semantic interace of SemTAG; For space reasons, we slightly simplified the KBGen input and removed type information. :TRIPLES ( (|Release-Of-Calcium646| |object| |Particle-In-Motion64582|) (|Release-Of-Calcium646| |base| |Endoplasmic-Reticulum64603|) (|Gated-Channel64605| |has-function||Release-Of-Calcium646|) (|Release-Of-Calcium646| |agent| |Gated-Channel64605|)) :INSTANCE-TYPES (|Particle-In-Motion64582| |instance-of| |Particle-In-Motion|) (|Endoplasmic-Reticulum64603| |instance-of| |Endoplasmic-Reticulum|) (|Gated-Channel64605| |instance-of| |Gated-Channel|) (|Release-Of-Calcium646| |instance-of| |Release-Of-Calcium|)) and a procedure was used to guess missing lexical entries. Alignment and Index Projection Given a Sentence/Input pair (S, I) provided by the KBGen Challenge, we match each entity and event variable in I to a substring in S. Matching uses the variable name, the name of the unary predicate true of that variable and the word form assigned to that predicte in the KBGen lexicon. Digits occurring in the input are removed and the string in the input sentence which is closest to either of the used units is decorated with that variable. Index variables are then projected up the syntactic trees to reflect headedness. For instance, the variable indexed with a noun is projected to the NP level; and the index projected to the NP of a prepositional phrase is project to the PP level. Grammar Extraction Grammar extraction proceeds in two steps as follows. First, the subtrees whose root node are indexed with an entity variable are extracted. This results in a set of NP and PP trees anchored with entity names and associated with the predication true of the indexing variable. Second, the subtrees capturing relations between variables are extracted. To perform this ex-"
J13-3005,{XMG}: e{X}tensible {M}eta{G}rammar,2013,59,28,3,0,5601,benoit crabbe,Computational Linguistics,0,"In this article, we introduce eXtensible MetaGrammar (xmg), a framework for specifying tree-based grammars such as Feature-Based Lexicalised Tree-Adjoining Grammars (FB-LTAG) and Interaction Grammars (IG). We argue that xmg displays three features which facilitate both grammar writing and a fast prototyping of tree-based grammars. Firstly, xmg is fully declarative. For instance, it permits a declarative treatment of diathesis that markedly departs from the procedural lexical rules often used to specify tree-based grammars. Secondly, the xmg language has a high notational expressivity in that it supports multiple linguistic dimensions, inheritance and a sophisticated treatment of identifiers. Thirdly, xmg is extensible in that its computational architecture facilitates the extension to other linguistic formalisms. We explain how this architecture naturally supports the design of three linguistic formalisms namely, FB-LTAG, IG, and Multi-Component Tree-Adjoining Grammar (MC-TAG). We further show how it permits a straightforward integration of additional mechanisms such as linguistic and formal principles. To further illustrate the declarativity, notational expressivity and extensibility of xmg , we describe the methodology used to specify an FB-LTAG for French augmented with a unification-based compositional semantics. This illustrates both how xmg facilitates the modelling of the tree fragment hierarchies required to specify tree-based grammars and of a syntax/semantics interface between semantic representations and syntactic trees. Finally, we briefly report on several grammars for French, English and German that were implemented using xmg and compare xmg to other existing grammar specification frameworks for tree-based grammars."
D13-1076,Using Paraphrases and Lexical Semantics to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Systems,2013,11,2,1,1,850,claire gardent,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"This paper explores to what extent lemmatisation, lexical resources, distributional semantics and paraphrases can increase the accuracy of supervised models for dialogue management. The results suggest that each of these factors can help improve performance but that the impact will vary depending on their combination and on the evaluation mode."
W12-4614,Using {FB}-{LTAG} Derivation Trees to Generate Transformation-Based Grammar Exercises,2012,13,0,1,1,850,claire gardent,Proceedings of the 11th International Workshop on Tree Adjoining Grammars and Related Formalisms ({TAG}+11),0,"Using a Feature-Based Lexicalised Tree Adjoining Grammar (FB-LTAG), we present an approach for generating pairs of sentences that are related by a syntactic transformation and we apply this approach to create language learning exercises. We argue that the derivation trees of an FB-LTAG provide a good level of representation for capturing syntactic transformations. We relate our approach to previous work on sentence reformulation, question generation and grammar exercise generation. We evaluate precision and linguistic coverage. And we demonstrate the genericity of the proposal by applying it to a range of transformations including the Passive/Active transformation, the pronominalisation of an NP, the assertion / yes-no question relation and the assertion / wh-question transformation."
W12-2017,Generating Grammar Exercises,2012,13,13,2,1,6264,laura perezbeltrachini,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"Grammar exercises for language learning fall into two distinct classes: those that are based on real life sentences extracted from existing documents or from the web; and those that seek to facilitate language acquisition by presenting the learner with exercises whose syntax is as simple as possible and whose vocabulary is restricted to that contained in the textbook being used. In this paper, we introduce a framework (called GramEx) which permits generating the second type of grammar exercises. Using generation techniques, we show that a grammar can be used to semi-automatically generate grammar exercises which target a specific learning goal; are made of short, simple sentences; and whose vocabulary is restricted to that used in a given textbook."
W12-1602,An End-to-End Evaluation of Two Situated Dialog Systems,2012,18,4,3,0,14078,lina rojasbarahona,Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"We present and evaluate two state-of-the art dialogue systems developed to support dialog with French speaking virtual characters in the context of a serious game: one hybrid statistical/symbolic and one purely statistical. We conducted a quantitative evaluation where we compare the accuracy of the interpreter and of the dialog manager used by each system; a user based evaluation based on 22 subjects using both the statistical and the hybrid system; and a corpus based evaluation where we examine such criteria as dialog coherence, dialog success, interpretation and generation errors in the corpus of Human-System interactions collected during the user-based evaluation. We show that although the statistical approach is slightly more robust, the hybrid strategy seems to be better at guiding the player through the game."
W12-1507,Generation for Grammar Engineering,2012,15,3,1,1,850,claire gardent,{INLG} 2012 Proceedings of the Seventh International Natural Language Generation Conference,0,"While in Computer Science, grammar engineering has led to the development of various tools for checking grammar coherence, completion, under- and over-generation, in Natural Langage Processing, most approaches developed to improve a grammar have focused on detecting under-generation and to a much lesser extent, over-generation. We argue that generation can be exploited to address other issues that are relevant to grammar engineering such as in particular, detecting grammar incompleteness, identifying sources of over-generation and analysing the linguistic coverage of the grammar. We present an algorithm that implements these functionalities and we report on experiments using this algorithm to analyse a Feature-Based Lexicalised Tree Adjoining Grammar consisting of roughly 1500 elementary trees."
W12-1526,{KBG}en {--} Text Generation from Knowledge Bases as a New Shared Task,2012,4,0,2,0.833333,40963,eva banik,{INLG} 2012 Proceedings of the Seventh International Natural Language Generation Conference,0,"In this paper we propose a new shared task where the aim is to produce coherent descriptions of concepts and relationships in a frame-based knowledge base (KB). We propose to use AURA, a freely available KB, for the shared task and illustrate an application context for NLG. We show that the same application context and the need for language generation tools can be generalized to other biology knowledge bases. We argue that the easy availability of input data and a larger research community -- both domain experts and knowledge representation experts -- which actively uses these knowledge bases, along with regular evaluation experiments, creates an ideal scenario for a shared task."
P12-1062,Error Mining on Dependency Trees,2012,12,5,1,1,850,claire gardent,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In recent years, error mining approaches were developed to help identify the most likely sources of parsing failures in parsing systems using handcrafted grammars and lexicons. However the techniques they use to enumerate and count n-grams builds on the sequential nature of a text corpus and do not easily extend to structured data. In this paper, we propose an algorithm for mining trees and apply it to detect the most likely sources of generation failure. We show that this tree mining algorithm permits identifying not only errors in the generation system (grammar, lexicon) but also mismatches between the structures contained in the input and the input structures expected by our generator as well as a few idiosyncrasies/error in the input data."
P12-1090,Classifying {F}rench Verbs Using {F}rench and {E}nglish Lexical Resources,2012,24,24,2,1,31336,ingrid falk,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We present a novel approach to the automatic acquisition of a Verbnet like classification of French verbs which involves the use (i) of a neural clustering method which associates clusters with features, (ii) of several supervised and unsupervised evaluation metrics and (iii) of various existing syntactic and semantic lexical resources. We evaluate our approach on an established test set and show that it outperforms previous related work with an F-measure of 0.70."
rojas-barahona-etal-2012-building,Building and Exploiting a Corpus of Dialog Interactions between {F}rench Speaking Virtual and Human Agents,2012,9,9,3,0,14078,lina rojasbarahona,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We describe the acquisition of a dialog corpus for French based on multi-task human-machine interactions in a serious game setting. We present a tool for data collection that is configurable for multiple games; describe the data collected using this tool and the annotation schema used to annotate it; and report on the results obtained when training a classifier on the annotated data to associate each player turn with a dialog move usable by a rule based dialog manager. The collected data consists of approximately 1250 dialogs, 10454 utterances and 168509 words and will be made freely available to academic and nonprofit research."
denis-etal-2012-representation,Representation of linguistic and domain knowledge for second language learning in virtual worlds,2012,9,2,3,0,39028,alexandre denis,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"There has been much debate, both theoretical and practical, on how to link ontologies and lexicons in natural language processing (NLP) applications. In this paper, we focus on an application in which lexicon and ontology are used to generate teaching material. We briefly describe the application (a serious game for language learning). We then zoom in on the representation and interlinking of the lexicon and of the ontology. We show how the use of existing standards and of good practice principles facilitates the design of our resources while satisfying the expressivity requirements set by natural language generation."
C12-1123,Error Mining with Suspicion Trees: Seeing the Forest for the Trees,2012,12,5,2,1,6260,shashi narayan,Proceedings of {COLING} 2012,0,"In recent years, error mining approaches have been proposed to identify the most likely sources of errors in symbolic parsers and generators. However the techniques used generate a flat list of suspicious forms ranked by decreasing order of suspicion. We introduce a novel algorithm that structures the output of error mining into a tree (called, suspicion tree) highlighting the relationships between suspicious forms. We illustrate the impact of our approach by applying it to detect and analyse the most likely sources of failure in surface realisation; and we show how the suspicion tree built by our algorithm helps presenting the errors identified by error mining in a linguistically meaningful way thus providing better support for error analysis. The right frontier of the tree highlights the relative importance of the main error cases while the subtrees of a node indicate how a given error case divides into smaller more specific cases"
C12-1124,Structure-Driven Lexicalist Generation,2012,25,7,2,1,6260,shashi narayan,Proceedings of {COLING} 2012,0,"We present a novel algorithm for surface realisation with lexicalist grammars. In this algorithm, the structure of the input is used both top-down to constrain the selection of applicable rules and bottom-up to filter the initial search space associated with local input trees. In addition, parallelism is used to recursively pursue the realisation of each daught er node in the input tree. We evaluate the algorithm on the input data provided by the Generation Challenge Surface Realisation Task and show that it drastically reduce processing time when compared with a simpler, top-down driven, lexicalist approach. Title and Abstract in Hindi"
2011.jeptalnrecital-invite.3,"G{\\'e}n{\\'e}ration de phrase : entr{\\'e}e, algorithmes et applications (Sentence Generation: Input, Algorithms and Applications)",2011,-1,-1,1,1,850,claire gardent,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Conf{\\'e}rences invit{\\'e}es,0,
2011.jeptalnrecital-court.34,Vers la d{\\'e}tection des dislocations {\\`a} gauche dans les transcriptions automatiques du Fran{\\c{c}}ais parl{\\'e} (Towards automatic recognition of left dislocation in transcriptions of Spoken {F}rench),2011,-1,-1,3,0,44956,corinna anderson,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Ce travail prend place dans le cadre plus g{\'e}n{\'e}ral du d{\'e}veloppement d{'}une plate-forme d{'}analyse syntaxique du fran{\c{c}}ais parl{\'e}. Nous d{\'e}crivons la conception d{'}un mod{\`e}le automatique pour r{\'e}soudre le lien anaphorique pr{\'e}sent dans les dislocations {\`a} gauche dans un corpus de fran{\c{c}}ais parl{\'e} radiophonique. La d{\'e}tection de ces structures devrait permettre {\`a} terme d{'}am{\'e}liorer notre analyseur syntaxique en enrichissant les informations prises en compte dans nos mod{\`e}les automatiques. La r{\'e}solution du lien anaphorique est r{\'e}alis{\'e}e en deux {\'e}tapes : un premier niveau {\`a} base de r{\`e}gles filtre les configurations candidates, et un second niveau s{'}appuie sur un mod{\`e}le appris selon le crit{\`e}re du maximum d{'}entropie. Une {\'e}valuation exp{\'e}rimentale r{\'e}alis{\'e}e par validation crois{\'e}e sur un corpus annot{\'e} manuellement donne une F-mesure de l{'}ordre de 40{\%}."
gardent-lorenzo-2010-identifying,Identifying Sources of Weakness in Syntactic Lexicon Extraction,2010,12,0,1,1,850,claire gardent,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Previous work has shown that large scale subcategorisation lexicons could be extracted from parsed corpora with reasonably high precision. In this paper, we apply a standard extraction procedure to a 100 millions words parsed corpus of french and obtain rather poor results. We investigate different factors likely to improve performance such as in particular, the specific extraction procedure and the parser used; the size of the input corpus; and the type of frames learned. We try out different ways of interleaving the output of several parsers with the lexicon extraction process and show that none of them improves the results. Conversely, we show that increasing the size of the input corpus and modifying the extraction procedure to better differentiate prepositional arguments from prepositional modifiers improves performance. In conclusion, we suggest that a more sophisticated approach to parser combination and better probabilistic models of the various types of prepositional objects in French are likely ways to get better results."
bedaride-gardent-2010-syntactic,Syntactic Testsuites and Textual Entailment Recognition,2010,5,1,2,1,39732,paul bedaride,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We focus on textual entailments mediated by syntax and propose a new methodology to evaluate textual entailment recognition systems on such data. The main idea is to generate a syntactically annotated corpus of pairs of (non-)entailments and to use error mining methodology from the parsing field to identify the most likely sources of errors. To generate the evaluation corpus we use a template based generation approach where sentences, semantic representations and syntactic annotations are all created at the same time. Furthermore, we adapt the error mining methodology initially proposed for parsing to the field of textual entailment. To illustrate the approach, we apply the proposed methodology to the Afazio RTE system (an hybrid system focusing on syntactic entailment) and show how it permits identifying the most likely sources of errors made by this system on a testsuite of 10 000 (non-)entailment pairs which is balanced in term of (non-)entailment and in term of syntactic annotations."
C10-2006,Benchmarking for syntax-based sentential inference,2010,17,1,2,1,39732,paul bedaride,Coling 2010: Posters,0,We propose a methodology for investigating how well NLP systems handle meaning preserving syntactic variations. We start by presenting a method for the semi automated creation of a benchmark where entailment is mediated solely by meaning preserving syntactic variations. We then use this benchmark to compare a semantic role labeller and two grammar based RTE systems. We argue that the proposed methodology (i) supports a modular evaluation of the ability of NLP systems to handle the syntax/semantic interface and (ii) permits focused error mining and error analysis.
C10-2039,Comparing the performance of two {TAG}-based surface realisers using controlled grammar traversal,2010,16,4,1,1,850,claire gardent,Coling 2010: Posters,0,"We present GENSEM, a tool for generating input semantic representations for two sentence generators based on the same reversible Tree Adjoining Grammar. We then show how GENSEM can be used to produced large and controlled benchmarks and test the relative performance of these generators."
C10-1042,{RTG} based surface realisation for {TAG},2010,14,12,1,1,850,claire gardent,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Surface realisation with grammars integrating flat semantics is known to be NP complete. In this paper, we present a new algorithm for surface realisation based on Feature Based Tree Adjoining Grammar (FTAG) which draws on the observation that an FTAG can be translated into a Regular Tree Grammar describing its derivation trees. We carry out an extensive testing of several variants of this algorithm using an automatically produced testsuite and compare the results obtained with those obtained using GenI, another FTAG based surface realiser."
W09-3744,Semantic Normalisation : a Framework and an Experiment,2009,18,11,2,1,39732,paul bedaride,Proceedings of the Eight International Conference on Computational Semantics,0,"We present a normalisation framework for linguistic representations and illustrate its use by normalising the Stanford Dependency graphs (SDs) produced by the Stanford parser into Labelled Stanford Dependency graphs (LSDs). The normalised representations are evaluated both on a testsuite of constructed examples and on free text. The resulting representations improve on standard Predicate/Argument structures produced by SRL by combining role labelling with the semantically oriented features of SDs. Furthermore, the proposed normalisation framework opens the way to stronger normalisation processes which should be useful in reducing the burden on inference."
R09-1015,Grouping Synonyms by Definitions,2009,17,1,2,1,31336,ingrid falk,Proceedings of the International Conference {RANLP}-2009,0,"We present a method for grouping the synonyms of a lemma according to its dictionary senses. The senses are defined by a large machine read- able dictionary for French, the TLFi (Tresor de la langue francaise informatise) and the syn- onyms are given by 5 synonym dictionaries (also for French). To evaluate the proposed method, we manually constructed a gold standard where for each (word, definition) pair and given the set of synonyms defined for that word by the 5 synonym dictionaries, 4 lexicographers speci- fied the set of synonyms they judge adequate. While inter-annotator agreement ranges on that task from 67% to at best 88% depending on the annotator pair and on the synonym dictionary being considered, the automatic procedure we propose scores a precision of 67% and a recall of 71%. The proposed method is compared with related work namely, word sense disambiguation, synonym lexicon acquisition and WordNet con- struction."
2009.jeptalnrecital-long.21,"Sens, synonymes et d{\\'e}finitions",2009,10,0,2,1,31336,ingrid falk,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article d{\'e}crit une m{\'e}thodologie visant la r{\'e}alisation d{'}une ressource s{\'e}mantique en fran{\c{c}}ais centr{\'e}e sur la synonymie. De mani{\`e}re compl{\'e}mentaire aux travaux existants, la m{\'e}thode propos{\'e}e n{'}a pas seulement pour objectif d{'}{\'e}tablir des liens de synonymie entre lex{\`e}mes, mais {\'e}galement d{'}apparier les sens possibles d{'}un lex{\`e}me avec les ensembles de synonymes appropri{\'e}s. En pratique, les sens possibles des lex{\`e}mes proviennent des d{\'e}finitions du TLFi et les synonymes de cinq dictionnaires accessibles {\`a} l{'}ATILF. Pour {\'e}valuer la m{\'e}thode d{'}appariement entre sens d{'}un lex{\`e}me et ensemble de synonymes, une ressource de r{\'e}f{\'e}rence a {\'e}t{\'e} r{\'e}alis{\'e}e pour 27 verbes du fran{\c{c}}ais par quatre lexicographes qui ont sp{\'e}cifi{\'e} manuellement l{'}association entre verbe, sens (d{\'e}finition TLFi) et ensemble de synonymes. Relativement {\`a} ce standard {\'e}talon, la m{\'e}thode d{'}appariement affiche une F-mesure de 0.706 lorsque l{'}ensemble des param{\`e}tres est pris en compte, notamment la distinction pronominal / non-pronominal pour les verbes du fran{\c{c}}ais et de 0.602 sans cette distinction."
amoia-gardent-2008-test,A Test Suite for Inference Involving Adjectives,2008,17,5,2,1,16069,marilisa amoia,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Recently, most of the research in NLP has concentrated on the creation of applications coping with textual entailment. However, there still exist very few resources for the evaluation of such applications. We argue that the reason for this resides not only in the novelty of the research field but also and mainly in the difficulty of defining the linguistic phenomena which are responsible for inference. As the TSNLP project has shown test suites provide optimal diagnostic and evaluation tools for NLP applications, as contrary to text corpora they provide a deep insight in the linguistic phenomena allowing control over the data. Thus in this paper, we present a test suite specifically developed for studying inference problems shown by English adjectives. The construction of the test suite is based on the deep linguistic analysis and following classification of entailment patterns of adjectives and follows the TSNLP guidelines on linguistic databases providing a clear coverage, systematic annotation of inference tasks, large reusability and simple maintenance. With the design of this test suite we aim at creating a resource supporting the evaluation of computational systems handling natural language inference and in particular at providing a benchmark against which to evaluate and compare existing semantic analysers."
C08-1032,Integrating a Unification-Based Semantics in a Large Scale Lexicalised {T}ree {A}djoining {G}rammar for {F}rench,2008,10,12,1,1,850,claire gardent,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"In contrast to LFG and HPSG, there is to date no large scale Tree Adjoining Grammar (TAG) equiped with a compositional semantics. In this paper, we report on the integration of a unification-based semantics into a Feature-Based Lexicalised TAG for French consisting of around 6 000 trees. We focus on verb semantics and show how factorisation can be used to support a compact and principled encoding of the semantic information that needs to be associated with each of the verbal elementary trees. The factorisation is made possible by the use of XMG, a high-level linguistic formalism designed to specify and compile computational grammars and in particular, grammars based on non-local trees or tree descriptions."
2008.jeptalnrecital-long.2,R{\\'e}{\\'e}criture et D{\\'e}tection d{'}Implication Textuelle,2008,-1,-1,2,1,39732,paul bedaride,Actes de la 15{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,Nous pr{\'e}sentons un syst{\`e}me de normalisation de la variation syntaxique qui permet de mieux reconna{\^\i}tre la relation d{'}implication textuelle entre deux phrases. Le syst{\`e}me est {\'e}valu{\'e} sur une suite de tests comportant 2 520 paires test et les r{\'e}sultats montrent un gain en pr{\'e}cision par rapport {\`a} un syst{\`e}me de base variant entre 29.8 et 78.5 points la complexit{\'e} des cas consid{\'e}r{\'e}s.
W07-2306,Spotting Overgeneration Suspects,2007,6,7,1,1,850,claire gardent,Proceedings of the Eleventh {E}uropean Workshop on Natural Language Generation ({ENLG} 07),0,"We present a method for quickly spotting overgeneration suspects (i.e., likely cause of overgeneration) in hand-coded grammars. The method is applied to a medium size Tree Adjoining Grammar (TAG) for French and is shown to help reduce the number of outputs by 70% almost all of it being overgeneration."
W07-1430,A first order semantic approach to adjectival inference,2007,7,2,2,1,16069,marilisa amoia,Proceedings of the {ACL}-{PASCAL} Workshop on Textual Entailment and Paraphrasing,0,"As shown in the formal semantics literature, adjectives can display very different inferential patterns depending on whether they are intersective, privative, subsective or plain non-subsective. Moreover, many of these classes are often described using second order constructs. In this paper, we adopt Hobbs's ontologically promiscuous approach and present a first order treatment of adjective semantics which opens the way for a sophisticated treatment of adjectival inference. The approach was implemented and tested using first order automated reasoners."
P07-2004,{S}em{TAG}: a platform for specifying {T}ree {A}djoining {G}rammars and performing {TAG}-based Semantic Construction,2007,12,6,1,1,850,claire gardent,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"In this paper, we introduce SemTag, a free and open software architecture for the development of Tree Adjoining Grammars integrating a compositional semantics. SemTag differs from XTAG in two main ways. First, it provides an expressive grammar formalism and compiler for factorising and specifying TAGs. Second, it supports semantic construction."
P07-1042,A Symbolic Approach to Near-Deterministic Surface Realisation using {T}ree {A}djoining {G}rammar,2007,15,22,1,1,850,claire gardent,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"Surface realisers divide into those used in generation (NLG geared realisers) and those mirroring the parsing process (Reversible realisers). While the first rely on grammars not easily usable for parsing, it is unclear how the second type of realisers could be parameterised to yield from among the set of possible paraphrases, the paraphrase appropriate to a given generation context. In this paper, we present a surface realiser which combines a reversible grammar (used for parsing and doing semantic construction) with a symbolic means of selecting paraphrases."
2007.jeptalnrecital-poster.7,Une r{\\'e}alisateur de surface bas{\\'e} sur une grammaire r{\\'e}versible,2007,-1,-1,1,1,850,claire gardent,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"En g{\'e}n{\'e}ration, un r{\'e}alisateur de surface a pour fonction de produire, {\`a} partir d{'}une repr{\'e}sentation conceptuelle donn{\'e}e, une phrase grammaticale. Les r{\'e}alisateur existants soit utilisent une grammaire r{\'e}versible et des m{\'e}thodes statistiques pour d{\'e}terminer parmi l{'}ensemble des sorties produites la plus plausible ; soit utilisent des grammaires sp{\'e}cialis{\'e}es pour la g{\'e}n{\'e}ration et des m{\'e}thodes symboliques pour d{\'e}terminer la paraphrase la plus appropri{\'e}e {\`a} un contexte de g{\'e}n{\'e}ration donn{\'e}. Dans cet article, nous pr{\'e}sentons GENI, un r{\'e}alisateur de surface bas{\'e} sur une grammaire d{'}arbres adjoints pour le fran{\c{c}}ais qui r{\'e}concilie les deux approches en combinant une grammaire r{\'e}versible avec une s{\'e}lection symbolique des paraphrases."
2007.jeptalnrecital-long.16,"{S}em{TAG}, une architecture pour le d{\\'e}veloppement et l{'}utilisation de grammaires d{'}arbres adjoints {\\`a} port{\\'e}e s{\\'e}mantique",2007,-1,-1,1,1,850,claire gardent,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans cet article, nous pr{\'e}sentons une architecture logicielle libre et ouverte pour le d{\'e}veloppement de grammaires d{'}arbres adjoints {\`a} port{\'e}e s{\'e}mantique. Cette architecture utilise un compilateur de m{\'e}tagrammaires afin de faciliter l{'}extension et la maintenance de la grammaire, et int{\`e}gre un module de construction s{\'e}mantique permettant de v{\'e}rifier la couverture aussi bien syntaxique que s{\'e}mantique de la grammaire. Ce module utilise un analyseur syntaxique tabulaire g{\'e}n{\'e}r{\'e} automatiquement {\`a} partir de la grammaire par le syst{\`e}me DyALog. Nous pr{\'e}sentons {\'e}galement les r{\'e}sultats de l{'}{\'e}valuation d{'}une grammaire du fran{\c{c}}ais d{\'e}velopp{\'e}e au moyen de cette architecture."
2007.jeptalnrecital-long.31,{\\'E}valuer {SYNLEX},2007,-1,-1,3,1,31336,ingrid falk,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"SYNLEX est un lexique syntaxique extrait semi-automatiquement des tables du LADL. Comme les autres lexiques syntaxiques du fran{\c{c}}ais disponibles et utilisables pour le TAL (LEFFF, DICOVALENCE), il est incomplet et n{'}a pas fait l{'}objet d{'}une {\'e}valuation permettant de d{\'e}terminer son rappel et sa pr{\'e}cision par rapport {\`a} un lexique de r{\'e}f{\'e}rence. Nous pr{\'e}sentons une approche qui permet de combler au moins partiellement ces lacunes. L{'}approche s{'}appuie sur les m{\'e}thodes mises au point en acquisition automatique de lexique. Un lexique syntaxique distinct de SYNLEX est acquis {\`a} partir d{'}un corpus de 82 millions de mots puis utilis{\'e} pour valider et compl{\'e}ter SYNLEX. Le rappel et la pr{\'e}cision de cette version am{\'e}lior{\'e}e de SYNLEX sont ensuite calcul{\'e}s par rapport {\`a} un lexique de r{\'e}f{\'e}rence extrait de DICOVALENCE."
W06-1805,Adjective based inference,2006,10,10,2,1,16069,marilisa amoia,Proceedings of the Workshop {KRAQ}{'}06: Knowledge and Reasoning for Language Processing,0,"In this paper, we propose a fine grained classification of english adjectives geared at modeling the distinct inference patterns licensed by each adjective class. We show how it can be implemented in description logic and illustrate the predictions made by a series of examples. The proposal has been implemented using Description logic as a semantic representation language and the prediction verified using the DL theorem prover Racer."
W06-1513,Three Reasons to Adopt {TAG}-Based Surface Realisation,2006,-1,-1,1,1,850,claire gardent,Proceedings of the Eighth International Workshop on Tree Adjoining Grammar and Related Formalisms,0,None
W06-1516,"{S}em{TAG}, the {LORIA} toolbox for {TAG}-based Parsing and Generation",2006,13,2,3,0,37803,eric kow,Proceedings of the Eighth International Workshop on Tree Adjoining Grammar and Related Formalisms,0,"In this paper, we introduce SemTAG, a toolbox for TAG-based parsing and generation. This environment supports the development of wide-coverage grammars and differs from existing environments for TAG such as XTAG, in that it includes a semantic dimension. SemTAG is open-source and freely available."
P06-2032,Coreference Handling in {XMG},2006,14,2,1,1,850,claire gardent,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,We claim that existing specification languages for tree based grammars fail to adequately support identifier managment. We then show that XMG (eXtensible Meta-Grammar) provides a sophisticated treatment of identifiers which is effective in supporting a linguist-friendly grammar design.
2006.jeptalnrecital-long.11,Extraction d{'}information de sous-cat{\\'e}gorisation {\\`a} partir des tables du {LADL},2006,9,15,1,1,850,claire gardent,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Les tables du LADL (Laboratoire d{'}Automatique Documentaire et Linguistique) contiennent des donn{\'e}es {\'e}lectroniques extensives sur les propri{\'e}t{\'e}s morphosyntaxiques et syntaxiques des foncteurs syntaxiques du fran{\c{c}}ais (verbes, noms, adjectifs). Ces donn{\'e}es, dont on sait qu{'}elles sont n{\'e}cessaires pour le bon fonctionnement des syst{\`e}mes de traitement automatique des langues, ne sont cependant que peu utilis{\'e}es par les syst{\`e}mes actuels. Dans cet article, nous identifions les raisons de cette lacune et nous proposons une m{\'e}thode de conversion des tables vers un format mieux appropri{\'e} au traitement automatique des langues."
2006.jeptalnrecital-long.12,Int{\\'e}gration d{'}une dimension s{\\'e}mantique dans les grammaires d{'}arbres adjoints,2006,-1,-1,1,1,850,claire gardent,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans cet article, nous consid{\'e}rons un formalisme linguistique pour lequel l{'}int{\'e}gration d{'}information s{\'e}mantique dans une grammaire {\`a} large couverture n{'}a pas encore {\'e}t{\'e} r{\'e}alis{\'e}e {\`a} savoir, les grammaires d{'}arbres adjoints (Tree Adjoining Grammar ou TAG). Nous proposons une m{\'e}thode permettant cette int{\'e}gration et d{\'e}crivons sa mise en oeuvre dans une grammaire noyau pour le fran{\c{c}}ais. Nous montrons en particulier que le formalisme de sp{\'e}cification utilis{\'e}, XMG, (Duchier et al., 2004) permet une factorisation importante des donn{\'e}es s{\'e}mantiques facilitant ainsi le d{\'e}veloppement, la maintenance et le d{\'e}boggage de la grammaire."
W05-1605,Generating and Selecting Grammatical Paraphrases,2005,13,22,1,1,850,claire gardent,Proceedings of the Tenth {E}uropean Workshop on Natural Language Generation ({ENLG}-05),0,"Natural language has a high paraphrastic power yet not all paraphrases are appropriate for all contexts. In this paper, we present a TAG based surface realiser which supports both the generation and the selection of paraphrases. To deal with the combinatorial explosion typical of such an NP-complete task, we introduce a number of new optimisations in a tabular, bottom-up surface realisation algorithm. We then show that one of these optimisations supports paraphrase selection."
W04-0910,Paraphrastic grammars,2004,19,4,1,1,850,claire gardent,Proceedings of the 2nd Workshop on Text Meaning and Interpretation,0,"Arguably, grammars which associate natural language expressions not only with a syntactic but also with a semantic representation, should do so in a way that capture paraphrasing relations between sentences whose core semantics are equivalent. Yet existing semantic grammars fail to do so. In this paper, we describe an ongoing project whose aim is the production of a paraphrastic grammar that is, a grammar which associates paraphrases with identical semantic representations. We begin by proposing a typology of paraphrases. We then show how this typology can be used to simultaneously guide the development of a grammar and of a testsuite designed to support the evaluation of this grammar."
W03-2410,Which bridges for bridging definite descriptions?,2003,8,16,1,1,850,claire gardent,Proceedings of 4th International Workshop on Linguistically Interpreted Corpora ({LINC}-03) at {EACL} 2003,0,This paper presents a corpus study of bridging definite descriptions in the french corpus PAROLE. It proposes a typology of bridging relations; describes a system for annotating NPs which allows for a user friendly collection of all relevant information on the bridging definite descriptions occurring in the corpus and discusses the results of the corpus study
E03-1030,Semantic construction in {F}-{TAG},2003,0,18,1,1,850,claire gardent,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,None
P02-1013,Generating Minimal Definite Descriptions,2002,10,77,1,1,850,claire gardent,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"The incremental algorithm introduced in (Dale and Reiter, 1995) for producing distinguishing descriptions does not always generate a minimal description. In this paper, I show that when generalised to sets of individuals and disjunctive properties, this approach might generate unnecessarily long and ambiguous and/or epistemically redundant descriptions. I then present an alternative, constraint-based algorithm and show that it builds on existing related algorithms in that (i) it produces minimal descriptions for sets of individuals using positive, negative and disjunctive properties, (ii) it straightforwardly generalises to n-ary relations and (iii) it is integrated with surface realisation."
P01-1028,Generating with a Grammar Based on Tree Descriptions: a Constraint-Based Approach,2001,19,5,1,1,850,claire gardent,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"While the generative view of language processing builds bigger units out of smaller ones by means of rewriting steps, the axiomatic view eliminates invalid linguistic structures out of a set of possible structures by means of well formedness principles. We present a generator based on the axiomatic view and argue that when combined with a TAG-like grammar and a flat semantics, this axiomatic view permits avoiding drawbacks known to hold either of top-down or of bottom-up generators."
A00-2042,Understanding {``}Each Other{''},2000,6,10,1,1,850,claire gardent,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Although natural language is ambiguous, various linguistic and extra-linguistic factors often help determine a preferred reading. In this paper, we show that model generation can be used to model this process in the case of reciprocal statements. The proposed analysis builds on insights from Dalrymple et al. 98 and is shown to provide an integrated, computational account of the interplay between model theoretic interpretation, knowledge-based reasoning and preferences that characterises the interpretation of reciprocals."
P99-1007,Unifying Parallels,1999,19,5,1,1,850,claire gardent,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"I show that the equational treatment of ellipsis proposed in (Dalrymple et al., 1991) can further be viewed as modeling the effect of parallelism on semantic interpretation. I illustrate this claim by showing that the account straightforwardly extends to a general treatment of sloppy identity on the one hand, and to deaccented foci on the other. I also briefly discuss the results obtained in a prototype implementation."
W98-0113,Describing discourse semantics,1998,8,30,1,1,850,claire gardent,Proceedings of the Fourth International Workshop on Tree Adjoining Grammars and Related Frameworks ({TAG}+4),0,"Descriptions. In recent years, both formal and computational linguistics have been exploiting descriptions of structures where previously the structures themselves were used. The practice started with (Marcus et al., 1983), who demonstrated the value of (syntactic) tree descriptions for near-deterministic incremental parsing. Vijay-Shankar (Vijay-Shankar and Joshi, 1988; Vijay-Shankar, 1992) used descriptions to maintain the monotonicity of syntactic derivations in the framework of Feature-Based Tree Adjoining Grammar. In semantics, both (Muskens, 1997) and (Egg et al., 1997) have shown the value of descriptions as an underspecified representation of scope ambiguities. The current paper further extends the use of descriptions, from individual sentences to discourse, showing their benefit for incremental, near-deterministic discourse processing. In particular, we show that using descriptions to describe the semantic representation of discourse permits: (1) a monotone treatment of local ambiguity; (2) a deterministic treatment of global ambiguity; and (3) a distinction to be made between xe2x80x9csimplexe2x80x9d local ambiguity and xe2x80x9cgarden-pathxe2x80x9d local ambiguity."
P96-1001,Higher-Order Coloured Unification and Natural Language Semantics,1996,11,21,1,1,850,claire gardent,34th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we show that Higher-Order Coloured Unification - a form of unification developed for automated theorem proving - provides a general theory for modeling the interface between the interpretation process and other sources of linguistic, non semantic information. In particular, it provides the general theory for the Primary Occurrence Restriction which (Dalrymple et al., 1991)'s analysis called for."
C96-1073,Focus and Higher-Order Unification,1996,13,14,1,1,850,claire gardent,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"Pulman has shown that Higher-Order Unification (HOU) can be used to model the interpretation of focus. In this paper, we extend the unification-based approach to cases which are often seen as a test-bed for focus theory: utterances with multiple focus operators and second occurrence expressions. We then show that the resulting analysis favourably compares with two prominent theories of focus (namely, Rooth's Alternative Semantics and Krifka's Structured Meanings theory) in that it correctly generates interpretations which these alternative theories cannot yield. Finally, we discuss the formal properties of the approach and argue that even though HOU need not terminate, for the class of unification-problems dealt with in this paper, HOU avoids this shortcoming and is in fact computationally tractable."
E95-1006,A Specification Language for {L}exical {F}unctional {G}rammars,1995,4,8,2,1,2545,patrick blackburn,Seventh Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper defines a language xcexbb for specifying LFG grammars. This enables constraints on LFG's composite ontology (c-structures synchronised with f-structures) to be stated directly; no appeal to the LFG construction algorithm is needed. We use xcexbb to specify schemata annotated rules and the LFG uniqueness, completeness and coherence principles. Broader issues raised by this work are noted and discussed."
E93-1004,Talking About Trees,1993,10,48,2,1,2545,patrick blackburn,Sixth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"In this paper we introduce a modal language LT for imposing constraints on trees, and an extension LT (LF) for imposing constraints on trees decorated with feature structures. The motivation for introducing these languages is to provide tools for formalising grammatical frameworks perspicuously, and the paper illustrates this by showing how the leading ideas of GPSG can be captured in LT (LF).In addition, the role of modal languages (and in particular, what we have called layered modal languages) as constraint formalisms for linguistic theorising is discussed in some detail."
E93-1018,A unification-based approach to multiple {VP} Ellipsis resolution,1993,13,4,1,1,850,claire gardent,Sixth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"An assumption shared by many theories of discourse is that discourse structure constrains anaphora resolution (cf. [Grosz and Sidner 1986] for definite NPs, [Lascarides and Asher 1991], [Nakhimovsky 1988] for temporal anaphora, [Webber 1990] for deictic pronouns and [Gardent 1991], [Prust and Scha 1990] for VP ellipsis). The aim of this paper is (i) to show that this assumption also applies to multiple VP ellipsis (VPE), (ii) to argue that other levels of linguistics information (such as syntax and semantics) interact with discourse structure in determining multiple VPE acceptability and (iii) to make these intuitions precise by providing a unification-based account of multiple VPE resolution."
C90-3082,The General Architecture of Generation in {ACORD},1990,1,2,3,0,57152,dieter kohl,{COLING} 1990 Volume 3: Papers presented to the 13th International Conference on Computational Linguistics,0,"This paper describes the general architecture of generation in the ACORD project. The central module of this architecture is a planning component, which allows to plan single sentences as an answer to a KB query. The planner works for three different languages (English, French and German) and for sentence generators based on two different grammar formalisms (UCG for English and French, LFG for German) independent of the particular grammar or grammar formalism. It uses several knowledge sources of the ACORD system to make its decisions. The output of the planner is used for the language specific generators as well as for the update of information needed for pronoun resolution."
C90-2022,Generating from a Deep Structure,1990,4,4,1,1,850,claire gardent,{COLING} 1990 Volume 2: Papers presented to the 13th International Conference on Computational Linguistics,0,"Noncanonical semantic representations are representations which cannot be derived by some grammar G although they are semantically equivalent to representations which can be derived by G. This paper presents a generation algorithm which deals with noncanonical input. The proposed approach also enhances portability and language independence in that (i) linguistic decisions made by independent modules (e.g., planner, transfer component) can be communicated to the generator in a natural way and (ii) the same algorithm coupled with different grammars will yield sentences in the corresponding languages."
P89-1034,Efficient Parsing for {F}rench,1989,12,5,1,1,850,claire gardent,27th Annual Meeting of the Association for Computational Linguistics,1,"Parsing with categorial grammars often leads to problems such as proliferating lexical ambiguity, spurious parses and overgeneration. This paper presents a parser for French developed on an unification based categorial grammar (FG) which avoids these problems. This parser is a bottom-up chart parser augmented with a heuristic eliminating spurious parses. The unicity and completeness of parsing are proved."
E89-1034,{F}rench Order Without Order,1989,4,10,2,0,51492,gabriel bes,Fourth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"To account for the semi-free word order of French, Unification Categorial Grammar is extended in two ways. First, verbal valencies are contained in a set rather than in a list. Second, type-raised NP's are described as two-sided functors. The new framework does not overgenerate i.e., it accepts all and only the sentences which are grammatical. This follows partly from the elimination of false lexical ambiguities - i.e., ambiguities introduced in order to account for all the possible positions a word can be in within a sentence -- and partly from a system of features constraining the possible combinations."
