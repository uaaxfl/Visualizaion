2001.mtsummit-papers.21,P98-1006,0,0.0223657,"internal evaluation. Cross-system evaluations tend to be performed infrequently, and require system-independent evaluation metrics. System-internal evaluation, on the other hand, is customized for a particular MT architecture and needs to be performed on a regular basis, in order to provide feedback to system developers and linguists. Given the high cost in time and money that is associated with human evaluation, automating the evaluation process is crucial in system-internal evaluation (for related efforts, see also Corston-Oliver et al. (2001), Ringger et al. (2001), Bangalore et al (2000), Alshawi et al. (1998) and Su et al. (1992)). Ideally, an automated evaluation procedure should provide two kinds of information: raw numbers that can be used for quantitative analysis over time, and information that helps in qualitative error analysis. In this paper, we propose an automated system-internal evaluation procedure for transferred semantic representations that fits these desiderata. We present results from using this evaluation procedure on the multilingual Microsoft MT system, and show how this approach can be used for error analysis. In an MT system that transfers linguistic representations from a so"
2001.mtsummit-papers.21,W00-1401,0,0.0132715,"system-internal evaluation. Cross-system evaluations tend to be performed infrequently, and require system-independent evaluation metrics. System-internal evaluation, on the other hand, is customized for a particular MT architecture and needs to be performed on a regular basis, in order to provide feedback to system developers and linguists. Given the high cost in time and money that is associated with human evaluation, automating the evaluation process is crucial in system-internal evaluation (for related efforts, see also Corston-Oliver et al. (2001), Ringger et al. (2001), Bangalore et al (2000), Alshawi et al. (1998) and Su et al. (1992)). Ideally, an automated evaluation procedure should provide two kinds of information: raw numbers that can be used for quantitative analysis over time, and information that helps in qualitative error analysis. In this paper, we propose an automated system-internal evaluation procedure for transferred semantic representations that fits these desiderata. We present results from using this evaluation procedure on the multilingual Microsoft MT system, and show how this approach can be used for error analysis. In an MT system that transfers linguistic re"
2001.mtsummit-papers.21,P01-1020,1,0.681845,"systems falls into two broad categories: cross-system evaluation, and system-internal evaluation. Cross-system evaluations tend to be performed infrequently, and require system-independent evaluation metrics. System-internal evaluation, on the other hand, is customized for a particular MT architecture and needs to be performed on a regular basis, in order to provide feedback to system developers and linguists. Given the high cost in time and money that is associated with human evaluation, automating the evaluation process is crucial in system-internal evaluation (for related efforts, see also Corston-Oliver et al. (2001), Ringger et al. (2001), Bangalore et al (2000), Alshawi et al. (1998) and Su et al. (1992)). Ideally, an automated evaluation procedure should provide two kinds of information: raw numbers that can be used for quantitative analysis over time, and information that helps in qualitative error analysis. In this paper, we propose an automated system-internal evaluation procedure for transferred semantic representations that fits these desiderata. We present results from using this evaluation procedure on the multilingual Microsoft MT system, and show how this approach can be used for error analysi"
2001.mtsummit-papers.21,W01-1406,0,0.0310027,"m Japanese into English. Another example involves the lack of complete resolution of all syntactic relations into semantic primitives, where that kind of analysis proves extremely hard: a German &quot;NP + genitive NP&quot; construction like &quot;die Anordnung der Tabelle&quot; is analyzed at logical form in terms of a Possessor relation, whereas the English counterpart &quot;the position of the table&quot; is currently analyzed as an unspecified prepositional relation. In our MT architecture, alignments between logical form subgraphs of source and target language are identified in a training phase using aligned corpora (Menezes & Richardson 2001). These aligned subgraphs are stored in an example base. During the translation process, a sentence in the source language is analyzed and its logical form is mapped against the example base of logical form mappings into the target language. From the subgraphs retrieved from the example base, a target logical form is constructed which serves as input into a generation component to produce a target string. For our evaluation experiment, we automatically construct classifiers that distinguish two sets of logical forms. In one scenario, we compare logical forms from two different languages to ass"
2001.mtsummit-papers.21,2001.mtsummit-papers.68,0,0.0236344,"ategories: cross-system evaluation, and system-internal evaluation. Cross-system evaluations tend to be performed infrequently, and require system-independent evaluation metrics. System-internal evaluation, on the other hand, is customized for a particular MT architecture and needs to be performed on a regular basis, in order to provide feedback to system developers and linguists. Given the high cost in time and money that is associated with human evaluation, automating the evaluation process is crucial in system-internal evaluation (for related efforts, see also Corston-Oliver et al. (2001), Ringger et al. (2001), Bangalore et al (2000), Alshawi et al. (1998) and Su et al. (1992)). Ideally, an automated evaluation procedure should provide two kinds of information: raw numbers that can be used for quantitative analysis over time, and information that helps in qualitative error analysis. In this paper, we propose an automated system-internal evaluation procedure for transferred semantic representations that fits these desiderata. We present results from using this evaluation procedure on the multilingual Microsoft MT system, and show how this approach can be used for error analysis. In an MT system that"
2001.mtsummit-papers.21,C92-2067,0,0.0106942,"-system evaluations tend to be performed infrequently, and require system-independent evaluation metrics. System-internal evaluation, on the other hand, is customized for a particular MT architecture and needs to be performed on a regular basis, in order to provide feedback to system developers and linguists. Given the high cost in time and money that is associated with human evaluation, automating the evaluation process is crucial in system-internal evaluation (for related efforts, see also Corston-Oliver et al. (2001), Ringger et al. (2001), Bangalore et al (2000), Alshawi et al. (1998) and Su et al. (1992)). Ideally, an automated evaluation procedure should provide two kinds of information: raw numbers that can be used for quantitative analysis over time, and information that helps in qualitative error analysis. In this paper, we propose an automated system-internal evaluation procedure for transferred semantic representations that fits these desiderata. We present results from using this evaluation procedure on the multilingual Microsoft MT system, and show how this approach can be used for error analysis. In an MT system that transfers linguistic representations from a source language to a ta"
2001.mtsummit-papers.21,2001.mtsummit-ebmt.4,0,\N,Missing
2001.mtsummit-papers.21,C98-1006,0,\N,Missing
2003.jeptalnrecital-long.23,C00-1007,0,0.203733,"tations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent Smets, Gamon, Corston-Oliver and Ringger approaches use syntactic representations: FERGUS (Bangalore and Rambow 2000) and Halogen (Langkilde 2000, Langkilde-Geary 2002) use syntax trees as an intermediate representation to determine the optimal string output. The adaptation of German Amalgam to French has been discussed elsewhere (Smets et al. 2003). In this paper, we discuss French Amalgam in some detail by presenting two of the machine-learned models and a tool which allows the researcher to manually inspect the relevant training data for each model. In this way, the researcher can understand why the model makes the decisions it makes and can improve the model by adding relevant linguistic features. The re"
2003.jeptalnrecital-long.23,W02-2105,1,0.833986,". This paper presents the French implementation of Amalgam, a machine-learned sentence realization system. It presents in some detail two of the machine-learned models employed in Amalgam and shows how linguistic intuition and knowledge can be combined with statistical techniques to improve the performance of the models. Keywords – Mots Clés Réalisation de phrase, génération automatique, arbres de décision, français. Sentence realization, generation, machine-learning, decision trees, French. 1 Introduction Amalgam is a multilingual sentence realization system. Developed originally for German (Corston-Oliver et al. 2002, Gamon et al. 2002), it has been adapted to French (Smets et al. 2003). Amalgam maps a representation of propositional content (henceforth &quot;logical form&quot;) to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language"
2003.jeptalnrecital-long.23,P02-1004,1,0.641736,"ench implementation of Amalgam, a machine-learned sentence realization system. It presents in some detail two of the machine-learned models employed in Amalgam and shows how linguistic intuition and knowledge can be combined with statistical techniques to improve the performance of the models. Keywords – Mots Clés Réalisation de phrase, génération automatique, arbres de décision, français. Sentence realization, generation, machine-learning, decision trees, French. 1 Introduction Amalgam is a multilingual sentence realization system. Developed originally for German (Corston-Oliver et al. 2002, Gamon et al. 2002), it has been adapted to French (Smets et al. 2003). Amalgam maps a representation of propositional content (henceforth &quot;logical form&quot;) to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and"
2003.jeptalnrecital-long.23,W97-0908,1,0.823887,"task, such as determining the context for the insertion of the subordinate conjunctions que and si, to 1016 for the more difficult task of determining the label of a constituent. The ordering model stands apart from the others, with 4,536 branching nodes (for details see Ringger et al. (in preparation)). Smets, Gamon, Corston-Oliver and Ringger 2.2 Data and feature extraction The training data for all the models consist of a set of 100,000 sentences drawn from software manuals. The sentences are analyzed in the NLPWin system, which provides a syntactic and logical form analysis (Heidorn 2000; Gamon et al. 1997). Nodes in the logical form representation are linked to the corresponding syntactic nodes, allowing us to learn contexts for the mapping from the logical form representation to a surface syntax tree. The data is split 70/30 for training versus model parameter tuning. For each set of data we build decision trees at several different levels of granularity (by manipulating the prior probability of tree structures to favor simpler structures) and select the model with the maximal accuracy as determined on the parameter tuning set. We attempt to standardize as much as possible the set of features"
2003.jeptalnrecital-long.23,W02-2103,0,0.147526,"ations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent Smets, Gamon, Corston-Oliver and Ringger approaches use syntactic representations: FERGUS (Bangalore and Rambow 2000) and Halogen (Langkilde 2000, Langkilde-Geary 2002) use syntax trees as an intermediate representation to determine the optimal string output. The adaptation of German Amalgam to French has been discussed elsewhere (Smets et al. 2003). In this paper, we discuss French Amalgam in some detail by presenting two of the machine-learned models and a tool which allows the researcher to manually inspect the relevant training data for each model. In this way, the researcher can understand why the model makes the decisions it makes and can improve the model by adding relevant linguistic features. The researcher can thus leverage the richness of the ling"
2003.jeptalnrecital-long.23,W98-1426,0,0.428872,"algam maps a representation of propositional content (henceforth &quot;logical form&quot;) to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent Smets, Gamon, Corston-Oliver and Ringger approaches use syntactic representations: FERGUS (Bangalore and Rambow 2000) and Halogen (Langkilde 2000, Langkilde-Geary 2002) use syntax trees as an intermediate representation to determine the optimal string output. The adaptation of German Amalgam to French has been discussed elsewhere (Smets et al. 2003). In this paper, we discuss French Amalgam in some detail by presenting two of the machine-learned models and a tool which allows the researcher to manually inspect the relevant training data for each model. In this way, the"
2003.jeptalnrecital-long.23,P98-1116,0,0.0800119,"algam maps a representation of propositional content (henceforth &quot;logical form&quot;) to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent Smets, Gamon, Corston-Oliver and Ringger approaches use syntactic representations: FERGUS (Bangalore and Rambow 2000) and Halogen (Langkilde 2000, Langkilde-Geary 2002) use syntax trees as an intermediate representation to determine the optimal string output. The adaptation of German Amalgam to French has been discussed elsewhere (Smets et al. 2003). In this paper, we discuss French Amalgam in some detail by presenting two of the machine-learned models and a tool which allows the researcher to manually inspect the relevant training data for each model. In this way, the"
2003.jeptalnrecital-long.23,2003.jeptalnrecital-long.23,1,0.0512755,"entence realization system. It presents in some detail two of the machine-learned models employed in Amalgam and shows how linguistic intuition and knowledge can be combined with statistical techniques to improve the performance of the models. Keywords – Mots Clés Réalisation de phrase, génération automatique, arbres de décision, français. Sentence realization, generation, machine-learning, decision trees, French. 1 Introduction Amalgam is a multilingual sentence realization system. Developed originally for German (Corston-Oliver et al. 2002, Gamon et al. 2002), it has been adapted to French (Smets et al. 2003). Amalgam maps a representation of propositional content (henceforth &quot;logical form&quot;) to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizatio"
2003.jeptalnrecital-long.23,A00-2023,0,\N,Missing
2003.jeptalnrecital-long.23,C02-1036,1,\N,Missing
2003.jeptalnrecital-long.23,C98-1112,0,\N,Missing
2003.mtsummit-papers.48,C00-1007,0,0.0179195,"in the technical domain, are equal to the quality of highly respected commercial systems. We comment on the difficulties that are encountered when using machine-learned sentence realization in the context of MT. 1 Introduction Recently, statistical and machine-learning approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternate sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), HALogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. Amalgam, the sentence realization system introduced into our machine translation system maps a semantic representation to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the contexts for which are primarily machinelearned. The resulting syntax tree contains all the necessary information on its leaf nodes from which a surface string can be r"
2003.mtsummit-papers.48,W02-2105,1,0.847013,". We comment on the difficulties that are encountered when using machine-learned sentence realization in the context of MT. 1 Introduction Recently, statistical and machine-learning approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternate sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), HALogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. Amalgam, the sentence realization system introduced into our machine translation system maps a semantic representation to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the contexts for which are primarily machinelearned. The resulting syntax tree contains all the necessary information on its leaf nodes from which a surface string can be read. Our sentence realization system was first applied to German, although its architectur"
2003.mtsummit-papers.48,2003.mtsummit-papers.8,1,0.826588,"in features and feature combinations are to be more trusted than others and are more “solid” indicators - the machine-learned component can (and should) only rely on the information present in the training data. The result is that correct feature transfer becomes a larger issue in the transfer component, and that it also may prove useful to have a posttransfer and pre-generation component which specifically deals with feature noise, underspecification, and overspecification of linguistic features. We are currently experimenting with a machine-learned approach to this “clean-up” component (see Corston-Oliver and Gamon 2003). The basic idea is to learn a set of rules which set or delete linguistic features based on function word lemmas, the presence of other features in the structural neighborhood, parts-of-speech and semantic relations. Initial results indicate that this approach results in a statistically significant improvement in translation quality. 8 Conclusion It is well known that the development of each component in the creation of a quality MT system can be labor-intensive and time-consuming. In order to reduce the time and effort, we have incorporated the Amalgam sentence realization system, trained on"
2003.mtsummit-papers.48,P02-1004,1,0.816965,"realization system Amalgam takes as its input an LF graph. An example of a French logical form is given in Figure 2. Amalgam first degraphs the logical form into a tree and then augments it by the insertion of function words, assignment of various syntactic features, syntactic labels, etc., to produce an unordered syntax tree. Amalgam then establishes intra-constituent order. After syntactic aggregation, insertion of punctuation, morphological inflection, and capitalization, an output string is read off the leaf nodes. The contexts for most of these linguistic operations are machine-learned (Gamon et al. 2002a). Amalgam is based on a division of labor between linguistically motivated, knowledgeengineered linguistic operations, and their machine-learned contexts. A linguist decides which stages and operations are necessary for sentence realization. The contexts or “triggering environments” in which these operations apply are machine-learned as classifiers on events extracted from LFs and corresponding syntax trees. Even the ordering component is viewed as a classification task: daughter constituents of any given parent node are ordered from left-to-right with a classifier determining which of the y"
2003.mtsummit-papers.48,C02-1036,1,0.937207,"realization system Amalgam takes as its input an LF graph. An example of a French logical form is given in Figure 2. Amalgam first degraphs the logical form into a tree and then augments it by the insertion of function words, assignment of various syntactic features, syntactic labels, etc., to produce an unordered syntax tree. Amalgam then establishes intra-constituent order. After syntactic aggregation, insertion of punctuation, morphological inflection, and capitalization, an output string is read off the leaf nodes. The contexts for most of these linguistic operations are machine-learned (Gamon et al. 2002a). Amalgam is based on a division of labor between linguistically motivated, knowledgeengineered linguistic operations, and their machine-learned contexts. A linguist decides which stages and operations are necessary for sentence realization. The contexts or “triggering environments” in which these operations apply are machine-learned as classifiers on events extracted from LFs and corresponding syntax trees. Even the ordering component is viewed as a classification task: daughter constituents of any given parent node are ordered from left-to-right with a classifier determining which of the y"
2003.mtsummit-papers.48,A00-2023,0,0.0207652,"the quality of highly respected commercial systems. We comment on the difficulties that are encountered when using machine-learned sentence realization in the context of MT. 1 Introduction Recently, statistical and machine-learning approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternate sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), HALogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. Amalgam, the sentence realization system introduced into our machine translation system maps a semantic representation to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the contexts for which are primarily machinelearned. The resulting syntax tree contains all the necessary information on its leaf nodes from which a surface string can be read. Our sentence realiza"
2003.mtsummit-papers.48,W02-2103,0,0.0149242,"highly respected commercial systems. We comment on the difficulties that are encountered when using machine-learned sentence realization in the context of MT. 1 Introduction Recently, statistical and machine-learning approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternate sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), HALogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. Amalgam, the sentence realization system introduced into our machine translation system maps a semantic representation to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the contexts for which are primarily machinelearned. The resulting syntax tree contains all the necessary information on its leaf nodes from which a surface string can be read. Our sentence realization system was first a"
2003.mtsummit-papers.48,W98-1426,0,0.0218638,"of hand-written generation components. The resulting systems are evaluated by human evaluators, and in the technical domain, are equal to the quality of highly respected commercial systems. We comment on the difficulties that are encountered when using machine-learned sentence realization in the context of MT. 1 Introduction Recently, statistical and machine-learning approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternate sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), HALogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. Amalgam, the sentence realization system introduced into our machine translation system maps a semantic representation to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the contexts for which are primarily machinelearned. The resulting synt"
2003.mtsummit-papers.48,P98-1116,0,0.0294409,"of hand-written generation components. The resulting systems are evaluated by human evaluators, and in the technical domain, are equal to the quality of highly respected commercial systems. We comment on the difficulties that are encountered when using machine-learned sentence realization in the context of MT. 1 Introduction Recently, statistical and machine-learning approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternate sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), HALogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. Amalgam, the sentence realization system introduced into our machine translation system maps a semantic representation to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the contexts for which are primarily machinelearned. The resulting synt"
2003.mtsummit-papers.48,W01-1406,0,0.0207849,"chine Translation system The machine translation system described here operates from English as source language to German or French. It uses broad-coverage analyzers (the NLPWin system (Heidorn, 2000)) for those languages, large multi-purpose monolingual dictionaries, automatically generated bilingual dictionaries (English-French and EnglishGerman), the Amalgam sentence realization system, and a transfer component. The transfer component consists of transfer patterns automatically acquired from sentence-aligned bilingual corpora using an alignment grammar and algorithm described in detail in (Menezes & Richardson, 2001). Training takes place on aligned sentence pairs which have been analyzed by the source and target NLPWin analysis systems to yield sentence-level semantic dependency graphs (Logical Forms or LFs). The LFs have fixed lexical choices for content words and represent the predicate-argument structure of a sentence. LFs include semantic information concerning relations between nodes of the graph (Heidorn, 2000). The LF structures, when aligned, allow the extraction of lexical and structural translation correspondences which are stored for use at runtime in the transfer database. The transfer databa"
2003.mtsummit-papers.48,2001.mtsummit-papers.68,0,0.0688641,"Missing"
2003.mtsummit-papers.48,2003.jeptalnrecital-long.23,1,0.210535,"gam, the sentence realization system introduced into our machine translation system maps a semantic representation to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the contexts for which are primarily machinelearned. The resulting syntax tree contains all the necessary information on its leaf nodes from which a surface string can be read. Our sentence realization system was first applied to German, although its architecture was designed to be language-independent. It was subsequently adapted to French, as described in (Smets et al., 2003), and is currently being ported to English. The purpose of this paper is to present a full scale machine translation system using this sentence realization module and to report the quality of the resulting translations. We also review the results of testing only the sentence realization system (i.e., with native, not transferred input) and point out the issues that arise in the translation context. Work in progress to address the issues specific to translation is briefly outlined. 2 Overview of the Machine Translation system The machine translation system described here operates from English a"
2003.mtsummit-papers.48,2001.mtsummit-ebmt.4,0,\N,Missing
2003.mtsummit-papers.48,P02-1040,0,\N,Missing
2003.mtsummit-papers.48,C98-1112,0,\N,Missing
2003.mtsummit-papers.8,H92-1022,0,0.0151511,"red from English to German, attempting to make the combination of features on each node in the transferred LFs more closely resemble the LFs that result from analysis of native German sentences. 3. Transformation-based learning error-driven Transformation-based error-driven learning (Brill 1993a, 1995), commonly referred to as “transformation-based learning” or TBL, is an automatic machine learning technique. The output of TBL is an ordered list of rules whose application to data results in a reduction in error. The best-known application of TBL has been to the task of part-of-speech tagging (Brill 1992, 1994). TBL has also been applied to a number of diverse linguistic tasks such as resolving syntactic attachment ambiguities (Brill and Resnik 1994), syntactic parsing (Brill 1993b), and word sense disambiguation (Dini et al 1998). Figure 1 gives pseudo-code that describes the learning phase of transformation-based learning. As is customary, we explain the learning phase with respect to the task of part-of-speech tagging. An initial part-of-speech tag is assigned to each word, typically by choosing randomly among the parts-of-speech observed for each word, or by choosing the most commonly obs"
2003.mtsummit-papers.8,P93-1035,0,0.184079,"h as tense, aspect, definiteness, number, and person. Relations between nodes are indicated by labeled arcs. The LF normalizes surface syntactic alternations such as active/passive. The LF is described in more detail in (Heidorn 2000). In our experiments, we employ our approach to clean up features on LFs transferred from English to German, attempting to make the combination of features on each node in the transferred LFs more closely resemble the LFs that result from analysis of native German sentences. 3. Transformation-based learning error-driven Transformation-based error-driven learning (Brill 1993a, 1995), commonly referred to as “transformation-based learning” or TBL, is an automatic machine learning technique. The output of TBL is an ordered list of rules whose application to data results in a reduction in error. The best-known application of TBL has been to the task of part-of-speech tagging (Brill 1992, 1994). TBL has also been applied to a number of diverse linguistic tasks such as resolving syntactic attachment ambiguities (Brill and Resnik 1994), syntactic parsing (Brill 1993b), and word sense disambiguation (Dini et al 1998). Figure 1 gives pseudo-code that describes the learni"
2003.mtsummit-papers.8,J95-4004,0,0.778422,"c tasks such as resolving syntactic attachment ambiguities (Brill and Resnik 1994), syntactic parsing (Brill 1993b), and word sense disambiguation (Dini et al 1998). Figure 1 gives pseudo-code that describes the learning phase of transformation-based learning. As is customary, we explain the learning phase with respect to the task of part-of-speech tagging. An initial part-of-speech tag is assigned to each word, typically by choosing randomly among the parts-of-speech observed for each word, or by choosing the most commonly observed part-ofspeech for each word. Transformations consist of what Brill (1995:545) calls a “rewrite rule” such as “Change the tag from modal to noun” and a “triggering environment” such as “The preceding word is a determiner.” Assign an initial value to each data point to create data set, D Repeat Find the transformation Ti that gives the best reduction in errors in D If (ErrorReduction(Ti) ≥ Minimum) Add Ti to the ordered list of rules, R Apply Ti to all relevant cases in D End if Until (ErrorReduction(Ti) &lt; Minimum) Emit R Figure 1: Pseudo-code for TBL learning The learning phase is a greedy search. During each iteration the transformation that results in the greates"
2003.mtsummit-papers.8,C94-2195,0,0.0267007,"LFs that result from analysis of native German sentences. 3. Transformation-based learning error-driven Transformation-based error-driven learning (Brill 1993a, 1995), commonly referred to as “transformation-based learning” or TBL, is an automatic machine learning technique. The output of TBL is an ordered list of rules whose application to data results in a reduction in error. The best-known application of TBL has been to the task of part-of-speech tagging (Brill 1992, 1994). TBL has also been applied to a number of diverse linguistic tasks such as resolving syntactic attachment ambiguities (Brill and Resnik 1994), syntactic parsing (Brill 1993b), and word sense disambiguation (Dini et al 1998). Figure 1 gives pseudo-code that describes the learning phase of transformation-based learning. As is customary, we explain the learning phase with respect to the task of part-of-speech tagging. An initial part-of-speech tag is assigned to each word, typically by choosing randomly among the parts-of-speech observed for each word, or by choosing the most commonly observed part-ofspeech for each word. Transformations consist of what Brill (1995:545) calls a “rewrite rule” such as “Change the tag from modal to noun"
2003.mtsummit-papers.8,P98-1051,0,0.0482285,"Missing"
2003.mtsummit-papers.8,2001.mtsummit-papers.53,0,0.0276786,"ule must gracefully handle erroneous input. We present a hybrid machine-learning approach to resolving errors in transferred linguistic representations, using a combination of decision tree learning and transformation-based learning. The resulting rule set changes binary feature values, thus correcting feature based deficiencies. We evaluate the effectiveness of this hybrid approach at reducing noise in transferred linguistic representations known as logical forms (LFs), and then evaluate the impact of those changes on translations generated in an English-to-German machine translation system (Dolan et al. 2001). 2. The Microsoft Research Translation system (MSR-MT) Machine The experiments presented here were performed in the context of the MSR MT system (Dolan et al. 2001). The MSR-MT system is a data-driven translation system with knowledgeengineered analysis components. At training time, an aligned bitext is analyzed into Logical Form (LF) graphs. An alignment algorithm finds mappings between the LF of the source and target language (Menezes et al. 2001). The learned mappings are then stored in a bilingual transfer memory. At translation time, a sentence in the source language is analyzed to LF. T"
2003.mtsummit-papers.8,P00-1036,0,0.0291972,"Missing"
2003.mtsummit-papers.8,W01-1406,0,0.0917729,"Missing"
2003.mtsummit-papers.8,W94-0111,0,0.015883,"that describe a set of transformations to be tried. For example, in partof-speech tagging, the pretheoretical intuition is that resolving part-of-speech ambiguities can be achieved with reference to very local contexts only. Templates can constrain the search space by considering the part-of-speech and/or lexeme of each token within a fairly small window of tokens on either side of the position under consideration. For example, one template might describe triggering environments that consider the part-ofspeech of a word to the left of the current token and the lexeme of the word to the right. Ramshaw and Marcus (1994) show that with the right set of templates, TBL appears to be immune to overtraining. If irrelevant templates are added, however, overtraining is likely, especially near the end of the list of transformations. An example of an irrelevant template in part-of-speech tagging is a triggering environment five tokens removed from the current token, i.e. a token that is unlikely to be in a dependency relation to the current token. It is also conceivable that omitting a relevant rule template could lead to a degradation in tagging accuracy, since some relevant phenomena will not be captured. Thus, cur"
2003.mtsummit-papers.8,J95-2004,0,0.0174011,"textual probabilities emitted by a stochastic tagger. On the downside, the run-time performance of TBL can be prohibitive. Performance during the learning phase can be improved by indexing schemes, by sampling from the set of possible transformations, or by assuming independence among the transformations (Samuel 1998, Ngai and Florian 2001, Hepple 2000). Similarly, the application of sequences of learned rules can be improved by indexing schemes that eliminate the vacuous application of rules to new data (Satta and Brill 1996) and by compiling the list of rules into a finite state transducer (Roche and Schabes 1995). The most glaring deficiency of transformationbased learning, and the motivation for the technique described in this paper, is the lack of a mechanism for navigating the space of possible transformations. In practice, researchers have managed the search for transformations by specifying templates that describe a set of transformations to be tried. For example, in partof-speech tagging, the pretheoretical intuition is that resolving part-of-speech ambiguities can be achieved with reference to very local contexts only. Templates can constrain the search space by considering the part-of-speech a"
2003.mtsummit-papers.8,2001.mtsummit-ebmt.4,0,\N,Missing
2003.mtsummit-papers.8,N01-1006,0,\N,Missing
2003.mtsummit-papers.8,H93-1047,0,\N,Missing
2003.mtsummit-papers.8,P96-1034,0,\N,Missing
2003.mtsummit-papers.8,C98-1050,0,\N,Missing
2005.eamt-1.15,C02-1076,0,0.06586,"Missing"
2005.eamt-1.15,C04-1046,0,0.0544511,"Missing"
2005.eamt-1.15,P04-1079,0,0.0270579,"Missing"
2005.eamt-1.15,2001.mtsummit-papers.12,0,0.112745,"Missing"
2005.eamt-1.15,P01-1020,1,0.73803,"Missing"
2005.eamt-1.15,2003.mtsummit-papers.9,0,0.0530754,"Missing"
2005.eamt-1.15,J93-1003,0,0.0268274,"Missing"
2005.eamt-1.15,2003.mtsummit-papers.36,0,0.0772139,"Missing"
2005.eamt-1.15,P02-1040,0,0.101875,"Missing"
2005.eamt-1.15,2004.tmi-1.8,0,0.0641029,"Missing"
2005.eamt-1.15,C04-1072,0,0.0449883,"Missing"
2005.eamt-1.15,P04-1077,0,0.0245731,"Missing"
2005.eamt-1.15,quirk-2004-training,0,0.671304,"bstrings the machine-translated sentence has with the human reference translation(s), the better the translation is. It has been shown that BLEU scores, despite their shortcomings, correlate surprisingly well with human judgment (Coughlin 2003). The BLEU metric and its relatives are typically computed for fixed multi-sentence test sets in order to track the performance of MT systems over time and to compare different MT systems with respect to these test sets. EAMT 2005 Conference Proceedings Moving from multi-sentence evaluation to single-sentence and even word-level evaluation, Blatz et al. (2004) survey a number of approaches to the estimation of confidence. The training data for their experiments consist of machine translations and human reference translations. This work uses naive Bayes and multilayer perceptrons for classification. For the system-internal evaluation of deployed MT systems, Quirk (2004) uses a small (350 sentence) corpus of machine translations that have been annotated for translation quality by human annotators. He represents translated sentences as feature vectors, training a classifier to emulate the human scoring. The features he uses include sentence perplexity"
2005.eamt-1.15,richardson-2004-machine,0,0.0258555,"Missing"
2005.eamt-1.15,2003.mtsummit-papers.48,1,0.873319,"Missing"
2005.eamt-1.15,P04-1078,0,0.0224431,"Missing"
2005.eamt-1.15,C04-1088,1,\N,Missing
2020.semeval-1.98,2020.semeval-1.139,0,0.143912,"Missing"
2020.semeval-1.98,N16-1016,0,0.211578,"Headline Rating Interface. on humor has been progressing slowly but steadily. As an effort to boost research and Figure 1: The funny headline data annotation interfaces. spur new ideas in this challenging area, we When editing, only the underlined tokens are replaceable. created a competitive task for automatically assessing humor in edited news headlines. Like other AI tasks, automatic humor recognition depends on labeled data. Nearly all existing humor datasets are annotated to study the binary task of whether a piece of text is funny (Mihalcea and Strapparava, 2005; Kiddon and Brun, 2011; Bertero and Fung, 2016; Raz, 2012; Filatova, 2012; Zhang and Liu, 2014; Reyes et al., 2012; Barbieri and Saggion, 2014). Such categorical data does not capture the non-binary character of humor, which makes it difficult to develop models that can predict a level of funniness. Humor occurs in various intensities, and certain jokes are much funnier than others, including the supposedly funniest joke in the world (Wiseman, 2011). A system’s ability to assess the degree of humor makes it useful in various applications, such as in humor generation where such a system can be used in a generate-and-test scheme to generate"
2020.semeval-1.98,2020.semeval-1.131,0,0.0650994,"Missing"
2020.semeval-1.98,2020.semeval-1.105,0,0.0617746,"Missing"
2020.semeval-1.98,N19-1423,0,0.0333654,"d (or penalty) is higher for a correct classification (or misclassification). We ignore cases where the two edited versions of a headline have the same ground truth funniness. 4.2 Benchmarks We provide several benchmarks in Table 3 to compare against participating systems: Model 1. BASELINE: assigns the mean rating (Subtask 1) or the majority label (Subtask 2) from the training set. 2. CBOW: the context independent word representations obtained using the pretrained GloVe word vectors with 300d embeddings and a dictionary of 2.2M words. 3. BERT: a regressor based on BERT base model embeddings (Devlin et al., 2019). 4. RoBERTa: same regressor as above but uses RoBERTa embeddings (Liu et al., 2019). For a thorough discussion of these benchmarks, we refer the reader to the Duluth system (Jin et al., 2020), who performed these ablation experiments. In summary, each benchmark result uses the edited headline, CONTEXT implies using the headline’s context (with the replaced word substituted with [MASK]), ORIG implies using the original headline, FT refers to finetuning, FREEZE implies feature extraction (no finetuning) and FUNLINES refers to using the FunLines training data. The results for Subtask 2 were obta"
2020.semeval-1.98,2020.semeval-1.133,0,0.0679261,"Missing"
2020.semeval-1.98,2020.semeval-1.106,0,0.0830987,"Missing"
2020.semeval-1.98,filatova-2012-irony,0,0.0432005,"r has been progressing slowly but steadily. As an effort to boost research and Figure 1: The funny headline data annotation interfaces. spur new ideas in this challenging area, we When editing, only the underlined tokens are replaceable. created a competitive task for automatically assessing humor in edited news headlines. Like other AI tasks, automatic humor recognition depends on labeled data. Nearly all existing humor datasets are annotated to study the binary task of whether a piece of text is funny (Mihalcea and Strapparava, 2005; Kiddon and Brun, 2011; Bertero and Fung, 2016; Raz, 2012; Filatova, 2012; Zhang and Liu, 2014; Reyes et al., 2012; Barbieri and Saggion, 2014). Such categorical data does not capture the non-binary character of humor, which makes it difficult to develop models that can predict a level of funniness. Humor occurs in various intensities, and certain jokes are much funnier than others, including the supposedly funniest joke in the world (Wiseman, 2011). A system’s ability to assess the degree of humor makes it useful in various applications, such as in humor generation where such a system can be used in a generate-and-test scheme to generate many potentially humorous"
2020.semeval-1.98,2020.coling-main.253,1,0.764712,"mor, which makes it difficult to develop models that can predict a level of funniness. Humor occurs in various intensities, and certain jokes are much funnier than others, including the supposedly funniest joke in the world (Wiseman, 2011). A system’s ability to assess the degree of humor makes it useful in various applications, such as in humor generation where such a system can be used in a generate-and-test scheme to generate many potentially humorous texts and rank them by funniness, R for example, to automatically fill in the blanks in Mad Libs for humorous effects (Hossain et al., 2017; Garimella et al., 2020). For our SemEval task, we provided a dataset that contains news headlines with short edits applied to them to make them humorous (see Table 1). This dataset was annotated as described in Hossain et al. (2019) using Amazon Mechanical Turk, where qualified human workers edited headlines to make them funny and the quality of humor in these headlines was assessed by a separate set of qualified human judges on a 0-3 funniness scale (see Figure 1). This method of quantifying humor enables the development of systems for automatically estimating the degree of humor in text. Our task is comprised of t"
2020.semeval-1.98,D17-1067,1,0.886339,"binary character of humor, which makes it difficult to develop models that can predict a level of funniness. Humor occurs in various intensities, and certain jokes are much funnier than others, including the supposedly funniest joke in the world (Wiseman, 2011). A system’s ability to assess the degree of humor makes it useful in various applications, such as in humor generation where such a system can be used in a generate-and-test scheme to generate many potentially humorous texts and rank them by funniness, R for example, to automatically fill in the blanks in Mad Libs for humorous effects (Hossain et al., 2017; Garimella et al., 2020). For our SemEval task, we provided a dataset that contains news headlines with short edits applied to them to make them humorous (see Table 1). This dataset was annotated as described in Hossain et al. (2019) using Amazon Mechanical Turk, where qualified human workers edited headlines to make them funny and the quality of humor in these headlines was assessed by a separate set of qualified human judges on a 0-3 funniness scale (see Figure 1). This method of quantifying humor enables the development of systems for automatically estimating the degree of humor in text. O"
2020.semeval-1.98,N19-1012,1,0.725762,"in the world (Wiseman, 2011). A system’s ability to assess the degree of humor makes it useful in various applications, such as in humor generation where such a system can be used in a generate-and-test scheme to generate many potentially humorous texts and rank them by funniness, R for example, to automatically fill in the blanks in Mad Libs for humorous effects (Hossain et al., 2017; Garimella et al., 2020). For our SemEval task, we provided a dataset that contains news headlines with short edits applied to them to make them humorous (see Table 1). This dataset was annotated as described in Hossain et al. (2019) using Amazon Mechanical Turk, where qualified human workers edited headlines to make them funny and the quality of humor in these headlines was assessed by a separate set of qualified human judges on a 0-3 funniness scale (see Figure 1). This method of quantifying humor enables the development of systems for automatically estimating the degree of humor in text. Our task is comprised of two Subtasks: This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 746 Proceedings of the 14th International Works"
2020.semeval-1.98,2020.acl-demos.28,1,0.544829,"Missing"
2020.semeval-1.98,2020.semeval-1.104,0,0.085387,"Missing"
2020.semeval-1.98,2020.semeval-1.128,0,0.0339942,"rovide several benchmarks in Table 3 to compare against participating systems: Model 1. BASELINE: assigns the mean rating (Subtask 1) or the majority label (Subtask 2) from the training set. 2. CBOW: the context independent word representations obtained using the pretrained GloVe word vectors with 300d embeddings and a dictionary of 2.2M words. 3. BERT: a regressor based on BERT base model embeddings (Devlin et al., 2019). 4. RoBERTa: same regressor as above but uses RoBERTa embeddings (Liu et al., 2019). For a thorough discussion of these benchmarks, we refer the reader to the Duluth system (Jin et al., 2020), who performed these ablation experiments. In summary, each benchmark result uses the edited headline, CONTEXT implies using the headline’s context (with the replaced word substituted with [MASK]), ORIG implies using the original headline, FT refers to finetuning, FREEZE implies feature extraction (no finetuning) and FUNLINES refers to using the FunLines training data. The results for Subtask 2 were obtained by using the model trained for Subtask 1 to assign funniness ratings to both the edited versions of a headline and then choosing the version scoring higher. 4.3 Subtask 1 RMSE Subtask 2 A"
2020.semeval-1.98,E17-2068,0,0.092225,"Missing"
2020.semeval-1.98,2020.semeval-1.109,0,0.0806931,"Missing"
2020.semeval-1.98,2020.semeval-1.130,0,0.0645548,"Missing"
2020.semeval-1.98,P11-2016,0,0.124567,"unity, research (b) The Headline Rating Interface. on humor has been progressing slowly but steadily. As an effort to boost research and Figure 1: The funny headline data annotation interfaces. spur new ideas in this challenging area, we When editing, only the underlined tokens are replaceable. created a competitive task for automatically assessing humor in edited news headlines. Like other AI tasks, automatic humor recognition depends on labeled data. Nearly all existing humor datasets are annotated to study the binary task of whether a piece of text is funny (Mihalcea and Strapparava, 2005; Kiddon and Brun, 2011; Bertero and Fung, 2016; Raz, 2012; Filatova, 2012; Zhang and Liu, 2014; Reyes et al., 2012; Barbieri and Saggion, 2014). Such categorical data does not capture the non-binary character of humor, which makes it difficult to develop models that can predict a level of funniness. Humor occurs in various intensities, and certain jokes are much funnier than others, including the supposedly funniest joke in the world (Wiseman, 2011). A system’s ability to assess the degree of humor makes it useful in various applications, such as in humor generation where such a system can be used in a generate-and"
2020.semeval-1.98,2020.semeval-1.132,0,0.0605349,"Missing"
2020.semeval-1.98,2020.semeval-1.142,0,0.085319,"Missing"
2020.semeval-1.98,2020.semeval-1.108,0,0.0585982,"Missing"
2020.semeval-1.98,2020.semeval-1.137,0,0.0616793,"Missing"
2020.semeval-1.98,H05-1067,0,0.560793,"the artificial intelligence community, research (b) The Headline Rating Interface. on humor has been progressing slowly but steadily. As an effort to boost research and Figure 1: The funny headline data annotation interfaces. spur new ideas in this challenging area, we When editing, only the underlined tokens are replaceable. created a competitive task for automatically assessing humor in edited news headlines. Like other AI tasks, automatic humor recognition depends on labeled data. Nearly all existing humor datasets are annotated to study the binary task of whether a piece of text is funny (Mihalcea and Strapparava, 2005; Kiddon and Brun, 2011; Bertero and Fung, 2016; Raz, 2012; Filatova, 2012; Zhang and Liu, 2014; Reyes et al., 2012; Barbieri and Saggion, 2014). Such categorical data does not capture the non-binary character of humor, which makes it difficult to develop models that can predict a level of funniness. Humor occurs in various intensities, and certain jokes are much funnier than others, including the supposedly funniest joke in the world (Wiseman, 2011). A system’s ability to assess the degree of humor makes it useful in various applications, such as in humor generation where such a system can be"
2020.semeval-1.98,S17-2005,0,0.037013,"which emphasizes the need for unified, shared humor tasks. Recently, competitive humor tasks including shared data have been posed to the research community. One example is #HashtagWars (Potash et al., 2017), a SemEval task from 2017 that attracted eight distinct teams, where the focus was on ranking the funniness of tweets from a television show. The HAHA competition (Chiruzzo et al., 2019) had 18 participants who detected and rated humor in Spanish language tweets. There were 10 entries in a SemEval task from 2017 that looked at the automatic detection, location, and interpretation of puns (Miller et al., 2017). Finally, a related SemEval 2018 task involved irony detection in tweets (Van Hee et al., 2018). Ours is the largest shared humor task to date in terms of participation. More than 300 participants signed up, 86 teams participated in the development phase, and 48 and 31 teams participated, respectively, in the two subtasks in the evaluation phase. By creating an intense focus on the same humor task from so many points of view, we were able to clearly understand how well these systems work as a function of different dimensions of humor, including which type of humor appears easiest to rate auto"
2020.semeval-1.98,2020.semeval-1.107,0,0.0754385,"Missing"
2020.semeval-1.98,2020.semeval-1.101,0,0.43185,"ion. Next, we summarize the top systems and other notable approaches. 749 5.1 Reuse of SubTask 1 System for Subtask 2 First, we note that for Subtask 2, most systems relied on the model they developed for Subtask 1. This involved using the model to estimate a real number funniness rating for each of the two edited headlines, and selecting the one which achieved the higher estimated rating. As a result, there was a strong correlation between teams’ placements in Subtask 1 and Subtask 2, with the top 3 teams in both tasks being the same. 5.2 The Hitachi System The winner of both tasks, Hitachi (Morishita et al., 2020), formulated the problem as sentence pair regression and exploited an ensemble of the PLMs BERT, GPT-2, RoBERTa, XLNet, Transformer-XL and XLM. Their training data uses the pairs of headlines, with the replacement word marked with special tokens, and they fine-tune 50 instances per PLM, each having a unique hyperparameter setting. After applying 5-fold cross validation, they selected the 20 best performing settings per PLM, for a total of 700 PLMs (7 PLMs × 20 hyperparameters × 5 folds). They combined the predictions of these models via Ridge regression in the ensemble to predict final funnine"
2020.semeval-1.98,2020.semeval-1.140,0,0.0713657,"Missing"
2020.semeval-1.98,D14-1162,0,0.0857162,"Missing"
2020.semeval-1.98,N18-1202,0,0.018575,"Missing"
2020.semeval-1.98,S17-2004,0,0.165128,"ich one is funnier. Inviting multiple participants to a shared task contrasts with most current work on computational humor, which consists of standalone projects, each exploring a different genre or type of humor. Such projects typically involve gathering new humor data and applying machine learning to solve a particular problem. Repeated attempts at the same problem are rare, hindering incremental progress, which emphasizes the need for unified, shared humor tasks. Recently, competitive humor tasks including shared data have been posed to the research community. One example is #HashtagWars (Potash et al., 2017), a SemEval task from 2017 that attracted eight distinct teams, where the focus was on ranking the funniness of tweets from a television show. The HAHA competition (Chiruzzo et al., 2019) had 18 participants who detected and rated humor in Spanish language tweets. There were 10 entries in a SemEval task from 2017 that looked at the automatic detection, location, and interpretation of puns (Miller et al., 2017). Finally, a related SemEval 2018 task involved irony detection in tweets (Van Hee et al., 2018). Ours is the largest shared humor task to date in terms of participation. More than 300 pa"
2020.semeval-1.98,N12-2012,0,0.0301636,"ce. on humor has been progressing slowly but steadily. As an effort to boost research and Figure 1: The funny headline data annotation interfaces. spur new ideas in this challenging area, we When editing, only the underlined tokens are replaceable. created a competitive task for automatically assessing humor in edited news headlines. Like other AI tasks, automatic humor recognition depends on labeled data. Nearly all existing humor datasets are annotated to study the binary task of whether a piece of text is funny (Mihalcea and Strapparava, 2005; Kiddon and Brun, 2011; Bertero and Fung, 2016; Raz, 2012; Filatova, 2012; Zhang and Liu, 2014; Reyes et al., 2012; Barbieri and Saggion, 2014). Such categorical data does not capture the non-binary character of humor, which makes it difficult to develop models that can predict a level of funniness. Humor occurs in various intensities, and certain jokes are much funnier than others, including the supposedly funniest joke in the world (Wiseman, 2011). A system’s ability to assess the degree of humor makes it useful in various applications, such as in humor generation where such a system can be used in a generate-and-test scheme to generate many poten"
2020.semeval-1.98,2020.semeval-1.127,0,0.0342846,"PT-2, RoBERTa, XLNet, Transformer-XL and XLM. Their training data uses the pairs of headlines, with the replacement word marked with special tokens, and they fine-tune 50 instances per PLM, each having a unique hyperparameter setting. After applying 5-fold cross validation, they selected the 20 best performing settings per PLM, for a total of 700 PLMs (7 PLMs × 20 hyperparameters × 5 folds). They combined the predictions of these models via Ridge regression in the ensemble to predict final funniness scores. Hitachi uses the additional training data from FunLines. 5.3 The Amobee System Amobee (Rozental et al., 2020) was the 2nd placed team for both Subtasks. Using PLM token embeddings, they trained 30 instances of BERT, RoBERTa and XLNet, combining them for an ensemble of 90 models. 5.4 The YNU-HPCC System Unlike the top two systems, the 3rd placed YNU-HPCC (Tomasulo et al., 2020) employed an ensemble method that uses only the edited headlines. They used multiple pre-processing methods (e.g., cased vs uncased, with or without punctuation), and they encoded the edited headlines using FastText, Word2Vec, ELMo and BERT encoders. The final ensemble consists of 11 different encodings (four FastText, two W2V,"
2020.semeval-1.98,2020.semeval-1.136,0,0.030738,"placed YNU-HPCC (Tomasulo et al., 2020) employed an ensemble method that uses only the edited headlines. They used multiple pre-processing methods (e.g., cased vs uncased, with or without punctuation), and they encoded the edited headlines using FastText, Word2Vec, ELMo and BERT encoders. The final ensemble consists of 11 different encodings (four FastText, two W2V, four Bert, one ELMo). For each of these encodings, a bidirectional GRU was trained using the encoded vectors. In the ensemble, the GRU predictions were concatenated and fed to an XGBoost regressor. 5.4.1 MLEngineer The MLEngineer (Shatnawi et al., 2020) team also used only the edited headlines. They fine-tune and combine four BERT sentence regression models to estimate a rating, and they combine it with the estimated rating from a model that incorporates RoBERTa embeddings and a Na¨ıve Bayes regressor to generate the final rating. 5.5 Rank 1 2 3 4 5 6 bench. 7 8 9 10 bench. 11 12 13 14 15 16 17 18 bench. 19 20 21 22 23 24 25 26 27 28 29 30 31 32 bench. 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 Team Hitachi Amobee YNU-HPCC MLEngineer LMML ECNU RoBERTa LT3 WMD Ferryman zxchen BERT Duluth will go XSYSIGMA LRG MeisterMorxrc JUST Farah Lune"
2020.semeval-1.98,2020.semeval-1.138,0,0.067375,"Missing"
2020.semeval-1.98,2020.semeval-1.110,0,0.0347534,"ion, they selected the 20 best performing settings per PLM, for a total of 700 PLMs (7 PLMs × 20 hyperparameters × 5 folds). They combined the predictions of these models via Ridge regression in the ensemble to predict final funniness scores. Hitachi uses the additional training data from FunLines. 5.3 The Amobee System Amobee (Rozental et al., 2020) was the 2nd placed team for both Subtasks. Using PLM token embeddings, they trained 30 instances of BERT, RoBERTa and XLNet, combining them for an ensemble of 90 models. 5.4 The YNU-HPCC System Unlike the top two systems, the 3rd placed YNU-HPCC (Tomasulo et al., 2020) employed an ensemble method that uses only the edited headlines. They used multiple pre-processing methods (e.g., cased vs uncased, with or without punctuation), and they encoded the edited headlines using FastText, Word2Vec, ELMo and BERT encoders. The final ensemble consists of 11 different encodings (four FastText, two W2V, four Bert, one ELMo). For each of these encodings, a bidirectional GRU was trained using the encoded vectors. In the ensemble, the GRU predictions were concatenated and fed to an XGBoost regressor. 5.4.1 MLEngineer The MLEngineer (Shatnawi et al., 2020) team also used o"
2020.semeval-1.98,S18-1005,0,0.0570961,"Missing"
2020.semeval-1.98,2020.semeval-1.135,0,0.172119,"Missing"
2020.semeval-1.98,2020.semeval-1.141,0,0.0598323,"Missing"
2020.semeval-1.98,2020.semeval-1.129,0,0.0936789,"ak xenia Smash KdeHumor uir SO heidy Hasyarasa frietz58 SSN NLP RMSE 0.49725 0.50726 0.51737 0.51966 0.52027 0.52187 0.52207 0.52532 0.52603 0.52776 0.52886 0.53036 0.53108 0.53228 0.53308 0.53318 0.53383 0.53396 0.53518 0.53954 0.54242 0.54670 0.54754 0.54803 0.55115 0.55226 0.55391 0.55791 0.55838 0.56454 0.56829 0.56983 0.57237 0.57369 0.57470 0.57471 0.57471 0.57479 0.57488 0.57527 0.57768 0.57946 0.58157 0.58286 0.59202 0.61643 0.62401 0.65099 0.68338 0.70333 0.72252 0.84476 Table 4: Official results and benchmarks for Subtask 1. The LMML and ECNU Systems These systems (Ballapuram, 2020; Zhang et al., 2020) estimate the funniness of headlines using a neural architecture that focuses on the importance of the replaced and replacement words against the contextual 750 words in the headline. They use BERT embeddings and compute feature vectors based on the global attention between the contextual words and the replaced (and replacement) word. These two vectors and the vectors of the replaced and replacement are combined, and the resulting vector is passed through a multi-layer perceptron to estimate the headline’s funniness. 5.6 Other Notable Approaches Rank Team Accuracy Reward ECNU used sentiment an"
C02-1036,W02-2105,1,0.652431,"Missing"
C04-1088,N01-1025,0,0.0145172,"sor) Verb Tsub Pron Tobj Noun (a verbal node with a pronominal deep subject and a nominal deep object) Noun Locn Noun (a nominal node with a nominal modifier indicating location) As with the previously discussed features, we measure per-document frequency of the observed modification structures. There are a total of 9377 such structures. 3.6 dimensional hyperspace, separating the training cases into the target classes. SVMs have been used successfully in text categorization and in other classification tasks involving highly dimensional feature vectors (e.g. Joachims 1998, Dumais et al. 1998). Diederich et al. (2003) have applied support vector machines to the problem of authorship attribution. For our experiments we have used John Platt’s Sequential Minimal Optimization (SMO) tool (Platt 1999). In the absence of evidence for the usefulness of more complicated kernel functions in similar experiments (Diederich et al. 2003), we used linear SVMs exclusively. n-gram frequency features The use of word n-gram frequencies is not appropriate for style classification tasks since these features are not sufficiently content-independent. In our experiments, for example, they could pick up on nouns referring to event"
C04-1088,J93-1003,0,0.0349948,"Missing"
C04-1088,J00-4001,0,\N,Missing
C04-1097,W01-0808,0,0.0216418,"German and find that a particular conditional model outperforms all others. We employ a version of that model in an evaluation on unordered trees from the Penn TreeBank. We offer this result on standard data as a reference-point for evaluations of ordering in sentence realization. 1 Introduction Word and constituent order play a crucial role in establishing the fluency and intelligibility of a sentence. In some systems, establishing order during the sentence realization stage of natural language generation has been accomplished by hand-crafted generation grammars in the past (see for example, Aikawa et al. (2001) and Reiter and Dale (2000)). In contrast, the Nitrogen (Langkilde and Knight, 1998a, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection. Nitrogen’s model does not take into consideration any non-surface linguistic features available during realization. The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. Like Nitro"
C04-1097,C00-1007,0,0.177575,"Missing"
C04-1097,A00-2018,0,0.0602743,"stems operating on similar input, and (2) to measure Amalgam’s capabilities on less domain-specific data than technical software manuals. We derive from the bracketed tree structures in the PTB using a deterministic procedure an abstract representation we refer to as a Dependency Structure Input Format (DSIF), which is only loosely related to NLPWin’s abstract predicateargument structures. The PTB to DSIF transformation pipeline includes the following stages, inspired by Langkilde-Geary’s (2002b) description: A. Deserialize the tree B. Label heads, according to Charniak’s head labeling rules (Charniak, 2000) C. Remove empty nodes and flatten any remaining empty non-terminals D. Relabel heads to conform more closely to the head conventions of NLPWin E. Label with logical roles, inferred from PTB functional roles F. Flatten to maximal projections of heads (MPH), except in the case of conjunctions G. Flatten non-branching non-terminals H. Perform dictionary look-up and morphological analysis I. Introduce structure for material between paired delimiters and for any coordination not already represented in the PTB J. Remove punctuation K. Remove function words L. Map the head of each maximal projection"
C04-1097,W02-2105,1,0.738121,"n system (Langkilde, 2000; Langkilde-Geary, 2002a, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent. drojas@indiana.edu Our research is carried out within the Amalgam broad coverage sentence realization system. Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (Corston-Oliver et al., 2002; Gamon et al., 2002a, 2002b; Smets et al., 2003). Amalgam has an explicit ordering stage that determines the order of constituents and their daughters. The input for this stage is an unordered tree of constituents; the output is an ordered tree of constituents or a ranked list of such trees. For ordering, Amalgam leverages tree constituent structure and, importantly, features of those constituents and the surrounding context. By separately establishing order within constituents, Amalgam heavily constrains the possible alternatives in later stages of the realization process. The design allows"
C04-1097,W02-2102,0,0.0284067,"Missing"
C04-1097,P02-1004,1,0.834978,"ngkilde-Geary, 2002a, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent. drojas@indiana.edu Our research is carried out within the Amalgam broad coverage sentence realization system. Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (Corston-Oliver et al., 2002; Gamon et al., 2002a, 2002b; Smets et al., 2003). Amalgam has an explicit ordering stage that determines the order of constituents and their daughters. The input for this stage is an unordered tree of constituents; the output is an ordered tree of constituents or a ranked list of such trees. For ordering, Amalgam leverages tree constituent structure and, importantly, features of those constituents and the surrounding context. By separately establishing order within constituents, Amalgam heavily constrains the possible alternatives in later stages of the realization process. The design allows for interaction betw"
C04-1097,P03-1054,0,0.0343259,"Missing"
C04-1097,A00-2023,0,0.0212868,"0)). In contrast, the Nitrogen (Langkilde and Knight, 1998a, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection. Nitrogen’s model does not take into consideration any non-surface linguistic features available during realization. The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. Like Nitrogen, the HALogen system (Langkilde, 2000; Langkilde-Geary, 2002a, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent. drojas@indiana.edu Our research is carried out within the Amalgam broad coverage sentence realization system. Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (Corston-Oliver et al., 20"
C04-1097,W02-2103,0,0.294206,"the Nitrogen (Langkilde and Knight, 1998a, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection. Nitrogen’s model does not take into consideration any non-surface linguistic features available during realization. The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. Like Nitrogen, the HALogen system (Langkilde, 2000; Langkilde-Geary, 2002a, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent. drojas@indiana.edu Our research is carried out within the Amalgam broad coverage sentence realization system. Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (Corston-Oliver et al., 2002; Gamon et al., 2002a"
C04-1097,W98-1426,0,0.0250888,"e employ a version of that model in an evaluation on unordered trees from the Penn TreeBank. We offer this result on standard data as a reference-point for evaluations of ordering in sentence realization. 1 Introduction Word and constituent order play a crucial role in establishing the fluency and intelligibility of a sentence. In some systems, establishing order during the sentence realization stage of natural language generation has been accomplished by hand-crafted generation grammars in the past (see for example, Aikawa et al. (2001) and Reiter and Dale (2000)). In contrast, the Nitrogen (Langkilde and Knight, 1998a, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection. Nitrogen’s model does not take into consideration any non-surface linguistic features available during realization. The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. Like Nitrogen, the HALogen system (Langkilde, 2000; Langkilde-Geary, 2002a, 2002b) uses word"
C04-1097,P98-1116,0,0.095937,"e employ a version of that model in an evaluation on unordered trees from the Penn TreeBank. We offer this result on standard data as a reference-point for evaluations of ordering in sentence realization. 1 Introduction Word and constituent order play a crucial role in establishing the fluency and intelligibility of a sentence. In some systems, establishing order during the sentence realization stage of natural language generation has been accomplished by hand-crafted generation grammars in the past (see for example, Aikawa et al. (2001) and Reiter and Dale (2000)). In contrast, the Nitrogen (Langkilde and Knight, 1998a, 1998b) system employs a word n-gram language model to choose among a large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection. Nitrogen’s model does not take into consideration any non-surface linguistic features available during realization. The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. Like Nitrogen, the HALogen system (Langkilde, 2000; Langkilde-Geary, 2002a, 2002b) uses word"
C04-1097,2001.mtsummit-papers.68,0,0.0144316,"ultiple random scramblings and average the results. We use the evaluation metrics employed in published evaluations of HALogen, FUF/SURGE, and FERGUS (e.g., Calloway, 2003), although our results are for ordering only. Coverage, or the percentage of inputs for which a system can produce a corresponding output, is uninformative for the Amalgam system, since in all cases, it can generate an output for any given DSIF. In addition to processing time per input, we apply four other metrics: exact match, NIST simple string accuracy (the complement of the familiar word error rate), the IBM Bleu score (Papineni et al., 2001), and the intra-constituent edit distance metric introduced earlier. We evaluate against ideal trees, directly computed from PTB bracketed tree structures. The results in Table 2 show the effects of varying the IOCC parameter. For both trials involving a greedy search, the results were averaged across 25 iterations. As should be expected, turning on the input-output faithfulness option (IOCC) improves the performance of the greedy search. Keeping coordinated material in the same relative order would only be called for in applications that plan discourse structure before or during generation. 7"
C04-1097,2003.jeptalnrecital-long.23,1,0.608048,"uses word n-grams, but it extracts the best-scoring surface realizations efficiently from a packed forest by constraining the search first within the scope of each constituent. drojas@indiana.edu Our research is carried out within the Amalgam broad coverage sentence realization system. Amalgam generates sentence strings from abstract predicate-argument structures (Figure 1), using a pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (Corston-Oliver et al., 2002; Gamon et al., 2002a, 2002b; Smets et al., 2003). Amalgam has an explicit ordering stage that determines the order of constituents and their daughters. The input for this stage is an unordered tree of constituents; the output is an ordered tree of constituents or a ranked list of such trees. For ordering, Amalgam leverages tree constituent structure and, importantly, features of those constituents and the surrounding context. By separately establishing order within constituents, Amalgam heavily constrains the possible alternatives in later stages of the realization process. The design allows for interaction between ordering choices and othe"
C04-1097,P02-1040,0,\N,Missing
C04-1097,C98-1112,0,\N,Missing
C04-1121,J93-1003,0,0.0116864,"by elimination of sets of features (e.g. elimination of linguistic analysis features etc.) Experimenting with the elimination of feature sets provides an answer to the question as to which qualitative sets of features play a significant role in the classification task Of course these methods can also be combined, for example by eliminating sets of features and then taking the top ranking n features from the remaining set. We used both techniques (and their combinations) in our experiments. The measure of “predictiveness” we employed is log likelihood ratio with respect to the target variable (Dunning 1993). In the experiments described below, n (in the n top-ranked features) ranged from 1000 to 40,000. The different feature set combinations we used were: • “all features” • “no linguistic features” (only word ngrams) • “surface features” (word ngrams, function word frequencies and POS ngrams) • “linguistic features only” (no word ngrams) 4 Results Given the four different rankings associated by users with their feedback, we experimented with two distinct classification scenarios: 1. classification of documents as belonging to category 1 versus category 4 2. classification of documents as belongi"
C04-1121,C04-1088,1,0.20407,"igram. 4 An NP consisting of a pronoun followed by a punctuation character. 5 An adjectival semantic node modified by a verbal proposition and a pronominal subject. This is in fact the representation for a copular construction of the form “pronoun be adjective to verb...” as in “I am happy to report...” 2 likelihood ratio. A second, more surprising result is that the use of abstract linguistic analysis features consistently contributes to the classification accuracy in sentiment classification. While results like this have been reported in the area of style classification (Baayen et al. 1996, Gamon 2004), they are noteworthy in a domain where stylistic markers have not been considered in the past, indicating the need for more research into the stylistic correlations of affect in text. 6 Acknowledgements We thank Anthony Aue and Eric Ringger (Microsoft Research) and Hang Li (Microsoft Research Asia) for helpful comments and discussions, and Chris Moore (Microsoft Product Support Services UK) for the initial request for sentiment classification based on the needs of Support Services at Microsoft. Thanks also go to Karin Berghoefer of the Butler-Hill group for manually annotating a subset of the"
C04-1121,P02-1053,0,0.0126461,"this is not as straightforward as one may think, given that sentiment is often expressed in more subtle and indirect ways. The literature on sentiment classification can be divided into approaches that rely on semantic resources, such as a sentiment or affect lexicon (Nasukawa and Yi 2003, Subasic and Huettner 2001), or a large scale knowledge base (Liu et al 2003) on the one hand, and approaches that try to learn patterns directly from tagged data, without additional resources (Dave et al 2003, Pang et al. 2003). Much research is also being directed at acquiring affect lexica automatically (Turney 2002, Turney and Littman 2002). There is also a considerable amount of research on classification of text as “subjective” or “objective” (Wiebe et al 2001, Yu and Hatzivassiloglou 2003), a task that is not relevant for the processing of very brief pieces of direct customer feedback. In many studies, research on sentiment classification is conducted on review-type data, such as movie or restaurant reviews. These data often consist of relatively well-formed, coherent and at least paragraph-length pieces of text. The results we present in this paper are based on customer feedback data from web survey"
C04-1121,W03-1017,0,\N,Missing
C04-1121,W02-1011,0,\N,Missing
C14-1140,N09-1054,0,0.0706006,", which use user-specific click through rate (CTR). Although these applications and our task share the use of CTR as a supervision signal, there is a key difference: Whereas in web search CTR is used as a predictor/feature at runtime, our task specifically aims at predicting interestingness in the absence of web usage features: Our input is completely unstructured and there is no assumption of prior user interaction data. Use of probabilistic models: Our semantic model is built over LDA (Blei et al., 2003) and has resemblances to Link-LDA models (Erosheva et al., 2004) and Comment-LDA models (Yano et al., 2009). However, these are tailored for blogs and associated comment discussions which is very different from our source to destination browsing transition logs. Guo et al., (2009b) used probabilistic models for discovering entity classes from query logs and in (Lin et al., 2012), latent intents in entity centric search were explored. Gao et al. (2011) employ statistical machine translation to connect two types of content, learning semantic translation of queries to document titles. None of the above models, however, are directly applicable to the joint topic mappings that are involved in source to"
corston-oliver-gamon-2004-normalizing,J93-2003,0,\N,Missing
corston-oliver-gamon-2004-normalizing,C00-2162,0,\N,Missing
corston-oliver-gamon-2004-normalizing,C96-2141,0,\N,Missing
corston-oliver-gamon-2004-normalizing,C02-1036,1,\N,Missing
corston-oliver-gamon-2004-normalizing,J95-4004,0,\N,Missing
corston-oliver-gamon-2004-normalizing,J04-2003,0,\N,Missing
corston-oliver-gamon-2004-normalizing,J03-1002,0,\N,Missing
corston-oliver-gamon-2004-normalizing,P00-1056,0,\N,Missing
corston-oliver-gamon-2004-normalizing,W03-0305,0,\N,Missing
D14-1002,P14-1066,1,0.0924512,"de PLSA (Hofmann 1990) and LDA (Blei et al. 2003). Recently, these models have been extended to handle cross-lingual cases, where there are pairs of corresponding documents in different languages (e.g., Dumais et al. 1997; Gao et al. 2011; Platt et al. 2010; Yih et al. 2011). By exploiting deep architectures, deep learning techniques are able to automatically discover from training data the hidden structures and the associated features at different levels of abstraction useful for a variety of tasks (e.g., Collobert et al. 2011; Hinton et al. 2012; Socher et al. 2012; Krizhevsky et al., 2012; Gao et al. 2014). Hinton and Salakhutdinov (2010) propose the most original approach based on an unsupervised version of the deep neural network to discover the hierarchical semantic structure embedded in queries and documents. Huang et al. (2013) significantly extends the approach so that the deep neural network can be trained on large-scale query-document pairs giving much better performance. The use of the convolutional neural network for text processing, central to our DSSM, was also described in Collobert et al. (2011) and Shen et al. (2014) but with very different applications. The DSSM described in Sec"
D14-1002,J93-2003,0,0.0385454,"combination (Row 9) is obtained by incorporating the DSSM feature vectors of source and target documents (i.e., 600 features in total) in the ranker. We thus conclude that on both tasks, automatic highlighting and contextual entity search, features drawn from the output layers of our deep semantic model result in significant gains after being added to a set of non-semantic features, and in comparison to other types of semantic models used in the past. |is the unigram probability of word where | in , and is the probability of translating into , trained on source-target document pairs using EM (Brown et al. 1993). The translation-based approach allows any pair of non-identical but semantically related words to have a nonzero matching score. As a result, it significantly outperforms BM25. BTLM (Row 4) follows the best performing bilingual topic model described in Gao et al. (2011), which is an extension of PLSA (Hofmann 1999). The model is trained on source-target document pairs using the EM algorithm with a constraint enforcing a source document and its target document to not only share the same prior topic distribution, but to also have similar fractions of words assigned to each topic. BLTM defines"
D14-1002,C14-1140,1,0.829571,"Missing"
D14-1002,D10-1025,0,0.0126544,"for interestingness. The task of contextual entity search, which is formulated as an information retrieval problem in this paper, is also related to research on entity resolution (Stefanidis et al. 2013). Latent Semantic Analysis (Deerwester et al. 1990) is arguably the earliest semantic model designed for IR. Generative topic models widely used for IR include PLSA (Hofmann 1990) and LDA (Blei et al. 2003). Recently, these models have been extended to handle cross-lingual cases, where there are pairs of corresponding documents in different languages (e.g., Dumais et al. 1997; Gao et al. 2011; Platt et al. 2010; Yih et al. 2011). By exploiting deep architectures, deep learning techniques are able to automatically discover from training data the hidden structures and the associated features at different levels of abstraction useful for a variety of tasks (e.g., Collobert et al. 2011; Hinton et al. 2012; Socher et al. 2012; Krizhevsky et al., 2012; Gao et al. 2014). Hinton and Salakhutdinov (2010) propose the most original approach based on an unsupervised version of the deep neural network to discover the hierarchical semantic structure embedded in queries and documents. Huang et al. (2013) significa"
D14-1002,D12-1110,0,0.00801862,"erative topic models widely used for IR include PLSA (Hofmann 1990) and LDA (Blei et al. 2003). Recently, these models have been extended to handle cross-lingual cases, where there are pairs of corresponding documents in different languages (e.g., Dumais et al. 1997; Gao et al. 2011; Platt et al. 2010; Yih et al. 2011). By exploiting deep architectures, deep learning techniques are able to automatically discover from training data the hidden structures and the associated features at different levels of abstraction useful for a variety of tasks (e.g., Collobert et al. 2011; Hinton et al. 2012; Socher et al. 2012; Krizhevsky et al., 2012; Gao et al. 2014). Hinton and Salakhutdinov (2010) propose the most original approach based on an unsupervised version of the deep neural network to discover the hierarchical semantic structure embedded in queries and documents. Huang et al. (2013) significantly extends the approach so that the deep neural network can be trained on large-scale query-document pairs giving much better performance. The use of the convolutional neural network for text processing, central to our DSSM, was also described in Collobert et al. (2011) and Shen et al. (2014) but with very differ"
D14-1002,W11-0329,0,0.0767306,"eywords of local feature vectors, the document. Figure 3 presents a sample of document snippets and their keywords detected by the DSSM according to the procedure elaborated in Figure 2. It is interesting to see that many names are identified as keywords although the DSSM is not designed explicitly for named entity recognition. useful to the corresponding tasks, with a manageable vector size. tanh 1 where ces. 3.2 and tanh (3) tanh (4) are learned linear projection matriTraining the DSSM To optimize the parameters of the DSSM of Figure 1, i.e., , , , we use a pair-wise rank loss as objective (Yih et al. 2011). Consider a source document and two candidate target documents and , where is more interesting than to a user when reading . We construct and , , where two pairs of documents , the former is preferred and should have a higher (2) where the max operation is performed for each dimension of across 1, … , respectively. 5 … the comedy festival formerly known as the us comedy arts festival is a comedy festival held each year in las vegas nevada from its 1985 inception to 2008 . it was held annually at the wheeler opera house and other venues in aspen colorado . the primary sponsor of the festival w"
D14-1002,2011.iwslt-evaluation.19,0,\N,Missing
D14-1002,N09-1054,0,\N,Missing
D15-1174,D14-1165,0,0.0272354,"follows: ef (es ,r,eo ;Θ) f (es ,r,e0 ;Θ) e0 ∈N eg(es ,r,?) e p(eo |es , r; Θ) = P Conditional probabilities for subject entities p(es |eo , r; Θ) are defined analogously. Here Θ denotes all the parameters of latent features. The denominator is defined using a set of entities that do not fill the object position in any relation triple (es , r, ?) in the training knowledge graph. Since the number of such entities is impractically large, we sample negative triples from the full set. We also limit the candidate entities to ones that have types consistent with the position in the relation triple (Chang et al., 2014; Yang et al., 2015), where the types are approximated following Toutanova and Chen (2015). Additionally, since the task of predicting textual relations is auxiliary to the main task, we use a weighting factor τ for the loss on predicting the arguments of textual relations (Toutanova and Chen, 2015). Denote T as a set of triples, we define the loss L(T ; Θ) as: X L(T ; Θ) = − log p(eo |es , r; Θ) (es ,r,eo )∈T − log p(es |eo , r; Θ) (es ,r,eo )∈T Let TKB and Ttext represent the set of knowledge base triples and textual relation triples respectively. The final training loss function is de2 All"
D15-1174,D13-1080,0,0.025555,"from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual cooccurrences of entity pairs and the expressed textua"
D15-1174,D14-1044,0,0.362479,"ss of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual cooccurrences of entity pairs and the expressed textual relations. Weston et"
D15-1174,D15-1205,0,0.0233037,"use scoring function f (es , r, eo ) to represent the model’s confidence in the existence of the triple. We present the models and then the loss function used to train Continuous representations for supervised relation extraction In contrast to the work reviewed so far, work on sentence-level relation extraction using direct supervision has focused heavily on representing sentence context. Models using hand-crafted features have evolved for more than a decade, and recently, models using continuous representations have been found to achieve new state-of-the-art performance (Zeng et al., 2014; Gormley et al., 2015). Compared to work on representation learning for sentence-level context, such as this recent work using LSTM models on constituency or dependency trees (Tai et al., 2015), our approach using a one-hidden-layer convolutional neural network is relatively simple. However, even such a simple approach has been shown to be very competitive (Kim, 2014). 3 1501 nsubj r tween entities and the subject and object positions of relations. For each relation type r, the model learns two latent feature vectors v(rs ) and v(ro ) of dimension K. For each entity (node) ei , the model also learns a latent featur"
D15-1174,P11-1055,0,0.224699,"of textual relations. 1 http://lemurproject.org/clueweb12/ FACC1/ Relation extraction using distant supervision A number of works have focused on extracting new instances of relations using information from textual mentions, without sophisticated modeling of prior knowledge from the knowledge base. Mintz et al. (2009) demonstrated that both surface context and dependency path context were helpful for the task, but did not model the compositional sub-structure of this context. Other work proposed more sophisticated models that reason about sentence-level hidden variables (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) or model the noise arising from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of te"
D15-1174,D14-1181,0,0.00486836,"Missing"
D15-1174,D11-1049,0,0.132637,"ithout requiring sentence-level annotations of textual mentions at training time. We group such related work into three groups based on whether KB, text, or both sources of information are used. Additionally, we discuss related work in the area of supervised relation extraction using continuous representations of text, even though we do not use supervision at the level of textual mentions. Knowledge base completion Nickel et al. (2015) provide a broad overview of machine learning models for knowledge graphs, including models based on observed graph features such as the path ranking algorithm (Lao et al., 2011), models based on continuous representations (latent features), and model combinations (Dong et al., 2014). These models predict new facts in a given knowledge base, based on information from existing entities and relations. From this line of work, most relevant to our study is prior work evaluating continuous representation models on the FB15k dataset. Yang et al. (2015) showed that a simple variant of a bilinear model D IST M ULT outperformed T RANS E (Bordes et al., 2013) and more richly parameterized models on this dataset. We therefore build upon the best performing prior model D IST M UL"
D15-1174,D12-1093,0,0.0611575,"iables (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) or model the noise arising from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the"
D15-1174,P09-1113,0,0.533576,"ed models on this dataset. We therefore build upon the best performing prior model D IST M ULT from this line of work, as well as additional models E and F developed in the context of text-augmented knowledge graphs (Riedel et al., 2013), and extend them to incorporate compositional representations of textual relations. 1 http://lemurproject.org/clueweb12/ FACC1/ Relation extraction using distant supervision A number of works have focused on extracting new instances of relations using information from textual mentions, without sophisticated modeling of prior knowledge from the knowledge base. Mintz et al. (2009) demonstrated that both surface context and dependency path context were helpful for the task, but did not model the compositional sub-structure of this context. Other work proposed more sophisticated models that reason about sentence-level hidden variables (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) or model the noise arising from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowled"
D15-1174,P15-1016,0,0.389139,"rmation was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual cooccurrences of entity pairs and the expressed textual relations. Weston et al. (2013) combined continuous representations from a knowledge base and textual mentions for prediction of new relations. The two representations were trained independently of each other and using different loss functions, and were only combined at inference time. Additionally, the employed representations of text were non-composit"
D15-1174,N13-1008,0,0.290368,"of Text and Knowledge Bases Kristina Toutanova Microsoft Research Redmond, WA, USA Danqi Chen∗ Computer Science Department Stanford University Patrick Pantel Microsoft Research Redmond, WA, USA Hoifung Poon Microsoft Research Redmond, WA, USA Pallavi Choudhury Microsoft Research Redmond, WA, USA Michael Gamon Microsoft Research Redmond, WA, USA Abstract Knowledge Base Models that learn to represent textual and knowledge base relations in the same continuous latent space are able to perform joint inferences among the two kinds of relations and obtain high accuracy on knowledge base completion (Riedel et al., 2013). In this paper we propose a model that captures the compositional structure of textual relations, and jointly optimizes entity, knowledge base, and textual relation representations. The proposed model significantly improves performance over a model that does not share parameters among textual relations with common sub-structure. 1 place_of_birth Honolulu Barack Obama city_of United States nationality Textual Mentions Barack Obama is the 44th and current President of United States. Obama was born in the United States just as he has always said. … ClueWeb Figure 1: A knowledge base fragment cou"
D15-1174,Q13-1030,0,0.00682238,"cused on extracting new instances of relations using information from textual mentions, without sophisticated modeling of prior knowledge from the knowledge base. Mintz et al. (2009) demonstrated that both surface context and dependency path context were helpful for the task, but did not model the compositional sub-structure of this context. Other work proposed more sophisticated models that reason about sentence-level hidden variables (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) or model the noise arising from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or cont"
D15-1174,D12-1042,0,0.309075,"1 http://lemurproject.org/clueweb12/ FACC1/ Relation extraction using distant supervision A number of works have focused on extracting new instances of relations using information from textual mentions, without sophisticated modeling of prior knowledge from the knowledge base. Mintz et al. (2009) demonstrated that both surface context and dependency path context were helpful for the task, but did not model the compositional sub-structure of this context. Other work proposed more sophisticated models that reason about sentence-level hidden variables (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) or model the noise arising from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising"
D15-1174,P15-1150,0,0.0210913,"uous representations for supervised relation extraction In contrast to the work reviewed so far, work on sentence-level relation extraction using direct supervision has focused heavily on representing sentence context. Models using hand-crafted features have evolved for more than a decade, and recently, models using continuous representations have been found to achieve new state-of-the-art performance (Zeng et al., 2014; Gormley et al., 2015). Compared to work on representation learning for sentence-level context, such as this recent work using LSTM models on constituency or dependency trees (Tai et al., 2015), our approach using a one-hidden-layer convolutional neural network is relatively simple. However, even such a simple approach has been shown to be very competitive (Kim, 2014). 3 1501 nsubj r tween entities and the subject and object positions of relations. For each relation type r, the model learns two latent feature vectors v(rs ) and v(ro ) of dimension K. For each entity (node) ei , the model also learns a latent feature vector of the same dimensionality. The score of a candidate triple (es , r, eo ) is defined as f (es , r, eo ) = v(rs ) |v(es ) + v(ro ) |v(eo ). It can be seen that whe"
D15-1174,W15-4007,1,0.610739,"odel this sub-structure and share parameters among related dependency paths, using a unified loss function learning entity and relation representations to maximize performance on the knowledge base link prediction task. We evaluate our approach on the FB15k-237 dataset, a knowledge base derived from the Free1499 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1499–1509, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. base subset FB15k (Bordes et al., 2013) and filtered to remove highly redundant relations (Toutanova and Chen, 2015). The knowledge base is paired with textual mentions for all entity pairs derived from ClueWeb121 with Freebase entity mention annotations (Gabrilovich et al., 2013). We show that using a convolutional neural network to derive continuous representations for textual relations boosts the overall performance on link prediction, with larger improvement on entity pairs that have textual mentions. 2 Related Work There has been a growing body of work on learning to predict relations between entities without requiring sentence-level annotations of textual mentions at training time. We group such relat"
D15-1174,P10-1040,0,0.0350795,"rformance. L-BFGS (Liu and Nocedal, 1989) and RProp (Riedmiller and Braun, 1993) were found to converge to similar function values, with RProp converging significantly faster. We thus used RProp for optimization. We initialized the KB+text models from the KB-only models and also from random initial values (sampled from a Gaussian distribution), and stopped optimization when the overall MRR on the validation set decreased. For each model type, we chose the better of random and KB-only initialization. The word embeddings in the C ONV models were initialized using the 50-dimensional vectors from Turian et al. (2010) in the main experiments, with a slight positive impact. The effect of initialization is discussed at the end of the section. The number of negative examples for each triple was set to 200. Performance improved substantially when the number of negative examples was increased and reached a plateau around 200. We chose the optimal number of latent feature dimensions via a grid search to optimize MRR on the validation set, testing the values 5, 10, 15, 35, 50, 100, 200 and 500. We also performed a grid search over the values of the parameter τ , testing values in the set {0.01, 0.1, 0.25, 0.5, 1}"
D15-1174,D14-1167,0,0.541781,"outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual cooccurrences of entity pairs and the expressed textual relations. Weston et al. (2013) combined continuous representations from a knowledge base and textual mentions for prediction of new relations. The two representations were trained independently of each other and using different loss functions, and were only combined at inference time. Additionally, the employed representations of text were non-compositional. In this work"
D15-1174,D13-1136,0,0.0553672,"al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual cooccurrences of entity pairs and the expressed textual relations. Weston et al. (2013) combined continuous representations from a knowledge base and textual mentions for prediction of new relations. The two representations were trained independently of each other and using different loss functions, and were only combined at inference time. Additionally, the employed representations of text were non-compositional. In this work we train continuous representations of knowledge base and textual relations jointly, which allows for deeper interactions between the 1500 sources of information. We directly build on the universal schema approach of Riedel et al. (2013) as well as the uni"
D15-1174,C14-1220,0,0.0333623,"triple. The models use scoring function f (es , r, eo ) to represent the model’s confidence in the existence of the triple. We present the models and then the loss function used to train Continuous representations for supervised relation extraction In contrast to the work reviewed so far, work on sentence-level relation extraction using direct supervision has focused heavily on representing sentence context. Models using hand-crafted features have evolved for more than a decade, and recently, models using continuous representations have been found to achieve new state-of-the-art performance (Zeng et al., 2014; Gormley et al., 2015). Compared to work on representation learning for sentence-level context, such as this recent work using LSTM models on constituency or dependency trees (Tai et al., 2015), our approach using a one-hidden-layer convolutional neural network is relatively simple. However, even such a simple approach has been shown to be very competitive (Kim, 2014). 3 1501 nsubj r tween entities and the subject and object positions of relations. For each relation type r, the model learns two latent feature vectors v(rs ) and v(ro ) of dimension K. For each entity (node) ei , the model also"
D19-1505,E12-1036,0,0.0214156,"on the large-scale ImageNet competition by a significant margin over shallow... Table 1: Example of an edit and its associated comment. The added words “Sutskever and” in post-edit version is marked in red. ally, in an ablation study we demonstrate that our various modeling choices, which tackle the inherent challenges of comment-edit understanding, each contribute positively to empirical results. 2 Related Work Document revisions have been the subject of several studies in recent years (Nunes et al., 2011; Fischer, 2013). Most prior work focuses on modeling document edits only. For instance, Bronner and Monz (2012) build a classifier to distinguish fluency edits from factual edits. Zhu et al. (2017) study the semantic distance between the content in different versions of documents to detect document revisions. Grossman et al. (2013) propose a hierarchical navigation method to display document revision histories. Some work utilizes comments associated with document edits as supplementary information to study the document revisions. For example, Yatskar et al. (2010) consider both comment and document revision for lexical simplification. However, they use comments as meta-data to identify trusted revision"
D19-1505,D17-1070,0,0.0267665,"CG Cosine-TfIdf Cosine-InferSent RankNet LambdaMART Gated RNN 0.266 0.228 0.257 0.310 0.283 0.519 0.597 0.658 0.726 0.716 0.470 0.471 0.503 0.549 0.531 0.597 0.600 0.620 0.661 0.647 0.137 0.115 0.034 0.152 0.158 0.291 0.305 0.055 0.384 0.411 0.444 0.497 0.061 0.593 0.628 0.296 0.302 0.139 0.352 0.363 0.453 0.461 0.320 0.502 0.511 CmntEdit-MT CmntEdit-CR 0.674 0.710 0.930 0.944 0.804 0.828 0.853 0.871 0.529 0.593 0.780 0.830 0.896 0.924 0.680 0.730 0.758 0.796 Table 3: Performance on Comment Ranking comment and edit vectors generated by the state-of-the-art sentence embedding method InferSent (Conneau et al., 2017). (iii) RankNet (Burges et al., 2005) minimizes the number of inversions in ranking by optimizing a pair– wise loss function. We use a previous neural network implementation4 with default settings. (iv) LambdaMART (Burges, 2010) leverages gradient boosted decision trees with a cost function derived from LambdaRank (Burges et al., 2007) for solving a ranking task. We use an existing python implementation5 , with a learning rate of 0.02 and 10 max leaf nodes. (v) Gated Recurrent Neural Network (Chung et al., 2014) models the sequence of words in comments and edits using pre-trained GloVe vectors"
D19-1505,P09-2044,0,0.0200689,"ith document edits as supplementary information to study the document revisions. For example, Yatskar et al. (2010) consider both comment and document revision for lexical simplification. However, they use comments as meta-data to identify trusted revisions, rather than directly modeling the relationship between comments and edits. Yang et al. (2017) featurize both comments and revisions to classify edit intent, but without explicitly modeling edit-comment relationship. Wikipedia revision history data (Nunes et al., 2008) has been used in many NLP tasks (Zesch, 2012; Max and Wisniewski, 2010; Ganter and Strube, 2009). For instance, Yamangil and Nelken (2008) model Wikipedia revision histories for improving sentence compression, Aji et al. (2010) propose a new term weighting model leveraging Wikipedia revision histories, and Zanzotto and Pennacchiotti (2010) expand textual entailment corpora from Wikipedia revision histories using co-training. Again, however, none of these meth5003 ods directly consider or model the relationship between comments and edits. At a basic level, modeling the connection between comments and edits can be seen as a text matching problem, with superficial similarity to other common"
D19-1505,max-wisniewski-2010-mining,0,0.0344274,"izes comments associated with document edits as supplementary information to study the document revisions. For example, Yatskar et al. (2010) consider both comment and document revision for lexical simplification. However, they use comments as meta-data to identify trusted revisions, rather than directly modeling the relationship between comments and edits. Yang et al. (2017) featurize both comments and revisions to classify edit intent, but without explicitly modeling edit-comment relationship. Wikipedia revision history data (Nunes et al., 2008) has been used in many NLP tasks (Zesch, 2012; Max and Wisniewski, 2010; Ganter and Strube, 2009). For instance, Yamangil and Nelken (2008) model Wikipedia revision histories for improving sentence compression, Aji et al. (2010) propose a new term weighting model leveraging Wikipedia revision histories, and Zanzotto and Pennacchiotti (2010) expand textual entailment corpora from Wikipedia revision histories using co-training. Again, however, none of these meth5003 ods directly consider or model the relationship between comments and edits. At a basic level, modeling the connection between comments and edits can be seen as a text matching problem, with superficial"
D19-1505,D14-1162,0,0.0840349,"a binary classification problem, and we can suitably set the output layer as p = softmax(γ T h) – the 5006 probability of predicting the positive class. Here γ is a trainable weight vector. Given the binary nature of the classification problem we can use the cross entropy loss: N mi  1 XX Le (Φ) = − yij log pij + N 5.1 i=1 j=1 (4)    1 − yij log 1 − pij , Implementation Details The CmntEdit model described in this section is implemented using the Pytorch3 framework and trained on a single Tesla P100 GPU with 16GB memory. The word vectors are initialized with pre-trained Glove embeddings (Pennington et al., 2014) using the default dimensionality of 100. We set the number of training epochs to 5, the maximum length of a comment to 30 tokens and the maximum length of an edit to 300 tokens. For the Comment Ranking task, we set the batch size to 10 and consider 5 candidate comments in each data sample: one true comment and 4 distractors. We thus have 5 comment-edit pairs for each data sample and 50 pair-wise samples for each training batch. For the Edit Anchoring task, we also set the batch size to 10 and consider 5 candidate sentences, which yields an identical 50 training instances per batch. It should"
D19-1505,P08-2035,0,0.0335945,"rmation to study the document revisions. For example, Yatskar et al. (2010) consider both comment and document revision for lexical simplification. However, they use comments as meta-data to identify trusted revisions, rather than directly modeling the relationship between comments and edits. Yang et al. (2017) featurize both comments and revisions to classify edit intent, but without explicitly modeling edit-comment relationship. Wikipedia revision history data (Nunes et al., 2008) has been used in many NLP tasks (Zesch, 2012; Max and Wisniewski, 2010; Ganter and Strube, 2009). For instance, Yamangil and Nelken (2008) model Wikipedia revision histories for improving sentence compression, Aji et al. (2010) propose a new term weighting model leveraging Wikipedia revision histories, and Zanzotto and Pennacchiotti (2010) expand textual entailment corpora from Wikipedia revision histories using co-training. Again, however, none of these meth5003 ods directly consider or model the relationship between comments and edits. At a basic level, modeling the connection between comments and edits can be seen as a text matching problem, with superficial similarity to other common NLP tasks, such as Question Answering (Se"
D19-1505,D17-1213,0,0.0254714,". (2017) study the semantic distance between the content in different versions of documents to detect document revisions. Grossman et al. (2013) propose a hierarchical navigation method to display document revision histories. Some work utilizes comments associated with document edits as supplementary information to study the document revisions. For example, Yatskar et al. (2010) consider both comment and document revision for lexical simplification. However, they use comments as meta-data to identify trusted revisions, rather than directly modeling the relationship between comments and edits. Yang et al. (2017) featurize both comments and revisions to classify edit intent, but without explicitly modeling edit-comment relationship. Wikipedia revision history data (Nunes et al., 2008) has been used in many NLP tasks (Zesch, 2012; Max and Wisniewski, 2010; Ganter and Strube, 2009). For instance, Yamangil and Nelken (2008) model Wikipedia revision histories for improving sentence compression, Aji et al. (2010) propose a new term weighting model leveraging Wikipedia revision histories, and Zanzotto and Pennacchiotti (2010) expand textual entailment corpora from Wikipedia revision histories using co-train"
D19-1505,N10-1056,0,0.039679,"everal studies in recent years (Nunes et al., 2011; Fischer, 2013). Most prior work focuses on modeling document edits only. For instance, Bronner and Monz (2012) build a classifier to distinguish fluency edits from factual edits. Zhu et al. (2017) study the semantic distance between the content in different versions of documents to detect document revisions. Grossman et al. (2013) propose a hierarchical navigation method to display document revision histories. Some work utilizes comments associated with document edits as supplementary information to study the document revisions. For example, Yatskar et al. (2010) consider both comment and document revision for lexical simplification. However, they use comments as meta-data to identify trusted revisions, rather than directly modeling the relationship between comments and edits. Yang et al. (2017) featurize both comments and revisions to classify edit intent, but without explicitly modeling edit-comment relationship. Wikipedia revision history data (Nunes et al., 2008) has been used in many NLP tasks (Zesch, 2012; Max and Wisniewski, 2010; Ganter and Strube, 2009). For instance, Yamangil and Nelken (2008) model Wikipedia revision histories for improving"
D19-1505,W10-3504,0,0.0122742,"ify trusted revisions, rather than directly modeling the relationship between comments and edits. Yang et al. (2017) featurize both comments and revisions to classify edit intent, but without explicitly modeling edit-comment relationship. Wikipedia revision history data (Nunes et al., 2008) has been used in many NLP tasks (Zesch, 2012; Max and Wisniewski, 2010; Ganter and Strube, 2009). For instance, Yamangil and Nelken (2008) model Wikipedia revision histories for improving sentence compression, Aji et al. (2010) propose a new term weighting model leveraging Wikipedia revision histories, and Zanzotto and Pennacchiotti (2010) expand textual entailment corpora from Wikipedia revision histories using co-training. Again, however, none of these meth5003 ods directly consider or model the relationship between comments and edits. At a basic level, modeling the connection between comments and edits can be seen as a text matching problem, with superficial similarity to other common NLP tasks, such as Question Answering (Seo et al., 2016; Yu et al., 2018), document search (Burges et al., 2005; Nalisnick et al., 2016), and textual entailment (Androutsopoulos and Malakasiotis, 2010), among others. Note however, that edits ar"
D19-1505,E12-1054,0,0.0190315,"ome work utilizes comments associated with document edits as supplementary information to study the document revisions. For example, Yatskar et al. (2010) consider both comment and document revision for lexical simplification. However, they use comments as meta-data to identify trusted revisions, rather than directly modeling the relationship between comments and edits. Yang et al. (2017) featurize both comments and revisions to classify edit intent, but without explicitly modeling edit-comment relationship. Wikipedia revision history data (Nunes et al., 2008) has been used in many NLP tasks (Zesch, 2012; Max and Wisniewski, 2010; Ganter and Strube, 2009). For instance, Yamangil and Nelken (2008) model Wikipedia revision histories for improving sentence compression, Aji et al. (2010) propose a new term weighting model leveraging Wikipedia revision histories, and Zanzotto and Pennacchiotti (2010) expand textual entailment corpora from Wikipedia revision histories using co-training. Again, however, none of these meth5003 ods directly consider or model the relationship between comments and edits. At a basic level, modeling the connection between comments and edits can be seen as a text matching"
D19-1505,I17-1095,0,0.0310549,"mple of an edit and its associated comment. The added words “Sutskever and” in post-edit version is marked in red. ally, in an ablation study we demonstrate that our various modeling choices, which tackle the inherent challenges of comment-edit understanding, each contribute positively to empirical results. 2 Related Work Document revisions have been the subject of several studies in recent years (Nunes et al., 2011; Fischer, 2013). Most prior work focuses on modeling document edits only. For instance, Bronner and Monz (2012) build a classifier to distinguish fluency edits from factual edits. Zhu et al. (2017) study the semantic distance between the content in different versions of documents to detect document revisions. Grossman et al. (2013) propose a hierarchical navigation method to display document revision histories. Some work utilizes comments associated with document edits as supplementary information to study the document revisions. For example, Yatskar et al. (2010) consider both comment and document revision for lexical simplification. However, they use comments as meta-data to identify trusted revisions, rather than directly modeling the relationship between comments and edits. Yang et"
E03-1006,C00-1007,0,0.014546,"ementation with particular attention to the degree to which the original system could be reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read. The p"
E03-1006,W02-2105,1,0.872705,"reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read. The promise of machine-learned approaches to sentence realization is that they can easily be ad"
E03-1006,P02-1004,1,0.793151,"that they can easily be adapted to new domains and ideally to new languages merely by retraining The architecture of Amalgam was intended to be languageindependent, although the system has previously only been applied to German sentence realization. Adapting this system to French allows us to assess which aspects of the system are truly language-independent and what must be added in order to account for French. The purpose of this paper is to focus on the adaptation of Amalgam to French. Discussions about the general architecture of the system can be found in Corston-Oliver et al. (2002) and Gamon et al. (2002b). 1 Overview of German Amalgam Amalgam takes as its input a logical form graph, i.e., a sentence-level dependency graph with fixed lexical choices for content words. This graph represents the predicate-argument structure of a sentence and includes semantic information concerning relations between nodes of the graph (Heidorn, 2002). Examples of French logical forms are given in section 3. Amalgam first degraphs the logical form into a tree and then augments it by the insertion of function words, 323 assignment of case and verb position features, syntactic labels, etc., to produce an unordered"
E03-1006,A00-2023,0,0.0234152,"o the degree to which the original system could be reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read. The promise of machine-learned"
E03-1006,W02-2103,0,0.0249065,"which the original system could be reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read. The promise of machine-learned approaches to sentence"
E03-1006,W98-1426,0,0.0224812,"dent as possible and was first implemented for German. We discuss the development of the French implementation with particular attention to the degree to which the original system could be reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree co"
E03-1006,P98-1116,0,0.0437571,"dent as possible and was first implemented for German. We discuss the development of the French implementation with particular attention to the degree to which the original system could be reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree co"
I08-1059,P01-1017,0,0.0447053,"Missing"
I08-1059,W07-1604,0,0.537721,"Felice and Pulman (2007) utilize a set of sophisticated syntactic and semantic analysis features to predict 5 common English prepositions. Obviously, this is impractical in a setting where noisy non-native text is subjected to proofing. Meanwhile, work on automated error detection on non-native text focuses primarily on detection of errors, rather than on the more difficult task of supplying viable corrections (e.g., Chodorow and Leacock, 2000). More recently, Han et al. (2004, 2006) use a maximum entropy classifier to propose article corrections in TESOL essays, while Izumi et al. (2003) and Chodorow et al. (2007) present techniques of automatic preposition choice modeling. These more recent efforts, nevertheless, do not attempt to integrate their methods into a more general proofing application designed to assist non-native speakers when writing English. Finally, Yi et al. (2008) designed a system that uses web counts to determine correct article usage for a given sentence, targeting ESL users. 4 System Description Our system consists of three major components: 1. Suggestion Provider (SP) 2. Language Model (LM) 3. Example Provider (EP) The Suggestion Provider contains modules for each error type discu"
I08-1059,W07-1607,0,0.562737,"Missing"
I08-1059,J93-1003,0,0.101425,"on sites are determined heuristically from the sequence of POS tags. Based on these features, we train a classifier for preposition choice and determiner choice. Currently we train decision tree classifiers with the WinMine toolkit (Chickering 2002). We also experimented with linear SVMs, but decision trees performed better overall and training and parameter optimization were considerably more efficient. Before training the classifiers, we perform feature ablation by imposing a count cutoff of 10, and by limiting the number of features to the top 75K features in terms of log likelihood ratio (Dunning 1993). We train two separate classifiers for both determiners and preposition:  decision whether or not a determiner/preposition should be present (presence/absence or pa classifier)  decision which determiner/preposition is the most likely choice, given that a determiner/preposition is present (choice or ch classifier) In the case of determiners, class values for the ch classifier are a/an and the. Preposition choice (equivalent to the ―confusion set‖ of a contextual speller) is limited to a set of 13 prepositions that figure prominently in the errors observed in the JLE corpus: about, as, at, b"
I08-1059,W02-2105,1,0.815053,"Missing"
I08-1059,han-etal-2004-detecting,0,0.212248,"Turner and Charniak (2007), for example, utilize a language model based on a statistical parser for Penn Tree Bank data. Similarly, De Felice and Pulman (2007) utilize a set of sophisticated syntactic and semantic analysis features to predict 5 common English prepositions. Obviously, this is impractical in a setting where noisy non-native text is subjected to proofing. Meanwhile, work on automated error detection on non-native text focuses primarily on detection of errors, rather than on the more difficult task of supplying viable corrections (e.g., Chodorow and Leacock, 2000). More recently, Han et al. (2004, 2006) use a maximum entropy classifier to propose article corrections in TESOL essays, while Izumi et al. (2003) and Chodorow et al. (2007) present techniques of automatic preposition choice modeling. These more recent efforts, nevertheless, do not attempt to integrate their methods into a more general proofing application designed to assist non-native speakers when writing English. Finally, Yi et al. (2008) designed a system that uses web counts to determine correct article usage for a given sentence, targeting ESL users. 4 System Description Our system consists of three major components: 1"
I08-1059,I08-2082,1,0.357362,"Missing"
I08-1059,N04-2006,0,0.131483,"Missing"
I08-1059,P00-1067,1,0.853874,"ot a significant problem for native speakers and hence remains unaddressed in proofing tools such as the grammar checker in Microsoft Word (Heidorn 2000). Plainly there is an Targeted Error Types Our system currently targets eight different error types: 1. Preposition presence and choice: In the other hand, ... (On the other hand ...) 2. Definite and indefinite determiner presence and choice: I am teacher... (am a teacher) 3. Gerund/infinitive confusion: I am interesting in this book. (interested in) 4. Auxiliary verb presence and choice: My teacher does is a good teacher (my teacher is...) 1 Liu et al. 2000 take a similar approach, retrieving example sentences from a large corpus. 449 5. Over-regularized verb inflection: I writed a letter (wrote) 6. Adjective/noun confusion: This is a China book (Chinese book) 7. Word order (adjective sequences and nominal compounds): I am a student of university (university student) 8. Noun pluralization: They have many knowledges (much knowledge) In this paper we will focus on the two most prominent and difficult errors: choice of determiner and prepositions. Empirical justification for targeting these errors comes from inspection of several corpora of non-nat"
I08-1059,W00-0708,0,0.590938,"Missing"
I08-1059,P06-1132,0,0.116761,"Missing"
I08-1059,N07-2045,0,0.363375,"Missing"
I08-1059,N07-1007,0,\N,Missing
J07-2010,W02-1011,0,0.0174124,"Missing"
J07-2010,P02-1053,0,0.00359996,"Missing"
N10-1019,E87-1007,0,0.190575,"National Corpus for DeFelice and Pulman (2007, 2008), Wall Street Journal in Knight and Chander (1994), a mix of Reuters and Encarta in Gamon et al. (2008, 2009). In order to partially address the problem of domain mismatch between learners’ writing and the news-heavy data sets often used in data-driven NLP applications, Han et al. (2004, 2006) use 31.5 million words from the MetaMetrics corpus, a diverse corpus of fiction, non-fiction and textbooks categorized by reading level. In addition to the classification approach to error detection, there is a line of research - going back to at least Atwell (1987) - that uses language models. The idea here is to detect errors in areas where the language model score is suspiciously low. Atwell (1987) uses a part-of-speech tag language model to detect errors, Chodorow and Leacock (2000) use mutual information and chi square statistics to identify unlikely function word and part-of-speech tag sequences, Turner and Charniak (2007) employ a language model based on a generative statistical parser, and Stehouwer and van Zaanen (2009) investigate a diverse set of language models with different backoff strategies to determine which choice, from a set of confusa"
N10-1019,W07-1604,0,0.43111,"Missing"
N10-1019,W07-1607,0,0.213206,"Missing"
N10-1019,C08-1022,0,0.565048,"Missing"
N10-1019,J93-1003,0,0.0866293,"Missing"
N10-1019,I08-1059,1,0.833515,"to detect errors in areas where the language model score is suspiciously low. Atwell (1987) uses a part-of-speech tag language model to detect errors, Chodorow and Leacock (2000) use mutual information and chi square statistics to identify unlikely function word and part-of-speech tag sequences, Turner and Charniak (2007) employ a language model based on a generative statistical parser, and Stehouwer and van Zaanen (2009) investigate a diverse set of language models with different backoff strategies to determine which choice, from a set of confusable words, is most likely in a given context. Gamon et al. (2008, 2009) use a combination of error-specific classifiers and a large generic language model with handtuned heuristics for combining their scores to maximize precision. Finally, Yi et al. (2008) and Hermet et al. (2008) use n-gram counts from the web as a language model approximation to identify likely errors and correction candidates. 2 Our Approach We combine evidence from the two kinds of datadriven models that have been used for error detection and correction (error-specific classifiers and a language model) through a meta-classifier. We use the term primary models for both the initial error"
N10-1019,W95-0104,0,0.220196,"Missing"
N10-1019,han-etal-2004-detecting,0,0.0772774,"Missing"
N10-1019,hermet-etal-2008-using,0,0.0229933,"re statistics to identify unlikely function word and part-of-speech tag sequences, Turner and Charniak (2007) employ a language model based on a generative statistical parser, and Stehouwer and van Zaanen (2009) investigate a diverse set of language models with different backoff strategies to determine which choice, from a set of confusable words, is most likely in a given context. Gamon et al. (2008, 2009) use a combination of error-specific classifiers and a large generic language model with handtuned heuristics for combining their scores to maximize precision. Finally, Yi et al. (2008) and Hermet et al. (2008) use n-gram counts from the web as a language model approximation to identify likely errors and correction candidates. 2 Our Approach We combine evidence from the two kinds of datadriven models that have been used for error detection and correction (error-specific classifiers and a language model) through a meta-classifier. We use the term primary models for both the initial errorspecific classifiers and a large generic language model. The meta-classifier takes the output of the primary models (language model scores and class probabilities) as input. Using a meta-classifier for ensemble learni"
N10-1019,W09-2111,1,0.771726,"preposition/article. This evaluation scheme, however, ignores one aspect of a real user scenario. Of all the suggested changes that are counted as wrong in our evaluation because they do not match an annotated error, some may in fact be innocuous or even helpful for a real user. Such a situation can arise for a variety of reasons: In some cases, there are legitimate alternative ways to correct an error. In other cases, the classifier has identified the location of an error although that error is of a different kind (which can be beneficial because it causes the user to make a correction - see Leacock et al., 2009). Gamon et al. (2009), for example manually evaluate preposition suggestions as belonging to one of three categories: (a) properly correcting an existing error, (b) offering a suggestion that neither improves nor degrades the user sentence, (c) offering a sugges167 tion that would degrade the user input. Obviously, (c) is a more serious error than (b). Similarly, Tetrault and Chodorow (2008) annotate their test set with preposition choices that are valid alternatives. We do not have similar information in the CLC data, but we can perform a manual analysis of a random subset of test data to est"
N10-1019,N04-2006,0,0.103437,"Missing"
N10-1019,W00-0708,0,0.0639083,"Missing"
N10-1019,W09-1007,0,0.0611013,"Missing"
N10-1019,C08-1109,0,0.599996,"Missing"
N10-1019,W08-1205,0,0.0762014,"Missing"
N10-1019,N07-2045,0,0.0291298,"illion words from the MetaMetrics corpus, a diverse corpus of fiction, non-fiction and textbooks categorized by reading level. In addition to the classification approach to error detection, there is a line of research - going back to at least Atwell (1987) - that uses language models. The idea here is to detect errors in areas where the language model score is suspiciously low. Atwell (1987) uses a part-of-speech tag language model to detect errors, Chodorow and Leacock (2000) use mutual information and chi square statistics to identify unlikely function word and part-of-speech tag sequences, Turner and Charniak (2007) employ a language model based on a generative statistical parser, and Stehouwer and van Zaanen (2009) investigate a diverse set of language models with different backoff strategies to determine which choice, from a set of confusable words, is most likely in a given context. Gamon et al. (2008, 2009) use a combination of error-specific classifiers and a large generic language model with handtuned heuristics for combining their scores to maximize precision. Finally, Yi et al. (2008) and Hermet et al. (2008) use n-gram counts from the web as a language model approximation to identify likely erro"
N10-1019,I08-2082,0,0.0581881,"ormation and chi square statistics to identify unlikely function word and part-of-speech tag sequences, Turner and Charniak (2007) employ a language model based on a generative statistical parser, and Stehouwer and van Zaanen (2009) investigate a diverse set of language models with different backoff strategies to determine which choice, from a set of confusable words, is most likely in a given context. Gamon et al. (2008, 2009) use a combination of error-specific classifiers and a large generic language model with handtuned heuristics for combining their scores to maximize precision. Finally, Yi et al. (2008) and Hermet et al. (2008) use n-gram counts from the web as a language model approximation to identify likely errors and correction candidates. 2 Our Approach We combine evidence from the two kinds of datadriven models that have been used for error detection and correction (error-specific classifiers and a language model) through a meta-classifier. We use the term primary models for both the initial errorspecific classifiers and a large generic language model. The meta-classifier takes the output of the primary models (language model scores and class probabilities) as input. Using a meta-class"
N10-1019,P03-2026,0,\N,Missing
N10-1019,O01-2002,0,\N,Missing
N12-1074,N10-1020,0,0.132636,"ertile avenues of research is modeling users and their interactions on Twitter. An extensive line of work characterizes users (Pear Analytics, 2009) and quantifies user influence (Cha et al., 2010; Romero et al., 2011; Wu et al., 2011; Bakshy et al., 2011). Popescu and Jain (2011) explored how businesses use Twitter to connect with their customer base. Popescu and Pennacchiotti (2011) and Qu et al. (2011) investigated how users react to events on social media. There also has been extensive work on modeling conversational interactions on Twitter (Honeycutt and Herring, 2009; Boyd et al., 2010; Ritter et al., 2010; Danescu-Niculescu-Mizil et al., 2011). Our work builds on these findings to predict response behavior on a large scale. Mining Twitter Social media has been used to detect events (Sakaki et al., 2010; Popescu and Pennacchiotti, 2010; Popescu et al., 2011), and even predict their outcomes (Asur and Huberman, 2010; Culotta, 2010). Similarly to this line of work, we mine the social network for event prediction. In contrast, our focus is on predicting events within the network. Response Prediction There has been significant work addressing the task of response prediction in news articles (Tsagki"
N12-1074,D11-1054,0,0.013986,"l networks has been investigated previously: Hong et al. (2011) focused on predicting responses for highly popular items, Rowe et al. (2011) targeted the prediction of conversations and their length and Suh et al. (2010) and Petrovic et al. (2011) predicted retweets. In contrast, our work targets tweets regardless of their popularity and attempts to predict both replies and retweets. Furthermore, we present a scalable method to use linguistic lexical features in discriminative models by leveraging global network statistics. A related task to ours is that of response generation, as explored by Ritter et al. (2011). Our work complements their approach by allowing to detect when the generation of a response is appropriate. Lastly, the task of predicting the spread of hashtags in microblogging networks (Tsur and Rappoport, 2012) is also closely related to our work and both approaches supplement each other as measures of impact. Ranking in News Feeds Different approaches were suggested for ranking items in social media (Das Sarma et al., 2010; Lakkaraju et al., 2011). Our work provides an important signal, which can be incorporated into any ranking approach. 3 Response Prediction on Twitter Our goal is to"
N12-1074,N09-1054,0,0.0112984,"uilds on these findings to predict response behavior on a large scale. Mining Twitter Social media has been used to detect events (Sakaki et al., 2010; Popescu and Pennacchiotti, 2010; Popescu et al., 2011), and even predict their outcomes (Asur and Huberman, 2010; Culotta, 2010). Similarly to this line of work, we mine the social network for event prediction. In contrast, our focus is on predicting events within the network. Response Prediction There has been significant work addressing the task of response prediction in news articles (Tsagkias et al., 2009; Tsagkias et al., 2010) and blogs (Yano et al., 2009; Yano and Smith, 2010; Balasubramanyan et al., 2011). The task of predicting responses in social networks has been investigated previously: Hong et al. (2011) focused on predicting responses for highly popular items, Rowe et al. (2011) targeted the prediction of conversations and their length and Suh et al. (2010) and Petrovic et al. (2011) predicted retweets. In contrast, our work targets tweets regardless of their popularity and attempts to predict both replies and retweets. Furthermore, we present a scalable method to use linguistic lexical features in discriminative models by leveraging g"
N12-1074,J96-1002,0,\N,Missing
N12-3006,D09-1111,1,0.814882,"Missing"
N12-3006,J93-2004,0,0.04636,"the NLP group at Microsoft Research. The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages. 1 Introduction The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language processing components, including part-of-speech taggers, chunkers, and parsers. There remain many differences in how these components are built, resulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differently-trained linguistic component, such as tokenization. The community recognizes this difficulty,"
N12-3006,J05-1004,0,0.0277178,". The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages. 1 Introduction The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language processing components, including part-of-speech taggers, chunkers, and parsers. There remain many differences in how these components are built, resulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differently-trained linguistic component, such as tokenization. The community recognizes this difficulty, and shared task organizers are now"
N12-3006,P06-1055,0,0.0153341,"Missing"
N12-3006,C08-1094,0,0.0225179,"Missing"
N12-3006,J08-2002,1,0.0600596,"abels, like ARG0, ARG1, …, ARG5 for core arguments, and labels like ARGMTMP,ARGM-LOC, etc. for adjunct-like arguments. The meaning of the numbered arguments is verb-specific, with ARG0 typically representing an agent-like role, and ARG1 a patient-like role. This implementation of an SRL system follows the approach described in (Xue and Palmer, 04), and includes two log-linear models for argument identification and classification. A single syntax tree generated by the MSR SPLAT split-merge parser is used as input. Non-overlapping arguments are derived using the dynamic programming algorithm by Toutanova et al. (2008). 3 3.1 Other Language Analysis Functionality Sentence Boundary / Tokenization This analyzer identifies sentence boundaries and breaks the input into tokens. Both are represented as offsets of character ranges. Each token has both a raw form from the string and a normalized form in the PTB specification, e.g., open and close parentheses are replaced by -LRB- and -RRB-, respectively, to remove ambiguity with parentheses indicating syntactic structure. A finite state machine using simple rules and abbreviations detects sentence boundaries with high accuracy, and a set of regular expressions toke"
N12-3006,W04-3212,0,0.0291436,"Missing"
N12-3006,J03-4003,0,\N,Missing
N16-1171,P08-1107,0,0.0310575,"l., 2008), analyzing how topics change over time (Wang and McCallum, 2006), understanding entity relations (Balasubramanyan and Cohen, 2011), analyzing communication networks (Nguyen et al., 2013), for authorship attribution (Seroussi et al., 2012), and discovering topics associated with authors (McCallum et al., 2005). Other generative models have also been used for analyzing email communication behavior (Navaroli et al., 2012), identifying links between an email sender and a recipient to detect potential anomalous communication (Huang and Zeng, 2006), and resolving personal names in emails (Elsayed et al., 2008). Representing workplace activities of the emails with probabilistic inference in graphical models where observed information is personalized to the email senders is what sets our work apart from previous research in computational models for emails. 2.3 Email Recipient Recommendation For recommending email recipients, interactions among email participants and content similarity are the signals that have been explored most. Carvalho and Cohen (2007) leveraged content similarity by creating tf-idf centroid vectors and determining k-nearest neighbors of a target email. Pal et al. (2007) presented"
N16-1171,N12-3006,1,0.715863,"OI) phrases. = b] = t] Subject and Body Token Representations Previous work in modeling email content mostly explored bag of words (e.g., (Graus et al., 2014)) or tf-idf vectors (e.g., (Carvalho and Cohen, 2007)) as the content representation of an email. For modeling activities in emails, we experiment with different linguistic representations of the email content. They are: • Lexical: as the lexical representation, we use the bag of words (BOW) from email body and subject, after Penn Tree Bank (PTB) style tokenization. • Syntactic: using heuristics on the output of a PTB constituent parser (Quirk et al., 2012), we identify Nouns (N) and Verb Phrases (VP) in email body and subject. • Semantic: we identify phrases in emails that represent topics, concept and entities discussed in the emails. We refer to them as Thing of Interest (TOI). To extract these key phrases, we use Web search queries as a source of information. Using queries as a dictionary of possible key phrases is useful but has limited coverage since many topics/concepts are discussed in emails but absent or not widely available in Web search queries. Instead of using queries directly, we use them to construct a training set of key phrases"
N16-1171,P12-2052,0,0.0154431,"various linguistic representations of content. 2.2 Generative models for Emails Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a frequently used generative topic model. Assuming a Dirichlet prior, LDA models learn probability distributions of words as latent topics in a corpus. In emails, LDA models have been used for learn1453 ing summary keywords (Dredze et al., 2008), analyzing how topics change over time (Wang and McCallum, 2006), understanding entity relations (Balasubramanyan and Cohen, 2011), analyzing communication networks (Nguyen et al., 2013), for authorship attribution (Seroussi et al., 2012), and discovering topics associated with authors (McCallum et al., 2005). Other generative models have also been used for analyzing email communication behavior (Navaroli et al., 2012), identifying links between an email sender and a recipient to detect potential anomalous communication (Huang and Zeng, 2006), and resolving personal names in emails (Elsayed et al., 2008). Representing workplace activities of the emails with probabilistic inference in graphical models where observed information is personalized to the email senders is what sets our work apart from previous research in computatio"
N19-1012,H05-1067,0,0.41719,"ous research on automated humor can be divided into work on datasets, analysis, detection, and generation. We will give examples of each. Datasets are important for automated understanding of humor and for training models. Starting at the simplest linguistic level, Engelthaler and Hills (2018) gathered almost 5,000 English words with funniness ratings for each one. Filatova (2012) found 1,905 Amazon product reviews classified as either regular or ironic/sarcastic, and Khodak et al. (2017) collected 1.3 million sarcastic statements from Reddit and a much larger set of non-sarcastic statements. Mihalcea and Strapparava (2005) collected about 24,000 oneliner jokes, Potash et al. (2017) shared a dataset to rank funny tweets for certain hashtags, and Miller et al. (2017) created a task for pun detection. Humor analysis, as we have done, is aimed at understanding what makes something funny. Building on the word-level corpus of Engelthaler and Hills (2018), Westbury and Hollis (2018) developed models to predict the funniness of 4,997 words. Looking at multi-word, but still short text, Shahaf et al. (2015) analyzed cartoon captions in order to understand what made some funnier than others. The work that is most similar"
N19-1012,N16-1016,0,0.28218,"edited headline funny, (ii) predicting whether an edited headline is funny, (iii) ranking multiple edits of the same headline on a funniness scale, (iv) generating humorous news headlines, and (v) recommending funny headlines personalized to a reader. Our dataset presents several opportunities for computational humor research since: Introduction Humor detection and generation continue to be challenging AI problems. While there have been some advances in automatic humor recognition (Khodak et al., 2017; Davidov et al., 2010; Barbieri and Saggion, 2014; Reyes et al., 2012; Cattle and Ma, 2018; Bertero and Fung, 2016; Yang et al., 2015), computerized humor generation has seen less progress (Binsted et al., 1997; Stock and Strapparava, 2003; Petrovi´c and Matthews, 2013). This is not surprising, given that humor involves in-depth world-knowledge, common sense, and the ability to perceive relationships across entities and objects at various levels of understanding. Even humans often fail at being funny or recognizing humor. A big hindrance to progress on humor research is the scarcity of public datasets. Further133 Proceedings of NAACL-HLT 2019, pages 133–142 c Minneapolis, Minnesota, June 2 - June 7, 2019."
N19-1012,S17-2005,0,0.078063,"tant for automated understanding of humor and for training models. Starting at the simplest linguistic level, Engelthaler and Hills (2018) gathered almost 5,000 English words with funniness ratings for each one. Filatova (2012) found 1,905 Amazon product reviews classified as either regular or ironic/sarcastic, and Khodak et al. (2017) collected 1.3 million sarcastic statements from Reddit and a much larger set of non-sarcastic statements. Mihalcea and Strapparava (2005) collected about 24,000 oneliner jokes, Potash et al. (2017) shared a dataset to rank funny tweets for certain hashtags, and Miller et al. (2017) created a task for pun detection. Humor analysis, as we have done, is aimed at understanding what makes something funny. Building on the word-level corpus of Engelthaler and Hills (2018), Westbury and Hollis (2018) developed models to predict the funniness of 4,997 words. Looking at multi-word, but still short text, Shahaf et al. (2015) analyzed cartoon captions in order to understand what made some funnier than others. The work that is most similar to ours is from West and Horvitz (2019), who looked at pairs of funny and normal headlines. While we employed editors to create funny headlines f"
N19-1012,C18-1157,0,0.310662,"tanding what makes an edited headline funny, (ii) predicting whether an edited headline is funny, (iii) ranking multiple edits of the same headline on a funniness scale, (iv) generating humorous news headlines, and (v) recommending funny headlines personalized to a reader. Our dataset presents several opportunities for computational humor research since: Introduction Humor detection and generation continue to be challenging AI problems. While there have been some advances in automatic humor recognition (Khodak et al., 2017; Davidov et al., 2010; Barbieri and Saggion, 2014; Reyes et al., 2012; Cattle and Ma, 2018; Bertero and Fung, 2016; Yang et al., 2015), computerized humor generation has seen less progress (Binsted et al., 1997; Stock and Strapparava, 2003; Petrovi´c and Matthews, 2013). This is not surprising, given that humor involves in-depth world-knowledge, common sense, and the ability to perceive relationships across entities and objects at various levels of understanding. Even humans often fail at being funny or recognizing humor. A big hindrance to progress on humor research is the scarcity of public datasets. Further133 Proceedings of NAACL-HLT 2019, pages 133–142 c Minneapolis, Minnesota"
N19-1012,D14-1162,0,0.0825416,"line build up towards an expected ending, and then change words towards the end to produce a coherent but surprising ending (e.g., IDs 3, 4 and 5). 3.2 Figure 3: Short headlines did not lend themselves to high humor scores, while longer headlines generally had more potential for humor. The blue line shows the raw distribution of headline lengths in our data, and the red line shows the mean funniness score over different lengths. Clusters of Replacement Words Each micro-edit used a new replacement word to change the headline. We clustered these replacement words using their GloVe word vectors (Pennington et al., 2014) and k-means clustering, with k = 20. Our manually-generated cluster names are shown in Table 2, where the clusters are ordered by the mean funniness score of the edited headlines whose replacement word is in the cluster. For each cluster, we show the frequency with which the cluster was used for replacement words and frequent sample words from the cluster. We can compare our automatically generated clusters with those of Westbury and Hollis (2018). They manually created six clusters from the 200 funniest, single words in Engelthaler and Hills (2018) and then they added more words algorithmica"
N19-1012,W10-2914,0,0.0533144,"nes. This new dataset enables various humor tasks, such as: (i) understanding what makes an edited headline funny, (ii) predicting whether an edited headline is funny, (iii) ranking multiple edits of the same headline on a funniness scale, (iv) generating humorous news headlines, and (v) recommending funny headlines personalized to a reader. Our dataset presents several opportunities for computational humor research since: Introduction Humor detection and generation continue to be challenging AI problems. While there have been some advances in automatic humor recognition (Khodak et al., 2017; Davidov et al., 2010; Barbieri and Saggion, 2014; Reyes et al., 2012; Cattle and Ma, 2018; Bertero and Fung, 2016; Yang et al., 2015), computerized humor generation has seen less progress (Binsted et al., 1997; Stock and Strapparava, 2003; Petrovi´c and Matthews, 2013). This is not surprising, given that humor involves in-depth world-knowledge, common sense, and the ability to perceive relationships across entities and objects at various levels of understanding. Even humans often fail at being funny or recognizing humor. A big hindrance to progress on humor research is the scarcity of public datasets. Further133"
N19-1012,P13-2041,0,0.417268,"Missing"
N19-1012,S17-2004,0,0.197617,"nalysis, detection, and generation. We will give examples of each. Datasets are important for automated understanding of humor and for training models. Starting at the simplest linguistic level, Engelthaler and Hills (2018) gathered almost 5,000 English words with funniness ratings for each one. Filatova (2012) found 1,905 Amazon product reviews classified as either regular or ironic/sarcastic, and Khodak et al. (2017) collected 1.3 million sarcastic statements from Reddit and a much larger set of non-sarcastic statements. Mihalcea and Strapparava (2005) collected about 24,000 oneliner jokes, Potash et al. (2017) shared a dataset to rank funny tweets for certain hashtags, and Miller et al. (2017) created a task for pun detection. Humor analysis, as we have done, is aimed at understanding what makes something funny. Building on the word-level corpus of Engelthaler and Hills (2018), Westbury and Hollis (2018) developed models to predict the funniness of 4,997 words. Looking at multi-word, but still short text, Shahaf et al. (2015) analyzed cartoon captions in order to understand what made some funnier than others. The work that is most similar to ours is from West and Horvitz (2019), who looked at pairs"
N19-1012,filatova-2012-irony,0,0.0857984,"ted dataset proportions and classifiers/feature sets. MaxUF is the highest score for the not-funny class, MinF is the lowest score for the funny class, and we also provide Krippendorff’s α for judge agreement. 5 Related Work 6 Previous research on automated humor can be divided into work on datasets, analysis, detection, and generation. We will give examples of each. Datasets are important for automated understanding of humor and for training models. Starting at the simplest linguistic level, Engelthaler and Hills (2018) gathered almost 5,000 English words with funniness ratings for each one. Filatova (2012) found 1,905 Amazon product reviews classified as either regular or ironic/sarcastic, and Khodak et al. (2017) collected 1.3 million sarcastic statements from Reddit and a much larger set of non-sarcastic statements. Mihalcea and Strapparava (2005) collected about 24,000 oneliner jokes, Potash et al. (2017) shared a dataset to rank funny tweets for certain hashtags, and Miller et al. (2017) created a task for pun detection. Humor analysis, as we have done, is aimed at understanding what makes something funny. Building on the word-level corpus of Engelthaler and Hills (2018), Westbury and Holli"
N19-1012,P82-1020,0,0.80842,"Missing"
N19-1012,D17-1067,1,0.936282,"mor, such as incongruity, superiority, and setup/punchline. Finally, we develop baseline classifiers that can predict whether or not an edited headline is funny, which is a first step toward automatically generating humorous headlines as an approach to creating topical humor. 1 (a) The Headline Editing Task. (b) The Headline Grading Task. Figure 1: Snapshots of the headline editing and grading interfaces. Only the underlined tokens are replaceable. more, the existing datasets address specific humor templates, such as funny one-liners (Mihalcea and Strapparava, 2006) and filling in Mad R Libs (Hossain et al., 2017). Creating a humor corpus is non-trivial, however, because it requires (i) human annotation, and (ii) a clear definition of humor to achieve good inter-annotator agreement. We introduce Humicroedit, a novel dataset for research in computational humor. First, we collect original news headlines from news media posted on Reddit (reddit.com). Then, we qualify expert annotators from Amazon Mechanical Turk (mturk.com) to (i) generate humor by applying small edits to these headlines, and to (ii) judge the humor in these edits. Our resulting dataset contains 15,095 edited news headlines and their nume"
N19-1012,P11-2016,0,0.492384,"Missing"
N19-1012,P14-5010,0,0.00264806,"dline as humorous, offensive, confusing, etc. The decision to strictly avoid edits of other partsof-speech (POS) words was motivated by the observation in our pilot experiments that those edits did not provide enough variety of humor. For example, when substituting adjectives and adverbs, our editors mostly used antonyms or superlatives. Switching nouns and verbs, on the other hand, enables the introduction of diverse novel connections between entities and actions. To identify the replaceable entities, we apply named entity recognition (NER) and POS tagging using the Stanford CoreNLP toolkit (Manning et al., 2014). We allow for replacement of only those entities that are well-known, according to the Microsoft Knowledge Base1 . This improves the likelihood that the terms are familiar to both headline editors and humor judges. We allow a noun (or verb) to be replaced if it is an unambiguous noun (or verb) in WordNet (Fellbaum, 1998) (i.e., has a single WordNet POS). Editors are only allowed to replace one of the selected replaceable words/entities in the headline. We refer to a single-term substitution of this type as a “micro-edit”, and we will use this term interchangeably with “edit” in the remainder"
N19-1012,D15-1284,0,0.325851,"(ii) predicting whether an edited headline is funny, (iii) ranking multiple edits of the same headline on a funniness scale, (iv) generating humorous news headlines, and (v) recommending funny headlines personalized to a reader. Our dataset presents several opportunities for computational humor research since: Introduction Humor detection and generation continue to be challenging AI problems. While there have been some advances in automatic humor recognition (Khodak et al., 2017; Davidov et al., 2010; Barbieri and Saggion, 2014; Reyes et al., 2012; Cattle and Ma, 2018; Bertero and Fung, 2016; Yang et al., 2015), computerized humor generation has seen less progress (Binsted et al., 1997; Stock and Strapparava, 2003; Petrovi´c and Matthews, 2013). This is not surprising, given that humor involves in-depth world-knowledge, common sense, and the ability to perceive relationships across entities and objects at various levels of understanding. Even humans often fail at being funny or recognizing humor. A big hindrance to progress on humor research is the scarcity of public datasets. Further133 Proceedings of NAACL-HLT 2019, pages 133–142 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association fo"
P01-1020,P98-1006,0,0.0338465,"gned to facilitate the identification of areas for investigation and improvement. It focuses on evaluating the wellformedness of output and does not address issues of evaluating content transfer. Researchers are now applying automated evaluation in MT and natural language generation tasks, both as system-internal goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT system, the more its output will resemble human-generated text. Indeed, MT might be considered a solved problem should it ever become impossible to distinguish automated output from human translation. We have observed that in gener"
P01-1020,W00-1401,0,0.0198838,"eas for investigation and improvement. It focuses on evaluating the wellformedness of output and does not address issues of evaluating content transfer. Researchers are now applying automated evaluation in MT and natural language generation tasks, both as system-internal goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT system, the more its output will resemble human-generated text. Indeed, MT might be considered a solved problem should it ever become impossible to distinguish automated output from human translation. We have observed that in general humans can easily and rel"
P01-1020,P98-1116,0,0.0115683,"n of machine translation (MT) output is an expensive process, often prohibitively so when evaluations must be performed quickly and frequently in order to measure progress. This paper describes an approach to automated evaluation designed to facilitate the identification of areas for investigation and improvement. It focuses on evaluating the wellformedness of output and does not address issues of evaluating content transfer. Researchers are now applying automated evaluation in MT and natural language generation tasks, both as system-internal goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT sys"
P01-1020,C94-1013,0,0.0234832,"goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT system, the more its output will resemble human-generated text. Indeed, MT might be considered a solved problem should it ever become impossible to distinguish automated output from human translation. We have observed that in general humans can easily and reliably categorize a sentence as either machine- or human-generated. Moreover, they can usually justify their decision. This observation suggests that evaluation of the wellformedness of output sentences can be treated as a classification problem: given a sentence, how accurately can w"
P01-1020,2001.mtsummit-papers.53,0,0.103203,"sification task that targets both linguistic features and more abstract features such as ngram perplexity. 2 Data Our corpus consists of 350,000 aligned SpanishEnglish sentence pairs taken from published computer software manuals and online help documents. We extracted 200,000 English sentences for building language models to evaluate per-sentence perplexity. From the remainder of the corpus, we extracted 100,000 aligned sentence pairs. The Spanish sentences in this latter sample were then translated by the Microsoft machine translation system, which was trained on documents from this domain (Richardson et al., 2001). This yielded a set of 200,000 English sentences, one half of which were English reference sentences, and the other half of which were MT output. (The Spanish sentences were not used in building or evaluating the classifiers). We split the 200,000 English sentences 90/10, to yield 180,000 sentences for training classifiers and 20,000 sentences that we used as held-out test data. Training and test data were evenly divided between reference English sentences and Spanish-to-English translations. 3 Features The selection of features used in our classification task was motivated by failure analysi"
P01-1020,2001.mtsummit-papers.68,0,0.11958,"uently in order to measure progress. This paper describes an approach to automated evaluation designed to facilitate the identification of areas for investigation and improvement. It focuses on evaluating the wellformedness of output and does not address issues of evaluating content transfer. Researchers are now applying automated evaluation in MT and natural language generation tasks, both as system-internal goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT system, the more its output will resemble human-generated text. Indeed, MT might be considered a solved problem should it ever become"
P01-1020,C92-2067,0,0.0259892,"ed evaluation designed to facilitate the identification of areas for investigation and improvement. It focuses on evaluating the wellformedness of output and does not address issues of evaluating content transfer. Researchers are now applying automated evaluation in MT and natural language generation tasks, both as system-internal goodness metrics and for the assessment of output. Langkilde and Knight (1998), for example, employ n-gram metrics to select among candidate outputs in natural language generation, while Ringger et al. (2001) use ngram perplexity to compare the output of MT systems. Su et al. (1992), Alshawi et al. (1998) and Bangalore et al. (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. To be useful to researchers, however, assessment must provide linguistic information that can guide in identifying areas where work is required. (See Nyberg et al., 1994 for useful discussion of this issue.) The better the MT system, the more its output will resemble human-generated text. Indeed, MT might be considered a solved problem should it ever become impossible to distinguish automated output from human translation. We have"
P01-1020,C98-1112,0,\N,Missing
P01-1020,C98-1006,0,\N,Missing
P02-1004,C00-1007,0,0.305172,"g can be performed purely by rules, by application of statistical models, or by a combination of both techniques. Among the systems that use statistical or machine learned techniques in sentence realization, there are various degrees of intermediate syntactic structure. Nitrogen (Langkilde and Knight, 1998a, 1998b) produces a large set of alternative surface realizations of an input structure (which can vary in abstractness). This set of candidate surface strings, represented as a word lattice, is then rescored by a wordbigram language model, to produce the bestranked output sentence. FERGUS (Bangalore and Rambow, 2000), on the other hand, employs a model of syntactic structure during sentence realization. In simple terms, it adds a tree-based stochastic model to the approach taken by the Nitrogen system. This tree-based model chooses a best-ranked XTAG representation for a given dependency structure. Possible linearizations of the XTAG representation are generated and then evaluated by a language model to pick the best possible linearization, as in Nitrogen. In contrast, the sentence realization system code-named Amalgam (A Machine Learned Generation Module) (Corston-Oliver et al., 2002; Gamon et al., 2002b"
P02-1004,W02-2105,1,0.905939,"ut sentence. FERGUS (Bangalore and Rambow, 2000), on the other hand, employs a model of syntactic structure during sentence realization. In simple terms, it adds a tree-based stochastic model to the approach taken by the Nitrogen system. This tree-based model chooses a best-ranked XTAG representation for a given dependency structure. Possible linearizations of the XTAG representation are generated and then evaluated by a language model to pick the best possible linearization, as in Nitrogen. In contrast, the sentence realization system code-named Amalgam (A Machine Learned Generation Module) (Corston-Oliver et al., 2002; Gamon et al., 2002b) employs a series of linguistic operations which map a semantic representation to a surface syntactic tree via intermediate syntactic representations. The contexts for most of these operations in Amalgam are machine learned. The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read. The goal of this paper is to show that it is possible to learn accurately the contexts for linguistically complex operations in sentence realization. We propose that learning the contexts for the application of these linguisti"
P02-1004,C02-1036,1,0.847241,"Missing"
P02-1004,W98-1426,0,0.660353,"tion, sentence realization, creates the surface string from an abstract (typically semantic) representation. This mapping from abstract representation to surface string can be direct, or it can employ intermediate syntactic representations which significantly constrain the output. Furthermore, the mapping can be performed purely by rules, by application of statistical models, or by a combination of both techniques. Among the systems that use statistical or machine learned techniques in sentence realization, there are various degrees of intermediate syntactic structure. Nitrogen (Langkilde and Knight, 1998a, 1998b) produces a large set of alternative surface realizations of an input structure (which can vary in abstractness). This set of candidate surface strings, represented as a word lattice, is then rescored by a wordbigram language model, to produce the bestranked output sentence. FERGUS (Bangalore and Rambow, 2000), on the other hand, employs a model of syntactic structure during sentence realization. In simple terms, it adds a tree-based stochastic model to the approach taken by the Nitrogen system. This tree-based model chooses a best-ranked XTAG representation for a given dependency str"
P02-1004,P98-1116,0,0.0316932,"anguage generation, sentence realization, creates the surface string from an abstract (typically semantic) representation. This mapping from abstract representation to surface string can be direct, or it can employ intermediate syntactic representations which significantly constrain the output. Furthermore, the mapping can be performed purely by rules, by application of statistical models, or by a combination of both techniques. Among the systems that use statistical or machine learned techniques in sentence realization, there are various degrees of intermediate syntactic structure. Nitrogen (Langkilde and Knight, 1998a, 1998b) produces a large set of alternative surface realizations of an input structure (which can vary in abstractness). This set of candidate surface strings, represented as a word lattice, is then rescored by a wordbigram language model, to produce the bestranked output sentence. FERGUS (Bangalore and Rambow, 2000), on the other hand, employs a model of syntactic structure during sentence realization. In simple terms, it adds a tree-based stochastic model to the approach taken by the Nitrogen system. This tree-based model chooses a best-ranked XTAG representation for a given dependency str"
P02-1004,P98-2199,0,0.0246939,"s around the baseline for these two types of extraposed clauses. Table 3. Accuracy of the extraposition model. Extraposable clause RELCL INFCL COMPCL Overall 7 Accuracy 0.8387 0.9202 0.9857 0.8612 Baseline 0.6093 0.9370 0.9429 0.6758 Syntactic aggregation Any sentence realization component that generates from an abstract semantic representation and strives to produce fluent output beyond simple templates will have to deal with coordination and the problem of duplicated material in coordination. This is generally viewed as a subarea of aggregation in the generation literature (Wilkinson, 1995; Shaw, 1998; Reape and Mellish, 1999; Dalianis and Hovy, 1993). In Amalgam, the approach we take is strictly intrasentential, along the lines of what has been called conjunction reduction in the linguistic literature (McCawley, 1988). While this may seem a fairly straightforward task compared to inter-sentential, semantic and lexical aggregation, it should be noted that the cross-linguistic complexity of the phenomenon makes it much less trivial than a first glance at English would suggest. In German, for example, position of the verb in the coordinated VPs plays an important role in determining which du"
P02-1004,C98-1112,0,\N,Missing
P02-1004,C98-2194,0,\N,Missing
P06-1032,P05-1066,0,0.0169251,"information from TV Candidate 5: And we can learn a lot of knowledge or new information from TV Table 3. Multiple replacement candidates generated by 45K training set He has published thirty-two pieces of papers. In this equal world, lots of people are still concerned on the colors of them … The inability of our translation system to handle such discontinuities in a unitary manner reflects the limited ability of current SMT modeling techniques to capture long-distance effects. Similar alternations are rife in bilingual data, e.g., ne…pas in French (Fox, 2002) and separable prefixes in German (Collins et al. 2005). As SMT models become more adept at modeling long-distance effects in a principled manner, monolingual proofing will benefit as well. The Missed category is heterogeneous. The SMT system has an inherent bias against deletion, with the result that unwanted determiners tended not to be deleted, especially in the smaller training sets. Other errors related to coverage in the development data set. Several occurrences of greengrocer’s apostrophes (tea’s, equipment’s) caused correction failures: these were not anticipated when engineering the training data. Likewise, the test data presented several"
P06-1032,W03-0209,0,0.0154667,"little progress in this area over the last decade. Research into computer feedback for ESL writers remains largely focused on smallscale pedagogical systems implemented within the framework of CALL (Computer Aided Language Learning) (Reuer 2003; Vanderventer Faltin, 2003), while commercial ESL grammar checkers remain brittle and difficult to customize to meet the needs of ESL writers of different first-language (L1) backgrounds and skill levels. Some researchers have begun to apply statistical techniques to identify learner errors in the context of essay evaluation (Chodorow & Leacock, 2000; Lonsdale & Strong-Krause, 2003), to detect non-native text (Tomokiyo & Jones, 2001), and to support lexical selection by ESL learners through first-language translation (Liu et al., 2000). However, none of this work appears to directly address the more general problem of how to robustly provide feedback to ESL writers—and for that matter non-native writers in any second language—in a way that is easily tailored to different L1 backgrounds and secondlanguage (L2) skill levels. In this paper, we show that a noisy channel model instantiated within the paradigm of Statistical Machine Translation (SMT) (Brown et al., 1993) can s"
P06-1032,2005.iwslt-1.12,0,0.0192267,"errors produced by ESL writers of specific L1 backgrounds can be captured in the channel model as an emergent property of training data consisting ESL sentences aligned with their corrected edited counterparts. The highest frequency errors and infelicities should emerge as targets for replacement, while lesser frequency or idiosyncratic problems will in general not surface as false flags. 2.3 Implementation In this paper, we explore the use of a large-scale production statistical machine translation system to correct a class of ESL errors. A detailed description of the system can be found in (Menezes & Quirk 2005) and (Quirk et al., 2005). In keeping with current best practices in SMT, our system is a phrasal machine translation system that attempts to learn mappings between “phrases” (which may not correspond to linguistic units) rather than individual words. What distinguishes 250 this system from other phrasal SMT systems is that rather than aligning simple sequences of words, it maps small phrasal “treelets” generated by a dependency parse to corresponding strings in the target. This “Tree-To-String” model holds promise in that it allows us to potentially benefit from being able to access a certain"
P06-1032,P03-1021,0,0.00252946,"consideration when it is sought to handle ungrammatical or otherwise illformed ESL input, but also simultaneously to capture relationships not involving contiguous strings, for example determiner-noun relations. In our pilot study, this system was employed without modification to the system architecture. The sole adjustment made was to have both Source (erroneous) and Target (correct) sentences tokenized using an English language tokenizer. N-best results for phrasal alignment and ordering models in the decoder were optimized by lambda training via Maximum Bleu, along the lines described in (Och, 2003). This procedure yielded a list of 14 words: knowledge, food, homework, fruit, news, color, nutrition, equipment, paper, advice, haste, information, lunch, and tea. 3 Countability errors involving these words are scattered across 46 sentences in the CLEC corpus. For a baseline representing the level of writing assistance currently available to the average ESL writer, we submitted these sentences to the proofing tools in Microsoft Word 2003. The spelling and grammar checkers correctly identified 21 of the 46 relevant errors, proposed one incorrect substitution (a few advice a few advices), and"
P06-1032,P00-1056,0,0.0393123,"Missing"
P06-1032,P05-1034,0,0.0146714,"Missing"
P06-1032,N01-1031,0,0.0382049,"Missing"
P06-1032,A00-2019,0,\N,Missing
P06-1032,P00-1067,0,\N,Missing
P06-1032,J93-2003,0,\N,Missing
P06-1032,C94-1002,0,\N,Missing
P06-1032,W02-1039,0,\N,Missing
P12-1059,C92-2082,0,0.0352979,"for Computational Linguistics • We propose an efficient learning technique and a robust implementation of our models, using real-world query data, and a realistic large set of entity types. • We empirically show that our models outperform the state of the art and that modeling latent intent contributes significantly to these results. 2 Related Work 2.1 Finding Semantic Classes A closely related problem is that of finding the semantic classes of entities. Automatic techniques for finding semantic classes include unsupervised clustering (Sch¨utze, 1998; Pantel and Lin, 2002), hyponym patterns (Hearst, 1992; Pantel et al., 2004; Kozareva et al., 2008), extraction patterns (Etzioni et al., 2005), hidden Markov models (Ritter et al., 2009), classification (Rahman and Ng, 2010) and many others. These techniques typically leverage large corpora, while projects such as WordNet (Miller et al., 1990) and Freebase (Bollacker et al., 2008) have employed editors to manually enumerate words and entities with their semantic classes. The aforementioned methods do not use query logs or explicitly determine the relative probabilities of different entity senses. A method might learn that there is independently"
P12-1059,P08-1119,0,0.0234511,"propose an efficient learning technique and a robust implementation of our models, using real-world query data, and a realistic large set of entity types. • We empirically show that our models outperform the state of the art and that modeling latent intent contributes significantly to these results. 2 Related Work 2.1 Finding Semantic Classes A closely related problem is that of finding the semantic classes of entities. Automatic techniques for finding semantic classes include unsupervised clustering (Sch¨utze, 1998; Pantel and Lin, 2002), hyponym patterns (Hearst, 1992; Pantel et al., 2004; Kozareva et al., 2008), extraction patterns (Etzioni et al., 2005), hidden Markov models (Ritter et al., 2009), classification (Rahman and Ng, 2010) and many others. These techniques typically leverage large corpora, while projects such as WordNet (Miller et al., 1990) and Freebase (Bollacker et al., 2008) have employed editors to manually enumerate words and entities with their semantic classes. The aforementioned methods do not use query logs or explicitly determine the relative probabilities of different entity senses. A method might learn that there is independently a high chance of eBay being a website and an"
P12-1059,C10-1105,0,0.0155835,"large set of entity types. • We empirically show that our models outperform the state of the art and that modeling latent intent contributes significantly to these results. 2 Related Work 2.1 Finding Semantic Classes A closely related problem is that of finding the semantic classes of entities. Automatic techniques for finding semantic classes include unsupervised clustering (Sch¨utze, 1998; Pantel and Lin, 2002), hyponym patterns (Hearst, 1992; Pantel et al., 2004; Kozareva et al., 2008), extraction patterns (Etzioni et al., 2005), hidden Markov models (Ritter et al., 2009), classification (Rahman and Ng, 2010) and many others. These techniques typically leverage large corpora, while projects such as WordNet (Miller et al., 1990) and Freebase (Bollacker et al., 2008) have employed editors to manually enumerate words and entities with their semantic classes. The aforementioned methods do not use query logs or explicitly determine the relative probabilities of different entity senses. A method might learn that there is independently a high chance of eBay being a website and an employer, but does not specify which usage is more common. This is especially problematic, for example, if one wishes to lever"
P12-1059,P11-1097,0,0.035427,"Missing"
P12-1059,J98-1004,0,0.0640862,"Missing"
P14-1143,N12-1092,0,0.0698924,"Missing"
P14-1143,W09-1119,0,0.0317926,"natural language processing task, called smart selection, which aims to address an important problem in text selection for touch-enabled devices; • We conduct a large crowd-sourced user study to collect a dataset of intended selections and simulated user selections, which we release to the academic community; • We propose a machine-learned ensemble model for smart selection, which combines various linguistic annotation methods with information from a large knowledge base and web graph; • We empirically show that our model can effectively address the smart selection task. Finkel et al., 2005; Ratinov and Roth, 2009). We found in our dataset (see Section 3.2) that only a quarter of what users intend to select consists in fact of named entities. Although an NER approach can be very useful, it is certainly not sufficient. The remainder of the data can be partially addressed with noun phrase (NP) detectors (Abney, 1991; Ramshaw and Marcus, 1995; Mu˜noz et al., 1999; Kudo and Matsumoto, 2001) and lists of items in a knowledge base (KB), but again, each is not alone sufficient. NP detectors and KB-based methods are further very susceptible to the generation of false positives (i.e., text contains many nested n"
P14-1143,W02-1001,0,0.256616,"subsumes the user selection is treated as a smart selection candidate. Scoring of candidates is by normalized length, under the assumption that in general the most specific (longest) unit is more likely to be the intended selection. 6 Note that this training set is generated automatically and is, by design, of a different nature than the manually labeled data we use to train and test the ensemble model. 1528 Our first unit spotter, labeled NER is geared towards recognizing named entities. We use a commercial and proprietary state-of-the-art NER system, trained using the perceptron algorithm (Collins, 2002) over more than a million hand-annotated labels. Our second approach uses purely syntactic information and treats noun phrases as units. We label this model as NP. For this purpose we parse the sentence containing the user selection with a syntactic parser following (Ratnaparkhi, 1999). We then treat every noun phrase that subsumes the user selection as a candidate smart selection. Finally, our third unit spotter, labeled KB, is based on the assumption that concepts and other entries in a knowledge base are, by nature, things that can be of interest to people. For our knowledge base lookup, we"
P14-1143,P05-1045,0,0.0407966,"• We introduce a new natural language processing task, called smart selection, which aims to address an important problem in text selection for touch-enabled devices; • We conduct a large crowd-sourced user study to collect a dataset of intended selections and simulated user selections, which we release to the academic community; • We propose a machine-learned ensemble model for smart selection, which combines various linguistic annotation methods with information from a large knowledge base and web graph; • We empirically show that our model can effectively address the smart selection task. Finkel et al., 2005; Ratinov and Roth, 2009). We found in our dataset (see Section 3.2) that only a quarter of what users intend to select consists in fact of named entities. Although an NER approach can be very useful, it is certainly not sufficient. The remainder of the data can be partially addressed with noun phrase (NP) detectors (Abney, 1991; Ramshaw and Marcus, 1995; Mu˜noz et al., 1999; Kudo and Matsumoto, 2001) and lists of items in a knowledge base (KB), but again, each is not alone sufficient. NP detectors and KB-based methods are further very susceptible to the generation of false positives (i.e., te"
P14-1143,N01-1025,0,0.105669,", which combines various linguistic annotation methods with information from a large knowledge base and web graph; • We empirically show that our model can effectively address the smart selection task. Finkel et al., 2005; Ratinov and Roth, 2009). We found in our dataset (see Section 3.2) that only a quarter of what users intend to select consists in fact of named entities. Although an NER approach can be very useful, it is certainly not sufficient. The remainder of the data can be partially addressed with noun phrase (NP) detectors (Abney, 1991; Ramshaw and Marcus, 1995; Mu˜noz et al., 1999; Kudo and Matsumoto, 2001) and lists of items in a knowledge base (KB), but again, each is not alone sufficient. NP detectors and KB-based methods are further very susceptible to the generation of false positives (i.e., text contains many nested noun phrases and knowledge base items include highly ambiguous terms). In our work, we leverage all three techniques in order to benefit from their complementary coverage of user selections. We further create a novel unit detector, called the hyperlink intent model. Based on the assumption that Wikipedia anchor texts are similar in nature to what users would select in a researc"
P14-1143,W99-0621,0,0.200999,"Missing"
P14-1143,W95-0107,0,0.297475,"hine-learned ensemble model for smart selection, which combines various linguistic annotation methods with information from a large knowledge base and web graph; • We empirically show that our model can effectively address the smart selection task. Finkel et al., 2005; Ratinov and Roth, 2009). We found in our dataset (see Section 3.2) that only a quarter of what users intend to select consists in fact of named entities. Although an NER approach can be very useful, it is certainly not sufficient. The remainder of the data can be partially addressed with noun phrase (NP) detectors (Abney, 1991; Ramshaw and Marcus, 1995; Mu˜noz et al., 1999; Kudo and Matsumoto, 2001) and lists of items in a knowledge base (KB), but again, each is not alone sufficient. NP detectors and KB-based methods are further very susceptible to the generation of false positives (i.e., text contains many nested noun phrases and knowledge base items include highly ambiguous terms). In our work, we leverage all three techniques in order to benefit from their complementary coverage of user selections. We further create a novel unit detector, called the hyperlink intent model. Based on the assumption that Wikipedia anchor texts are similar i"
R13-1055,W06-1615,0,0.0718872,"13 September 2013. Correspondence Learning. We then show that a straightforward ensemble learner can, for some domains, improve results further, without any need for specialized learning algorithms. Since most work in domain-adaptation only provides published results on pairwise adaptation between domains and not on multi-domain adaptation, we hope to establish a new baseline for future adaptation techniques to compare against. 2 Related Work Of direct importance to the discussion in this paper are results from domain adaptation in polarity detection. One of the earlier successful approaches (Blitzer et al. 2006, 2007) involved Structural Correspondence Learning (SCL). SCL identifies “pivot” features that are both highly discriminative in the labeled source domain data and also frequent in the unlabeled target domain data. In a subsequent step, linear predictors for the pivot terms are learned from the unlabeled target data and from the source data. Daumé (2007) approached domain adaptation from a fully labeled source domain to a partially labeled target domain by augmenting the feature space. Instead of using a single, general, feature set for source and target, three distinct feature sets are creat"
R13-1055,P07-1056,0,0.566422,"source data. Daumé (2007) approached domain adaptation from a fully labeled source domain to a partially labeled target domain by augmenting the feature space. Instead of using a single, general, feature set for source and target, three distinct feature sets are created: the general set of features, a source-domain specific version of the feature set, and a target-specific version of the feature set. Li and Zong (NLP-KE 2008) explore a classifier combination technique they call “MultipleLabel Consensus Training” which results in better accuracy than non-adapted models on the data sets used in Blitzer et al. (2007). They also addressed the multi-domain sentiment analysis problem using feature –level fusion and classifier-level fusion approaches in Li and Zong (ACL 2008). Dredze and Crammer (2008) have proposed a multi-domain online learning framework based on parameter combination from multiple Confidence Weighted (CW) classifiers. Their MultiDomain Regularization (MDR) framework seeks to learn domain specific parameters guided by the shared parameter across domains. Samdani and Yih (2011) propose an ensemble learner that consists of classifiers trained on different feature groups. The feature groups ar"
R13-1055,P07-1033,0,0.772243,"a new baseline for future adaptation techniques to compare against. 2 Related Work Of direct importance to the discussion in this paper are results from domain adaptation in polarity detection. One of the earlier successful approaches (Blitzer et al. 2006, 2007) involved Structural Correspondence Learning (SCL). SCL identifies “pivot” features that are both highly discriminative in the labeled source domain data and also frequent in the unlabeled target domain data. In a subsequent step, linear predictors for the pivot terms are learned from the unlabeled target data and from the source data. Daumé (2007) approached domain adaptation from a fully labeled source domain to a partially labeled target domain by augmenting the feature space. Instead of using a single, general, feature set for source and target, three distinct feature sets are created: the general set of features, a source-domain specific version of the feature set, and a target-specific version of the feature set. Li and Zong (NLP-KE 2008) explore a classifier combination technique they call “MultipleLabel Consensus Training” which results in better accuracy than non-adapted models on the data sets used in Blitzer et al. (2007). Th"
R13-1055,W06-0301,0,0.0187219,"a recent overview see Pang & Lee 2008 and Liu 2012). Early work focused on algorithms for mining sentiment dictionaries (Hatzivassiloglou and McKeown 1997, Turney 2002); this was followed by the exploration of supervised techniques (Pang et al. 2002) and, somewhat more recently, by investigations of domain adaptation techniques. Also more recently, the focus has broadened from the detection of polarity (negative/positive sentiment) to more nuanced approaches that try to identify targets and holders of sentiment, sentiment strength, or finer-grained mood distinctions (e.g. Wilson et al. 2006, Kim and Hovy 2006). Within the polarity detection paradigm, a number of common assumptions have been shared in the community and are frequently repeated in the literature. Two of these fundamental assumptions are: 1. Obtaining sufficient labeled data for supervised training is expensive 2. Sentiment models trained on one domain tend to perform poorly on new, unseen domains A conclusion that is often drawn from these assumptions is that domain adaptation of sentiment models from a domain with sufficient labeled data to a new domain with little labeled data is an important problem and requires new and sophisticat"
R13-1055,P02-1053,0,0.0106977,"is inexpensive to come by, the “kitchen sink” approach, while technically nonglamorous, might be perfectly adequate in practice. We also show that the common anecdotal evidence for sentiment terms that “flip” polarity across domains is not borne out empirically. 1 Introduction Automatic detection and analysis of sentiment around products, brands, political issues etc. has triggered a large amount of research in the past 15 – 20 years (for a recent overview see Pang & Lee 2008 and Liu 2012). Early work focused on algorithms for mining sentiment dictionaries (Hatzivassiloglou and McKeown 1997, Turney 2002); this was followed by the exploration of supervised techniques (Pang et al. 2002) and, somewhat more recently, by investigations of domain adaptation techniques. Also more recently, the focus has broadened from the detection of polarity (negative/positive sentiment) to more nuanced approaches that try to identify targets and holders of sentiment, sentiment strength, or finer-grained mood distinctions (e.g. Wilson et al. 2006, Kim and Hovy 2006). Within the polarity detection paradigm, a number of common assumptions have been shared in the community and are frequently repeated in the literatur"
R13-1055,P08-2065,0,0.0520868,"Missing"
R13-1055,D08-1072,0,0.0401143,"Missing"
R13-1055,W02-1011,0,0.0320932,"nglamorous, might be perfectly adequate in practice. We also show that the common anecdotal evidence for sentiment terms that “flip” polarity across domains is not borne out empirically. 1 Introduction Automatic detection and analysis of sentiment around products, brands, political issues etc. has triggered a large amount of research in the past 15 – 20 years (for a recent overview see Pang & Lee 2008 and Liu 2012). Early work focused on algorithms for mining sentiment dictionaries (Hatzivassiloglou and McKeown 1997, Turney 2002); this was followed by the exploration of supervised techniques (Pang et al. 2002) and, somewhat more recently, by investigations of domain adaptation techniques. Also more recently, the focus has broadened from the detection of polarity (negative/positive sentiment) to more nuanced approaches that try to identify targets and holders of sentiment, sentiment strength, or finer-grained mood distinctions (e.g. Wilson et al. 2006, Kim and Hovy 2006). Within the polarity detection paradigm, a number of common assumptions have been shared in the community and are frequently repeated in the literature. Two of these fundamental assumptions are: 1. Obtaining sufficient labeled data"
R13-1055,P97-1023,0,0.0981554,"the fact that labeled data nowadays is inexpensive to come by, the “kitchen sink” approach, while technically nonglamorous, might be perfectly adequate in practice. We also show that the common anecdotal evidence for sentiment terms that “flip” polarity across domains is not borne out empirically. 1 Introduction Automatic detection and analysis of sentiment around products, brands, political issues etc. has triggered a large amount of research in the past 15 – 20 years (for a recent overview see Pang & Lee 2008 and Liu 2012). Early work focused on algorithms for mining sentiment dictionaries (Hatzivassiloglou and McKeown 1997, Turney 2002); this was followed by the exploration of supervised techniques (Pang et al. 2002) and, somewhat more recently, by investigations of domain adaptation techniques. Also more recently, the focus has broadened from the detection of polarity (negative/positive sentiment) to more nuanced approaches that try to identify targets and holders of sentiment, sentiment strength, or finer-grained mood distinctions (e.g. Wilson et al. 2006, Kim and Hovy 2006). Within the polarity detection paradigm, a number of common assumptions have been shared in the community and are frequently repeated in"
W02-2105,W01-0808,0,0.160919,"l., 1999) and the grammatical relations of noun phrases (Corston-Oliver, 2000) as well as performing lexical selection (Bangalore and Rambow, 2000b). Traditional knowledge engineering approaches to sentence realization founder on our inadequate understanding of the mapping from propositional content to surface form, a mapping that encompasses such problems as equi-NP deletion, movement operations such as extraposition and left-dislocation, ordering of modifiers within constituents, and voice alternations. Research in knowledge engineered solutions to these problems continues (see for example, Aikawa et al., 2001). However, the task of broad-coverage sentence realization would appear to be of comparable complexity to sentence analysis, and equally difficult to adapt to new domains. Sentence realization therefore appears to be an ideal candidate for statistical and machinelearned approaches. Some recently described systems have attempted to side-step the encoding decisions involved in sentence realization by proposing alternative realizations of an input semantic form and leaving it to a word-level language model to select the most likely candidate sentence for output. The Nitrogen system (Langkilde and"
W02-2105,C00-1007,0,0.535591,"planning phase in that it only executes decisions made previously.” Although the kinds and number of decisions to be made during sentence realization will depend on the nature of the prior sentence planning phase and on the linguistic complexity of the domain, myriad encoding decisions must still be made. Machine learning approaches have been successfully applied to such aspects of sentence realization as determining the appropriate form of referring expressions (Poesio et al., 1999) and the grammatical relations of noun phrases (Corston-Oliver, 2000) as well as performing lexical selection (Bangalore and Rambow, 2000b). Traditional knowledge engineering approaches to sentence realization founder on our inadequate understanding of the mapping from propositional content to surface form, a mapping that encompasses such problems as equi-NP deletion, movement operations such as extraposition and left-dislocation, ordering of modifiers within constituents, and voice alternations. Research in knowledge engineered solutions to these problems continues (see for example, Aikawa et al., 2001). However, the task of broad-coverage sentence realization would appear to be of comparable complexity to sentence analysis, a"
W02-2105,P00-1059,0,0.0872084,"planning phase in that it only executes decisions made previously.” Although the kinds and number of decisions to be made during sentence realization will depend on the nature of the prior sentence planning phase and on the linguistic complexity of the domain, myriad encoding decisions must still be made. Machine learning approaches have been successfully applied to such aspects of sentence realization as determining the appropriate form of referring expressions (Poesio et al., 1999) and the grammatical relations of noun phrases (Corston-Oliver, 2000) as well as performing lexical selection (Bangalore and Rambow, 2000b). Traditional knowledge engineering approaches to sentence realization founder on our inadequate understanding of the mapping from propositional content to surface form, a mapping that encompasses such problems as equi-NP deletion, movement operations such as extraposition and left-dislocation, ordering of modifiers within constituents, and voice alternations. Research in knowledge engineered solutions to these problems continues (see for example, Aikawa et al., 2001). However, the task of broad-coverage sentence realization would appear to be of comparable complexity to sentence analysis, a"
W02-2105,W00-1401,0,0.0219099,"Missing"
W02-2105,W00-1008,1,0.79937,"no doubt a widely-held belief, namely that “This phase is not a planning phase in that it only executes decisions made previously.” Although the kinds and number of decisions to be made during sentence realization will depend on the nature of the prior sentence planning phase and on the linguistic complexity of the domain, myriad encoding decisions must still be made. Machine learning approaches have been successfully applied to such aspects of sentence realization as determining the appropriate form of referring expressions (Poesio et al., 1999) and the grammatical relations of noun phrases (Corston-Oliver, 2000) as well as performing lexical selection (Bangalore and Rambow, 2000b). Traditional knowledge engineering approaches to sentence realization founder on our inadequate understanding of the mapping from propositional content to surface form, a mapping that encompasses such problems as equi-NP deletion, movement operations such as extraposition and left-dislocation, ordering of modifiers within constituents, and voice alternations. Research in knowledge engineered solutions to these problems continues (see for example, Aikawa et al., 2001). However, the task of broad-coverage sentence realization"
W02-2105,P01-1023,0,0.0112687,"al language sentences from logical form inputs. We describe the decomposition of the task of sentence realization into a linguistically informed series of steps, with particular attention to the linguistic issues that arise in German. We report on the evaluation of component steps and of the overall system. 1 Introduction Since the mid 1990s, there has been increasing interest in the application of statistical and machine learning techniques to various aspects of natural language generation, ranging from learning plans for high-level organization of texts and dialogues (Zukerman et al., 1998; Duboue and McKeown, 2001) or ensuring that the macro properties of generated texts such as the distribution of sentence lengths and lexical variety mirror the properties of naturally occurring texts (Oberlander and Brew, 2000) to sentence planning (Walker et al., 2001.). As generation proceeds through successively less abstract stages, nearing the final output string, it would appear that current generation systems are still likely to employ knowledge-engineered approaches. Indeed Walker et al. (2001), commenting on sentence realization, rather succinctly summarize what is no doubt a widely-held belief, namely that “T"
W02-2105,C02-1036,1,0.638852,"current paper we describe an on-going research project code-named Amalgam. Amalgam is a (predominantly) machine-learned generation module that performs sentence realization and a small degree of lexical selection. Amalgam takes as input a logical form graph. Proceeding through a series of machine-learned and knowledgeengineered steps, it transforms that graph into a fully articulated tree structure from which an output sentence can be read. Amalgam has been successfully applied to the realization of non-trivial German sentences in diverse technical domains. An extended description is given in Gamon et al. (2002a). 2 Linguistic issues in German generation Although English and German are closely related languages, they now differ typologically in dramatic ways. German makes a three-way distinction in lexical gender (masculine, feminine, neuter). Nominal elements are morphologically marked for one of four grammatical cases (nominative, accusative, dative and genitive), with adjectives and determiners agreeing in gender, number and case. Verbal position is fixed, and is sensitive to clause type; the order of other constituents is relatively free. So-called “separable prefixes” are elements that may occu"
W02-2105,W98-1426,0,0.676318,"et al., 2001). However, the task of broad-coverage sentence realization would appear to be of comparable complexity to sentence analysis, and equally difficult to adapt to new domains. Sentence realization therefore appears to be an ideal candidate for statistical and machinelearned approaches. Some recently described systems have attempted to side-step the encoding decisions involved in sentence realization by proposing alternative realizations of an input semantic form and leaving it to a word-level language model to select the most likely candidate sentence for output. The Nitrogen system (Langkilde and Knight, 1998a, 1998b), for example, uses a rather permissive knowledge-engineered component to propose candidate output sentences that are then scored using word bigrams. Statistics garnered from actual texts are thus used as a substitute for deeper knowledge. The addition of syntactic information, either to constrain the range of candidate sentences or to augment the n-gram model, has produced favorable improvements over n-gram models used alone (Bangalore and Rambow, 2000a; Ratnaparkhi, 2000). In the current paper we describe an on-going research project code-named Amalgam. Amalgam is a (predominantly)"
W02-2105,P98-1116,0,0.508107,"et al., 2001). However, the task of broad-coverage sentence realization would appear to be of comparable complexity to sentence analysis, and equally difficult to adapt to new domains. Sentence realization therefore appears to be an ideal candidate for statistical and machinelearned approaches. Some recently described systems have attempted to side-step the encoding decisions involved in sentence realization by proposing alternative realizations of an input semantic form and leaving it to a word-level language model to select the most likely candidate sentence for output. The Nitrogen system (Langkilde and Knight, 1998a, 1998b), for example, uses a rather permissive knowledge-engineered component to propose candidate output sentences that are then scored using word bigrams. Statistics garnered from actual texts are thus used as a substitute for deeper knowledge. The addition of syntactic information, either to constrain the range of candidate sentences or to augment the n-gram model, has produced favorable improvements over n-gram models used alone (Bangalore and Rambow, 2000a; Ratnaparkhi, 2000). In the current paper we describe an on-going research project code-named Amalgam. Amalgam is a (predominantly)"
W02-2105,A00-2026,0,0.0142837,"word-level language model to select the most likely candidate sentence for output. The Nitrogen system (Langkilde and Knight, 1998a, 1998b), for example, uses a rather permissive knowledge-engineered component to propose candidate output sentences that are then scored using word bigrams. Statistics garnered from actual texts are thus used as a substitute for deeper knowledge. The addition of syntactic information, either to constrain the range of candidate sentences or to augment the n-gram model, has produced favorable improvements over n-gram models used alone (Bangalore and Rambow, 2000a; Ratnaparkhi, 2000). In the current paper we describe an on-going research project code-named Amalgam. Amalgam is a (predominantly) machine-learned generation module that performs sentence realization and a small degree of lexical selection. Amalgam takes as input a logical form graph. Proceeding through a series of machine-learned and knowledgeengineered steps, it transforms that graph into a fully articulated tree structure from which an output sentence can be read. Amalgam has been successfully applied to the realization of non-trivial German sentences in diverse technical domains. An extended description is"
W02-2105,N01-1003,0,0.0117568,"he evaluation of component steps and of the overall system. 1 Introduction Since the mid 1990s, there has been increasing interest in the application of statistical and machine learning techniques to various aspects of natural language generation, ranging from learning plans for high-level organization of texts and dialogues (Zukerman et al., 1998; Duboue and McKeown, 2001) or ensuring that the macro properties of generated texts such as the distribution of sentence lengths and lexical variety mirror the properties of naturally occurring texts (Oberlander and Brew, 2000) to sentence planning (Walker et al., 2001.). As generation proceeds through successively less abstract stages, nearing the final output string, it would appear that current generation systems are still likely to employ knowledge-engineered approaches. Indeed Walker et al. (2001), commenting on sentence realization, rather succinctly summarize what is no doubt a widely-held belief, namely that “This phase is not a planning phase in that it only executes decisions made previously.” Although the kinds and number of decisions to be made during sentence realization will depend on the nature of the prior sentence planning phase and on the"
W02-2105,C98-1112,0,\N,Missing
W05-0408,M95-1004,0,0.0460216,"Missing"
W05-0408,J81-4005,0,0.698925,"Missing"
W05-0408,W02-1011,0,0.0292126,"Missing"
W05-0408,P04-1035,0,0.173708,"Missing"
W05-0408,P02-1053,0,0.147912,"very reliably. We then use these newly identified terms in various scenarios for the sentiment classification of sentences. We show that our approach outperforms Turney’s original approach. Combining our approach with a Naive Bayes bootstrapping method yields a further small improvement of classifier performance. We finally compare our results to precision and recall figures that can be obtained on the same data set with labeled data. 1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002, Wiebe et al. 2001, Bai et al. 2004, Yu and Hatzivassiloglou 2003 and many others). The identification and classification of sentiment constitutes a problem that is orthogonal to the usual task of text classification. Whereas in traditional text classification the focus is on topic identification, in sentiment classification the focus is on the assessment of the writer’s sentiment toward the topic. Movie and product reviews have been the main focus of many of the recent studies in this area (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002). T"
W05-0408,W03-1017,0,0.329067,"ios for the sentiment classification of sentences. We show that our approach outperforms Turney’s original approach. Combining our approach with a Naive Bayes bootstrapping method yields a further small improvement of classifier performance. We finally compare our results to precision and recall figures that can be obtained on the same data set with labeled data. 1 Introduction The field of sentiment classification has received considerable attention from researchers in recent years (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002, Wiebe et al. 2001, Bai et al. 2004, Yu and Hatzivassiloglou 2003 and many others). The identification and classification of sentiment constitutes a problem that is orthogonal to the usual task of text classification. Whereas in traditional text classification the focus is on topic identification, in sentiment classification the focus is on the assessment of the writer’s sentiment toward the topic. Movie and product reviews have been the main focus of many of the recent studies in this area (Pang and Lee 2002, Pang et al. 2004, Turney 2002, Turney and Littman 2002). Typically, these reviews are classified at the document level, and the class labels are “pos"
W06-3803,W05-1504,0,0.013649,"s in the background graph, the weight of the “incoming” edge is added to the existing edge weight. Figure 1 shows a subset of a TD graph for the first two sentences of topic N57. The visualization is generated by the Pajek tool (Bagatelj and Mrvar). P (i , j ) P (i ) P ( j ) Equation 2: Pointwise Mutual Information (PMI) between two terms i and j. Figure 1: A subset of a TD graph of the first two sentences of topic N57. 2 This view is supported by examining dependency structures derived from the Penn Tree Bank and mapping the probability of a dependency to the distance between words. See also Eisner and Smith (2005) who explore this generalization for dependency parsing. 3 We also computed results from a graph where the edge weight is determined only by term distance, without PMI. These results were consistently worse than the ones reported here. 4 We are grateful to an anonymous reviewer for pointing this out. 20 • • • • 4.3 Graph features 4.3.1 Simple Graph features In novelty detection, graph based features allow to assess the change a graph undergoes through the addition of a new sentence. The intuition behind these features is that the more a graph changes when a sentence is added, the more likely t"
W06-3803,P04-3020,0,0.120333,"ved from graph representations and we have restricted ourselves to representations that do not require linguistic analysis. Simple bag-of-word metrics like KL divergence establish a baseline for classifier performance. More sophisticated metrics can be defined on the basis of graph representations. Graph representations of text can be constructed without performing linguistic analysis, by using term distances in sentences and pointwise mutual information between terms to form edges between term-vertices. A term-distance based representation has been used successfully for a variety of tasks in Mihalcea (2004) and Mihalcea and Tarau (2004). 2 Previous work There were 13 participants and 54 submitted runs for the 2004 TREC novelty track task 2. Each participant submitted up to five runs with different system configurations. Metrics and approaches varied widely, from purely string based approaches to systems that used sophisticated linguistic components for synonymy resolution, coreference resolution and named entity recognition. Many systems employed a thresholding approach to the task, defining a novelty metric and then determining a sentence to be novel if the threshold is exceeded (e.g. Blott et"
W06-3803,W04-3252,0,\N,Missing
W09-2111,C94-1002,0,0.0194874,"m or examination conditions, leaving unresolved the more practical question as to whether the ESL Assistant can actually help a perRelated Work Language learner error correction techniques typically fall into either of two categories: rule-based or data-driven. Eeg-Olofsson and Knutsson (2003) report on a rule-based system that detects and corrects preposition errors in non-native Swedish text. Rule-based approaches have also been used to predict definiteness and indefiniteness of Japanese noun phrases as a preprocessing step for Japanese to English machine translation (Murata and Nagao 1993; Bond et al, 1994; Heine, 1998), a task that is similar to the prediction of English articles. More recently, data-driven approaches have gained popularity and been applied to article prediction in English (Knight and Chander 1994; Minnen et al, 2000; Turner and Charniak 2007), to an array of Japanese learners’ errors in English (Izumi et al, 2003), to verb errors (Lee and Seneff, 2008), and to article and preposition correction in texts written by non-native ELLs (Han et al, 2004, 2006; Nagata et al, 2005; Nagata et al, 2006; De Felice and Pulman, 2007; Chodorow et al, 2007; Gamon et al, 2008, 2009; Tetreault"
W09-2111,W07-1604,0,0.094797,"Missing"
W09-2111,W07-1607,0,0.122066,"Missing"
W09-2111,I08-1059,1,0.849467,"interactions, like article and preposition errors. Rule-based approaches handle those error types that are amenable to simpler solutions. For example, a regular expression is sufficient for identifying when a modal is (incorrectly) followed by a tensed verb. The output of all modules, both machine-learned and rule-based, is filtered through a very large language model. Only when the language model finds that the likelihood of the suggested rewrite is suffi74 ciently larger than the original text is a suggestion shown to the user. For a detailed description of ESL Assistant’s architecture, see Gamon et al (2008, 2009). Although this and the systems cited in section 2 are designed to be used by non-native writers, system performance is typically reported in relation to native text – the prediction of a preposition, for example, will ideally be consistent with usage in native, edited text. An error is counted each time the system predicts a token that differs from the observed usage and a correct prediction is counted each time the system predicts the usage that occurs in the text. Although somewhat artificial, this approach to evaluation offers the advantages of being fully automatable and having abu"
W09-2111,han-etal-2004-detecting,1,0.931498,"Missing"
W09-2111,P98-1085,0,0.0371609,"onditions, leaving unresolved the more practical question as to whether the ESL Assistant can actually help a perRelated Work Language learner error correction techniques typically fall into either of two categories: rule-based or data-driven. Eeg-Olofsson and Knutsson (2003) report on a rule-based system that detects and corrects preposition errors in non-native Swedish text. Rule-based approaches have also been used to predict definiteness and indefiniteness of Japanese noun phrases as a preprocessing step for Japanese to English machine translation (Murata and Nagao 1993; Bond et al, 1994; Heine, 1998), a task that is similar to the prediction of English articles. More recently, data-driven approaches have gained popularity and been applied to article prediction in English (Knight and Chander 1994; Minnen et al, 2000; Turner and Charniak 2007), to an array of Japanese learners’ errors in English (Izumi et al, 2003), to verb errors (Lee and Seneff, 2008), and to article and preposition correction in texts written by non-native ELLs (Han et al, 2004, 2006; Nagata et al, 2005; Nagata et al, 2006; De Felice and Pulman, 2007; Chodorow et al, 2007; Gamon et al, 2008, 2009; Tetreault and Chodorow,"
W09-2111,P03-2026,0,0.0762794,"Missing"
W09-2111,N04-2006,0,0.320835,"Missing"
W09-2111,P08-1021,0,0.0272888,"Missing"
W09-2111,W00-0708,0,0.143988,"es: rule-based or data-driven. Eeg-Olofsson and Knutsson (2003) report on a rule-based system that detects and corrects preposition errors in non-native Swedish text. Rule-based approaches have also been used to predict definiteness and indefiniteness of Japanese noun phrases as a preprocessing step for Japanese to English machine translation (Murata and Nagao 1993; Bond et al, 1994; Heine, 1998), a task that is similar to the prediction of English articles. More recently, data-driven approaches have gained popularity and been applied to article prediction in English (Knight and Chander 1994; Minnen et al, 2000; Turner and Charniak 2007), to an array of Japanese learners’ errors in English (Izumi et al, 2003), to verb errors (Lee and Seneff, 2008), and to article and preposition correction in texts written by non-native ELLs (Han et al, 2004, 2006; Nagata et al, 2005; Nagata et al, 2006; De Felice and Pulman, 2007; Chodorow et al, 2007; Gamon et al, 2008, 2009; Tetreault and Chodorow, 2008a). 1 http://www.eslassistant.com 73 Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 73–81, c Boulder, Colorado, June 2009. 2009 Association for Computati"
W09-2111,1993.tmi-1.18,0,0.0102476,"bly artificial classroom or examination conditions, leaving unresolved the more practical question as to whether the ESL Assistant can actually help a perRelated Work Language learner error correction techniques typically fall into either of two categories: rule-based or data-driven. Eeg-Olofsson and Knutsson (2003) report on a rule-based system that detects and corrects preposition errors in non-native Swedish text. Rule-based approaches have also been used to predict definiteness and indefiniteness of Japanese noun phrases as a preprocessing step for Japanese to English machine translation (Murata and Nagao 1993; Bond et al, 1994; Heine, 1998), a task that is similar to the prediction of English articles. More recently, data-driven approaches have gained popularity and been applied to article prediction in English (Knight and Chander 1994; Minnen et al, 2000; Turner and Charniak 2007), to an array of Japanese learners’ errors in English (Izumi et al, 2003), to verb errors (Lee and Seneff, 2008), and to article and preposition correction in texts written by non-native ELLs (Han et al, 2004, 2006; Nagata et al, 2005; Nagata et al, 2006; De Felice and Pulman, 2007; Chodorow et al, 2007; Gamon et al, 200"
W09-2111,P06-1031,0,0.0327211,"Missing"
W09-2111,C08-1109,0,0.0606258,"Missing"
W09-2111,W08-1205,0,0.0720649,"Missing"
W09-2111,N07-2045,0,0.192999,"ta-driven. Eeg-Olofsson and Knutsson (2003) report on a rule-based system that detects and corrects preposition errors in non-native Swedish text. Rule-based approaches have also been used to predict definiteness and indefiniteness of Japanese noun phrases as a preprocessing step for Japanese to English machine translation (Murata and Nagao 1993; Bond et al, 1994; Heine, 1998), a task that is similar to the prediction of English articles. More recently, data-driven approaches have gained popularity and been applied to article prediction in English (Knight and Chander 1994; Minnen et al, 2000; Turner and Charniak 2007), to an array of Japanese learners’ errors in English (Izumi et al, 2003), to verb errors (Lee and Seneff, 2008), and to article and preposition correction in texts written by non-native ELLs (Han et al, 2004, 2006; Nagata et al, 2005; Nagata et al, 2006; De Felice and Pulman, 2007; Chodorow et al, 2007; Gamon et al, 2008, 2009; Tetreault and Chodorow, 2008a). 1 http://www.eslassistant.com 73 Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 73–81, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Noun Relat"
W09-2111,I05-1071,0,\N,Missing
W09-2111,C98-1082,0,\N,Missing
W10-1005,E87-1007,0,0.351727,"Missing"
W10-1005,H01-1052,0,0.0945078,"Missing"
W10-1005,P01-1005,0,0.0964542,"Missing"
W10-1005,W07-1604,0,0.0345083,"Missing"
W10-1005,W07-1607,0,0.0440662,"Missing"
W10-1005,C08-1022,0,0.298042,"Missing"
W10-1005,I08-1059,1,0.869747,"Missing"
W10-1005,N10-1019,1,0.779005,"Missing"
W10-1005,hermet-etal-2008-using,0,0.124774,"Missing"
W10-1005,J03-3005,0,0.0713681,"Missing"
W10-1005,C08-1109,0,0.0487889,"Missing"
W10-1005,I08-2082,0,0.0434557,"Missing"
W11-1422,A00-2019,0,0.0801692,"(1986) used low-likelihood sequences of POS tags as indicators for the presence of an error. Sjöbergh (2005) used a chunker to detect unlikely chunks in native Swedish writing compared to the chunks derived from a large corpus of well-formed Swedish writing. Bigert and Knutsson (2002) employed a statistical method to identify a variety of 181 errors in Swedish writing as rare sequences of morpho-syntactic tags. They significantly reduced false positives by using additional methods to determine whether the unexpected sequence is due to phrase or sentence boundaries or due to rare single tags. Chodorow and Leacock (2000) utilized mutual information and chi-square statistics to identify typical contexts for a small set of targeted words from a large well-formed corpus. Comparing these statistics to the ones found in a novel sentence, they could identify unlikely contexts for the targeted words that were often good indicators of the presence of an error. Sun et al. (2007) mined for patterns that consist of POS tags and function words. The patterns are of variable length and can also contain gaps. Patterns were then combined in a classifier to distinguish correct from erroneous sentences. Wagner et al. (2007) co"
W11-1422,W07-1604,0,0.166478,"Missing"
W11-1422,C08-1022,0,0.121898,"Missing"
W11-1422,P07-1010,0,0.0299621,"rpus. Comparing these statistics to the ones found in a novel sentence, they could identify unlikely contexts for the targeted words that were often good indicators of the presence of an error. Sun et al. (2007) mined for patterns that consist of POS tags and function words. The patterns are of variable length and can also contain gaps. Patterns were then combined in a classifier to distinguish correct from erroneous sentences. Wagner et al. (2007) combined parse probabilities from a set of statistical parsers and POS tag n-gram probabilities in a classifier to detect ungrammatical sentences. Okanohara and Tsujii (2007) differed from the previous approaches in that they directly used discriminative language models to distinguish correct from incorrect sentences, without the direct modeling of errorindicating patterns. Park and Levy (2011) use a noisy channel model with a base language model and a set of error-specific noise models for error detection and correction. In contrast to previous work, we cast the task as a sequence modeling problem. This provides a flexible framework in which multiple statistical and linguistic signals can be combined and calibrated by supervised learning. The approach is error-ag"
W11-1422,N10-1019,1,0.946832,"hen sum1/sum2. The final feature addresses the intuition that an erroneous word may cause n-grams that contain the word to be less likely than adjacent but non-overlapping n-grams. Overlap to adjacent ratio is the sum of probabilities of n-grams including wi, divided by the sum of probabilities of n-grams that are adjacent to wi but do not include it. Note that this use of a host of language model features is substantially different from using a single language model score on hypothesized error and potential correction to filter out unlikely correction candidates as in Gamon et al. (2008) and Gamon (2010). 5.2 String features String features capture information about the characters in a token and the tokens in a sentence. Two binary features indicate whether a token is capitalized (initial capitalization or all capitalized), one feature indicates the token length in characters and one feature measures the number of tokens in the sentence. 5.3 Linguistic Analysis features Each sentence is linguistically analyzed by a PCFG-LA parser (Petrov et al., 2006) trained on the Penn Treebank (Marcus et al., 1993). A number of features are extracted from the constituency tree to assess the syntactic compl"
W11-1422,P11-1094,0,0.148106,"that consist of POS tags and function words. The patterns are of variable length and can also contain gaps. Patterns were then combined in a classifier to distinguish correct from erroneous sentences. Wagner et al. (2007) combined parse probabilities from a set of statistical parsers and POS tag n-gram probabilities in a classifier to detect ungrammatical sentences. Okanohara and Tsujii (2007) differed from the previous approaches in that they directly used discriminative language models to distinguish correct from incorrect sentences, without the direct modeling of errorindicating patterns. Park and Levy (2011) use a noisy channel model with a base language model and a set of error-specific noise models for error detection and correction. In contrast to previous work, we cast the task as a sequence modeling problem. This provides a flexible framework in which multiple statistical and linguistic signals can be combined and calibrated by supervised learning. The approach is error-agnostic and can easily be extended with additional statistical or linguistic features. 3. Error detection by sequence modeling Errors consist of a sub-sequence of tokens in a longer token sequence. They can be identified by"
W11-1422,O01-2002,0,0.0514109,"Missing"
W11-1422,P06-1055,0,0.0195772,"a single language model score on hypothesized error and potential correction to filter out unlikely correction candidates as in Gamon et al. (2008) and Gamon (2010). 5.2 String features String features capture information about the characters in a token and the tokens in a sentence. Two binary features indicate whether a token is capitalized (initial capitalization or all capitalized), one feature indicates the token length in characters and one feature measures the number of tokens in the sentence. 5.3 Linguistic Analysis features Each sentence is linguistically analyzed by a PCFG-LA parser (Petrov et al., 2006) trained on the Penn Treebank (Marcus et al., 1993). A number of features are extracted from the constituency tree to assess the syntactic complexity of the whole sentence, the syntactic complexity of the local environment of a token, and simple constituency information for each token. These features are: label 184 of the parent and grandparent node, number of sibling nodes, number of siblings of the parent, presence of a governing head node, label of the governing head node, and length of path to the root. An additional feature indicates whether the POS tag assigned by the parser does not mat"
W11-1422,N10-1018,0,0.141896,"Missing"
W11-1422,D10-1094,0,0.0457358,"Missing"
W11-1422,P07-1011,0,0.0602691,"iting as rare sequences of morpho-syntactic tags. They significantly reduced false positives by using additional methods to determine whether the unexpected sequence is due to phrase or sentence boundaries or due to rare single tags. Chodorow and Leacock (2000) utilized mutual information and chi-square statistics to identify typical contexts for a small set of targeted words from a large well-formed corpus. Comparing these statistics to the ones found in a novel sentence, they could identify unlikely contexts for the targeted words that were often good indicators of the presence of an error. Sun et al. (2007) mined for patterns that consist of POS tags and function words. The patterns are of variable length and can also contain gaps. Patterns were then combined in a classifier to distinguish correct from erroneous sentences. Wagner et al. (2007) combined parse probabilities from a set of statistical parsers and POS tag n-gram probabilities in a classifier to detect ungrammatical sentences. Okanohara and Tsujii (2007) differed from the previous approaches in that they directly used discriminative language models to distinguish correct from incorrect sentences, without the direct modeling of errorin"
W11-1422,C08-1109,0,0.538701,"Missing"
W11-1422,J93-2004,0,0.0360825,"and potential correction to filter out unlikely correction candidates as in Gamon et al. (2008) and Gamon (2010). 5.2 String features String features capture information about the characters in a token and the tokens in a sentence. Two binary features indicate whether a token is capitalized (initial capitalization or all capitalized), one feature indicates the token length in characters and one feature measures the number of tokens in the sentence. 5.3 Linguistic Analysis features Each sentence is linguistically analyzed by a PCFG-LA parser (Petrov et al., 2006) trained on the Penn Treebank (Marcus et al., 1993). A number of features are extracted from the constituency tree to assess the syntactic complexity of the whole sentence, the syntactic complexity of the local environment of a token, and simple constituency information for each token. These features are: label 184 of the parent and grandparent node, number of sibling nodes, number of siblings of the parent, presence of a governing head node, label of the governing head node, and length of path to the root. An additional feature indicates whether the POS tag assigned by the parser does not match the tag assigned by the POS tagger, which may in"
W11-1422,N03-1033,0,0.00832406,"e only two states “O” and “I”, and both states can transition to each other. Since there are no states with asymmetric transition properties that would introduce a bias towards states with fewer transitions, label bias is not a problem for us. Figure 1 shows the structure of our MEMM with a Markov order of five (the diagram only shows the complete set of arcs for the last state). The input sentence contains the token sequence the past year I was stayed … with the error was stayed. Instead of using the tokens themselves as observations, we chose to use POS tags assigned by an automatic tagger (Toutanova et al. 2003). This choice was motivated by data sparseness. Learning a model that observes individual lexical items and predicts a sequence of error/non-error tags would be ideal, but given the many different error types and triggering contexts for an error, such a model would require much more training data. A large set of features that serve as constraints on the state transition models are extracted for each state. These features are described in Section 5. Note that the model structure would lend itself to a factorial conditional random field (McCallum et al. 2003) which allows the joint labeling of P"
W11-1422,N10-2012,0,0.0378233,"Missing"
W11-1422,han-etal-2010-using,0,\N,Missing
W11-1422,N06-2024,0,\N,Missing
W11-1422,E87-1007,0,\N,Missing
W11-1422,izumi-etal-2004-overview,0,\N,Missing
W11-1422,I08-1059,1,\N,Missing
W11-1825,J93-1003,0,0.0914792,"alues were rounded to the closest integer. We found, however, that adding these features did not improve results. 2.2.2 Feature combination and reduction We experimented with feature reduction and feature combination within the set of features described here. For feature reduction we tried a number of simple approaches that typically work well in text classification. The latter is similar to the task at hand, in that there is a very large but sparse feature set. We tried two feature reduction methods: a simple count cutoff, and selection of the top n features in terms of log likelihood ratio (Dunning, 1993) with the target values. For a count cutoff, we used cutoffs from 3 to 10, but we failed to observe any consistent gains. Only low cutoffs (3 and occasionally 5) would ever produce any small improvements on the development set. Using 159 log likelihood ratio (as determined on the training set), we reduced the total number of features to between 10,000 and 75,000. None of these experiments improved results, however. One potential reason for this negative result may be that there were a lot of features in our set that capture the same phenomenon in different ways, i.e. which correlate highly. By"
W11-1825,de-marneffe-etal-2006-generating,0,0.0209894,"Missing"
W11-1825,P08-2026,0,0.0458061,"ndicate edges with posterior > 0.95; edges with posterior < 0.05 were omitted. Most of the ambiguity is in the attachment of “elicited”. words in the sentence. Since proteins may consist of multiple words, for paths we picked a single representative word for each protein to act as its starting point and ending point. Generally this was the token inside the protein that is closest to the root of the dependency parse. In the case of ties, we picked the rightmost such node. 2.1.1 McClosky-Charniak-Stanford parses The organizers provide parses from a version of the McClosky-Charniak parser, MCCC (McClosky and Charniak, 2008), which is a two-stage parser/reranker trained on the GENIA corpus. In addition, we used an improved set of parsing models that leverage unsupervised data, MCCC-I (McClosky, 2010). In both cases, the Stanford Parser was used to convert constituency trees in the Penn Treebank format into labeled dependency parses: we used the collapsed dependency format. 2.1.2 Dependency posteriors Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al. 2008). McClosky-Charniak parses in two passes: the first pass"
W11-1825,N10-1004,0,0.011112,"ltiple words, for paths we picked a single representative word for each protein to act as its starting point and ending point. Generally this was the token inside the protein that is closest to the root of the dependency parse. In the case of ties, we picked the rightmost such node. 2.1.1 McClosky-Charniak-Stanford parses The organizers provide parses from a version of the McClosky-Charniak parser, MCCC (McClosky and Charniak, 2008), which is a two-stage parser/reranker trained on the GENIA corpus. In addition, we used an improved set of parsing models that leverage unsupervised data, MCCC-I (McClosky, 2010). In both cases, the Stanford Parser was used to convert constituency trees in the Penn Treebank format into labeled dependency parses: we used the collapsed dependency format. 2.1.2 Dependency posteriors Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al. 2008). McClosky-Charniak parses in two passes: the first pass is a generative model that produces a set of n-best candidates, and the 156 second pass is a discriminative reranker that uses a rich set of features including non-local informat"
W11-1825,P08-1023,0,0.00685334,"ion of the McClosky-Charniak parser, MCCC (McClosky and Charniak, 2008), which is a two-stage parser/reranker trained on the GENIA corpus. In addition, we used an improved set of parsing models that leverage unsupervised data, MCCC-I (McClosky, 2010). In both cases, the Stanford Parser was used to convert constituency trees in the Penn Treebank format into labeled dependency parses: we used the collapsed dependency format. 2.1.2 Dependency posteriors Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al. 2008). McClosky-Charniak parses in two passes: the first pass is a generative model that produces a set of n-best candidates, and the 156 second pass is a discriminative reranker that uses a rich set of features including non-local information. We renormalized the outputs from this log-linear discriminative model to get a posterior distribution over the 50-best parses. This set of parses preserved some of the syntactic ambiguity present in the sentence. The Stanford parser deterministically converts phrase-structure trees into labeled dependency graphs (de Marneffe et al., 2006). We converted each"
W11-1825,N10-1123,1,0.766932,"ticipating systems, so it would be interesting to consider whether there are some annotations in the development set that cannot be predicted by any of the participating systems1. If this is the case, then those triggers and edges would present an interesting topic for discussion. This might result either in a modification of the annotation protocols, or an opportunity for all systems to learn more. After a certain amount of feature engineering, we found it difficult to achieve further improvements in F1. Perhaps we need a significant shift in architecture, such as a shift to joint inference (Poon and Vanderwende, 2010). Our system may be limited by the pipeline architecture. 1 Our system output for the 2011development set can be downloaded from http://research.microsoft.com/bionlp/ 162 MWEs (multi-word entities) are a challenge. Better multi-word triggers accuracy may improve system performance. Multi-word proteins often led to incorrect part-of-speech tags and parse trees. Cursory inspection of the Epigenetics task shows that some domain-specific knowledge would have been beneficial. Our system had significant difficulties with the rare inverse event types, e.g. “demethylation” (e.g., there are 319 example"
W11-1825,J08-1002,0,\N,Missing
W11-1825,D08-1022,0,\N,Missing
W11-1825,W09-1402,0,\N,Missing
W11-1825,D09-1098,0,\N,Missing
W97-0908,P88-1024,0,0.322816,"al NLP. I This work has benefited from comments and suggestions from other membersof the Natural LanguageProcessinggroupat MicrosoftResearch.Particularthanks go to SimonCorston.Bill Dolan, Ken Felder, KarenJensen, MartinePenenaro,Hisami Suzuki, and LucyVanderwende. One approach to multilingual development is to rely on theoretical concepts such as Universal Grammar. The goal is to create a grammar that can easily be parameterized to handle many languages. Wu (1994) describes an effort aimed at accounting for word order variation, but his focus is on the demonstration of a theoretical concept. Kameyama (1988) describes a prototype shared grammar for the syntax of simple nominal expressions for five languages, but the focus of the effort is only on the noun phrase, which makes the approach not applicable to a large-scale effort. Principle-based parsers are also designed with universal grammar in mind (Lin 1994), but have yet to demonstrate largescale coverage in several languages. Other efforts have been presented in the literature, with a focus on generation (Bateman et al. 1991.) An effort to port a grammar of English to French and Spanish is also underway at SRI (Rayner et al. 1996.) The approac"
W97-0908,C94-1079,0,0.0284078,"rely on theoretical concepts such as Universal Grammar. The goal is to create a grammar that can easily be parameterized to handle many languages. Wu (1994) describes an effort aimed at accounting for word order variation, but his focus is on the demonstration of a theoretical concept. Kameyama (1988) describes a prototype shared grammar for the syntax of simple nominal expressions for five languages, but the focus of the effort is only on the noun phrase, which makes the approach not applicable to a large-scale effort. Principle-based parsers are also designed with universal grammar in mind (Lin 1994), but have yet to demonstrate largescale coverage in several languages. Other efforts have been presented in the literature, with a focus on generation (Bateman et al. 1991.) An effort to port a grammar of English to French and Spanish is also underway at SRI (Rayner et al. 1996.) The approach taken in the MSNLP project focused from the beginning on possibilities for grammar sharing between languages to facilitate grammar development and reduce the development time. We want to stress that our use of the term &quot;grammar sharing&quot; is not to be confused with &quot;code sharing.&quot; Grammar sharing, in our u"
