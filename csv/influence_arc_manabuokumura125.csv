2020.aacl-main.10,W13-3512,0,0.0524236,"2 Related Work the hidden representation as follows: ~h = 1 2c A number of related research efforts have been done to help to learn better word embeddings aiming at different aspects. For example, Neelakantan et al. (2014) proposed an extension that learns multiple embeddings per word type. Ammar et al. (2016) proposed methods for estimating embeddings for different languages in a single shared embedding space. There is also a lot of work that incorporates internal information of words, such as character-level information (Chen et al., 2015; Bojanowski et al., 2017) and morpheme information (Luong et al., 2013; Qiu et al., 2014). Our research aims at another aspect and focuses on incorporating word clusters into the CBOW model, which has not been studied before. 3.1 ~xt+i , (1) i=−c,i6=0 where c is the window size. We use negative sampling (Mikolov et al., 2013b) to train the CBOW model by maximizing the following objective function: logσ(~hT ~ot ) + k X logσ(−~hT ~oj ), (2) j=1 where k is the size of the negative sample, ~oj is the j-th noise word embedding and σ is the sigmoid function. Each word in the negative sample is drawn from the unigram distribution. There have also been some previous res"
2020.aacl-main.10,Q17-1010,0,0.0942011,"020. 2020 Association for Computational Linguistics 2 Related Work the hidden representation as follows: ~h = 1 2c A number of related research efforts have been done to help to learn better word embeddings aiming at different aspects. For example, Neelakantan et al. (2014) proposed an extension that learns multiple embeddings per word type. Ammar et al. (2016) proposed methods for estimating embeddings for different languages in a single shared embedding space. There is also a lot of work that incorporates internal information of words, such as character-level information (Chen et al., 2015; Bojanowski et al., 2017) and morpheme information (Luong et al., 2013; Qiu et al., 2014). Our research aims at another aspect and focuses on incorporating word clusters into the CBOW model, which has not been studied before. 3.1 ~xt+i , (1) i=−c,i6=0 where c is the window size. We use negative sampling (Mikolov et al., 2013b) to train the CBOW model by maximizing the following objective function: logσ(~hT ~ot ) + k X logσ(−~hT ~oj ), (2) j=1 where k is the size of the negative sample, ~oj is the j-th noise word embedding and σ is the sigmoid function. Each word in the negative sample is drawn from the unigram distrib"
2020.aacl-main.10,D17-1309,0,0.0484502,"Missing"
2020.aacl-main.10,2012.eamt-1.60,0,0.0426019,"n PTB Wiki2 58.80 66.00 58.39 57.85 65.48 63.93 Table 1: Perplexity results on PTB and Wiki2. 4.3 Low-resource NMT We applied our method to the standard long-short term memory networks (LSTMs) based sequenceto-sequence (seq2seq) model on two datasets: German-English (de-en) with 153K sentence pairs 3 https://github.com/facebookresearch/fastText When we set the minimum count of word occurrence to 1, the standard CBOW does not perform well. 5 https://github.com/salesforce/awd-lstm-lm 4 82 from IWSLT 2014 (Cettolo et al., 2014), EnglishVietnamese (en-vi) with 133K sentence pairs from IWSLT 2015 (Cettolo et al., 2012). The detailed data statistics of two low-resource NMT datasets is in Table 2. We used the opennmt-py toolkit6 with a 2-layer bidirectional LSTM with hidden size of 500 and set the training epoch to 30. The word embedding size is set to 500 and the batch size is 64. We trained the seq2seq models by the SGD optimizer with start learning rate being 1.0, which will be decayed by 0.5 if perplexity does not decrease on the validation set. Other hyper-parameters were kept default. We also include some published results based on LSTM-based seq2seq models to gauge the result of our baseline. As shown"
2020.aacl-main.10,D14-1113,0,0.0195351,"1 We used ClusterCat (https://github.com/jonsafari/clustercat) as the implementation. 2 https://github.com/yukunfeng/cluster-cbow 80 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 80–86 c December 4 - 7, 2020. 2020 Association for Computational Linguistics 2 Related Work the hidden representation as follows: ~h = 1 2c A number of related research efforts have been done to help to learn better word embeddings aiming at different aspects. For example, Neelakantan et al. (2014) proposed an extension that learns multiple embeddings per word type. Ammar et al. (2016) proposed methods for estimating embeddings for different languages in a single shared embedding space. There is also a lot of work that incorporates internal information of words, such as character-level information (Chen et al., 2015; Bojanowski et al., 2017) and morpheme information (Luong et al., 2013; Qiu et al., 2014). Our research aims at another aspect and focuses on incorporating word clusters into the CBOW model, which has not been studied before. 3.1 ~xt+i , (1) i=−c,i6=0 where c is the window s"
2020.aacl-main.10,2014.iwslt-evaluation.1,0,0.0372576,"Missing"
2020.aacl-main.10,D14-1162,0,0.105114,"Missing"
2020.aacl-main.10,C14-1015,0,0.0246946,"hidden representation as follows: ~h = 1 2c A number of related research efforts have been done to help to learn better word embeddings aiming at different aspects. For example, Neelakantan et al. (2014) proposed an extension that learns multiple embeddings per word type. Ammar et al. (2016) proposed methods for estimating embeddings for different languages in a single shared embedding space. There is also a lot of work that incorporates internal information of words, such as character-level information (Chen et al., 2015; Bojanowski et al., 2017) and morpheme information (Luong et al., 2013; Qiu et al., 2014). Our research aims at another aspect and focuses on incorporating word clusters into the CBOW model, which has not been studied before. 3.1 ~xt+i , (1) i=−c,i6=0 where c is the window size. We use negative sampling (Mikolov et al., 2013b) to train the CBOW model by maximizing the following objective function: logσ(~hT ~ot ) + k X logσ(−~hT ~oj ), (2) j=1 where k is the size of the negative sample, ~oj is the j-th noise word embedding and σ is the sigmoid function. Each word in the negative sample is drawn from the unigram distribution. There have also been some previous researches that utiliz"
2020.aacl-main.10,N16-1139,0,0.0392281,"Missing"
2020.aacl-main.10,D11-1141,0,0.129519,"Missing"
2020.aacl-main.10,Q18-1032,0,0.054332,"Missing"
2020.aacl-main.10,P17-1184,0,0.0188732,"rameters of our standard LSTM model on language modeling task. en-vi 133,317 1,268 1,553 54,169 25,615 5 seq2seq with attention (Luong and Manning, 2015) AC+LL (Bahdanau et al., 2017) NPMT (Huang et al., 2018) Our seq2seq with attention CBOW Our ReIn+ReOut 5.1 Targeted Perplexity Results To show the gain for frequent and infrequent words, we measured the perplexity for frequent and infrequent words in the test data separately. Specifically, we calculated the perplexity of the next word, when an infrequent word is given as the current word. A similar analysis on language models can be found in Vania and Lopez (2017). Our analysis do not contain new words in the test dataset. The results are shown in Table 7. As we see, ReIn+ReOut is more effective than CBOW in learning both the embeddings of frequent and infrequent words, as we explained in Sec. 3.2.1. en-vi 23.3 27.69 28.16 28.24 28.67 Table 3: BLEU scores on two low-resource MT datasets. NPMT in Huang et al. (2018) used a neural phrase-based machine translation model and AC+LL in Bahdanau et al. (2017) used a one-layer GRU encoder and decoder with attention. 4.4 Analysis In this section, we analyse ReIn+ReOut on the basis of LM experiments with en and"
2020.aacl-main.10,D14-1108,0,0.0292458,"ost well-known methods for obtaining word embeddings is based on Continuous Bag-of-Words (CBOW) (Mikolov et al., 2013a) and there have been many research efforts to extend it. In this paper, we focus on incorporating word clusters into CBOW model. Each word cluster consists of words that function similarly. By aggregating such words, we can alleviate data sparsity, even though each of those words is infrequent. In the past few years, word clusters have been applied to various tasks, such as named-entity recognition (Ritter et al., 2011), machine translation (Wuebker et al., 2013) and parsing (Kong et al., 2014). Many word clustering algorithms can be applied to a raw corpus with different languages and help us obtain word clusters easily without additional language resources. In our method, we keep only very frequent words and replace the other words with their clusters for both input and output words in the CBOW model. We evaluate our cluster-incorporated word embeddings2 on downstream tasks, in which finetuning of word embeddings is involved. The evaluation for frequent words, for which our method also works well, on word similarity tasks can be found in appendix A. For the downstream tasks, we ch"
2020.aacl-main.10,D13-1138,0,0.0297482,"semantic information. One of the most well-known methods for obtaining word embeddings is based on Continuous Bag-of-Words (CBOW) (Mikolov et al., 2013a) and there have been many research efforts to extend it. In this paper, we focus on incorporating word clusters into CBOW model. Each word cluster consists of words that function similarly. By aggregating such words, we can alleviate data sparsity, even though each of those words is infrequent. In the past few years, word clusters have been applied to various tasks, such as named-entity recognition (Ritter et al., 2011), machine translation (Wuebker et al., 2013) and parsing (Kong et al., 2014). Many word clustering algorithms can be applied to a raw corpus with different languages and help us obtain word clusters easily without additional language resources. In our method, we keep only very frequent words and replace the other words with their clusters for both input and output words in the CBOW model. We evaluate our cluster-incorporated word embeddings2 on downstream tasks, in which finetuning of word embeddings is involved. The evaluation for frequent words, for which our method also works well, on word similarity tasks can be found in appendix A."
2020.aacl-main.10,2015.iwslt-evaluation.11,0,0.0184906,"Missing"
2020.coling-main.192,W15-4319,0,0.0322076,"Missing"
2020.coling-main.192,N19-1423,0,0.0145435,"Missing"
2020.coling-main.192,W19-4427,0,0.0559945,"Missing"
2020.coling-main.192,D12-1039,0,0.0298724,"important for a human to understand non-standard words, current neural network-based text normalization methods do not consider this information. We assume that text normalization more intuitive to humans is possible by explicitly considering such features in a neural network-based method. Based on this assumption, in this work, we propose neural text normalization models that leverage both string and sound similarities. Experimental results show that our proposed models outperformed a baseline and achieved state-of-the-art results in the text normalization track on WNUT-2015. 2 Related Work Han et al. (2012) proposed a ranking-based text normalization method that incorporates the matching degree of surrounding word n-grams to the target word and the edit distance from existing words. Li and Liu (2012) proposed a text normalization method leveraging phonetic information to translate nonstandard words into standard ones. Ansari et al. (2017) proposed an automatic optimization-based nearest neighbor matching approach leveraging string and phonetic similarity. Jin (2015) achieved the best This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/license"
2020.coling-main.192,W15-4313,0,0.0257767,"s outperformed a baseline and achieved state-of-the-art results in the text normalization track on WNUT-2015. 2 Related Work Han et al. (2012) proposed a ranking-based text normalization method that incorporates the matching degree of surrounding word n-grams to the target word and the edit distance from existing words. Li and Liu (2012) proposed a text normalization method leveraging phonetic information to translate nonstandard words into standard ones. Ansari et al. (2017) proposed an automatic optimization-based nearest neighbor matching approach leveraging string and phonetic similarity. Jin (2015) achieved the best This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 2126 Proceedings of the 28th International Conference on Computational Linguistics, pages 2126–2131 Barcelona, Spain (Online), December 8-13, 2020 【 Architecture of Deep Levenshtein 】 【 Architecture of Seq2Seq 】 Normalized edit distance Attention Encoder Cosine similarity Decoder you LSTM LSTM LSTM LSTM Embedding layer Embedding layer … + Word: x coming Token + + … you Character string Word: y Figure 1: How to introduce the char"
2020.coling-main.192,N19-2024,0,0.0189753,"ore Af ter yes → AS Bef ore Af ter By using Double Metaphone, similar to Deep Levenshtein, Deep Metaphone can predict the sound similarity between two words. Therefore, we can use the vector that captures the feature of the sound. 3.3 Incorporating new features to Seq2Seq In this study, we use a bi-directional LSTM for the encoder and decoder. Lourentzou et al. (2019) incoras an input to the encoder. We further incorporate the character porated only token embeddings etoken xt feature cleven , obtained from Deep Levenshtein, and the sound feature cphone , obtained from Deep Metaxt xt 2 phone. Mansfield et al. (2019) empirically tested addition, concatenation, and multi-layer perceptron to combine new features with token embeddings. They reported that concatenation outperforms the are and cphone other two methods. Therefore, we choose concatenation. Our new feature vectors cleven xt xt incorporated into the Seq2Seq encoder as follows: henc = Encoder([etoken ; cleven ; cphone ; henc t xt xt xt t−1 ]). 4 (2) Experiments 4.1 Experimental settings We used WNUT-2015 Shared Task2 (Baldwin et al., 2015),3 which is a task of normalizing social media texts, for our evaluation. The official dataset consists of 4,91"
2020.coling-main.192,P18-1186,0,0.0145464,"Mani et al. (2020) used Seq2Seq in automatic speech recognition error correction, a task similar to text normalization. Taking these trends into account, in this work, we propose a neural text normalization model that leverages both string and sound similarity. 3 Methodology Figure 1 shows an overview of our proposed method. In this study, we perform text normalization based on the method of Lourentzou et al. (2019), which incorporates token embeddings as an input to the encoder. Our method expands their work by utilizing features related to character strings and sounds. 3.1 Deep Levenshtein Moon et al. (2018) proposed Deep Levenshtein, a network that captures the feature of character strings based on the Levenshtein edit distance (Levenshtein, 1966) in order to correct any character fluctuations of named entities in a text. In this study, we incorporate this mechanism into the Seq2Seq model to make it more robust to a broken text. Deep Levenshtein is a neural network that takes two words x and y as an input and then outputs hidden representations for the character strings of the words. Two words x and y are fed into the word embedding layer, and then we obtain word embeddings ex and ey , respectiv"
2020.coling-main.192,N18-1202,0,0.0554979,"Missing"
2020.coling-main.192,D19-3011,0,0.0241981,"characters delete characters randomly python → pyhon 3. Replace characters replace characters randomly python → pyhtno 4. Extend characters extend words ending with {u, y, s, r} beer → beerrrr 5. Extend short vowels stretch short vowels {a, i, u, e, o} cat → caaat 6. Delete symbol delete apostrophe I’m → Im 7. Misplaced sign insert apostrophe in different position don’t → do’nt 8. Typo replace with another character that is at a near position on a keyboard hello → jello 9. Convert to another token convert to completely different token python → ruby Table 1: Noise generator. 3.2 Deep Metaphone Raghuvanshi et al. (2019) revealed that relying only on surface text similarities cannot capture phonetic differences between words. Furthermore, Han et al. (2013) showed that sound-related features are effective in text normalization. In this work, we propose Deep Metaphone to capture sound features for text normalization by learning phonetic edit distance. Deep Metaphone has the same network structure as Deep Levenshtein. The difference between them is the training data they use. Deep Metaphone learns the phonetic edit distance, which is the edit distance of the strings obtained from Double Metaphone (Philips, 2000)"
2020.coling-main.192,W16-2209,0,0.0272688,"model were set to the same as , of LSTM and cphone in (Lourentzou et al., 2019), where it was 100. The sizes of hidden layers, cleven xt xt for Deep Levenshtein and Deep Metaphone were tuned from {10, 20, 30, 40, 50} on the validation data, randomly extracted 100 sentences from the training data. When only Deep Levenshtein was used, 20 was selected. When only Deep Metaphone was used, 50 was selected. When both were used, 10 was selected for each of them. The reason why we set the range of smaller values from 10 to 50 compared with the size of token embeddings, 100, is based on the finding in (Sennrich and Haddow, 2016). They in our case, should be smaller reported that the size of the secondary embeddings, i.e., cleven and cphone xt xt token than that of the primary embeddings, i.e., ext in our case. 4.2 Compared models In the experiments, we compared a baseline model and our models, which are listed below. Two-stage Seq2Seq (baseline): A model proposed by Lourentzou et al. (2019). We reimplemented this model and will report its performance in addition to the scores reported in their paper. Two-stage Seq2Seq + LS (Levenshtein): A model where we add character string features to the input of the encoder in th"
2020.coling-main.28,W05-0909,0,0.13943,"an input snippet of source code and with a hierarchical copy mechanism. Our models outperformed the existing methods in terms of our modified F1 and accuracy. Our proposed copy mechanism is applicable to tree-structured inputs such as discourse structures, cooking recipes, and social network services. Moreover, replacing the most frequent subword seems to be useful in tasks where the vocabulary is relatively small. There remain two major issues to address. The first is the need for better evaluation metrics. We believe that this task requires a metric that can accept synonyms such as METEOR (Banerjee and Lavie, 2005). However, some words that are considered synonymous in WordNet8 are used differently in the context of source code. For example, increment is an operation that increases the value of a variable by 1 in source code. It cannot be replaced with a word such as increase, even if they are synonymous with each other. Therefore, we need an evaluation metric that takes into account the subtle difference between synonyms. The second is to consider context in source code. Our approach generates function names only from the information inside the function. However, the behavior of other functions and the"
2020.coling-main.28,N18-2097,0,0.0501046,"Expr LSTM Output(t-2) index Binary Expr:equals Block Stmt Output(t-1) Of Output(t-1) target Enc(subwords1) Unary Expr:pos increment Block Stmt Binary Expr:equals Bi-LSTM IfStmt Return Stmt Expression Stmt Expression Stmt Decoder BlockStmt target_key Copy index index target_key Copy-Dist target target_key key Figure 2: Overview of the function naming with our model. The input for the encoder is not just a sequence of tokens but a set of paths from a leaf to another leaf in the tree. Thus, the existing copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Yang et al., 2018; Hsu et al., 2018; Cohan et al., 2018) cannot be directly applied. We observed that our best-performing model was the one that uses a combination of a hierarchical copy mechanism and a strategy to replace the most frequent word in an input snippet of source code with a delexicalized placeholder. In particular, the score of the best-performing model was increased in terms of our modified F1 and accuracy, calculated on the Java-small and Java-large2 datasets by Alon et al. (2019a). 2 Code2seq We first describe code2seq (Alon et al., 2019a), an existing model that we extend in this paper. Code2seq first converts an input snippet of s"
2020.coling-main.28,P16-1154,0,0.160865,"Expr IfStmt Object Variable Declaration Field Access Expr LSTM Output(t-2) index Binary Expr:equals Block Stmt Output(t-1) Of Output(t-1) target Enc(subwords1) Unary Expr:pos increment Block Stmt Binary Expr:equals Bi-LSTM IfStmt Return Stmt Expression Stmt Expression Stmt Decoder BlockStmt target_key Copy index index target_key Copy-Dist target target_key key Figure 2: Overview of the function naming with our model. The input for the encoder is not just a sequence of tokens but a set of paths from a leaf to another leaf in the tree. Thus, the existing copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Yang et al., 2018; Hsu et al., 2018; Cohan et al., 2018) cannot be directly applied. We observed that our best-performing model was the one that uses a combination of a hierarchical copy mechanism and a strategy to replace the most frequent word in an input snippet of source code with a delexicalized placeholder. In particular, the score of the best-performing model was increased in terms of our modified F1 and accuracy, calculated on the Java-small and Java-large2 datasets by Alon et al. (2019a). 2 Code2seq We first describe code2seq (Alon et al., 2019a), an existing model that we extend in"
2020.coling-main.28,P16-1014,0,0.16206,"pr Variable Declaration Expr IfStmt Object Variable Declaration Field Access Expr LSTM Output(t-2) index Binary Expr:equals Block Stmt Output(t-1) Of Output(t-1) target Enc(subwords1) Unary Expr:pos increment Block Stmt Binary Expr:equals Bi-LSTM IfStmt Return Stmt Expression Stmt Expression Stmt Decoder BlockStmt target_key Copy index index target_key Copy-Dist target target_key key Figure 2: Overview of the function naming with our model. The input for the encoder is not just a sequence of tokens but a set of paths from a leaf to another leaf in the tree. Thus, the existing copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Yang et al., 2018; Hsu et al., 2018; Cohan et al., 2018) cannot be directly applied. We observed that our best-performing model was the one that uses a combination of a hierarchical copy mechanism and a strategy to replace the most frequent word in an input snippet of source code with a delexicalized placeholder. In particular, the score of the best-performing model was increased in terms of our modified F1 and accuracy, calculated on the Java-small and Java-large2 datasets by Alon et al. (2019a). 2 Code2seq We first describe code2seq (Alon et al., 2019a), an existing model"
2020.coling-main.28,P17-1019,0,0.0171408,"(2019) used a hierarchical attention network for function name generation. In this model, the important information of the lower layer is passed to the upper layer by a recursive network. Our model also took into account the hierarchical structure in our copy mechanism, as described in Section 3.2. We extended code2seq by adding the ability to copy subwords in the input source code. The copy mechanism is a technique that copies subwords in the input to the output (Gu et al., 2016; Gulcehre et al., 2016). Copy mechanisms have been shown to be effective in many tasks such as question-answering (He et al., 2017), document summarization (See et al., 2017), headline generation (Nallapati et al., 2016) and question generation (Zhao et al., 2018). The existing copy mechanisms (Nallapati et al., 2016) presuppose a sequence of words as an input. Although Yang et al. (2018) and Hsu et al. (2018) proposed a copy mechanism with hierarchical attention networks at word and sentence levels and Cohan et al. (2018) proposed a copy mechanism with hierarchical attention networks at word and section levels, they both assumed the input is a sequence of words, sentences, or sections. Thus, their copy mechanisms cannot"
2020.coling-main.28,P18-1013,0,0.0454148,"tion Field Access Expr LSTM Output(t-2) index Binary Expr:equals Block Stmt Output(t-1) Of Output(t-1) target Enc(subwords1) Unary Expr:pos increment Block Stmt Binary Expr:equals Bi-LSTM IfStmt Return Stmt Expression Stmt Expression Stmt Decoder BlockStmt target_key Copy index index target_key Copy-Dist target target_key key Figure 2: Overview of the function naming with our model. The input for the encoder is not just a sequence of tokens but a set of paths from a leaf to another leaf in the tree. Thus, the existing copy mechanisms (Gulcehre et al., 2016; Gu et al., 2016; Yang et al., 2018; Hsu et al., 2018; Cohan et al., 2018) cannot be directly applied. We observed that our best-performing model was the one that uses a combination of a hierarchical copy mechanism and a strategy to replace the most frequent word in an input snippet of source code with a delexicalized placeholder. In particular, the score of the best-performing model was increased in terms of our modified F1 and accuracy, calculated on the Java-small and Java-large2 datasets by Alon et al. (2019a). 2 Code2seq We first describe code2seq (Alon et al., 2019a), an existing model that we extend in this paper. Code2seq first converts"
2020.coling-main.28,P16-1195,0,0.133134,"whose elem.key is the same as target key. However, the function name would be inappropriate as it implies that the function returns the value of the object. Such a function name adversely affects readability and sometimes causes bugs, especially in collaborative environments. A proper function name such as indexOfTarget in this case, instead of getTargetValue, can help programmers understand the code efficiently and avoid possible bugs (Takang et al., 1996; Binkley et al., 2013). Automatically generating such function names has been studied as a generation task in natural language processing (Iyer et al., 2016). Figure 1: Example of a function and its inappropriate name. (Java) Recently, various neural network-based approaches have been proposed to solve this problem by generating a function name from given source code (Allamanis et al., 2016; Alon et al., 2018; Fernandes et al., 2018). In these approaches, a function name is treated as a sequence of subwords (get, Target and Value in Figure 1). Since these approaches heavily rely on a subword-based predefined dictionary to generate a function name, it is difficult to generate a function name containing low-frequency or unknown subwords. To solve th"
2020.coling-main.28,W04-3250,0,0.0647436,"Missing"
2020.coling-main.28,W04-1013,0,0.0164829,"positons just before an uppercase character follows lowercase characters because programmers generally use camel case when writing code with Java. Long variable names were truncated to have at most 6 subwords. We used only the paths that had less than 9 subwords. We used TensorFlow to implement our models. We used F1 as an evaluation metric, following Alon et al. (2019a), and added accuracy as another. Furthermore, to correctly evaluate outputs with repeating tokens, we also used modified-F1 (F1**), calculated with the modified unigram precision of Papineni et al. (2002) and unigram recall of Lin (2004). F1** can prevent the models that repeatedly output subwords in the Gold function name from unreasonably obtaining high scores. We calculated the above metrics on the basis of the number of subwords. The accuracy measure was defined to be the number of correctly generated function names divided by the total number of test instances. Here, we supposed an output is correct only if it is completely the same as the gold function name, while we calculated the other metrics by counting the overlap of subwords between generated function names and gold function names. We trained and evaluated each mo"
2020.coling-main.28,P17-2045,0,0.0133674,"ble Our model Table 3: Outputs for the top box of Figure 5 5 Output contains morpheme has data is morpheme single analysis morpheme morpheme morpheme name morpheme has morpheme Table 4: Outputs for the bottom box of Figure 5 Related Work There have been a lot of research efforts on tasks where source code is the input. Hindle et al. (2012) and Babii et al. (2019) constructed language models for the source code. Raychev et al. (2015) proposed a method for outputting variable names in the source code. Iyer et al. (2016) proposed a model to summarize the behavior of functions in the source code. Loyola et al. (2017) proposed a method for generating descriptions of source code changes. 323 While these studies focus on source code as an input, their outputs are not function names. As a method for representing source code, Allamanis et al. (2015b) converted a snippet of the source code into AST and proposed a method for generating a short description of the behavior of the snippet. Allamanis et al. (2018) later proposed a method for detecting inappropriate variable names using AST. We also used AST to represent the input snippet of source code while many other researches treat the source code as a sequence"
2020.coling-main.28,D15-1166,0,0.386541,"nning and end of the sequence, respectively. 2.3 Decoder with Attention The decoder inherits the averaged vector of all possible paths between the leaf nodes in the AST as an initial state s0 . To prevent the computational space from becoming too large, the maximum number of paths is set to 200; if there are more than 200 paths, 200 paths are randomly selected. At each time step t, the decoder calculates the current hidden state st = LST M (st−1 , yt−1 ), where yt−1 is the embedding of the predicted subword in the previous time step. By using st , the decoder calculates the attention weights (Luong et al., 2015) on the paths, each of which connects two leaf nodes. The weight on the r-th path is defined as follows: exp(dTa tanh(Wa [st ; qr ])) , Σr′ exp(dTa tanh(Wa [st ; qr′ ])) X δiT sof tmax(Wl [Σr atr qr ; st ]), pvoc (w) = atr = (1) (2) i:w=wi where qr is the vector representation of the r-th path in the encoder, Wa is a weight matrix for the linear transformation, and da is a parameter vector. Finally, the output layer calculates the label distribution at time step t as pvoc (w), where Wl is a weight matrix. δi is a one-hot vector, where only the i-th element is 1, and the others are 0. wi is the"
2020.coling-main.28,K16-1028,0,0.0172367,"model, the important information of the lower layer is passed to the upper layer by a recursive network. Our model also took into account the hierarchical structure in our copy mechanism, as described in Section 3.2. We extended code2seq by adding the ability to copy subwords in the input source code. The copy mechanism is a technique that copies subwords in the input to the output (Gu et al., 2016; Gulcehre et al., 2016). Copy mechanisms have been shown to be effective in many tasks such as question-answering (He et al., 2017), document summarization (See et al., 2017), headline generation (Nallapati et al., 2016) and question generation (Zhao et al., 2018). The existing copy mechanisms (Nallapati et al., 2016) presuppose a sequence of words as an input. Although Yang et al. (2018) and Hsu et al. (2018) proposed a copy mechanism with hierarchical attention networks at word and sentence levels and Cohan et al. (2018) proposed a copy mechanism with hierarchical attention networks at word and section levels, they both assumed the input is a sequence of words, sentences, or sections. Thus, their copy mechanisms cannot be directly applied to our setting because each input is assumed to be a set of paths in"
2020.coling-main.28,P02-1040,0,0.106701,"urce code into a sequence of subwords at the positons just before an uppercase character follows lowercase characters because programmers generally use camel case when writing code with Java. Long variable names were truncated to have at most 6 subwords. We used only the paths that had less than 9 subwords. We used TensorFlow to implement our models. We used F1 as an evaluation metric, following Alon et al. (2019a), and added accuracy as another. Furthermore, to correctly evaluate outputs with repeating tokens, we also used modified-F1 (F1**), calculated with the modified unigram precision of Papineni et al. (2002) and unigram recall of Lin (2004). F1** can prevent the models that repeatedly output subwords in the Gold function name from unreasonably obtaining high scores. We calculated the above metrics on the basis of the number of subwords. The accuracy measure was defined to be the number of correctly generated function names divided by the total number of test instances. Here, we supposed an output is correct only if it is completely the same as the gold function name, while we calculated the other metrics by counting the overlap of subwords between generated function names and gold function names."
2020.coling-main.28,P17-1099,0,0.15993,"+ wc′ T gt . (9) Experiments 4.1 Experimental Settings We evaluated our approaches on the following two datasets: Java-small and Java-large.4 Java-small consists of 691,974 functions for training, 23,844 for development, and 57,088 for testing. Java-large 4 https://github.com/tech-Srl/code2seq#datasets 319 consists of 15,344,512 functions for training, 320,866 for development, and 417,003 for testing. The models for comparison are as follows: • Code2seq We described the model in Section 2. We reran the code5 of Alon et al. (2019a). • Copy This is a 2-layer LSTM-based pointer-generator model (See et al., 2017). We experimented with OpenNMT-py6 with the copy attn option. • Pointer This is a variant of our hierarchical copy mechanism. Following the decoder of Fernandes et al. (2018),7 this model only points to tokens (Vinyals et al., 2015) and does not generate any tokens. This model was prepared to verify the report of Fernandes et al. (2018) that a pointer-network works effectively and yields higher F1 scores than code2seq on the function naming task. For training all the models, we used momentum-SGD (Qian, 1999) as an optimizer. The batch size was set to 256, and the dimension of subword embedding"
2020.coling-main.28,D18-1424,0,0.0140142,"yer is passed to the upper layer by a recursive network. Our model also took into account the hierarchical structure in our copy mechanism, as described in Section 3.2. We extended code2seq by adding the ability to copy subwords in the input source code. The copy mechanism is a technique that copies subwords in the input to the output (Gu et al., 2016; Gulcehre et al., 2016). Copy mechanisms have been shown to be effective in many tasks such as question-answering (He et al., 2017), document summarization (See et al., 2017), headline generation (Nallapati et al., 2016) and question generation (Zhao et al., 2018). The existing copy mechanisms (Nallapati et al., 2016) presuppose a sequence of words as an input. Although Yang et al. (2018) and Hsu et al. (2018) proposed a copy mechanism with hierarchical attention networks at word and sentence levels and Cohan et al. (2018) proposed a copy mechanism with hierarchical attention networks at word and section levels, they both assumed the input is a sequence of words, sentences, or sections. Thus, their copy mechanisms cannot be directly applied to our setting because each input is assumed to be a set of paths in AST. Fernandes et al. (2018) proposed a meth"
2020.coling-main.424,P14-1085,0,0.0277713,"the ranking of entities, based on the corresponding value such as the building’s height. Given the ranked list in the table, they tracked an interesting value and paired it with a candidate template to generate trivia facts. Unlike previous methods, the proposed HTM algorithm extracts trivia facts for a given entity, using a single Wikipedia article based on its hierarchical structure and summary. Using hierarchical information is currently widely applied in various tasks as a good indicator to improve the task performance, such as text classification (Yang et al., 2016), text summarization (Christensen et al., 2014), language modeling (Gulordava et al., 2018), and neural machine translation (Shi et al., 2016). To the best of our knowledge, the proposed HTM algorithm represents the first approach that exploits the hierarchical information in a Wikipedia article. 3 Hierarchical Trivia Miner In this section, we describe the proposed HTM algorithm. Algorithm 1 shows the architecture of the HTM algorithm. 3.1 Top-down Processing HTM receives a single Wikipedia article for the target entity and extracts trivia facts. It performs topdown processing using a surprise score (in Section 3.2) to extract trivia sente"
2020.coling-main.424,N18-1108,0,0.0116959,"onding value such as the building’s height. Given the ranked list in the table, they tracked an interesting value and paired it with a candidate template to generate trivia facts. Unlike previous methods, the proposed HTM algorithm extracts trivia facts for a given entity, using a single Wikipedia article based on its hierarchical structure and summary. Using hierarchical information is currently widely applied in various tasks as a good indicator to improve the task performance, such as text classification (Yang et al., 2016), text summarization (Christensen et al., 2014), language modeling (Gulordava et al., 2018), and neural machine translation (Shi et al., 2016). To the best of our knowledge, the proposed HTM algorithm represents the first approach that exploits the hierarchical information in a Wikipedia article. 3 Hierarchical Trivia Miner In this section, we describe the proposed HTM algorithm. Algorithm 1 shows the architecture of the HTM algorithm. 3.1 Top-down Processing HTM receives a single Wikipedia article for the target entity and extracts trivia facts. It performs topdown processing using a surprise score (in Section 3.2) to extract trivia sentences, starting from a section to a sentence."
2020.coling-main.424,W04-3250,0,0.0767055,"Missing"
2020.coling-main.424,Y18-1031,0,0.124255,"rivia question from obtained relational expressions. Rather than using a relational database, Lin and Chalupsky (2003) used the notion of “rarity” to measure interestingness to discover interesting facts and hidden connections such as relationships between people. Recently, Prakash et al. (2015) demonstrated that a Wikipedia article for a given entity can be a good source to mine trivia facts. In response to this report, there have been several attempts to develop automatic trivia fact extraction algorithms for various Wikipedia article domains such as movies (Prakash et al., 2015), TV shows (Niina and Shimada, 2018), and people (Tsurel et al., 2017). In addition, DBpedia, which provides structured data for Wikipedia contents, can be used to mine trivia facts for the domains of artists and actors (Fatma et al., 2017). Despite the success of the previous studies, however, the existing trivia fact mining algorithms face the following two issues: they are strongly dependent on the target domains and incur high computational cost. To address these issues, we aim to develop a new automatic trivia fact extraction algorithm that focuses on the Wikipedia’s hierarchical structure. The structure in a Wikipedia arti"
2020.coling-main.424,D16-1159,0,0.0198504,"ked list in the table, they tracked an interesting value and paired it with a candidate template to generate trivia facts. Unlike previous methods, the proposed HTM algorithm extracts trivia facts for a given entity, using a single Wikipedia article based on its hierarchical structure and summary. Using hierarchical information is currently widely applied in various tasks as a good indicator to improve the task performance, such as text classification (Yang et al., 2016), text summarization (Christensen et al., 2014), language modeling (Gulordava et al., 2018), and neural machine translation (Shi et al., 2016). To the best of our knowledge, the proposed HTM algorithm represents the first approach that exploits the hierarchical information in a Wikipedia article. 3 Hierarchical Trivia Miner In this section, we describe the proposed HTM algorithm. Algorithm 1 shows the architecture of the HTM algorithm. 3.1 Top-down Processing HTM receives a single Wikipedia article for the target entity and extracts trivia facts. It performs topdown processing using a surprise score (in Section 3.2) to extract trivia sentences, starting from a section to a sentence. Thus, HTM first ranks each section compared to the"
2020.coling-main.424,N16-1174,0,0.0107471,"ts. The rows in the tables are sorted as the ranking of entities, based on the corresponding value such as the building’s height. Given the ranked list in the table, they tracked an interesting value and paired it with a candidate template to generate trivia facts. Unlike previous methods, the proposed HTM algorithm extracts trivia facts for a given entity, using a single Wikipedia article based on its hierarchical structure and summary. Using hierarchical information is currently widely applied in various tasks as a good indicator to improve the task performance, such as text classification (Yang et al., 2016), text summarization (Christensen et al., 2014), language modeling (Gulordava et al., 2018), and neural machine translation (Shi et al., 2016). To the best of our knowledge, the proposed HTM algorithm represents the first approach that exploits the hierarchical information in a Wikipedia article. 3 Hierarchical Trivia Miner In this section, we describe the proposed HTM algorithm. Algorithm 1 shows the architecture of the HTM algorithm. 3.1 Top-down Processing HTM receives a single Wikipedia article for the target entity and extracts trivia facts. It performs topdown processing using a surprise"
2020.coling-main.464,C08-2006,0,0.123089,"Missing"
2020.coling-main.464,P19-1098,0,0.142958,"ormation retrieval and summarization, and methods using maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998) have been proposed (Boudin et al., 2008; Guo and Sanner, 2010). DPPs (Macchi, 1975; Aho and Ullman, 1972) has also been proposed as a way for taking account of diversity. This method performs better than MMR in these ﬁelds. DPPs have been applied to recommendation problems as a way of taking diversity into account (Wilhelm et al., 2018; Chen et al., 2018; Mariet et al., 2019). Multi-document summarization models combine DPPs and neural networks to take account of diversity (Cho et al., 2019a; Cho et al., 2019b). DPPs have been also used in studies on clustering verbs (Reichart and Korhonen, 2013) and extracting example sentences with speciﬁc words (Tolmachev and Kurohashi, 2017). 5310 Training 0 &lt;latexit sha1_base64=""FntAtNFeatnpYacTL6ffSPljjyY="">AAAENnichZFNa9RAGMefTXyp8aVbvQhegktLvSyTF6iIQsGLhx764raFZlkm6Ww6dHYSktmFNeQL+AU8eFIQEb+Ady8evXjoxZuIeBEqePHgs9moXZtdn5DMk/8z//k9M+PHgqeKkKOapp85e+783AXj4qXLV+brC1e306ifBKwVRCJKdn2aMsElaymuBNuNE0Z7vmA7/uH9UX1nwJKUR/KhGsas3aOh5F0eUIVSZ6H2fs28Z3qCdZW5bHg+C7nMaJLQYZ4FGLlhFuEVqAzdTCqaZ2udzLLy3Fz6XfFFn41lu1p2qmUXZc+bpPyp2hOIk3C7mmJXU+yZFKd6I4"
2020.coling-main.464,D19-5412,0,0.0644121,"ormation retrieval and summarization, and methods using maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998) have been proposed (Boudin et al., 2008; Guo and Sanner, 2010). DPPs (Macchi, 1975; Aho and Ullman, 1972) has also been proposed as a way for taking account of diversity. This method performs better than MMR in these ﬁelds. DPPs have been applied to recommendation problems as a way of taking diversity into account (Wilhelm et al., 2018; Chen et al., 2018; Mariet et al., 2019). Multi-document summarization models combine DPPs and neural networks to take account of diversity (Cho et al., 2019a; Cho et al., 2019b). DPPs have been also used in studies on clustering verbs (Reichart and Korhonen, 2013) and extracting example sentences with speciﬁc words (Tolmachev and Kurohashi, 2017). 5310 Training 0 &lt;latexit sha1_base64=""FntAtNFeatnpYacTL6ffSPljjyY="">AAAENnichZFNa9RAGMefTXyp8aVbvQhegktLvSyTF6iIQsGLhx764raFZlkm6Ww6dHYSktmFNeQL+AU8eFIQEb+Ady8evXjoxZuIeBEqePHgs9moXZtdn5DMk/8z//k9M+PHgqeKkKOapp85e+783AXj4qXLV+brC1e306ifBKwVRCJKdn2aMsElaymuBNuNE0Z7vmA7/uH9UX1nwJKUR/KhGsas3aOh5F0eUIVSZ6H2fs28Z3qCdZW5bHg+C7nMaJLQYZ4FGLlhFuEVqAzdTCqaZ2udzLLy3Fz6XfFFn41lu1p2qmUXZc+bpPyp2hOIk3C7mmJXU+yZFKd6I4"
2020.coling-main.464,N19-1423,0,0.0789434,"fting their center of gravity on the vertical line of their feet to stabilize their bodies. I think it’s to use their feather as pillows. gold Table 1: Example of a question and its answers where multiple answers are considered to be appropriate. The answer in bold is the best answer. Sentences in the same color indicate they have the same content. Gold indicates answers included in the reference answer set. subsets of a given dataset and give a higher probability mass to more diverse and non-redundant subsets. To estimate the answer importance and the similarity between answers, we use BERT (Devlin et al., 2019), which has achieved high accuracy in various tasks. Focusing on a Japanese CQA site, we built training and evaluation datasets through crowdsourcing and conducted experiments demonstrating that the proposed method outperformed several baseline methods. 2 Related Work 2.1 Work on CQA CQA has been actively studied (Nakov et al., 2017; Nakov et al., 2016; Nakov et al., 2015). The most relevant task to our study is answer selection. There are two approaches to the answer selection task. One approach is to predict the best answers (Tian et al., 2013; Dong et al., 2015). The other is to rank the en"
2020.coling-main.464,2020.lrec-1.676,0,0.138255,"y one appropriate answer for a question that asks for facts, etc. For example, Table 1 shows example answers to the question “Why do birds turn their heads backwards when they sleep?”, which asks for reasons. The best answer A1 says it is to keep out the cold and A4 says it is for balance, which is also an appropriate answer. In such a case, if users view only the best answer, they miss other appropriate answers. One possible way to deal with this problem is to use ranking methods (Wang et al., 2009; Xue et al., 2008; Bian et al., 2008; Zhao et al., 2017; Tay et al., 2017; Zhang et al., 2017; Laskar et al., 2020). Since the studies use the best answers as training data, similar answers are ranked higher although the existing methods try to reduce redundancy by using the similarity between a question and an answer or between answers. As a result, users need to read through carefully the answers that are ranked higher to obtain the information they want to know. We propose a new task of selecting a diverse and non-redundant answer set from all the answers, instead of ranking the answers. We treat the set of answers as the input, from which the system determines both the appropriate number of answers to"
2020.coling-main.464,S15-2047,0,0.0398627,"Missing"
2020.coling-main.464,S17-2003,0,0.0479026,"Missing"
2020.coling-main.464,P13-1085,0,0.0178991,"ell and Goldstein, 1998) have been proposed (Boudin et al., 2008; Guo and Sanner, 2010). DPPs (Macchi, 1975; Aho and Ullman, 1972) has also been proposed as a way for taking account of diversity. This method performs better than MMR in these ﬁelds. DPPs have been applied to recommendation problems as a way of taking diversity into account (Wilhelm et al., 2018; Chen et al., 2018; Mariet et al., 2019). Multi-document summarization models combine DPPs and neural networks to take account of diversity (Cho et al., 2019a; Cho et al., 2019b). DPPs have been also used in studies on clustering verbs (Reichart and Korhonen, 2013) and extracting example sentences with speciﬁc words (Tolmachev and Kurohashi, 2017). 5310 Training 0 &lt;latexit sha1_base64=""FntAtNFeatnpYacTL6ffSPljjyY="">AAAENnichZFNa9RAGMefTXyp8aVbvQhegktLvSyTF6iIQsGLhx764raFZlkm6Ww6dHYSktmFNeQL+AU8eFIQEb+Ady8evXjoxZuIeBEqePHgs9moXZtdn5DMk/8z//k9M+PHgqeKkKOapp85e+783AXj4qXLV+brC1e306ifBKwVRCJKdn2aMsElaymuBNuNE0Z7vmA7/uH9UX1nwJKUR/KhGsas3aOh5F0eUIVSZ6H2fs28Z3qCdZW5bHg+C7nMaJLQYZ4FGLlhFuEVqAzdTCqaZ2udzLLy3Fz6XfFFn41lu1p2qmUXZc+bpPyp2hOIk3C7mmJXU+yZFKd6I449De4gxahyzMS41Ri3eieuM43unqB4TO6X12V4CQ8P1K1OvUGapAjzdGKVSQPKWI/qL8GDfYgggD70gIEEhbkACik+e2ABgRi1NmSoJZjxos"
2020.coling-main.464,W17-5014,0,0.0143733,"2010). DPPs (Macchi, 1975; Aho and Ullman, 1972) has also been proposed as a way for taking account of diversity. This method performs better than MMR in these ﬁelds. DPPs have been applied to recommendation problems as a way of taking diversity into account (Wilhelm et al., 2018; Chen et al., 2018; Mariet et al., 2019). Multi-document summarization models combine DPPs and neural networks to take account of diversity (Cho et al., 2019a; Cho et al., 2019b). DPPs have been also used in studies on clustering verbs (Reichart and Korhonen, 2013) and extracting example sentences with speciﬁc words (Tolmachev and Kurohashi, 2017). 5310 Training 0 &lt;latexit sha1_base64=""FntAtNFeatnpYacTL6ffSPljjyY="">AAAENnichZFNa9RAGMefTXyp8aVbvQhegktLvSyTF6iIQsGLhx764raFZlkm6Ww6dHYSktmFNeQL+AU8eFIQEb+Ady8evXjoxZuIeBEqePHgs9moXZtdn5DMk/8z//k9M+PHgqeKkKOapp85e+783AXj4qXLV+brC1e306ifBKwVRCJKdn2aMsElaymuBNuNE0Z7vmA7/uH9UX1nwJKUR/KhGsas3aOh5F0eUIVSZ6H2fs28Z3qCdZW5bHg+C7nMaJLQYZ4FGLlhFuEVqAzdTCqaZ2udzLLy3Fz6XfFFn41lu1p2qmUXZc+bpPyp2hOIk3C7mmJXU+yZFKd6I449De4gxahyzMS41Ri3eieuM43unqB4TO6X12V4CQ8P1K1OvUGapAjzdGKVSQPKWI/qL8GDfYgggD70gIEEhbkACik+e2ABgRi1NmSoJZjxos4gBwO9fZzFcAZF9RC/If7tlarE/9GaaeEOkCLwTdBpwiL5QF6RY/KOvCZfyM+pa2XFGqNehjj6Yy+LO/OPr2"
2021.acl-long.115,W05-0909,0,0.0496049,"the same content set (metric max and value max) are added to the placeholder memory as higher-ranked candidates in the searching space. The placeholder memory is reset to empty in the following sentence of Yˆtemp and the alignment starts again from the next content set of table sources. 6 Experiments We conducted experiments on the proposed dataset to evaluate the performance of the text generation models and verify the effectiveness of the approach of using different table representations. 6.1 Automatic Evaluation Metrics We used BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) to evaluate the informativeness of generated texts. We computed the BERTS CORE (Zhang et al., 2020) to assess the similarity between the generated texts and the ground-truth table descriptions by using contextualized token embeddings of pretrained BERT (Devlin et al., 2019), which have been shown to be effective for paraphrase detection. Considering both references and table contents, we also used the PARENT metric, proposed by Dhingra et al. (2019). In our experiments, we modified the PARENT calculation by adding noun phrases of table captions as table contents and used only 1456 targeted ta"
2021.acl-long.115,2020.acl-main.708,0,0.26128,"ata-to-text generation studies have shown significant improvement in generating faithful text aligned with data sources. A copy mechanism has been widely explored to improve faithfulness in various ways. Wiseman et al. (2017) used joint probabilities to let models choose between copying records from data sources or generating from a vocabulary. Puduppully et al. (2019) improved a similar approach by modeling entity representations as a unit of copying. This approach has proven to be effective in generating descriptive text that explicitly mentions facts from sources. However, as introduced by Chen et al. (2020a), humans have the ability to produce more analyti• We introduce a new dataset for table-totext generation focusing on numerical reasoning. The dataset consists of textual descriptions of numerical tables from scientific papers. Our dataset is publicly available on https://github.com/titech-nlp/numeric-nlg. • We adopt template-guided text generation (Kale and Rastogi, 2020a) for a table-to-text generation task and propose injecting preexecuted numerical operations in the template to guide numerical-reasoning-based text generation. We compare different types of templates for table representati"
2021.acl-long.115,2020.acl-main.18,0,0.352787,"ata-to-text generation studies have shown significant improvement in generating faithful text aligned with data sources. A copy mechanism has been widely explored to improve faithfulness in various ways. Wiseman et al. (2017) used joint probabilities to let models choose between copying records from data sources or generating from a vocabulary. Puduppully et al. (2019) improved a similar approach by modeling entity representations as a unit of copying. This approach has proven to be effective in generating descriptive text that explicitly mentions facts from sources. However, as introduced by Chen et al. (2020a), humans have the ability to produce more analyti• We introduce a new dataset for table-totext generation focusing on numerical reasoning. The dataset consists of textual descriptions of numerical tables from scientific papers. Our dataset is publicly available on https://github.com/titech-nlp/numeric-nlg. • We adopt template-guided text generation (Kale and Rastogi, 2020a) for a table-to-text generation task and propose injecting preexecuted numerical operations in the template to guide numerical-reasoning-based text generation. We compare different types of templates for table representati"
2021.acl-long.115,N19-1423,0,0.00607741,"ources. 6 Experiments We conducted experiments on the proposed dataset to evaluate the performance of the text generation models and verify the effectiveness of the approach of using different table representations. 6.1 Automatic Evaluation Metrics We used BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) to evaluate the informativeness of generated texts. We computed the BERTS CORE (Zhang et al., 2020) to assess the similarity between the generated texts and the ground-truth table descriptions by using contextualized token embeddings of pretrained BERT (Devlin et al., 2019), which have been shown to be effective for paraphrase detection. Considering both references and table contents, we also used the PARENT metric, proposed by Dhingra et al. (2019). In our experiments, we modified the PARENT calculation by adding noun phrases of table captions as table contents and used only 1456 targeted table contents for table sources. 6.2 Implementation Details We trained a pointer-generator model using the Adagrad optimizer with a batch size of 8 and a learning rate of 0.15. For fine-tuning the GPT2 model, the Adam optimizer set weight decay to 3 × 10−5 . Following Raffel"
2021.acl-long.115,P19-1483,0,0.0174021,"sing different table representations. 6.1 Automatic Evaluation Metrics We used BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) to evaluate the informativeness of generated texts. We computed the BERTS CORE (Zhang et al., 2020) to assess the similarity between the generated texts and the ground-truth table descriptions by using contextualized token embeddings of pretrained BERT (Devlin et al., 2019), which have been shown to be effective for paraphrase detection. Considering both references and table contents, we also used the PARENT metric, proposed by Dhingra et al. (2019). In our experiments, we modified the PARENT calculation by adding noun phrases of table captions as table contents and used only 1456 targeted table contents for table sources. 6.2 Implementation Details We trained a pointer-generator model using the Adagrad optimizer with a batch size of 8 and a learning rate of 0.15. For fine-tuning the GPT2 model, the Adam optimizer set weight decay to 3 × 10−5 . Following Raffel et al. (2020), the T5 model was fine-tuned with a constant learning rate of 0.001. We trained all models for a maximum of ten epochs with early stopping based on the loss score on"
2021.acl-long.115,2020.acl-main.210,0,0.0224537,"ntain only table numbers without any sentences describing table facts. We hired experts in the computer science field to clean and annotate the extracted descriptions in the following steps: Related Work The power of tables in presenting data efficiently further encourages research done by exploring the tables as data sources in natural language tasks, such as table-to-text generation (Liang et al., 2009; Wiseman et al., 2017; Lebret et al., 2016; Parikh et al., 2020), table question answering (Pasupat and Liang, 2015; Wang et al., 2018), and table-based fact verification (Chen et al., 2020b; Gupta et al., 2020). Recent research on the table-to-text generation task is starting to generate text with more reasoning. Murakami et al. (2017) explored stock prices to generate market comments by adding generalization tags of possible arithmetic operations to cover mathematical reasoning. Nie et al. (2018) proposed operation-guided attentions by exploring the results of pre-executed numerical operations. The dataset closest to ours is L OGICNLG, by Chen et al. (2020a), who first introduced logical text generation using open-domain tables with unknown schemas. Different from our target text for generation, wh"
2021.acl-long.115,2020.emnlp-main.527,0,0.175008,"improved a similar approach by modeling entity representations as a unit of copying. This approach has proven to be effective in generating descriptive text that explicitly mentions facts from sources. However, as introduced by Chen et al. (2020a), humans have the ability to produce more analyti• We introduce a new dataset for table-totext generation focusing on numerical reasoning. The dataset consists of textual descriptions of numerical tables from scientific papers. Our dataset is publicly available on https://github.com/titech-nlp/numeric-nlg. • We adopt template-guided text generation (Kale and Rastogi, 2020a) for a table-to-text generation task and propose injecting preexecuted numerical operations in the template to guide numerical-reasoning-based text generation. We compare different types of templates for table representations in pre-trained models. 1451 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1451–1465 August 1–6, 2021. ©2021 Association for Computational Linguistics • We propose a copy mechanism for pre-trained models, that uses general placeholders covering tabl"
2021.acl-long.115,2020.inlg-1.14,0,0.0483836,"improved a similar approach by modeling entity representations as a unit of copying. This approach has proven to be effective in generating descriptive text that explicitly mentions facts from sources. However, as introduced by Chen et al. (2020a), humans have the ability to produce more analyti• We introduce a new dataset for table-totext generation focusing on numerical reasoning. The dataset consists of textual descriptions of numerical tables from scientific papers. Our dataset is publicly available on https://github.com/titech-nlp/numeric-nlg. • We adopt template-guided text generation (Kale and Rastogi, 2020a) for a table-to-text generation task and propose injecting preexecuted numerical operations in the template to guide numerical-reasoning-based text generation. We compare different types of templates for table representations in pre-trained models. 1451 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1451–1465 August 1–6, 2021. ©2021 Association for Computational Linguistics • We propose a copy mechanism for pre-trained models, that uses general placeholders covering tabl"
2021.acl-long.115,D16-1128,0,0.0607728,"Missing"
2021.acl-long.115,P09-1011,0,0.0194762,"numbers in their captions as keywords for the collection. An example of a table and its description is shown in Figure 1. Data Cleansing and Annotation Extracted table descriptions can be noisy since they may contain only table numbers without any sentences describing table facts. We hired experts in the computer science field to clean and annotate the extracted descriptions in the following steps: Related Work The power of tables in presenting data efficiently further encourages research done by exploring the tables as data sources in natural language tasks, such as table-to-text generation (Liang et al., 2009; Wiseman et al., 2017; Lebret et al., 2016; Parikh et al., 2020), table question answering (Pasupat and Liang, 2015; Wang et al., 2018), and table-based fact verification (Chen et al., 2020b; Gupta et al., 2020). Recent research on the table-to-text generation task is starting to generate text with more reasoning. Murakami et al. (2017) explored stock prices to generate market comments by adding generalization tags of possible arithmetic operations to cover mathematical reasoning. Nie et al. (2018) proposed operation-guided attentions by exploring the results of pre-executed numerical operati"
2021.acl-long.115,W04-1013,0,0.0378679,"lated placeholders from the same content set (metric max and value max) are added to the placeholder memory as higher-ranked candidates in the searching space. The placeholder memory is reset to empty in the following sentence of Yˆtemp and the alignment starts again from the next content set of table sources. 6 Experiments We conducted experiments on the proposed dataset to evaluate the performance of the text generation models and verify the effectiveness of the approach of using different table representations. 6.1 Automatic Evaluation Metrics We used BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) to evaluate the informativeness of generated texts. We computed the BERTS CORE (Zhang et al., 2020) to assess the similarity between the generated texts and the ground-truth table descriptions by using contextualized token embeddings of pretrained BERT (Devlin et al., 2019), which have been shown to be effective for paraphrase detection. Considering both references and table contents, we also used the PARENT metric, proposed by Dhingra et al. (2019). In our experiments, we modified the PARENT calculation by adding noun phrases of table captions as table"
2021.acl-long.115,D18-1422,0,0.0175222,"loring the tables as data sources in natural language tasks, such as table-to-text generation (Liang et al., 2009; Wiseman et al., 2017; Lebret et al., 2016; Parikh et al., 2020), table question answering (Pasupat and Liang, 2015; Wang et al., 2018), and table-based fact verification (Chen et al., 2020b; Gupta et al., 2020). Recent research on the table-to-text generation task is starting to generate text with more reasoning. Murakami et al. (2017) explored stock prices to generate market comments by adding generalization tags of possible arithmetic operations to cover mathematical reasoning. Nie et al. (2018) proposed operation-guided attentions by exploring the results of pre-executed numerical operations. The dataset closest to ours is L OGICNLG, by Chen et al. (2020a), who first introduced logical text generation using open-domain tables with unknown schemas. Different from our target text for generation, which consists of several sentences in a paragraph, they proposed a task of generating only one sentence from selected table contents. 3 Numerical Table-to-Text Dataset We created numericNLG, a new table-to-text dataset focusing on a text generation task with numerical reasoning. We collected"
2021.acl-long.115,P02-1040,0,0.110473,"records of TOP in Step 1, the related placeholders from the same content set (metric max and value max) are added to the placeholder memory as higher-ranked candidates in the searching space. The placeholder memory is reset to empty in the following sentence of Yˆtemp and the alignment starts again from the next content set of table sources. 6 Experiments We conducted experiments on the proposed dataset to evaluate the performance of the text generation models and verify the effectiveness of the approach of using different table representations. 6.1 Automatic Evaluation Metrics We used BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) to evaluate the informativeness of generated texts. We computed the BERTS CORE (Zhang et al., 2020) to assess the similarity between the generated texts and the ground-truth table descriptions by using contextualized token embeddings of pretrained BERT (Devlin et al., 2019), which have been shown to be effective for paraphrase detection. Considering both references and table contents, we also used the PARENT metric, proposed by Dhingra et al. (2019). In our experiments, we modified the PARENT calculation by adding noun phrases of tab"
2021.acl-long.115,2020.emnlp-main.89,0,0.0318933,"ample of a table and its description is shown in Figure 1. Data Cleansing and Annotation Extracted table descriptions can be noisy since they may contain only table numbers without any sentences describing table facts. We hired experts in the computer science field to clean and annotate the extracted descriptions in the following steps: Related Work The power of tables in presenting data efficiently further encourages research done by exploring the tables as data sources in natural language tasks, such as table-to-text generation (Liang et al., 2009; Wiseman et al., 2017; Lebret et al., 2016; Parikh et al., 2020), table question answering (Pasupat and Liang, 2015; Wang et al., 2018), and table-based fact verification (Chen et al., 2020b; Gupta et al., 2020). Recent research on the table-to-text generation task is starting to generate text with more reasoning. Murakami et al. (2017) explored stock prices to generate market comments by adding generalization tags of possible arithmetic operations to cover mathematical reasoning. Nie et al. (2018) proposed operation-guided attentions by exploring the results of pre-executed numerical operations. The dataset closest to ours is L OGICNLG, by Chen et al. (20"
2021.acl-long.115,P15-1142,0,0.0327304,"Figure 1. Data Cleansing and Annotation Extracted table descriptions can be noisy since they may contain only table numbers without any sentences describing table facts. We hired experts in the computer science field to clean and annotate the extracted descriptions in the following steps: Related Work The power of tables in presenting data efficiently further encourages research done by exploring the tables as data sources in natural language tasks, such as table-to-text generation (Liang et al., 2009; Wiseman et al., 2017; Lebret et al., 2016; Parikh et al., 2020), table question answering (Pasupat and Liang, 2015; Wang et al., 2018), and table-based fact verification (Chen et al., 2020b; Gupta et al., 2020). Recent research on the table-to-text generation task is starting to generate text with more reasoning. Murakami et al. (2017) explored stock prices to generate market comments by adding generalization tags of possible arithmetic operations to cover mathematical reasoning. Nie et al. (2018) proposed operation-guided attentions by exploring the results of pre-executed numerical operations. The dataset closest to ours is L OGICNLG, by Chen et al. (2020a), who first introduced logical text generation"
2021.acl-long.115,P19-1195,0,0.0463536,"Missing"
2021.acl-long.115,P17-1099,0,0.338191,"like ours, we also include a template-based generator and a pointer-generator network as baselines. 5.1 Non-pre-trained Models Template-based Generator We design a domain-specific template-based generator covering two types of sentences in producing table descriptions: table referring sentences and data description sentences. Since our task focuses on numerical-reasoning descriptions, we define templatized sentences using maximum records in table TOP : <table id&gt; shows <caption&gt;. we can see that <hmax &gt; outperforms other <thmax &gt; with <valmax &gt; of <mmax &gt;. Pointer-Generator Pointer-generator (See et al., 2017) is a sequence-to-sequence model with attention and a copy mechanism. This model copes with the out-of-vocabulary problem in data-to-text generation by jointly copying from source texts and generating from a vocabulary. 5.2 Pre-trained Models Fine-tuned GPT2 GPT2 (Radford et al., 2019) is a pre-trained language model with a decoder-only transformer architecture. We fine-tuned the GPT2 model by using table representation PT as a prefix of our input. Specifically, we fed the concatenation of table representation PT and table description Y to the model and generated Y . In the inference phase, we"
2021.acl-long.115,2020.acl-main.101,0,0.0243932,"ing mathematical operations only on targeted cells to limit the calculation. In this study, we cover maximum, minimum, and difference operations. Examples of a preprocessed table, data table, and pre-executed operation table are shown in Figure 2. Linearized Table Supporting transfer learning of pre-trained transformers to our table-to-text generation task, we prepare a linearized table PT as an input representation so that it similar to the representation that encoder has seen during pre-training. T is converted to a flat string PT = w1 , ..., w|PT |, similar to that used in many prior work (Wang et al., 2020; Chen et al., 2020a; Kale and Rastogi, 2020b), where wi denotes the i-th word in paragraph PT with length |PT |. In this study, we adopt the template-based input representation, introduced by Kale and Rastogi (2020a), to handle representation bias between a structured data T and a natural language utterance PT , where PT is generated using a manually defined template. We propose not only covering data table TD in the template but also injecting the pre-executed numerical operations of table T through TOP to guide numerical-reasoningbased text generation. We consider four different methods3 fo"
2021.eacl-demos.27,N15-1184,0,0.00985692,"ularity (Eq.(12)) and the surprisingness (Eq.(14)). Similar to Eq.(11), we tuned the weight parameters λppl and λsup on the validation dataset. • Pop+Com+Sup: This method selects five personal relationships on the basis of a combination of the popularity, the commonness, and the surprisingness (Eq.(11)). Prior to running these baselines and proposed methods, we obtained word vectors from Japanese Wikipedia articles by utilizing word2vec.5 In this step, all sentences were tokenized using MeCab6 with the NEologd dictionary. We further tuned the word vectors by utilizing a retrofitting approach (Faruqui et al., 2015)7 with Wikipedia’s category information to consider similarities between persons. The retrofitting approach can refine word vectors using graph information by making word vectors close to each other when they have a link in the graph. To construct a graph for personal similarities, we linked two words if a Wikipedia category includes the words. Because some person names have several articles due to their ambiguity, we skipped such words in this step.8 In the end, we reran the retrofitting with the default hyperparameters. Then, we mapped the obtained word vectors of person names to 300 cluster"
2021.eacl-demos.27,C14-1140,0,0.0251803,"category. Korn et al. (2019) mined trivia facts from superlative tables in Wikipedia articles. They utilized a template-based approach for semi-automatically generating natural language statements as fun facts. Their work had actually been incorporated into the search engine by Google. Kwon et al. (2020) proposed a method to obtain sentences including trivia facts by focusing on a tendency of the Wikipedia article structure that a paragraph containing trivial facts is not similar to other paragraphs in a article. The supervised approaches have also been used for extracting interesting facts. Gamon et al. (2014) proposed models that predict the level of interest a user gives to various text spans in a document by observing the user’s browsing behavior via clicks from one page to another. Prakash et al. (2015) constructed a labeled dataset for movie entities and proposed a method for extracting interesting sentences from Wikipedia articles and ordering them based on interestingness by utilizing Rank-SVM trained with the constructed dataset. Fatma et al. (2017) proposed a method for automatically mining trivia facts for an entity of a given domain in knowledge graphs by utilizing deep convolutional neu"
2021.eacl-demos.27,W04-3250,0,0.108273,"Missing"
2021.eacl-demos.27,2020.coling-main.424,1,0.87451,"stingness by utilizing Rank-SVM, trained in a supervised manner. Tsurel et al. (2017) proposed an algorithm that automatically mines trivia facts from Wikipedia by utilizing its category structure. Their approach can rank categories for an entity based on their trivia quality induced from the categories. Fatma et al. (2017) proposed a method for automatically mining trivia facts for an entity of a given domain in knowledge graphs by utilizing deep convolutional neural networks, trained in a supervised manner. Korn et al. (2019) mined trivia facts from superlative tables in Wikipedia articles. Kwon et al. (2020) proposed a method to obtain sentences including trivia facts with utilizing paragraph structures in Wikipedia articles. However, some of these approaches work only on structured datasets such as knowledge graphs or Wikipedia categories. In addition, while supervised approaches can work on unstructured natural language texts, the applicable domain is restricted due to the lack of annotated datasets. Hence, the current approaches for extracting interesting facts gin. You can watch our demo video for this plugin at a shared directory in our google drive. 2 Figure 2: A screenshot of our chrome pl"
2021.eacl-demos.27,Y18-1031,0,0.0214657,"he results of the human evaluation show that the proposed method could extract more interesting relationships between persons from Japanese Wikipedia articles than a popularity-based baseline method. We demonstrate our proposed method as a chrome plugin on google search. 1 Introduction Interesting facts are useful information for a variety of important tasks. For example, in data mining, the interesting facts can enhance user engagement in search engines (Fatma et al., 2017). In natural language processing, the interesting facts can improve user experience with automatic conversation systems (Niina and Shimada, 2018). However, if we rely on experts to gather the interesting facts, the cost becomes quite high. As a solution, several approaches have been developed to extract interesting facts automatically. Lin and Chalupsky (2003) proposed a set of unsupervised link discovery methods that can compute interestingness on graph data represented as a set of entities connected by a set of binary relations. Ex.1: Tim Burton and Johnny Depp When Tim Burton met Johnny Depp for the first time, he had the impression that Johnny Depp was a hopelessly poor actor. Ex.2: Chien-Ming Wang and Suzuki Ichiro Chien-Ming Wang"
2021.eacl-main.125,D10-1049,0,0.029214,"r, 2018), has been widely used in various fields such as sports (Wiseman et al., 2017; Puduppully et al., 2019), finance (Murakami et al., 2017; Aoki et al., 2018, 2019), and medical care (Portet et al., 2009; Jing et al., 2018). Neural generation methods have been attracting increased attention in the field of data-totext generation (Liu et al., 2018; Iso et al., 2019), although rule-based approaches have been the mainstream (Kukich, 1983; Reiter et al., 2005). The task of generating weather-forecast comments has traditionally been tackled in the field of data-to-text generation (Belz, 2007; Angeli et al., 2010; Mei et al., 2016). For example, there are efforts in generating weather-forecast comments intended for marine shipping or offshore oil facilities (Kittredge et al., 1986; Reiter et al., 2005), as well as local weather forecasts for more general use (Kerpedjiev, 1992; Liang et al., 2009). Prior research has examined the second and third problems mentioned in Section 1 (Murakami et al., 2017; Puduppully et al., 2019). For the second problem, we need to incorporate information for time and area into a generation model to generate time-dependent expressions. For the third problem, we must carry"
2021.eacl-main.125,N07-1021,0,0.272096,", and observations, has become a mainstream tool for supporting today’s weather forecasts around the world. Weather forecasters obtain numerical outputs from the simulation models and use their scientific knowledge and historical data to come up with forecast comments such as “sunny and sometimes cloudy”. However, writing local or personalized weather comments for end users is labor intensive and requires a solid knowledge of meteorology. Therefore, the task of generating weather-forecast comments has traditionally been addressed in the field of data-to-text generation (Goldberg et al., 1994; Belz, 2007). In this paper, we focus on the task of generating weather-forecast comments from meteorological 1 https://github.com/titech-nlp/ pinpoint-weather Air pressure simulations. While previous studies have mainly focused on database records and tables (Sripada et al., 2004; Liang et al., 2009), which are modified results by experts based on their local knowledge (Reiter et al., 2005), we use raw simulation results of NWP models as inputs for text generation. This is closer to the real-world scenarios, in which meteorological specialists describe weather comments by interpreting such numerical data"
2021.eacl-main.125,D19-1299,0,0.0245625,"Missing"
2021.eacl-main.125,P18-1240,0,0.0160848,"apanese weather comments from simulation results of NWP models and meteorological observation data. The results of both automatic and human evaluations indicate that our model improves the informativeness of generated comments compared with baselines. 2 Related Work Data-to-text generation, which is the task of automatically producing descriptions from nonlinguistic data (Gatt and Krahmer, 2018), has been widely used in various fields such as sports (Wiseman et al., 2017; Puduppully et al., 2019), finance (Murakami et al., 2017; Aoki et al., 2018, 2019), and medical care (Portet et al., 2009; Jing et al., 2018). Neural generation methods have been attracting increased attention in the field of data-totext generation (Liu et al., 2018; Iso et al., 2019), although rule-based approaches have been the mainstream (Kukich, 1983; Reiter et al., 2005). The task of generating weather-forecast comments has traditionally been tackled in the field of data-to-text generation (Belz, 2007; Angeli et al., 2010; Mei et al., 2016). For example, there are efforts in generating weather-forecast comments intended for marine shipping or offshore oil facilities (Kittredge et al., 1986; Reiter et al., 2005), as well as loc"
2021.eacl-main.125,A92-1007,0,0.609101,"cting increased attention in the field of data-totext generation (Liu et al., 2018; Iso et al., 2019), although rule-based approaches have been the mainstream (Kukich, 1983; Reiter et al., 2005). The task of generating weather-forecast comments has traditionally been tackled in the field of data-to-text generation (Belz, 2007; Angeli et al., 2010; Mei et al., 2016). For example, there are efforts in generating weather-forecast comments intended for marine shipping or offshore oil facilities (Kittredge et al., 1986; Reiter et al., 2005), as well as local weather forecasts for more general use (Kerpedjiev, 1992; Liang et al., 2009). Prior research has examined the second and third problems mentioned in Section 1 (Murakami et al., 2017; Puduppully et al., 2019). For the second problem, we need to incorporate information for time and area into a generation model to generate time-dependent expressions. For the third problem, we must carry out content selection to explicitly provide useful information, such as sunny and rain, for consumers. In the table-to-text task, which aims to generate a description from a structured table, there have been recent efforts to improve the correctness of generated texts"
2021.eacl-main.125,C86-1132,0,0.265527,"), and medical care (Portet et al., 2009; Jing et al., 2018). Neural generation methods have been attracting increased attention in the field of data-totext generation (Liu et al., 2018; Iso et al., 2019), although rule-based approaches have been the mainstream (Kukich, 1983; Reiter et al., 2005). The task of generating weather-forecast comments has traditionally been tackled in the field of data-to-text generation (Belz, 2007; Angeli et al., 2010; Mei et al., 2016). For example, there are efforts in generating weather-forecast comments intended for marine shipping or offshore oil facilities (Kittredge et al., 1986; Reiter et al., 2005), as well as local weather forecasts for more general use (Kerpedjiev, 1992; Liang et al., 2009). Prior research has examined the second and third problems mentioned in Section 1 (Murakami et al., 2017; Puduppully et al., 2019). For the second problem, we need to incorporate information for time and area into a generation model to generate time-dependent expressions. For the third problem, we must carry out content selection to explicitly provide useful information, such as sunny and rain, for consumers. In the table-to-text task, which aims to generate a description from"
2021.eacl-main.125,P83-1022,0,0.307405,"ments compared with baselines. 2 Related Work Data-to-text generation, which is the task of automatically producing descriptions from nonlinguistic data (Gatt and Krahmer, 2018), has been widely used in various fields such as sports (Wiseman et al., 2017; Puduppully et al., 2019), finance (Murakami et al., 2017; Aoki et al., 2018, 2019), and medical care (Portet et al., 2009; Jing et al., 2018). Neural generation methods have been attracting increased attention in the field of data-totext generation (Liu et al., 2018; Iso et al., 2019), although rule-based approaches have been the mainstream (Kukich, 1983; Reiter et al., 2005). The task of generating weather-forecast comments has traditionally been tackled in the field of data-to-text generation (Belz, 2007; Angeli et al., 2010; Mei et al., 2016). For example, there are efforts in generating weather-forecast comments intended for marine shipping or offshore oil facilities (Kittredge et al., 1986; Reiter et al., 2005), as well as local weather forecasts for more general use (Kerpedjiev, 1992; Liang et al., 2009). Prior research has examined the second and third problems mentioned in Section 1 (Murakami et al., 2017; Puduppully et al., 2019). Fo"
2021.eacl-main.125,D16-1128,0,0.0426945,"Missing"
2021.eacl-main.125,P09-1011,0,0.18772,"y and sometimes cloudy”. However, writing local or personalized weather comments for end users is labor intensive and requires a solid knowledge of meteorology. Therefore, the task of generating weather-forecast comments has traditionally been addressed in the field of data-to-text generation (Goldberg et al., 1994; Belz, 2007). In this paper, we focus on the task of generating weather-forecast comments from meteorological 1 https://github.com/titech-nlp/ pinpoint-weather Air pressure simulations. While previous studies have mainly focused on database records and tables (Sripada et al., 2004; Liang et al., 2009), which are modified results by experts based on their local knowledge (Reiter et al., 2005), we use raw simulation results of NWP models as inputs for text generation. This is closer to the real-world scenarios, in which meteorological specialists describe weather comments by interpreting such numerical data. We believe it will be more helpful for less experienced forecasters. There has been little research on generating descriptions from a sequence of raw numerical data even in the data-to-text generation (Gatt and Krahmer, 2018). We illustrate the three characteristic problems of weather-co"
2021.eacl-main.125,W04-1013,0,0.0183021,"dings were both 512. We set the dimension size of the hidden veca tors for the meta-data hm i and observation data hi to 64. The model was trained using the Adam optimizer (Kingma and Ba, 2015). We applied an early stopping strategy with a minimum number of 25 epochs. We stopped training if there was no improvement in validation loss for three consecutive epochs. Evaluation Metrics For the automatic evaluation, since reference texts written by meteorological experts generally mention important information such as sunny and rain, we used BLEU-47 (Papineni et al., 2002) and ROUGE-18 (F1 score) (Lin, 2004) to see whether generated texts properly mention the important information as reference texts do. However, since these metrics based on word overlapping rely on the reference texts, they cannot be used to assess the correctness of the generated texts if their expressions are different from the reference texts. Thus, we also calculated precision, recall and F1 scores of weather labels, which are extracted from the generated texts, to see how they properly describe important information in comparison with those of the reference texts. For the human evaluation, we asked five participants to give"
2021.eacl-main.125,P02-1040,0,0.110503,"n states of our model and size of the word embeddings were both 512. We set the dimension size of the hidden veca tors for the meta-data hm i and observation data hi to 64. The model was trained using the Adam optimizer (Kingma and Ba, 2015). We applied an early stopping strategy with a minimum number of 25 epochs. We stopped training if there was no improvement in validation loss for three consecutive epochs. Evaluation Metrics For the automatic evaluation, since reference texts written by meteorological experts generally mention important information such as sunny and rain, we used BLEU-47 (Papineni et al., 2002) and ROUGE-18 (F1 score) (Lin, 2004) to see whether generated texts properly mention the important information as reference texts do. However, since these metrics based on word overlapping rely on the reference texts, they cannot be used to assess the correctness of the generated texts if their expressions are different from the reference texts. Thus, we also calculated precision, recall and F1 scores of weather labels, which are extracted from the generated texts, to see how they properly describe important information in comparison with those of the reference texts. For the human evaluation,"
2021.eacl-main.125,Q18-1013,0,0.0546871,"Missing"
2021.eacl-main.125,P19-1197,0,0.0817262,"; Puduppully et al., 2019). For the second problem, we need to incorporate information for time and area into a generation model to generate time-dependent expressions. For the third problem, we must carry out content selection to explicitly provide useful information, such as sunny and rain, for consumers. In the table-to-text task, which aims to generate a description from a structured table, there have been recent efforts to improve the correctness of generated texts by implicitly introducing a content-matching constraint (Wang et al., 2020), explicitly specifying the content in the table (Ma et al., 2019) or incorporating copy mechanism (Lebret et al., 2016). Nonetheless, the techniques proposed in the table-to-text task are not directly applicable to datasets consisting of raw numerical data, such as simulation results of NWP models, and texts since they rely on task-specific architectures such as the copy mechanism copying words from tables. In addition, Puduppully et al. (2019) proposed a method for generating summaries of basketball games by using the correspondence between entities in text and input tabular data extracted using the information-extraction method (Wiseman et al., 2017). How"
2021.eacl-main.125,N16-1086,0,0.0190113,"dely used in various fields such as sports (Wiseman et al., 2017; Puduppully et al., 2019), finance (Murakami et al., 2017; Aoki et al., 2018, 2019), and medical care (Portet et al., 2009; Jing et al., 2018). Neural generation methods have been attracting increased attention in the field of data-totext generation (Liu et al., 2018; Iso et al., 2019), although rule-based approaches have been the mainstream (Kukich, 1983; Reiter et al., 2005). The task of generating weather-forecast comments has traditionally been tackled in the field of data-to-text generation (Belz, 2007; Angeli et al., 2010; Mei et al., 2016). For example, there are efforts in generating weather-forecast comments intended for marine shipping or offshore oil facilities (Kittredge et al., 1986; Reiter et al., 2005), as well as local weather forecasts for more general use (Kerpedjiev, 1992; Liang et al., 2009). Prior research has examined the second and third problems mentioned in Section 1 (Murakami et al., 2017; Puduppully et al., 2019). For the second problem, we need to incorporate information for time and area into a generation model to generate time-dependent expressions. For the third problem, we must carry out content selecti"
2021.eacl-main.125,2020.acl-main.101,0,0.198938,"ond and third problems mentioned in Section 1 (Murakami et al., 2017; Puduppully et al., 2019). For the second problem, we need to incorporate information for time and area into a generation model to generate time-dependent expressions. For the third problem, we must carry out content selection to explicitly provide useful information, such as sunny and rain, for consumers. In the table-to-text task, which aims to generate a description from a structured table, there have been recent efforts to improve the correctness of generated texts by implicitly introducing a content-matching constraint (Wang et al., 2020), explicitly specifying the content in the table (Ma et al., 2019) or incorporating copy mechanism (Lebret et al., 2016). Nonetheless, the techniques proposed in the table-to-text task are not directly applicable to datasets consisting of raw numerical data, such as simulation results of NWP models, and texts since they rely on task-specific architectures such as the copy mechanism copying words from tables. In addition, Puduppully et al. (2019) proposed a method for generating summaries of basketball games by using the correspondence between entities in text and input tabular data extracted u"
2021.eacl-main.267,D19-1371,0,0.0140209,"scheme is inspired by the promising results of the pointer-generator network (See et al., 2017) in the summarization task. The network deals with the out-of-vocabulary issue by joint copying from source texts and generating from vocabularies. Recent studies have shown that pre-trained encoders can be successfully fine-tuned for downstream NLP tasks, thus avoiding the need to train a new model from scratch. A pre-trained encoder BERT (Devlin et al., 2019) was trained on the BooksCorpus (800M words) and Wikipedia (2,500M words). For better-contextualized representation in the scientific domain, Beltagy et al. (2019) introduced a domain-specific BERT model, SciBERT, which was trained on 1.14M papers from Semantic Scholar. Friedrich et al. (2020) implemented both BERT and SciBERT on their models to solve the information extraction task and achieved significant performance gains. 3 Related Work Table information extraction is beneficial to cover unknown table schemes and understand the table contents. Milosevic et al. (2019) proposed a framework for table information extraction in biomedical domains by defining rules for all possible variables. Specifically, for numerical variables, they retrieved metric-ty"
2021.eacl-main.267,N19-1423,0,0.0171912,"bles was done by Milosevic et al. (2016) to automatically detect table structures from XML tables. Our pointer-generator-based model in the metrictype generation scheme is inspired by the promising results of the pointer-generator network (See et al., 2017) in the summarization task. The network deals with the out-of-vocabulary issue by joint copying from source texts and generating from vocabularies. Recent studies have shown that pre-trained encoders can be successfully fine-tuned for downstream NLP tasks, thus avoiding the need to train a new model from scratch. A pre-trained encoder BERT (Devlin et al., 2019) was trained on the BooksCorpus (800M words) and Wikipedia (2,500M words). For better-contextualized representation in the scientific domain, Beltagy et al. (2019) introduced a domain-specific BERT model, SciBERT, which was trained on 1.14M papers from Semantic Scholar. Friedrich et al. (2020) implemented both BERT and SciBERT on their models to solve the information extraction task and achieved significant performance gains. 3 Related Work Table information extraction is beneficial to cover unknown table schemes and understand the table contents. Milosevic et al. (2019) proposed a framework f"
2021.eacl-main.267,2020.acl-main.116,0,0.0165334,"twork deals with the out-of-vocabulary issue by joint copying from source texts and generating from vocabularies. Recent studies have shown that pre-trained encoders can be successfully fine-tuned for downstream NLP tasks, thus avoiding the need to train a new model from scratch. A pre-trained encoder BERT (Devlin et al., 2019) was trained on the BooksCorpus (800M words) and Wikipedia (2,500M words). For better-contextualized representation in the scientific domain, Beltagy et al. (2019) introduced a domain-specific BERT model, SciBERT, which was trained on 1.14M papers from Semantic Scholar. Friedrich et al. (2020) implemented both BERT and SciBERT on their models to solve the information extraction task and achieved significant performance gains. 3 Related Work Table information extraction is beneficial to cover unknown table schemes and understand the table contents. Milosevic et al. (2019) proposed a framework for table information extraction in biomedical domains by defining rules for all possible variables. Specifically, for numerical variables, they retrieved metric-types by searching a set of possible tokens in the dictionary. Focusing on numerical tables, Nourbakhsh et al. (2020) extracted metri"
2021.eacl-main.267,P19-1513,0,0.0223378,"eneficial to cover unknown table schemes and understand the table contents. Milosevic et al. (2019) proposed a framework for table information extraction in biomedical domains by defining rules for all possible variables. Specifically, for numerical variables, they retrieved metric-types by searching a set of possible tokens in the dictionary. Focusing on numerical tables, Nourbakhsh et al. (2020) extracted metric-types in earning reports by using similarity scores between the corresponding non-numeric text for the leftmost cells and stored metric-types. The work closest to ours is the one by Hou et al. (2019), who used tables from the experimental result section, combined with the title and abstract as document representations to extract triples of tasks, 1 Dataset is nlp/metrictable available on https://github.com/titech3.1 Metric-Type Identification for Numerical Tables Datasets We automatically extracted tables from the PDF files of scientific papers in the computational linguistics domain using PDFMiner and Tabula as extraction tools and filtered only numerical tables related to experimental results using the keywords evaluation, result, comparison, and performance. We used papers from the ACL"
2021.eacl-main.267,P18-1031,0,0.0163825,"UGE-1. 5.3 Implementation Details We implemented our models using the AllenNLP library (Gardner et al., 2018). In our pointergenerator-based model, we used pre-trained word embeddings for initialization and two-layer BiLSTMs with 256 hidden sizes in both the caption and header-level encoders. We used dropout (Srivastava and Hovy, 2014) with the probability p = 0.1. For optimization in the training phase, we used Adam as the optimizer with a batch size of 10 and a learning rate of 3 × 10−3 and 3 × 10−5 in pointer-generator-based and BERT-based, respectively, with a slanted triangular schedule (Howard and Ruder, 2018). We trained the model for a maximum of 20 epochs with early stopping on the validation set (patience of 10) and set α to 0.5. We used the original BERT and the domain-specific SciBERT uncased model to fine-tune our BERTbased model. 6 6.1 Results Experimental Results Model comparison The performances of the proposed and baseline models are shown in Table 2. We can see that the Pointer-Generator Supervised-Attention model initialized by Glove embeddings outperformed the baseline in predicting metric-type location. The accuracy of this model in the metric-type generation part mostly scored bette"
2021.eacl-main.267,C16-1291,0,0.0186997,"from a table caption and generating word wm from the metric-type vocabulary, where pcopy ∈ [0, 1]. We use a softmax function to compute the probability distribution over the metric-type vocabulary: Pvocab (wm ) = softmax(Ccapt ). (7) Then, we obtain the following probability distribution over the extended vocabulary: n X P (wm ) = pcopy acapti + i:wi =(wm ) (1 − pcopy )Pvocab (wm ), (8) where i is the index of metric-type tokens in the vocabulary. Learning objective For training, we exploit the negative log-likelihood objective as the loss function. In addition, we adopt supervised attention (Liu et al., 2016) for jointly supervising the row and column header-level attention to obtain the metrictype header-level. We combine all loss functions in the location classification and token generation model, and define α as the weight as follows: X zhloc log phlocc + L = −((1 − α)( c u+v X (4) which includes the probabilities of the metric-types located in row headers (prh ), located in column headers (pch ), or not located in the headers (pcapt ), where prh + pch + pcapt = 1. whleveli = [arhk prh ; achl pch ], Metric-type generation gates In our pointergenerator network, we use the sigmoid layer to obtain"
2021.eacl-main.267,D19-1387,0,0.0136001,"r-level Encoder Crh Cch Col Header-level Encoder ???? ???? ???? Metric-type Header-location Gate Metric-type Header-level Gate Metric-type header-loc capt/row/col capt/row/col capt/row/col Metric-type header-level weight 0.4 0.2 0.2 0.2 0.4 0.2 0.2 0.2 0.3 0.2 0.1 0.5 ???? Figure 4: Architecture of proposed pointer-generator-based model to identify metric-types in tables. types of input text, pairs of question and answer, a [CLS] token is appended before question tokens, and [SEP] tokens are placed after question and after answer tokens, to separate the question and answer segments. Following Liu and Lapata (2019), we customize these preprocessing schemes by inserting [CLS] before each segment and inserting [SEP] after each segment. We divide our inputs into several segments: caption, row header level 1 to u, and column header level 1 to v. The input text after preprocessing is denoted as a sequence of tokens X = (x1 , x2 , · · ·, xn ). There are three kinds of embedding assigned to each xi : token embeddings representing the meaning of each token, segmentation embeddings indicating the segment boundaries of a sequence of tokens, and position embeddings covering token position within the sequences. Sin"
2021.eacl-main.267,D15-1166,0,0.0135079,"pe header-location outputs. In the generation scheme, we adopt the pointer-generator network to take into account captions as source texts and the metric-type vocabulary in the metric-type generation gate. The architecture of our model is shown in Figure 4. Header encoder We use the vector representation of each header-level by averaging the vectors of all header name tokens in the same level. Given Erhk and Echl as the averages of the initial vector representations of the row and column header-level vectors, respectively, we use the BiLSTM encoder with the dot attention mechanism proposed by Luong et al. (2015) to obtain the representations of the row and column header-levels and select the last hidden state of the last level combined with the weighted hidden states as header-level contexts, as follows: Crh = [Crhu ; u X arhk Crhk ], (2) k=1 Cch = [Cchv ; v X achl Cchl ]. (3) l=1 Caption encoder As with the headers, we use the BiLSTM encoder with attention acapti to compute the context vector of caption Ccapt . Metric-type header-location gates We feed the concatenation of the row and column header contexts to the softmax layer to obtain the metric-type header-location probability: phloc = softmax(["
2021.eacl-main.267,P17-1099,0,0.0120712,"et, and metric for leaderboard construction. In our study, we represent the tables in more generic ways, preventing the original table structure in the multi-level headers form. We intend to retain the ability of a table to cover complex categorization in the headers and efficiently present all values. A previous study that also explored multi-dimensional tables was done by Milosevic et al. (2016) to automatically detect table structures from XML tables. Our pointer-generator-based model in the metrictype generation scheme is inspired by the promising results of the pointer-generator network (See et al., 2017) in the summarization task. The network deals with the out-of-vocabulary issue by joint copying from source texts and generating from vocabularies. Recent studies have shown that pre-trained encoders can be successfully fine-tuned for downstream NLP tasks, thus avoiding the need to train a new model from scratch. A pre-trained encoder BERT (Devlin et al., 2019) was trained on the BooksCorpus (800M words) and Wikipedia (2,500M words). For better-contextualized representation in the scientific domain, Beltagy et al. (2019) introduced a domain-specific BERT model, SciBERT, which was trained on 1."
2021.eacl-main.267,P14-1060,0,0.0265468,"Predicted LRow LCol Gen 0 0 0 LCol 0 80 6 Gen 0 3 46 Table 6: Confusion matrix of Fine-tuned SciBERT prediction. where d is the number of w ˆm whose characters are all found in wm in the same order. For example, the predicted token RG1 is regarded as correct when the reference token is ROUGE-1. 5.3 Implementation Details We implemented our models using the AllenNLP library (Gardner et al., 2018). In our pointergenerator-based model, we used pre-trained word embeddings for initialization and two-layer BiLSTMs with 256 hidden sizes in both the caption and header-level encoders. We used dropout (Srivastava and Hovy, 2014) with the probability p = 0.1. For optimization in the training phase, we used Adam as the optimizer with a batch size of 10 and a learning rate of 3 × 10−3 and 3 × 10−5 in pointer-generator-based and BERT-based, respectively, with a slanted triangular schedule (Howard and Ruder, 2018). We trained the model for a maximum of 20 epochs with early stopping on the validation set (patience of 10) and set α to 0.5. We used the original BERT and the domain-specific SciBERT uncased model to fine-tune our BERTbased model. 6 6.1 Results Experimental Results Model comparison The performances of the propo"
2021.eacl-main.296,N19-1423,0,0.0321136,"2 (Merity et al., 2016), extracted from Wikipedia articles, as our external data. As shown in Algorithm 1, data loader loads one batch of negative samples, i.e., sentences from WikiText2, which are labeled with 0. Encoder For encoding the text input, i.e., φ(x, W ), we used a Bidirectional LSTM with attention (Hochreiter and Schmidhuber, 1997; Xu et al., 2015), with the number of hidden units being 150. For the pre-trained word embeddings, we experimented with GloVe Vectors (Pennington et al., 2014) and set the dimension to 300. In our experiments, we did not adopt the widely used BERT model (Devlin et al., 2019), as Ruff et al. (2019) showed that BERT model did not improve the performance. Settings As for the optimization of parameters, Adam (Kingma and Ba, 2014) with a base learning rate of 0.001 was used for 50 epochs. The batch sizes were set to 32 and 64 for Reuters and Newsgroups, respectively. For the initialization of mSVDD model, we employed two operation steps. In the absence of negative samples, mSVDD was 5 http://qwone.com/json/20Newsgroups http://daviddlewis.com/resources/testcollections/ reuters21578/ 6 first pre-trained on target samples by using an AutoEncoder with two objectives: 1) w"
2021.eacl-main.296,D14-1162,0,0.0870117,"s often adopted as the training dataset (Mikolov et al., 2013). So we also chose one publicly available corpus WikiText-2 (Merity et al., 2016), extracted from Wikipedia articles, as our external data. As shown in Algorithm 1, data loader loads one batch of negative samples, i.e., sentences from WikiText2, which are labeled with 0. Encoder For encoding the text input, i.e., φ(x, W ), we used a Bidirectional LSTM with attention (Hochreiter and Schmidhuber, 1997; Xu et al., 2015), with the number of hidden units being 150. For the pre-trained word embeddings, we experimented with GloVe Vectors (Pennington et al., 2014) and set the dimension to 300. In our experiments, we did not adopt the widely used BERT model (Devlin et al., 2019), as Ruff et al. (2019) showed that BERT model did not improve the performance. Settings As for the optimization of parameters, Adam (Kingma and Ba, 2014) with a base learning rate of 0.001 was used for 50 epochs. The batch sizes were set to 32 and 64 for Reuters and Newsgroups, respectively. For the initialization of mSVDD model, we employed two operation steps. In the absence of negative samples, mSVDD was 5 http://qwone.com/json/20Newsgroups http://daviddlewis.com/resources/te"
2021.eacl-main.296,P19-1398,0,0.335048,"compact representation for the description of target data. The compact representation could be a set of prototypes or subspaces obtained by optimizing a reconstruction error on the target training data. Regarding the features for representing text in OCC, document-to-word co-occurrence matrices or hand-crafted features have been commonly used in most of the previous work (Manevitz and Yousef, 2001, 2007; Kumaraswamy et al., 2015). Pretrained vectors have been popular for many NLP tasks (Mikolov et al., 2013; Bengio et al., 2003). The recent context vector data description (CVDD), proposed by Ruff et al. (2019), fully uses word embedding knowledge and a neural network structure to process one-class classification problems. Ruff et al. (2018) introduced deep support vector data description (deep SVDD), a fully unsupervised method for deep one-class classification for image data. Deep SVDD learns to extract the common factors of target training samples with a neural network to minimize the radius of a hypersphere that encloses the network representations of the data. The learned hypersphere, with a center c and a neural feature transformer φ(x), can be an end-to-end 3378 Proceedings of the 16th Confer"
2021.emnlp-main.188,W01-1605,0,0.584324,"ntroduction Feng and Hirst, 2014a; Joty et al., 2015; Wang et al., 2017; Kobayashi et al., 2020) and are appliTextual coherence is essential for writing a natural cable to many downstream tasks, such as machine language text that is comprehensible to readers. To recognize the coherent structure of a natural lan- translation (Guzmán et al., 2014; Joty et al., 2017) and sentence compression (Sporleder and Lapata, guage text, Rhetorical Structure Theory (RST) is 2005). applied to describe an internal discourse structure for the text as a constituent tree (Mann and ThompIn discourse segmentation, Carlson et al. (2001) son, 1988). A discourse tree in RST consists of proposed a method for using lexical information elementary discourse units (EDUs), spans that de- and syntactic parsing results. Many researchers scribe recursive connections between EDUs, and (Fisher and Roark, 2007; Xuan Bach et al., 2012; nuclearity and relation labels that describe relation- Feng and Hirst, 2014b) utilized these clues as feaships for each connection. tures in a classifier although automatic parsing erFigure 1 (a) shows an example RST discourse rors degraded segmentation performance. To avoid tree. A span including one or mor"
2021.emnlp-main.188,P13-1048,0,0.199926,"N (N We’ve got a lot )N (S to do , )S )N )Elaboration (S he acknowledged . )S )Attribution Figure 1: An example discourse tree structure. label, such as Attribution and Elaboration, is used to describe the relation between the given spans (Mann and Thompson, 1988; Carlson and Marcu, 2001). To build such trees, RST parsing consists of discourse segmentation, a task to detect EDU boundaries in a given text, and discourse parsing, a task to link spans for detected EDUs. In this paper, we focus on discourse segmentation and sentence-level discourse parsing, which are indispensable in RST parsing (Joty et al., 2013; 1 Introduction Feng and Hirst, 2014a; Joty et al., 2015; Wang et al., 2017; Kobayashi et al., 2020) and are appliTextual coherence is essential for writing a natural cable to many downstream tasks, such as machine language text that is comprehensible to readers. To recognize the coherent structure of a natural lan- translation (Guzmán et al., 2014; Joty et al., 2017) and sentence compression (Sporleder and Lapata, guage text, Rhetorical Structure Theory (RST) is 2005). applied to describe an internal discourse structure for the text as a constituent tree (Mann and ThompIn discourse segmentat"
2021.emnlp-main.188,J15-3002,0,0.116421,"he acknowledged . )S )Attribution Figure 1: An example discourse tree structure. label, such as Attribution and Elaboration, is used to describe the relation between the given spans (Mann and Thompson, 1988; Carlson and Marcu, 2001). To build such trees, RST parsing consists of discourse segmentation, a task to detect EDU boundaries in a given text, and discourse parsing, a task to link spans for detected EDUs. In this paper, we focus on discourse segmentation and sentence-level discourse parsing, which are indispensable in RST parsing (Joty et al., 2013; 1 Introduction Feng and Hirst, 2014a; Joty et al., 2015; Wang et al., 2017; Kobayashi et al., 2020) and are appliTextual coherence is essential for writing a natural cable to many downstream tasks, such as machine language text that is comprehensible to readers. To recognize the coherent structure of a natural lan- translation (Guzmán et al., 2014; Joty et al., 2017) and sentence compression (Sporleder and Lapata, guage text, Rhetorical Structure Theory (RST) is 2005). applied to describe an internal discourse structure for the text as a constituent tree (Mann and ThompIn discourse segmentation, Carlson et al. (2001) son, 1988). A discourse tree i"
2021.emnlp-main.188,N16-1024,0,0.0228545,"sentence-level discourse parsing. In spite of the performance improvement of these models, a restricted number of labeled RST discourse trees is still a problem. In the discourse segmentation and parsing tasks, most prior work is on the basis of discriminative models, which learn mapping from input texts to predicted labels. Thus, there still remains room for improving model performance by considering mapping from predictable labels to input texts to exploit more label information. To consider such information in a model, Mabona et al. (2019) introduced a generative model-based parser, RNNG (Dyer et al., 2016), to document-level RST discourse parsing. Different from our LMGC, this model unidirectionally predicts action sequences. In this research, we model LMGC for the discourse segmentation and sentence-level discourse parsing tasks. LMGC utilizes a BERT-style bidirectional Transformer encoder (Devlin et al., 2019) to avoid prediction bias caused by using different decoding directions. Since LMGC is on the basis of generative models, it can jointly consider an input text and its predictable labels, and map the embeddings of both input tokens and labels onto the same space. Due to this characterist"
2021.emnlp-main.188,P14-1048,0,0.161705,")S )N )Elaboration (S he acknowledged . )S )Attribution Figure 1: An example discourse tree structure. label, such as Attribution and Elaboration, is used to describe the relation between the given spans (Mann and Thompson, 1988; Carlson and Marcu, 2001). To build such trees, RST parsing consists of discourse segmentation, a task to detect EDU boundaries in a given text, and discourse parsing, a task to link spans for detected EDUs. In this paper, we focus on discourse segmentation and sentence-level discourse parsing, which are indispensable in RST parsing (Joty et al., 2013; 1 Introduction Feng and Hirst, 2014a; Joty et al., 2015; Wang et al., 2017; Kobayashi et al., 2020) and are appliTextual coherence is essential for writing a natural cable to many downstream tasks, such as machine language text that is comprehensible to readers. To recognize the coherent structure of a natural lan- translation (Guzmán et al., 2014; Joty et al., 2017) and sentence compression (Sporleder and Lapata, guage text, Rhetorical Structure Theory (RST) is 2005). applied to describe an internal discourse structure for the text as a constituent tree (Mann and ThompIn discourse segmentation, Carlson et al. (2001) son, 1988)"
2021.emnlp-main.188,W04-3250,0,0.0715317,"gmentation 6,768 4,524 - 905 636 861 991 602 951 Table 1: The number of sentences for each task. 5.1.2 Evaluation Metrics In task (a), we evaluated the segmentation in microaveraged precision, recall, and F1 score with respect to the start position of each EDU. The position at the beginning of a sentence was ignored. In task (b), we evaluated the parsing in microaveraged F1 score with respect to span, nuclearity, and relation. In task (c) for parsing with automatic segmentation, we evaluated both the segmentation and parsing in micro-averaged F1 score. We used the paired bootstrap resampling (Koehn, 2004) for the significance test in all tasks when comparing two systems. 5.1.3 Compared Methods As our proposed methods, we used LMGCe , LMGCs , LMGCu , and LMGCr , which respectively model probability P (x, e), P (x, e, s), P (x, e, u), and P (x, e, r) with initialized label embeddings. We represent LMGC with Average and Concatenate label embeddings as Enhance and Extend, respectively. We used the base discourse segmenter and parser described in Section 3 as our baseline. We reproduced the base discourse segmenter BiLSTMCRF4 (Wang et al., 2018b). Because BiLSTM-CRF adopted the hidden states of ELM"
2021.emnlp-main.188,P19-1410,0,0.094499,"on et al., 2002) show that LMGC can achieve the state-of-the-art scores in both discourse segmentation and sentence-level discourse parsing. LMGC utilizing our enhanced label embeddings achieves the best F1 score of 96.72 in discourse segmentation. Furthermore, in sentence-level discourse parsing, LMGC utilizing our enhanced relation label embeddings achieves the best relation F1 scores of 84.69 with gold EDU boundaries and 81.18 with automatically segmented boundaries, respectively. 2 Related Work Feng and Hirst, 2014a; Joty et al., 2015; Wang et al., 2017; Kobayashi et al., 2020). Recently, Lin et al. (2019) tried to jointly perform discourse segmentation and sentence-level discourse parsing with pointer-networks and achieved the state-ofthe-art F1 scores in both discourse segmentation and sentence-level discourse parsing. In spite of the performance improvement of these models, a restricted number of labeled RST discourse trees is still a problem. In the discourse segmentation and parsing tasks, most prior work is on the basis of discriminative models, which learn mapping from input texts to predicted labels. Thus, there still remains room for improving model performance by considering mapping f"
2021.emnlp-main.188,D19-1233,0,0.0803629,"hieved the state-ofthe-art F1 scores in both discourse segmentation and sentence-level discourse parsing. In spite of the performance improvement of these models, a restricted number of labeled RST discourse trees is still a problem. In the discourse segmentation and parsing tasks, most prior work is on the basis of discriminative models, which learn mapping from input texts to predicted labels. Thus, there still remains room for improving model performance by considering mapping from predictable labels to input texts to exploit more label information. To consider such information in a model, Mabona et al. (2019) introduced a generative model-based parser, RNNG (Dyer et al., 2016), to document-level RST discourse parsing. Different from our LMGC, this model unidirectionally predicts action sequences. In this research, we model LMGC for the discourse segmentation and sentence-level discourse parsing tasks. LMGC utilizes a BERT-style bidirectional Transformer encoder (Devlin et al., 2019) to avoid prediction bias caused by using different decoding directions. Since LMGC is on the basis of generative models, it can jointly consider an input text and its predictable labels, and map the embeddings of both"
2021.emnlp-main.188,N19-4009,0,0.0388739,"Missing"
2021.emnlp-main.188,N18-1202,0,0.0453834,"r the significance test in all tasks when comparing two systems. 5.1.3 Compared Methods As our proposed methods, we used LMGCe , LMGCs , LMGCu , and LMGCr , which respectively model probability P (x, e), P (x, e, s), P (x, e, u), and P (x, e, r) with initialized label embeddings. We represent LMGC with Average and Concatenate label embeddings as Enhance and Extend, respectively. We used the base discourse segmenter and parser described in Section 3 as our baseline. We reproduced the base discourse segmenter BiLSTMCRF4 (Wang et al., 2018b). Because BiLSTM-CRF adopted the hidden states of ELMo (Peters et al., 2018) as word embeddings, we also tried the last hidden state of MPNet as the word embeddings for BiLSTM-CRF for fairness. We retrained the segmenter in five runs, and the experimental results are showed in Appendix C. The publicly shared BiLSTM-CRF by Wang et al. (2018b) is our base segmenter in the following experiments. As for the base parser, we retrained two models, 2-stage Parser5 (Wang et al., 2017) and span-based Parser6 (Kobayashi et al., 2020). Different from the setting of Lin et al. (2019), we retrained 2stage Parser in the sentence-level rather than in the document-level. Since the exp"
2021.emnlp-main.188,2020.acl-main.240,0,0.022406,"deling (MPNet) takes the advantages of both masked language modeling and permuted language modeling while overcoming their issues. Compared with Bert (Devlin et al., 2019) and XLNet (Yang et al., 2019), MPNet considered more information about tokens and positions, and achieved better results for several downsteam tasks (GLUE, SQuAD, etc). Taking into account its better performance, we choose pre-trained MPNet (Song et al., 2020) as our language model. Because considering all possible inter-dependence between zt is intractable, we follow the decomposition of pseudo-log-likelihood scores (PLL) (Salazar et al., 2020) in the model. Thus, we decompose and calculate logarithmic P (z) as follows: log P (z; θ) (2) a X ≈ P LL(z; θ) = log P (zt |z<t , z>t , Mt ; θ), t=1 where z<t is the first sub-sequence (z1 , · · · , zt−1 ) in z and z>t is the latter sub-sequence (zt+1 , · · · , za ) in z. Mt denotes the mask token [MASK] at position t. P (zt |z<t , z>t , Mt ; θ) is computed by two-stream self-attention (Yang et al., 2019). In inference, we select z based on 1 a P LL(z; θ). This model converts z into continuous vectors w = {w1 , · · · , wa } through the embedding layer. Multi-head attention layers further tran"
2021.emnlp-main.188,N18-2074,0,0.0715992,"Missing"
2021.emnlp-main.188,H05-1033,0,0.266801,"Missing"
2021.emnlp-main.188,P18-1216,0,0.389714,"n RST consists of proposed a method for using lexical information elementary discourse units (EDUs), spans that de- and syntactic parsing results. Many researchers scribe recursive connections between EDUs, and (Fisher and Roark, 2007; Xuan Bach et al., 2012; nuclearity and relation labels that describe relation- Feng and Hirst, 2014b) utilized these clues as feaships for each connection. tures in a classifier although automatic parsing erFigure 1 (a) shows an example RST discourse rors degraded segmentation performance. To avoid tree. A span including one or more EDUs is a node this problem, Wang et al. (2018b) used BiLSTMof the tree. Given two adjacent non-overlapping CRF (Huang et al., 2015) to handle an input withspans, their nuclearity can be either nucleus or out these clues in an end-to-end manner. Lin satellite, denoted by N and S, where the nucleus et al. (2019) jointly performed discourse segmentarepresents a more salient or essential piece of infor- tion and sentence-level discourse parsing in their mation than the satellite. Furthermore, a relation pointer-network-based model. They also intro2432 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pag"
2021.emnlp-main.188,P17-2029,0,0.18217,"S )Attribution Figure 1: An example discourse tree structure. label, such as Attribution and Elaboration, is used to describe the relation between the given spans (Mann and Thompson, 1988; Carlson and Marcu, 2001). To build such trees, RST parsing consists of discourse segmentation, a task to detect EDU boundaries in a given text, and discourse parsing, a task to link spans for detected EDUs. In this paper, we focus on discourse segmentation and sentence-level discourse parsing, which are indispensable in RST parsing (Joty et al., 2013; 1 Introduction Feng and Hirst, 2014a; Joty et al., 2015; Wang et al., 2017; Kobayashi et al., 2020) and are appliTextual coherence is essential for writing a natural cable to many downstream tasks, such as machine language text that is comprehensible to readers. To recognize the coherent structure of a natural lan- translation (Guzmán et al., 2014; Joty et al., 2017) and sentence compression (Sporleder and Lapata, guage text, Rhetorical Structure Theory (RST) is 2005). applied to describe an internal discourse structure for the text as a constituent tree (Mann and ThompIn discourse segmentation, Carlson et al. (2001) son, 1988). A discourse tree in RST consists of p"
2021.emnlp-main.330,P16-1046,0,0.025871,"NeRoBERTa can extract coherent sentences for a summary of a given document by utilizing nested tree structures1 of two Document summarization is a task of creating a concise summary from a given document while keeping the original content. In general, sentence extraction methods, which select sentences in a document to create its summary, have the advantages of truthfulness compared with abstractive methods (Cao et al., 2018) and of fluency compared with word extraction methods (Xu et al., 2020). Neural networks have achieved great success in sentence extraction-based document summarization (Cheng and Lapata, 2016; Zhou et al., 2018). Recently, Liu and Lapata (2019) proposed BERTSUM, which utilizes BERT (Devlin et al., 2019) for sentence representations to create a summary. Although the use of BERT resulted in significant performance improvement, this method decides the selection for each sentence independently. Xu et al. (2020) proposed DISCOBERT by considering inter-sentence information through discourse 1 Kikuchi et al. (2014) considered the nested tree strucgraphs to construct a coherent summary. Although ture in the traditional non-neural tree-trimming method. Their they achieved remarkable scores"
2021.emnlp-main.330,N19-1423,0,0.0127323,"two Document summarization is a task of creating a concise summary from a given document while keeping the original content. In general, sentence extraction methods, which select sentences in a document to create its summary, have the advantages of truthfulness compared with abstractive methods (Cao et al., 2018) and of fluency compared with word extraction methods (Xu et al., 2020). Neural networks have achieved great success in sentence extraction-based document summarization (Cheng and Lapata, 2016; Zhou et al., 2018). Recently, Liu and Lapata (2019) proposed BERTSUM, which utilizes BERT (Devlin et al., 2019) for sentence representations to create a summary. Although the use of BERT resulted in significant performance improvement, this method decides the selection for each sentence independently. Xu et al. (2020) proposed DISCOBERT by considering inter-sentence information through discourse 1 Kikuchi et al. (2014) considered the nested tree strucgraphs to construct a coherent summary. Although ture in the traditional non-neural tree-trimming method. Their they achieved remarkable scores in ROUGE, it was method extracted words by tracking their parent words and 4039 Proceedings of the 2021 Conferen"
2021.emnlp-main.330,D13-1158,0,0.0194143,"document can be parsed into a tree format with the RST parser, where each leaf node is an EDU, a text span in the document. Each text span has two types, nucleus and satellite. While the nucleus spans contain semantically salient information, the satellite spans support and modify the nucleus ones. We use the recent state-of-the-art RST parser2 (Kobayashi et al., 2020) to build an RST discourse tree (RST-DT) for all documents and convert it to an Inter-Sentential RST-DT (ISRST-DT). The ISRST-DT is first converted into a dependency-based discourse tree (ISDEPDT) using the method described in (Hirao et al., 2013). Then, parent-child dependency relationships for each sentence can be formed. We construct a directed graph for the discourse dependencies (Ishigaki et al., 2019). A dependency parser is used to build up the syntactic dependency relationships between words (Manning et al., 2014). We construct an undirected graph for the syntactic dependencies by following the previous settings (Marcheggiani and Titov, 2017). 3 Our Model Ishigaki et al. (2019) consider dependency information through hierarchical attention modules (Kamigaito et al., 2018) trained in supervised attention for dependency heads (Ka"
2021.emnlp-main.330,R19-1059,1,0.886718,"raditional non-neural tree-trimming method. Their they achieved remarkable scores in ROUGE, it was method extracted words by tracking their parent words and 4039 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4039–4044 c November 7–11, 2021. 2021 Association for Computational Linguistics different trees, syntactic and discourse dependency trees (Zhao and Huang, 2017). Figure 1 shows the proposed NeRoBERTa to select sentences from a given document. Different from the previous works that focused on inter-sentence information using discourse graphs (Ishigaki et al., 2019; Xu et al., 2020), NeRoBERTa considers both intra- and inter-sentence information (syntactic and discourse graphs) together as a nested tree. The nested tree is encoded as a vector space representation through a graph attention network (Veliˇckovi´c et al., 2018) on a BERT-based encoder. In this tree, we can explicitly represent sentence information at “root” words for each syntactic dependency tree without relying only on “[CLS]” tokens. This representation is useful to extract informative and coherent sentences in that it can capture keywords in a sentence for considering textual coherence"
2021.emnlp-main.330,N18-1155,1,0.844921,"ased discourse tree (ISDEPDT) using the method described in (Hirao et al., 2013). Then, parent-child dependency relationships for each sentence can be formed. We construct a directed graph for the discourse dependencies (Ishigaki et al., 2019). A dependency parser is used to build up the syntactic dependency relationships between words (Manning et al., 2014). We construct an undirected graph for the syntactic dependencies by following the previous settings (Marcheggiani and Titov, 2017). 3 Our Model Ishigaki et al. (2019) consider dependency information through hierarchical attention modules (Kamigaito et al., 2018) trained in supervised attention for dependency heads (Kamigaito et al., 2017). Unlike the previous work, our model uses constructed graph information through graph encoder layers that directly focus on the relationships between nodes defined by edges in the graph. We explain the details of our model in this section. Let wi be the i-th token in a document D = {w1 , w2 , ..., wn }. Our model predicts p(1|D, k), the probability of the k-th sentence in D being kept in a summary through the following modules. 3.1 Pre-trained Document Encoder We append “[CLS]” and “[SEP]” tokens between sentences t"
2021.emnlp-main.330,I17-2002,1,0.829347,"3). Then, parent-child dependency relationships for each sentence can be formed. We construct a directed graph for the discourse dependencies (Ishigaki et al., 2019). A dependency parser is used to build up the syntactic dependency relationships between words (Manning et al., 2014). We construct an undirected graph for the syntactic dependencies by following the previous settings (Marcheggiani and Titov, 2017). 3 Our Model Ishigaki et al. (2019) consider dependency information through hierarchical attention modules (Kamigaito et al., 2018) trained in supervised attention for dependency heads (Kamigaito et al., 2017). Unlike the previous work, our model uses constructed graph information through graph encoder layers that directly focus on the relationships between nodes defined by edges in the graph. We explain the details of our model in this section. Let wi be the i-th token in a document D = {w1 , w2 , ..., wn }. Our model predicts p(1|D, k), the probability of the k-th sentence in D being kept in a summary through the following modules. 3.1 Pre-trained Document Encoder We append “[CLS]” and “[SEP]” tokens between sentences to encode a whole document (Liu and Lapata, 2019). Then, BERT is used to build"
2021.emnlp-main.330,2020.acl-main.451,0,0.394546,"ntences in a document. We propose a nested tree-based extractive summarization model on RoBERTa (NeRoBERTa), where nested tree structures consist of syntactic and discourse trees in a given document. Experimental results on the CNN/DailyMail dataset showed that NeRoBERTa outperforms baseline models in ROUGE. Human evaluation results also showed that NeRoBERTa achieves significantly better scores than the baselines in terms of coherence and yields comparable scores to the state-of-the-art models. 0 Intra 1 Inter 0 (DiscoBERT) 1 Abstract ?$ Figure 1: Different from the previous work, DISCOBERT (Xu et al., 2020), NeRoBERTa selects sentences by considering both intra- and inter-sentence relationships as a nested tree structure. still difficult to construct a coherent summary compared to BERTSUM in human evaluation. Zhong et al. (2020) attempted to change the paradigm by formulating summary-level extraction with a RoBERTa encoder and achieved the state-of-theart results on the CNN/DailyMail dataset. In spite of the successful results of the above BERT-related methods, their sentence representations have room for improvement. As Liu et al. (2019) reported, “[CLS]”, a pre-defined token for indicating sen"
2021.emnlp-main.330,P14-2052,1,0.525423,"2018) and of fluency compared with word extraction methods (Xu et al., 2020). Neural networks have achieved great success in sentence extraction-based document summarization (Cheng and Lapata, 2016; Zhou et al., 2018). Recently, Liu and Lapata (2019) proposed BERTSUM, which utilizes BERT (Devlin et al., 2019) for sentence representations to create a summary. Although the use of BERT resulted in significant performance improvement, this method decides the selection for each sentence independently. Xu et al. (2020) proposed DISCOBERT by considering inter-sentence information through discourse 1 Kikuchi et al. (2014) considered the nested tree strucgraphs to construct a coherent summary. Although ture in the traditional non-neural tree-trimming method. Their they achieved remarkable scores in ROUGE, it was method extracted words by tracking their parent words and 4039 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4039–4044 c November 7–11, 2021. 2021 Association for Computational Linguistics different trees, syntactic and discourse dependency trees (Zhao and Huang, 2017). Figure 1 shows the proposed NeRoBERTa to select sentences from a given document. Differ"
2021.emnlp-main.330,D17-1225,0,0.0275423,"(2020) proposed DISCOBERT by considering inter-sentence information through discourse 1 Kikuchi et al. (2014) considered the nested tree strucgraphs to construct a coherent summary. Although ture in the traditional non-neural tree-trimming method. Their they achieved remarkable scores in ROUGE, it was method extracted words by tracking their parent words and 4039 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4039–4044 c November 7–11, 2021. 2021 Association for Computational Linguistics different trees, syntactic and discourse dependency trees (Zhao and Huang, 2017). Figure 1 shows the proposed NeRoBERTa to select sentences from a given document. Different from the previous works that focused on inter-sentence information using discourse graphs (Ishigaki et al., 2019; Xu et al., 2020), NeRoBERTa considers both intra- and inter-sentence information (syntactic and discourse graphs) together as a nested tree. The nested tree is encoded as a vector space representation through a graph attention network (Veliˇckovi´c et al., 2018) on a BERT-based encoder. In this tree, we can explicitly represent sentence information at “root” words for each syntactic depende"
2021.emnlp-main.330,2020.acl-main.552,0,0.04056,"the CNN/DailyMail dataset showed that NeRoBERTa outperforms baseline models in ROUGE. Human evaluation results also showed that NeRoBERTa achieves significantly better scores than the baselines in terms of coherence and yields comparable scores to the state-of-the-art models. 0 Intra 1 Inter 0 (DiscoBERT) 1 Abstract ?$ Figure 1: Different from the previous work, DISCOBERT (Xu et al., 2020), NeRoBERTa selects sentences by considering both intra- and inter-sentence relationships as a nested tree structure. still difficult to construct a coherent summary compared to BERTSUM in human evaluation. Zhong et al. (2020) attempted to change the paradigm by formulating summary-level extraction with a RoBERTa encoder and achieved the state-of-theart results on the CNN/DailyMail dataset. In spite of the successful results of the above BERT-related methods, their sentence representations have room for improvement. As Liu et al. (2019) reported, “[CLS]”, a pre-defined token for indicating sentence representations on BERT, is insufficient to express sentence information. Even in RoBERTa, it is also a problem due to the lack of next sentence prediction in its pretraining step. Therefore, for further improving summar"
2021.emnlp-main.330,W04-3250,0,0.0503967,"-of-the-art model, sampled 100 documents from the test dataset. MatchSum. The informativeness score for Match“Amazon Mturk” was used for the experiments, Sum was lower than RoBERTa and NeRoBERTa. and human evaluators graded scores from 1 to 5 Table 3 shows example extracted sentences from (5 is the best) in terms of four evaluation critea document and their discourse graph. In this exria.5 Because summaries from DISCOBERT were ample, the discourse information alone was not worse than ones from BERTSUM in their human enough in that S3 and S10 have the same dis4 The paired-bootstrap-resampling (Koehn, 2004) was used course information, while S3 is more similar to (p &lt; 0.05). 5 the third sentence in the gold summary. RoBERTa 40 human evaluators who obtained both US high school and US bachelor degrees participated in the experiments. and DiRoBERTa constructed the same summary in4042 Model Coh Infor Read Red Focused Research Award. MatchSum RoBERTa NeRoBERTa 4.06 4.02 4.08† 4.11 4.14 4.14 4.09 4.09 4.10 4.17 4.12 4.16 References Table 2: Human evaluation results. † indicates that the improvement with NeRoBERTa from RoBERTa was statistically significant.4 S1 Barcelona club president josep maria bart"
2021.emnlp-main.330,D19-1387,0,0.244392,"y of a given document by utilizing nested tree structures1 of two Document summarization is a task of creating a concise summary from a given document while keeping the original content. In general, sentence extraction methods, which select sentences in a document to create its summary, have the advantages of truthfulness compared with abstractive methods (Cao et al., 2018) and of fluency compared with word extraction methods (Xu et al., 2020). Neural networks have achieved great success in sentence extraction-based document summarization (Cheng and Lapata, 2016; Zhou et al., 2018). Recently, Liu and Lapata (2019) proposed BERTSUM, which utilizes BERT (Devlin et al., 2019) for sentence representations to create a summary. Although the use of BERT resulted in significant performance improvement, this method decides the selection for each sentence independently. Xu et al. (2020) proposed DISCOBERT by considering inter-sentence information through discourse 1 Kikuchi et al. (2014) considered the nested tree strucgraphs to construct a coherent summary. Although ture in the traditional non-neural tree-trimming method. Their they achieved remarkable scores in ROUGE, it was method extracted words by tracking"
2021.emnlp-main.330,P18-1061,0,0.0197979,"oherent sentences for a summary of a given document by utilizing nested tree structures1 of two Document summarization is a task of creating a concise summary from a given document while keeping the original content. In general, sentence extraction methods, which select sentences in a document to create its summary, have the advantages of truthfulness compared with abstractive methods (Cao et al., 2018) and of fluency compared with word extraction methods (Xu et al., 2020). Neural networks have achieved great success in sentence extraction-based document summarization (Cheng and Lapata, 2016; Zhou et al., 2018). Recently, Liu and Lapata (2019) proposed BERTSUM, which utilizes BERT (Devlin et al., 2019) for sentence representations to create a summary. Although the use of BERT resulted in significant performance improvement, this method decides the selection for each sentence independently. Xu et al. (2020) proposed DISCOBERT by considering inter-sentence information through discourse 1 Kikuchi et al. (2014) considered the nested tree strucgraphs to construct a coherent summary. Although ture in the traditional non-neural tree-trimming method. Their they achieved remarkable scores in ROUGE, it was me"
2021.emnlp-main.330,2021.ccl-1.108,0,0.0606257,"Missing"
2021.emnlp-main.330,P14-5010,0,0.00335786,"dify the nucleus ones. We use the recent state-of-the-art RST parser2 (Kobayashi et al., 2020) to build an RST discourse tree (RST-DT) for all documents and convert it to an Inter-Sentential RST-DT (ISRST-DT). The ISRST-DT is first converted into a dependency-based discourse tree (ISDEPDT) using the method described in (Hirao et al., 2013). Then, parent-child dependency relationships for each sentence can be formed. We construct a directed graph for the discourse dependencies (Ishigaki et al., 2019). A dependency parser is used to build up the syntactic dependency relationships between words (Manning et al., 2014). We construct an undirected graph for the syntactic dependencies by following the previous settings (Marcheggiani and Titov, 2017). 3 Our Model Ishigaki et al. (2019) consider dependency information through hierarchical attention modules (Kamigaito et al., 2018) trained in supervised attention for dependency heads (Kamigaito et al., 2017). Unlike the previous work, our model uses constructed graph information through graph encoder layers that directly focus on the relationships between nodes defined by edges in the graph. We explain the details of our model in this section. Let wi be the i-th"
2021.emnlp-main.330,D17-1159,0,0.0252639,"ee (RST-DT) for all documents and convert it to an Inter-Sentential RST-DT (ISRST-DT). The ISRST-DT is first converted into a dependency-based discourse tree (ISDEPDT) using the method described in (Hirao et al., 2013). Then, parent-child dependency relationships for each sentence can be formed. We construct a directed graph for the discourse dependencies (Ishigaki et al., 2019). A dependency parser is used to build up the syntactic dependency relationships between words (Manning et al., 2014). We construct an undirected graph for the syntactic dependencies by following the previous settings (Marcheggiani and Titov, 2017). 3 Our Model Ishigaki et al. (2019) consider dependency information through hierarchical attention modules (Kamigaito et al., 2018) trained in supervised attention for dependency heads (Kamigaito et al., 2017). Unlike the previous work, our model uses constructed graph information through graph encoder layers that directly focus on the relationships between nodes defined by edges in the graph. We explain the details of our model in this section. Let wi be the i-th token in a document D = {w1 , w2 , ..., wn }. Our model predicts p(1|D, k), the probability of the k-th sentence in D being kept i"
2021.findings-acl.152,2020.acl-main.630,0,0.024606,"reveal that our method taps into the deeper 1743 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1743–1750 August 1–6, 2021. ©2021 Association for Computational Linguistics potential of BERT, leading to optimism that BERT can be further improved for text classification as well as other downstream tasks. 2 Related Work Apart from the pre-trained models for learning general language representations mentioned above, some studies have focused specifically on leveraging the representations of classes or the higher level global information. Examples include tBERT (Peinelt et al., 2020), which combines topic models with BERT for pairwise semantic similarity detection, and LCM (Guo et al., 2020), which generates an enhancement distribution to the one-hot vector representing the classes by calculating the similarity between instances and labels to improve the classification performance. Moreover, the label embedding has increasingly taken a leading role in related research. It is a technique in which the contents of labels are also embedded, so that the model can be trained to deal with the label information and input features at the same time. It is proven to be effective in"
2021.findings-acl.152,D14-1162,0,0.0860599,"2014; Zhang et al., 2015; Conneau et al., 2017; Johnson and Zhang, 2017; Zhang et al., 2017; Shen et al., 2018), recurrent models (Liu et al., 2016; Yogatama et al., 2017; Seo et al., 2017; Wang et al., 2018b), and attention mechanisms (Yang et al., 2016; Lin et al., 2017). Pre-trained models have also been greatly beneficial in text classification in that they help streamline the training process by avoiding a start from zero (Stein et al., 2019; Wang et al., 2017; Jiang et al., 2019). One group of approaches has focused on word embeddings, such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014); another has focused on contextualized word embeddings, from CoVe (McCann et al., 2017) to ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), ULMFiT (Howard and Ruder, 2018), and BERT (Devlin et al., 2019). BERT has achieved particularly impressive performances across a variety of NLP tasks. With its success, models pre-trained on a large amount of data, such as ERNIE (Zhang et al., 2019), RoBERTa (Liu et al., 2019), UniLM (Dong et al., 2019), and XLnet (Yang et al., 2019), have become popular thanks to their ability in learning contextualized representations. These models are bas"
2021.findings-acl.152,N18-1202,0,0.00887788,"t models (Liu et al., 2016; Yogatama et al., 2017; Seo et al., 2017; Wang et al., 2018b), and attention mechanisms (Yang et al., 2016; Lin et al., 2017). Pre-trained models have also been greatly beneficial in text classification in that they help streamline the training process by avoiding a start from zero (Stein et al., 2019; Wang et al., 2017; Jiang et al., 2019). One group of approaches has focused on word embeddings, such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014); another has focused on contextualized word embeddings, from CoVe (McCann et al., 2017) to ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), ULMFiT (Howard and Ruder, 2018), and BERT (Devlin et al., 2019). BERT has achieved particularly impressive performances across a variety of NLP tasks. With its success, models pre-trained on a large amount of data, such as ERNIE (Zhang et al., 2019), RoBERTa (Liu et al., 2019), UniLM (Dong et al., 2019), and XLnet (Yang et al., 2019), have become popular thanks to their ability in learning contextualized representations. These models are based on the multi-layered bidirectional attention mechanism (Vaswani et al., 2017) and are trained through the masked wo"
2021.findings-acl.152,P18-1216,0,0.118056,"asets demonstrate its effectiveness. 1 Introduction Text classification is a classic problem in natural language processing (NLP). The task is to annotate a predefined class or classes to a given text, where text representation is an important intermediate step. A variety of neural models have been developed to learn better text representations, including convolution models (Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Conneau et al., 2017; Johnson and Zhang, 2017; Zhang et al., 2017; Shen et al., 2018), recurrent models (Liu et al., 2016; Yogatama et al., 2017; Seo et al., 2017; Wang et al., 2018b), and attention mechanisms (Yang et al., 2016; Lin et al., 2017). Pre-trained models have also been greatly beneficial in text classification in that they help streamline the training process by avoiding a start from zero (Stein et al., 2019; Wang et al., 2017; Jiang et al., 2019). One group of approaches has focused on word embeddings, such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014); another has focused on contextualized word embeddings, from CoVe (McCann et al., 2017) to ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), ULMFiT (Howard and Ruder, 201"
2021.naacl-industry.32,Q17-1010,0,0.0053715,"2019). We treat the predicted QS as rQ (x, y), the reward of the ad-text’s quality. To train a classifier to predict QS from the given x and y, we prepare an ad dataset from our ad-text databases. This dataset consists of a title, description, and score that represent the quality of each ad. The QS ranges from 1 to 10, where a lower score corresponds to lower ad quality and a higher score indicates higher ad quality. We develop a simple regression model to predict the QS from the title and description. In the model, the title and description are joined and encoded into embeddings by fastText (Bojanowski et al., 2017) and max-pooling of simple word embedding (SWEM) (Shen et al., 2018). After the encoding, we use the gradient boosting regression tree (GBRT) (Ke et al., 2017) to predict the QS. We developed a simple model because that the predicted quality score is used as a reward in RL, which requires a large computational time; therefore, efforts should be made to shorten it. 3 3.1 Experimental Settings Dataset In this dataset, we used the meta-description as the content of the LP. In addition to the ad-texts, we prepared Japanese Wikipedia articles2 to pretrain the language model. The fine-tuning of LM w"
2021.naacl-industry.32,P18-1041,0,0.0132816,"quality. To train a classifier to predict QS from the given x and y, we prepare an ad dataset from our ad-text databases. This dataset consists of a title, description, and score that represent the quality of each ad. The QS ranges from 1 to 10, where a lower score corresponds to lower ad quality and a higher score indicates higher ad quality. We develop a simple regression model to predict the QS from the title and description. In the model, the title and description are joined and encoded into embeddings by fastText (Bojanowski et al., 2017) and max-pooling of simple word embedding (SWEM) (Shen et al., 2018). After the encoding, we use the gradient boosting regression tree (GBRT) (Ke et al., 2017) to predict the QS. We developed a simple model because that the predicted quality score is used as a reward in RL, which requires a large computational time; therefore, efforts should be made to shorten it. 3 3.1 Experimental Settings Dataset In this dataset, we used the meta-description as the content of the LP. In addition to the ad-texts, we prepared Japanese Wikipedia articles2 to pretrain the language model. The fine-tuning of LM was performed on the ad-text dataset. All texts in this dataset were"
2021.naacl-industry.32,D17-1062,0,0.0165433,"iew of proposed ad-text generation method 2.2.1 Fluency 2.2.2 Fluency is an essential factor in generating natural language texts. In addition, the length limitation of the text must be considered, as space for advertising is limited. If the ad-text is truncated owing to space limitations, its fluency is significantly degraded. To address these problems, our fluency reward consists of two types of scores as follows: rF = sLM (y) + sLen (y), (6) where sLM (y) is a grammatical score and sLen (y) scores the fidelity of |y |to the given desired length. We use the function described in Eq. (10) of Zhang and Lapata (2017) as the first score sLM (y). The second score, sLen (y), measures the appropriateness of the length of the generated text. The length of the generated text must not exceed the length limit. However, to maintain informativeness, it should not be significantly shorter than the limit. We incorporate these factors into sLen . Let ytitle be the title part of y, ydesc be the description part of y, Ctitle be the length limit of the title part, Cdesc be the length limit of the description part, and sl be a score function for each part of the generated text. The score sLen is calculated as follows: sLe"
2021.naacl-main.127,P19-1061,0,0.0150032,"aim at improving the Span and Nuclearity scores. Unsupervised RST parsing methods have also been proposed recently (Kobayashi et al., 2019; Nishida and Nakayama, 2020). Since they are unsupervised, they do not require any annotated corpora. However, they can predict only tree structures and 1601 2 Nguyen et al. (2020) proposed a similar approach in NMT and introduced a method named data diversification: it diversifies the training data by using multiple forward and backward translation models. We can find some weak supervision approaches for other discourse representation formalisms such as (Badene et al., 2019). Teacher parsers Autoparsed trees Ptch=1 Ptch=k Training Unlabeled Documents CNN … … Gold data: RST-DT Autoparsed trees Silver data: Agreement subtrees Student parser Pstu Pre-training Gold data: RST-DT Pstu Fine-tuning Figure 1: Overview of proposed method. In the subtree extraction step, the teacher RST parsers first annotate trees to unlabeled documents, and then the proposed subtree extraction method constructs large silver data. In the training step, the student parser is trained through pre-training and fine-tuning. cannot predict nucleus and relation labels. Therefore, the predicted tr"
2021.naacl-main.127,E17-1028,0,0.0165477,". Kobayashi et al. (2020) proposed another top-down RST parsing method exploiting multiple granularity levels in a document and achieved the best Span and Nuclearity scores on the RST-DT, i.e., F1 of 87.0 and 74.6, respectively. Since the RST-DT, the largest treebank, contains only 385 documents, several studies have been conducted on overcoming the problem of a limited number of training data. Braud et al. (2016) leveraged multi-task learning not only with 13 related tasks as an auxiliary task but also for multiple views of discourse structures, such as Constituent, Nuclearity, and Relation. Braud et al. (2017) used multilingual RST discourse datasets that share the same underlying linguistic theory. Huber and Carenini (2019) adopted distant supervision with an auxiliary task of sentiment classification to create largescale training data, i.e., they trained a two-stage RST parser (Wang et al., 2017a) with RST trees automatically built based on attention and sentiment scores from the Multiple-Instance Learning network, which was trained with a review dataset. However, these studies need other annotated corpora than the RST-DT, which means we still face the problem of being dependent on costly annotat"
2021.naacl-main.127,W01-1605,0,0.226951,"r ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of documents, and thus it is difficult to obtain a large amount of human-annotated data for RST parsing. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), although even this is still small with only 385 documents.1 Many RST parsing methods have recently been developed based on neural models (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017; Braud et al., 2016, 2017). Among them, Kobayashi et al. (2020) is the current state-of-theart system and is based on the neural top-down method. While its Span and Nuclearity scores achieved the highest level, its Relation score still has room for improvement. One of the reasons for its poor Relation score might be its small amount of training data for solving the 18-class classificatio"
2021.naacl-main.127,W11-0401,0,0.0365009,"d theories for representing the discourse structure of a text as a tree. RST trees are a kind of constituent tree, whose leaves are Elementary Discourse Units (EDUs), i.e., clause-like units, and whose non-terminal nodes cover text spans consisting of either a sequence of EDUs or a single EDU. The label of a non-terminal node represents the attribution of a text span, i.e., nucleus (N) or satellite (S). A discourse relation is also assigned between two adjacent non-terminal nodes. 1 We can find some exceptions for other languages such In most cases, RST parsers have been devel- as Spanish (da Cunha et al., 2011) and German (Stede and oped on the basis of supervised learning algorithms Neumann, 2014). 1600 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1600–1612 June 6–11, 2021. ©2021 Association for Computational Linguistics (Sennrich et al., 2016) introduced a simple learning framework: first pre-train an NMT model with silver data, i.e., pseudo-parallel data generated by automatic back-translation, and then fine-tune it with gold data, i.e., real parallel data, to overcome the data sparseness prob"
2021.naacl-main.127,P14-1048,0,0.0273741,"cale data. We first pre-train the student parser by using the obtained silver data. We then fine-tune parameters of the parser on gold data, using the RST-DT. Experimental results on the RST-DT clearly indicate the effectiveness of our silver data. Our method obtained remarkable Nuclearity and Relation F1 scores of 75.0 and 63.2, respectively. 2 Related Work Early studies on RST parsing were based on traditional supervised learning methods with handcrafted features and the shift-reduce or CKY-like parsing algorithms (duVerle and Prendinger, 2009; Feng and Hirst, 2012; Joty et al., 2013, 2015; Feng and Hirst, 2014). Recently, Wang et al. (2017b) proposed a shift-reduce parser based on SVMs and achieved the current best results in classical statistical models on the RST-DT. The method first built nuclearity-labeled RST trees and then assigned relation labels between two adjacent spans consisting of a single or multiple EDUs. Inspired by the success of neural networks in many NLP tasks, several neural network-based models have been proposed for RST parsing (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017). Yu et al. (2018) proposed a shift-reduce parser based on neural networks and l"
2021.naacl-main.127,D19-1235,0,0.0143347,"n a document and achieved the best Span and Nuclearity scores on the RST-DT, i.e., F1 of 87.0 and 74.6, respectively. Since the RST-DT, the largest treebank, contains only 385 documents, several studies have been conducted on overcoming the problem of a limited number of training data. Braud et al. (2016) leveraged multi-task learning not only with 13 related tasks as an auxiliary task but also for multiple views of discourse structures, such as Constituent, Nuclearity, and Relation. Braud et al. (2017) used multilingual RST discourse datasets that share the same underlying linguistic theory. Huber and Carenini (2019) adopted distant supervision with an auxiliary task of sentiment classification to create largescale training data, i.e., they trained a two-stage RST parser (Wang et al., 2017a) with RST trees automatically built based on attention and sentiment scores from the Multiple-Instance Learning network, which was trained with a review dataset. However, these studies need other annotated corpora than the RST-DT, which means we still face the problem of being dependent on costly annotated corpora. Jiang et al. (2016) proposed a framework for enriching training data based on co-training to improve the"
2021.naacl-main.127,P14-1002,0,0.199435,"(3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of documents, and thus it is difficult to obtain a large amount of human-annotated data for RST parsing. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), although even this is still small with only 385 documents.1 Many RST parsing methods have recently been developed based on neural models (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017; Braud et al., 2016, 2017). Among them, Kobayashi et al. (2020) is the current state-of-theart system and is based on the neural top-down method. While its Span and Nuclearity scores achieved the highest level, its Relation score still has room for improvement. One of the reasons for its poor Relation score might be its small amount of training data for solving the 18-class classification problem. Currently, we can refer to various studies on improving neural models for NLP tasks through acquiring large-scale synthetic training data, sometimes cal"
2021.naacl-main.127,C16-1245,0,0.0171105,"ingual RST discourse datasets that share the same underlying linguistic theory. Huber and Carenini (2019) adopted distant supervision with an auxiliary task of sentiment classification to create largescale training data, i.e., they trained a two-stage RST parser (Wang et al., 2017a) with RST trees automatically built based on attention and sentiment scores from the Multiple-Instance Learning network, which was trained with a review dataset. However, these studies need other annotated corpora than the RST-DT, which means we still face the problem of being dependent on costly annotated corpora. Jiang et al. (2016) proposed a framework for enriching training data based on co-training to improve the performance for infrequent relation labels. However, the method failed to improve the overall Relation score, while they did not aim at improving the Span and Nuclearity scores. Unsupervised RST parsing methods have also been proposed recently (Kobayashi et al., 2019; Nishida and Nakayama, 2020). Since they are unsupervised, they do not require any annotated corpora. However, they can predict only tree structures and 1601 2 Nguyen et al. (2020) proposed a similar approach in NMT and introduced a method named"
2021.naacl-main.127,P13-1048,0,0.0360339,"ubtrees to handle large-scale data. We first pre-train the student parser by using the obtained silver data. We then fine-tune parameters of the parser on gold data, using the RST-DT. Experimental results on the RST-DT clearly indicate the effectiveness of our silver data. Our method obtained remarkable Nuclearity and Relation F1 scores of 75.0 and 63.2, respectively. 2 Related Work Early studies on RST parsing were based on traditional supervised learning methods with handcrafted features and the shift-reduce or CKY-like parsing algorithms (duVerle and Prendinger, 2009; Feng and Hirst, 2012; Joty et al., 2013, 2015; Feng and Hirst, 2014). Recently, Wang et al. (2017b) proposed a shift-reduce parser based on SVMs and achieved the current best results in classical statistical models on the RST-DT. The method first built nuclearity-labeled RST trees and then assigned relation labels between two adjacent spans consisting of a single or multiple EDUs. Inspired by the success of neural networks in many NLP tasks, several neural network-based models have been proposed for RST parsing (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017). Yu et al. (2018) proposed a shift-reduce parser b"
2021.naacl-main.127,J15-3002,0,0.0619835,"Missing"
2021.naacl-main.127,W04-3250,0,0.109255,"ur model is statistically significantly better than underlined scores at p-level &lt; 0.01 in pairwise comparison.8 cannot handle, by acquiring training instances with the help of the teacher parser. 5.4 5.3 S 80 Comparison with state-of-the-art parsers Detailed Analysis of Relation Labeling Finally, we compare our SBP+AST with the enTo investigate the effectiveness of SBP+AST in semble to current state-of-the-art parsers. Table more detail, we show Relation F1 scores for re- 3 shows the micro-averaged F scores. We used 1 lation labels with SBP, SBP+AST, and the two- Paired Bootstrap Resampling (Koehn, 2004) for stage parser in Figure 4. The results of SBP and the significance test. We can see that our method SBP+AST were obtained from a five-model ensem- achieved the best scores except for Span. The gains ble. In most relation labels, since the two-stage against the previous best scores were 0.4, 3.0, and parser, the teacher parser, is comparable or supe- 2.7 points for Nuclearity, Relation, and Full, rerior to SBP, i.e., the student parser, the performance spectively. In particular, the gains for Relation and of SBP+AST can be improved. It finally outper- Full are remarkable. formed the two-sta"
2021.naacl-main.127,D14-1220,0,0.375346,"tion for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of documents, and thus it is difficult to obtain a large amount of human-annotated data for RST parsing. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), although even this is still small with only 385 documents.1 Many RST parsing methods have recently been developed based on neural models (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017; Braud et al., 2016, 2017). Among them, Kobayashi et al. (2020) is the current state-of-theart system and is based on the neural top-down method. While its Span and Nuclearity scores achieved the highest level, its Relation score still has room for improvement. One of the reasons for its poor Relation score might be its small amount of training data for solving the 18-class classification problem. Currently, we can refer to various studies on improving neural models for NLP tasks through acquiring large-scale synthetic training data, sometimes called silver data."
2021.naacl-main.127,D16-1035,0,0.0452543,"Missing"
2021.naacl-main.127,P14-1043,0,0.253332,"tion for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of documents, and thus it is difficult to obtain a large amount of human-annotated data for RST parsing. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), although even this is still small with only 385 documents.1 Many RST parsing methods have recently been developed based on neural models (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017; Braud et al., 2016, 2017). Among them, Kobayashi et al. (2020) is the current state-of-theart system and is based on the neural top-down method. While its Span and Nuclearity scores achieved the highest level, its Relation score still has room for improvement. One of the reasons for its poor Relation score might be its small amount of training data for solving the 18-class classification problem. Currently, we can refer to various studies on improving neural models for NLP tasks through acquiring large-scale synthetic training data, sometimes called silver data."
2021.naacl-main.127,P19-1410,0,0.06264,"y using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser. 1 Introduction (Wang et al., 2017b; Yu et al., 2018; Kobayashi et al., 2020; Lin et al., 2019; Zhang et al., 2020), which require a high-quality annotated corpus of sufficient size. Generally, they train the following three components of the RST parsing: (1) structure prediction by splitting a text span consisting of contiguous EDUs into two smaller ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a hu"
2021.naacl-main.127,D17-1133,0,0.0881458,"ans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of documents, and thus it is difficult to obtain a large amount of human-annotated data for RST parsing. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), although even this is still small with only 385 documents.1 Many RST parsing methods have recently been developed based on neural models (Ji and Eisenstein, 2014; Li et al., 2014a, 2016; Liu and Lapata, 2017; Braud et al., 2016, 2017). Among them, Kobayashi et al. (2020) is the current state-of-theart system and is based on the neural top-down method. While its Span and Nuclearity scores achieved the highest level, its Relation score still has room for improvement. One of the reasons for its poor Relation score might be its small amount of training data for solving the 18-class classification problem. Currently, we can refer to various studies on improving neural models for NLP tasks through acquiring large-scale synthetic training data, sometimes called silver data. Among them, one of the studie"
2021.naacl-main.127,N06-1020,0,0.169545,"fferent parsers that rely on different parsing algorithms is that we can acquire instances that the student parser cannot correctly parse yet the teacher parser can parse as the training data. Second, using multiple different RST parsers in a semi-supervised manner in our work might seem reminiscent of co- or tritraining. While co- or tri-training is attractive, it is time consuming to repeat the step of alternately training multiple different neural network-based parsers many times. Thus, previous studies have focused on simplifying the repetition step in constituency and dependency parsing (McClosky et al., 2006; Yu et al., 2015; Pekar et al., 2014; Weiss et al., 2015; Li et al., 2014b). We believe our method is similar to these simplified version as a semi-supervised framework with two different RST parsers. adopt a simple pre-training and fine-tuning strategy, which is inspired by the NMT research (Sennrich et al., 2016), to train a student RST parser. Since early statistical RST parsing methods relied on handcrafted features, i.e., sentence-level features obtained from parse trees and documentlevel features, they require complete documents with complete sentences for their feature extraction. On t"
2021.naacl-main.127,D17-1136,0,0.0133782,"trees and evaluated system results with RST-DT; this setting is without any silver data. micro-averaged F1 scores of Span, Nuclearity, ReWith AST as the silver data, performance in all lation, and Full, based on RST-Parseval (Marcu, metrics improved against the baseline. In most met2000). Span, Nuclearity, Relation, and Full were rics, AST achieved the best scores. In particular, used to evaluate unlabeled, nuclearity-labeled, the gains in Relation and Full were impressive. DT relation-labeled, and fully-labeled tree structures, and ADT, which consist of document-level RST respectively. Since Morey et al. (2017) made a trees, also outperformed the baseline. However, the suggestion to use a standard parseEval toolkit for gains against the baseline were smaller than those evaluation, we also report the results using this in by AST. We believe this is related to the size and Appendix C. quality of the silver data. The number of trees and nodes in ADT is only 2,142 and 57,940, respec4.4 Compared Methods tively, while AST has 175,709 trees and 2,279,275 To demonstrate the effectiveness of our proposed nodes. Thus, a small number of silver data for method, we pre-trained the span-based neural top- pre-trai"
2021.naacl-main.127,2020.tacl-1.15,0,0.0140569,"le-Instance Learning network, which was trained with a review dataset. However, these studies need other annotated corpora than the RST-DT, which means we still face the problem of being dependent on costly annotated corpora. Jiang et al. (2016) proposed a framework for enriching training data based on co-training to improve the performance for infrequent relation labels. However, the method failed to improve the overall Relation score, while they did not aim at improving the Span and Nuclearity scores. Unsupervised RST parsing methods have also been proposed recently (Kobayashi et al., 2019; Nishida and Nakayama, 2020). Since they are unsupervised, they do not require any annotated corpora. However, they can predict only tree structures and 1601 2 Nguyen et al. (2020) proposed a similar approach in NMT and introduced a method named data diversification: it diversifies the training data by using multiple forward and backward translation models. We can find some weak supervision approaches for other discourse representation formalisms such as (Badene et al., 2019). Teacher parsers Autoparsed trees Ptch=1 Ptch=k Training Unlabeled Documents CNN … … Gold data: RST-DT Autoparsed trees Silver data: Agreement subt"
2021.naacl-main.127,W14-6105,0,0.0180731,"arsing algorithms is that we can acquire instances that the student parser cannot correctly parse yet the teacher parser can parse as the training data. Second, using multiple different RST parsers in a semi-supervised manner in our work might seem reminiscent of co- or tritraining. While co- or tri-training is attractive, it is time consuming to repeat the step of alternately training multiple different neural network-based parsers many times. Thus, previous studies have focused on simplifying the repetition step in constituency and dependency parsing (McClosky et al., 2006; Yu et al., 2015; Pekar et al., 2014; Weiss et al., 2015; Li et al., 2014b). We believe our method is similar to these simplified version as a semi-supervised framework with two different RST parsers. adopt a simple pre-training and fine-tuning strategy, which is inspired by the NMT research (Sennrich et al., 2016), to train a student RST parser. Since early statistical RST parsing methods relied on handcrafted features, i.e., sentence-level features obtained from parse trees and documentlevel features, they require complete documents with complete sentences for their feature extraction. On the other hand, recent neural models d"
2021.naacl-main.127,W05-1513,0,0.0873679,"Missing"
2021.naacl-main.127,P16-1009,0,0.215174,"e attribution of a text span, i.e., nucleus (N) or satellite (S). A discourse relation is also assigned between two adjacent non-terminal nodes. 1 We can find some exceptions for other languages such In most cases, RST parsers have been devel- as Spanish (da Cunha et al., 2011) and German (Stede and oped on the basis of supervised learning algorithms Neumann, 2014). 1600 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1600–1612 June 6–11, 2021. ©2021 Association for Computational Linguistics (Sennrich et al., 2016) introduced a simple learning framework: first pre-train an NMT model with silver data, i.e., pseudo-parallel data generated by automatic back-translation, and then fine-tune it with gold data, i.e., real parallel data, to overcome the data sparseness problem. Since the frameworks successfully improved the NMT systems, it has become a standard approach. Inspired by the above research, we propose a method for improving a student neural parser by exploiting large-scale silver data, thus generating RST trees using an automatic RST parser.2 Specifically, we improve the state-of-the-art neural RST"
2021.naacl-main.127,P17-2029,0,0.258284,"We create large-scale silver data from an unlabeled corpus by using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser. 1 Introduction (Wang et al., 2017b; Yu et al., 2018; Kobayashi et al., 2020; Lin et al., 2019; Zhang et al., 2020), which require a high-quality annotated corpus of sufficient size. Generally, they train the following three components of the RST parsing: (1) structure prediction by splitting a text span consisting of contiguous EDUs into two smaller ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for de"
2021.naacl-main.127,D18-1116,0,0.0479865,"Missing"
2021.naacl-main.127,P15-1032,0,0.0260396,"that we can acquire instances that the student parser cannot correctly parse yet the teacher parser can parse as the training data. Second, using multiple different RST parsers in a semi-supervised manner in our work might seem reminiscent of co- or tritraining. While co- or tri-training is attractive, it is time consuming to repeat the step of alternately training multiple different neural network-based parsers many times. Thus, previous studies have focused on simplifying the repetition step in constituency and dependency parsing (McClosky et al., 2006; Yu et al., 2015; Pekar et al., 2014; Weiss et al., 2015; Li et al., 2014b). We believe our method is similar to these simplified version as a semi-supervised framework with two different RST parsers. adopt a simple pre-training and fine-tuning strategy, which is inspired by the NMT research (Sennrich et al., 2016), to train a student RST parser. Since early statistical RST parsing methods relied on handcrafted features, i.e., sentence-level features obtained from parse trees and documentlevel features, they require complete documents with complete sentences for their feature extraction. On the other hand, recent neural models do not necessarily ne"
2021.naacl-main.127,W15-2201,0,0.0218308,"ly on different parsing algorithms is that we can acquire instances that the student parser cannot correctly parse yet the teacher parser can parse as the training data. Second, using multiple different RST parsers in a semi-supervised manner in our work might seem reminiscent of co- or tritraining. While co- or tri-training is attractive, it is time consuming to repeat the step of alternately training multiple different neural network-based parsers many times. Thus, previous studies have focused on simplifying the repetition step in constituency and dependency parsing (McClosky et al., 2006; Yu et al., 2015; Pekar et al., 2014; Weiss et al., 2015; Li et al., 2014b). We believe our method is similar to these simplified version as a semi-supervised framework with two different RST parsers. adopt a simple pre-training and fine-tuning strategy, which is inspired by the NMT research (Sennrich et al., 2016), to train a student RST parser. Since early statistical RST parsing methods relied on handcrafted features, i.e., sentence-level features obtained from parse trees and documentlevel features, they require complete documents with complete sentences for their feature extraction. On the other hand, re"
2021.naacl-main.127,C18-1047,0,0.215639,"le silver data from an unlabeled corpus by using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser. 1 Introduction (Wang et al., 2017b; Yu et al., 2018; Kobayashi et al., 2020; Lin et al., 2019; Zhang et al., 2020), which require a high-quality annotated corpus of sufficient size. Generally, they train the following three components of the RST parsing: (1) structure prediction by splitting a text span consisting of contiguous EDUs into two smaller ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, i"
2021.naacl-main.127,2020.acl-main.569,0,0.0432201,"-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser. 1 Introduction (Wang et al., 2017b; Yu et al., 2018; Kobayashi et al., 2020; Lin et al., 2019; Zhang et al., 2020), which require a high-quality annotated corpus of sufficient size. Generally, they train the following three components of the RST parsing: (1) structure prediction by splitting a text span consisting of contiguous EDUs into two smaller ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for details). However, it is costly to annotate RST trees for a huge collection of docu"
2021.naacl-main.127,stede-neumann-2014-potsdam,0,0.0608475,"Missing"
2021.naacl-main.127,P17-2041,0,0.347521,"We create large-scale silver data from an unlabeled corpus by using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser. 1 Introduction (Wang et al., 2017b; Yu et al., 2018; Kobayashi et al., 2020; Lin et al., 2019; Zhang et al., 2020), which require a high-quality annotated corpus of sufficient size. Generally, they train the following three components of the RST parsing: (1) structure prediction by splitting a text span consisting of contiguous EDUs into two smaller ones or merging two adjacent spans into a larger one, (2) nuclearity status prediction for two adjacent spans by solving a 3-class classification problem, and (3) relation label prediction for two adjacent spans by solving an 18-class classification problem (see Section 3.3 for de"
2021.socialnlp-1.3,Q17-1010,0,0.0091802,"Missing"
2021.socialnlp-1.3,D18-1449,1,0.874029,"Missing"
2021.socialnlp-1.3,P19-1250,1,0.612232,"i) user feedback will be biased by where comments appear in a comment thread (also known as “position bias” (Craswell et al., 2008)). A typical example for (i) can be seen in political comments, where the “goodness” of the comment tends to be decided on the basis of the political views of the majority of the users rather than on its quality. A typical example of (ii) can be illustrated by a case where earlier comments tend to receive more feedback since they are displayed at the top of the page, which implies later comments will be ignored irrespective of their quality. To resolve this issue, Fujita et al. (2019) introduced a metric representing a comment’s constructiveness (see Section 2 for details), which has also been studied in argument analysis (Kolhatkar and Taboada, 2017a; Napoles et al., 2017a). Interestingly, they found empirical evidence that the constructiveness has no correlation with the user feedback, which has been commonly used for ranking comments. This implies that we need to consider the constructiveness rather than the user feedback to avoid unfavorable situations (i) and (ii) in real services. In this paper, we take their study one step further towards practical application. Spec"
2021.socialnlp-1.3,W17-3002,0,0.337997,"n be seen in political comments, where the “goodness” of the comment tends to be decided on the basis of the political views of the majority of the users rather than on its quality. A typical example of (ii) can be illustrated by a case where earlier comments tend to receive more feedback since they are displayed at the top of the page, which implies later comments will be ignored irrespective of their quality. To resolve this issue, Fujita et al. (2019) introduced a metric representing a comment’s constructiveness (see Section 2 for details), which has also been studied in argument analysis (Kolhatkar and Taboada, 2017a; Napoles et al., 2017a). Interestingly, they found empirical evidence that the constructiveness has no correlation with the user feedback, which has been commonly used for ranking comments. This implies that we need to consider the constructiveness rather than the user feedback to avoid unfavorable situations (i) and (ii) in real services. In this paper, we take their study one step further towards practical application. Specifically, in collaboration with Yahoo! JAPAN News, we report a case study of deploying a model that ranks constructive comments in a commercial service. The Introduction"
2021.socialnlp-1.3,W17-4218,0,0.324695,"n be seen in political comments, where the “goodness” of the comment tends to be decided on the basis of the political views of the majority of the users rather than on its quality. A typical example of (ii) can be illustrated by a case where earlier comments tend to receive more feedback since they are displayed at the top of the page, which implies later comments will be ignored irrespective of their quality. To resolve this issue, Fujita et al. (2019) introduced a metric representing a comment’s constructiveness (see Section 2 for details), which has also been studied in argument analysis (Kolhatkar and Taboada, 2017a; Napoles et al., 2017a). Interestingly, they found empirical evidence that the constructiveness has no correlation with the user feedback, which has been commonly used for ranking comments. This implies that we need to consider the constructiveness rather than the user feedback to avoid unfavorable situations (i) and (ii) in real services. In this paper, we take their study one step further towards practical application. Specifically, in collaboration with Yahoo! JAPAN News, we report a case study of deploying a model that ranks constructive comments in a commercial service. The Introduction"
2021.socialnlp-1.3,E17-2068,0,0.0087148,"etween an article and its comment, where this setting is a kind of stacking ensemble. Model: The model was an L2-regularized L2loss linear rankSVM model that was implemented as an instance of the well-known SVM tool LIBLINEAR (ver. 2.1.1) (Lin, 2020). The cost parameter C was determined from {2−13 , . . . , 21 } on the basis of the performance on the validation set. Features: The features consisted of two factors. The first was the expected C-score, which was determined by first computing the probabilities of C-scores (considered as classes) using the opensource library fastText (ver. 0.2.0) (Joulin et al., 2017; Facebook, 2020)2 with word embeddings trained on news articles and then calculating their expected value. The second feature was the Euclidean distance between the comment and title vectors, each of which consisted of the frequencies of words. • Model-14: The model with the highest NDCGL. It is a gradient boosting model (pointwise learning) with features based on maximal substrings and words. Model: The model was based on LightGBM (ver. 2.2.1) (Microsoft, 2020; Ke et al., 2017), a tree-based gradient boosting framework. The parameters were hand-tuned with a tuning guide (LightGBM Doc., 2020)"
2021.socialnlp-1.3,D18-2012,0,0.0201552,"Missing"
2021.socialnlp-1.3,W04-3230,0,0.0603111,"Missing"
2021.socialnlp-1.3,P16-3007,0,0.0252811,"rmation retrieval task (Kato and Liu, 2019). Their purpose to find good models for a specific task is almost the same as ours, and the main difference (ignoring the task) is that the competition in our work was conducted within a company. As this kind of work towards a commercial service is rarely released in the form of an academic paper, we expect that our findings will become valuable knowledge for practitioners in this field. Related Work Constructiveness: Analyzing the comments on online news services or discussion forums has been extensively studied (Wanas et al., 2008; Ma et al., 2012; Llewellyn et al., 2016; Shi and Lam, 2018). In this line of research, many studies have focused on ranking comments (Hsu et al., 2009; Das Sarma et al., 2010; Brand and Van Der Merwe, 2014; Wei et al., 2016). However, the prior approaches have been based on user feedback, which is completely different from constructiveness. Constructiveness has been introduced in argument analysis frameworks (Napoles et al., 2017a,b; Kolhatkar and Taboada, 2017a,b; Kolhatkar et al., 2020). The purpose of these studies was to classify constructive comments, whereas Fujita et al. (2019) recently expanded their tasks to a ranking one."
2021.socialnlp-1.3,P16-2032,0,0.128498,"iences, and simplified explanations of the article. There is a limit, however, on the number of comments that can be displayed on a page, and as users typically do not have the time or inclination to read through all the comments, ideally they should be ranked in some way. Prioritizing the comments for display is directly linked to user satisfaction, so improving this ranking is an important issue for such services. There have already been multiple studies on comment ranking in online news services and discussion forums (Hsu et al., 2009; Das Sarma et al., 2010; Brand and Van Der Merwe, 2014; Wei et al., 2016). All of these studies have utilized user feedback (e.g., “Like”-button clicks in Figure 1) as their ranking metrics. Although such user feedback is ∗ 1 Equal contribution. https://news.yahoo.co.jp/ 24 Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media, pages 24–35 Online Workshop, June 10, 2021. ©2021 Association for Computational Linguistics Precondition • Related to article and not libelous Main conditions • Intended to stimulate discussions • Objective and supported by fact • New idea, solution, or insight • User’s unique experience characterist"
2021.socialnlp-1.3,W17-0802,0,0.121146,"s, where the “goodness” of the comment tends to be decided on the basis of the political views of the majority of the users rather than on its quality. A typical example of (ii) can be illustrated by a case where earlier comments tend to receive more feedback since they are displayed at the top of the page, which implies later comments will be ignored irrespective of their quality. To resolve this issue, Fujita et al. (2019) introduced a metric representing a comment’s constructiveness (see Section 2 for details), which has also been studied in argument analysis (Kolhatkar and Taboada, 2017a; Napoles et al., 2017a). Interestingly, they found empirical evidence that the constructiveness has no correlation with the user feedback, which has been commonly used for ranking comments. This implies that we need to consider the constructiveness rather than the user feedback to avoid unfavorable situations (i) and (ii) in real services. In this paper, we take their study one step further towards practical application. Specifically, in collaboration with Yahoo! JAPAN News, we report a case study of deploying a model that ranks constructive comments in a commercial service. The Introduction In online news service"
C00-2160,P99-1072,0,0.0358571,"Missing"
C04-1023,W03-1602,0,0.0334088,"it is very important to compose a title that will stimulate their interest in the technology. However, technical specialists are not necessarily good at composing appealing titles, because it isn’t clear what sort of titles will stimulate the interest of lay readers in the technology. In the ﬁeld of NLP and linguistics, there are few researches which help the specialists compose appealing titles for lay readers. Several researches have been reported on title generation (Jin and Hauptmann, 2000) (Berger and Mittal, 2000) and readability of texts (Minel et al., 1997) (Hartley and Sydes, 1997) (Inui et al., 2003). However, the researches on title generation focus on generating a very compact summary of the document rather than composing an appealing title. The previous researches on readability mainly see it as comprehensibility rather than interestingness. In this regard, our previous study (Senda and Sinohara, 2002) clariﬁed what sort of content and wording in titles are eﬀective in stimulating lay readers’ interest in the technology by an analysis of a parallel corpus of Japanese technical paper titles and Japanese newspapers headlines. The study categorized the eﬀective content and wording of the"
C04-1023,W97-0705,0,0.0120145,"logy intended for readers including laypersons, it is very important to compose a title that will stimulate their interest in the technology. However, technical specialists are not necessarily good at composing appealing titles, because it isn’t clear what sort of titles will stimulate the interest of lay readers in the technology. In the ﬁeld of NLP and linguistics, there are few researches which help the specialists compose appealing titles for lay readers. Several researches have been reported on title generation (Jin and Hauptmann, 2000) (Berger and Mittal, 2000) and readability of texts (Minel et al., 1997) (Hartley and Sydes, 1997) (Inui et al., 2003). However, the researches on title generation focus on generating a very compact summary of the document rather than composing an appealing title. The previous researches on readability mainly see it as comprehensibility rather than interestingness. In this regard, our previous study (Senda and Sinohara, 2002) clariﬁed what sort of content and wording in titles are eﬀective in stimulating lay readers’ interest in the technology by an analysis of a parallel corpus of Japanese technical paper titles and Japanese newspapers headlines. The study catego"
C04-1023,C92-4203,0,0.0156913,"database is (b) in Figure 1. Figure 3 shows an example of the database window. The upper pane in the window shows the phrases in titles and the headlines describing related technologies. The lower part of the window presents search boxes for menu-based retrieval and keyword-based retrieval. The pulldown menu options are organized by technical ﬁelds. Users can search the clues for revising the title of the draft version from these search boxes as well as by scrolling through the window. The role of this parallel corpus is basically the same as the one of “Example-Based Translation Aid (EBTA)” (Sato, 1992) (Furugori and Takeda, 1993) (Kumano and Tanaka, 1998). EBTA researches have shown that parallel translation examples (pairs of source text and its translation equivalent) are very helpful for translators to translate the similar text because parallel translation examples give them useful clues for translation. From the viewpoint that paper titles and newspapers headlines for related technologies are also regarded as parallel translation examples (pairs of text for specialists and its translation equivalent for lay readers) describing newly-developed technologies, the our database is expected"
C04-1023,C02-1133,1,0.736409,"Missing"
C04-1023,H01-1011,0,\N,Missing
C04-1077,C00-1072,0,0.108816,"Missing"
C04-1077,W03-0507,1,0.820221,"Missing"
C04-1077,P03-1048,0,0.0185685,"at we need other important techniques such as those for maintaining the consistency of words and phrases that refer to the same object, and for making the results more readable; however, they are not included here. fixed number, the situation was the same as TSC2. At DUC 2002, extracts (important sentences) were used, and this allowed us to evaluate sentence extraction. However, it is not possible to measure the effectiveness of redundant sentences reduction since the corpus was not annotated to show sentence with same content. In addition, this is the same even if we use the SummBank corpus (Radev et al., 2003). In any case, because many of the current summarization systems for multiple documents are based on sentence extraction, we believe these corpora to be unsuitable as sets of documents for evaluation. On this basis, in TSC3, we assumed that the process of multiple document summarization consists of the following three steps, and we produce a corpus for the evaluation of the system at each of the three steps4 . Step 1 Extract important sentences from a given set of documents Step 2 Minimize redundant sentences from the result of Step 1 Step 3 Rewrite the result of Step 2 to reduce the size of t"
C04-1077,W97-0710,0,0.0537484,"set of sentences that human annotators judge as being important in a document set (Fukusima and Okumura, 2001; Zechner, 1996; Paice, 1990). 4 This is based on general ideas of a summarization system and is not intended to impose any conditions on a summarization system. (a) (b) (c) (d) Doc. x Mainichi articles abstract Doc. y Yomiuri articles Figure 1: An example of an abstract and its sources. 2. A set of sentences that are suitable as a source for producing an abstract, i.e., a set of sentences in the original documents that correspond to the sentences in the abstracts(Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing and McKeown, 1999). When we consider how summaries are produced, it seems more natural to identify important segments in the document set and then produce summaries by combining and rephrasing such information than to select important sentences and revise them as summaries. Therefore, we believe that second type of extract is superior and thus we prepared the extracts in that way. However, as stated in the previous section, with multiple document summarization, there may be more than one sentence with the same content, and thus we may have more than one set of sentences in"
C04-1077,C96-2166,0,0.206176,"tomatic evaluation using a scoring program. We adopted an intrinsic evaluation by human judges for step 3, which is currently under evaluation. We provide details of the extracts prepared for steps 1 and 2 and their evaluation measures in the following sections. We do not report the overall evaluation results for TSC3. 2.2 Data Preparation for Sentence Extraction We begin with guidelines for annotating important sentences (extracts). We think that there are two kinds of extract. 1. A set of sentences that human annotators judge as being important in a document set (Fukusima and Okumura, 2001; Zechner, 1996; Paice, 1990). 4 This is based on general ideas of a summarization system and is not intended to impose any conditions on a summarization system. (a) (b) (c) (d) Doc. x Mainichi articles abstract Doc. y Yomiuri articles Figure 1: An example of an abstract and its sources. 2. A set of sentences that are suitable as a source for producing an abstract, i.e., a set of sentences in the original documents that correspond to the sentences in the abstracts(Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing and McKeown, 1999). When we consider how summaries are produced, it seems more natu"
C12-1086,W04-3230,0,0.125963,"Missing"
C12-1086,I08-7018,0,0.0757932,"Missing"
C18-1274,D14-1179,0,0.0474408,"Missing"
C18-1274,P17-1174,0,0.0276391,". , hTx ) from the word embedding vectors of source-language words x = (x1 , x2 , . . . , xj , . . . , xTx ) and the linguistic feature’s embedding vectors as follows: hj = fenc (hj−1 , (Eword (xj )[ ; |F | k=1 Ek (xjk )])), (9) where Eword is an embedding layer for source-language words, Ek is an embedding layer for the k-th type of linguistic features, and |F |is the number of types of linguistic features (i.e., |F |= 4). Note that the model has not used NE tags as a linguistic feature, and the incorporation of NE tags into NMT is still under exploration. 2.3 NMT Based on Chunk/Phrase Units Ishiwatari et al. (2017) have improved the attention-based NMT by designing chunk-based decoders, each of which models global dependencies by a chunk-level decoder and local word dependencies by a word-level decoder. In their decoders, the chunk-level decoder first generates a chunk representation. Then, the word-level decoder predicts each target word from the chunk representation. Wang et al. (2017) have improved attention-based NMT by integrating a phrase memory, which stores target phrases provided by a statistical machine translation (SMT) (Pal et al., 2010) model, to perform a phrase-by-phrase translation rathe"
C18-1274,2005.mtsummit-papers.11,0,0.0683783,"g), and English-to-Romanian (En-Ro) translation tasks to confirm the effectiveness of the proposed model. 3244 Table 1: Statistics on Experimental Data (# of parallel sentences) Training Data Development Data Test Data En-Jp 1,320,591 1,768 1,802 En-Bg 363,112 3,000 3,000 En-Ro 357,247 1,972 3,000 En-Jp En-Bg En-Ro Table 2: Vocabulary Size Source Language Target Language 78,591 57,771 27,872 30,000 27,651 30,000 4.1 Experimental Data We evaluated the En-Jp translation performance on the ASPEC, which is used in WAT 20175 , and the En-Bg and En-Ro translation performance on the Europarl corpus (Koehn, 2005). The English and Japanese sentences are tokenized by spaCy6 and KyTea (Neubig et al., 2011), respectively. The Bulgarian and Romanian sentences are tokenized by byte-pair encoding (Sennrich et al., 2016) implemented in sentencepiece7 , where we set the vocabulary size to 30,000. The words that appeared less than five times in the En-Jp training data and those less than twice in the English side of the En-Bg and En-Ro training data were replaced with the special symbol ⟨UNK⟩. In the training, all words were lowercased by lowercase.perl8 , and long sentences with over 50 words were filtered out"
C18-1274,D15-1166,0,0.0609428,"roparl corpus. The evaluation results show that the proposed model achieves up to 3.11 point improvement in BLEU. 1 Introduction Neural machine translation (NMT) models based on the encoder-decoder model, also known as the sequence-to-sequence model (Sutskever et al., 2014), have successfully shown their quality translation. Consequently, various NMT models are studied in the field of machine translation. To date, the most successful model is the bi-directional multi-layered encoder-decoder model with long short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and an attention mechanism (Luong et al., 2015; Bahdanau et al., 2015), also known as attention-based NMT. LSTM and the attention mechanism are introduced to mitigate the difficulty in handling long sentences in the encoder-decoder model. The conventional attention-based NMT model is known to achieve high translation accuracy in bilingual evaluation understudy (BLEU). However, this model encounters two general problems: (i) it tends to have difficulty in translating words with multiple meanings because their translations have high ambiguity, and (ii) translation of compound words seems difficult because the encoder receives only a word, a"
C18-1274,P11-2093,0,0.017711,"f the proposed model. 3244 Table 1: Statistics on Experimental Data (# of parallel sentences) Training Data Development Data Test Data En-Jp 1,320,591 1,768 1,802 En-Bg 363,112 3,000 3,000 En-Ro 357,247 1,972 3,000 En-Jp En-Bg En-Ro Table 2: Vocabulary Size Source Language Target Language 78,591 57,771 27,872 30,000 27,651 30,000 4.1 Experimental Data We evaluated the En-Jp translation performance on the ASPEC, which is used in WAT 20175 , and the En-Bg and En-Ro translation performance on the Europarl corpus (Koehn, 2005). The English and Japanese sentences are tokenized by spaCy6 and KyTea (Neubig et al., 2011), respectively. The Bulgarian and Romanian sentences are tokenized by byte-pair encoding (Sennrich et al., 2016) implemented in sentencepiece7 , where we set the vocabulary size to 30,000. The words that appeared less than five times in the En-Jp training data and those less than twice in the English side of the En-Bg and En-Ro training data were replaced with the special symbol ⟨UNK⟩. In the training, all words were lowercased by lowercase.perl8 , and long sentences with over 50 words were filtered out. In the En-Jp task, we used the development and test data employed in WAT 2017. In the En-R"
C18-1274,W10-3707,0,0.0308894,"xploration. 2.3 NMT Based on Chunk/Phrase Units Ishiwatari et al. (2017) have improved the attention-based NMT by designing chunk-based decoders, each of which models global dependencies by a chunk-level decoder and local word dependencies by a word-level decoder. In their decoders, the chunk-level decoder first generates a chunk representation. Then, the word-level decoder predicts each target word from the chunk representation. Wang et al. (2017) have improved attention-based NMT by integrating a phrase memory, which stores target phrases provided by a statistical machine translation (SMT) (Pal et al., 2010) model, to perform a phrase-by-phrase translation rather than a word-by-word translation. Their model dynamically selects a word or phrase to be output at each decoding step. Meanwhile Ishiwatari et al. (2017) and Wang et al. (2017) have incorporated chunks identified by a chunker and a SMT model, respectively, our work focuses on chunk information of NE tags. Note that our proposed model can incorporate general chunk information, such as chunks found by a chunker. We will leave this aspect for future work. 3242 3 NMT Incorporating NE Tags In this section, we propose a new NMT model, incorpora"
C18-1274,P02-1040,0,0.103601,"Missing"
C18-1274,W16-2209,0,0.461987,"vector si is calculated as the weighted average, where a weight is ai , over all the encoder’s hidden states: exp(h¯i · hj ) , ai (j) = ∑Tx exp(h¯i · hj ′ ) j ′ =1 si = Tx ∑ ai (j)hj , (6) (7) j=1 where Tx is the length of the source-language sentence x, and · denotes the inner product. In the decoder, the probability distribution of the output word is calculated on the basis of the context vector si in addition to the decoder’s hidden state h¯i : p(yi |y1:i−1 , c) = sof tmax(proj([h¯i ; si ])), (8) where [;] denotes the concatenation of the two vectors. 2.2 Linguistic Input Features for NMT Sennrich and Haddow (2016) have improved the attention-based NMT model by using the linguistic features of source-language sentences. In particular, the encoder receives the lemma, subword tags, POS tags, and dependency labels of source-language sentences in addition to source-language words. A lemma is the original form of the word. Subword tags express prefix, stem, and suffix. A POS tag is part-of-speech information of a word, such as a noun or a verb. Moreover, a dependency label indicates a syntactic relation between words, such as a head and dependents. The encoder converts each of the linguistic features into it"
C18-1274,P16-1162,0,0.11012,"velopment Data Test Data En-Jp 1,320,591 1,768 1,802 En-Bg 363,112 3,000 3,000 En-Ro 357,247 1,972 3,000 En-Jp En-Bg En-Ro Table 2: Vocabulary Size Source Language Target Language 78,591 57,771 27,872 30,000 27,651 30,000 4.1 Experimental Data We evaluated the En-Jp translation performance on the ASPEC, which is used in WAT 20175 , and the En-Bg and En-Ro translation performance on the Europarl corpus (Koehn, 2005). The English and Japanese sentences are tokenized by spaCy6 and KyTea (Neubig et al., 2011), respectively. The Bulgarian and Romanian sentences are tokenized by byte-pair encoding (Sennrich et al., 2016) implemented in sentencepiece7 , where we set the vocabulary size to 30,000. The words that appeared less than five times in the En-Jp training data and those less than twice in the English side of the En-Bg and En-Ro training data were replaced with the special symbol ⟨UNK⟩. In the training, all words were lowercased by lowercase.perl8 , and long sentences with over 50 words were filtered out. In the En-Jp task, we used the development and test data employed in WAT 2017. In the En-Ro task, we used the newsdev-2016 as the development data and randomly sampled 3,000 parallel sentences from the"
C18-1274,D17-1149,0,0.0221206,"s of linguistic features (i.e., |F |= 4). Note that the model has not used NE tags as a linguistic feature, and the incorporation of NE tags into NMT is still under exploration. 2.3 NMT Based on Chunk/Phrase Units Ishiwatari et al. (2017) have improved the attention-based NMT by designing chunk-based decoders, each of which models global dependencies by a chunk-level decoder and local word dependencies by a word-level decoder. In their decoders, the chunk-level decoder first generates a chunk representation. Then, the word-level decoder predicts each target word from the chunk representation. Wang et al. (2017) have improved attention-based NMT by integrating a phrase memory, which stores target phrases provided by a statistical machine translation (SMT) (Pal et al., 2010) model, to perform a phrase-by-phrase translation rather than a word-by-word translation. Their model dynamically selects a word or phrase to be output at each decoding step. Meanwhile Ishiwatari et al. (2017) and Wang et al. (2017) have incorporated chunks identified by a chunker and a SMT model, respectively, our work focuses on chunk information of NE tags. Note that our proposed model can incorporate general chunk information,"
C92-1062,P81-1022,0,0.0407128,"Missing"
C92-1062,C90-2004,0,0.0180354,"tractive in t h a t it can be simply added on top of logic grammars that are directly available in Prolog. However, the main drawback in using top down recursive descent parsing methods is that it might result in an infinite loop for left recursive grammars. The recent version using Static Discontinuity Grammars(SDG)[5] augmented with Abramson&apos;s metarules can solve this problem by adding loop control as a constraint on parsing. According to the comparison tests reported in [2], the approach appears to be considerably faster than Popowich&apos;s FIGG. Another approach of Bottom-up filtering strategy[4] attempts to reduce the nondeterminism in parsing. Different ID rules are constrained to have at most one category in common and the knowledge of the leftmost constituent is used for phrase level initialization. As an investigation of our approach, we have implemented a small parser, called GHW, using SlCStus prolog on a Sun 3-60 workstation. To reduce spurious parses, the parser adopts the technique of the left-corner parsing method to detect the edges that canACTESDECOLlNG-92.NAN&apos;I~.23-28hofrr 1992 406 not start a constituent in the bottom-up rules invoking stage. The technique is similar to"
C92-1062,J85-4001,0,\N,Missing
C94-2121,J86-3001,0,0.0398703,"Consider for example the live lexieal chains in the imaginary text that consists of 24 sentences in Figure 2. In this text, the b o u n d a r y strength can be computed as follows: w ( a , 4 ) = 1,,.,(7,s) 1,w(9,10) ~- 1,w(13,14) -- 3 , . . . . 24) ( 4 - 13) (14 - is) ( 8 - 9) (14 - 18) LexiThe second importance of lexic~d chains is that they provide a clue for the deternfination of segment boundaries. (Jertain spans of sentences in a text form selnantic units and are usually called segments. It is crucial to identify the segment boundaries as a first step to construct the structure of a text[2]. 4.1 ( i - 2 123456789012345678901234 start~end However, our salience of lexical chains is, of course, rather naive and must be refined by using other kinds of inibrmation, such as Japanese topicM marker &apos;wa&apos;. text i The Evahmtion We, try to segnient the texts ill section 3.2 and apply the above measure to the lexical chains that were tbluned. We pick out three texts(No.3,4,5), which are fi:om the exam ques tions of the Japanese language, that ask us to partiglon the texts into a given n u m b e r of segments. The system&apos;s performmwe is judged by the com. p~rison with segment boundaries marke"
C94-2121,P93-1020,0,0.200837,"t has begun[l 0]. Taking into account tiffs correspondence of [exieal chain boundaries to segment boundaries, we measure the plausibilit;y el each point; in the text; as ~ segment hotmdary: tbr each point between sentences n an(l &apos;n k I (where it ranges fl&apos;om 1 to the m|nlt)er el&apos; sentences in the text minus 1), compute the stun of the numl)er of lexical chains that en(l at the sentence ?z and the n u m b e r of lexical chains that begin at the sentence n + 1. We call this naive measure of a degree of agreement of the start and end points of lexicM chains w ( n , n + l) boundary strength like [14]. The points ill the text are selected in the order of b o u n d a r y strength as candidates of segment boundaries. Consider for example the live lexieal chains in the imaginary text that consists of 24 sentences in Figure 2. In this text, the b o u n d a r y strength can be computed as follows: w ( a , 4 ) = 1,,.,(7,s) 1,w(9,10) ~- 1,w(13,14) -- 3 , . . . . 24) ( 4 - 13) (14 - is) ( 8 - 9) (14 - 18) LexiThe second importance of lexic~d chains is that they provide a clue for the deternfination of segment boundaries. (Jertain spans of sentences in a text form selnantic units and are usually ca"
C94-2121,C92-2070,0,0.0157481,"five types of thesaural relations. The word sense of a word can be determined in its context. For example, in the context {universe, star, universe, star, galaxy, sun}, the word &apos;earth&apos; has a &apos;planet&apos; sense, not a &apos;ground&apos; one. As clear from this example, lexical chains (&apos;an be used as a contextual aid to resolve word sense ambiguity[10]. In the generation process of lexical chains, by choosing the lexical chain that the current word is added to, its word sense is determined. Thus, we regard word sense disambiguation as selecting the most likely category number of the thesaurus, as similar to [16]. l&apos;;arlier we proposed incremental disambiguation method that uses intrasentential information, such as selectional restrictions and case frames[l 2]. In the next section, we describe incremental disambiguation method that uses lexical chains as intersentential(contextual) information. 3 Generation of Lexical Chains In the last section, we showed that lexical chains carl play a role of local context, t]owever, multiple lexical chains might cooccur in portions of a text and they might vary in their plausibility as local context. For this reason, for lexical chains to function truly as local co"
C94-2121,H92-1089,0,0.0194352,"points of the more import a n t lexical chains can get the more b o u n d a r y strength. This refinement of the measure is in the process and yields a certain extent of improvement of the system&apos;s performance. 760 Moreover, this ewduation method is not necessarily adequate since partitioning into a larger n u m b e r of smaller segments might be possible and be necessary for the given texts. And so we will have to consider the evaluation method that the agreement with h m n a n subjects is tested in future. Ilowever, since h u m a n subjects do not always agree with each other on segmentation[6, 4, 14], our evaluation method using the texts in the questions with model answers is considered to be a good simplification. Several other methods to text segmentation have been proposed. Kozima[7] and Youmans[17] proposed statistical measures(they are named LCP and VMP respectively), which indicate the plausibility of text points as a segment boundary. Their hills or valleys tend to indicate segment boundaries. However, they only showed the correlation between their measures and segment boundaries by their intuil,ive analysis of few sample texts, and so we cannot compare our system&apos;s and their perf"
C94-2121,P93-1041,0,0.242431,"performance. 760 Moreover, this ewduation method is not necessarily adequate since partitioning into a larger n u m b e r of smaller segments might be possible and be necessary for the given texts. And so we will have to consider the evaluation method that the agreement with h m n a n subjects is tested in future. Ilowever, since h u m a n subjects do not always agree with each other on segmentation[6, 4, 14], our evaluation method using the texts in the questions with model answers is considered to be a good simplification. Several other methods to text segmentation have been proposed. Kozima[7] and Youmans[17] proposed statistical measures(they are named LCP and VMP respectively), which indicate the plausibility of text points as a segment boundary. Their hills or valleys tend to indicate segment boundaries. However, they only showed the correlation between their measures and segment boundaries by their intuil,ive analysis of few sample texts, and so we cannot compare our system&apos;s and their performance precisely. ltearst[5] independently proposes a similar measure for text segmentation and evaluates the performance o[ her method with precision and recall rates. However, her segmenta"
C94-2121,J92-1001,0,0.019999,"o[&apos; each point in the text as a segment boundary by computing a degree of agreement of the sta.rt and end points of lexical chains. 755 Morris and Itirst[10] pointed out the above two importance of lexical cohesion for discourse analysis and presented a way of computing lexical chains by using Roger&apos;s International Thesaurus[15]. IIowever, in spite of their mention to the importance, they did not present the way of word sense disambiguation based on lexical cohesion and they only showed the correspondences between lexical chains and segment boundaries by their intuitive analysis. McRoy&apos;s work[8] can be considered as the one that uses the information of lexical cohesion for word sense disambiguation, but her method does not; take into account the necessity to arrange lexical chains dynamically. Moreover, her word sense disambignation method based on lexical cohesion is not evaluated fully. In section two we outline what lexical cohesion is. In section three we explain the way of incremental generation o f lexical chains in tandem with word sense disambiguation and describe the result of the evaluation of our disambiguation method. In section four we explain the measure of the plausibi"
C94-2121,J91-1002,0,0.960844,"ed here. 2Reference by flfll NPs, substitution mtd lcxical cohe-. sion in Ilalllday and Hasan&apos;s classillcation a.re included here. cohesion is far easier to idenlAfy than reference b e c a u s e 1)oth words in lexical cohesion relation a p p e a r in a text while one word in reference relation is a pr<mom, or elided and has less information to infer the other word in the relation automatically. Based on this observation, we use lexical cohesion as a linguistic device for discourse analysis. We call a sequence of words which are in lexieal cohesion relation with each other a Icxical chain like [10]. l,exical chains tend to indicate portions of a text; that form a semantic uttit. And so vari.ous lexical chains tend to appear in a text corre. spou(ling to the change of the topic. Therefore, I. lexical chains provide a local context to aid in the resolution of word sense ambiguity; 2. lexical <&apos;hains provide a <&apos;lue for the determination of segnlent boundaries of the text[10]. ]n this paper, we first describe how word sense ambiguity can t)e resolved with the aid of lexical cohesion. During the process of generating lexi<&apos;al chains incrementally, they are recorded in a register in the orde"
C94-2121,P94-1002,0,\N,Missing
C96-2147,J95-2003,0,0.525061,"Missing"
C96-2147,P86-1031,0,0.719389,"Missing"
C96-2147,A92-1028,0,0.0895747,"Missing"
C96-2147,J94-2003,0,\N,Missing
C96-2147,C94-2188,0,\N,Missing
C96-2147,C94-2107,0,\N,Missing
C96-2147,J94-2006,0,\N,Missing
C96-2147,P87-1022,0,\N,Missing
C96-2147,J86-3001,0,\N,Missing
C96-2147,P93-1009,0,\N,Missing
C98-2140,A94-1010,0,0.0133789,"ork, we now plan to calculate the weights for a subset of the texts by clustering the training texts. Since there may be some differences among real texts which reflect the differences of their author, their style, their genre, etc., we think that clustering a set of the training texts and caiculating the weights for each cluster, rather than calculating the weights for the entire set of texts, might improve the accuracy. In the area of speech recognition, to improve the accuracy of the language models, clustering the training data is considered to be a promising method for automatic training(Carter, 1994; Iyer et al., 1994). Carter presents a method for clustering the sentences in a training corpus automatically into some subcorpora on the criterion of entropy reduction and calculating separate language model parameters for each cluster. He asserts that this kind of clustering offers a way to improve the performance of a model significantly. Acknowledgments The authors would like to express our gratitude to Kadokawa publisher for allowing us to use their thesaurus, and Dr.Shigenobu Aoki of G u n m a Univ. and Dr.Teruo Matsuzawa of J A I S T for their suggestions of statistical analysis, and D"
C98-2140,J87-1002,0,0.0192359,"e the accuracy. In this paper, we describe a method for identifying segment boundaries of a Japanese text with the aid of multiple surface linguistic cues, such as conjunctives, ellipsis, types of sentences, and lexical cohesion. There are a variety of methods for combining multiple knowledge sources (linguistic cues)(McRoy, 1992). Among them, a weighted sum of the scores for all cues that reflects their contribution to identifying the correct segment boundaries is often used as the 881 overall measure to rank the possible segment boundaries. In the past researches (Kurohashi and Nagao, 1994; Cohen, 1987), the weights for each cue tend to be determined by intuition or trial and error. Since determining weights by hand is a labor-intensive task and the weights do not always to achieve optimal or even near-optimal performance(Rayner et al., 1994), we think it is better to determine the weights automatically in order to both avoid the need for expert hand tuning and achieve performance that is at least locally optimal. We begin by assuming the existence of training texts with the correct segment boundaries and use the method of multiple regression analysis for automatically training the weights."
C98-2140,P92-1032,0,0.021952,"r test. Changing the group for the test, we evaluate the performance by the cross validation(Weiss and Kulikowski, 1991). 5. Use only selected cues by applying the stepwise method. As mentioned in section 4, we use the stepwise method for selecting useful cues for training sets. The condition is the same as for the case 4 except for the cue selection. 6. Answer from five human subjects. By this experiment, we try to clarify the upper bound of the performance of the text segmentation task, which can be considered to indicate the degree of the difficulty of the task(Passonneau and Litman, 1993; Gale et al., 1992). Figure 1,2 and table 1 show the results of the experiments. Two figures show the system's mean performance of 14 texts. Table 1 shows the 5 subjects' mean performance of 14 texts (experiment 6). We think table 1 shows the upper bound of the performance of the text segmentation task. We also calculate the lower bound of the performance of the task(&quot;lowerbound&quot; in figure 2). It can be calculated by considering the case where the system selects boundary candidates at random. In the case, the precision equals to the mean probability that each candidate will be a correct boundary. The recall is e"
C98-2140,P93-1041,0,0.479102,"tifying segment boundaries in a text is considered as a first step to construct the discourse structure(Grosz and Sidner, 1986). The use of surface linguistic cues in a text for identification of segment boundaries has been extensively researched, since it is impractical to assume the use of world knowledge for discourse analysis of real texts. Among a variety of surface cues, lexical cohesion(Halliday and Hasan, 1976), the surface relationship among words that are semantically similar, has recently received much attention and has been widely used for text segmentation(Morris and Hirst, 1991; Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994). Okumura and Honda (Okumura and Honda, 1994) found that the information oflexical cohesion is not enough and incorporation of other surface information may improve the accuracy. In this paper, we describe a method for identifying segment boundaries of a Japanese text with the aid of multiple surface linguistic cues, such as conjunctives, ellipsis, types of sentences, and lexical cohesion. There are a variety of methods for combining multiple knowledge sources (linguistic cues)(McRoy, 1992). Among them, a weighted sum of the scores for all cues that refl"
C98-2140,P95-1015,0,0.616038,"Missing"
C98-2140,J92-1001,0,0.0245714,"s been widely used for text segmentation(Morris and Hirst, 1991; Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994). Okumura and Honda (Okumura and Honda, 1994) found that the information oflexical cohesion is not enough and incorporation of other surface information may improve the accuracy. In this paper, we describe a method for identifying segment boundaries of a Japanese text with the aid of multiple surface linguistic cues, such as conjunctives, ellipsis, types of sentences, and lexical cohesion. There are a variety of methods for combining multiple knowledge sources (linguistic cues)(McRoy, 1992). Among them, a weighted sum of the scores for all cues that reflects their contribution to identifying the correct segment boundaries is often used as the 881 overall measure to rank the possible segment boundaries. In the past researches (Kurohashi and Nagao, 1994; Cohen, 1987), the weights for each cue tend to be determined by intuition or trial and error. Since determining weights by hand is a labor-intensive task and the weights do not always to achieve optimal or even near-optimal performance(Rayner et al., 1994), we think it is better to determine the weights automatically in order to b"
C98-2140,C94-2121,1,0.724498,"n a text is considered as a first step to construct the discourse structure(Grosz and Sidner, 1986). The use of surface linguistic cues in a text for identification of segment boundaries has been extensively researched, since it is impractical to assume the use of world knowledge for discourse analysis of real texts. Among a variety of surface cues, lexical cohesion(Halliday and Hasan, 1976), the surface relationship among words that are semantically similar, has recently received much attention and has been widely used for text segmentation(Morris and Hirst, 1991; Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994). Okumura and Honda (Okumura and Honda, 1994) found that the information oflexical cohesion is not enough and incorporation of other surface information may improve the accuracy. In this paper, we describe a method for identifying segment boundaries of a Japanese text with the aid of multiple surface linguistic cues, such as conjunctives, ellipsis, types of sentences, and lexical cohesion. There are a variety of methods for combining multiple knowledge sources (linguistic cues)(McRoy, 1992). Among them, a weighted sum of the scores for all cues that reflects their contribution to identifying t"
C98-2140,P93-1020,0,0.0781972,"ng and the remaining group for test. Changing the group for the test, we evaluate the performance by the cross validation(Weiss and Kulikowski, 1991). 5. Use only selected cues by applying the stepwise method. As mentioned in section 4, we use the stepwise method for selecting useful cues for training sets. The condition is the same as for the case 4 except for the cue selection. 6. Answer from five human subjects. By this experiment, we try to clarify the upper bound of the performance of the text segmentation task, which can be considered to indicate the degree of the difficulty of the task(Passonneau and Litman, 1993; Gale et al., 1992). Figure 1,2 and table 1 show the results of the experiments. Two figures show the system's mean performance of 14 texts. Table 1 shows the 5 subjects' mean performance of 14 texts (experiment 6). We think table 1 shows the upper bound of the performance of the text segmentation task. We also calculate the lower bound of the performance of the task(&quot;lowerbound&quot; in figure 2). It can be calculated by considering the case where the system selects boundary candidates at random. In the case, the precision equals to the mean probability that each candidate will be a correct bound"
C98-2140,H94-1040,0,0.0621782,"Missing"
C98-2140,C96-2164,0,0.044311,"Missing"
C98-2140,J91-1002,0,\N,Missing
C98-2140,C94-2183,0,\N,Missing
C98-2140,H94-1014,0,\N,Missing
C98-2140,P94-1002,0,\N,Missing
C98-2140,J86-3001,0,\N,Missing
D07-1063,W00-1303,0,0.440902,"s, “Dep(1), Dep(2), ..., Dep(m)”, by D, where Dep(i) = j means that bi modifies bj . Given the sequence B of chunks as an input, dependency analysis is defined as the problem of finding the sequence D of the dependency patterns that maximizes the conditional probability P (D |B). A number of the conventional methods assume that dependency probabilities are independent of each other and apQ proximate P (D |B) with m−1 i=1 P (Dep(i) |B). P (Dep(i) |B) is estimated using machine learning algorithms. For example, Haruno et al. (1999) used Decision Trees, Sekine (2000) used Maximum Entropy Models, Kudo and Matsumoto (2000) used Support Vector Machines. Another notable method is Cascaded Chunking Model by Kudo and Matsumoto (2002). In their model, a sentence is parsed by series of the following processes: whether or not the current chunk modifies the following chunk is estimated, and if it is so, the two chunks are merged together. Sassano (2004) parsed a sentence efficiently using a stack. The stack controls the modifier being analyzed. These conventional methods determine the modifiee of each chunk based on the likeliness of dependencies between two chunks (in terms of dependency tree, the likeliness of parent"
D07-1063,W02-2016,0,0.647416,"chunks as an input, dependency analysis is defined as the problem of finding the sequence D of the dependency patterns that maximizes the conditional probability P (D |B). A number of the conventional methods assume that dependency probabilities are independent of each other and apQ proximate P (D |B) with m−1 i=1 P (Dep(i) |B). P (Dep(i) |B) is estimated using machine learning algorithms. For example, Haruno et al. (1999) used Decision Trees, Sekine (2000) used Maximum Entropy Models, Kudo and Matsumoto (2000) used Support Vector Machines. Another notable method is Cascaded Chunking Model by Kudo and Matsumoto (2002). In their model, a sentence is parsed by series of the following processes: whether or not the current chunk modifies the following chunk is estimated, and if it is so, the two chunks are merged together. Sassano (2004) parsed a sentence efficiently using a stack. The stack controls the modifier being analyzed. These conventional methods determine the modifiee of each chunk based on the likeliness of dependencies between two chunks (in terms of dependency tree, the likeliness of parent-child relations between two nodes). The difference between the conventional methods and the proposed method"
D07-1063,J94-4001,0,0.475308,"Missing"
D07-1063,C04-1002,0,0.643831,"probabilities are independent of each other and apQ proximate P (D |B) with m−1 i=1 P (Dep(i) |B). P (Dep(i) |B) is estimated using machine learning algorithms. For example, Haruno et al. (1999) used Decision Trees, Sekine (2000) used Maximum Entropy Models, Kudo and Matsumoto (2000) used Support Vector Machines. Another notable method is Cascaded Chunking Model by Kudo and Matsumoto (2002). In their model, a sentence is parsed by series of the following processes: whether or not the current chunk modifies the following chunk is estimated, and if it is so, the two chunks are merged together. Sassano (2004) parsed a sentence efficiently using a stack. The stack controls the modifier being analyzed. These conventional methods determine the modifiee of each chunk based on the likeliness of dependencies between two chunks (in terms of dependency tree, the likeliness of parent-child relations between two nodes). The difference between the conventional methods and the proposed method is that the proposed method determines the modifiees based on the likeliness of ancestor-descendant relations in addition to parent-child relations, while the conventional methods tried to capture characteristics that ca"
D07-1063,C00-2110,0,0.281536,"by B, and a sequence of dependency patterns, “Dep(1), Dep(2), ..., Dep(m)”, by D, where Dep(i) = j means that bi modifies bj . Given the sequence B of chunks as an input, dependency analysis is defined as the problem of finding the sequence D of the dependency patterns that maximizes the conditional probability P (D |B). A number of the conventional methods assume that dependency probabilities are independent of each other and apQ proximate P (D |B) with m−1 i=1 P (Dep(i) |B). P (Dep(i) |B) is estimated using machine learning algorithms. For example, Haruno et al. (1999) used Decision Trees, Sekine (2000) used Maximum Entropy Models, Kudo and Matsumoto (2000) used Support Vector Machines. Another notable method is Cascaded Chunking Model by Kudo and Matsumoto (2002). In their model, a sentence is parsed by series of the following processes: whether or not the current chunk modifies the following chunk is estimated, and if it is so, the two chunks are merged together. Sassano (2004) parsed a sentence efficiently using a stack. The stack controls the modifier being analyzed. These conventional methods determine the modifiee of each chunk based on the likeliness of dependencies between two chunks"
D10-1081,W04-3236,0,\N,Missing
D10-1081,Y03-1017,0,\N,Missing
D10-1081,W98-1210,0,\N,Missing
D10-1081,C08-1113,0,\N,Missing
D10-1081,H94-1020,0,\N,Missing
D10-1081,P07-2055,0,\N,Missing
D10-1081,P06-1085,0,\N,Missing
D10-1081,N09-1036,0,\N,Missing
D10-1081,P09-1012,0,\N,Missing
D10-1081,P06-2056,0,\N,Missing
D10-1081,W99-0702,0,\N,Missing
D13-1121,Y00-1002,0,0.0604865,"ons. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alternation between the passive and active voices as a subtask of paraphrasing a simple sentence. They manually introduced case alternation rules on the basis of verb types and case patterns and transformed passive sentences into active sentences. Murata et al. (2006) developed a machinelearning-based method for Japanese ca"
D13-1121,J10-4006,0,0.019436,"pus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alternation between the passive and active voices as a subtask of paraphrasin"
D13-1121,W07-1522,0,0.160739,"Missing"
D13-1121,C02-1122,1,0.790053,"al ablative genitive Table 1: Examples of Japanese postpositional case particles and their typical grammatical functions. and tagged their cases in the active voice. Then, they trained SVM classiﬁers using the tagged corpus. Their features for training SVM were made by using several lexical resources such as IPAL (IPA, 1987), the Japanese thesaurus Bunrui Goi Hyo (NLRI, 1993), and the output of Kondo et al.’s method. 3 Lexicalized Case Frames To acquire knowledge for case alternation, we exploit lexicalized case frames that are automatically constructed from 6.9 billion Web sentences by using Kawahara and Kurohashi (2002)’s method. In short, their method ﬁrst parses the input sentences, and then constructs case frames by collecting reliable modiﬁer-head relations from the resulting parses. These case frames are constructed for each predicate like PropBank frames (Palmer et al., 2005), for each meaning of the predicate like FrameNet frames (Fillmore et al., 2003), and for each voice. However, neither pseudo-semantic role labels such as Arg1 in PropBank nor information about frames deﬁned in FrameNet are included in these case frames. Each case frame describes surface cases that each predicate has and instances"
D13-1121,N06-1023,1,0.94546,"ned from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and ﬁnds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness. 1 Introduction Predicate-argument structure analysis is one of the fundamental techniques for many natural language applications such as recognition of textual entailment, information retrieval, and machine translation. In Japanese, the relationship between a predicate and its argument is usually represented by using case particles1 (Kawahara and Kurohashi, 2006; Taira et al., 2008; Yoshikawa et al., 2011). However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure. There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates. For example, while the Kyoto University Text Corpus (Kawahara et al., 2004), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1 Japanese is a head-ﬁnal language. Word order does not mark syntactic relations."
D13-1121,kawahara-etal-2004-toward,1,0.755809,"al entailment, information retrieval, and machine translation. In Japanese, the relationship between a predicate and its argument is usually represented by using case particles1 (Kawahara and Kurohashi, 2006; Taira et al., 2008; Yoshikawa et al., 2011). However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure. There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates. For example, while the Kyoto University Text Corpus (Kawahara et al., 2004), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1 Japanese is a head-ﬁnal language. Word order does not mark syntactic relations. Instead, postpositional case particles function as case markers. (1) 女が woman-ga 男に man-ni 突き落とされた． was pushed down (A woman was pushed down by a man.) (2) 男が man-ga 女を woman-wo 突き落とした． pushed down (A man pushed down a woman.) Both representations have their own advantages. Surface case analysis is easier than normalized-case analysis, especially when we consider omitted arguments, which are also called zero ana"
D13-1121,korhonen-etal-2006-large,0,0.175816,"mes by collecting reliable modiﬁer-head relations from the resulting parses. These case frames are constructed for each predicate like PropBank frames (Palmer et al., 2005), for each meaning of the predicate like FrameNet frames (Fillmore et al., 2003), and for each voice. However, neither pseudo-semantic role labels such as Arg1 in PropBank nor information about frames deﬁned in FrameNet are included in these case frames. Each case frame describes surface cases that each predicate has and instances that can ﬁll a case slot, which is fully lexicalized like the subcategorization lexicon VALEX (Korhonen et al., 2006). We list some Japanese postpositional case particles with their typical grammatical functions in Table 1 and show examples of case frames in Table 2.4 Ideally, one case frame is constructed for each meaning and voice of the target predicate. However, since Kawahara and Kurohashi’s method is unsupervised, several case frames are actually constructed 4 Niyotte in Table 2 is a Japanese functional phrase that indicates agent in this case. We treat niyotte as a case particle in this paper for the sake of simplicity. 1215 Case Frame: “突き落とされる-5 (be pushed down-5)” { 京子 (Kyoko):3, 監督 (manager):1, ·"
D13-1121,J04-1003,0,0.029428,"panese. Our method leverages several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did"
D13-1121,P08-1050,0,0.01896,"on patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alter"
D13-1121,P98-2127,0,0.150639,"ces of the ga-case of the case frame “突き落とされ る-5 (be pushed down-5)” and the wo-case of the case frame “突き落とす-4 (push down-4),” which are considered to be aligned and represent patient, are similar. Thus, we exploit semantic similarity simSEM between the instances of the corresponding cases. We ﬁrst deﬁne an asymmetric similarity measure between C1 and C2 , each of which is a set of case slot instances, as follows: sima (C1 , C2 ) = 1  max (sim(i1 , i2 )), i2 ∈C2 |C1 | i1 ∈C1 where sim(i1 , i2 ) is the similarity between instances. In this study, we apply a distributional similarity measure (Lin, 1998), which was computed from the Web corpus used to construct the case frames. We next deﬁne a symmetric similarity measure between C1 and C2 as an average of sima (C1 , C2 ) and sima (C2 , C1 ). 1 sims (C1 , C2 ) = (sima (C1 , C2 )+sima (C2 , C1 )). 2 Then we deﬁne semantic similarity of a case alignment A between case frames CF1 and CF2 . simSEM (A) = 3. Preference of alternation patterns: fP P . N 1  sims (C1,i , C2,a(i) ), N i=1 1217  ,GHQWLID FRUUHVSRQGLQJ FDVHIUDPH where N denotes the number of case slots of CF1 , C1,i denotes a set of instances of the i-th case slot of CF1 , and C"
D13-1121,A00-2034,0,0.0687522,"ve voices in Japanese. Our method leverages several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-craft"
D13-1121,P06-2076,0,0.133723,"passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alternation between the passive and active voices as a subtask of paraphrasing a simple sentence. They manually introduced case alternation rules on the basis of verb types and case patterns and transformed passive sentences into active sentences. Murata et al. (2006) developed a machinelearning-based method for Japanese case alternation. They extracted 3,576 case particles in passive sentences from the Kyoto University Text Corpus Case particle ga wo ni de kara no Case Frame: “突き落とされる-4 (be pushed down-4)” { 女性 (woman):5, 僕 (I):2, 女 (woman):2, · · · }-ga { 海 (sea):229, 川 (bottom):115, 池 (pond):51, · · · }-ni { 継母(stepmother):2, ペガサス(Pegasus):2, · · · }-niyotte ··· Grammatical function nominative accusative dative locative, instrumental ablative genitive Table 1: Examples of Japanese postpositional case particles and their typical grammatical functions. an"
D13-1121,P98-2151,0,0.0145837,"f the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1 Japanese is a head-ﬁnal language. Word order does not mark syntactic relations. Instead, postpositional case particles function as case markers. (1) 女が woman-ga 男に man-ni 突き落とされた． was pushed down (A woman was pushed down by a man.) (2) 男が man-ga 女を woman-wo 突き落とした． pushed down (A man pushed down a woman.) Both representations have their own advantages. Surface case analysis is easier than normalized-case analysis, especially when we consider omitted arguments, which are also called zero anaphors (Nagao and Hasida, 1998). In Japanese, zero anaphora frequently occurs, and the omitted unnormalizedcase of a zero anaphor is often the same as the surface case of its antecedent (Sasano and Kurohashi, 2011). Therefore, surface case analysis suits zero anaphora resolution. On the other hand, when 2 Ga, wo, and ni are typical Japanese postpositional case particles. In most cases, they indicate nominative, accusative, and dative, respectively. 1213 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1213–1223, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for"
D13-1121,J05-1004,0,0.0714087,"several lexical resources such as IPAL (IPA, 1987), the Japanese thesaurus Bunrui Goi Hyo (NLRI, 1993), and the output of Kondo et al.’s method. 3 Lexicalized Case Frames To acquire knowledge for case alternation, we exploit lexicalized case frames that are automatically constructed from 6.9 billion Web sentences by using Kawahara and Kurohashi (2002)’s method. In short, their method ﬁrst parses the input sentences, and then constructs case frames by collecting reliable modiﬁer-head relations from the resulting parses. These case frames are constructed for each predicate like PropBank frames (Palmer et al., 2005), for each meaning of the predicate like FrameNet frames (Fillmore et al., 2003), and for each voice. However, neither pseudo-semantic role labels such as Arg1 in PropBank nor information about frames deﬁned in FrameNet are included in these case frames. Each case frame describes surface cases that each predicate has and instances that can ﬁll a case slot, which is fully lexicalized like the subcategorization lexicon VALEX (Korhonen et al., 2006). We list some Japanese postpositional case particles with their typical grammatical functions in Table 1 and show examples of case frames in Table 2."
D13-1121,I11-1085,1,0.838446,"nstead, postpositional case particles function as case markers. (1) 女が woman-ga 男に man-ni 突き落とされた． was pushed down (A woman was pushed down by a man.) (2) 男が man-ga 女を woman-wo 突き落とした． pushed down (A man pushed down a woman.) Both representations have their own advantages. Surface case analysis is easier than normalized-case analysis, especially when we consider omitted arguments, which are also called zero anaphors (Nagao and Hasida, 1998). In Japanese, zero anaphora frequently occurs, and the omitted unnormalizedcase of a zero anaphor is often the same as the surface case of its antecedent (Sasano and Kurohashi, 2011). Therefore, surface case analysis suits zero anaphora resolution. On the other hand, when 2 Ga, wo, and ni are typical Japanese postpositional case particles. In most cases, they indicate nominative, accusative, and dative, respectively. 1213 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1213–1223, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics we focus on the resulting predicate argument structures, the normalized-case structure is more useful. Speciﬁcally, since a normalized-case structure repres"
D13-1121,P08-1057,0,0.0164512,"raints on alternation patterns and lexical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dea"
D13-1121,D09-1067,0,0.100645,"xical case frames obtained from a large Web corpus, which are constructed for each meaning and voice of each predicate. 2 Related Work Levin (1993) grouped English verbs into classes on the basis of their shared meaning components and syntactic behavior, deﬁned in terms of diathesis alternations. Hence, diathesis alternations have been the topic of interest for a number of researchers in the ﬁeld of automatic verb classiﬁcation, which aims to induce possible verb frames from corpora (e.g., McCarthy 2000; Lapata and Brew 2004; Joanis et al. 2008; Schulte im Walde et al. 2008; Li and Brew 2008; Sun and Korhonen 2009; Theijssen et al. 2012). Baroni and Lenci (2010) used distributional slot similarity to distinguish between verbs undergoing the causative-inchoative alternations, and verbs that do not alternate. There is some work on passive-to-active voice transformation in Japanese. Baldwin and Tanaka (2000) empirically identiﬁed the range and frequency of basic verb alternation, including activepassive alternation, in Japanese. They automatically extracted alternation types by using hand-crafted case frames but did not evaluate the quality. Kondo et al. (2001) dealt with case alternation between the pass"
D13-1121,D08-1055,0,0.165115,"ur method aligns a case frame in the passive voice to a corresponding case frame in the active voice and ﬁnds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness. 1 Introduction Predicate-argument structure analysis is one of the fundamental techniques for many natural language applications such as recognition of textual entailment, information retrieval, and machine translation. In Japanese, the relationship between a predicate and its argument is usually represented by using case particles1 (Kawahara and Kurohashi, 2006; Taira et al., 2008; Yoshikawa et al., 2011). However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure. There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates. For example, while the Kyoto University Text Corpus (Kawahara et al., 2004), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1 Japanese is a head-ﬁnal language. Word order does not mark syntactic relations. Instead, postpositio"
D13-1121,I11-1126,0,0.146631,"ase frame in the passive voice to a corresponding case frame in the active voice and ﬁnds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness. 1 Introduction Predicate-argument structure analysis is one of the fundamental techniques for many natural language applications such as recognition of textual entailment, information retrieval, and machine translation. In Japanese, the relationship between a predicate and its argument is usually represented by using case particles1 (Kawahara and Kurohashi, 2006; Taira et al., 2008; Yoshikawa et al., 2011). However, since case particles vary depending on the voices, we have to take case alternation into account to represent predicate-argument structure. There are thus two major types of representations: one uses surface cases, and the other uses normalized-cases for the base form of predicates. For example, while the Kyoto University Text Corpus (Kawahara et al., 2004), one of the major Japanese corpora that contains annotations of predicate-argument structures, adopts 1 Japanese is a head-ﬁnal language. Word order does not mark syntactic relations. Instead, postpositional case particles functi"
D13-1121,C98-2122,0,\N,Missing
D13-1121,C98-2146,0,\N,Missing
D14-1017,P07-2045,0,0.0140636,"Missing"
D14-1017,J93-2003,0,0.0941501,"tion tasks, which prove the effectiveness of our methods under grammatically different language pairs. 1 Introduction Word alignment is an important component in statistical machine translation (SMT). For instance phrase-based SMT (Koehn et al., 2003) is based on the concept of phrase pairs that are automatically extracted from bilingual data and rely on word alignment annotation. Similarly, the model for hierarchical phrase-based SMT is built from exhaustively extracted phrases that are, in turn, heavily reliant on word alignment. The Generative word alignment models, such as the IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996), are popular methods for automatically aligning bilingual texts, but are restricted to represent one-to-many correspondence 153 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 153–158, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics in BLEU scores in the NTCIR10. 2 = Statistical word alignment with posterior regularization framework (xs , xt ) Z = → − → q (− y |x) xs Given a bilingual sentence x = where and xt denote a source and target sentence, respectively, the bilingual se"
D14-1017,P14-1139,0,0.0127301,"methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (2004) proposed arithmetic means of two models as a score for the filtering, whereas Liang et al. (2006) reported better results using geometric means. The joint training method (Liang et al., 2006) enforces agreement between two directional models. Posterior regularization (Ganchev et al., 2010) is an alternative agreement method which directly encodes agreement during training. DeNero and Macherey (2011) and Chang et al. (2014) also enforce agreement during decoding. However, these agreement models do not take into account the difference in language pairs, which is crucial for linguistically different language pairs, such as Japanese and English: although content words may be aligned with each other by introducing some agreement constraints, function words are difficult to align. We focus on the posterior regularization framework and improve upon the previous work by proposing new constraint functions that take into account the difference in languages in terms of content words and function words. In particular, we d"
D14-1017,N06-1014,0,0.190871,"ika-cho, Soraku-gun, Kyoto, Japan Abstract of each word. To resolve this weakness, various symmetrization methods are proposed. Och and Ney (2003) and Koehn et al. (2003) propose various heuristic methods to combine two directional models to represent many-to-many relationships. As an alternative to heuristic methods, filtering methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (2004) proposed arithmetic means of two models as a score for the filtering, whereas Liang et al. (2006) reported better results using geometric means. The joint training method (Liang et al., 2006) enforces agreement between two directional models. Posterior regularization (Ganchev et al., 2010) is an alternative agreement method which directly encodes agreement during training. DeNero and Macherey (2011) and Chang et al. (2014) also enforce agreement during decoding. However, these agreement models do not take into account the difference in language pairs, which is crucial for linguistically different language pairs, such as Japanese and English: although content words may be aligned with each"
D14-1017,N12-1047,0,0.0375542,"Missing"
D14-1017,C04-1032,0,0.023836,"Yokohama, Japan 2 National Institute of Information and Communication Technology 3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan Abstract of each word. To resolve this weakness, various symmetrization methods are proposed. Och and Ney (2003) and Koehn et al. (2003) propose various heuristic methods to combine two directional models to represent many-to-many relationships. As an alternative to heuristic methods, filtering methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (2004) proposed arithmetic means of two models as a score for the filtering, whereas Liang et al. (2006) reported better results using geometric means. The joint training method (Liang et al., 2006) enforces agreement between two directional models. Posterior regularization (Ganchev et al., 2010) is an alternative agreement method which directly encodes agreement during training. DeNero and Macherey (2011) and Chang et al. (2014) also enforce agreement during decoding. However, these agreement models do not take into account the difference in language pairs, which is crucial for linguistically diffe"
D14-1017,P11-1043,0,0.0127509,"to heuristic methods, filtering methods employ a threshold to control the trade-off between precision and recall based on a score estimated from the posterior probabilities from two directional models. Matusov et al. (2004) proposed arithmetic means of two models as a score for the filtering, whereas Liang et al. (2006) reported better results using geometric means. The joint training method (Liang et al., 2006) enforces agreement between two directional models. Posterior regularization (Ganchev et al., 2010) is an alternative agreement method which directly encodes agreement during training. DeNero and Macherey (2011) and Chang et al. (2014) also enforce agreement during decoding. However, these agreement models do not take into account the difference in language pairs, which is crucial for linguistically different language pairs, such as Japanese and English: although content words may be aligned with each other by introducing some agreement constraints, function words are difficult to align. We focus on the posterior regularization framework and improve upon the previous work by proposing new constraint functions that take into account the difference in languages in terms of content words and function wo"
D14-1017,J03-1002,0,0.0214356,"K 4.91K 4.57K NTCIR10 Japanese English 2.02M 53.4M 49.4M 114K 183K 2K 73K 67.3K 4.38K 5.04K 8.6K 334K 310K 10.4K 12.7K Figure 1: Precision Recall graph in Hansard French-English Figure 2: Precision Recall graph in KFTT Figure 3: AER in Hansard French-English Figure 4: AER in KFTT 156 Table 2: Results of word alignment evaluation with the heuristics-based method (GDF) method symmetric f2f c2c f2c precision 0.4595 0.4633 0.4606 0.4630 KFTT recall AER 0.5942 48.18 0.5997 47.73 0.5964 48.02 0.5998 47.74 F 0.5182 0.5227 0.5198 0.5226 Table 3: Results of translation evaluation by AER and F-measure (Och and Ney, 2003). Since there exists no distinction for sure-possible alignments in the KFTT data, we use only sure alignment for our evaluation, both for the FrenchEnglish and the Japanese-English tasks. Table 2 summarizes our results. The baseline method is symmetric constraint (Ganchev et al., 2010) shown in Table 2. The numbers in bold and in italics indicate the best score and the second best score, respectively. The differences between f2f,f2c and baseline in KFTT are statistically significant at p &lt; 0.05 using the signtest, but in hansard corpus, there exist no significant differences between the basel"
D14-1017,P02-1040,0,0.0919616,"may be treated as content words, based on the previous work of Setiawan et al. (2007). Experiments on word alignment tasks showed better alignment qualities measured by F-measure and AER on both the Hansard task and KFTT. We also observed large gain in BLEU, 0.2 on average, when compared with the previous posterior regularization method under NTCIR10 task. As our future work, we will investigate more precise methods for deciding function words and content words for better alignment and translation qualities. Translation evaluation Next, we performed a translation evaluation, measured by BLEU (Papineni et al., 2002). We compared the grow-diag-final and filtering method (Liang et al., 2006) for creating phrase tables. The threshold for the filtering factor was set to 0.1 which was the best setting in the word alignment experiment in section 4.2 under KFTT. From the English side of the training data, we trained a word using the 5-gram model with SRILM (Stolcke and others, 2002). “Moses” toolkit was used as a decoder (Koehn et al., 2007) and the model parameters were tuned by k-best MIRA (Cherry and Foster, 2012). In order to avoid tuning instability, we evaluated the average of five runs (Hopkins and May,"
D14-1017,P08-1112,0,0.0467484,"Missing"
D14-1017,P07-1090,0,0.373945,"t the difference in language pairs, which is crucial for linguistically different language pairs, such as Japanese and English: although content words may be aligned with each other by introducing some agreement constraints, function words are difficult to align. We focus on the posterior regularization framework and improve upon the previous work by proposing new constraint functions that take into account the difference in languages in terms of content words and function words. In particular, we differentiate between content words and function words by frequency in bilingual data, following Setiawan et al. (2007). Experimental results show that the proposed methods achieved better alignment qualities on the French-English Hansard data and the JapaneseEnglish Kyoto free translation task (KFTT) measured by AER and F-measure. In translation evaluations, we achieved statistically significant gains Generative word alignment models, such as IBM Models, are restricted to oneto-many alignment, and cannot explicitly represent many-to-many relationships in a bilingual text. The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agre"
D14-1017,C96-2141,0,0.299019,"ffectiveness of our methods under grammatically different language pairs. 1 Introduction Word alignment is an important component in statistical machine translation (SMT). For instance phrase-based SMT (Koehn et al., 2003) is based on the concept of phrase pairs that are automatically extracted from bilingual data and rely on word alignment annotation. Similarly, the model for hierarchical phrase-based SMT is built from exhaustively extracted phrases that are, in turn, heavily reliant on word alignment. The Generative word alignment models, such as the IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996), are popular methods for automatically aligning bilingual texts, but are restricted to represent one-to-many correspondence 153 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 153–158, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics in BLEU scores in the NTCIR10. 2 = Statistical word alignment with posterior regularization framework (xs , xt ) Z = → − → q (− y |x) xs Given a bilingual sentence x = where and xt denote a source and target sentence, respectively, the bilingual sentence is aligned by a manyto"
D14-1017,D11-1125,0,0.0492598,"Missing"
D14-1017,N03-1017,0,0.0362618,"ith each other during training, and propose new constraints that can take into account the difference between function words and content words. Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline. We also observed gains in Japanese-toEnglish translation tasks, which prove the effectiveness of our methods under grammatically different language pairs. 1 Introduction Word alignment is an important component in statistical machine translation (SMT). For instance phrase-based SMT (Koehn et al., 2003) is based on the concept of phrase pairs that are automatically extracted from bilingual data and rely on word alignment annotation. Similarly, the model for hierarchical phrase-based SMT is built from exhaustively extracted phrases that are, in turn, heavily reliant on word alignment. The Generative word alignment models, such as the IBM Models (Brown et al., 1993) and HMM (Vogel et al., 1996), are popular methods for automatically aligning bilingual texts, but are restricted to represent one-to-many correspondence 153 Proceedings of the 2014 Conference on Empirical Methods in Natural Languag"
D14-1017,E12-1045,0,\N,Missing
D15-1143,N10-1028,0,0.0896146,"nsduction Grammar (ITG) model (Neubig et al., 2011) wherein phrases of various granularities are 1217 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1217–1227, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. learned in a hierarchical back-off process. We extend it by incorporating arbitrary Hiero rules when backing off to smaller spans. For efficient inference, we use a fast two-step bi-parsing approach (Xiao et al., 2012) which basically runs in a time complexity of O(|f |3 ). Slice sampling for an SCFG (Blunsom and Cohn, 2010) is used for efficiently sampling a derivation tree from a reduced space of possible derivations. Our model achieved higher or at least comparable BLEU scores against the previous Bayesian SCFG model on language pairs; German/French/Spanish-English in the NewsCommentary corpus, and Japanese-English in the NTCIR10 corpus. When compared against heuristically extracted model through the GIZA++ pipeline, our model achieved comparable score on a full size Germany-English language pair in Europarl v7 corpus with significantly less grammar size. 2 Related Work Various criteria have been proposed to p"
D15-1143,P09-1088,0,0.107802,"a rule table, i.e., a synchronous grammar, may be composed of spuriously many rules with potential errors especially when it was automatically acquired from a parallel corpus. As a result, the increase in the rule table incurs a large amount of time for decoding and may result in lower translation quality. Pruning a rule table either on the basis of significance test (Johnson et al., 2007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f |3 |e|3 ) when we use dynamic programming SCFG biparsing (Wu, 1997). Gibbs sampling without biparsing (Levenberg et al., 2012) can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees. Even though we may learn a statistically sound model o"
D15-1143,W10-1703,0,0.0126229,"ions, we combine them as a part of a sampling process; we treat the derivation trees acquired from different iterations as additional training data, and increment the corresponding customers into our model. Hyperparameters are resampled after the merging process. The new features are directly computed from the merged model. 6 Experiments 6.1 Comparison with Previous Bayesian Model First, we compared the previous Bayesian model (Gen) with our hierarchical back-off model (Back). We used the first 100K sentence pairs of the WMT10 News-Commentary corpus for German/Spanish/French-to-English pairs (Callison-Burch et al., 2010) and NTCIR10 corpus for Japanese-English (Goto et al., 2013) for the translation model. All sentences are lowercased and filtered to preserve at most 40 words on both source and target sides. We sampled 20 iterations for Gen and Back and combined the last 10 iterations for extracting the translation model.5 The batch size was set to 64. The language models were estimated from the all-English side of the WMT News-Commentary and europarl-v7. In NTCIR10, we simply used the all-English side of the training data. All the 5-gram language models were estimated using SRILM (Stolcke and others, 2002) w"
D15-1143,J14-1007,0,0.0220988,"Missing"
D15-1143,P11-2031,0,0.0259328,"here are the average of three tuning runs (Hopkins and May, 2011). Table 1 lists the results measured using BLEU (Papineni et al., 2002).The term Sample denotes the combination size for each model. The term SIZE in the table denotes the number of the extracted grammar types composed of Hiero rules and phrase pairs. The numbers in italic denotes the score of Back, significantly improved from the score of 1 sampled combinated Gen. The numbers in bold denotes the score of Back + future, significantly improved from the score of 1 sampled combinated Back. All significance test are performed using Clark et al. (2011) under p-value of 0.05. Back performed better than Gen on Spanish-English and French-English language pairs. Note that the gains were achieved with the comparable grammar size. When comparing German-English and Japanese-English language pairs, there are no significant differences between Back and Gen. The combination of our Back with future score during slice sampling (+future) achieved further gains over the slice sampling without future scores, and slightly decrese the grammar size, compared to Back. However, there are still no significant difference between Back+future and Gen on German-Eng"
D15-1143,C10-2021,0,0.0174855,"ing et al., 2012; Zens et al., 2012). Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014). A model is learned by sampling derivation trees in a parallel corpus and by accumulating the rules in the"
D15-1143,P13-1077,0,0.0150821,"lating the detailed balance, its time complexity of O(|f |3 |e|3 ) is still impractical for a large-scale experiment. We efficiently carried out large-scale experiments on the basis of the two-step bi-parsing of Xiao et al. combined with slice sampling of Blunsom and Cohn. After learning a Bayesian model, it is not directly used in a decoder since it is composed of only minimum rules without considering phrases of various granularities. As a consequence, it is a standard practice to obtain word alignment from derivation trees and to extract SCFG rules heuristically from the word-aligned data (Cohn and Haffari, 2013). The work by Neubig et al. (2011) was the first attempt to directly use the learned model on the basis of a Bayesian ITG in which phrases of many granularities were encoded in the model by employing a hierarchical back-off procedure. Our work is strongly motivated by their work, but greatly differs in that our model can incorporate many arbitrary Hiero rules, not limited to ITGstyle binary branching rules. 3 Model We use Hiero grammar (Chiang, 2007), an instance of an SCFG, which is defined as a contextfree grammar for two languages. Let Σ denote a set of terminal symbols in the source langua"
D15-1143,W06-3105,0,0.425911,"ecreasing translation quality, e.g., Fisher’s exact test (Johnson et al., 2007) or relative entropy (Ling et al., 2012; Zens et al., 2012). Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014). A"
D15-1143,D08-1033,0,0.045527,"Missing"
D15-1143,P10-4002,0,0.0242682,"or those back-off scores. The conditional model probabilities in two directions, Pmodel (f |e) and Pmodel (e|f ), are estimated by marginalizing the joint probability Pmodel (f, e): Pmodel (f |e) = ∑ Pmodel (f, e) . ′ f ′ Pmodel (f , e) (19) The inverse direction Pmodel (e|f ) is estimated, similarly. The lexical probabilities in two directions, Plex (f |e) and Plex (e|f ), are scored by IBM Model probabilities between the source and target terminal symbols in rules and phrase pairs. In addition to the above features, we use Word penalty for each rule and phrase pair used in the cdec decoder (Dyer et al., 2010). As indicated in previous studies (Koehn et al., 2003; DeNero et al., 2006), the translation quality of generative models is lower than that of models with heuristically extracted rules and phrase pairs. DeNero et al. (2006) reported that considering multiple phrase boundaries is important for improving translation quality. The generative models, in particular Bayesian models, are strict in determining phrase boundaries since their models are usually estimated from sampled derivations. As a result, translation quality is poorer when 4 Note that the correct way to decode from our model is to s"
D15-1143,D11-1125,0,0.0169852,"65.5k fr-en 1.54M 1.83M 55.6M 65.5k 72.5k 61.9k 70.5k ja-en 1.80M 2.03M 27.8M 67.3k 73.0k 310k 333k Table 2: The number of words in training data de en TM 31.3M 32.8M LM 50.5M Dev 55.1k 58.8k Test 59.4k 55.5k Table 3: The number of words in training data We use GIZA++ and Moses default parameters for training. Decoding was carried out using the cdec decoder (?). Feature weights were tuned on the development data by running MIRA (Chiang, 2012) for 20 iterations with 16 parallel. For other parameters, we used cdec’s default values. The numbers reported here are the average of three tuning runs (Hopkins and May, 2011). Table 1 lists the results measured using BLEU (Papineni et al., 2002).The term Sample denotes the combination size for each model. The term SIZE in the table denotes the number of the extracted grammar types composed of Hiero rules and phrase pairs. The numbers in italic denotes the score of Back, significantly improved from the score of 1 sampled combinated Gen. The numbers in bold denotes the score of Back + future, significantly improved from the score of 1 sampled combinated Back. All significance test are performed using Clark et al. (2011) under p-value of 0.05. Back performed better t"
D15-1143,D07-1103,0,0.194885,"basis of the machine translation model. With HPBSMT, a restricted form of an SCFG, i.e., Hiero grammar, is usually used and is especially suited for linguistically divergent language pairs, such as Japanese and English. However, a rule table, i.e., a synchronous grammar, may be composed of spuriously many rules with potential errors especially when it was automatically acquired from a parallel corpus. As a result, the increase in the rule table incurs a large amount of time for decoding and may result in lower translation quality. Pruning a rule table either on the basis of significance test (Johnson et al., 2007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f |3 |e|3 ) when we use dynamic programming SCFG biparsing (Wu,"
D15-1143,N03-1017,0,0.199013,"ilities in two directions, Pmodel (f |e) and Pmodel (e|f ), are estimated by marginalizing the joint probability Pmodel (f, e): Pmodel (f |e) = ∑ Pmodel (f, e) . ′ f ′ Pmodel (f , e) (19) The inverse direction Pmodel (e|f ) is estimated, similarly. The lexical probabilities in two directions, Plex (f |e) and Plex (e|f ), are scored by IBM Model probabilities between the source and target terminal symbols in rules and phrase pairs. In addition to the above features, we use Word penalty for each rule and phrase pair used in the cdec decoder (Dyer et al., 2010). As indicated in previous studies (Koehn et al., 2003; DeNero et al., 2006), the translation quality of generative models is lower than that of models with heuristically extracted rules and phrase pairs. DeNero et al. (2006) reported that considering multiple phrase boundaries is important for improving translation quality. The generative models, in particular Bayesian models, are strict in determining phrase boundaries since their models are usually estimated from sampled derivations. As a result, translation quality is poorer when 4 Note that the correct way to decode from our model is to score every phrase pair created during decoding with ba"
D15-1143,P07-2045,0,0.0145853,"Missing"
D15-1143,D12-1021,0,0.0497587,"Missing"
D15-1143,D12-1088,0,0.038562,"Missing"
D15-1143,W02-1018,0,0.0414167,"ntly less grammar size. 2 Related Work Various criteria have been proposed to prune a phrase table without decreasing translation quality, e.g., Fisher’s exact test (Johnson et al., 2007) or relative entropy (Ling et al., 2012; Zens et al., 2012). Although those methods are easily applied for pruning a rule table, they heavily rely on the heuristically determined threshold parameter to trade off the translation quality and decoding speed of an MT system. Previously, EM-algorithm based generative models were exploited for generating compact phrase and rule tables. Joint phrase alignment model (Marcu and Wong, 2002) can directly express many-to-many word aligments without heuristic phrase extraction. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom"
D15-1143,P11-1064,1,0.480702,"s sampling without biparsing (Levenberg et al., 2012) can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees. Even though we may learn a statistically sound model on the basis of non-parametric Bayesian methods, current approaches for an SCFG still rely on exhaustive heuristic rule extraction from the wordalignment decided by derivation trees since the learned models cannot handle rules and phrases of various granularities. We propose a model on the basis of the previous work on the non-parametric Inversion Transduction Grammar (ITG) model (Neubig et al., 2011) wherein phrases of various granularities are 1217 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1217–1227, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. learned in a hierarchical back-off process. We extend it by incorporating arbitrary Hiero rules when backing off to smaller spans. For efficient inference, we use a fast two-step bi-parsing approach (Xiao et al., 2012) which basically runs in a time complexity of O(|f |3 ). Slice sampling for an SCFG (Blunsom and Cohn, 2010) is used for efficiently sam"
D15-1143,J03-1002,0,0.0330693,"Missing"
D15-1143,P03-1021,0,0.0510498,"heuristic method, we directly extract rules and phrase pairs from the learned models which are represented as Chinese restaurant tables. To limit grammar size, we include only phrase pairs that are selected at least once in the sample. During this extraction process, we limit the source or target terminal symbol size of phrase pairs to 5. For each extracted rule or phase pair, we compute a set of feature scores used for a HPBSMT decoder; a weighted combination of multiple features is necessary in SMT since the model learned from training data may not fit well to translate an unseen test data (Och, 2003). We use the following six features; the joint model probability Pmodel is calculated by Equation (2) for rules and by Equation (5) for phrase pairs. The joint posterior probability Pposterior (f, e) is estimated from the posterior probabilities for every rule and phrase pair in derivation trees through relative count estimation, motivated by Neubig et al. (2011) 4 . The joint posterior probability is considered as an approximation for those back-off scores. The conditional model probabilities in two directions, Pmodel (f |e) and Pmodel (e|f ), are estimated by marginalizing the joint probabil"
D15-1143,C12-1176,0,0.200354,"granularities. We propose a model on the basis of the previous work on the non-parametric Inversion Transduction Grammar (ITG) model (Neubig et al., 2011) wherein phrases of various granularities are 1217 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1217–1227, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. learned in a hierarchical back-off process. We extend it by incorporating arbitrary Hiero rules when backing off to smaller spans. For efficient inference, we use a fast two-step bi-parsing approach (Xiao et al., 2012) which basically runs in a time complexity of O(|f |3 ). Slice sampling for an SCFG (Blunsom and Cohn, 2010) is used for efficiently sampling a derivation tree from a reduced space of possible derivations. Our model achieved higher or at least comparable BLEU scores against the previous Bayesian SCFG model on language pairs; German/French/Spanish-English in the NewsCommentary corpus, and Japanese-English in the NTCIR10 corpus. When compared against heuristically extracted model through the GIZA++ pipeline, our model achieved comparable score on a full size Germany-English language pair in Euro"
D15-1143,D12-1089,0,0.0958848,"a restricted form of an SCFG, i.e., Hiero grammar, is usually used and is especially suited for linguistically divergent language pairs, such as Japanese and English. However, a rule table, i.e., a synchronous grammar, may be composed of spuriously many rules with potential errors especially when it was automatically acquired from a parallel corpus. As a result, the increase in the rule table incurs a large amount of time for decoding and may result in lower translation quality. Pruning a rule table either on the basis of significance test (Johnson et al., 2007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f |3 |e|3 ) when we use dynamic programming SCFG biparsing (Wu, 1997). Gibbs sampling without biparsing (Levenberg"
D15-1143,N13-1038,0,0.0306641,"tion. If the Score(rspi ) is less than usp , we prune the rspi from cube. Similar to Blunsom and Cohn (2010), if the span sp is not in the current derivation, the rules with low probability are pruned acd denotes a rule in cording to Equation (14). Let rsp d with span sp, P (d|u) is calculated by: sp∈d d ) P (rsp ∑ rj ∈rsp P (rj )I(usp &lt; Score(rj )) |φ | p| ∏ [θp ]dpp ∏ |φ (14) P (usp |d) = Beta(usp ; a, 1.0), ∏ pruning is conducted against the score denoted by the equation 10 , which is very similar to Xiao et al. (2012).3 For faster bi-parsing, we run sampling in parallel in the same way as Zhao and Huang (2013), in which bi-parsing is performed in parallel among the bilingual sentences in a mini-batch. The updates to the model are synchronized by incrementing and decrementing customers for the bilingual sentences in the mini-batch. Note that the biparsing for each mini-batch is conducted on the fixed model parameters after the synchronised parameter updates. In addition to the model parameters, hyperparameters are re-sampled after each training iteration following the discount and strength hyperparameter resampling in a hierarchical Pitman-Yor process (Teh, 2006). In particular, we resample ⟨dp , θp"
D15-1143,P02-1040,0,0.0940086,"27.8M 67.3k 73.0k 310k 333k Table 2: The number of words in training data de en TM 31.3M 32.8M LM 50.5M Dev 55.1k 58.8k Test 59.4k 55.5k Table 3: The number of words in training data We use GIZA++ and Moses default parameters for training. Decoding was carried out using the cdec decoder (?). Feature weights were tuned on the development data by running MIRA (Chiang, 2012) for 20 iterations with 16 parallel. For other parameters, we used cdec’s default values. The numbers reported here are the average of three tuning runs (Hopkins and May, 2011). Table 1 lists the results measured using BLEU (Papineni et al., 2002).The term Sample denotes the combination size for each model. The term SIZE in the table denotes the number of the extracted grammar types composed of Hiero rules and phrase pairs. The numbers in italic denotes the score of Back, significantly improved from the score of 1 sampled combinated Gen. The numbers in bold denotes the score of Back + future, significantly improved from the score of 1 sampled combinated Back. All significance test are performed using Clark et al. (2011) under p-value of 0.05. Back performed better than Gen on Spanish-English and French-English language pairs. Note that"
D15-1143,D14-1180,0,0.0144786,"ion. DeNero et al. (2006) proposed IBM Model 3 based many-to-many alignment model. Rule arithmetic method (Cmejrek and Zhou, 2010) can generate SCFG rules by combining other rule pairs through an insideoutside algorithm. However, those previous attempts were restricted in that the rules and phrases were induced by heuristic combination. Bayesian SCFG models can induce a compact model by incorporating sophisticated nonparametric Bayesian models for an SCFG, such as a dirichlet process (DeNero et al., 2008; Blunsom et al., 2009; Chung et al., 2014) or Pitman-Yor process (Levenberg et al., 2012; Peng and Gildea, 2014). A model is learned by sampling derivation trees in a parallel corpus and by accumulating the rules in the sampled trees into the model. Due to the O(|f |3 |e|3 ) time complexity for bi-parsing a bilingual sentence, previous studies relied on biparsing at the initialization step, and conducted Gibbs sampling by local operators (Blunsom et al., 2009; Levenberg et al., 2012) or sampling on fixed word alignments (Chung et al., 2014; Peng and Gildea, 2014). As a result, the inference can easily result in local optimum, wherein induced derivation trees may strongly depend on the initial trees. Xia"
D15-1143,P06-1124,0,0.367232,"2: Derivation tree generated from the hierarchical back-off model we reach phrase pairs which are generated without any back-offs. Let a discount parameter be dp , a strength parameter be θp , and a base measure be Gp0 . More formally, the generative process is represented as follows: GX ∼ Prule (dr , θr , Gphrase ), Gphrase ∼ Pphrase (dp , θp , GX ), X → ⟨s/t⟩ ∼ Gphrase , X → ⟨α/β⟩ ∼ GX , (4) where s is source side terminals and t is target side terminals in phrase pair ⟨s/t⟩. Pphrase is composed of three states, i.e., model, back-off, and base, and follows a hierarchical Pitman-Yor process (Teh, 2006). model: We draw a phrase pair ⟨s/t⟩ with the probability similar to Equation (2): ck − dp · |φpk | , θ p + np (5) where ck is the numbers of customers of a phrase pair pk and np is the number of all customers Note that this state is reachable when the phrase pair ⟨s/t⟩ exists in the model in the same manner as Equation (2). back-off: We will back off to smaller phrases using a rule generated by Prule as follows: θp + dp · |φp | cback + γb · Gb · θp + np cback + cbase + γb ·Prule (dr , θr , Gphrase ) ∏ · Pphrase (dp , θp , GX ), X∈⟨α/β⟩ base: As an alternative to the back-off state, we may rea"
D15-1143,J97-3002,0,0.677402,"007) or entropy (Ling et al., 2012; Zens et al., 2012) used in PBSMT can be easily applied for HPBSMT. However, these methods still rely on a heuristically determined threshold parameter. Bayesian SCFG methods (Blunsom et al., 2009) solve the spurious rule extraction problem by directly inducing a compact rule table from a parallel corpus on the basis of a non-parametric Bayesian model without any heuristics. Training for Bayesian SCFG models infers a derivation tree for each training instance, which demands the time complexity of O(|f |3 |e|3 ) when we use dynamic programming SCFG biparsing (Wu, 1997). Gibbs sampling without biparsing (Levenberg et al., 2012) can avoid this problem, though the induced derivation trees may strongly depend on initial derivation trees. Even though we may learn a statistically sound model on the basis of non-parametric Bayesian methods, current approaches for an SCFG still rely on exhaustive heuristic rule extraction from the wordalignment decided by derivation trees since the learned models cannot handle rules and phrases of various granularities. We propose a model on the basis of the previous work on the non-parametric Inversion Transduction Grammar (ITG) m"
D15-1143,J07-2003,0,\N,Missing
D16-1140,D13-1176,0,\N,Missing
D16-1140,D15-1044,0,\N,Missing
D16-1140,P00-1041,0,\N,Missing
D16-1140,W03-0501,0,\N,Missing
D16-1140,W08-1105,0,\N,Missing
D16-1140,C08-1018,0,\N,Missing
D16-1140,N10-1131,0,\N,Missing
D16-1140,W04-1013,0,\N,Missing
D16-1140,W11-1610,0,\N,Missing
D16-1140,D15-1042,0,\N,Missing
D16-1140,D13-1155,0,\N,Missing
D16-1140,W01-0100,0,\N,Missing
D16-1140,W12-3018,0,\N,Missing
D16-1140,P16-1154,0,\N,Missing
D16-1140,P16-1094,0,\N,Missing
D16-1140,D10-1050,0,\N,Missing
D16-1140,P16-1014,0,\N,Missing
D16-1140,N16-1012,0,\N,Missing
D16-1140,N16-1005,0,\N,Missing
D16-1210,C00-1004,0,0.0964455,"ulating F-measure (Fraser and Marcu, 2007)6 . We introduced itg and sym into the HMM and IBM Model 4. Training is bootstrapped from IBM Model 1, followed by HMM and IBM Model 4. All models were trained with five consecutive iterations. In the many-to-many alignment extraction, we used the filtering method (Matusov et al., 2004), where a threshold is optimized on the corresponding AER of the baseline model (i.e., HMM+sym or IBM Model 4+sym)7 . 5 BTEC Corpus is a subset of IWSLT 2007. To uniform tokenization, we retokenized all Japanese sentences both in IWSLT 2007 and BTEC Corpus using ChaSen (Asahara and Matsumoto, 2000). 6 Since there exists no distinction for sure-possible alignments in the KFTT and BTEC data sets, we treat all alignments of them as sure alignments. 7 We tried values from 0.1 to 1.0 at an interval of 0.1. 2001 Table 2 shows the results of word alignment evaluations8 , where none denotes that the model has no constraint. In KFTT and BTEC Corpus, itg achieved significant improvement against sym and none on IBM Model 4 (p ≤ 0.05)9 . However, in the Hansard Corpus, itg shows no improvement against sym. This indicates that capturing structural coherence by itg yields a significant benefit to wor"
D16-1210,J93-2003,0,0.0759293,"thods are not helpful for locating alignments with long distances because they do not use any syntactic structures. In contrast, the proposed method symmetrizes alignments in consideration of their structural coherence by using the ITG constraint softly in the posterior regularization framework (Ganchev et al., 2010). The ITG constraint is also compatible with word alignments that are not covered by ITG parse trees. Hence, the proposed method is robust to ITG parse errors compared to other alignment methods that directly use an ITG model. Compared to the HMM (Vogel et al., 1996), IBM Model 4 (Brown et al., 1993), and the baseline agreement method (Ganchev et al., 2010), the experimental results show that the proposed method significantly improves alignment performance regarding the Japanese-English KFTT and BTEC corpus, and in translation evaluation, the proposed method shows comparable or statistical significantly better performance on the JapaneseEnglish KFTT and IWSLT 2007 corpus. 1 Previous researches have improved bidirectional word alignments by jointly training two directional models to agree with each other (Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2010). Such a constraint on"
D16-1210,N12-1047,0,0.0145612,"statistical significance test was performed by the paired bootstrap resampling (Koehn, 2004). 10 The posterior thresholds were decided in the same way as the word alignment evaluation. 11 This setting is generally used for Ja-En translation tasks (Murakami et al., 2007). 9 vs. IBM Model 4+sym on the BTEC corpus. We would like to improve our model by imposing our ITG constraint on decoding steps in future. 4.2 Comparison between Symmetric and ITG Constraint Figure 1: Word alignment examples on the BTEC corpus. rameters as default settings. Parameter tuning was conducted by 100-best batch MIRA (Cherry and Foster, 2012) with 25 iterations. Table 3 shows the average BLEU of five different tunings12 . In both KFTT and IWSLT 2007, itg achieved significant improvement against both none and sym on HMM model. On IBM Model4, itg significantly outperforms none and is comparable to sym in KFTT, while itg significantly outperforms sym and is comparable to none in IWSLT 2007. 4 Discussion 4.1 Effects of ITG Constraints on Word Alignment and Translation We discuss the effect of our ITG constraint on word alignment and machine translation. As described in Section 2, the ITG constraint is imposed in the E-step of the EM a"
D16-1210,W07-0403,0,0.0603047,"Missing"
D16-1210,J07-2003,0,0.0575014,"6; Grac¸a et al., 2008; Ganchev et al., 2010). Such a constraint on the agreement in a training phase is one of the most effective approaches to word alignment. However, none of the previous agreement constraints have taken into account syntactic structures. Therefore, they have difficulty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 199"
D16-1210,P11-2031,0,0.0294483,"straint are directly reflected in the machine translation results because the phrase tables are extracted from the posterior probabilities calculated in training steps. Therefore, our ITG constraint has a potential to achieve a large improvement of machine translation performance relative to an improvement of alignment performance, such as IBM Model 4+itg 12 The values in bold represent the best score, and † indicates that the comparisons are not significant over the corresponding model (i.e., HMM+itg or IBM Model 4+itg) according to the bootstrap resampling test (p ≤ 0.05). We used multeval (Clark et al., 2011) for significance testing. 2002 In KFTT, itg is comparable to sym on IBM Model 4 in machine translation; however, itg achieved significant improvement in terms of word alignment, which follows the previous reports that better word alignment does not always result in better translation (Ganchev et al., 2008; Yang et al., 2013). On the other hand, in BTEC, itg outperforms sym both on word alignment and machine translation. Figure 1 shows that IBM Model 4+sym often generates wrong gappy alignments such as “ga (Ja)-I (En)” and “ga (Ja)-my (En)”. These wrong alignments disturb the phrase extraction"
D16-1210,P07-1003,0,0.0288247,"effective approaches to word alignment. However, none of the previous agreement constraints have taken into account syntactic structures. Therefore, they have difficulty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1998–2004, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics We propose an alignment method that use"
D16-1210,J07-3002,0,0.0327245,"used the first 10K sentence pairs in the training data for the IWSLT 2007 translation task, which were manually annotated with word alignment (Chooi-Ling et al., 2010), as the BTEC Corpus. In translation evaluations, we used the KFTT and Ja-En IWSLT 2007 translation tasks5 . Table 1 shows each corpus size. In each training data set, all words were lowercased and sentences with over 80 words on either side were removed. 3.1 Word Alignment Evaluation We measured the performance of word alignment with AER and F-measure (Och and Ney, 2003). We used only sure alignments for calculating F-measure (Fraser and Marcu, 2007)6 . We introduced itg and sym into the HMM and IBM Model 4. Training is bootstrapped from IBM Model 1, followed by HMM and IBM Model 4. All models were trained with five consecutive iterations. In the many-to-many alignment extraction, we used the filtering method (Matusov et al., 2004), where a threshold is optimized on the corresponding AER of the baseline model (i.e., HMM+sym or IBM Model 4+sym)7 . 5 BTEC Corpus is a subset of IWSLT 2007. To uniform tokenization, we retokenized all Japanese sentences both in IWSLT 2007 and BTEC Corpus using ChaSen (Asahara and Matsumoto, 2000). 6 Since ther"
D16-1210,P08-1112,0,0.0536238,"Missing"
D16-1210,N03-1017,0,0.0205063,"onal models to agree with each other (Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2010). Such a constraint on the agreement in a training phase is one of the most effective approaches to word alignment. However, none of the previous agreement constraints have taken into account syntactic structures. Therefore, they have difficulty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical"
D16-1210,P07-2045,0,0.00624361,"e relation of long-distance words. We discuss more details about the effectiveness of the ITG constraint in Section 4.1. 3.2 Translation Evaluation We measured translation performance with BLEU (Papineni et al., 2002). All language models are 5-gram and trained using SRILM (Stolcke and others, 2002) on target side sentences in the training data. When extracting phrases, we apply the method proposed by Matusov et al. (2004), where many-tomany alignments are generated based on the averages of the posterior probabilities from two directional models10 . We used the Moses phrase-based SMT systems (Koehn et al., 2007) for decoding. We set the distortion-limit parameter to infinite11 , and other pa8 The values in bold indicate the best score. The statistical significance test was performed by the paired bootstrap resampling (Koehn, 2004). 10 The posterior thresholds were decided in the same way as the word alignment evaluation. 11 This setting is generally used for Ja-En translation tasks (Murakami et al., 2007). 9 vs. IBM Model 4+sym on the BTEC corpus. We would like to improve our model by imposing our ITG constraint on decoding steps in future. 4.2 Comparison between Symmetric and ITG Constraint Figure 1"
D16-1210,W04-3250,0,0.119987,"e models are 5-gram and trained using SRILM (Stolcke and others, 2002) on target side sentences in the training data. When extracting phrases, we apply the method proposed by Matusov et al. (2004), where many-tomany alignments are generated based on the averages of the posterior probabilities from two directional models10 . We used the Moses phrase-based SMT systems (Koehn et al., 2007) for decoding. We set the distortion-limit parameter to infinite11 , and other pa8 The values in bold indicate the best score. The statistical significance test was performed by the paired bootstrap resampling (Koehn, 2004). 10 The posterior thresholds were decided in the same way as the word alignment evaluation. 11 This setting is generally used for Ja-En translation tasks (Murakami et al., 2007). 9 vs. IBM Model 4+sym on the BTEC corpus. We would like to improve our model by imposing our ITG constraint on decoding steps in future. 4.2 Comparison between Symmetric and ITG Constraint Figure 1: Word alignment examples on the BTEC corpus. rameters as default settings. Parameter tuning was conducted by 100-best batch MIRA (Cherry and Foster, 2012) with 25 iterations. Table 3 shows the average BLEU of five differen"
D16-1210,W13-2263,0,0.0170773,"alignment. However, none of the previous agreement constraints have taken into account syntactic structures. Therefore, they have difficulty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1998–2004, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics We propose an alignment method that uses an ITG constraint to e"
D16-1210,N06-1014,0,0.0348688,"ITG model. Compared to the HMM (Vogel et al., 1996), IBM Model 4 (Brown et al., 1993), and the baseline agreement method (Ganchev et al., 2010), the experimental results show that the proposed method significantly improves alignment performance regarding the Japanese-English KFTT and BTEC corpus, and in translation evaluation, the proposed method shows comparable or statistical significantly better performance on the JapaneseEnglish KFTT and IWSLT 2007 corpus. 1 Previous researches have improved bidirectional word alignments by jointly training two directional models to agree with each other (Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2010). Such a constraint on the agreement in a training phase is one of the most effective approaches to word alignment. However, none of the previous agreement constraints have taken into account syntactic structures. Therefore, they have difficulty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (C"
D16-1210,W13-3523,0,0.058296,"Missing"
D16-1210,C04-1032,0,0.391898,"ro Sumita2 eiichiro.sumita@nict.go.jp Tokyo Institute of Technology 2 National Institute of Information and Communication Technology 1 Abstract tasks other than SMT, such as bilingual lexicon extraction (Liu et al., 2013). The most conventional approaches to word alignment are the IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which align each source word to a single target word (i.e., directional models). In these models, bidirectional word alignments are traditionally induced by combining the Viterbi alignments in each direction using heuristics (Och and Ney, 2003). Matusov et al. (2004) exploited a symmetrized posterior probability for bidirectional word alignments. In these methods, each directional model is independently trained. We propose a novel unsupervised word alignment method that uses a constraint based on Inversion Transduction Grammar (ITG) parse trees to jointly unify two directional models. Previous agreement methods are not helpful for locating alignments with long distances because they do not use any syntactic structures. In contrast, the proposed method symmetrizes alignments in consideration of their structural coherence by using the ITG constraint softly"
D16-1210,W03-0301,0,0.16238,"Missing"
D16-1210,2007.iwslt-1.23,0,0.0267202,"sed by Matusov et al. (2004), where many-tomany alignments are generated based on the averages of the posterior probabilities from two directional models10 . We used the Moses phrase-based SMT systems (Koehn et al., 2007) for decoding. We set the distortion-limit parameter to infinite11 , and other pa8 The values in bold indicate the best score. The statistical significance test was performed by the paired bootstrap resampling (Koehn, 2004). 10 The posterior thresholds were decided in the same way as the word alignment evaluation. 11 This setting is generally used for Ja-En translation tasks (Murakami et al., 2007). 9 vs. IBM Model 4+sym on the BTEC corpus. We would like to improve our model by imposing our ITG constraint on decoding steps in future. 4.2 Comparison between Symmetric and ITG Constraint Figure 1: Word alignment examples on the BTEC corpus. rameters as default settings. Parameter tuning was conducted by 100-best batch MIRA (Cherry and Foster, 2012) with 25 iterations. Table 3 shows the average BLEU of five different tunings12 . In both KFTT and IWSLT 2007, itg achieved significant improvement against both none and sym on HMM model. On IBM Model4, itg significantly outperforms none and is c"
D16-1210,J03-1002,0,0.0550643,".titech.ac.jp Eiichiro Sumita2 eiichiro.sumita@nict.go.jp Tokyo Institute of Technology 2 National Institute of Information and Communication Technology 1 Abstract tasks other than SMT, such as bilingual lexicon extraction (Liu et al., 2013). The most conventional approaches to word alignment are the IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996), which align each source word to a single target word (i.e., directional models). In these models, bidirectional word alignments are traditionally induced by combining the Viterbi alignments in each direction using heuristics (Och and Ney, 2003). Matusov et al. (2004) exploited a symmetrized posterior probability for bidirectional word alignments. In these methods, each directional model is independently trained. We propose a novel unsupervised word alignment method that uses a constraint based on Inversion Transduction Grammar (ITG) parse trees to jointly unify two directional models. Previous agreement methods are not helpful for locating alignments with long distances because they do not use any syntactic structures. In contrast, the proposed method symmetrizes alignments in consideration of their structural coherence by using the"
D16-1210,P02-1040,0,0.113232,"cally different language pair such as Ja-En. For example, some function words appear more than once in both a source and target sentence, and they are not symmetrically aligned with each other, especially in regards to the Ja-En language pair. Although the baseline methods tend to be unable to align such long-distance word pairs, the proposed method can correctly catch them because itg can determine the relation of long-distance words. We discuss more details about the effectiveness of the ITG constraint in Section 4.1. 3.2 Translation Evaluation We measured translation performance with BLEU (Papineni et al., 2002). All language models are 5-gram and trained using SRILM (Stolcke and others, 2002) on target side sentences in the training data. When extracting phrases, we apply the method proposed by Matusov et al. (2004), where many-tomany alignments are generated based on the averages of the posterior probabilities from two directional models10 . We used the Moses phrase-based SMT systems (Koehn et al., 2007) for decoding. We set the distortion-limit parameter to infinite11 , and other pa8 The values in bold indicate the best score. The statistical significance test was performed by the paired bootstrap"
D16-1210,C12-1142,0,0.018121,"d for our ITG parsing. Our two-step parsing first parses a bilingual sentence in the bottom up manner, and then derives the Viterbi alignment z ∗ in the top down manner. To parse a bilingual sentence x = {f , e}, we define the probability for each ITG rule. The probability of a rule A → fi /ej is defined as: P (A → fi /ej ) = − → − p θ (zi,j = 1|x) + ← p θ (zi,j = 1|x) . 2 We provide a constant value pnull 3 both to P (A → ϵ/ej ) and P (A → fi /ϵ). To reduce computational cost, the probabilities of phrasal rules P (A → ⟨Y /Z⟩) and P (A → [Y /Z]) are not trained, which are set to 0.5 following Saers et al. (2012). In addition to the probability of each ITG rule, we must provide a probability to an one-to-many alignment because the two step parsing approach must pre-compute probabilities for all one-to-many alignments in the first step. An one-to-many alignment 2 3 We set n to 30 in our experiments. We set pnull to 10−5 . can be decomposed to a rule A → fi /ej and some A → ϵ/ej rules under the ITG form. We select a set of rules with the highest probability for an one-tomany alignment using Viterbi algorithm, which has a complexity of O(|e|). 2.3 Previous Agreement Constraint This section provides an ov"
D16-1210,takezawa-etal-2002-toward,1,0.572641,"ed as a soft constraint in the posterior regularization framework (Ganchev et al., 2010). In addition, our ITG constraint works also on word alignments that are not covered by ITG parse trees, as a standard symmetric constraint. Hence, the proposed method is robust to ITG parse errors compared to an alignment method that uses an ITG directly in model training (e.g., Zhang and Gildea (2004, 2005)). Word alignment evaluations show that the proposed method achieves significant gains in Fmeasure and alignment error rate (AER) on the KFTT (Neubig, 2011) and the BTEC JapaneseEnglish (Ja-En) corpus (Takezawa et al., 2002). Machine translation evaluations show that our constraint significantly outperforms or is comparable to the baseline symmetric constraint (Ganchev et al., 2010) in BLEU on the KFTT Ja-En and IWSLT 2007 Ja-En corpus (Fordyce, 2007). 2 ITG Constraint in the Posterior Regularization Framework 2.1 Overview The proposed method introduces an ITG constraint into the posterior regularization framework (Ganchev et al., 2010) in model training. The proposed model is trained as follows, where agreement constraints are imposed in the E-step of the EM algorithm1 : E-step: 1. Calculate a source-to-target p"
D16-1210,C96-2141,0,0.571603,"onal models. Previous agreement methods are not helpful for locating alignments with long distances because they do not use any syntactic structures. In contrast, the proposed method symmetrizes alignments in consideration of their structural coherence by using the ITG constraint softly in the posterior regularization framework (Ganchev et al., 2010). The ITG constraint is also compatible with word alignments that are not covered by ITG parse trees. Hence, the proposed method is robust to ITG parse errors compared to other alignment methods that directly use an ITG model. Compared to the HMM (Vogel et al., 1996), IBM Model 4 (Brown et al., 1993), and the baseline agreement method (Ganchev et al., 2010), the experimental results show that the proposed method significantly improves alignment performance regarding the Japanese-English KFTT and BTEC corpus, and in translation evaluation, the proposed method shows comparable or statistical significantly better performance on the JapaneseEnglish KFTT and IWSLT 2007 corpus. 1 Previous researches have improved bidirectional word alignments by jointly training two directional models to agree with each other (Liang et al., 2006; Grac¸a et al., 2008; Ganchev et"
D16-1210,J97-3002,0,0.668646,"ty recovering the alignments with long distances, which frequently occur, especially in grammatically different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1998–2004, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics We propose an alignment method that uses an ITG constraint to encourage agreement between two directional models in consideration of their structural coherence. Our ITG constraint is based on"
D16-1210,C12-1176,0,0.0172759,"n, we present our ITG parsing method, which uses bracketing ITG (Wu, 1997). The rules of the bracketing ITG are as follows: A → ⟨Y /Z⟩, A → [Y /Z], A → fi /ej , A → fi /ϵ, and A → ϵ/ej , where A, Y , and Z are non-terminal symbols, fi and ej are terminal strings, ϵ is a null symbol, ⟨⟩ denotes the inversion of two phrase positions, and [] denotes the reversion of two phrase positions. In general, a bracketing ITG has O(|f |3 |e|3 ) time complexity for parsing a sentence pair {f , e}, where |f |and |e |are the lengths of f and e. For efficient ITG parsing, we use the two-step parsing approach (Xiao et al., 2012), which has been proposed to induce Synchronous Context Free Grammar (SCFG) using n-best pruning2 with time complexity O(|f |3 ). Because ITG is a kind of SCFG, this method can be adopted for our ITG parsing. Our two-step parsing first parses a bilingual sentence in the bottom up manner, and then derives the Viterbi alignment z ∗ in the top down manner. To parse a bilingual sentence x = {f , e}, we define the probability for each ITG rule. The probability of a rule A → fi /ej is defined as: P (A → fi /ej ) = − → − p θ (zi,j = 1|x) + ← p θ (zi,j = 1|x) . 2 We provide a constant value pnull 3 bo"
D16-1210,P13-1017,0,0.0173604,"ance, such as IBM Model 4+itg 12 The values in bold represent the best score, and † indicates that the comparisons are not significant over the corresponding model (i.e., HMM+itg or IBM Model 4+itg) according to the bootstrap resampling test (p ≤ 0.05). We used multeval (Clark et al., 2011) for significance testing. 2002 In KFTT, itg is comparable to sym on IBM Model 4 in machine translation; however, itg achieved significant improvement in terms of word alignment, which follows the previous reports that better word alignment does not always result in better translation (Ganchev et al., 2008; Yang et al., 2013). On the other hand, in BTEC, itg outperforms sym both on word alignment and machine translation. Figure 1 shows that IBM Model 4+sym often generates wrong gappy alignments such as “ga (Ja)-I (En)” and “ga (Ja)-my (En)”. These wrong alignments disturb the phrase extraction, because excessively long phrase pairs are extracted by bridging the gaps in wrong alignments or simply no phrase pairs are extracted from wrong gappy alignments. Consequently, the phrase table generated by IBM Model 4+sym tend to be sparse and contain longer phrase pairs than the one generated by IBM Model 4+itg. 5 Conclusi"
D16-1210,C04-1060,0,0.0378121,"y different language pairs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1998–2004, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics We propose an alignment method that uses an ITG constraint to encourage agreement between two directional models in consideration of their structural coherence. Our ITG constraint is based on the Viterbi alignment decided by a bracketing ITG parse tree, and used as a soft constraint in the posterior regu"
D16-1210,P05-1059,0,0.0437533,"rs. Introduction Word alignment is an important component of statistical machine translation (SMT) systems such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2007). In addition, word alignment is utilized for multi-lingual Some unsupervised word alignment models such as DeNero and Klein (2007) and Kondo et al. (2013), have been based on syntactic structures. In particular, it has been proven that Inversion Transduction Grammar (ITG) (Wu, 1997), which captures structural coherence between parallel sentences, helps in word alignment (Zhang and Gildea, 2004; Zhang and Gildea, 2005). However, ITG has not been introduced into an agreement constraint so far. 1998 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1998–2004, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics We propose an alignment method that uses an ITG constraint to encourage agreement between two directional models in consideration of their structural coherence. Our ITG constraint is based on the Viterbi alignment decided by a bracketing ITG parse tree, and used as a soft constraint in the posterior regularization framework (Gan"
D16-1210,2007.iwslt-1.1,0,\N,Missing
D17-1246,C14-1154,0,0.0297851,"ntext or not. Table 3 also shows that SGNS-based models are better than SVD-based models. As we discussed in Section 3.2, we used two weighting schemes for each model. Although the AUC of each decaying weight model is larger than that of the corresponding uniform weight model, the differences were not statistically significant. 5 Related Work The previous studies focused on distinguishing non-standard usages that are multi-word expressions or idiomatic expressions (Kiela and Clark, 2013; Salehi et al., 2015; Li and Sporleder, 2010). The task of this research is similar to new sense detection (Cook et al., 2014). Our research target includes jargon, whose actual meaning is difficult to infer without specific knowledge about its usage (Huang and Riloff, 2010). Recent studies in computational linguistics have used word embeddings and other techniques to capture various semantic changes in words, such as diachronic changes, geographical variations, and sentiment changes (Mitra et al., 2014; Kulkarni et al., 2015; Frermann and Lapata, 2016; Eisenstein et al., 2010; Hamilton et al., 2016; Yang and Eisenstein, 2016). A few researchers have exploited output embeddings for natural language applications such"
D17-1246,D10-1124,0,0.0316643,"Missing"
D17-1246,Q16-1003,0,0.0154324,"i-word expressions or idiomatic expressions (Kiela and Clark, 2013; Salehi et al., 2015; Li and Sporleder, 2010). The task of this research is similar to new sense detection (Cook et al., 2014). Our research target includes jargon, whose actual meaning is difficult to infer without specific knowledge about its usage (Huang and Riloff, 2010). Recent studies in computational linguistics have used word embeddings and other techniques to capture various semantic changes in words, such as diachronic changes, geographical variations, and sentiment changes (Mitra et al., 2014; Kulkarni et al., 2015; Frermann and Lapata, 2016; Eisenstein et al., 2010; Hamilton et al., 2016; Yang and Eisenstein, 2016). A few researchers have exploited output embeddings for natural language applications such as document ranking (Mitra et al., 2016) and improving language models (Press and Wolf, 2017). 13 Although we also conducted experiments with a sigmoid function for the SGNS IN-IN model and with the cosine similarity for the SVD model, their accuracies were worse than those in Table 3. 2326 6 Conclusion We presented a model that uses context embeddings to distinguish Japanese non-standard usages from standard ones on social medi"
D17-1246,W16-1817,0,0.0246182,"ent 4.1 Methods for Comparative Evaluation Our model has three characteristics: (input and output) word embeddings, decaying weights, and a general balanced corpus. We evaluated each of these characteristics in a task distinguishing nonstandard usages from standard ones. First, we verified the effectiveness of the input and output embeddings. We tested a method in which only input embeddings are used to calculate the similarity: the cosine similarity between IN and v IN instead of σ(v IN · v OU T ), which vw wj wt wj t is a similar framework to that of previous work (Neelakantan et al., 2014; Gharbieh et al., 2016). We then tested a method based on the positive pointwise mutual information (PPMI) (Levy et al., 2015; Hamilton et al., 2016). Here, suppose that M is a matrix in which each element is a PPMI of IN · v OU T in Equation (1) is words wi and wj . vw wj t replaced with the (t, j)-element of the low-rank approximation of M obtained through singular value decomposition (SVD). We refer to this model as SVD. 7 code.google.com/archive/p/word2vec/ This weighting scheme is mentioned in (Levy et al., 2015). 2325 8 Corpus BCCWJ9 Web10 Wikipedia11 Newspaper12 Description #word Japanese Balanced 131,913 Cor"
D17-1246,D16-1057,0,0.114458,"eights, and a general balanced corpus. We evaluated each of these characteristics in a task distinguishing nonstandard usages from standard ones. First, we verified the effectiveness of the input and output embeddings. We tested a method in which only input embeddings are used to calculate the similarity: the cosine similarity between IN and v IN instead of σ(v IN · v OU T ), which vw wj wt wj t is a similar framework to that of previous work (Neelakantan et al., 2014; Gharbieh et al., 2016). We then tested a method based on the positive pointwise mutual information (PPMI) (Levy et al., 2015; Hamilton et al., 2016). Here, suppose that M is a matrix in which each element is a PPMI of IN · v OU T in Equation (1) is words wi and wj . vw wj t replaced with the (t, j)-element of the low-rank approximation of M obtained through singular value decomposition (SVD). We refer to this model as SVD. 7 code.google.com/archive/p/word2vec/ This weighting scheme is mentioned in (Levy et al., 2015). 2325 8 Corpus BCCWJ9 Web10 Wikipedia11 Newspaper12 Description #word Japanese Balanced 131,913 Corpus Sentences randomly 336,048 picked from the Web Japanese Wikipedia 1,081,154 Japanese Newspapers 1,204,914 #token 1.1b 6.0b"
D17-1246,D13-1147,0,0.0316887,"s that input embeddings should be used in combination with output embeddings for the task of judging whether a word matches its context or not. Table 3 also shows that SGNS-based models are better than SVD-based models. As we discussed in Section 3.2, we used two weighting schemes for each model. Although the AUC of each decaying weight model is larger than that of the corresponding uniform weight model, the differences were not statistically significant. 5 Related Work The previous studies focused on distinguishing non-standard usages that are multi-word expressions or idiomatic expressions (Kiela and Clark, 2013; Salehi et al., 2015; Li and Sporleder, 2010). The task of this research is similar to new sense detection (Cook et al., 2014). Our research target includes jargon, whose actual meaning is difficult to infer without specific knowledge about its usage (Huang and Riloff, 2010). Recent studies in computational linguistics have used word embeddings and other techniques to capture various semantic changes in words, such as diachronic changes, geographical variations, and sentiment changes (Mitra et al., 2014; Kulkarni et al., 2015; Frermann and Lapata, 2016; Eisenstein et al., 2010; Hamilton et al"
D17-1246,Q15-1016,0,0.299208,"eddings, decaying weights, and a general balanced corpus. We evaluated each of these characteristics in a task distinguishing nonstandard usages from standard ones. First, we verified the effectiveness of the input and output embeddings. We tested a method in which only input embeddings are used to calculate the similarity: the cosine similarity between IN and v IN instead of σ(v IN · v OU T ), which vw wj wt wj t is a similar framework to that of previous work (Neelakantan et al., 2014; Gharbieh et al., 2016). We then tested a method based on the positive pointwise mutual information (PPMI) (Levy et al., 2015; Hamilton et al., 2016). Here, suppose that M is a matrix in which each element is a PPMI of IN · v OU T in Equation (1) is words wi and wj . vw wj t replaced with the (t, j)-element of the low-rank approximation of M obtained through singular value decomposition (SVD). We refer to this model as SVD. 7 code.google.com/archive/p/word2vec/ This weighting scheme is mentioned in (Levy et al., 2015). 2325 8 Corpus BCCWJ9 Web10 Wikipedia11 Newspaper12 Description #word Japanese Balanced 131,913 Corpus Sentences randomly 336,048 picked from the Web Japanese Wikipedia 1,081,154 Japanese Newspapers 1,"
D17-1246,C10-2078,0,0.0244279,"mbination with output embeddings for the task of judging whether a word matches its context or not. Table 3 also shows that SGNS-based models are better than SVD-based models. As we discussed in Section 3.2, we used two weighting schemes for each model. Although the AUC of each decaying weight model is larger than that of the corresponding uniform weight model, the differences were not statistically significant. 5 Related Work The previous studies focused on distinguishing non-standard usages that are multi-word expressions or idiomatic expressions (Kiela and Clark, 2013; Salehi et al., 2015; Li and Sporleder, 2010). The task of this research is similar to new sense detection (Cook et al., 2014). Our research target includes jargon, whose actual meaning is difficult to infer without specific knowledge about its usage (Huang and Riloff, 2010). Recent studies in computational linguistics have used word embeddings and other techniques to capture various semantic changes in words, such as diachronic changes, geographical variations, and sentiment changes (Mitra et al., 2014; Kulkarni et al., 2015; Frermann and Lapata, 2016; Eisenstein et al., 2010; Hamilton et al., 2016; Yang and Eisenstein, 2016). A few res"
D17-1246,P14-1096,0,0.0255034,"nguishing non-standard usages that are multi-word expressions or idiomatic expressions (Kiela and Clark, 2013; Salehi et al., 2015; Li and Sporleder, 2010). The task of this research is similar to new sense detection (Cook et al., 2014). Our research target includes jargon, whose actual meaning is difficult to infer without specific knowledge about its usage (Huang and Riloff, 2010). Recent studies in computational linguistics have used word embeddings and other techniques to capture various semantic changes in words, such as diachronic changes, geographical variations, and sentiment changes (Mitra et al., 2014; Kulkarni et al., 2015; Frermann and Lapata, 2016; Eisenstein et al., 2010; Hamilton et al., 2016; Yang and Eisenstein, 2016). A few researchers have exploited output embeddings for natural language applications such as document ranking (Mitra et al., 2016) and improving language models (Press and Wolf, 2017). 13 Although we also conducted experiments with a sigmoid function for the SGNS IN-IN model and with the cosine similarity for the SVD model, their accuracies were worse than those in Table 3. 2326 6 Conclusion We presented a model that uses context embeddings to distinguish Japanese non"
D17-1246,D14-1113,0,0.0382013,"arget instances. 4 Experiment 4.1 Methods for Comparative Evaluation Our model has three characteristics: (input and output) word embeddings, decaying weights, and a general balanced corpus. We evaluated each of these characteristics in a task distinguishing nonstandard usages from standard ones. First, we verified the effectiveness of the input and output embeddings. We tested a method in which only input embeddings are used to calculate the similarity: the cosine similarity between IN and v IN instead of σ(v IN · v OU T ), which vw wj wt wj t is a similar framework to that of previous work (Neelakantan et al., 2014; Gharbieh et al., 2016). We then tested a method based on the positive pointwise mutual information (PPMI) (Levy et al., 2015; Hamilton et al., 2016). Here, suppose that M is a matrix in which each element is a PPMI of IN · v OU T in Equation (1) is words wi and wj . vw wj t replaced with the (t, j)-element of the low-rank approximation of M obtained through singular value decomposition (SVD). We refer to this model as SVD. 7 code.google.com/archive/p/word2vec/ This weighting scheme is mentioned in (Levy et al., 2015). 2325 8 Corpus BCCWJ9 Web10 Wikipedia11 Newspaper12 Description #word Japan"
D17-1246,E17-2025,0,0.161991,"small for word pairs that do not co-occur in the training corpus. We exploited this tendency for recognizing non-standard usages; if the dot-product between the embeddings of the target word and the context words is small, it should indicate a non-standard usage, on the condition that the embeddings have been learned on a general balanced corpus where words correspond to their standard meanings in most cases. v IN is widely used as a word embedding in many studies, while v OU T has not been in the limelight; only a few researchers have examined the effectiveness of v OU T (Mitra et al., 2016; Press and Wolf, 2017). In recent studies, embeddings v IN are usually used for measuring the similarity between words. However, given the characteristics described in the previous paragraph and SGNS’s equivalence with shifted positive pointwise mutual information (Levy and Goldberg, 2014), if we want to measure to what extent word wt tends to co-occur with wk in the training data, then we should use the simiIN · v OU T , instead of v IN · v IN . larity of vw wk wt wk t In this study, we show the importance of using in a task where we need to see if a word matches its context. v OU T 2324 Figure 1: Overview of our"
D17-1246,P10-1029,0,0.0202946,"schemes for each model. Although the AUC of each decaying weight model is larger than that of the corresponding uniform weight model, the differences were not statistically significant. 5 Related Work The previous studies focused on distinguishing non-standard usages that are multi-word expressions or idiomatic expressions (Kiela and Clark, 2013; Salehi et al., 2015; Li and Sporleder, 2010). The task of this research is similar to new sense detection (Cook et al., 2014). Our research target includes jargon, whose actual meaning is difficult to infer without specific knowledge about its usage (Huang and Riloff, 2010). Recent studies in computational linguistics have used word embeddings and other techniques to capture various semantic changes in words, such as diachronic changes, geographical variations, and sentiment changes (Mitra et al., 2014; Kulkarni et al., 2015; Frermann and Lapata, 2016; Eisenstein et al., 2010; Hamilton et al., 2016; Yang and Eisenstein, 2016). A few researchers have exploited output embeddings for natural language applications such as document ranking (Mitra et al., 2016) and improving language models (Press and Wolf, 2017). 13 Although we also conducted experiments with a sigmo"
D17-1246,kawahara-kurohashi-2006-case,0,0.0220099,"d the ranking in terms of the area under the ROC curve (AUC) (Davis and Goadrich, 2006). 4.3 Results Table 3 shows the AUC for each model.13 First, we examined the impact of the choice of training corpus for obtaining word embeddings. The models with BCCWJ are constantly better than those with other corpora, although BCCWJ is smaller than the others (Table 2). This result suggests that use of a balanced corpus is crucial in our method for this task. 9 The Balanced Corpus of Contemporary Written Japanese (Maekawa et al., 2010). 10 Japanese sentences are collected using the method described in (Kawahara and Kurohashi, 2006). 11 We downloaded Japanese Wikipedia articles in July 2016 from https://dumps.wikimedia.org/jawiki/. 12 We used editions of the Mainichi Shimbun, Nihon Keizai Shimbun, and Yomiuri Shimbun published from 1994 to 2004. corpus BCCWJ Web Wikipedia Newspaper SGNS IN-OUT decay uni .875 .846 .827 .844 .870 .842 .821 .839 SGNS IN-IN decay uni SVD decay uni .846 .817 .824 .825 .821 .771 .739 .770 .837 .807 .805 .810 .813 .765 .732 .764 Table 3: Area under the ROC curve (AUC) in usage classification task for each model. Next, we examined the impact of context embeddings. Table 3 shows that our model (S"
D17-1246,N15-1099,0,0.0224251,"should be used in combination with output embeddings for the task of judging whether a word matches its context or not. Table 3 also shows that SGNS-based models are better than SVD-based models. As we discussed in Section 3.2, we used two weighting schemes for each model. Although the AUC of each decaying weight model is larger than that of the corresponding uniform weight model, the differences were not statistically significant. 5 Related Work The previous studies focused on distinguishing non-standard usages that are multi-word expressions or idiomatic expressions (Kiela and Clark, 2013; Salehi et al., 2015; Li and Sporleder, 2010). The task of this research is similar to new sense detection (Cook et al., 2014). Our research target includes jargon, whose actual meaning is difficult to infer without specific knowledge about its usage (Huang and Riloff, 2010). Recent studies in computational linguistics have used word embeddings and other techniques to capture various semantic changes in words, such as diachronic changes, geographical variations, and sentiment changes (Mitra et al., 2014; Kulkarni et al., 2015; Frermann and Lapata, 2016; Eisenstein et al., 2010; Hamilton et al., 2016; Yang and Eis"
D17-1246,Y16-3006,0,0.0297227,"erver increases”. 1 This is interlinear-gloss text representation. POSS, NOM, PRS respectively represent the possessive case, nominative case, and present tense. The third line is the standard translation of the Japanese sentence. The Japanese word “鯖 (saba)” (i.e., mackerel) is used to mean computer server by Japanese computer geeks because saba happens to have a pronunciation that is similar to s¯ab¯a (i.e., computer server). When a word is used in a meaning that is different from its dictionary meaning, we call such a usage non-standard.2 Non-standard usages can be found in many languages (Sboev, 2016). For example, the word “catfish” means a ray-finned fish as in a standard dictionary, but on social media, it can mean a person who pretends to be someone else in order to create a fake identity. Such non-standard usages would be an obstacle to a variety of language processings including machine translation; Google Translate cannot correctly interpret examples such as this. Humans, however, would be able to notice non-standard usages from the inconsistency between the expected word meaning and the context. The purpose of this work is to develop a method for distinguishing non-standard usages"
D19-1587,D15-1263,0,0.141551,"similarity score function that is defined for merging two adjacent spans into a large one. Experimental results on English and German RST treebanks showed that our parser based on span merging achieved the best score, around 0.8 F1 score, which is close to the scores of the previous supervised parsers. 1 Introduction Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one of the theories that are most widely utilized for representing a discourse structure of a text in downstream NLP applications, such as automatic summarization (Marcu, 1998; Hirao et al., 2013), sentiment analysis (Bhatia et al., 2015), and text categorization (Ji and Smith, 2017). RST represents a text as a kind of constituent tree, whose leaves are Elementary Discourse Units (EDUs), clause-like units, and whose non-terminal nodes cover text spans consisting of a sequence of EDUs or a singleton EDU. The label of a non-terminal node represents the attribution of a text span, nucleus or satellite. A discourse relation is also assigned between two adjacent nonterminal nodes. Since the RST tree can be regarded as a standard constituent (phrase structure) tree, syntactic parsing models for constituency parsing can also be succe"
D19-1587,E17-1028,0,0.116463,"it is significant that our method outperformed baselines on D2S2E settings. Moreover, we compared our method with some supervised RST parsers on the test set of RSTDT. Table 3 shows the results. In the table, HHN16 denotes a simple transition-based RST parser (Hayashi et al., 2016), and WLW17 denotes a current state-of-the-art transition-based RST parser (Wang et al., 2017). Although the score of our method is lower than the scores of the supervised parsers, the score is close to that of HHN16. The results demonstrated the effectiveness of our unsupervised RST parsing method. For reference, (Braud et al., 2017) reported that their supervised RST parser obtained .802 span F1 score on PCC. However, we cannot compare the score with our score because the test set differs from ours. 4 Conclusion This paper proposed two kinds of unsupervised RST parsing methods based on dynamic programming that can build the optimal RST tree in terms of either a span splitting score or a span merging score. We regarded a document as a text span consisting of three different granularity levels and built trees at each level, a document tree, paragraph trees for each paragraph, and sentence trees for each sentence. Then, we"
D19-1587,W01-1605,0,0.0730738,"of a text span, nucleus or satellite. A discourse relation is also assigned between two adjacent nonterminal nodes. Since the RST tree can be regarded as a standard constituent (phrase structure) tree, syntactic parsing models for constituency parsing can also be successfully applied to RST parsing. In most cases, RST parsers have been developed on the basis of supervised learning algorithms, which require a high quality annotated corpus of sufficient size. As a result, research on RST parsing has focused on English, with the largest annotated corpus being the RST Discourse Treebank (RST-DT) (Carlson et al., 2001). These supervised RST parsing methods might not be applied to languages with only a small-size corpus. This paper presents two types of language independent unsupervised RST parsing methods based on the CKY-like dynamic programming-based approach. One method builds the most likely parse tree in terms of a dissimilarity score defined for splitting a text span into two smaller ones. The other builds the optimal tree in terms of a similarity score defined for merging two adjacent text spans into a larger one. The similarity and dissimilarity scores between text spans are calculated on the basis"
D19-1587,W16-3616,1,0.857101,"T trees contain the multi-nuclear relations is larger. In fact, the ratio of the documents whose RST trees contain multi-nuclear relations in PCC is 0.712, while that in RST-DT is 0.464. We can obtain the information of sentences boundaries in most cases, however, sometimes we cannot obtain the information of paragraph boundaries. Thus, it is significant that our method outperformed baselines on D2S2E settings. Moreover, we compared our method with some supervised RST parsers on the test set of RSTDT. Table 3 shows the results. In the table, HHN16 denotes a simple transition-based RST parser (Hayashi et al., 2016), and WLW17 denotes a current state-of-the-art transition-based RST parser (Wang et al., 2017). Although the score of our method is lower than the scores of the supervised parsers, the score is close to that of HHN16. The results demonstrated the effectiveness of our unsupervised RST parsing method. For reference, (Braud et al., 2017) reported that their supervised RST parser obtained .802 span F1 score on PCC. However, we cannot compare the score with our score because the test set differs from ours. 4 Conclusion This paper proposed two kinds of unsupervised RST parsing methods based on dynam"
D19-1587,D13-1158,1,0.930181,"Missing"
D19-1587,P14-1002,0,0.17613,"he left span ` from i-th to k-th atomic text unit1 and the right span r from k+1 to j-th atomic text unit are given, we define the similarity score between them as follows: −→ −−→ 1 sim(`i:k , − rk+1:j )= 2 ( −→ ) −−→ `i:k · − rk+1:j −→ −−→ + 1 . r k k` kk− i:k −→ −−→ indicate the vector represenHere, `i:k and − rk+1:j tations of the left and right spans, which are defined as a concatenation of two vectors for the left most atomic unit and the right most atomic unit as follows: −→ − →], `i:k = [→ ui ; − u k (2) − − − → − − → − r = [u ; → u ]. k+1:j k+1 j The definition is inspired by that of (Ji and Eisenstein, 2014), which employs words at the beginning and end of a text span as important features to build an RST tree. → − ut is the vector representation of a t-th atomic unit ut , which is defined on the basis of SIF (Arora et al., 2017)2 as follows: X a → − → − ut = w. (3) p(w) + a w∈Wt − p(w) is the occurrence probability for word w, → w is the vector representation of the word and Wt is a set of words in ut . We use a concatenation of two word vectors obtained from ELMo (Peters et al., 2018) and Glove (Pennington et al., 2014) as a vector representation of a word. a is a parameter to decay the score o"
D19-1587,P17-1092,0,0.0131247,"erging two adjacent spans into a large one. Experimental results on English and German RST treebanks showed that our parser based on span merging achieved the best score, around 0.8 F1 score, which is close to the scores of the previous supervised parsers. 1 Introduction Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one of the theories that are most widely utilized for representing a discourse structure of a text in downstream NLP applications, such as automatic summarization (Marcu, 1998; Hirao et al., 2013), sentiment analysis (Bhatia et al., 2015), and text categorization (Ji and Smith, 2017). RST represents a text as a kind of constituent tree, whose leaves are Elementary Discourse Units (EDUs), clause-like units, and whose non-terminal nodes cover text spans consisting of a sequence of EDUs or a singleton EDU. The label of a non-terminal node represents the attribution of a text span, nucleus or satellite. A discourse relation is also assigned between two adjacent nonterminal nodes. Since the RST tree can be regarded as a standard constituent (phrase structure) tree, syntactic parsing models for constituency parsing can also be successfully applied to RST parsing. In most cases,"
D19-1587,W04-3250,0,0.0351274,"PCC contains two invalid RST trees, maz-12666 and maz-8838, we excluded them. Dataset Gran. Split Merge RB D2E .602 RST-DT D2S2E .755 D2P2S2E .793 .656 .788 .811 .545 .751 .803 D2E .656 D2S2E .757 D2P2S2E .787 .669 .760 .784 .626 .749 .789 PCC Table 2: Micro Span F1 scores for RST-DT and PCC. 3.2 Results Table 2 shows the evaluation results. Merge outperformed Split in most cases, and D2P2S2E achieved the best scores among our variants on both RST-DT and PCC. To clearly show the differences between our proposed method and RB, we performed significance tests, using paired bootstrap resampling (Koehn, 2004) at significance level=0.05. The results showed that there were significant differences between our method and RB at all the settings (D2E, D2S2E, D2P2S2E) for English, and at D2E and D2S2E for German, while there were no significant differences at D2P2S2E for German. The results imply that merging two adjacent spans and dividing with three granularity levels is suitable for unsupervised RST parsing. Comparing our methods with the baseline, right-branching (RB), we could find larger differences without considering the granularity levels of a document, whereas the differences became smaller by"
D19-1587,W98-1124,0,0.390101,"nes. The second builds the optimal tree in terms of a similarity score function that is defined for merging two adjacent spans into a large one. Experimental results on English and German RST treebanks showed that our parser based on span merging achieved the best score, around 0.8 F1 score, which is close to the scores of the previous supervised parsers. 1 Introduction Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one of the theories that are most widely utilized for representing a discourse structure of a text in downstream NLP applications, such as automatic summarization (Marcu, 1998; Hirao et al., 2013), sentiment analysis (Bhatia et al., 2015), and text categorization (Ji and Smith, 2017). RST represents a text as a kind of constituent tree, whose leaves are Elementary Discourse Units (EDUs), clause-like units, and whose non-terminal nodes cover text spans consisting of a sequence of EDUs or a singleton EDU. The label of a non-terminal node represents the attribution of a text span, nucleus or satellite. A discourse relation is also assigned between two adjacent nonterminal nodes. Since the RST tree can be regarded as a standard constituent (phrase structure) tree, synt"
D19-1587,D14-1162,0,0.0979854,"− − → − r = [u ; → u ]. k+1:j k+1 j The definition is inspired by that of (Ji and Eisenstein, 2014), which employs words at the beginning and end of a text span as important features to build an RST tree. → − ut is the vector representation of a t-th atomic unit ut , which is defined on the basis of SIF (Arora et al., 2017)2 as follows: X a → − → − ut = w. (3) p(w) + a w∈Wt − p(w) is the occurrence probability for word w, → w is the vector representation of the word and Wt is a set of words in ut . We use a concatenation of two word vectors obtained from ELMo (Peters et al., 2018) and Glove (Pennington et al., 2014) as a vector representation of a word. a is a parameter to decay the score of frequent words and is defined as a = (1 − α)/(αZ), where α is a hyper parameter and Z is the total number of words. 2.3 Dynamic Programming-Based Approach for Building Optimal Trees We propose a dynamic programming-based approach to obtain the optimal tree in terms of either the total split or merge score from all the possible trees. We first illustrate the algorithm with a merge (similarity) score. We define V [b][e], which stores the maximum merge score for a span ub:e consisting from b-th unit to e-th unit, as fol"
D19-1587,N18-1202,0,0.0113315,"`i:k = [→ ui ; − u k (2) − − − → − − → − r = [u ; → u ]. k+1:j k+1 j The definition is inspired by that of (Ji and Eisenstein, 2014), which employs words at the beginning and end of a text span as important features to build an RST tree. → − ut is the vector representation of a t-th atomic unit ut , which is defined on the basis of SIF (Arora et al., 2017)2 as follows: X a → − → − ut = w. (3) p(w) + a w∈Wt − p(w) is the occurrence probability for word w, → w is the vector representation of the word and Wt is a set of words in ut . We use a concatenation of two word vectors obtained from ELMo (Peters et al., 2018) and Glove (Pennington et al., 2014) as a vector representation of a word. a is a parameter to decay the score of frequent words and is defined as a = (1 − α)/(αZ), where α is a hyper parameter and Z is the total number of words. 2.3 Dynamic Programming-Based Approach for Building Optimal Trees We propose a dynamic programming-based approach to obtain the optimal tree in terms of either the total split or merge score from all the possible trees. We first illustrate the algorithm with a merge (similarity) score. We define V [b][e], which stores the maximum merge score for a span ub:e consisting"
D19-1587,stede-neumann-2014-potsdam,0,0.0280019,"e calculated on the basis of their distributional representations. Note that since our method is fully unsupervised, the parser predicts only the skeleton of an RST tree. Moreover, we exploit multiple granularity levels of a document: (1) we first independently build three types of trees, a tree whose leaves correspond to a paragraph, a tree whose leaves correspond to a sentence, and a tree whose leaves correspond to an EDU, and then (2) merge them to obtain the whole RST tree. We conducted experimental evaluation on English and German datasets, RST-DT and the Potsdam Commentary Corpus (PCC) (Stede and Neumann, 2014), respectively. The results demonstrated that our method with span merging outperformed our method with span splitting and ob5797 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5797–5802, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics tained .811 and .784 span scores for RST-DT and PCC, respectively. The scores are close to the scores of early supervised RST parsers. 2 Unsupervised RST Parsing 2.1 Motivation Generally, RST trees with m"
D19-1587,P17-2029,0,0.160955,"RST trees contain multi-nuclear relations in PCC is 0.712, while that in RST-DT is 0.464. We can obtain the information of sentences boundaries in most cases, however, sometimes we cannot obtain the information of paragraph boundaries. Thus, it is significant that our method outperformed baselines on D2S2E settings. Moreover, we compared our method with some supervised RST parsers on the test set of RSTDT. Table 3 shows the results. In the table, HHN16 denotes a simple transition-based RST parser (Hayashi et al., 2016), and WLW17 denotes a current state-of-the-art transition-based RST parser (Wang et al., 2017). Although the score of our method is lower than the scores of the supervised parsers, the score is close to that of HHN16. The results demonstrated the effectiveness of our unsupervised RST parsing method. For reference, (Braud et al., 2017) reported that their supervised RST parser obtained .802 span F1 score on PCC. However, we cannot compare the score with our score because the test set differs from ours. 4 Conclusion This paper proposed two kinds of unsupervised RST parsing methods based on dynamic programming that can build the optimal RST tree in terms of either a span splitting score o"
D19-6505,W04-3250,0,0.134895,"Missing"
D19-6505,L18-1275,0,0.138657,"ce to be translated, by constructing an encoder that is based on explicit coreference relations. The proposed model can directly take into account relationships between sentences via a graph structured encoder constructed with a coreference resolution toolkit. Therefore, it does not need to attend to all input tokens. This characteristic enables our proposed model to handle more sentences in a step, compared with the previous models, and it may improve translation quality when a source text has many sentences. Experimental results on English-to-Japanese translation pairs in OpenSubtitles2018 (Lison et al., 2018) show that our proposed model can significantly improve the previous model in terms of BLEU scores. In addition, we observe that our model is especially effective in translating a sentence which is a part of a long text, compared to the previous model. We present neural machine translation models for translating a sentence in a text by using a graph-based encoder which can consider coreference relations provided within the text explicitly. The graph-based encoder can dynamically encode the source text without attending to all tokens in the text. In experiments, our proposed models provide stat"
D19-6505,D15-1166,0,0.0610526,"Ti . The concatenated token sequence is represented as: p (yi |y1 , . . . , yi−1 , x) = sof tmax(g (si , di )), si = dec (si−1 , emb(yi−1 ), di ) , di = Tx X a (si−1 , hj ) hj , (1) j=1 ht = enc (emb(xt ), ht−1 , ht+1 ) , N (x11 , · · · , x1T1 , x21 , · · · , x2T2 , · · · , xN 1 , · · · , xTN ). where i is the position of an output token, t is the position of an input token, emb(·) is a function that returns the embedding of an input word, g is a 2-layer feedforward neural network (FFNN), dec is a decoder forward-LSTM, enc is an encoder bidirectional-LSTM (Bi-LSTM), and a is a dot attention (Luong et al., 2015) for calculating the attention weight. 3 Coreference Resolution (2) The coreference resolution system extracts Nc clusters of coreferring mentions (c1 , · · · , cNc ), which are defined as: ck = (maink , subk ), (3) where maink is a span of the representative mention in a cluster of coreferring mentions, and subk is a span of another mention in the cluster.2 In general, because many mentions are in a single cluster, the same maink is sometimes paired to different mentions. To use coreference relations in our graph-based encoder, we need to consider word-based coreference relations. Let head(·)"
D19-6505,D18-1325,0,0.0176744,"anslated one by one. In contrast to this premise, real sentences are often an element of a larger unit, such as a document. This means that a sentence is not always semantically self-contained in itself. To correctly interpret a sentence which is a part of a document, it is important to consider its context, preceding and/or succeeding sentences. In order to tackle the problem, Seq2Seq models that can receive two sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Wang et al., 2017) have been utilized. For capturing multiple-sentence information more effectively, Miculicich et al. (2018); Zhang et al. (2018) incorporated document-level attention modules into Seq2Seq models. Stojanovski and Fraser (2018) proposed a Seq2Seq model which can capture antecedents of pronouns 2 Sequence-to-Sequence Model In this section, we explain the standard Seq2Seq model proposed by Bahdanau et al. (2014), which 45 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 45–50 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics y11 → −1 s1 Output Sentences Decoder y21 → −1 s2 y31 → −1 s3 y12 → −2 s1 y22 → −2 s2 − → h21 ← − h21"
D19-6505,N18-1118,0,0.219887,"-toSequence (Seq2Seq) models (Bahdanau et al., 2014). Most Seq2Seq models are used based on the premise that each sentence is independently translated one by one. In contrast to this premise, real sentences are often an element of a larger unit, such as a document. This means that a sentence is not always semantically self-contained in itself. To correctly interpret a sentence which is a part of a document, it is important to consider its context, preceding and/or succeeding sentences. In order to tackle the problem, Seq2Seq models that can receive two sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Wang et al., 2017) have been utilized. For capturing multiple-sentence information more effectively, Miculicich et al. (2018); Zhang et al. (2018) incorporated document-level attention modules into Seq2Seq models. Stojanovski and Fraser (2018) proposed a Seq2Seq model which can capture antecedents of pronouns 2 Sequence-to-Sequence Model In this section, we explain the standard Seq2Seq model proposed by Bahdanau et al. (2014), which 45 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 45–50 c Hong Kong, China, November 3, 2019. 2"
D19-6505,P02-1040,0,0.10417,"ces and generating a single sentence, proposed by Bawden et al. (2018) (Concat)5 . We compared our proposed models, Coref-mean (Cor-m) and Coref-gate (Cor-g), with the baseline. In order to evaluate the effectiveness of succeeding sentences, we also experimented with the cases of inputting the same number of preceding and succeeding sentences for the target sentence to be translated at the center, for Cor-g. We denote this setting as Coref-gatecentered (Cor-g-c). The number of weight parameters for each model is 111,057k for the baseline and Cor-m, and 111,558k for Cor-g. We used BLEU scores (Papineni et al., 2002) to evaluate the translation performance for each model. All reported BLEU scores in the experiments are averages for three times and are based on MeCab tokenization. Significance tests were conducted by paired bootstrap resampling (Koehn, 2004) with multevel (Clark et al., 2011)6 . 5 In our preliminary comparison, there are no statistically significant differences in translation performances between Concat and the method of inputting and outputting concatenated multiple sentences, also proposed by Bawden et al. (2018). From the computational efficiency perspective, therefore, we chose Concat"
D19-6505,P11-2031,0,0.0309604,"putting the same number of preceding and succeeding sentences for the target sentence to be translated at the center, for Cor-g. We denote this setting as Coref-gatecentered (Cor-g-c). The number of weight parameters for each model is 111,057k for the baseline and Cor-m, and 111,558k for Cor-g. We used BLEU scores (Papineni et al., 2002) to evaluate the translation performance for each model. All reported BLEU scores in the experiments are averages for three times and are based on MeCab tokenization. Significance tests were conducted by paired bootstrap resampling (Koehn, 2004) with multevel (Clark et al., 2011)6 . 5 In our preliminary comparison, there are no statistically significant differences in translation performances between Concat and the method of inputting and outputting concatenated multiple sentences, also proposed by Bawden et al. (2018). From the computational efficiency perspective, therefore, we chose Concat as our baseline. 6 https://github.com/jhclark/multeval 7 These results are close to the reported BLEU scores of the Ja-En caption translations in Pryzant et al. (2018) 48 are anaphora, and cataphora is rarely observed in the test set. Ignoring the succeeding sentences, Cor-g-c at"
D19-6505,L18-1182,0,0.0667736,"Missing"
D19-6505,D16-1245,0,0.0257192,"nce relationships can be effectively utilized. Figure1 shows the network structure of our proposed model. At first, input sentences are analyzed by using a coreference resolution system. After that, the encoder part is structured based on the coreference resolution results, and the input text is encoded into hidden states. Then, the hidden states are converted to a translated text via attention distributions and the decoder. During the translation, the attention distri0 xij 0 = tail(maink ), xij = head(subk ). (4) 1 https://github.com/huggingface/neuralcoref. This code is based on the work by Clark and Manning (2016). 2 We treat a nominal noun which is the antecedent of a pronoun or a proper noun as a representative mention. 46 refers to a word tail(main1 ) → − i0 i0 h j 0 . βj 0 is calculated as follows: head(sub1 ) I have two daughters . They are · · · refers to a span main1 → − 0 → − 0 βji 0 = sigmoid(Wt h ij 0 + Ws h it−1 ), sub1 Figure 2: An example of a word-based coreference relation. where Wt and Ws are weight matrices. The backward encoding is similarly processed by replacing reff with refb . Finally, the forward and backward hidden states are concatenated to → − ← − hit = [ h it ; h it ] for eac"
D19-6505,W17-4811,0,0.0294308,"tically improved with Sequence-toSequence (Seq2Seq) models (Bahdanau et al., 2014). Most Seq2Seq models are used based on the premise that each sentence is independently translated one by one. In contrast to this premise, real sentences are often an element of a larger unit, such as a document. This means that a sentence is not always semantically self-contained in itself. To correctly interpret a sentence which is a part of a document, it is important to consider its context, preceding and/or succeeding sentences. In order to tackle the problem, Seq2Seq models that can receive two sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Wang et al., 2017) have been utilized. For capturing multiple-sentence information more effectively, Miculicich et al. (2018); Zhang et al. (2018) incorporated document-level attention modules into Seq2Seq models. Stojanovski and Fraser (2018) proposed a Seq2Seq model which can capture antecedents of pronouns 2 Sequence-to-Sequence Model In this section, we explain the standard Seq2Seq model proposed by Bahdanau et al. (2014), which 45 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 45–50 c Hong Kong, China"
D19-6505,P18-1117,0,0.019327,"models (Bahdanau et al., 2014). Most Seq2Seq models are used based on the premise that each sentence is independently translated one by one. In contrast to this premise, real sentences are often an element of a larger unit, such as a document. This means that a sentence is not always semantically self-contained in itself. To correctly interpret a sentence which is a part of a document, it is important to consider its context, preceding and/or succeeding sentences. In order to tackle the problem, Seq2Seq models that can receive two sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Wang et al., 2017) have been utilized. For capturing multiple-sentence information more effectively, Miculicich et al. (2018); Zhang et al. (2018) incorporated document-level attention modules into Seq2Seq models. Stojanovski and Fraser (2018) proposed a Seq2Seq model which can capture antecedents of pronouns 2 Sequence-to-Sequence Model In this section, we explain the standard Seq2Seq model proposed by Bahdanau et al. (2014), which 45 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 45–50 c Hong Kong, China, November 3, 2019. 2019 Association for"
D19-6505,D17-1301,0,0.0191692,"al., 2014). Most Seq2Seq models are used based on the premise that each sentence is independently translated one by one. In contrast to this premise, real sentences are often an element of a larger unit, such as a document. This means that a sentence is not always semantically self-contained in itself. To correctly interpret a sentence which is a part of a document, it is important to consider its context, preceding and/or succeeding sentences. In order to tackle the problem, Seq2Seq models that can receive two sentences (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Wang et al., 2017) have been utilized. For capturing multiple-sentence information more effectively, Miculicich et al. (2018); Zhang et al. (2018) incorporated document-level attention modules into Seq2Seq models. Stojanovski and Fraser (2018) proposed a Seq2Seq model which can capture antecedents of pronouns 2 Sequence-to-Sequence Model In this section, we explain the standard Seq2Seq model proposed by Bahdanau et al. (2014), which 45 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 45–50 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Lingui"
D19-6505,D18-1049,0,0.0313502,"Missing"
E06-1026,P97-1023,0,0.684876,"n Technology for affect analysis of texts has recently gained attention in both academic and industrial areas. It can be applied to, for example, a survey of new products or a questionnaire analysis. Automatic sentiment analysis enables a fast and comprehensive investigation. The most fundamental step for sentiment analysis is to acquire the semantic orientations of words: desirable or undesirable (positive or negative). For example, the word “beautiful” is positive, while the word “dirty” is negative. Many researchers have developed several methods for this purpose and obtained good results (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Takamura et al., 2005; Kobayashi et al., 2001). One of the next problems to be solved is to acquire semantic orientations of phrases, or multi-term expressions. No computational model for semantically oriented phrases has been proposed so far although some researchers have used techniques developed for single words. The purpose of this paper is to propose computational models for phrases with semantic orientations as well as classification methods based on the models. Indeed the semantic orientations of phrases depend on context just as the seman"
E06-1026,J93-1007,0,0.0445408,"incorporate the unlabeled data in the classification of 3-term evaluative expressions. They focused on the utilization of context information such as neighboring words and emoticons. Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words is used. The three methods above are based on context information. In contrast, our method exploits the internal structure of the semantic orientations of phrases. Inui (2004) introduced an attribute plus/minus for each word and proposed several rules that determine the semantic orientations of phrases on the basis of the plus/minus attribute values and the positive/negative attribute values of the component words."
E06-1026,P05-1017,1,0.933574,"ademic and industrial areas. It can be applied to, for example, a survey of new products or a questionnaire analysis. Automatic sentiment analysis enables a fast and comprehensive investigation. The most fundamental step for sentiment analysis is to acquire the semantic orientations of words: desirable or undesirable (positive or negative). For example, the word “beautiful” is positive, while the word “dirty” is negative. Many researchers have developed several methods for this purpose and obtained good results (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Takamura et al., 2005; Kobayashi et al., 2001). One of the next problems to be solved is to acquire semantic orientations of phrases, or multi-term expressions. No computational model for semantically oriented phrases has been proposed so far although some researchers have used techniques developed for single words. The purpose of this paper is to propose computational models for phrases with semantic orientations as well as classification methods based on the models. Indeed the semantic orientations of phrases depend on context just as the semantic orientations of words do, but we would like to obtain the most ba"
E06-1026,kamps-etal-2004-using,0,0.0821696,"Missing"
E06-1026,P02-1053,0,0.141721,"res in document classification according to semantic orientation. Pang et al. (2002) used bigrams. Matsumoto et al. (2005) used sequential patterns and tree patterns. Although such patterns were proved to be effective in document classification, the semantic orientations of the patterns themselves are not considered. Suzuki et al. (2006) used the ExpectationMaximization algorithm and the naive bayes classifier to incorporate the unlabeled data in the classification of 3-term evaluative expressions. They focused on the utilization of context information such as neighboring words and emoticons. Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words i"
E06-1026,H05-1044,0,0.109176,"method is similar to Turney’s in the sense that cooccurrence with seed words is used. The three methods above are based on context information. In contrast, our method exploits the internal structure of the semantic orientations of phrases. Inui (2004) introduced an attribute plus/minus for each word and proposed several rules that determine the semantic orientations of phrases on the basis of the plus/minus attribute values and the positive/negative attribute values of the component words. For example, a rule [negative+minus=positive] determines “low (minus) risk (negative)” to be positive. Wilson et al. (2005) worked on phrase-level semantic orientations. They introduced a polarity shifter, which is almost equivalent to the plus/minus attribute above. They manually created the list of polarity shifters. The method that we propose in this paper is an automatic version of Inui’s or Wilson et al.’s idea, in the sense that the method automatically creates word clusters and their polarity shifters. 3 Latent Variable Models for Semantic Orientations of Phrases As mentioned in the Introduction, the semantic orientation of a phrase is not a mere sum of its component words. If we know that “low risk” is pos"
E06-1026,W02-1011,0,0.0455765,"ours, and also from Torisawa’s, in that a probabilistic model is used for feature extraction. 2.2 Identification of Semantic Orientations The semantic orientation classification of words has been pursued by several researchers (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Takamura et al., 2005). However, no computational model for semantically oriented phrases has been proposed to date although research for a similar purpose has been proposed. Some researchers used sequences of words as features in document classification according to semantic orientation. Pang et al. (2002) used bigrams. Matsumoto et al. (2005) used sequential patterns and tree patterns. Although such patterns were proved to be effective in document classification, the semantic orientations of the patterns themselves are not considered. Suzuki et al. (2006) used the ExpectationMaximization algorithm and the naive bayes classifier to incorporate the unlabeled data in the classification of 3-term evaluative expressions. They focused on the utilization of context information such as neighboring words and emoticons. Turney (2002) applied an internet-based technique to the semantic orientation classi"
E06-1026,H05-1043,0,0.0543259,"– 67.02 0.73 91.7 81.39 0.60 174.0 81.94 0.64 60.0 76 74 72 70 68 66 64 62 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 beta Figure 4: U-shaped model with standard dataset This kind of ellipsis often occurs in Japanese. 206 Table 2: Contingency table of classification result by the U-shaped model Gold standard positive neutral negative sum U-shaped model positive neutral negative 1856 281 69 202 2021 394 102 321 2335 2160 2623 2798 identified. To tackle these examples, we will need methods for correctly identifying attributes and objects. Some researchers are starting to work on this problem (e.g., Popescu and Etzioni (2005)). We succeeded in addressing the data-sparseness problem by introducing a latent variable. However, this problem still causes some errors. Precise statistics cannot be obtained for infrequent words. This problem will be solved by incorporating other resources such as thesaurus or a dictionary, or combining our method with other methods using external wider contexts (Suzuki et al., 2006; Turney, 2002; Baron and Hirst, 2004). 4.3 Examples of Obtained Clusters Next, we qualitatively evaluate the proposed methods. For several clusters z, we extract the words that occur more than twice in the whol"
E06-1026,H05-2017,0,\N,Missing
E09-1089,W04-1013,0,0.687494,"s at most K. This problem is called maximum coverage problem with knapsack constraint (MCKP), which is an NP-hard problem (Khuller et al., 1999). We should note that MCKP is different from a knapsack problem. MCKP merely has a constraint of knapsack form. Filatova and Hatzivassiloglou (2004) pointed out that text summarization can be formalized by MCKP. The performance of the method depends on how to represent words and which words to use. We represent words with their stems. We use only the words that are content words (nouns, verbs, or adjectives) and not in the stopword list used in ROUGE (Lin, 2004). The weights wj of words are also an important factor of good performance. We tested two weighting schemes proposed by Yih et al. (2007). The ﬁrst one is interpolated weights, which are interpolated values of the generative word probability in the entire document and that in the beginning part of the document (namely, the ﬁrst 100 words). Each probability is estimated with the maximum likelihood principle. The second one is trained weights. These values are estimated by the logistic regression trained on data instances, which are labeled 1 if the word appears in a summary in the training data"
E09-1089,W01-0100,0,0.620623,"andomized algorithm, and a branch-andbound method. On the basis of the results of comparative experiments, we also augment the summarization model so that it takes into account the relevance to the document cluster. Through experiments, we showed that the augmented model is superior to the best-performing method of DUC’04 on ROUGE-1 without stopwords. 1 Introduction Automatic text summarization is one of the tasks that have long been studied in natural language processing. This task is to create a summary, or a short and concise document that describes the content of a given set of documents (Mani, 2001). One well-known approach to text summarization is the extractive method, which selects some linguistic units (e.g., sentences) from given documents in order to generate a summary. The extractive method has an advantage that the grammaticality is guaranteed at least at the level of the linguistic units. Since the actual generation of linguistic expressions has not achieved the level of the practical use, we focus on the extractive method in this paper, especially the method based on the sentence extraction. Most of the extractive summarization methods rely on sequentially solving binary classi"
E09-1089,N07-1056,0,0.0405934,"future work, we will try other conceptual units such as basic elements (Hovy et al., 2006) proposed for summary evaluation. We also plan to include compressed sentences into the set of candidate sentences to be selected as done by Yih et al. (2007). We also plan to design other decoding algorithms for text summarization (e.g., pipage approach (Ageev and Sviridenko, 2004)). As discussed in Section 6.2, integration with similaritybased models is worth consideration. We will incorporate techniques for arranging sentences into an appropriate order, while the current work concerns only selection. Deshpande et al. (2007) proposed a selection and ordering technique, which is applicable only to the unit cost case such as selection and ordering of words for title generation. We plan to reﬁne their model so that it can be applied to general text summarization. Table 7: ROUGE-1 of MCKP-Rel with byte constraints, evaluated without stopwords. Underlined are the values signiﬁcantly different from peer65. greedy g-greedy rand100k stack30 exact exactopt peer65 interpolated train 0.374 (0.1) 0.377 (0.4) 0.371 (0.0) 0.385 (0.2) 0.373 (0.2) 0.366 (0.3) 0.384 (0.1) 0.386 (0.3) 0.383 (0.3) 0.384 (0.4) 0.385 (0.1) 0.384 (0.4"
E09-1089,C04-1057,0,0.873064,"uster. They used the centroid to rank sentences, together with the MMR-like redundancy score. Both relevance and redundancy are taken into consideration, but no global viewpoint is given. In CLASSY, which is the best-performing method in DUC’04, Conroy et al. (2004) scored sentences with the sum of tf-idf scores of words. They also incorporated sentence compression based on syntactic or heuristic rules. McDonald (2007) formulated text summarization as a knapsack problem and obtained the global solution and its approximate solutions. Its relation to our method will be discussed in Section 6.1. Filatova and Hatzivassiloglou (2004) ﬁrst formulated text summarization as MCKP. Their decoding method is a greedy one and will be empirically compared with other decoding methods in this paper. Yih et al. (2007) used a slightlymodiﬁed stack decoding. The optimization problem they solved was the MCKP with the last sentence truncation. Their stack decoding is one of the decoding methods discussed in this paper. Ye et al. (2007) is another example of coverage-based methods. Shen et al. (2007) regarded summarization as a sequential labelling task and solved it with Conditional Random Fields. Although the model is globally optimized"
E09-1089,W00-0405,0,0.544217,"Missing"
E09-1089,hovy-etal-2006-automated,0,0.204041,"nguistics 781 Hatzivassiloglou, 2004), which compose the meaning of a sentence. Sentence si is represented by a set of conceptual units {ei1 , · · · , ei|si |}. For example, the sentence “The man bought a book and read it” could be regarded as consisting of two conceptual units “the man bought a book” and “the man read the book”. It is not easy, however, to determine the appropriate granularity of conceptual units. A simple way would be to regard the above sentence as consisting of four conceptual units “man”, “book”, “buy”, and “read”. There is some work on the deﬁnition of conceptual units. Hovy et al. (2006) proposed to use basic elements, which are dependency subtrees obtained by trimming dependency trees. Although basic elements were proposed for evaluation of summaries, they can probably be used also for summary generation. However, such novel units have not proved to be useful for summary generation. Since we focus more on algorithms and models in this paper, we simply use words as conceptual units. The goal of text summarization is to cover as many conceptual units as possible using only a small number of sentences. In other words, the goal is to ﬁnd a subset S(⊂ D) that covers as many conce"
E09-1089,N03-1020,0,0.0421846,"documents, are given. One summary is to be generated for each cluster. Following the most relevant previous method (Yih et al., 2007), we set the target length to 100 words. DUC’03 (2003) dataset was used as the training dataset for trained weights. All the documents were segmented into sentences using a script distributed by DUC. Words are stemmed by Porter’s stemmer (Porter, 1980). ROUGE version 1.5.5 (Lin, 2004) was used for evaluation.2 Among others, we focus on ROUGE-1 in the discussion of the result, because ROUGE-1 has proved to have strong correlation with human annotation (Lin, 2004; Lin and Hovy, 2003). Wilcoxon signed rank test for paired samples with signiﬁcance level 0.05 was used for the signiﬁcance test of the difference in ROUGE1. The simplex method and the branch-and-bound method implemented in GLPK (Makhorin, 2006) were used to solve respectively linear and integer programming problems. The methods that are compared here are the greedy algorithm (greedy), the greedy algorithm with performance guarantee (g-greedy), the randomized algorithm (rand), the stack decoding (stack), and the branch-and-bound method (exact). Table 2: ROUGE of MCKP with trained weights. Underlined ROUGE-1 score"
H05-1019,W04-1003,0,0.0188577,"or topics 7-12, 13-18 and 25-30 on Ó  , Ó , and Ó .  respectively. Note that each human subject, A to E, was a retired professional journalist; that is, they shared a common background. Table 3 shows the Pearson’s correlation coefficient ( ) and Spearman’s rank correlation coefficient Ö for the human subjects. The results show that every pair has a high correlation. Therefore, changing the human subject has little influence as regards creating references and evaluating system summaries. The evaluation by human subjects is stable. This result agrees with DUC’s additional evaluation results (Harman and Over, 2004). However, the behavior of the correlations between humans with different backgrounds is uncertain. The correlation might be fragile if we introduce a human subject whose background is different from the others. 4.3 Compared Automatic Evaluation Methods We compared our method with ROUGE-N and ROUGE-L described below. We used only content words to calculate the ROUGE scores because the correlation coefficient decreased if we did not remove functional words. WSK-based method We use WSK instead of ESK in equation (6)-(8).   ;=&gt;Z1ADC ;=&gt;Z1A (16)  Here, function SU returns the number of"
H05-1019,C04-1077,1,0.811115,"luations require a huge effort and the cost is considerable. Moreover, we cannot automatically evaluate a new system even if we use the corpora built for these workshops, and we cannot conduct reevaluation experiments. To cope with this situation, there is a particular need to establish a high quality automatic evaluation method. Once this is done, we can expect great progress to be made on natural language generation. In this paper, we propose a novel automatic evaluation method for natural language generation technologies. Our method is based on the Extended String Subsequence Kernel (ESK) (Hirao et al., 2004b) which is a kind of convolution kernel (Collins and Duffy, 2001). ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations. We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 (Hirao et al., 2004a). The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more r"
H05-1019,C04-1064,1,0.847492,"luations require a huge effort and the cost is considerable. Moreover, we cannot automatically evaluate a new system even if we use the corpora built for these workshops, and we cannot conduct reevaluation experiments. To cope with this situation, there is a particular need to establish a high quality automatic evaluation method. Once this is done, we can expect great progress to be made on natural language generation. In this paper, we propose a novel automatic evaluation method for natural language generation technologies. Our method is based on the Extended String Subsequence Kernel (ESK) (Hirao et al., 2004b) which is a kind of convolution kernel (Collins and Duffy, 2001). ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations. We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 (Hirao et al., 2004a). The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more r"
H05-1019,N03-1020,0,0.306123,"eneration. In this paper, we propose a novel automatic evaluation method for natural language generation technologies. Our method is based on the Extended String Subsequence Kernel (ESK) (Hirao et al., 2004b) which is a kind of convolution kernel (Collins and Duffy, 2001). ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations. We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 (Hirao et al., 2004a). The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more robust. 2 Related Work Automatic evaluation methods for automatic summarization and machine translation are grouped into two classes. One is the longest common subsequence (LCS) based approach (Hori et al., 2003; Lin, 2004a; Lin, 2004b; Lin and Och, 2004). The other is the N-gram based approach (Papineni et al., 145 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natu"
H05-1019,P04-1077,0,0.382558,"hod for natural language generation technologies. Our method is based on the Extended String Subsequence Kernel (ESK) (Hirao et al., 2004b) which is a kind of convolution kernel (Collins and Duffy, 2001). ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations. We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 (Hirao et al., 2004a). The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more robust. 2 Related Work Automatic evaluation methods for automatic summarization and machine translation are grouped into two classes. One is the longest common subsequence (LCS) based approach (Hori et al., 2003; Lin, 2004a; Lin, 2004b; Lin and Och, 2004). The other is the N-gram based approach (Papineni et al., 145 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 145–152, Vancouver, Octo"
H05-1019,W04-1013,0,0.385578,"aper, we propose a novel automatic evaluation method for natural language generation technologies. Our method is based on the Extended String Subsequence Kernel (ESK) (Hirao et al., 2004b) which is a kind of convolution kernel (Collins and Duffy, 2001). ESK allows us to calculate the similarities between a pair of texts taking account of word sequences, their word sense sequences and their combinations. We conducted an experimental evaluation using automatic summarization evaluation data developed for TSC-3 (Hirao et al., 2004a). The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more robust. 2 Related Work Automatic evaluation methods for automatic summarization and machine translation are grouped into two classes. One is the longest common subsequence (LCS) based approach (Hori et al., 2003; Lin, 2004a; Lin, 2004b; Lin and Och, 2004). The other is the N-gram based approach (Papineni et al., 145 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Languag"
H05-1019,P02-1040,0,0.0948615,"eam 1 0 a-DREAM cosmonaut-my great is my Becoming-DREAM Becoming-SPACEMAN Becoming-a Becoming-ambition 2 Becoming-an Becoming-astronaut Becoming-cosmonaut Becoming-dream Becoming-great 1 1 1 0 1 1     1 0 0 0 0  1   0  0  0 a-SPACEMAN 2 a-cosmonaut a-dream a-great a-is a-my an-DREAM an-SPACEMAN an-ambition an-astronaut an-is an-my 2002; Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b; Soricut and Brill, 2004). Hori et. al (2003) proposed an automatic evaluation method for speech summarization based on word recognition accuracy. They reported that their method is superior to BLEU (Papineni et al., 2002) in terms of the correlation between human assessment and automatic evaluation. Lin (2004a; 2004b) and Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L. They applied ROUGE-L to the evaluation of summarization and machine translation. The results showed that the LCS-based measure is comparable to Ngram-based automatic evaluation methods. However, these methods tend to be strongly influenced by word order. Various N-gram-based methods have been proposed since BLEU, which is now widely used for the evaluation of machine translation. Lin et al. (2003) proposed a"
H05-1019,P04-1078,0,0.0189371,"AN-dream   0 cosmonaut-DREAM   0 an 0 1 SPACEMAN-great cosmonaut-dream  0   0 1 astronaut 0 1 SPACEMAN-is 1 1 cosmonaut-great  0  cosmonaut 1 0 SPACEMAN-my cosmonaut-is 1 0    0  0 dream 1 0 a-DREAM cosmonaut-my great is my Becoming-DREAM Becoming-SPACEMAN Becoming-a Becoming-ambition 2 Becoming-an Becoming-astronaut Becoming-cosmonaut Becoming-dream Becoming-great 1 1 1 0 1 1     1 0 0 0 0  1   0  0  0 a-SPACEMAN 2 a-cosmonaut a-dream a-great a-is a-my an-DREAM an-SPACEMAN an-ambition an-astronaut an-is an-my 2002; Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b; Soricut and Brill, 2004). Hori et. al (2003) proposed an automatic evaluation method for speech summarization based on word recognition accuracy. They reported that their method is superior to BLEU (Papineni et al., 2002) in terms of the correlation between human assessment and automatic evaluation. Lin (2004a; 2004b) and Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L. They applied ROUGE-L to the evaluation of summarization and machine translation. The results showed that the LCS-based measure is comparable to Ngram-based automatic evaluation methods. However, these methods tend"
I05-1005,W04-0907,0,0.155182,"the semantic relationship between the relative clause and its head noun has several application domains, such as machine translation from Japanese[5]. It also has a place in text understanding tasks, such as splitting a long sentence into multiple shorter sentences, and removing less important clauses to shorten a sentence[6]. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 46–57, 2005. c Springer-Verlag Berlin Heidelberg 2005  Corpus-Based Analysis of Japanese Relative Clause Constructions 47 Previously, relative clauses had been analyzed with rule-based methods that utilized case frames[5,2]. Using hand-crafted rules and knowledge creates several problems: the high cost of constructing them, and lower scalability and coverage. Recently, due to the availability of very large corpora, corpus-based and machine learning-based approaches have been actively investigated[7]. Cooccurrence information between nouns and verbs can be calculated from the syntactically parsed corpus, and this information can be used preferentially instead of handcrafted case frames to determine whether a noun can be the ﬁller of a case-slot of a verb[7,11]. However, merely using the cooccurrence information b"
I05-1005,1991.mtsummit-papers.16,0,0.0703773,"Japanese relative clause constructions and a corpus of Japanese relative clause construction instances, we present a machine learning based approach to classifying RCC’s. We present a set of lexical and semantic features that characterize RCC’s, and integrate them as a classiﬁer to determine RCC types. We use decision tree learning as the machine learning algorithm. Distinguishing case-slot gapping and head restrictive relative clauses, or resolving the semantic relationship between the relative clause and its head noun has several application domains, such as machine translation from Japanese[5]. It also has a place in text understanding tasks, such as splitting a long sentence into multiple shorter sentences, and removing less important clauses to shorten a sentence[6]. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 46–57, 2005. c Springer-Verlag Berlin Heidelberg 2005  Corpus-Based Analysis of Japanese Relative Clause Constructions 47 Previously, relative clauses had been analyzed with rule-based methods that utilized case frames[5,2]. Using hand-crafted rules and knowledge creates several problems: the high cost of constructing them, and lower scalability and coverage. Recent"
I05-1005,C02-1122,0,0.133757,"ss important clauses to shorten a sentence[6]. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 46–57, 2005. c Springer-Verlag Berlin Heidelberg 2005  Corpus-Based Analysis of Japanese Relative Clause Constructions 47 Previously, relative clauses had been analyzed with rule-based methods that utilized case frames[5,2]. Using hand-crafted rules and knowledge creates several problems: the high cost of constructing them, and lower scalability and coverage. Recently, due to the availability of very large corpora, corpus-based and machine learning-based approaches have been actively investigated[7]. Cooccurrence information between nouns and verbs can be calculated from the syntactically parsed corpus, and this information can be used preferentially instead of handcrafted case frames to determine whether a noun can be the ﬁller of a case-slot of a verb[7,11]. However, merely using the cooccurrence information between nouns and verbs instead of case frames cannot provide a good solution to the analysis of Japanese relative clauses. Clauses with high occurrence probability of the main verb and the head noun can sometimes be head restrictive. Moreover, just because the head noun can be the"
I05-1038,P01-1070,0,0.0176595,"roughly divided into two groups: the ones based on hand-crafted rules and the ones based on machine learning. The system “SAIQA” [1], Xu et al. [2] used hand-crafted rules for question classiﬁcation. However, methods based on pattern matching have the following two drawbacks: high cost of making rules or patterns by hand and low coverage. Machine learning can be considered to solve these problems. Li et al. [3] used SNoW for question classiﬁcation. The SNoW is a multi-class classiﬁer that is speciﬁcally tailored for learning in the presence of a very large number of features. Zukerman et al. [4] used decision tree. Ittycheriah et al. [5] used maximum entropy. Suzuki [6] used Support Vector Machines (SVMs). Suzuki [6] compared question classiﬁcation using machine learning methods (decision tree, maximum entropy, SVM) with a rule-based method. The result showed that the accuracy of question classiﬁcation with SVM is the highest of all. According to Suzuki [6], a lot of information is needed to improve the accuracy of question classiﬁcation and SVM is suitable for question classiﬁcation, because SVM can classify questions with high accuracy even when the dimension of the feature space i"
I05-1038,N01-1005,0,0.0709723,"based on hand-crafted rules and the ones based on machine learning. The system “SAIQA” [1], Xu et al. [2] used hand-crafted rules for question classiﬁcation. However, methods based on pattern matching have the following two drawbacks: high cost of making rules or patterns by hand and low coverage. Machine learning can be considered to solve these problems. Li et al. [3] used SNoW for question classiﬁcation. The SNoW is a multi-class classiﬁer that is speciﬁcally tailored for learning in the presence of a very large number of features. Zukerman et al. [4] used decision tree. Ittycheriah et al. [5] used maximum entropy. Suzuki [6] used Support Vector Machines (SVMs). Suzuki [6] compared question classiﬁcation using machine learning methods (decision tree, maximum entropy, SVM) with a rule-based method. The result showed that the accuracy of question classiﬁcation with SVM is the highest of all. According to Suzuki [6], a lot of information is needed to improve the accuracy of question classiﬁcation and SVM is suitable for question classiﬁcation, because SVM can classify questions with high accuracy even when the dimension of the feature space is large. Moreover, Zhang et al. [7] compare"
I05-1038,C02-1150,0,\N,Missing
I08-1019,W99-0625,0,0.0190509,"From the table, we can see a large difference in distributions of EQ and no-relation pairs. This difference suggests that the clusterwise classification approach is reasonable. We split the dataset into three clusters: highsimilarity cluster, intermediate-similarity cluster, and low-similarity cluster. Intuitively, we expected that a pair in the high-similarity cluster would have many common bigrams, that a pair in the intermediate-similarity cluster would have many common unigrams but few common bigrams, and that a pair in the low-similarity cluster would have few common unigrams or bigrams. Hatzivassiloglou et al. (1999; 2001) proposed a method based on supervised machine learning to identify whether two paragraphs contain similar information. However, we found it was difficult to accurately identify EQ pairs between two sentences simply by using similarities as features. Zhang et al. (2003) presented a method of classifying CST relations between sentence pairs. However, their method used the same features for every type of CST, resulting in low recall and precision. We thus select better features for each CST type, and for each cluster of EQ. The EQ identification task is apparently related to Textual Entai"
I08-1019,P02-1047,0,0.043184,"and then construct a classifier for each cluster to identify equivalence relations. We also adopt a “coarse-to-fine” approach. We further propose using the identified equivalence relations to address the task of identifying transition relations. 1 Introduction A document generally consists of semantic units called sentences and various relations hold between them. The analysis of the structure of a document by identifying the relations between sentences is called discourse analysis. The discourse structure of one document has been the target of the traditional discourse analysis (Marcu, 2000; Marcu and Echihabi, 2002; Yokoyama et al., 2003), based on rhetorical structure theory (RST) (Mann and Thompson, 1987). § Yasunari Miyabe currently works at Toshiba Solutions Corporation. 141 1. ABC telephone company announced on the 9th that the number of users of its mobile-phone service had reached one million. Users can access the Internet, reserve train tickets, as well as make phone calls through this service. 2. ABC said on the 18th that the number of users of its mobile-phone service had reached 1,500,000. This service includes Internet access, and enables train-ticket reservations and telephone calls. The pa"
I08-1019,J00-3005,0,0.0176184,"imilarities, and then construct a classifier for each cluster to identify equivalence relations. We also adopt a “coarse-to-fine” approach. We further propose using the identified equivalence relations to address the task of identifying transition relations. 1 Introduction A document generally consists of semantic units called sentences and various relations hold between them. The analysis of the structure of a document by identifying the relations between sentences is called discourse analysis. The discourse structure of one document has been the target of the traditional discourse analysis (Marcu, 2000; Marcu and Echihabi, 2002; Yokoyama et al., 2003), based on rhetorical structure theory (RST) (Mann and Thompson, 1987). § Yasunari Miyabe currently works at Toshiba Solutions Corporation. 141 1. ABC telephone company announced on the 9th that the number of users of its mobile-phone service had reached one million. Users can access the Internet, reserve train tickets, as well as make phone calls through this service. 2. ABC said on the 18th that the number of users of its mobile-phone service had reached 1,500,000. This service includes Internet access, and enables train-ticket reservations a"
I08-1019,W03-0507,1,0.833972,"th sentences have a verb or not. The head verbs are extracted using rules proposed by Hatayama (2001). 3. Salient words: This feature indicates whether the salient words of the two sentences are the same or not. We approximate the salient word with the gaor the wa-case word that appears first. 4. Numeric expressions and units (Nanba et al., 2005): The first feature indicates whether the two sentences share a numeric expression or not. The second feature is similarly defined for numeric units. 4 Experiments on identifying EQ pairs We used the Text Summarization Challenge (TSC) 2 and 3 corpora (Okumura et al., 2003) and the Workshop on Multimodal Summarization for Trend Information (Must) corpus (Kato et al., 2005). These two corpora contained 115 sets of related news articles (10 documents per set on average) on various events. A document contained 9.9 sentences on average. Etoh et al. (2005) annotated these two corpora with CST types. There were 471,586 pairs of sentences and 798 pairs of these had EQ. We conducted the experiments with 10-fold cross-validation (i.e., approximately 425,000 pairs on average, out of which approximately 700 pairs are in EQ, are in the training dataset for each fold). The a"
I08-1019,W00-1009,0,0.749983,"Missing"
I08-1039,C04-1121,0,0.00875875,"tion 3, we discuss well-known methods that use word-level polarities and describe our motivation. In Section 4, we describe our proposed model, how to train the model, and how to classify sentences using the model. We present our experiments and results in Section 5. Finally in Section 6, we conclude our work and mention possible future work. 2 Related Work Supervised machine learning methods including Support Vector Machines (SVM) are often used in sentiment analysis and shown to be very promising (Pang et al., 2002; Matsumoto et al., 2005; Kudo and Matsumoto, 2004; Mullen and Collier, 2004; Gamon, 2004). One of the advantages of these methods is that a wide variety of features such as dependency trees and sequences of words can easily be incorporated (Matsumoto et al., 2005; Kudo and Matsumoto, 2004; Pang et al., 2002). Our attempt in this paper is not to use the information included in those substructures of sentences, but to use the word-level polarities, which is a resource usually at hand. Thus our work is an instantiation of the idea to use a resource on one linguistic layer (e.g., word level) to the analysis of another layer (sentence level). There have been some pieces of work which f"
I08-1039,W04-3239,0,0.0262567,"ction 2, we briefly present the related work. In Section 3, we discuss well-known methods that use word-level polarities and describe our motivation. In Section 4, we describe our proposed model, how to train the model, and how to classify sentences using the model. We present our experiments and results in Section 5. Finally in Section 6, we conclude our work and mention possible future work. 2 Related Work Supervised machine learning methods including Support Vector Machines (SVM) are often used in sentiment analysis and shown to be very promising (Pang et al., 2002; Matsumoto et al., 2005; Kudo and Matsumoto, 2004; Mullen and Collier, 2004; Gamon, 2004). One of the advantages of these methods is that a wide variety of features such as dependency trees and sequences of words can easily be incorporated (Matsumoto et al., 2005; Kudo and Matsumoto, 2004; Pang et al., 2002). Our attempt in this paper is not to use the information included in those substructures of sentences, but to use the word-level polarities, which is a resource usually at hand. Thus our work is an instantiation of the idea to use a resource on one linguistic layer (e.g., word level) to the analysis of another layer (sentence level). The"
I08-1039,P07-1055,0,0.0758729,"n included in those substructures of sentences, but to use the word-level polarities, which is a resource usually at hand. Thus our work is an instantiation of the idea to use a resource on one linguistic layer (e.g., word level) to the analysis of another layer (sentence level). There have been some pieces of work which focus on multiple levels in text. Mao and Lebanon (2006) proposed a method that captures local sentiment flow in documents using isotonic conditional random fields. Pang and Lee (2004) proposed to eliminate objective sentences before the sentiment classification of documents. McDonald et al. (2007) proposed a model for classifying sentences and documents simultaneously. They experimented with joint classification of subjectivity for sentence-level, 297 and sentiment for document-level, and reported that their model obtained higher accuracy than the standard document classification model. Although these pieces of work aim to predict not sentence-level but document-level sentiments, their concepts are similar to ours. However, all the above methods require annotated corpora for all levels, such as both subjectivity for sentences and sentiments for documents, which are fairly expensive to"
I08-1039,W04-3253,0,0.100844,"t the related work. In Section 3, we discuss well-known methods that use word-level polarities and describe our motivation. In Section 4, we describe our proposed model, how to train the model, and how to classify sentences using the model. We present our experiments and results in Section 5. Finally in Section 6, we conclude our work and mention possible future work. 2 Related Work Supervised machine learning methods including Support Vector Machines (SVM) are often used in sentiment analysis and shown to be very promising (Pang et al., 2002; Matsumoto et al., 2005; Kudo and Matsumoto, 2004; Mullen and Collier, 2004; Gamon, 2004). One of the advantages of these methods is that a wide variety of features such as dependency trees and sequences of words can easily be incorporated (Matsumoto et al., 2005; Kudo and Matsumoto, 2004; Pang et al., 2002). Our attempt in this paper is not to use the information included in those substructures of sentences, but to use the word-level polarities, which is a resource usually at hand. Thus our work is an instantiation of the idea to use a resource on one linguistic layer (e.g., word level) to the analysis of another layer (sentence level). There have been some pieces o"
I08-1039,W02-1011,0,0.0165792,"Missing"
I08-1039,P04-1035,0,0.0690942,"al., 2005; Kudo and Matsumoto, 2004; Pang et al., 2002). Our attempt in this paper is not to use the information included in those substructures of sentences, but to use the word-level polarities, which is a resource usually at hand. Thus our work is an instantiation of the idea to use a resource on one linguistic layer (e.g., word level) to the analysis of another layer (sentence level). There have been some pieces of work which focus on multiple levels in text. Mao and Lebanon (2006) proposed a method that captures local sentiment flow in documents using isotonic conditional random fields. Pang and Lee (2004) proposed to eliminate objective sentences before the sentiment classification of documents. McDonald et al. (2007) proposed a model for classifying sentences and documents simultaneously. They experimented with joint classification of subjectivity for sentence-level, 297 and sentiment for document-level, and reported that their model obtained higher accuracy than the standard document classification model. Although these pieces of work aim to predict not sentence-level but document-level sentiments, their concepts are similar to ours. However, all the above methods require annotated corpora f"
I08-1039,P05-1015,0,0.0313786,"be seen as an instance of convolution kernels, which was proposed by Haussler (1999). Convolution kernels are a general class of kernel functions which are calculated on the basis of kernels between substructures of inputs. Our proposed kernel treats sentences as input, and treats sentiment words as substructures of sentences. We can use high degree polynomial kernels as both K which is a kernel between substructures, i.e. sentiment words, of sentences, and K 0 which is a kernel between sentences to make the 300 We used two datasets, customer reviews 1 (Hu and Liu, 2004) and movie reviews 2 (Pang and Lee, 2005) to evaluate sentiment classification of sentences. Both of these two datasets are often used for evaluation in sentiment analysis researches. The number of examples and other statistics of the datasets are shown in Table 1. Our method cannot be applied to sentences which contain no sentiment words. We therefore eliminated such sentences from the datasets. “Available” in Table 1 means the number of examples to which our method can be applied. “Sentiment Words” shows the number of sentiment words that are found in the given sentences. Please remember that sentiment words are defined as those wo"
I11-1093,W95-0107,0,0.0696003,"eal of memory. Therefore, we limit the maximum length of the word-chunks. We use word chunks consisting of up to five or ten words. 2 3.3 Features The features used in our experiment are shown in Table 1. As features for Linear-Chain perceptron, we used the following. Here, k denotes the current word position. wk is the k-th word, and pk is the Part-Of-Speech (POS) tag of k-th word. We used the word and the POS of the k-th word and the words in 2-word windows before and after the k-th word with the current NE-tag tk and the NE tag tk−1 of the previous word. Each NE tag is represented as IOB1 (Ramshaw and Marcus, 1995). This representation uses three tags I, O and B, to represent the inside, outside and beginning of a chunk. B is only used at the beginning of a chunk which immediately follows another chunk that NE class is the same. Each tag is expressed with NE classes, like I-CL, B-CL, where CL is an NE class.4 To realize a fast training speed for Linear-Chain, we only used the valid combination of tk and tk−1 in terms of the chunk representation. • NE Chunking and Classification (NECC, for short) (Carreras et al., 2002): This method consists of two parts. The first part is a base NE recognition as in our"
I11-1093,J95-4004,0,0.415026,"Missing"
I11-1093,W02-2004,0,0.554288,"Missing"
I11-1093,sekine-etal-2002-extended,0,0.707669,"ere L is the upper bound length of the entities. The computational cost might not be a big problem, when we use these learning algorithms to recognize a small number of types of NEs, such as the seven types in MUC (Grishman and Sundheim, 1996), the eight types in IREX (Committee, 1999), and the four types in the CoNLL shared task (Tjong Kim Sang and De Meulder, 2003). However, the computational cost will be higher than ever, when we recognize a large types of classes like Sekine’s extended NE hierarchy that includes about 200 types of NEs for covering several types of needs of IE, QA, and IR (Sekine et al., 2002). This paper proposes a word-chunk-based NE recognition method for creating fast NE recogWe propose a Named Entity (NE) recognition method in which word chunks are repeatedly decomposed and concatenated. We can obtain features from word chunks, such as the first word of a word chunk and the last word of a word chunk, which cannot be obtained in word-sequence-based recognition methods. However, each word chunk may include a part of an NE or multiple NEs. To solve this problem, we use the following operators: SHIFT for separating the first word from a word chunk, POP for separating the last word"
I11-1093,W02-1001,0,0.682933,"web page is http://chasen-legacy.sourceforge.jp/. Words may include partial NEs because words segmented with ChaSen do not always correspond with NE boundaries. If such problems occur when we segment the training data, we annotated a word chunk with the type of the NE included in the word chunk. We did not deal with the difference between NE boundaries and word boundaries in this experiment. 831 3.2 Algorithms to be Compared We use the multiclass perceptron algorithm for NECC, SR and SPJR. Thus all of the algorithms are based on perceptron (Rosenblatt, 1958). We apply the averaged perceptron (Collins, 2002a) for all the training algorithms. All the learners and NE recognizers were implemented with C + +. We used perceptron-based algorithms because perceptron-based algorithms usually show the faster training speed and lower usage of memory than training algorithms, such as MEMM (McCallum et al., 2000), CRFs (Lafferty et al., 2001), and so on. Actually, when we applied a CRFs implementation based on LBFGS (Liu and Nocedal, 1989) to the training data, the implementation consumed 72GB memory which is our machine memory size. We select the number of the iteration that shows the highest F-measure in"
I11-1093,N03-1028,0,0.305914,"Missing"
I11-1093,C96-1079,0,0.131875,"here K is the number of types of classes and N is the length of the sentence. Semi-Markovbased algorithms, such as semi-Markov perceptron and semi-Markov CRFs, enumerate NE candidates represented by word chunks in advance for capturing features such as the first word of a chunk and the last word of a chunk. Therefore, the computational cost of a semi-Markov perceptron is O(KLN ), where L is the upper bound length of the entities. The computational cost might not be a big problem, when we use these learning algorithms to recognize a small number of types of NEs, such as the seven types in MUC (Grishman and Sundheim, 1996), the eight types in IREX (Committee, 1999), and the four types in the CoNLL shared task (Tjong Kim Sang and De Meulder, 2003). However, the computational cost will be higher than ever, when we recognize a large types of classes like Sekine’s extended NE hierarchy that includes about 200 types of NEs for covering several types of needs of IE, QA, and IR (Sekine et al., 2002). This paper proposes a word-chunk-based NE recognition method for creating fast NE recogWe propose a Named Entity (NE) recognition method in which word chunks are repeatedly decomposed and concatenated. We can obtain featu"
I11-1093,E99-1023,0,0.0583844,", for short) (Yamada, 2007): This algorithm is based on shift-reduce parsing for wordsequences. It uses two operators. The first one is shift which concatenates a word and its following word chunk. The other is reduce for annotating an NE label to current word chunk.3 The algorithm is different from ours in that the initial inputs of their method are word sequences. Thus, each word chunk is constructed little by little. Therefore, the algorithm cannot use features obtained from word chunks at the early stage. 4 We compared five types of chunk representation: IOB1 , IOB2, IOE1, IOE2 (Tjong Kim Sang and Veenstra, 1999) and Start/End (SE) (Uchimoto et al., 2000) in terms of the number of the NE tags. The number of the NE tags for each representation is as follows; IOB1 is 202, IOB2 is 377, IOE1 is 202, IOE2 is 377, and SE is 730. This experiment uses IOB1 because IOB1 has one of the lowest number of NE tags. The number of NE tags is related to the training speed of Linear-Chain. Actually, Linear-Chain using IOB1-based training data was about 2.4 times faster than Linear-Chain using SE-based training data in our pilot study with small training data. 2 This is because when we ran Semi-Markov without the chunk"
I11-1093,W02-2024,0,0.0377725,"th NE classes, like I-CL, B-CL, where CL is an NE class.4 To realize a fast training speed for Linear-Chain, we only used the valid combination of tk and tk−1 in terms of the chunk representation. • NE Chunking and Classification (NECC, for short) (Carreras et al., 2002): This method consists of two parts. The first part is a base NE recognition as in our method. The second part is NE classification. Unlike in our method, this method just classifies given word chunks without decomposing and concatenating them. This method was used in the best system of the shared task of CoNLL 2002 (Tjong Kim Sang, 2002). • Shift-Reduce Parser for NE Recognition (SR, for short) (Yamada, 2007): This algorithm is based on shift-reduce parsing for wordsequences. It uses two operators. The first one is shift which concatenates a word and its following word chunk. The other is reduce for annotating an NE label to current word chunk.3 The algorithm is different from ours in that the initial inputs of their method are word sequences. Thus, each word chunk is constructed little by little. Therefore, the algorithm cannot use features obtained from word chunks at the early stage. 4 We compared five types of chunk repre"
I11-1093,P10-1110,0,0.0604148,"Missing"
I11-1093,P00-1042,0,0.106228,"Missing"
I11-1093,P08-1067,0,0.0770767,"Missing"
I11-1093,N10-1069,0,0.050053,"Missing"
I11-1093,P06-1059,0,0.423553,"Missing"
I11-1103,R09-1052,1,0.850541,"t Vector Machines with various features. We think considering constraints among relations might contribute to improve the performance of identifying relations. Therefore, we realize it with a Markov logic network. Relation identiﬁcation is considered as a problem to ﬁnd labeled edges between pairs of nodes, where a node is an answer in a thread. Structured output learning is a method to predict such a structure (Tsochantaridis et al., 2004; Crammer et al., 2006). Morita et al. proposed a model based on structured output learning to identify agreement and disagreement relations in a discourse (Morita et al., 2009). Yang et al. used structured Support Vector Machines to extract contexts and answers for questions in threads of online forums (Yang et al., 2009). tive relations”, respectively. We consider that these relations might be useful for identiﬁcation of logical relations and that identiﬁcation of these relations is easier than that of logical relations. Thus, we incorporate identiﬁcation of these superrelations into our model. We brieﬂy describe our related work in section two. Then, we show the logical relations between answers in section three and present our model with global constraints using"
I11-1103,W00-1009,0,0.0148102,"her answers or questions to the original questioner to ask for further details. Figure 1: Example of a QA thread Here, since the relation between (a1) and (a2) is “equivalence” and the relation between (a1) and (a4) is “contradiction”, we expect that the relation between (a2) and (a4) will be the same as the one between (a1) and (a4). This type of constraint is what we incorporate into the model. 3.2 Relations for Answer-Answer Pairs We deﬁne the logical relations for answer-answer pairs according to Radev’s work that deﬁnes 24 types of relations between texts for multidocument summarization (Radev, 2000). Table 1 shows the relations we consider. 4 Relation Identiﬁcation Model with Global Constraints We propose a joint identiﬁcation model of logical relations between answers in a thread. We consider that there are some constraints between logical relations. However, since not all relations satisfy a same constraint, we group logical relations into two types of super-relations on the basis of two kinds of commonality; transitivity and semantic similarity. To incorporate constraints between relations, we try to identify relations for all pairs in a thread jointly. For these purposes, we take an"
I11-1103,P04-1085,0,0.0175292,"identiﬁcation of discourse relations in meetings or dialogs was tackled by some researchers. Hillard et al. demonstrated that automatic identiﬁcation of agreement and disagreement is feasible by using various textual, durational, and acoustic features (Hillard et al., 2003). Galley et al. described a statistical approach for modeling agreements and disagreements in conversational interaction, and classiﬁed utterances as agreement or disagreement by using the adjacency pairs and features that represent various pragmatic inﬂuences of previous agreements or disagreements to the target utterance (Galley et al., 2004). Jimbo et al. proposed a model of relation identiﬁcation for Community-based Question Answering services (Jimbo et al., 2010). Their model identiﬁed relations using Support Vector Machines with various features. We think considering constraints among relations might contribute to improve the performance of identifying relations. Therefore, we realize it with a Markov logic network. Relation identiﬁcation is considered as a problem to ﬁnd labeled edges between pairs of nodes, where a node is an answer in a thread. Structured output learning is a method to predict such a structure (Tsochantarid"
I11-1103,N03-2012,0,0.0136321,"November 8 – 13, 2011. 2011 AFNLP similar questions, factuality and so on, and their question type taxonomy is based on the expected answer. Achananuparp et al. proposed a model to extract a diverse set of answers (Achananuparp et al., 2010). Their approach is based on a graph whose edges have weight about similarity and redundancy. Meanwhile, identiﬁcation of discourse relations in meetings or dialogs was tackled by some researchers. Hillard et al. demonstrated that automatic identiﬁcation of agreement and disagreement is feasible by using various textual, durational, and acoustic features (Hillard et al., 2003). Galley et al. described a statistical approach for modeling agreements and disagreements in conversational interaction, and classiﬁed utterances as agreement or disagreement by using the adjacency pairs and features that represent various pragmatic inﬂuences of previous agreements or disagreements to the target utterance (Galley et al., 2004). Jimbo et al. proposed a model of relation identiﬁcation for Community-based Question Answering services (Jimbo et al., 2010). Their model identiﬁed relations using Support Vector Machines with various features. We think considering constraints among re"
I11-1103,P08-1082,0,0.0261559,"ve and conclude our paper in section six. 2 Related Work The growing popularity of Community-based Question Answering services has prompted many researchers to investigate their characteristics and to propose models for applications using them. Question search and ranking answers are an important application because there are many threads in these services. Jeon et al. discussed a practical method for ﬁnding existing question and answer pairs in response to a newly submitted question (Jeon et al., 2005). Surdeanu et al. proposed an approach for ranking the answers retrieved by Yahoo! Answers (Surdeanu et al., 2008). Wang et al. proposed the ranking model for answers (Wang et al., 2009). Wang et al. proposed a model based on a deep belief network for the semantic relevance of question-answer pairs (Wang et al., 2010). The user’s qualiﬁcations affect the quality of his or her answer. For example, an IT expert may provide a good answer to a question about computers. Jurczyk and Agichtein proposed a model to estimate the authority of users as a means of identifying better answers (Jurczyk and Agichtein, 2007). Pal and Konstan proposed the expert identiﬁcation model (Pal and Konstan, 2010) . Each user has a"
I11-1103,I05-1038,1,0.870193,"re sentence. The question types whose answers are nouns require a speciﬁc class of named entity for the answer, e.g. the class of named entity, PERSON for the question type, Person. We call this class focused NE class. Table 7 shows the correspondence between a question type and a named entity class. Question. I’m planning a journey to Hokkaido. Can you suggest some good sightseeing places? Figure 2: Example of a question sential question is the latter sentence. Therefore, we extract the core sentence and the questionfocus and estimate the question type, on the basis of Tamura et al.’s model (Tamura et al., 2005). The core sentence is the most important sentence, that is, the one requiring an answer in the question. Usually, the core sentence is an interrogative one such as “Where is the most famous place in Hokkaido?”. But questioners sometimes ask questions without an interrogative form, such as “Please tell me some good sightseeing places Table 7: Question type and focused NE class Question type Person Product Facility Location Time Number Focused NE class PERSON ARTIFACT LOCATION, ORGANIZATION LOCATION DATE, TIME MONEY, PERCENT For n-gram features, we also consider unigram and bigram for the ﬁrst"
I11-1103,P10-1125,0,0.0156636,"models for applications using them. Question search and ranking answers are an important application because there are many threads in these services. Jeon et al. discussed a practical method for ﬁnding existing question and answer pairs in response to a newly submitted question (Jeon et al., 2005). Surdeanu et al. proposed an approach for ranking the answers retrieved by Yahoo! Answers (Surdeanu et al., 2008). Wang et al. proposed the ranking model for answers (Wang et al., 2009). Wang et al. proposed a model based on a deep belief network for the semantic relevance of question-answer pairs (Wang et al., 2010). The user’s qualiﬁcations affect the quality of his or her answer. For example, an IT expert may provide a good answer to a question about computers. Jurczyk and Agichtein proposed a model to estimate the authority of users as a means of identifying better answers (Jurczyk and Agichtein, 2007). Pal and Konstan proposed the expert identiﬁcation model (Pal and Konstan, 2010) . Each user has a background. If a user is an amateur in some ﬁeld, he or she cannot understand a difﬁcult question of the ﬁeld. For a user-oriented question ranking, Chen and Kao proposed a model to classify a question as"
I11-1103,D09-1054,0,0.0679126,"Missing"
I11-1103,C08-1063,0,0.163693,"er. Also, when a question tends to have various answers, e.g. the questioner asks for opinions (e.g. “What are your song recommendations?”), it is insufﬁcient to read only the best answer. Generally, as the only one best answer is chosen from the answers, a user may miss other beneﬁcial answers. When a user checks these services with a mobile device, its small display is inefﬁcient to browse all answers. To alleviate these problems, it would be useful to get an overview of answers in a thread, such as by identifying the relations between the answers or by summarizing them (Jimbo et al., 2010; Liu et al., 2008). The purpose of this study is to identify logical relations between answers with a high degree of accuracy, as a basis of these methods. We propose an identiﬁcation model with global constraints on logical relations between answers. Among the relations, there are some constraints like a transitive law. To this end, it is necessary to identify relations in a thread at once, and identiﬁed relations need to satisfy as many of these constraints as possible. Our model is based on a Markov logic network and incorporates these constraints as formulas of ﬁrst order logic. Also, we group logical relat"
I11-1124,C08-1003,0,0.294181,"Missing"
I11-1124,E09-1006,0,0.162593,"Missing"
I11-1124,W06-1615,0,0.347047,"proposed an adaptive kernel approach that mapped the marginal distribution of source and target data into a common kernel space. They also conducted sample selection to make the conditional probabilities between the two domains closer. Raina et al. (2007) proposed self-taught learning that utilized sparse coding to construct higher level features from the unlabeled data collected from the Web. This method was based on unsupervised learning. Tur (2009) proposed a co-adaptation algorithm where both co-training and DA techniques were used to improve the performance of the model. The research by Blitzer et al. (2006) involved work on semi-supervised DA, where they calculated the weight of words around the pivot features (words that frequently appeared both in source and target data and behaved similarly in both) to model some words in one domain that behaved similarly in another. They applied SVD to the matrix of the weights, generated a new feature space, and used the new features with the original features. The closest work to ours is that by McClosky et al. (2010) who focused on the problem where the best model for each document is not obvious when parsing a document collection of heterogeneous domains"
I11-1124,P06-1012,0,0.0932844,"a better than a classifier developed only from the target data. A classifier in a semi-supervised approach is developed from large amounts of labeled source data and unlabeled target data with the aim of classifying target data better than a classifier developed only from the source data. Finally, a classifier is developed from a large amount of labeled source data with the aim of classifying target data accurately in the unsupervised approach. We focused on the supervised DA of WSD in this paper. Many researchers have investigated DA within or outside the area of natural language processing. Chan and Ng (2006) carried out the DA of WSD by estimating class priors using an EM algorithm. Chan and Ng (2007) also conducted the DA of WSD by estimating class priors using the EM algorithm, but this was supervised DA using 1107 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1107–1115, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP active learning. In addition, Daum´e III (2007) worked on the supervised DA. He augmented an input space and made triple length features that were general, source-specific, and target-specific. This was easy to implement, cou"
I11-1124,P07-1007,0,0.0450841,"ed approach is developed from large amounts of labeled source data and unlabeled target data with the aim of classifying target data better than a classifier developed only from the source data. Finally, a classifier is developed from a large amount of labeled source data with the aim of classifying target data accurately in the unsupervised approach. We focused on the supervised DA of WSD in this paper. Many researchers have investigated DA within or outside the area of natural language processing. Chan and Ng (2006) carried out the DA of WSD by estimating class priors using an EM algorithm. Chan and Ng (2007) also conducted the DA of WSD by estimating class priors using the EM algorithm, but this was supervised DA using 1107 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1107–1115, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP active learning. In addition, Daum´e III (2007) worked on the supervised DA. He augmented an input space and made triple length features that were general, source-specific, and target-specific. This was easy to implement, could be used with various DA methods, and could easily be extended to multidomain adaptation prob"
I11-1124,W10-2608,0,0.0848514,"Missing"
I11-1124,P07-1033,0,0.579585,"Missing"
I11-1124,I08-7018,0,0.0601083,"ed as the algorithm for decision tree learning and a binary tree was generated. The experiments were conducted with five-fold cross validation. The threshold values for pruning were optimized with preliminary experiments using 1/4 of the training data set as a development data set. Here, the entropy of a node was tuned as the threshold value in 0.1 increments. The value of a smaller tree was used when more than one threshold value gave the same accuracy. 4 Data Three data were used for the experiments: (1) the sub-corpus of white papers in the Balanced Corpus of Contemporary Japanese (BCCWJ) (Maekawa, 2008), (2) the sub-corpus of documents from a Q&A site on the WWW of BCCWJ, and (3) Real World Computing (RWC) text databases (newspaper articles) (Hashida et al., 1998). DAs were conducted in six directions according to various source and target data. Word senses were annotated in these corpora according to a Japanese dictionary, i.e., the Iwanami Kokugo Jiten (Nishio et al., 1994). It has three levels for sense IDs, and we used the fine-level sense in the experiments. Multi-sense words that appeared equal or more than 50 times in both source and target data were selected as the target words in th"
I11-1124,N10-1004,0,0.0374077,"roposed a co-adaptation algorithm where both co-training and DA techniques were used to improve the performance of the model. The research by Blitzer et al. (2006) involved work on semi-supervised DA, where they calculated the weight of words around the pivot features (words that frequently appeared both in source and target data and behaved similarly in both) to model some words in one domain that behaved similarly in another. They applied SVD to the matrix of the weights, generated a new feature space, and used the new features with the original features. The closest work to ours is that by McClosky et al. (2010) who focused on the problem where the best model for each document is not obvious when parsing a document collection of heterogeneous domains. They studied it as a new task of multiple source parser adaptation. They proposed a method of parsing a sentence that first predicts accuracies for various parsing models using a regression model, and then uses the parsing model with the highest predicted accuracy. The main difference is that their work was about parsing but ours discussed here is about Japanese WSD. They also assumed that they had labeled corpora in heterogeneous domains but we have no"
I11-1124,W10-2605,0,0.144721,"Missing"
I11-1124,P07-1034,0,0.140897,") extended the work in (Daum´e III, 2007) to semi-supervised DA. It inherited the advantages of the supervised version and outperformed it by using unlabeled target data. Agirre and de Lacalle (2008) worked on the semi-supervised DA of WSD. They applied singular value decomposition (SVD) to a matrix of unlabeled target data and a large amount of unlabeled source data, and trained a classifier with them. Agirre and de Lacalle (2009) worked on the supervised DA using almost the same method, but they used a small amount of labeled source data instead of the large amount of unlabeled source data. Jiang and Zhai (2007) demonstrated that performance increased as examples were weighted when DA was applied. This method could be used with various other supervised or semi-supervised DA methods. In addition, they tried to identify and remove source data that misled DA, but they concluded that it was only effective if examples were not weighted. Zhong et al. (2009) proposed an adaptive kernel approach that mapped the marginal distribution of source and target data into a common kernel space. They also conducted sample selection to make the conditional probabilities between the two domains closer. Raina et al. (200"
I11-1158,P08-2008,0,0.030755,"ense disambiguation is not novel, and dates back to 90’s. Fujii et al. (1998) proposed a method for the verb sense disambiguation, which is based on the k-nearest neighbors. In their method, each instance is represented as a case frame and the similarity of two instances is calculated as a weighted sum of the similarities of case ﬁllers. Fujii et al. also proposed a framework of active learning. To disambiguate verb senses, Chen and Palmer (2009) proposed to use linguistic and semantic features including the voice of the given sentence, the presence of a PP adjunct, and the named entity tags. Dligach and Palmer (2008) proposed to use co-occurrence with other verbs as features. Wagner et al. (2009) proposed to use verb clusters generated with statistics on verb subcategorization. 1382 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1382–1386, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 3 Potts Model We introduce the probability model that we will use for our task. This model gives the probability distribution over a set of nodes associated with random variables, where some pairs of variables are dependent on each other. If the variables can have more"
I11-1158,I08-7018,0,0.0323502,"oach to construct a classiﬁer with those various clues as features. Support vector machines are employed in this work, although other classiﬁers are also applicable. Note that even if a case ﬁller in the test instance did not appear in the training data, the feature ρi (c) corresponding to this case ﬁller conveys the information on the selectional preference of the verb against the case ﬁller. 5 Experiments 5.1 Experimental Settings The proposed method for verb sense disambiguation is evaluated on the white paper part of BCCWJ corpus, the ﬁrst balanced corpus of contemporary written Japanese (Maekawa, 2008), which was also used as a test set for SemEval-2 Japanese word sense disambiguation task (Okumura et al., 2010). The dataset used in this research was created by the preliminary annotation for SemEval2. The senses are deﬁned in the Iwanami Kokugo Jiten (Nishio et al., 1994), a Japanese dictionary. Among three levels of sense IDs deﬁned in this dictionary, the middle-level sense was used in the empirical evaluation, which is the same level of senses used in the SemEval-2. From the dataset above, we selected most ambiguous 14 verbs whose empirical sense distributions (i.e., estimated with the m"
I11-1158,S10-1012,1,0.847663,"in this work, although other classiﬁers are also applicable. Note that even if a case ﬁller in the test instance did not appear in the training data, the feature ρi (c) corresponding to this case ﬁller conveys the information on the selectional preference of the verb against the case ﬁller. 5 Experiments 5.1 Experimental Settings The proposed method for verb sense disambiguation is evaluated on the white paper part of BCCWJ corpus, the ﬁrst balanced corpus of contemporary written Japanese (Maekawa, 2008), which was also used as a test set for SemEval-2 Japanese word sense disambiguation task (Okumura et al., 2010). The dataset used in this research was created by the preliminary annotation for SemEval2. The senses are deﬁned in the Iwanami Kokugo Jiten (Nishio et al., 1994), a Japanese dictionary. Among three levels of sense IDs deﬁned in this dictionary, the middle-level sense was used in the empirical evaluation, which is the same level of senses used in the SemEval-2. From the dataset above, we selected most ambiguous 14 verbs whose empirical sense distributions (i.e., estimated with the maximum likelihood principle) have a high entropy and appear more than 100 times in the dataset. The statistics o"
I11-1158,P05-1017,1,0.914621,"orized probability function is called variational free energy: F (c) = ∑ c = −α −β − i ij i ci n exp(αδ(n, ai ) + β j wij ρj (n)) . (3) The ﬁxed point equation for i ∈ / L can be obtained by removing αδ(c, ai ) from above. This ﬁxed point equation is solved by an iterative computation. In the actual implementation, we represent ρi with a linear combination of the discrete Tchebycheff polynomials (Tanaka and Morita, 1996). Details on the Potts model and its computation can be found in the literature (Nishimori, 2001). 4 Proposed Method 4.1 Construction of Lexical Networks We follow the work by Takamura et al. (2005) to construct a lexical network. We link two words if one word appears in the gloss of the other. Each link belongs to one of two groups: the sameorientation links SL and the different-orientation links DL. If a negation word appears in the gloss of an entry word, the words after the negation word are linked to the entry word with DL. Otherwise, those words are linked with SL. In case of Japanese, the auxiliaries “nai” and “nu” are regarded as negation words. We next set weights W = (wij ) to links : wij =  √ 1   d(i)d(j)     (lij ∈ SL) −√ 1 d(i)d(j) (lij ∈ DL) , (4) 0 otherwise ρi (ci"
I11-1158,N07-1037,1,0.816547,"g instances are used as observed variables, with their index set being L in Equation (2). Pi (c) is estimated for each case. In our experiments on Japanese, surface cases are employed: wo (accusative), ga (nominative), ni (dative/locative), de (locative/instrumental), no (genitive/others), e (locative/illative), to (comitative), kara (elative), yori (comparative), made (terminative). Although our model is also applicable to deep cases, we focus on surface cases since deep case recognition itself is a challenging task. 4.3 Estimation of β In some pieces of previous work (Takamura et al., 2005; Takamura et al., 2007), it has been shown that the optimal β can be obtained by estimating the critical temperature, at which phase transition occurs from paramagnetic phase (variables are randomly oriented) to ferromagnetic phase (most of the variables have the same value). We follow these pieces of previous work. In practice, when the maximum of the spatial averages of the approximated probabilities ∑ maxc i ρi (c)/N exceeds a threshold during increasing β, we consider that the phase transition has occurred. We select the value of β slightly before the phase transition. 4.4 Discriminative Training Probability ρi"
I11-1158,J98-4002,0,\N,Missing
I11-1158,W09-3401,0,\N,Missing
I13-1019,C00-1004,0,0.0299959,"using only simple rules, and the lexicon augmentation approach would be better for them. However, this is not true for onomatopoeias. Although Japanese is rich in onomatopoeias and some of them do not Introduction Morphological analysis is the ﬁrst step in many natural language applications. Since words are not segmented by explicit delimiters in Japanese, Japanese morphological analysis consists of two subtasks: word segmentation and part-of-speech (POS) tagging. Japanese morphological analysis has successfully adopted lexicon-based approaches for newspaper articles (Kurohashi et al., 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004), in which an input sentence is transformed into a lattice of candidate words using a pre-deﬁned lexicon, and an optimal path in the lattice is then selected. Figure 1 shows an example of a word lattice for morphological analysis and an optimal path. Since the transformation from a sentence into a word lattice basically depends on the pre-deﬁned lexicon, the existence of unknown words, i.e., words that are not included in the predeﬁned lexicon, is a major problem in Japanese morphological analysis. There are two major approaches to this problem: one is to augment the lexico"
I13-1019,P12-1109,0,0.0250511,"and thus, all the variations cannot be covered by a dictionary. In addition, as mentioned above, since we [adjective] Figure 2: Example of a word lattice with new nodes “ぉぃ,” “ぉぃしかった,” and “でーーす.” The broken lines indicate the added nodes and paths, and the bold lines indicate the optimal path. have to take into account the adjacent word to accurately recognize rendaku, the lexical knowledge alone is not sufﬁcient for rendaku recognition. For languages other than Japanese, there is much work on text normalization that aims to handle informal expressions in social media (Beaufort et al., 2010; Liu et al., 2012; Han et al., 2012). However, their target languages are segmented languages such as English and French, and thus they can focus only on normalization. On the other hand, since Japanese is an unsegmented language, we have to also consider the word segmentation task. 3 Proposed Method 3.1 Overview We use the rule-based Japanese morphological analyzer JUMAN version 5.1 as our baseline system. Basically we only improve the method for building a word lattice and do not change the process for ﬁnding an optimal path from the lattice. That is, our proposed system only adds new nodes to the word latti"
I13-1019,C04-1066,0,0.0273809,"ana and Chinese characters. 䈚䈎䈦䈢 Lattice: BOS 䈌 [Unknown word] (at) [Verb] [particle] 䈚 䈎䈦䈢 䈆 [Unknown word] 䈪 (do) (bought) (go out) [verb] [verb] [verb] 䊷䊷 [Unknown word] 䈍䈇 䈪䊷䊷䈜 [interjection] [auxiliary verb] (hey) 2.4 Related work 䈪 (scolded) 䈜 (do) EOS [verb] (was) 䈌䈆䈚䈎䈦䈢 (delicious) Much work has been done on Japanese unknown word processing. Several approaches aimed to acquire unknown words from a corpus in advance (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008) and others aimed to introduce better unknown word model to morphological analyzer (Nagata, 1999; Uchimoto et al., 2001; Asahara and Matsumoto, 2004; Nakagawa and Uchimoto, 2007). However, there are few works that focus on certain types of unknown words. Kazama et al. (1999)’s work is one of them. Kazama et al. improved the morphological analyzer JUMAN to deal with the informal expressions in online chat conversations. They focused on substitution and insertion, which are also the target of this paper. However, while our approach aims to develop heuristics to ﬂexibly search the lexicon, they expanded the lexicon, and thus their approach cannot deal with an inﬁnite number of derivations, such as “冷たーーい,” and “冷ーたー いー” for the original word"
I13-1019,P10-1079,0,0.0312367,"owercases into a word, and thus, all the variations cannot be covered by a dictionary. In addition, as mentioned above, since we [adjective] Figure 2: Example of a word lattice with new nodes “ぉぃ,” “ぉぃしかった,” and “でーーす.” The broken lines indicate the added nodes and paths, and the bold lines indicate the optimal path. have to take into account the adjacent word to accurately recognize rendaku, the lexical knowledge alone is not sufﬁcient for rendaku recognition. For languages other than Japanese, there is much work on text normalization that aims to handle informal expressions in social media (Beaufort et al., 2010; Liu et al., 2012; Han et al., 2012). However, their target languages are segmented languages such as English and French, and thus they can focus only on normalization. On the other hand, since Japanese is an unsegmented language, we have to also consider the word segmentation task. 3 Proposed Method 3.1 Overview We use the rule-based Japanese morphological analyzer JUMAN version 5.1 as our baseline system. Basically we only improve the method for building a word lattice and do not change the process for ﬁnding an optimal path from the lattice. That is, our proposed system only adds new nodes"
I13-1019,C96-2202,0,0.308474,"nsformed into a lattice of candidate words using a pre-deﬁned lexicon, and an optimal path in the lattice is then selected. Figure 1 shows an example of a word lattice for morphological analysis and an optimal path. Since the transformation from a sentence into a word lattice basically depends on the pre-deﬁned lexicon, the existence of unknown words, i.e., words that are not included in the predeﬁned lexicon, is a major problem in Japanese morphological analysis. There are two major approaches to this problem: one is to augment the lexicon by acquiring unknown words from a corpus in advance (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008) and the other is to introduce better unknown word processing to the morphological ana162 International Joint Conference on Natural Language Processing, pages 162–170, Nagoya, Japan, 14-18 October 2013. Unknown words derived from known words Type Unknown word Rendaku* (sequential voicing) (たまご) ざけ ((tamago-)zake, sake-nog) Substitution with long sound symbols* ほんとー (troo) Substitution with lowercases* ぁなた (y0u) Substitution with normal symbols うれ∪い (h@ppy) Insertion of long sound symbols* 冷たーーーい (coooool) Insertion of lowercases* 冷たぁぁぁい (coooool) Insertion of vow"
I13-1019,D08-1045,1,0.775899,"e of candidate words using a pre-deﬁned lexicon, and an optimal path in the lattice is then selected. Figure 1 shows an example of a word lattice for morphological analysis and an optimal path. Since the transformation from a sentence into a word lattice basically depends on the pre-deﬁned lexicon, the existence of unknown words, i.e., words that are not included in the predeﬁned lexicon, is a major problem in Japanese morphological analysis. There are two major approaches to this problem: one is to augment the lexicon by acquiring unknown words from a corpus in advance (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008) and the other is to introduce better unknown word processing to the morphological ana162 International Joint Conference on Natural Language Processing, pages 162–170, Nagoya, Japan, 14-18 October 2013. Unknown words derived from known words Type Unknown word Rendaku* (sequential voicing) (たまご) ざけ ((tamago-)zake, sake-nog) Substitution with long sound symbols* ほんとー (troo) Substitution with lowercases* ぁなた (y0u) Substitution with normal symbols うれ∪い (h@ppy) Insertion of long sound symbols* 冷たーーーい (coooool) Insertion of lowercases* 冷たぁぁぁい (coooool) Insertion of vowel characters 冷たあああい (coooool)"
I13-1019,den-etal-2008-proper,0,0.0322159,"Missing"
I13-1019,P99-1036,0,0.307605,"new words that often consist of katakana and Chinese characters. 䈚䈎䈦䈢 Lattice: BOS 䈌 [Unknown word] (at) [Verb] [particle] 䈚 䈎䈦䈢 䈆 [Unknown word] 䈪 (do) (bought) (go out) [verb] [verb] [verb] 䊷䊷 [Unknown word] 䈍䈇 䈪䊷䊷䈜 [interjection] [auxiliary verb] (hey) 2.4 Related work 䈪 (scolded) 䈜 (do) EOS [verb] (was) 䈌䈆䈚䈎䈦䈢 (delicious) Much work has been done on Japanese unknown word processing. Several approaches aimed to acquire unknown words from a corpus in advance (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008) and others aimed to introduce better unknown word model to morphological analyzer (Nagata, 1999; Uchimoto et al., 2001; Asahara and Matsumoto, 2004; Nakagawa and Uchimoto, 2007). However, there are few works that focus on certain types of unknown words. Kazama et al. (1999)’s work is one of them. Kazama et al. improved the morphological analyzer JUMAN to deal with the informal expressions in online chat conversations. They focused on substitution and insertion, which are also the target of this paper. However, while our approach aims to develop heuristics to ﬂexibly search the lexicon, they expanded the lexicon, and thus their approach cannot deal with an inﬁnite number of derivations,"
I13-1019,D12-1039,0,0.0610311,"variations cannot be covered by a dictionary. In addition, as mentioned above, since we [adjective] Figure 2: Example of a word lattice with new nodes “ぉぃ,” “ぉぃしかった,” and “でーーす.” The broken lines indicate the added nodes and paths, and the bold lines indicate the optimal path. have to take into account the adjacent word to accurately recognize rendaku, the lexical knowledge alone is not sufﬁcient for rendaku recognition. For languages other than Japanese, there is much work on text normalization that aims to handle informal expressions in social media (Beaufort et al., 2010; Liu et al., 2012; Han et al., 2012). However, their target languages are segmented languages such as English and French, and thus they can focus only on normalization. On the other hand, since Japanese is an unsegmented language, we have to also consider the word segmentation task. 3 Proposed Method 3.1 Overview We use the rule-based Japanese morphological analyzer JUMAN version 5.1 as our baseline system. Basically we only improve the method for building a word lattice and do not change the process for ﬁnding an optimal path from the lattice. That is, our proposed system only adds new nodes to the word lattice built by the bas"
I13-1019,P07-2055,0,0.0238124,"䈎䈦䈢 Lattice: BOS 䈌 [Unknown word] (at) [Verb] [particle] 䈚 䈎䈦䈢 䈆 [Unknown word] 䈪 (do) (bought) (go out) [verb] [verb] [verb] 䊷䊷 [Unknown word] 䈍䈇 䈪䊷䊷䈜 [interjection] [auxiliary verb] (hey) 2.4 Related work 䈪 (scolded) 䈜 (do) EOS [verb] (was) 䈌䈆䈚䈎䈦䈢 (delicious) Much work has been done on Japanese unknown word processing. Several approaches aimed to acquire unknown words from a corpus in advance (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008) and others aimed to introduce better unknown word model to morphological analyzer (Nagata, 1999; Uchimoto et al., 2001; Asahara and Matsumoto, 2004; Nakagawa and Uchimoto, 2007). However, there are few works that focus on certain types of unknown words. Kazama et al. (1999)’s work is one of them. Kazama et al. improved the morphological analyzer JUMAN to deal with the informal expressions in online chat conversations. They focused on substitution and insertion, which are also the target of this paper. However, while our approach aims to develop heuristics to ﬂexibly search the lexicon, they expanded the lexicon, and thus their approach cannot deal with an inﬁnite number of derivations, such as “冷たーーい,” and “冷ーたー いー” for the original word “冷たい.” In addition, Ikeda et"
I13-1019,I08-1025,1,0.83086,"ed as others. We counted the number of different outputs for 100,000 sentences. We then calculated the estimated numbers of positive/negative changes for the sentences by using the equations: pared with trie search. However, the number of potential entries of onomatopoeias without repetition is not so large, and thus our system adds all possible entries of onomatopoeias without repetition into the trie-based lexicon in advance. 4 Experiments 4.1 Setting We used 100,000 Japanese sentences to evaluate our approach. These sentences were obtained from an open search engine infrastructure TSUBAKI (Shinzato et al., 2008), which included at least one hiragana character and consisted of more than twenty characters We ﬁrst estimated the recall. Since it is too costly to create a set of data with all unknown words annotated, we made a set of data with only our target unknown words annotated. We could apply a set of regular expressions to reduce the unknown word candidates by limiting the type of unknown words. We manually annotated 100 expressions for each type, and estimated the recall. A high recall, however, does not always imply that the proposed system performs well. It might be possible that our proposed me"
I13-1019,C00-1057,0,0.0449741,"Missing"
I13-1019,W01-0512,0,0.182595,"often consist of katakana and Chinese characters. 䈚䈎䈦䈢 Lattice: BOS 䈌 [Unknown word] (at) [Verb] [particle] 䈚 䈎䈦䈢 䈆 [Unknown word] 䈪 (do) (bought) (go out) [verb] [verb] [verb] 䊷䊷 [Unknown word] 䈍䈇 䈪䊷䊷䈜 [interjection] [auxiliary verb] (hey) 2.4 Related work 䈪 (scolded) 䈜 (do) EOS [verb] (was) 䈌䈆䈚䈎䈦䈢 (delicious) Much work has been done on Japanese unknown word processing. Several approaches aimed to acquire unknown words from a corpus in advance (Mori and Nagao, 1996; Murawaki and Kurohashi, 2008) and others aimed to introduce better unknown word model to morphological analyzer (Nagata, 1999; Uchimoto et al., 2001; Asahara and Matsumoto, 2004; Nakagawa and Uchimoto, 2007). However, there are few works that focus on certain types of unknown words. Kazama et al. (1999)’s work is one of them. Kazama et al. improved the morphological analyzer JUMAN to deal with the informal expressions in online chat conversations. They focused on substitution and insertion, which are also the target of this paper. However, while our approach aims to develop heuristics to ﬂexibly search the lexicon, they expanded the lexicon, and thus their approach cannot deal with an inﬁnite number of derivations, such as “冷たーーい,” and “冷"
I13-1019,W04-3230,0,0.845235,"the lexicon augmentation approach would be better for them. However, this is not true for onomatopoeias. Although Japanese is rich in onomatopoeias and some of them do not Introduction Morphological analysis is the ﬁrst step in many natural language applications. Since words are not segmented by explicit delimiters in Japanese, Japanese morphological analysis consists of two subtasks: word segmentation and part-of-speech (POS) tagging. Japanese morphological analysis has successfully adopted lexicon-based approaches for newspaper articles (Kurohashi et al., 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004), in which an input sentence is transformed into a lattice of candidate words using a pre-deﬁned lexicon, and an optimal path in the lattice is then selected. Figure 1 shows an example of a word lattice for morphological analysis and an optimal path. Since the transformation from a sentence into a word lattice basically depends on the pre-deﬁned lexicon, the existence of unknown words, i.e., words that are not included in the predeﬁned lexicon, is a major problem in Japanese morphological analysis. There are two major approaches to this problem: one is to augment the lexicon by acquiring unkno"
I13-1078,W10-3208,1,0.846591,"on developing sentiment lexicon with three sentiment classes. For instance, Takamura et al. (2005) have developed a lexicon of emotion words tagged with the classes desirable and undesirable using Spin model. A number of other polarity sentiment lexicons are available in English such as SentiWordNet 3.0 (Esuli et al., 2010), Subjectivity Word List (Wilson et al., 2005), WordNet-Affect list (Strapparava et al., 2004), Taboada‟s adjective list (Taboada et al., 2006). On the other hand, several polarity sentiment lexicons have been developed in different languages like Hindi, Bengali and Telegu (Das and Bandyopadhyay, 2010), Japanese (Torii et al., 2012) etc. Among all these publicly available sentiment lexicons, SentiWordNet is one of the well-known and widely used ones (number of citations is higher than other resources 1 ), having been uti1 http://citeseerx.ist.psu.edu/index 674 International Joint Conference on Natural Language Processing, pages 674–679, Nagoya, Japan, 14-18 October 2013. lized in several applications such as sentiment analysis, opinion mining and emotion analysis. Undoubtedly, manual compilation is the best way to create such an emotion lexicon but is much expensive in terms of time and hum"
I13-1078,esuli-sebastiani-2006-sentiwordnet,0,0.13151,"Missing"
I13-1078,strapparava-valitutti-2004-wordnet,0,0.533599,"Missing"
I13-1078,P07-2034,0,0.0562995,"Missing"
I13-1078,P05-1017,1,0.957128,"ional classes like happy, sad, fear, anger, surprise and disgust. In the previous example, frequent appearance of words from the happy class in a blog document would imply that the writer of the comment is quite happy with the new rule proposed by the Government. Several works have been conducted on building emotional corpora in different languages such as in English (Aman and Szpakowicz, 2007), Chinese (Yang et al., 2007; Quan and Ren, 2010), and Bengali (Das and Bandyopadhyay, 2010) etc. All these works have focused on developing sentiment lexicon with three sentiment classes. For instance, Takamura et al. (2005) have developed a lexicon of emotion words tagged with the classes desirable and undesirable using Spin model. A number of other polarity sentiment lexicons are available in English such as SentiWordNet 3.0 (Esuli et al., 2010), Subjectivity Word List (Wilson et al., 2005), WordNet-Affect list (Strapparava et al., 2004), Taboada‟s adjective list (Taboada et al., 2006). On the other hand, several polarity sentiment lexicons have been developed in different languages like Hindi, Bengali and Telegu (Das and Bandyopadhyay, 2010), Japanese (Torii et al., 2012) etc. Among all these publicly availabl"
I13-1078,N07-1037,1,0.824129,"ives if the adjectives appear in a conjunctive form in the corpus. If the adjectives are connected by “and”, the link belongs to SL. If they are connected by “but”, the link belongs to DL. We call this network the gloss-thesaurus-corpus network (GTC). We have used gloss-thesauruscorpus network in our experiments. ferent values of β. Then the accuracies were computed manually as well as using the WordNet Affect lexicon. We also classified the words into two classes, i.e. positive and negative. The accuracies of two classes were calculated using the SentiWordNet. 4.2 5.1 Classification of Words Takamura et al., (2007) used the Potts model for extracting semantic orientation of phrases (pair of adjective and a noun): positive, negative or neutral. In contrast to that, we have used the Potts model for identifying the emotional class (es) of a word. We have used one seed word from each class to start with the experiment. Each seed word is assigned a class manually. We therefore estimate the state of nodes in the lexical network for each class of emotions. The only drawback is that, it could not assign any emotional class to a word which is not present in the lexical network. These words may be referred to as"
I13-1078,J11-2001,0,0.100786,"Missing"
I13-1078,baccianella-etal-2010-sentiwordnet,0,0.423122,"Missing"
I13-1078,H05-1044,0,0.0227693,"ks have been conducted on building emotional corpora in different languages such as in English (Aman and Szpakowicz, 2007), Chinese (Yang et al., 2007; Quan and Ren, 2010), and Bengali (Das and Bandyopadhyay, 2010) etc. All these works have focused on developing sentiment lexicon with three sentiment classes. For instance, Takamura et al. (2005) have developed a lexicon of emotion words tagged with the classes desirable and undesirable using Spin model. A number of other polarity sentiment lexicons are available in English such as SentiWordNet 3.0 (Esuli et al., 2010), Subjectivity Word List (Wilson et al., 2005), WordNet-Affect list (Strapparava et al., 2004), Taboada‟s adjective list (Taboada et al., 2006). On the other hand, several polarity sentiment lexicons have been developed in different languages like Hindi, Bengali and Telegu (Das and Bandyopadhyay, 2010), Japanese (Torii et al., 2012) etc. Among all these publicly available sentiment lexicons, SentiWordNet is one of the well-known and widely used ones (number of citations is higher than other resources 1 ), having been uti1 http://citeseerx.ist.psu.edu/index 674 International Joint Conference on Natural Language Processing, pages 674–679, N"
I13-1078,P97-1023,0,0.0600689,"ork is shown in Table 1. Next, we assign weights W = (wij) to links as follows: By minimizing F(n) under the condition that ( ) ,∑ , we obtain the following fixed point equation for : ( ) Potts Model for Construction of Emotional Lexicon No. of words 20497 3751 55285 8482 Table 1. Statistics of Lexical network We have also constructed another network, the gloss thesaurus network (GT), by linking syno676 nyms, antonyms and hypernyms, in addition to the above linked words. Only antonym links are in DL. We enhanced the gloss-thesaurus network with co-occurrence information extracted from corpus. Hatzivassiloglou and McKeown (1997) focused on conjunctive expressions such as “simple and well-received” and “simplistic but well-received”, where the former pair of words tend to have the same semantic orientation, and the latter tend to have the opposite orientation. Following their method, we connect two adjectives if the adjectives appear in a conjunctive form in the corpus. If the adjectives are connected by “and”, the link belongs to SL. If they are connected by “but”, the link belongs to DL. We call this network the gloss-thesaurus-corpus network (GTC). We have used gloss-thesauruscorpus network in our experiments. fere"
I13-1078,W11-1710,1,0.822398,"Missing"
I13-1078,taboada-etal-2006-methods,0,\N,Missing
I17-1080,P13-1099,0,0.0270186,"heuristic rules, extractive methods based on sentence classification/regression, and abstractive methods based on neural networks. We compared the performance of these methods through evaluations both by human judges and automatic scoring using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) (Lin, 2004). The experimental results show that an abstractive approach using an encoder-decoder model with a copying mechanism achieves the highest score for both ROUGE and evaluations by human judges. 2 ROUGE score or the bigram frequency included in a summary (Peyrard and Eckale-Kohler, 2016; Li et al., 2013). Other approaches consider summarization tasks as a classification problem. They adopt supervised machine learning techniques to solve them (Hirao et al., 2002; Shen et al., 2007). Abstractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al., 2004) or statistical machine translation models (Bank et al., 2010; Wubben et al., 2012; Cohn and Lapata, 2013) and templates (Oya et al., 2014). In addition, encoder-decoder approaches have been proposed in recent years. They were originally applied to machine translation tasks (Luong et al., 2015"
I17-1080,W04-1013,0,0.0346261,"ngs of the The 8th International Joint Conference on Natural Language Processing, pages 792–800, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP a question as an input. We therefore developed a number of methods designed for question summarization: extractive methods based on simple heuristic rules, extractive methods based on sentence classification/regression, and abstractive methods based on neural networks. We compared the performance of these methods through evaluations both by human judges and automatic scoring using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) (Lin, 2004). The experimental results show that an abstractive approach using an encoder-decoder model with a copying mechanism achieves the highest score for both ROUGE and evaluations by human judges. 2 ROUGE score or the bigram frequency included in a summary (Peyrard and Eckale-Kohler, 2016; Li et al., 2013). Other approaches consider summarization tasks as a classification problem. They adopt supervised machine learning techniques to solve them (Hirao et al., 2002; Shen et al., 2007). Abstractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al"
I17-1080,D15-1166,0,0.131748,"Missing"
I17-1080,W14-4318,0,0.0253473,"d to other sequence-tosequence tasks including sentence summarization (Rush et al., 2015; Kikuchi et al., 2016; Gu et al., 2016). Related Work Text summarization is one of the problems that have been studied for a long time in the field of natural language processing. In many of the existing summarization tasks including the shared tasks in Document Understanding Conference (DUC)2 , documents from newspapers or scientific articles are considered as an input. There are also other summarization tasks in which other types of input are assumed such as conversations or email threads (Duboue, 2012; Oya and Carenini, 2014; Oya et al., 2014). Unlike the researches, we assume a question as an input. As a related attempt in Question Answering researches, Tamura et al. (2005) worked on classification of multiple-sentence questions into classes such as yes/no questions and definition questions, and attempted to extract the question sentence that was the most important in finding the correct class. However, the extracted question sentence is not always a summary of the question. Consider the last sentence “Wiil it or will it not?” for the aforementioned Table 1 question. This last sentence is important in finding th"
I17-1080,W14-4407,0,0.0312026,"highest score for both ROUGE and evaluations by human judges. 2 ROUGE score or the bigram frequency included in a summary (Peyrard and Eckale-Kohler, 2016; Li et al., 2013). Other approaches consider summarization tasks as a classification problem. They adopt supervised machine learning techniques to solve them (Hirao et al., 2002; Shen et al., 2007). Abstractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al., 2004) or statistical machine translation models (Bank et al., 2010; Wubben et al., 2012; Cohn and Lapata, 2013) and templates (Oya et al., 2014). In addition, encoder-decoder approaches have been proposed in recent years. They were originally applied to machine translation tasks (Luong et al., 2015; Bahdanau et al., 2015; Cao et al., 2017), and have been actively applied to other sequence-tosequence tasks including sentence summarization (Rush et al., 2015; Kikuchi et al., 2016; Gu et al., 2016). Related Work Text summarization is one of the problems that have been studied for a long time in the field of natural language processing. In many of the existing summarization tasks including the shared tasks in Document Understanding Confer"
I17-1080,P16-1172,0,0.0216194,"tractive methods based on simple heuristic rules, extractive methods based on sentence classification/regression, and abstractive methods based on neural networks. We compared the performance of these methods through evaluations both by human judges and automatic scoring using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) (Lin, 2004). The experimental results show that an abstractive approach using an encoder-decoder model with a copying mechanism achieves the highest score for both ROUGE and evaluations by human judges. 2 ROUGE score or the bigram frequency included in a summary (Peyrard and Eckale-Kohler, 2016; Li et al., 2013). Other approaches consider summarization tasks as a classification problem. They adopt supervised machine learning techniques to solve them (Hirao et al., 2002; Shen et al., 2007). Abstractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al., 2004) or statistical machine translation models (Bank et al., 2010; Wubben et al., 2012; Cohn and Lapata, 2013) and templates (Oya et al., 2014). In addition, encoder-decoder approaches have been proposed in recent years. They were originally applied to machine translation tasks ("
I17-1080,W03-0501,0,0.0909238,"Evaluation (ROUGE) (Lin, 2004). The experimental results show that an abstractive approach using an encoder-decoder model with a copying mechanism achieves the highest score for both ROUGE and evaluations by human judges. 2 ROUGE score or the bigram frequency included in a summary (Peyrard and Eckale-Kohler, 2016; Li et al., 2013). Other approaches consider summarization tasks as a classification problem. They adopt supervised machine learning techniques to solve them (Hirao et al., 2002; Shen et al., 2007). Abstractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al., 2004) or statistical machine translation models (Bank et al., 2010; Wubben et al., 2012; Cohn and Lapata, 2013) and templates (Oya et al., 2014). In addition, encoder-decoder approaches have been proposed in recent years. They were originally applied to machine translation tasks (Luong et al., 2015; Bahdanau et al., 2015; Cao et al., 2017), and have been actively applied to other sequence-tosequence tasks including sentence summarization (Rush et al., 2015; Kikuchi et al., 2016; Gu et al., 2016). Related Work Text summarization is one of the problems that have been studied for"
I17-1080,D15-1044,0,0.067868,"rao et al., 2002; Shen et al., 2007). Abstractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al., 2004) or statistical machine translation models (Bank et al., 2010; Wubben et al., 2012; Cohn and Lapata, 2013) and templates (Oya et al., 2014). In addition, encoder-decoder approaches have been proposed in recent years. They were originally applied to machine translation tasks (Luong et al., 2015; Bahdanau et al., 2015; Cao et al., 2017), and have been actively applied to other sequence-tosequence tasks including sentence summarization (Rush et al., 2015; Kikuchi et al., 2016; Gu et al., 2016). Related Work Text summarization is one of the problems that have been studied for a long time in the field of natural language processing. In many of the existing summarization tasks including the shared tasks in Document Understanding Conference (DUC)2 , documents from newspapers or scientific articles are considered as an input. There are also other summarization tasks in which other types of input are assumed such as conversations or email threads (Duboue, 2012; Oya and Carenini, 2014; Oya et al., 2014). Unlike the researches, we assume a question a"
I17-1080,W12-1513,0,0.295908,"ctively applied to other sequence-tosequence tasks including sentence summarization (Rush et al., 2015; Kikuchi et al., 2016; Gu et al., 2016). Related Work Text summarization is one of the problems that have been studied for a long time in the field of natural language processing. In many of the existing summarization tasks including the shared tasks in Document Understanding Conference (DUC)2 , documents from newspapers or scientific articles are considered as an input. There are also other summarization tasks in which other types of input are assumed such as conversations or email threads (Duboue, 2012; Oya and Carenini, 2014; Oya et al., 2014). Unlike the researches, we assume a question as an input. As a related attempt in Question Answering researches, Tamura et al. (2005) worked on classification of multiple-sentence questions into classes such as yes/no questions and definition questions, and attempted to extract the question sentence that was the most important in finding the correct class. However, the extracted question sentence is not always a summary of the question. Consider the last sentence “Wiil it or will it not?” for the aforementioned Table 1 question. This last sentence is"
I17-1080,P16-1154,0,0.342455,"tractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al., 2004) or statistical machine translation models (Bank et al., 2010; Wubben et al., 2012; Cohn and Lapata, 2013) and templates (Oya et al., 2014). In addition, encoder-decoder approaches have been proposed in recent years. They were originally applied to machine translation tasks (Luong et al., 2015; Bahdanau et al., 2015; Cao et al., 2017), and have been actively applied to other sequence-tosequence tasks including sentence summarization (Rush et al., 2015; Kikuchi et al., 2016; Gu et al., 2016). Related Work Text summarization is one of the problems that have been studied for a long time in the field of natural language processing. In many of the existing summarization tasks including the shared tasks in Document Understanding Conference (DUC)2 , documents from newspapers or scientific articles are considered as an input. There are also other summarization tasks in which other types of input are assumed such as conversations or email threads (Duboue, 2012; Oya and Carenini, 2014; Oya et al., 2014). Unlike the researches, we assume a question as an input. As a related attempt in Ques"
I17-1080,I05-1038,1,0.821674,"zation is one of the problems that have been studied for a long time in the field of natural language processing. In many of the existing summarization tasks including the shared tasks in Document Understanding Conference (DUC)2 , documents from newspapers or scientific articles are considered as an input. There are also other summarization tasks in which other types of input are assumed such as conversations or email threads (Duboue, 2012; Oya and Carenini, 2014; Oya et al., 2014). Unlike the researches, we assume a question as an input. As a related attempt in Question Answering researches, Tamura et al. (2005) worked on classification of multiple-sentence questions into classes such as yes/no questions and definition questions, and attempted to extract the question sentence that was the most important in finding the correct class. However, the extracted question sentence is not always a summary of the question. Consider the last sentence “Wiil it or will it not?” for the aforementioned Table 1 question. This last sentence is important in finding the class of this question, which is a yes/no question, but is not appropriate as a summary, because it is impossible to understand what is being asked mer"
I17-1080,C02-1053,0,0.0734977,"of these methods through evaluations both by human judges and automatic scoring using Recall-Oriented Understudy for Gisting Evaluation (ROUGE) (Lin, 2004). The experimental results show that an abstractive approach using an encoder-decoder model with a copying mechanism achieves the highest score for both ROUGE and evaluations by human judges. 2 ROUGE score or the bigram frequency included in a summary (Peyrard and Eckale-Kohler, 2016; Li et al., 2013). Other approaches consider summarization tasks as a classification problem. They adopt supervised machine learning techniques to solve them (Hirao et al., 2002; Shen et al., 2007). Abstractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al., 2004) or statistical machine translation models (Bank et al., 2010; Wubben et al., 2012; Cohn and Lapata, 2013) and templates (Oya et al., 2014). In addition, encoder-decoder approaches have been proposed in recent years. They were originally applied to machine translation tasks (Luong et al., 2015; Bahdanau et al., 2015; Cao et al., 2017), and have been actively applied to other sequence-tosequence tasks including sentence summarization (Rush et al., 201"
I17-1080,P82-1020,0,0.846825,"Missing"
I17-1080,D16-1140,1,0.838233,"hen et al., 2007). Abstractive summarization approaches include methods based on syntactic transduction (Dorr et al., 2003; Zajic et al., 2004) or statistical machine translation models (Bank et al., 2010; Wubben et al., 2012; Cohn and Lapata, 2013) and templates (Oya et al., 2014). In addition, encoder-decoder approaches have been proposed in recent years. They were originally applied to machine translation tasks (Luong et al., 2015; Bahdanau et al., 2015; Cao et al., 2017), and have been actively applied to other sequence-tosequence tasks including sentence summarization (Rush et al., 2015; Kikuchi et al., 2016; Gu et al., 2016). Related Work Text summarization is one of the problems that have been studied for a long time in the field of natural language processing. In many of the existing summarization tasks including the shared tasks in Document Understanding Conference (DUC)2 , documents from newspapers or scientific articles are considered as an input. There are also other summarization tasks in which other types of input are assumed such as conversations or email threads (Duboue, 2012; Oya and Carenini, 2014; Oya et al., 2014). Unlike the researches, we assume a question as an input. As a relat"
I17-1080,P00-1041,0,\N,Missing
I17-1080,D10-1050,0,\N,Missing
I17-2002,2016.amta-researchers.10,0,0.0160326,"end of sentence symbols, respectively. Introduction The sequence-to-sequence (Seq2Seq) model has been successfully used in natural language generation tasks such as machine translation (MT) (Bahdanau et al., 2014) and text summarization (Rush et al., 2015). In the Seq2Seq model, attention, which encodes an input sentence by generating an alignment between output and input words, plays an important role. Without the attention mechanism, the performance of the Seq2Seq model degrades significantly (Bahdanau et al., 2014). To improve the alignment quality, Mi et al. (2016), Liu et al. (2016), and Chen et al. (2016) proposed a method that learns attention with the given alignments in a supervised manner, which is called supervised attention. By utilizing supervised attention, the translation quality of MT is improved. The Seq2Seq model can also be applied to other NLP tasks. We can regard parsing as a translation task from a sentence to an S-expression, and Vinyals et al. (2015) proposed a constituent parsing method based on the Seq2Seq model. Their method achieved the state-of-the-art performance. In their method, based on the alignment between a nonterminal and input words, the attention mechanism has"
I17-2002,P97-1003,0,0.247328,"nd Huang, 2016). We used dropout layers (Srivastava et al., 2014) to • Right word: On the contrary, the syntactic head of a simple English noun phrase is often at the end of the span. The alignment example in Fig. 3b is produced by this method, where an output token is linked to the rightmost word of the span. • Span word: Here, we unify the above two methods. All output tokens are linked to their leftmost word, except the ending bracket tokens, which are linked to their rightmost word. Figure 3c shows an alignment example produced by this method. • Lexical head: Lexicalization (Eisner, 1996; Collins, 1997), which annotates grammar nonterminals with their head words, is useful for resolving the syntactic ambiguities involved by such linguistic phenomena as co1 For head annotations, we used ptbconv 3.0 tool (Yamada and Matsumoto, 2003), which is available from http:// www.jaist.ac.jp/h-yamada/. 9 Setting P WSJ Section 22 R F1 AER P WSJ Section 23 R F1 AER Seq2Seq Seq2Seq+random Seq2Seq+first Seq2Seq+last 88.1 67.1 70.3 66.7 88.0 66.3 69.7 66.1 88.1 66.7 70.0 66.4 – 96.3 0.0 0.0 88.3 66.5 69.6 66.1 87.6 65.5 68.7 64.8 88.0 66.0 69.2 65.4 – 96.3 0.0 0.0 Seq2Seq+head Seq2Seq+left Seq2Seq+right Seq2S"
I17-2002,D16-1001,0,0.0389053,"data into three parts: The Wall Street Journal (WSJ) sections 02-21 for training, section 22 for development and section 23 for testing. In our models, the dimensions of the input word embeddings, the fed label embeddings, the hidden layers, and an attention vector were respectively set to 150, 30, 200, and 200. The LSTM depth was set to 3. Label set Lcon had a size of 61. The input vocabulary size of PTB was set to 42393. Supervised attention rate λ was set to 1.0. To use entire words as a vocabulary, we integrated word dropout (Iyyer et al., 2015) into our models with smoothing rate 0.8375 (Cross and Huang, 2016). We used dropout layers (Srivastava et al., 2014) to • Right word: On the contrary, the syntactic head of a simple English noun phrase is often at the end of the span. The alignment example in Fig. 3b is produced by this method, where an output token is linked to the rightmost word of the span. • Span word: Here, we unify the above two methods. All output tokens are linked to their leftmost word, except the ending bracket tokens, which are linked to their rightmost word. Figure 3c shows an alignment example produced by this method. • Lexical head: Lexicalization (Eisner, 1996; Collins, 1997),"
I17-2002,P15-1030,0,0.0325395,"rd LSTM to encode previously predicted label yt−1 into hidden state st . For each time t, with a 2-layer feed-forward neural network r, encoder and decoder hidden lay→ ers h and − s t are used to calculate the attention weight: → exp(r(hi , − s t )) . αti = ∑n → exp(r(h ′ , − s )) i i′ =1 − t=1 −λ × logP (yt |yt−1 , ..., y0 , x) n ∑ m ∑ i=1 t=1 ait × logαti , to jointly learn the attention and output distributions. All our alignments are represented by oneto-many links between input words x and nonterminals y. 3 Design of our Alignments In the traditional parsing framework (Hall et al., 2014; Durrett and Klein, 2015), lexical features have been proven to be useful in improving parsing performance. Inspired by previous work, we enhance the attention mechanism utilizing the linguistically-motivated annotations between surface words and nonterminals by supervised attention. In this paper, we define four types of alignments for supervised attention. The first three methods use the monolexical properties of heads without incurring any inferential costs of lexicalized annotations. Although the last needs manually constructed annotation schemes, it can capture bilexical relationships along dependency arcs. The f"
I17-2002,C96-1058,0,0.0611496,".8375 (Cross and Huang, 2016). We used dropout layers (Srivastava et al., 2014) to • Right word: On the contrary, the syntactic head of a simple English noun phrase is often at the end of the span. The alignment example in Fig. 3b is produced by this method, where an output token is linked to the rightmost word of the span. • Span word: Here, we unify the above two methods. All output tokens are linked to their leftmost word, except the ending bracket tokens, which are linked to their rightmost word. Figure 3c shows an alignment example produced by this method. • Lexical head: Lexicalization (Eisner, 1996; Collins, 1997), which annotates grammar nonterminals with their head words, is useful for resolving the syntactic ambiguities involved by such linguistic phenomena as co1 For head annotations, we used ptbconv 3.0 tool (Yamada and Matsumoto, 2003), which is available from http:// www.jaist.ac.jp/h-yamada/. 9 Setting P WSJ Section 22 R F1 AER P WSJ Section 23 R F1 AER Seq2Seq Seq2Seq+random Seq2Seq+first Seq2Seq+last 88.1 67.1 70.3 66.7 88.0 66.3 69.7 66.1 88.1 66.7 70.0 66.4 – 96.3 0.0 0.0 88.3 66.5 69.6 66.1 87.6 65.5 68.7 64.8 88.0 66.0 69.2 65.4 – 96.3 0.0 0.0 Seq2Seq+head Seq2Seq+left Seq"
I17-2002,P14-1022,0,0.0132644,"layer stacked forward LSTM to encode previously predicted label yt−1 into hidden state st . For each time t, with a 2-layer feed-forward neural network r, encoder and decoder hidden lay→ ers h and − s t are used to calculate the attention weight: → exp(r(hi , − s t )) . αti = ∑n → exp(r(h ′ , − s )) i i′ =1 − t=1 −λ × logP (yt |yt−1 , ..., y0 , x) n ∑ m ∑ i=1 t=1 ait × logαti , to jointly learn the attention and output distributions. All our alignments are represented by oneto-many links between input words x and nonterminals y. 3 Design of our Alignments In the traditional parsing framework (Hall et al., 2014; Durrett and Klein, 2015), lexical features have been proven to be useful in improving parsing performance. Inspired by previous work, we enhance the attention mechanism utilizing the linguistically-motivated annotations between surface words and nonterminals by supervised attention. In this paper, we define four types of alignments for supervised attention. The first three methods use the monolexical properties of heads without incurring any inferential costs of lexicalized annotations. Although the last needs manually constructed annotation schemes, it can capture bilexical relationships al"
I17-2002,P15-1162,0,0.0359884,"Missing"
I17-2002,C16-1291,0,0.145898,"and &lt;/s&gt; are start and end of sentence symbols, respectively. Introduction The sequence-to-sequence (Seq2Seq) model has been successfully used in natural language generation tasks such as machine translation (MT) (Bahdanau et al., 2014) and text summarization (Rush et al., 2015). In the Seq2Seq model, attention, which encodes an input sentence by generating an alignment between output and input words, plays an important role. Without the attention mechanism, the performance of the Seq2Seq model degrades significantly (Bahdanau et al., 2014). To improve the alignment quality, Mi et al. (2016), Liu et al. (2016), and Chen et al. (2016) proposed a method that learns attention with the given alignments in a supervised manner, which is called supervised attention. By utilizing supervised attention, the translation quality of MT is improved. The Seq2Seq model can also be applied to other NLP tasks. We can regard parsing as a translation task from a sentence to an S-expression, and Vinyals et al. (2015) proposed a constituent parsing method based on the Seq2Seq model. Their method achieved the state-of-the-art performance. In their method, based on the alignment between a nonterminal and input words, the"
I17-2002,D15-1166,0,0.106565,"odel and five-model ensembles. We used the products of the output probabilities for the ensemble. • Last word: All output tokens were linked to the end of the sentence tokens in the input sentence. We evaluated the compared methods using bracketing Precision, Recall and F-measure. We used evalb2 as a parsing evaluation. We also evalAll models were written in C++ on Dynet (Neubig et al., 2017). We compared Seq2Seq models with and with2 10 http://nlp.cs.nyu.edu/evalb/ uated the learned attention using alignment error rate (AER) (Och and Ney, 2003) on their alignments. Following a previous work (Luong et al., 2015), attention evaluation was conducted on gold output. on the English Penn Treebank against the baseline methods. These results emphasize, the effectiveness of our alignments in parsing performances. Acknowledgement We thank the anonymous reviewers for their careful reading and useful comments. 4.2 Results Table 1 shows the results. All our lexical head, left word, right word and span word alignments improved bracket F-measure of baseline on every setting. From the +random, +first, and +last results, only supervised attention itself did not improve the parsing performances. Furthermore, each AER"
I17-2002,D16-1249,0,0.0316216,"utput tokens. &lt;s&gt; and &lt;/s&gt; are start and end of sentence symbols, respectively. Introduction The sequence-to-sequence (Seq2Seq) model has been successfully used in natural language generation tasks such as machine translation (MT) (Bahdanau et al., 2014) and text summarization (Rush et al., 2015). In the Seq2Seq model, attention, which encodes an input sentence by generating an alignment between output and input words, plays an important role. Without the attention mechanism, the performance of the Seq2Seq model degrades significantly (Bahdanau et al., 2014). To improve the alignment quality, Mi et al. (2016), Liu et al. (2016), and Chen et al. (2016) proposed a method that learns attention with the given alignments in a supervised manner, which is called supervised attention. By utilizing supervised attention, the translation quality of MT is improved. The Seq2Seq model can also be applied to other NLP tasks. We can regard parsing as a translation task from a sentence to an S-expression, and Vinyals et al. (2015) proposed a constituent parsing method based on the Seq2Seq model. Their method achieved the state-of-the-art performance. In their method, based on the alignment between a nonterminal an"
I17-2002,J03-1002,0,0.00594689,"size was set to ten. The decoding was performed on both a single model and five-model ensembles. We used the products of the output probabilities for the ensemble. • Last word: All output tokens were linked to the end of the sentence tokens in the input sentence. We evaluated the compared methods using bracketing Precision, Recall and F-measure. We used evalb2 as a parsing evaluation. We also evalAll models were written in C++ on Dynet (Neubig et al., 2017). We compared Seq2Seq models with and with2 10 http://nlp.cs.nyu.edu/evalb/ uated the learned attention using alignment error rate (AER) (Och and Ney, 2003) on their alignments. Following a previous work (Luong et al., 2015), attention evaluation was conducted on gold output. on the English Penn Treebank against the baseline methods. These results emphasize, the effectiveness of our alignments in parsing performances. Acknowledgement We thank the anonymous reviewers for their careful reading and useful comments. 4.2 Results Table 1 shows the results. All our lexical head, left word, right word and span word alignments improved bracket F-measure of baseline on every setting. From the +random, +first, and +last results, only supervised attention it"
I17-2002,D15-1044,0,0.0712984,"rpus showed that the bracketing F-measure was improved by supervised attention. 1 (NP )S )NP (VP XX XX XX the chef cooks )VP (NP )NP XX XX the soup Figure 1: S-expression format for Vinyals et al. (2015)’s Seq2seq constituency parser. The Seq2seq model employs “&lt;s&gt; (S (NP XX XX )NP (VP XX (NP XX XX )NP )VP )S &lt;/s&gt;” as output tokens. &lt;s&gt; and &lt;/s&gt; are start and end of sentence symbols, respectively. Introduction The sequence-to-sequence (Seq2Seq) model has been successfully used in natural language generation tasks such as machine translation (MT) (Bahdanau et al., 2014) and text summarization (Rush et al., 2015). In the Seq2Seq model, attention, which encodes an input sentence by generating an alignment between output and input words, plays an important role. Without the attention mechanism, the performance of the Seq2Seq model degrades significantly (Bahdanau et al., 2014). To improve the alignment quality, Mi et al. (2016), Liu et al. (2016), and Chen et al. (2016) proposed a method that learns attention with the given alignments in a supervised manner, which is called supervised attention. By utilizing supervised attention, the translation quality of MT is improved. The Seq2Seq model can also be a"
I17-2002,W03-3023,0,0.135784,"produced by this method, where an output token is linked to the rightmost word of the span. • Span word: Here, we unify the above two methods. All output tokens are linked to their leftmost word, except the ending bracket tokens, which are linked to their rightmost word. Figure 3c shows an alignment example produced by this method. • Lexical head: Lexicalization (Eisner, 1996; Collins, 1997), which annotates grammar nonterminals with their head words, is useful for resolving the syntactic ambiguities involved by such linguistic phenomena as co1 For head annotations, we used ptbconv 3.0 tool (Yamada and Matsumoto, 2003), which is available from http:// www.jaist.ac.jp/h-yamada/. 9 Setting P WSJ Section 22 R F1 AER P WSJ Section 23 R F1 AER Seq2Seq Seq2Seq+random Seq2Seq+first Seq2Seq+last 88.1 67.1 70.3 66.7 88.0 66.3 69.7 66.1 88.1 66.7 70.0 66.4 – 96.3 0.0 0.0 88.3 66.5 69.6 66.1 87.6 65.5 68.7 64.8 88.0 66.0 69.2 65.4 – 96.3 0.0 0.0 Seq2Seq+head Seq2Seq+left Seq2Seq+right Seq2Seq+span 89.2 89.6 89.2 89.3 88.9 89.4 88.9 89.1 89.1 89.5 89.0 89.2 6.9 1.8 4.7 1.6 89.2 89.4 89.5 89.2 88.1 88.7 88.6 88.4 88.6 89.0 89.1 88.8 6.9 1.7 4.7 1.6 Vinyals et al. (2015) w att† Vinyals et al. (2015) w/o att† Seq2Seq+beam"
K19-1086,D15-1176,0,0.0954788,"Missing"
K19-1086,P16-1100,0,0.0275558,"addition, the improvements can be further obtained when our injection method is used together with the previous methods. We also conducted a comprehensive comparison of these injection methods. Finally, we set up several experiments to check the effects of infrequent words on our model, and we also compared our model with several previous work on 6 common language modeling datasets. Our results show that: network (CNN) to encode characters and then concatenated these encoded character-level representations and word-level representations for part-ofspeech tagging and named entity recognition. Luong and Manning (2016) introduced a characterword neural machine translation model that only consults character-level representations for rare words encoded with a deep LSTM. As research efforts for language models, Kang et al. (2011) used a simple character-word NLM designed for Chinese. Miyamoto and Cho (2016) introduced a gate mechanism between word embeddings and character embeddings obtained from a bidirectional LSTM (BiLSTM) for English. Verwimp et al. (2017) directly concatenated word and character embeddings without other subnetworks to encode the characters for English and Dutch. Although there are a numbe"
K19-1086,N18-1128,0,0.387595,"ethods. - When injecting word-level information into character-aware NLMs, discarding rare words in the training data can help improve the performance. 3 2 Related Work Model Description For language modeling, we basically use a LSTM network (Hochreiter and Schmidhuber, 1997). We denote the hidden state of LSTM for the t-th word wt as ht ∈ Rd , where d is the embedding size. We incorporate word-level information using the neural network shown in Figure 1. We describe the details in the following subsections. Many work have attempted to improve characteraware NLMs in recent years. For example, Assylbekov and Takhanov (2018) proposed several ways of reusing weights in character-aware NLMs. Gerz et al. (2018) achieved an improved result on 50 typologically diverse languages by injecting subword-level information into word vectors at the softmax. For a thorough review of past researches, readers are recommended to read the work by Vania and Lopez (2017), who performed a systematic comparison across different models based on different subword units (characters, character trigrams, BPE, etc.). One direction related to our research is to inject word-level information into character-aware neural models. Aside from lang"
K19-1086,Q17-1010,0,0.657439,"d seen in the training data is less than or equal to θ, we discard it. We refer the model that discards infrequent words as Char-BiLSTMadd-Word-LSTM-Word (g = 0.5, n = 1, θ = 5/15/25). The result is shown in Table 7. 6 Experiments on 6 Common Datasets In addition to the above datasets, we also set up 6 common language modeling datasets: English Penn Treebank (PTB) (Marcus et al., 1993) and 5 non-English datasets with rich morphology from the 2013 ACL Workshop on Machine Translation5 , which have been commonly used for evaluating character-aware NLMs (Botha and Blunsom, 2014; Kim et al., 2016; Bojanowski et al., 2017; Assylbekov and Takhanov, 2018). Since some of previous work has tested their model on PTB, we also included PTB in our experiment. We used the preprocessed small version of nonEnglish datasets by Botha and Blunsom (2014) and followed the same split as the previous work. The data statistics is provided in Table 9. The results of our proposed models and previous work are shown in Table 6. We used CharBiLSTM-LSTM and Char-BiLSTM-add-WordLSTM as baseline models. For our models, we set the frequency threshold θ to 5 and also set n to 2 as these settings help improve our character-aware NLMs, as d"
K19-1086,D16-1209,0,0.0369099,"Missing"
K19-1086,D15-1042,0,0.0705429,"Missing"
K19-1086,W15-3904,0,0.0789621,"Missing"
K19-1086,Q18-1032,0,0.0516269,"Missing"
K19-1086,P17-1184,0,0.26323,"anabu Okumura1 1 Tokyo Institute of Technology 2 National Institute of Advanced Industrial Science and Technology (AIST) {yukun@lr.,kamigaito@lr.,takamura@,oku@}pi.titech.ac.jp Abstract For example, although words “husbandman” and “salesman” share the suffix “man” in their surface forms, standard NLMs cannot capture such information in obtaining the relationship between the two words. A common way to deal with these issues is to use character information of each word to calculate the word representation, and it is often referred to as character-aware NLMs (Ling et al., 2015; Kim et al., 2016; Vania and Lopez, 2017; Gerz et al., 2018). Our research focuses on utilizing advantages of both character-level information and word-level information in characteraware NLMs. Previous work usually combines word-level information and character-level information at the input of LSTM layers through a gating mechanism, or averaging or concatenation of word vectors. Because these approaches generally target at the input vectors, the word-level information cannot be explicitly taken into account at the output layer for predicting the next word. To deal with this problem, we propose an improved character-aware neural lan"
K19-1086,E17-1040,0,0.0450402,"Missing"
mochizuki-okumura-2000-comparison,J91-1002,0,\N,Missing
mochizuki-okumura-2000-comparison,E99-1011,0,\N,Missing
mochizuki-okumura-2000-comparison,P93-1020,0,\N,Missing
mochizuki-okumura-2000-comparison,C96-2166,0,\N,Missing
N07-1037,P97-1023,0,0.533087,"e the semantic orientations of the adjective-noun pairs. Information from seed words is diffused to unseen nouns on the network. We also propose a method for enlarging the seed set by using the output of an existing method for the seed words of the probability computation. Empirical evaluation shows that our method works well both for seen and unseen nouns, and that the enlarged seed set significantly improves the classification performance of the proposed model. 2 Related Work The semantic orientation classification of words has been pursued by several researchers. Some of them used corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al., 2001; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2005). Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classifi"
N07-1037,kamps-etal-2004-using,0,0.1309,"Missing"
N07-1037,W06-1642,0,0.148491,"mantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words is used. In addition to individual seed words, Kanayama and Nasukawa (2006) used more complicated syntactic patterns that were manually created. The four methods above are based on context information. In contrast, our method exploits the internal structure of the semantic orientations of phrases. Wilson et al. (2005) worked on phrase-level semantic orientations. They introduced a polarity shifter. They manually created the list of polarity shifters. Inui (2004) also proposed a similar idea. 293 Takamura et al. (2006) proposed to use based on latent variable models for sentiment classification of noun-adjective pairs. Their model consists of variables respectively re"
N07-1037,P04-3020,0,0.0140355,"Missing"
N07-1037,H05-1052,0,0.0152758,"Missing"
N07-1037,J93-1007,0,0.0254585,"siloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al., 2001; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2005). Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words is used. In addition to individual seed words, Kanayama and Nasukawa (2006) used more complicated syntactic patterns that were manually created. The four methods above are based on context information. In contrast, our method exploits the internal structure of the semantic orientations of phrases. Wilson et al. (2005) worked on phrase-level semantic orientations. They introduced a polarity shifter. They manually cr"
N07-1037,P05-1017,1,0.762674,"opose a method for enlarging the seed set by using the output of an existing method for the seed words of the probability computation. Empirical evaluation shows that our method works well both for seen and unseen nouns, and that the enlarged seed set significantly improves the classification performance of the proposed model. 2 Related Work The semantic orientation classification of words has been pursued by several researchers. Some of them used corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al., 2001; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2005). Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in t"
N07-1037,E06-1026,1,0.848075,"ation, the proposed method can classify phrases consisting of unseen words. We also propose to use unlabeled data for a seed set of probability computation. Empirical evaluation shows the effectiveness of the proposed method. 1 Introduction Technology for affect analysis of texts has recently gained attention in both academic and industrial areas. It can be applied to, for example, a survey of new products or a questionnaire analysis. Automatic sentiment analysis enables a fast and comprehensive investigation. A computational model for the semantic orientations of phrases has been proposed by Takamura et al. (2006). However, their method cannot deal with the words that did not appear in the training data. The purpose of this paper is to propose a method for extracting semantic orientations of phrases, which is applicable also to expressions consisting of unseen words. In our method, we regard this task as the noun classification problem for each adjective; the nouns that become respectively positive (negative, or neutral) when combined with a given adjective are distinguished from the other nouns. We create a lexical network with words being nodes, by connecting two words if one of the two appears in th"
N07-1037,P02-1053,0,0.0201361,"he output of an existing method for the seed words of the probability computation. Empirical evaluation shows that our method works well both for seen and unseen nouns, and that the enlarged seed set significantly improves the classification performance of the proposed model. 2 Related Work The semantic orientation classification of words has been pursued by several researchers. Some of them used corpora (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003), while others used dictionaries (Kobayashi et al., 2001; Kamps et al., 2004; Takamura et al., 2005; Esuli and Sebastiani, 2005). Turney (2002) applied an internet-based technique to the semantic orientation classification of phrases, which had originally been developed for word sentiment classification. In their method, the number of hits returned by a search-engine, with a query consisting of a phrase and a seed word (e.g., “phrase NEAR good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words i"
N07-1037,P06-1134,0,0.0176434,"639/2160) unambiguous ambiguous 94.85 65.25 (736/776) (903/1384) total 69.59 (6086/8746) 79.68 (6969/8746) Table 2: Confusion matrices of classification result with labeled+unlabeled seed set Potts model Gold standard positive neutral negative sum positive 964 198 39 1201 seen nouns neutral negative 254 60 1656 286 397 1544 2307 1890 the difference between them; “high salary” is positive, while “low (cheap) commission” is also positive. 6 sum 1278 2140 1980 5398 positive 126 60 46 232 unseen nouns neutral negative 84 30 427 104 157 350 668 484 sum 240 591 553 1384 ity have strong interaction (Wiebe and Mihalcea, 2006). • The value of α must be properly set, because lower α can be better for the seed words added by the classifier, Conclusion We proposed a method for extracting semantic orientations of phrases (pairs of an adjective and a noun). For each adjective, we constructed a Potts system, which is actually a lexical network extracted from glosses in a dictionary. We empirically showed that the proposed method works well in terms of classification accuracy. Future work includes the following: • We assumed that each word has a semantic orientation. However, word senses and subjectiv298 • To address word"
N07-1037,H05-1044,0,0.0217498,"R good”) is used to determine the orientation. Baron and Hirst (2004) extracted collocations with Xtract (Smadja, 1993) and classified the collocations using the orientations of the words in the neighboring sentences. Their method is similar to Turney’s in the sense that cooccurrence with seed words is used. In addition to individual seed words, Kanayama and Nasukawa (2006) used more complicated syntactic patterns that were manually created. The four methods above are based on context information. In contrast, our method exploits the internal structure of the semantic orientations of phrases. Wilson et al. (2005) worked on phrase-level semantic orientations. They introduced a polarity shifter. They manually created the list of polarity shifters. Inui (2004) also proposed a similar idea. 293 Takamura et al. (2006) proposed to use based on latent variable models for sentiment classification of noun-adjective pairs. Their model consists of variables respectively representing nouns, adjectives, semantic orientations, and latent clusters, as well as the edges between the nodes. The words that are similar in terms of semantic orientations, such as “risk” and “mortality” (i.e., the positive orientation emerg"
N15-1150,D11-1054,0,\N,Missing
N15-1150,W04-3243,0,\N,Missing
N15-1150,C00-2163,0,\N,Missing
N15-1150,J93-2003,0,\N,Missing
N15-1150,J90-2002,0,\N,Missing
N15-1150,P02-1040,0,\N,Missing
N15-1150,P13-1095,0,\N,Missing
N15-1150,P07-2045,0,\N,Missing
N15-1150,N03-1017,0,\N,Missing
N15-1150,2011.iwslt-evaluation.18,0,\N,Missing
N15-1150,P00-1056,0,\N,Missing
N15-1150,W11-2123,0,\N,Missing
nanba-okumura-2002-examinations,W00-0408,0,\N,Missing
nanba-okumura-2002-examinations,W00-0403,0,\N,Missing
nanba-okumura-2002-examinations,W01-0100,0,\N,Missing
nanba-okumura-2004-comparison,W00-0408,0,\N,Missing
nanba-okumura-2004-comparison,P02-1040,0,\N,Missing
nanba-okumura-2004-comparison,N03-1020,0,\N,Missing
nanba-okumura-2004-comparison,nanba-okumura-2002-examinations,1,\N,Missing
P05-1017,J96-1002,0,0.040049,"ground. We have an objective function and its approximation method. We thus have a measure of goodness in model estimation and can use another better approximation method, such as Bethe approximation (Tanaka et al., 2003). The theory tells us which update rule to use. We also have a notion of magnetization, which can be used for hyperparameter estimation. We can use a plenty of knowledge, methods and algorithms developed in the field of statistical mechanics. We can also extend our model to a multiclass model (Q-Ising model). Another interesting point is the relation to maximum entropy model (Berger et al., 1996), which is popular in the natural language processing community. Our model can be obtained by maximizing the entropy of the probability distribution Q(x) under constraints regarding the energy function. 5 Experiments We used glosses, synonyms, antonyms and hypernyms of WordNet (Fellbaum, 1998) to construct an English lexical network. For part-of-speech tagging and lemmatization of glosses, we used TreeTagger (Schmid, 1994). 35 stopwords (quite frequent words such as “be” and “have”) are removed from the lexical network. Negation words include 33 words. In addition to usual negation words such"
P05-1017,H92-1046,0,0.0781048,"ined according to the averages values of the spins. Despite the heuristic flavor of this decision rule, it has a theoretical background related to maximizer of posterior marginal (MPM) estimation, or ‘finite-temperature decoding’ (Iba, 1999; Marroquin, 1985). In MPM, the average is the marginal distribution over xi obtained from the distribution over x. We should note that the finite-temperature decoding is quite different from annealing type algorithms or ‘zero-temperature decoding’, which correspond to maximum a posteriori (MAP) estimation and also often used in natural language processing (Cowie et al., 1992). Since the model estimation has been reduced to simple update calculations, the proposed model is similar to conventional spreading activation approaches, which have been applied, for example, to word sense disambiguation (Veronis and Ide, 1990). Actually, the proposed model can be regarded as a spreading activation model with a specific update rule, as long as we are dealing with 2-class model (2-Ising model). However, there are some advantages in our modelling. The largest advantage is its theoretical background. We have an objective function and its approximation method. We thus have a mea"
P05-1017,P97-1023,0,0.979087,"for extraction of semantic orientations of words. To calculate the association strength of a word with positive (negative) seed words, they used the number of hits returned by a search engine, with a query consisting of the word and one of seed words (e.g., “word NEAR good”, “word NEAR bad”). They regarded the difference of two association strengths as a measure of semantic orientation. They also proposed to use Latent Semantic Analysis to compute the association strength with seed words. An empirical evaluation was conducted on 3596 words extracted from General Inquirer (Stone et al., 1966). Hatzivassiloglou and McKeown (1997) focused on conjunctive expressions such as “simple and 133 Proceedings of the 43rd Annual Meeting of the ACL, pages 133–140, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics well-received” and “simplistic but well-received”, where the former pair of words tend to have the same semantic orientation, and the latter tend to have the opposite orientation. They first classify each conjunctive expression into the same-orientation class or the different-orientation class. They then use the classified expressions to cluster words into the positive class and the negative class. T"
P05-1017,W03-0404,0,0.443684,"Missing"
P05-1017,C90-2067,0,0.0245303,"arroquin, 1985). In MPM, the average is the marginal distribution over xi obtained from the distribution over x. We should note that the finite-temperature decoding is quite different from annealing type algorithms or ‘zero-temperature decoding’, which correspond to maximum a posteriori (MAP) estimation and also often used in natural language processing (Cowie et al., 1992). Since the model estimation has been reduced to simple update calculations, the proposed model is similar to conventional spreading activation approaches, which have been applied, for example, to word sense disambiguation (Veronis and Ide, 1990). Actually, the proposed model can be regarded as a spreading activation model with a specific update rule, as long as we are dealing with 2-class model (2-Ising model). However, there are some advantages in our modelling. The largest advantage is its theoretical background. We have an objective function and its approximation method. We thus have a measure of goodness in model estimation and can use another better approximation method, such as Bethe approximation (Tanaka et al., 2003). The theory tells us which update rule to use. We also have a notion of magnetization, which can be used for h"
P05-1017,kamps-etal-2004-using,0,\N,Missing
P06-1105,W02-2016,0,0.828969,"y, however, to also consider features for the contextual information of the two bunsetsus. One such feature is the constraint that two case elements with the same case do not modify a verb. Statistical Japanese dependency analysis takes into account syntactic information but tends not to take into account lexical information, such as cooccurrence between a case element and a verb. Introduction The recent availability of more corpora has enabled much information about dependency relations to be obtained by using a Japanese dependency analyzer such as KNP (Kurohashi and Nagao, 1994) or CaboCha (Kudo and Matsumoto, 2002). Although this information is less accurate than manually annotated information, these automatic analyzers provide a large amount of co-occurrence information as well as information about combinations of multiple cases that tend to modify a verb. Dependency parsing is a basic technology for processing Japanese and has been the subject of much research. The Japanese dependency structure is usually represented by the relationship between phrasal units called bunsetsu, each of which consists of one or more content words that may be followed by any number of function words. The dependency between"
P06-1105,P05-1022,0,0.0110448,"es a latent semantic class of cooccurrence (hidden class). Probabilistic parameters P (n|z), P (hr, vi|z), and P (z) in Equation (9) can be estimated by using the EM algorithm. In our experiments, the dimension of the hidden class z was set to 300. As a result, the collected hn, r, vi total 102,581,924 pairs. The number of n and v is 57,315 and 15,098, respectively. The particles for which the co-occurrence probability was estimated were the set of case particles, the “ha” case particle, and a class of “fukujoshi” Many methods for reranking the parsing of English sentences have been proposed (Charniak and Johnson, 2005; Collins and Koo, 2005; Henderson and Titov, 2005), all of which are discriminative methods which learn the difference between the best parse and next-best parses. While our reranking model using generation probability is quite simple, we can easily verify our hypothesis that the two proposed probabilities have an effect on improving the parsing accuracy. We can also verify that the parsing accuracy improves by using imprecise information obtained from an automatically parsed corpus. Klein and Manning proposed a generative model in which syntactic (PCFG) and semantic (lexical dependency) stru"
P06-1105,C04-1002,0,0.449025,"e cases One constraint in Japanese is that multiple nouns of the same case do not modify a verb. Previous work on Japanese dependency analysis has assumed that all the dependency relations are independent of one another. It is therefore necessary to also consider such a constraint as a feature for contextual information. Uchimoto et al., for example, used as such a feature whether a particular type of bunsetsu is between two bunsetsus in a dependency relation (Uchimoto et al., 1999), and Sassano used information about what is just before and after the modifying bunsetsu and modifyee bunsetsu (Sassano, 2004). In the artiﬁcial example shown in Figure 1, it is natural to consider that “keisatsu-de” will modify “hogo-shita”. Statistical Japanese dependency analyzers (Uchimoto et al., 2000; Kudo and Matsumoto, 2002), however, will output the result where “keisatsu-de” modiﬁes “arui-teiru”. This is because in sentences without internal punctuation a noun tends to modify the nearest verb, and these analyzers do not take into account a combination of multiple cases. Another kind of information useful in dependency analysis is the co-occurrence of a noun and a verb, which indicates to what degree the nou"
P06-1105,J05-1003,0,0.0337533,"f cooccurrence (hidden class). Probabilistic parameters P (n|z), P (hr, vi|z), and P (z) in Equation (9) can be estimated by using the EM algorithm. In our experiments, the dimension of the hidden class z was set to 300. As a result, the collected hn, r, vi total 102,581,924 pairs. The number of n and v is 57,315 and 15,098, respectively. The particles for which the co-occurrence probability was estimated were the set of case particles, the “ha” case particle, and a class of “fukujoshi” Many methods for reranking the parsing of English sentences have been proposed (Charniak and Johnson, 2005; Collins and Koo, 2005; Henderson and Titov, 2005), all of which are discriminative methods which learn the difference between the best parse and next-best parses. While our reranking model using generation probability is quite simple, we can easily verify our hypothesis that the two proposed probabilities have an effect on improving the parsing accuracy. We can also verify that the parsing accuracy improves by using imprecise information obtained from an automatically parsed corpus. Klein and Manning proposed a generative model in which syntactic (PCFG) and semantic (lexical dependency) structures are scored with"
P06-1105,W98-1510,0,0.0339265,"next-best parses in which the verb to modify was different from that in the ﬁrstranked one. For example, parses 1 and 3 in Figure 2 are the only candidates for reranking. In our experiments, n is set to 50. The score we used for reranking the parses was the product of the probability of the posterior context model and the probability of our proposed model: score = Pcontext (T )α × P (T ), they do not take into account the combination of dependencies. Shirai et al. also proposed a statistical model of Japanese language which integrates lexical association statistics with syntactic preference (Shirai et al., 1998). Our proposed model differs from their method in that it explicitly uses the combination of multiple cases. 5.3 Estimation of co-occurrence probability (8) where Pcontext (T ) is the probability of the posterior context model. The α here is a parameter with which we can adjust the balance of the two probabilities, and is ﬁxed to the best value by considering development data (different from the training data)1 . Reranking Candidate 1 Candidate Candidate 2 Candidate 3 Candidate We estimated the co-occurrence probability of the particle set and the co-occurrence probability of the case element"
P06-1105,J02-3001,0,0.0144358,"ticles rsi (T ). P (ni,j (T )|ri,j (T ), vi (T )) (5) j=1 We call P (rsi (T )|vi (T )) the co-occurrence probability of the particle set and the verb, and we call P (ni,j (T )|ri,j (T ), vi (T )) the co-occurrence probability of the case element set and the verb. In the actual dependency analysis, we try to select the dependency structure Tˆ that maximizes the Equation (5) from the possible parses T for the inputted sentence: m(T ) Tˆ = argmax T ∏ P (rsi (T )|vi (T )) × i=1 ci (T ) ∏ P (ni,j (T )|ri,j (T ), vi (T )). (6) j=1 The proposed model is inspired by the semantic role labeling method (Gildea and Jurafsky, 2002), which uses the frame element group in place of the particle set. It differs from the previous parsing models in that we take into account the dependency relations among particles in the set of case elements that modify a verb. This information can constrain the combination of particles (cases) among bunsetsus that modify a verb. Assuming the independence among particles, we can rewrite Equation (5) as def P (esi (T )|vi (T )) = P (rsi (T ), nsi (T )|vi (T )) (1) = P (rsi (T )|vi (T )) × P (nsi (T )|rsi (T ), vi (T )) (2) &apos; P (rsi (T )|vi (T )) × m(T ) ci (T ) P (T ) = ci (T ) ∏ ∏ ∏ P (ni,j ("
P06-1105,P05-1023,0,0.0299095,"class). Probabilistic parameters P (n|z), P (hr, vi|z), and P (z) in Equation (9) can be estimated by using the EM algorithm. In our experiments, the dimension of the hidden class z was set to 300. As a result, the collected hn, r, vi total 102,581,924 pairs. The number of n and v is 57,315 and 15,098, respectively. The particles for which the co-occurrence probability was estimated were the set of case particles, the “ha” case particle, and a class of “fukujoshi” Many methods for reranking the parsing of English sentences have been proposed (Charniak and Johnson, 2005; Collins and Koo, 2005; Henderson and Titov, 2005), all of which are discriminative methods which learn the difference between the best parse and next-best parses. While our reranking model using generation probability is quite simple, we can easily verify our hypothesis that the two proposed probabilities have an effect on improving the parsing accuracy. We can also verify that the parsing accuracy improves by using imprecise information obtained from an automatically parsed corpus. Klein and Manning proposed a generative model in which syntactic (PCFG) and semantic (lexical dependency) structures are scored with separate models (Klein and M"
P06-1105,E99-1026,0,0.375144,"showed that our method can improve the accuracy of the results obtained using the existing method. 1 Work on statistical Japanese dependency analysis has usually assumed that all the dependency relations in a sentence are independent of each other, and has considered the bunsetsus in a sentence independently when judging whether or not a pair of bunsetsus is in a dependency relation. In judging which bunsetsu a bunsetsu modiﬁes, this type of work has used as features the information of two bunsetsus, such as the head words of the two bunsetsus, and the morphemes at the ends of the bunsetsus (Uchimoto et al., 1999). It is necessary, however, to also consider features for the contextual information of the two bunsetsus. One such feature is the constraint that two case elements with the same case do not modify a verb. Statistical Japanese dependency analysis takes into account syntactic information but tends not to take into account lexical information, such as cooccurrence between a case element and a verb. Introduction The recent availability of more corpora has enabled much information about dependency relations to be obtained by using a Japanese dependency analyzer such as KNP (Kurohashi and Nagao, 19"
P06-1105,N04-1037,0,0.0561258,"formation between bunsetsus such as punctuation and the distance between bunsetsus. The weight of given features is learned 834 In summary, we think that statistical Japanese dependency analysis needs to take into account at least two more kinds of information: the dependency relation between multiple cases where multiple nouns of the same case do not modify a verb, and the co-occurrence of nouns and verbs. One way to use such information in statistical dependency analysis is to directly use it as features. However, Kehler et al. pointed out that this does not make the analysis more accurate (Kehler et al., 2004). This paper therefore presents a model that uses the co-occurrence information separately and reranks the analysis candidates generated by the existing machine learning model. 4 In the transformation from Equation (2) to Equation (3), we assume that the set of noun nsi (T ) is independent of the verb vi (T ). And in the transformation from Equation (3) to Equation (4), we assume that the noun ni,j (T ) is dependent on only its following particle ri,j (T ). Now we assume the dependency structure T of the whole sentence is composed of only the dependency relation between case elements and verbs"
P06-1105,2000.iwpt-1.43,0,0.0720801,"ency relations are independent of one another. It is therefore necessary to also consider such a constraint as a feature for contextual information. Uchimoto et al., for example, used as such a feature whether a particular type of bunsetsu is between two bunsetsus in a dependency relation (Uchimoto et al., 1999), and Sassano used information about what is just before and after the modifying bunsetsu and modifyee bunsetsu (Sassano, 2004). In the artiﬁcial example shown in Figure 1, it is natural to consider that “keisatsu-de” will modify “hogo-shita”. Statistical Japanese dependency analyzers (Uchimoto et al., 2000; Kudo and Matsumoto, 2002), however, will output the result where “keisatsu-de” modiﬁes “arui-teiru”. This is because in sentences without internal punctuation a noun tends to modify the nearest verb, and these analyzers do not take into account a combination of multiple cases. Another kind of information useful in dependency analysis is the co-occurrence of a noun and a verb, which indicates to what degree the noun tends to modify the verb. In the above example, the possible modifyees of “keisatsu-de” are “aruiteiru” and “hogo-shita”. Taking into account information about the co-occurrence o"
P06-1145,W00-0730,0,0.1001,"Missing"
P06-1145,P00-1010,0,0.111276,"Missing"
P06-1145,W01-1311,0,0.0585789,"Missing"
P06-1145,sekine-isahara-2000-irex,0,\N,Missing
P06-2002,W03-0405,0,0.115841,"e only applicable to the desired relationship. Patterns like <target> ’s <hook> are very likely to be applicable to all of these relationships at the same time, so we would like to be able to discard them automatically. Hence, we propose the following improvements for a Rote extractor: • A new pattern generalization procedure that allows the inclusion of wildcards in the patterns. • The combination with Named Entity recognition, so people, locations, organizations and dates are replaced by their entity type in the patterns, in order to increase their degree of generality. This is in line with Mann and Yarowsky (2003)’s modification, consisting in replacing all numbers in the patterns with the symbol ####. • A new precision calculation procedure, in a way that the patterns obtained for a given relationship are evaluated on the corpus for different relationships, in order to improve the detection of over-general patterns. <hook> was born in <target> <hook> ( <target> - 1870 ) In order to measure the precision of the extracted patterns, a new corpus is downloaded using the hook Dickens as the only query word, and the system looks for appearances of the patterns in the corpus. For every occurrence in which th"
P06-2002,P05-1060,0,0.384876,"avichandran and Hovy, 2002). 5. Download a separate corpus, called hook corpus, containing just the hook (in the example, Dickens). 6. Apply the previous patterns to the hook corpus, calculate the precision of each pattern Introduction Recently, there is an increasing interest in automatically extracting structured information from large corpora and, in particular, from the Web (Craven et al., 1999). Because of the difficulty of collecting annotated data, several procedures have been described that can be trained on unannotated textual corpora (Riloff and Schmelzenbach, 1998; Soderland, 1999; Mann and Yarowsky, 2005). An interesting approach is that of rote extractors (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002), which look for textual contexts that happen to convey a certain relationship between two concepts. In this paper, we describe some contributions to the training of Rote extractors, including a procedure for generalizing the patterns, and a more complex way of calculating their accuracy. We first introduce the general structure of a rote extractor and its limitations. Next, we describe the proposed modifications (Sections 2, 3 and 4) and the evaluation performed (Section"
P06-2002,J93-2004,0,0.0400917,"Missing"
P06-2002,P02-1006,0,0.788622,"e, Dickens). 6. Apply the previous patterns to the hook corpus, calculate the precision of each pattern Introduction Recently, there is an increasing interest in automatically extracting structured information from large corpora and, in particular, from the Web (Craven et al., 1999). Because of the difficulty of collecting annotated data, several procedures have been described that can be trained on unannotated textual corpora (Riloff and Schmelzenbach, 1998; Soderland, 1999; Mann and Yarowsky, 2005). An interesting approach is that of rote extractors (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002), which look for textual contexts that happen to convey a certain relationship between two concepts. In this paper, we describe some contributions to the training of Rote extractors, including a procedure for generalizing the patterns, and a more complex way of calculating their accuracy. We first introduce the general structure of a rote extractor and its limitations. Next, we describe the proposed modifications (Sections 2, 3 and 4) and the evaluation performed (Section 5). 9 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 9–16, c Sydney, July 2006. 2006 Association"
P06-2002,W98-1106,0,0.40608,"trings obtained with a suffix tree constructor (Ravichandran and Hovy, 2002). 5. Download a separate corpus, called hook corpus, containing just the hook (in the example, Dickens). 6. Apply the previous patterns to the hook corpus, calculate the precision of each pattern Introduction Recently, there is an increasing interest in automatically extracting structured information from large corpora and, in particular, from the Web (Craven et al., 1999). Because of the difficulty of collecting annotated data, several procedures have been described that can be trained on unannotated textual corpora (Riloff and Schmelzenbach, 1998; Soderland, 1999; Mann and Yarowsky, 2005). An interesting approach is that of rote extractors (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002), which look for textual contexts that happen to convey a certain relationship between two concepts. In this paper, we describe some contributions to the training of Rote extractors, including a procedure for generalizing the patterns, and a more complex way of calculating their accuracy. We first introduce the general structure of a rote extractor and its limitations. Next, we describe the proposed modifications (Sections 2, 3 an"
P06-2002,P99-1008,0,0.138668,"Missing"
P06-2002,C92-2082,0,\N,Missing
P06-2078,W00-0408,0,0.224264,"per, we investigate an automatic evaluation method that can reduce the errors of traditional automatic methods by using several evaluation results obtained manually. We conducted some experiments using the data of the Text Summarization Challenge 2 (TSC-2). A comparison with conventional automatic methods shows that our method outperforms other methods usually used. 1 Introduction Recently, the evaluation of computer-produced summaries has become recognized as one of the problem areas that must be addressed in the field of automatic summarization. To solve this problem, a number of automatic (Donaway et al., 2000, Hirao et al., 2005, Lin et al., 2003, Lin, 2004, Hori et al., 2003) and manual methods (Nenkova et al., 2004, Teufel et al., 2004) have been proposed. Manual methods evaluate summaries correctly, because humans evaluate them, but are costly. On the other hand, automatic methods, which use evaluation tools or programs, are low cost, although these methods cannot evaluate summaries as accurately as manual methods. In this paper, we investigate an automatic method that can reduce the errors of traditional automatic methods by using several evaluation results obtained manually. Unlike other auto"
P06-2078,N03-1020,0,0.133374,"Missing"
P06-2078,W04-1013,0,0.376738,"reduce the errors of traditional automatic methods by using several evaluation results obtained manually. We conducted some experiments using the data of the Text Summarization Challenge 2 (TSC-2). A comparison with conventional automatic methods shows that our method outperforms other methods usually used. 1 Introduction Recently, the evaluation of computer-produced summaries has become recognized as one of the problem areas that must be addressed in the field of automatic summarization. To solve this problem, a number of automatic (Donaway et al., 2000, Hirao et al., 2005, Lin et al., 2003, Lin, 2004, Hori et al., 2003) and manual methods (Nenkova et al., 2004, Teufel et al., 2004) have been proposed. Manual methods evaluate summaries correctly, because humans evaluate them, but are costly. On the other hand, automatic methods, which use evaluation tools or programs, are low cost, although these methods cannot evaluate summaries as accurately as manual methods. In this paper, we investigate an automatic method that can reduce the errors of traditional automatic methods by using several evaluation results obtained manually. Unlike other automatic methods, our method estimates manual evalua"
P06-2078,nanba-okumura-2004-comparison,1,0.881875,"Missing"
P06-2078,N04-1019,0,0.0601586,"Missing"
P06-2078,2001.mtsummit-papers.68,0,0.0167669,"eplacement). The degree of the human revision, called the “edit distance,” is computed from the number of revised characters divided by the number of characters in the original summary. If the summary’s quality was so low that a revision of more than half of the original summary was required, the judges stopped the revision and a score of 0.5 was given. The effectiveness of evaluation by the revision method was confirmed in our previous work (Nanba et al., 2004). We compared evaluation by revision with ranking evaluation. We also tested other automatic methods: content-based evaluation, BLEU (Papineni et al., 2001) and ROUGE-1 (Lin, 2004), and compared their results with that of evaluation by revision as reference. As a result, we found that evaluation by revision is effective for recognizing slight differences between computer-produced summaries. 4.5 Experimental Results and Discussion Exp-1: An experiment for Points 2 and 3 based on Kazawa’s method To address Points 2 and 3, we evaluated summaries by the method based on Kazawa’s method using 12 measures, described in Section 4.4, as measures to calculate topical similarities between summaries, and compared these measures by Gap. The experimental resul"
P06-2078,C92-2067,0,0.299602,"summary to be evaluated and the pooled summaries? Kazawa et al. used Equation 2 to calculate similarities. Sim ( x, xij ) = min( |xij |, |x |) (2) where xij ∩ x indicates the number of discourse units1 that appear in both xij and x, and |x |represents the number of words in x. However, there are many other measures that can be used to calculate the topical similarities between two documents (or passages). As well as Yasuda’s method does, using WH is another way to calculate similarities between a summary to be evaluated and pooled summaries indirectly. Yasuda et al. (2003) tested DP matching (Su et al., 1992), BLEU (Papineni et al., 2002), and NIST2 , for the calculation of WH. However there are many other measures for summary evaluation. Investigation of an Automatic Method using Multiple Manual Evaluation Results Overview of Our Evaluation Method and Essential Points to be Discussed We investigate an automatic method using multiple evaluation results by a manual method based on Kazawa’s and Yasuda’s method. The procedure of our evaluation method is shown as follows; |xij ∩ x | 1 Rhetorical Structure Theory Discourse Treebank. www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalog Id=LDC2002T07 Lingu"
P06-2078,W04-3254,0,0.030238,"Missing"
P06-2078,E03-1010,0,\N,Missing
P06-2078,P02-1040,0,\N,Missing
P06-2078,H05-1019,1,\N,Missing
P11-2039,R09-1008,0,0.0290907,"zivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in documents: single words and events (named entity pairs with a verb or a noun). While their work was for generic summarization, our method is designed specifically for query-oriented summarization. MMR-based methods are also popular for queryoriented summarization (Jagarlamudi et al., 2005; Li et al., 2008; Hasegawa et al., 2010; Lin et al., 2010b). Moreover, graph-based methods for summarization and sentence retrieval are popular (Otterbacher et al., 2005; Varadarajan and Hristidis, 2006; 224 Bosma, 2009). Unlike existing graph-based methods, our method explicitly computes indirect relationships between the query and words in the documents to enrich the information need representation. To this end, our method utilizes within-sentence cooccurrences of words. The approach taken by Jagarlamudi et al. (2005) is similar to our proposed method in that it uses word co-occurrence and dependencies within sentences in order to measure relevance of words to the query. However, while their approach measures the generic relevance of each word based on Hyperspace Analogue to Language (Lund and Burgess, 1996"
P11-2039,P10-1084,0,0.0315407,"ugget example 2 ロサンゼルス 批評 家 協会 賞 の アニメ 賞 Los Angeles Film Critics Association Award for Best Animated Film Figure 1: Question and gold-standard nuggets example in NTCIR-8 ACLIA2 dataset We evaluate our proposed method using Japanese complex question answering test collections from NTCIR ACLIA–Advanced Cross-lingual Information Access task (Mitamura et al., 2008; Mitamura et al., 2010). However, our method can easily be extended for handling other languages. 2 Related Work Much work has been done for generic multidocument summarization (Takamura and Okumura, 2009a; Takamura and Okumura, 2009b; Celikyilmaz and Hakkani-Tur, 2010; Lin et al., 2010a; Lin and Bilmes, 2010). Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) criteria for non-redundant sentence selection, which consist of document similarity and redundancy penalty. McDonald (2007) presented an approximate dynamic programming approach to maximize the MMR criteria. Yih et al. (2007) formulated the document summarization problem as an MCKP, and proposed a supervised method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in docume"
P11-2039,C04-1057,0,0.212056,"mmarization (Liu and Liu, 2009), in particular, on sentence selection from a given set of source documents that contain relevant sentences. One well-known challenge in selecting sentences relevant to the information need is the vocabulary mismatch between the query (i.e. information need representation) and the candidate sentences. Hence, to enrich the information need representation, we build a co-occurrence 223 We therefore use word pairs as the basic unit for computing sentence scores, and then formulate the summarization problem as a Maximum Cover Problem with Knapsack Constraints (MCKP) (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a). This problem is an optimization problem that maximizes the total score of words covered by a summary under a summary length limit. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 223–229, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics • Question Sen to Chihiro no Kamikakushi (Spirited Away) is a full-length animated movie from Japan. The user wants to know how it was received overseas. • Nugget example 1 全米 映画 批評 会議 の アニメ 賞 National Board of Review of Motion Pictures"
P11-2039,W00-1303,0,0.0533003,"r-expression-based heuristics to detect glossary of terms in articles. As the descriptions of these glossaries are usually very useful for answering BIOGRAPHY and DEFINITION questions, we treated each term description (generally multiple sentences) as a single sentence. 226 We used Mecab (Kudo et al., 2004) for morphological analysis, and calculated base word scores sb (w) using Mainichi articles from 1991 to 2005. We also used Mecab to convert each word to its base form and to filter using POS tags to extract content words. As for dependency parsing for distance computation, we used Cabocha (Kudo and Matsumoto, 2000). We did not use a stop word list or any other external knowledge. Following the NTCIR-9 one click access task setting1 , we aimed at generating summaries of Japanese 500 characters or less. To evaluate the summaries, we followed the practices at the TAC summarization tasks (Dang, 2008) and NTCIR ACLIA tasks, and computed pyramid-based precision with an allowance parameter of C, recall, F β (where β is 1 or 3) scores. The value of C was determined based on the average nugget length for each question type of the ACLIA2 collection (Mitamura et al., 2010). Precision and recall are computed based"
P11-2039,W04-3230,0,0.0132303,"n document sets also occasionally contain documents that were eventually never used for nugget extraction (Mitamura et al., 2008; Mitamura et al., 2010). We preprocessed the Japanese documents basically by automatically detecting sentence boundaries based on Japanese punctuation marks, but we also used regular-expression-based heuristics to detect glossary of terms in articles. As the descriptions of these glossaries are usually very useful for answering BIOGRAPHY and DEFINITION questions, we treated each term description (generally multiple sentences) as a single sentence. 226 We used Mecab (Kudo et al., 2004) for morphological analysis, and calculated base word scores sb (w) using Mainichi articles from 1991 to 2005. We also used Mecab to convert each word to its base form and to filter using POS tags to extract content words. As for dependency parsing for distance computation, we used Cabocha (Kudo and Matsumoto, 2000). We did not use a stop word list or any other external knowledge. Following the NTCIR-9 one click access task setting1 , we aimed at generating summaries of Japanese 500 characters or less. To evaluate the summaries, we followed the practices at the TAC summarization tasks (Dang, 2"
P11-2039,N10-1134,0,0.246649,"ilm Critics Association Award for Best Animated Film Figure 1: Question and gold-standard nuggets example in NTCIR-8 ACLIA2 dataset We evaluate our proposed method using Japanese complex question answering test collections from NTCIR ACLIA–Advanced Cross-lingual Information Access task (Mitamura et al., 2008; Mitamura et al., 2010). However, our method can easily be extended for handling other languages. 2 Related Work Much work has been done for generic multidocument summarization (Takamura and Okumura, 2009a; Takamura and Okumura, 2009b; Celikyilmaz and Hakkani-Tur, 2010; Lin et al., 2010a; Lin and Bilmes, 2010). Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) criteria for non-redundant sentence selection, which consist of document similarity and redundancy penalty. McDonald (2007) presented an approximate dynamic programming approach to maximize the MMR criteria. Yih et al. (2007) formulated the document summarization problem as an MCKP, and proposed a supervised method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in documents: single words and events (named entity"
P11-2039,N10-1041,0,0.0290779,"アニメ 賞 Los Angeles Film Critics Association Award for Best Animated Film Figure 1: Question and gold-standard nuggets example in NTCIR-8 ACLIA2 dataset We evaluate our proposed method using Japanese complex question answering test collections from NTCIR ACLIA–Advanced Cross-lingual Information Access task (Mitamura et al., 2008; Mitamura et al., 2010). However, our method can easily be extended for handling other languages. 2 Related Work Much work has been done for generic multidocument summarization (Takamura and Okumura, 2009a; Takamura and Okumura, 2009b; Celikyilmaz and Hakkani-Tur, 2010; Lin et al., 2010a; Lin and Bilmes, 2010). Carbonell and Goldstein (1998) proposed the Maximal Marginal Relevance (MMR) criteria for non-redundant sentence selection, which consist of document similarity and redundancy penalty. McDonald (2007) presented an approximate dynamic programming approach to maximize the MMR criteria. Yih et al. (2007) formulated the document summarization problem as an MCKP, and proposed a supervised method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in documents: single words"
P11-2039,P09-2066,0,0.0210528,"cy in sentence selection, it would be difficult to cover both of these nuggets in the summary because of the word overlaps. 1 Introduction Automatic text summarization aims at reducing the amount of text the user has to read while preserving important contents, and has many applications in this age of digital information overload (Mani, 2001). In particular, query-oriented multi-document summarization is useful for helping the user satisfy his information need efficiently by gathering important pieces of information from multiple documents. In this study, we focus on extractive summarization (Liu and Liu, 2009), in particular, on sentence selection from a given set of source documents that contain relevant sentences. One well-known challenge in selecting sentences relevant to the information need is the vocabulary mismatch between the query (i.e. information need representation) and the candidate sentences. Hence, to enrich the information need representation, we build a co-occurrence 223 We therefore use word pairs as the basic unit for computing sentence scores, and then formulate the summarization problem as a Maximum Cover Problem with Knapsack Constraints (MCKP) (Filatova and Hatzivassiloglou,"
P11-2039,W01-0100,0,0.216136,"n. Each nugget represents a particular award that the movie received, and the two Japanese nugget strings have as many as three words in common: “批評 (review/critic)”, “アニメ (animation)” and “賞 (award).” Thus, if we use single words as the basis for penalising redundancy in sentence selection, it would be difficult to cover both of these nuggets in the summary because of the word overlaps. 1 Introduction Automatic text summarization aims at reducing the amount of text the user has to read while preserving important contents, and has many applications in this age of digital information overload (Mani, 2001). In particular, query-oriented multi-document summarization is useful for helping the user satisfy his information need efficiently by gathering important pieces of information from multiple documents. In this study, we focus on extractive summarization (Liu and Liu, 2009), in particular, on sentence selection from a given set of source documents that contain relevant sentences. One well-known challenge in selecting sentences relevant to the information need is the vocabulary mismatch between the query (i.e. information need representation) and the candidate sentences. Hence, to enrich the in"
P11-2039,H05-1115,0,0.0357397,"d method. Whereas, our method is unsupervised. Filatova and Hatzivassiloglou (2004) also formulated summarization as an MCKP, and they used two types of concepts in documents: single words and events (named entity pairs with a verb or a noun). While their work was for generic summarization, our method is designed specifically for query-oriented summarization. MMR-based methods are also popular for queryoriented summarization (Jagarlamudi et al., 2005; Li et al., 2008; Hasegawa et al., 2010; Lin et al., 2010b). Moreover, graph-based methods for summarization and sentence retrieval are popular (Otterbacher et al., 2005; Varadarajan and Hristidis, 2006; 224 Bosma, 2009). Unlike existing graph-based methods, our method explicitly computes indirect relationships between the query and words in the documents to enrich the information need representation. To this end, our method utilizes within-sentence cooccurrences of words. The approach taken by Jagarlamudi et al. (2005) is similar to our proposed method in that it uses word co-occurrence and dependencies within sentences in order to measure relevance of words to the query. However, while their approach measures the generic relevance of each word based on Hype"
P11-2039,E09-1089,1,0.905165,"articular, on sentence selection from a given set of source documents that contain relevant sentences. One well-known challenge in selecting sentences relevant to the information need is the vocabulary mismatch between the query (i.e. information need representation) and the candidate sentences. Hence, to enrich the information need representation, we build a co-occurrence 223 We therefore use word pairs as the basic unit for computing sentence scores, and then formulate the summarization problem as a Maximum Cover Problem with Knapsack Constraints (MCKP) (Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009a). This problem is an optimization problem that maximizes the total score of words covered by a summary under a summary length limit. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 223–229, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics • Question Sen to Chihiro no Kamikakushi (Spirited Away) is a full-length animated movie from Japan. The user wants to know how it was received overseas. • Nugget example 1 全米 映画 批評 会議 の アニメ 賞 National Board of Review of Motion Pictures Best Animated Feature • Nug"
P12-2068,J10-3005,0,0.0355848,"p), and SRL-based F1 (F1-SRL). In driven”. In addition, “say” is the main verb in this our experiment, we have three models. McDonald sentence and hard to be deleted due to the syntactic is a re-implementation of McDonald (2006). Clarke significance. and Lapata (2008) also re-implemented McDonald’s The second example in Table 4 requires to idenmodel with an ILP solver and experimented it on the tify a coreference relation between artificial lake and WNC Corpus. 9 MLN with SRL and MLN w/o Roadford Reservour. We consider that discourse SRL are our Markov Logic models with and with- constraints (Clarke and Lapata, 2010) help our model out SR Constraints, respectively. handle these cases. Discourse and coreference inforNote our three models have no constraint for the mation enable our model to select important argulength of compression. Therefore, we think the com- ments and their predicates. pression rate of the better system should get closer to 5 Conclusion that of human compression. In comparison between In this paper, we proposed new semantic conMLN models and McDonald, the former models outstraints for sentence compression. Our model with perform the latter model on both F1-Dep and F1global constraints"
P12-2068,C08-1018,0,0.168832,"tion. We expect that SR features also help our system to improve readability in sentence compression and summarization. 1 Introduction Recent work in document summarization do not only extract sentences but also compress sentences. Sentence compression enables summarizers to reduce the redundancy in sentences and generate informative summaries beyond the extractive summarization systems (Knight and Marcu, 2002). Conventional approaches to sentence compression exploit various linguistic properties based on lexical information and syntactic dependencies (McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2008; Galanis and Androutsopoulos, 2010). In contrast, our approach utilizes another property based on semantic roles (SRs) which improves weaknesses of syntactic dependencies. Syntactic dependencies are not sufficient to compress some complex sentences with coordination, with passive voice, and with an auxiliary verb. Figure 1 shows an example 2 Why are Semantic Roles Useful for Compressing Sentences? with a coordination structure. 1 1 Before describing our system, we show the statisThis example is from Written News Compression Cortics in terms of predicates, arguments and their relapus (http://j"
P12-2068,N10-1131,0,0.343879,"features also help our system to improve readability in sentence compression and summarization. 1 Introduction Recent work in document summarization do not only extract sentences but also compress sentences. Sentence compression enables summarizers to reduce the redundancy in sentences and generate informative summaries beyond the extractive summarization systems (Knight and Marcu, 2002). Conventional approaches to sentence compression exploit various linguistic properties based on lexical information and syntactic dependencies (McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2008; Galanis and Androutsopoulos, 2010). In contrast, our approach utilizes another property based on semantic roles (SRs) which improves weaknesses of syntactic dependencies. Syntactic dependencies are not sufficient to compress some complex sentences with coordination, with passive voice, and with an auxiliary verb. Figure 1 shows an example 2 Why are Semantic Roles Useful for Compressing Sentences? with a coordination structure. 1 1 Before describing our system, we show the statisThis example is from Written News Compression Cortics in terms of predicates, arguments and their relapus (http://jamesclarke.net/research/resources)."
P12-2068,W08-2123,0,0.0528298,"Missing"
P12-2068,C10-1081,0,0.021202,"dependency relation between Harari and became directly. SRs allow us to model the relations between a predicate and its arguments in a direct fashion. SR constraints are also advantageous in that we can compress sentences with semantic information. In Figure 1, became has three arguments, Harari as A1, businessman as A2, and shortly afterward as AM-TMP. As shown in this example, shortly afterword can be omitted (shaded boxes). In general, modifier arguments like AM-TMP or AM-LOC are more likely to be reduced than complement cases like A0-A4. We can implement such properties by SR constraints. Liu and Gildea (2010) suggests that SR features contribute to generating more readable sentence in machine translation. We expect that SR features also help our system to improve readability in sentence compression and summarization. 1 Introduction Recent work in document summarization do not only extract sentences but also compress sentences. Sentence compression enables summarizers to reduce the redundancy in sentences and generate informative summaries beyond the extractive summarization systems (Knight and Marcu, 2002). Conventional approaches to sentence compression exploit various linguistic properties based"
P12-2068,H05-1066,0,0.0795211,"Missing"
P12-2068,E06-1038,0,0.77525,"more readable sentence in machine translation. We expect that SR features also help our system to improve readability in sentence compression and summarization. 1 Introduction Recent work in document summarization do not only extract sentences but also compress sentences. Sentence compression enables summarizers to reduce the redundancy in sentences and generate informative summaries beyond the extractive summarization systems (Knight and Marcu, 2002). Conventional approaches to sentence compression exploit various linguistic properties based on lexical information and syntactic dependencies (McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2008; Galanis and Androutsopoulos, 2010). In contrast, our approach utilizes another property based on semantic roles (SRs) which improves weaknesses of syntactic dependencies. Syntactic dependencies are not sufficient to compress some complex sentences with coordination, with passive voice, and with an auxiliary verb. Figure 1 shows an example 2 Why are Semantic Roles Useful for Compressing Sentences? with a coordination structure. 1 1 Before describing our system, we show the statisThis example is from Written News Compression Cortics in terms of p"
P12-2068,N03-1026,0,0.0363577,"d) by role(i, j, +r). 4 Experiment and Result 4.1 Experimental Setup Our experimental setting follows previous work (Clarke and Lapata, 2008). As stated in Section 2, we employed the WNC Corpus. For preprocessing, we performed POS tagging by stanford-tagger. 5 and dependency parsing by MST-parser (McDonald et al., 2005). In addition, LTH 6 was exploited to perform both dependency parsing and SR labeling. We implemented our model by Markov Thebeast with Gurobi optimizer. 7 Our evaluation consists of two types of automatic evaluations. The first evaluation is dependency based evaluation same as Riezler et al. (2003). We performed dependency parsing on gold data and system outputs by RASP. 8 Then we calculated precision, recall, and F1 for the set of label(head, modi f ier). In order to demonstrate how well our SR constraints keep correct predicate-argument structures in compression, we propose SRL based evaluation. We performed SR labeling on gold data pos(i, +p1 ) ∧ pos(i + 1, +p2 ) ⇒ inComp(i). (9) POS features are often more reasonable than word 5 http://nlp.stanford.edu/software/tagger.shtml 6 form features to combine with the other properties. http://nlp.cs.lth.se/software/semantic_ parsing:_propban"
P13-1083,D09-1037,0,0.0182123,"ls: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceedings of the 51st Annual Meeting of the Association for Computationa"
P13-1083,P05-1067,0,0.0375111,"syntactic dependencies between pairs of POS tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they ar"
P13-1083,P07-1035,0,0.105138,"Eiichiro Sumita† , Hiroya Takamura‡ , Manabu Okumura‡ † National Institute of Information and Communications Technology {akihiro.tamura, taro.watanabe, eiichiro.sumita}@nict.go.jp † Precision and Intelligence Laboratory, Tokyo Institute of Technology {takamura, oku}@pi.titech.ac.jp Abstract [Example 1] 1 あなた は インターネット が 利用 でき ない Japanese POS: noun particle This paper proposes a nonparametric Bayesian method for inducing Part-ofSpeech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual infinite tree model (Finkel et al., 2007) to a bilingual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model. [Example 2] particle noun verb auxiliary verb You can not use the Internet . 私 Japane"
P13-1083,D09-1071,0,0.0564114,"Missing"
P13-1083,P06-1121,0,0.0231334,"guage into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceeding"
P13-1083,I11-1136,0,0.0659352,"Missing"
P13-1083,D11-1125,0,0.0332039,"Missing"
P13-1083,W06-3601,0,0.0610141,"Missing"
P13-1083,P10-1145,0,0.0147666,"d method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the othe"
P13-1083,N03-1017,0,0.00724269,"e included in the Japanese sentences and the English sentences, respectively. The Japanese POS tags come from the secondlevel POS tags in the IPA POS tagset (Asahara and Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments using the “grow-diag-finaland” heuristic (Koehn et al., 2003). Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements. The Japanese sentences are parsed using CaboCha (Kudo and Matsumoto, 2002), which generates dependency structures using a phrasal unit called a bunsetsu8 , rather than a word unit as in English or Chinese dependency parsing. Since we focus on the word-level POS induction, each bunsetsu-based dependency tree is converted into its corresponding word-based dependency tree using the following heuristic9 : first, the last function word inside each bunsetsu is identified as the head word10 ; then, th"
P13-1083,C12-1120,0,0.0368962,"Missing"
P13-1083,J03-1002,0,0.0064088,"lish sentences were tokenized and POS tagged using TreeTagger (Schmid, 1994), where 43 and 58 types of POS tags are included in the Japanese sentences and the English sentences, respectively. The Japanese POS tags come from the secondlevel POS tags in the IPA POS tagset (Asahara and Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments using the “grow-diag-finaland” heuristic (Koehn et al., 2003). Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements. The Japanese sentences are parsed using CaboCha (Kudo and Matsumoto, 2002), which generates dependency structures using a phrasal unit called a bunsetsu8 , rather than a word unit as in English or Chinese dependency parsing. Since we focus on the word-level POS induction, each bunsetsu-based dependency tree is converted into its corresponding word-based dependency tree using the fo"
P13-1083,P03-1021,0,0.159428,"Missing"
P13-1083,W04-3250,0,0.192157,"Missing"
P13-1083,P02-1040,0,0.0860707,"Missing"
P13-1083,W02-2016,0,0.0772343,"Matsumoto, 2003) and the English POS tags are derived from the Penn Treebank. Note that the Japanese POS tags are used for initialization of hidden states and the English POS tags are used as observations emitted by hidden states. Word-by-word alignments for the sentence pairs are produced by first running GIZA++ (Och and Ney, 2003) in both directions and then combining the alignments using the “grow-diag-finaland” heuristic (Koehn et al., 2003). Note that we ran GIZA++ on all of the NTCIR-9 training data in order to obtain better alignements. The Japanese sentences are parsed using CaboCha (Kudo and Matsumoto, 2002), which generates dependency structures using a phrasal unit called a bunsetsu8 , rather than a word unit as in English or Chinese dependency parsing. Since we focus on the word-level POS induction, each bunsetsu-based dependency tree is converted into its corresponding word-based dependency tree using the following heuristic9 : first, the last function word inside each bunsetsu is identified as the head word10 ; then, the remaining words are treated as dependents of the head word in the same bunsetsu; finally, a bunsetsu-based dependency structure is transformed to a word-based dependency str"
P13-1083,P05-1034,0,0.0379419,"between pairs of POS tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed withou"
P13-1083,D07-1072,0,0.16601,"nd parameters (e.g., ϕ′k and ϕ′′k ) for each information. Specifically, x′t and ϕ′k are introduced for the surface form of aligned words, and x′′t and ϕ′′k for the POS of aligned words. Consider, for example, Example 1 in Figure 1. The POS tag of “利用” generates the string “利用+use+verb” as the observation in the joint model, while it generates “利用”, “use”, and “verb” independently in the independent model. 3.4 POS Refinement We have assumed a completely unsupervised way of inducing POS tags in dependency trees. Another realistic scenario is to refine the existing POS tags (Finkel et al., 2007; Liang et al., 2007) so that each refined sub-POS tag may reflect the information from the aligned words while preserving the handcrafted distinction from original POS tagset. Major difference is that we introduce separate transition probabilities πks and observation ′ distributions (ϕsk , ϕks ) for each existing POS tag s. Then, each node t is constrained to follow the distributions indicated by the initially assigned POS tag st , and we use the pair (st , zt ) as a state representation. β|γ ∼ GEM(γ), πk |α0 , β ∼ DP(α0 , β), ϕk ∼ H, ϕ′k ∼ H ′ , zt′ |zt ∼ Multinomial(πzt ), xt |zt ∼ F (ϕzt ), x′t |zt ∼ F ′ (ϕ′zt"
P13-1083,W11-2119,0,0.0290777,"Missing"
P13-1083,C04-1090,0,0.0333421,"s represent syntactic dependencies between pairs of POS tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal"
P13-1083,P06-1077,0,0.0845012,"s in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “"
P13-1083,P08-1066,0,0.025975,"S tags. The proposed method builds on this model by incorporating the aligned words in the other language into the observations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the l"
P13-1083,P09-1063,0,0.0382929,"Missing"
P13-1083,N12-1045,0,0.0450049,"Missing"
P13-1083,D08-1022,0,0.0195747,"ations. We investigate two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceedings of the 51st Annual"
P13-1083,P11-1125,1,0.791018,"Missing"
P13-1083,N12-1026,1,0.875093,"Missing"
P13-1083,P08-1064,0,0.0187394,"te two types of models: (i) a joint model and (ii) an independent model. In the joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceedings of the 51st Annual Meeting of the Asso"
P13-1083,P11-1084,0,0.0126965,"joint model, each hidden state jointly emits both a source word and its aligned target word as an observation. The independent model separately emits words in two languages from hidden states. By inferring POS Introduction In recent years, syntax-based SMT has made promising progress by employing either dependency parsing (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008; Mi and Liu, 2010) or constituency parsing (Huang et al., 2006; Liu et al., 2006; Galley et al., 2006; Mi and Huang, 2008; Zhang et al., 2008; Cohn and Blunsom, 2009; Liu et al., 2009; Mi and Liu, 2010; Zhang et al., 2011) on the source side, the target side, or both. However, dependency parsing, which is a popular choice for Japanese, can incorporate only shallow syntactic information, i.e., POS tags, compared with the richer syntactic phrasal categories in constituency parsing. Moreover, existing POS tagsets might not be optimal for SMT because they are constructed without considering the language in the other side. Consider the examples in Figure 1. The Japanese noun “利用” in 841 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 841–851, c Sofia, Bulgaria, August 4"
P13-1083,P11-1087,0,\N,Missing
P13-1083,P07-2045,0,\N,Missing
P13-1101,P11-1049,0,0.399332,"document cluster. We also encode obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary. In order to handle the subtree extraction problem, we investigate a new class of submodular maximization problem, and a new algorithm that has the approximation ratio 12 (1 − e−1 ). Our experiments with the NTCIR ACLIA test collections show that our approach outperforms a state-of-the-art algorithm. 1 Introduction Text summarization is often addressed as a task of simultaneously performing sentence extraction and sentence compression (Berg-Kirkpatrick et al., 2011; Martins and Smith, 2009). Joint models of sentence extraction and compression have a great benefit in that they have a large degree of freedom as far as controlling redundancy goes. In contrast, conventional two-stage approaches (Zajic et al., 2006), which first generate candidate compressed sentences and then use them to generate a summary, have less computational complexity than joint models. However, two-stage approaches are suboptimal for text summarization. For example, when we compress sentences first, the compressed sentences may fail to contain important pieces of information due to"
P13-1101,N06-1023,0,0.0195308,"idf over Mainichi newspaper articles from 1991 to 2005. For the de1028 Lin and Bilmes (2011) Subtree extraction (SbE) Sentence extraction (NC) POURPRE 0.215 0.268 0.278 Precision 0.126 0.238 0.206 Recall 0.201 0.213 0.215 F1 0.135 0.159 0.139 F3 0.174 0.190 0.183 Table 1: Results on ACLIA2 test data. pendency parsing, we used KNP (Kurohashi and Kawahara, 2009b). Since KNP internally has a flag that indicates either an “obligatory case” or an “adjacent case”, we regarded dependency relations flagged by KNP as obligatory in the sentence compression. KNP utilizes Kyoto University’s case frames (Kawahara and Kurohashi, 2006) as the resource for detecting obligatory or adjacent cases. To evaluate the summaries, we followed the practices of the TAC summarization tasks (Dang, 2008) and NTCIR ACLIA tasks, and computed pyramid-based precision with the allowance parameter, recall, and Fβ (where β is 1 or 3) scores. The allowance parameter was determined from the average nugget length for each question type of the ACLIA2 collection (Mitamura et al., 2010). Precision and recall are computed from the nuggets that the summary covered along with their weights. One of the authors of this paper manually evaluated whether each"
P13-1101,N10-1134,0,0.345605,"ple, when we compress sentences first, the compressed sentences may fail to contain important pieces of information due to the length limit imposed on each sentence. On the other hand, when we extract sentences first, an important sentence may fail to be selected, simply because it is long. Enumerating a huge number of compressed sentences is also infeasible. Joint models can prune unimportant or redundant descriptions without resorting to enumeration. Meanwhile, submodular maximization has recently been applied to the text summarization task, and the methods thereof have performed very well (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2011). Formalizing summarization as a submodular maximization problem has an important benefit inthat the problem can be solved by using a greedy algorithm with a performance guarantee. We therefore decided to formalize the task of simultaneously performing sentence extraction and compression as a submodular maximization problem. That is, we extract subsentences for making the summary directly from all available subsentences in the documents and not in a stepwise fashion. However, there is a difficulty with such a formalization. In the past, the resulting"
P13-1101,P11-1052,0,0.206779,"sentences first, the compressed sentences may fail to contain important pieces of information due to the length limit imposed on each sentence. On the other hand, when we extract sentences first, an important sentence may fail to be selected, simply because it is long. Enumerating a huge number of compressed sentences is also infeasible. Joint models can prune unimportant or redundant descriptions without resorting to enumeration. Meanwhile, submodular maximization has recently been applied to the text summarization task, and the methods thereof have performed very well (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2011). Formalizing summarization as a submodular maximization problem has an important benefit inthat the problem can be solved by using a greedy algorithm with a performance guarantee. We therefore decided to formalize the task of simultaneously performing sentence extraction and compression as a submodular maximization problem. That is, we extract subsentences for making the summary directly from all available subsentences in the documents and not in a stepwise fashion. However, there is a difficulty with such a formalization. In the past, the resulting maximization problem"
P13-1101,P11-2039,1,0.915958,"ompressed sentences may fail to contain important pieces of information due to the length limit imposed on each sentence. On the other hand, when we extract sentences first, an important sentence may fail to be selected, simply because it is long. Enumerating a huge number of compressed sentences is also infeasible. Joint models can prune unimportant or redundant descriptions without resorting to enumeration. Meanwhile, submodular maximization has recently been applied to the text summarization task, and the methods thereof have performed very well (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2011). Formalizing summarization as a submodular maximization problem has an important benefit inthat the problem can be solved by using a greedy algorithm with a performance guarantee. We therefore decided to formalize the task of simultaneously performing sentence extraction and compression as a submodular maximization problem. That is, we extract subsentences for making the summary directly from all available subsentences in the documents and not in a stepwise fashion. However, there is a difficulty with such a formalization. In the past, the resulting maximization problem has been often accompa"
P13-1101,W09-1801,0,0.0282021,"e obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary. In order to handle the subtree extraction problem, we investigate a new class of submodular maximization problem, and a new algorithm that has the approximation ratio 12 (1 − e−1 ). Our experiments with the NTCIR ACLIA test collections show that our approach outperforms a state-of-the-art algorithm. 1 Introduction Text summarization is often addressed as a task of simultaneously performing sentence extraction and sentence compression (Berg-Kirkpatrick et al., 2011; Martins and Smith, 2009). Joint models of sentence extraction and compression have a great benefit in that they have a large degree of freedom as far as controlling redundancy goes. In contrast, conventional two-stage approaches (Zajic et al., 2006), which first generate candidate compressed sentences and then use them to generate a summary, have less computational complexity than joint models. However, two-stage approaches are suboptimal for text summarization. For example, when we compress sentences first, the compressed sentences may fail to contain important pieces of information due to the length limit imposed o"
P14-2052,P13-1020,0,0.132407,"Missing"
P14-2052,P13-1101,1,0.60525,"Missing"
P14-2052,P11-1049,0,0.200251,"Missing"
P14-2052,C10-2105,0,0.017709,"n based on RST use EDUs as extraction textual units. We converted the rhetorical relations 12 between EDUs to the relations between sentences to build the nested tree structure. We could 10 thus take into account both relations between sentences and relations between words. 8 Figure 2: Example of one sentence. Each line corresponds to one EDU. Number of selected sentences from source document tion, their method required large sets of data to calculate the accurate probability. There have been some studies that have used discourse structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations Extracting a subtree from the dependency tree of to build an RST-Discourse Tree (RST-DT) that has 4 words is one approach to sentence compression a hierarchical structure of the relations. There are (Tomita et al., 2009; Qian and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese st"
P14-2052,W01-1605,0,0.409306,"Missing"
P14-2052,D13-1156,0,0.0298168,"ta to calculate the accurate probability. There have been some studies that have used discourse structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations Extracting a subtree from the dependency tree of to build an RST-Discourse Tree (RST-DT) that has 4 words is one approach to sentence compression a hierarchical structure of the relations. There are (Tomita et al., 2009; Qian and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese studies have only extracted rooted subtrees cleus and a satellite. The nucleus is more salient 0 from sentences. We allowed our model to extract to the discourse structure, while the other span, the EDU選択 文選択 information. 参照要約 a subtree that did not include the root word (See提案手法 satellite, represents supporting RSTthe sentence with an asterisk ∗ in Figure 1). The DT is a tree whose terminal nodes correspond method of Filippova and St"
P14-2052,N13-1136,0,0.00537175,"as extraction textual units. We converted the rhetorical relations 12 between EDUs to the relations between sentences to build the nested tree structure. We could 10 thus take into account both relations between sentences and relations between words. 8 Figure 2: Example of one sentence. Each line corresponds to one EDU. Number of selected sentences from source document tion, their method required large sets of data to calculate the accurate probability. There have been some studies that have used discourse structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations Extracting a subtree from the dependency tree of to build an RST-Discourse Tree (RST-DT) that has 4 words is one approach to sentence compression a hierarchical structure of the relations. There are (Tomita et al., 2009; Qian and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese studies have only extracted r"
P14-2052,P02-1057,0,0.0824756,"Missing"
P14-2052,P09-1075,0,0.015587,"Missing"
P14-2052,C04-1057,0,0.0689065,"rtant content in the source document. The results from an empirical evaluation revealed that our method based on the trimming of the nested tree significantly improved the summarization of texts. 1 Introduction Extractive summarization is one well-known approach to text summarization and extractive methods represent a document (or a set of documents) as a set of some textual units (e.g., sentences, clauses, and words) and select their subset as a summary. Formulating extractive summarization as a combinational optimization problem greatly improves the quality of summarization (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009). There has recently been increasing attention focused on approaches that jointly optimize sentence extraction and sentence compression (Tomita et al., 2009; 315 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 315–320, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Source document John was running on a track in the park. He looks very tired. Mike said he is training for a race. The race is held next month. John was running on a track in the park. ＊ He looks ve"
P14-2052,W08-1105,0,0.109308,"an and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese studies have only extracted rooted subtrees cleus and a satellite. The nucleus is more salient 0 from sentences. We allowed our model to extract to the discourse structure, while the other span, the EDU選択 文選択 information. 参照要約 a subtree that did not include the root word (See提案手法 satellite, represents supporting RSTthe sentence with an asterisk ∗ in Figure 1). The DT is a tree whose terminal nodes correspond method of Filippova and Strube (2008) allows the to EDUs and whose nonterminal nodes indicate model to extract non-rooted subtrees in sentence the relations. Hirao et al. converted RST-DTs compression tasks that compress a single sentence into dependency-based discourse trees (DEP-DTs) with a given compression ratio. However, it is not whose nodes corresponded to EDUs and whose trivial to apply their method to text summarizaedges corresponded to the head modifier relationtion because no compression ratio is given to senships of EDUs. See Hirao et al. for details (Hirao tences. None of these methods use the discourse et al., 2013)"
P14-2052,W09-1802,0,0.0293277,"structures locally to optimize the order of selected sentences (Nishikawa et al., 2010; Christensen et al., 2013). 3 Generating summary from nested tree 3.1 Building Nested Tree with RST A document in RST is segmented into EDUs and adjacent EDUs are linked with rhetorical relations Extracting a subtree from the dependency tree of to build an RST-Discourse Tree (RST-DT) that has 4 words is one approach to sentence compression a hierarchical structure of the relations. There are (Tomita et al., 2009; Qian and Liu, 2013; Morita 78 types of rhetorical relations between two spans, 2 et al., 2013; Gillick and Favre, 2009). However, and each span has one of two aspects of a nuthese studies have only extracted rooted subtrees cleus and a satellite. The nucleus is more salient 0 from sentences. We allowed our model to extract to the discourse structure, while the other span, the EDU選択 文選択 information. 参照要約 a subtree that did not include the root word (See提案手法 satellite, represents supporting RSTthe sentence with an asterisk ∗ in Figure 1). The DT is a tree whose terminal nodes correspond method of Filippova and Strube (2008) allows the to EDUs and whose nonterminal nodes indicate model to extract non-rooted subtr"
P14-2052,D13-1158,1,0.609684,"Missing"
P14-2052,W04-1013,0,0.03652,"Missing"
P14-2052,W98-1124,0,0.257936,"ct Qian and Liu, 2013; Morita et al., 2013; Gillick and Favre, 2009; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011). We can only extract important content by trimming redundant parts from sentences. However, as these methods did not include the discourse structures of documents, the generated summaries lacked coherence. It is important for generated summaries to have a discourse structure that is similar to that of the source document. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one way of introducing the discourse structure of a document to a summarization task (Marcu, 1998; Daum´e III and Marcu, 2002; Hirao et al., 2013). Hirao et al. recently transformed RST trees into dependency trees and used them for single document summarization (Hirao et al., 2013). They formulated the summarization problem as a tree knapsack problem with constraints represented by the dependency trees. We propose a method of summarizing a single document that utilizes dependency between sentences obtained from rhetorical structures and dependency between words obtained from a dependency parser. We have explained our method with an example in Figure 1. First, we represent a document as a"
P16-1211,C02-1122,0,0.0558889,"Missing"
P16-1211,C12-1117,0,0.0254074,"Missing"
P16-1211,C14-1103,0,0.0254769,"Missing"
P17-2044,D15-1042,0,0.324961,"by deleting words as opposed to abstractive compression lies in the small search space. Another one is that the compressed sentence is more likely to be free from incorrect information not mentioned in the source sentence. There are many sentence compression models for Japanese (Harashima and Kurohashi, 2012; Hirao et al., 2009; Hori and Furui, 2004) and for English (Knight and Marcu, 2000; Turner and Charniak, 2005; Clarke and Lapata, 2006). In recent years, a high-quality English sentence compression model by deleting words was trained on a large training dataset (Filippova and Altun, 2013; Filippova et al., 2015). While it is impractical to create a large 2 Creating training dataset for Japanese Filippova et al.’s method of creating training data consists of the conditions imposed on news articles and the following three steps: (1) identification of shared content words, (2) transformation of a dependency tree, and (3) extraction of the min281 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 281–286 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2044 Figure 1: Dep"
P17-2044,D13-1155,0,0.0721707,"e advantage of compression by deleting words as opposed to abstractive compression lies in the small search space. Another one is that the compressed sentence is more likely to be free from incorrect information not mentioned in the source sentence. There are many sentence compression models for Japanese (Harashima and Kurohashi, 2012; Hirao et al., 2009; Hori and Furui, 2004) and for English (Knight and Marcu, 2000; Turner and Charniak, 2005; Clarke and Lapata, 2006). In recent years, a high-quality English sentence compression model by deleting words was trained on a large training dataset (Filippova and Altun, 2013; Filippova et al., 2015). While it is impractical to create a large 2 Creating training dataset for Japanese Filippova et al.’s method of creating training data consists of the conditions imposed on news articles and the following three steps: (1) identification of shared content words, (2) transformation of a dependency tree, and (3) extraction of the min281 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 281–286 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/"
P17-2044,W08-1105,0,0.0582185,"Missing"
P17-2044,C12-1067,0,0.31763,"ant information and grammaticality. Robust sentence compression systems are useful by themselves and also as a module in an extractive summarization system (Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013). In this paper, we work on Japanese sentence compression by deleting words. One advantage of compression by deleting words as opposed to abstractive compression lies in the small search space. Another one is that the compressed sentence is more likely to be free from incorrect information not mentioned in the source sentence. There are many sentence compression models for Japanese (Harashima and Kurohashi, 2012; Hirao et al., 2009; Hori and Furui, 2004) and for English (Knight and Marcu, 2000; Turner and Charniak, 2005; Clarke and Lapata, 2006). In recent years, a high-quality English sentence compression model by deleting words was trained on a large training dataset (Filippova and Altun, 2013; Filippova et al., 2015). While it is impractical to create a large 2 Creating training dataset for Japanese Filippova et al.’s method of creating training data consists of the conditions imposed on news articles and the following three steps: (1) identification of shared content words, (2) transformation of"
P17-2044,P09-1093,0,0.503233,"ity. Robust sentence compression systems are useful by themselves and also as a module in an extractive summarization system (Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013). In this paper, we work on Japanese sentence compression by deleting words. One advantage of compression by deleting words as opposed to abstractive compression lies in the small search space. Another one is that the compressed sentence is more likely to be free from incorrect information not mentioned in the source sentence. There are many sentence compression models for Japanese (Harashima and Kurohashi, 2012; Hirao et al., 2009; Hori and Furui, 2004) and for English (Knight and Marcu, 2000; Turner and Charniak, 2005; Clarke and Lapata, 2006). In recent years, a high-quality English sentence compression model by deleting words was trained on a large training dataset (Filippova and Altun, 2013; Filippova et al., 2015). While it is impractical to create a large 2 Creating training dataset for Japanese Filippova et al.’s method of creating training data consists of the conditions imposed on news articles and the following three steps: (1) identification of shared content words, (2) transformation of a dependency tree, a"
P17-2044,D16-1140,1,0.93556,"ased on the following three characteristics of the Japanese language: (a) Abbreviation of nouns and nominalization of verbs frequently occur in Japanese. (b) Words that are not verbs can also be the root node especially in headlines. (c) Subjects and objects that can be easily estimated from the context are often omitted. The created training dataset is used to train three models. The first model is the original Filippova et al.’s model, an encoder-decoder model with a long short-term memory (LSTM), which we extend in this paper to make the other two models that can control the output length (Kikuchi et al., 2016), because controlling the output length makes a compressed sentence more informative under the desired length. In English, high-quality sentence compression models by deleting words have been trained on automatically created large training datasets. We work on Japanese sentence compression by a similar approach. To create a large Japanese training dataset, a method of creating English training dataset is modified based on the characteristics of the Japanese language. The created dataset is used to train Japanese sentence compression models based on the recurrent neural network. 1 Introduction"
P17-2044,W13-3508,0,0.0163057,"tence compression by a similar approach. To create a large Japanese training dataset, a method of creating English training dataset is modified based on the characteristics of the Japanese language. The created dataset is used to train Japanese sentence compression models based on the recurrent neural network. 1 Introduction Sentence compression is the task of shortening a sentence while preserving its important information and grammaticality. Robust sentence compression systems are useful by themselves and also as a module in an extractive summarization system (Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013). In this paper, we work on Japanese sentence compression by deleting words. One advantage of compression by deleting words as opposed to abstractive compression lies in the small search space. Another one is that the compressed sentence is more likely to be free from incorrect information not mentioned in the source sentence. There are many sentence compression models for Japanese (Harashima and Kurohashi, 2012; Hirao et al., 2009; Hori and Furui, 2004) and for English (Knight and Marcu, 2000; Turner and Charniak, 2005; Clarke and Lapata, 2006). In recent years, a high-quality English sentenc"
P17-2044,P05-1036,0,0.230908,"ule in an extractive summarization system (Berg-Kirkpatrick et al., 2011; Thadani and McKeown, 2013). In this paper, we work on Japanese sentence compression by deleting words. One advantage of compression by deleting words as opposed to abstractive compression lies in the small search space. Another one is that the compressed sentence is more likely to be free from incorrect information not mentioned in the source sentence. There are many sentence compression models for Japanese (Harashima and Kurohashi, 2012; Hirao et al., 2009; Hori and Furui, 2004) and for English (Knight and Marcu, 2000; Turner and Charniak, 2005; Clarke and Lapata, 2006). In recent years, a high-quality English sentence compression model by deleting words was trained on a large training dataset (Filippova and Altun, 2013; Filippova et al., 2015). While it is impractical to create a large 2 Creating training dataset for Japanese Filippova et al.’s method of creating training data consists of the conditions imposed on news articles and the following three steps: (1) identification of shared content words, (2) transformation of a dependency tree, and (3) extraction of the min281 Proceedings of the 55th Annual Meeting of the Association"
P17-2044,P06-1048,0,\N,Missing
P17-2044,P11-1049,0,\N,Missing
P19-1099,P00-1041,0,0.355572,"Missing"
P19-1099,P18-1063,0,0.0259154,"018) and extraction of actual fact descriptions from a source text (Cao et al., 2018). Although summary length control of abstractive summarization has been studied, previous studies focus on incorporation of a length controlling method to neural abstractive summarization models (Kikuchi et al., 2016; Fan et al., 2018; Liu et al., 2018; Fevry and Phang, 2018; Schumann, 2018). In contrast, our research focuses on a global optimization method. Optimization methods for optimizing a model with respect to evaluation scores, such as reinforcement learning (Ranzato et al., 2015; Paulus et al., 2018; Chen and Bansal, 2018; Wu and Hu, 2018) and minimum risk training (Ayana et al., 2017), have been proposed for summarization models based on neural encoder-decoders. Our method is similar to that of Ayana et al. (2017) in terms of applying MRT to neural encoder-decoders. There are two differences between our method and Ayana et al.’s: (i) our method uses only the part of the summary generated by a model within the length constraint for calculating the ROUGE score and (ii) it penalizes summaries that exceed the length of the reference regardless of its ROUGE score. 3 Summary Length Controllable Models In this secti"
P19-1099,N16-1012,0,0.174657,"Missing"
P19-1099,W03-0501,0,0.26132,"Missing"
P19-1099,W18-2706,0,0.0799593,", 2016; See et al., 2017) and copy mechanisms (Gu et al., 2016; Zeng et al., 2016) have been proposed for overcoming the unknown word problem. Other methods for the improvement of abstractive summarization models include use of existing summaries as soft templates with a source text (Li et al., 2018) and extraction of actual fact descriptions from a source text (Cao et al., 2018). Although summary length control of abstractive summarization has been studied, previous studies focus on incorporation of a length controlling method to neural abstractive summarization models (Kikuchi et al., 2016; Fan et al., 2018; Liu et al., 2018; Fevry and Phang, 2018; Schumann, 2018). In contrast, our research focuses on a global optimization method. Optimization methods for optimizing a model with respect to evaluation scores, such as reinforcement learning (Ranzato et al., 2015; Paulus et al., 2018; Chen and Bansal, 2018; Wu and Hu, 2018) and minimum risk training (Ayana et al., 2017), have been proposed for summarization models based on neural encoder-decoders. Our method is similar to that of Ayana et al. (2017) in terms of applying MRT to neural encoder-decoders. There are two differences between our method an"
P19-1099,K18-1040,0,0.0630047,"echanisms (Gu et al., 2016; Zeng et al., 2016) have been proposed for overcoming the unknown word problem. Other methods for the improvement of abstractive summarization models include use of existing summaries as soft templates with a source text (Li et al., 2018) and extraction of actual fact descriptions from a source text (Cao et al., 2018). Although summary length control of abstractive summarization has been studied, previous studies focus on incorporation of a length controlling method to neural abstractive summarization models (Kikuchi et al., 2016; Fan et al., 2018; Liu et al., 2018; Fevry and Phang, 2018; Schumann, 2018). In contrast, our research focuses on a global optimization method. Optimization methods for optimizing a model with respect to evaluation scores, such as reinforcement learning (Ranzato et al., 2015; Paulus et al., 2018; Chen and Bansal, 2018; Wu and Hu, 2018) and minimum risk training (Ayana et al., 2017), have been proposed for summarization models based on neural encoder-decoders. Our method is similar to that of Ayana et al. (2017) in terms of applying MRT to neural encoder-decoders. There are two differences between our method and Ayana et al.’s: (i) our method uses onl"
P19-1099,P03-1021,0,0.0141561,"y length control. One is an LSTM based summarization model, and the other is a CNN based one. They proposed to enforce the desired length in the decoding of training and generation. Their models, however, leave much room for improvement, at least with regard to two aspects. One aspect is that the summarization performance is still worse than other state-of-the-art models. The other is that their models sometimes fail to control the output length. In this paper, we address these two issues by incorporating global training based on a minimum risk training (MRT) under the length constraint. MRT (Och, 2003) is used to optimize a model globally for an arbitrary evaluation metric. It was also applied for optimizing the neural summarization model for headline generation with respect to ROUGE (Ayana et al., 2017), which is based on an overlap of words with reference summaries (Lin, 2004). However, how to use MRT under a length constraint was an open problem; thus we propose a global optimization under length constraint (GOLC) for neural summarization models. We show that neural summarization models trained with GOLC can control the output length better than the existing methods. This is because our"
P19-1099,D15-1044,0,0.179565,"Missing"
P19-1099,D13-1155,0,0.0675463,"Missing"
P19-1099,P17-1099,0,0.603108,"ved post-editing time. 2 Related Work There are many models for text summarization such as rule-based models (Dorr et al., 2003) and statistical models (Banko et al., 2000; Zajic et al., 2004; Filippova and Strube, 2008; Woodsend et al., 2010; Filippova and Altun, 2013). Recently, abstractive summarization models based on neural encoder-decoders have been proposed (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Paulus et al., 2018). There are mainly two research directions: model architectures and optimization methods. Pointer networks (Vinyals and Le, 2015; Gulcehre et al., 2016; See et al., 2017) and copy mechanisms (Gu et al., 2016; Zeng et al., 2016) have been proposed for overcoming the unknown word problem. Other methods for the improvement of abstractive summarization models include use of existing summaries as soft templates with a source text (Li et al., 2018) and extraction of actual fact descriptions from a source text (Cao et al., 2018). Although summary length control of abstractive summarization has been studied, previous studies focus on incorporation of a length controlling method to neural abstractive summarization models (Kikuchi et al., 2016; Fan et al., 2018; Liu et"
P19-1099,W08-1105,0,0.0854886,"Missing"
P19-1099,P16-1159,0,0.026186,"used as inputs of a decoder. Thus, the way of decreasing the probability of generating overlength summaries is not trivial. 4.2 Minimum Risk Training In MRT, unlike MLE, the probability of a word at each step is calculated using previously generated words as in the test phase. MRT aims at optimizing a model for an evaluation metric by minimizLM RT (θ) = ∑ ∑ Qθ (y′ |x)∆(y, y′ ), ˜ (x,y)∈D y′ ∈S(x) (2) where Qθ (y′ |x) ∝ pθ (y′ |x)γ . ∆(y, y′ ) is a loss function of the negative ROUGE between a reference summary y and a summary to be evaluated ˜ y′ , γ is a smoothing factor and S(x) = S(x)∪{y} (Shen et al., 2016). S(x) is a set of summaries that can be generated by a model for a given x. Including reference summaries into the set of sampled summaries can increase the probabilities of generating reference summaries, which will be analyzed in Section 6. From Equation (2), we see that the probability of generating a summary is weighted by its ROUGE score. Since MRT optimizes summarization models in terms of a ROUGE score, the length of summaries generated by models depends on the type of a ROUGE score, i.e., summary lengths will be long if we choose ROUGE recall as ∆, while summary lengths will be short"
P19-1099,P16-1154,0,0.016153,"here are many models for text summarization such as rule-based models (Dorr et al., 2003) and statistical models (Banko et al., 2000; Zajic et al., 2004; Filippova and Strube, 2008; Woodsend et al., 2010; Filippova and Altun, 2013). Recently, abstractive summarization models based on neural encoder-decoders have been proposed (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Paulus et al., 2018). There are mainly two research directions: model architectures and optimization methods. Pointer networks (Vinyals and Le, 2015; Gulcehre et al., 2016; See et al., 2017) and copy mechanisms (Gu et al., 2016; Zeng et al., 2016) have been proposed for overcoming the unknown word problem. Other methods for the improvement of abstractive summarization models include use of existing summaries as soft templates with a source text (Li et al., 2018) and extraction of actual fact descriptions from a source text (Cao et al., 2018). Although summary length control of abstractive summarization has been studied, previous studies focus on incorporation of a length controlling method to neural abstractive summarization models (Kikuchi et al., 2016; Fan et al., 2018; Liu et al., 2018; Fevry and Phang, 2018; Sch"
P19-1099,P16-1014,0,0.0452981,"Missing"
P19-1099,D10-1050,0,0.0723689,"Missing"
P19-1099,D16-1140,1,0.890502,"ore, editors have to summarize a source text under a length constraint by reordering and paraphrasing. For summarization, both extractive and abstractive methods have been widely studied. Extractive methods are based on selection of sentences from source texts without using reordering or paraphrasing. In contrast, abstractive methods generate summaries as new sentences. Therefore, abstractive methods can rely on the reordering and paraphrasing required for summary and title generation. However, most abstractive summarization methods are not able to control the summary length. To this problem, Kikuchi et al. (2016) and Liu et al. (2018) proposed abstractive summarization models with a capability of summary length control. One is an LSTM based summarization model, and the other is a CNN based one. They proposed to enforce the desired length in the decoding of training and generation. Their models, however, leave much room for improvement, at least with regard to two aspects. One aspect is that the summarization performance is still worse than other state-of-the-art models. The other is that their models sometimes fail to control the output length. In this paper, we address these two issues by incorporati"
P19-1099,P18-1015,0,0.100347,"tly, abstractive summarization models based on neural encoder-decoders have been proposed (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Paulus et al., 2018). There are mainly two research directions: model architectures and optimization methods. Pointer networks (Vinyals and Le, 2015; Gulcehre et al., 2016; See et al., 2017) and copy mechanisms (Gu et al., 2016; Zeng et al., 2016) have been proposed for overcoming the unknown word problem. Other methods for the improvement of abstractive summarization models include use of existing summaries as soft templates with a source text (Li et al., 2018) and extraction of actual fact descriptions from a source text (Cao et al., 2018). Although summary length control of abstractive summarization has been studied, previous studies focus on incorporation of a length controlling method to neural abstractive summarization models (Kikuchi et al., 2016; Fan et al., 2018; Liu et al., 2018; Fevry and Phang, 2018; Schumann, 2018). In contrast, our research focuses on a global optimization method. Optimization methods for optimizing a model with respect to evaluation scores, such as reinforcement learning (Ranzato et al., 2015; Paulus et al., 2018; Chen"
P19-1099,W04-1013,0,0.143599,"ect is that the summarization performance is still worse than other state-of-the-art models. The other is that their models sometimes fail to control the output length. In this paper, we address these two issues by incorporating global training based on a minimum risk training (MRT) under the length constraint. MRT (Och, 2003) is used to optimize a model globally for an arbitrary evaluation metric. It was also applied for optimizing the neural summarization model for headline generation with respect to ROUGE (Ayana et al., 2017), which is based on an overlap of words with reference summaries (Lin, 2004). However, how to use MRT under a length constraint was an open problem; thus we propose a global optimization under length constraint (GOLC) for neural summarization models. We show that neural summarization models trained with GOLC can control the output length better than the existing methods. This is because our training procedure makes use of overlength summaries. While the probabilities of generating sum1039 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1039–1048 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computation"
P19-1099,D18-1444,0,0.655386,"rize a source text under a length constraint by reordering and paraphrasing. For summarization, both extractive and abstractive methods have been widely studied. Extractive methods are based on selection of sentences from source texts without using reordering or paraphrasing. In contrast, abstractive methods generate summaries as new sentences. Therefore, abstractive methods can rely on the reordering and paraphrasing required for summary and title generation. However, most abstractive summarization methods are not able to control the summary length. To this problem, Kikuchi et al. (2016) and Liu et al. (2018) proposed abstractive summarization models with a capability of summary length control. One is an LSTM based summarization model, and the other is a CNN based one. They proposed to enforce the desired length in the decoding of training and generation. Their models, however, leave much room for improvement, at least with regard to two aspects. One aspect is that the summarization performance is still worse than other state-of-the-art models. The other is that their models sometimes fail to control the output length. In this paper, we address these two issues by incorporating global training bas"
P19-1099,P17-1101,0,0.261948,"Missing"
P19-1250,W17-3002,0,0.289254,"pical situation for (a) can be seen in political comments, where the “goodness” of the comment will be decided on the basis of the political views of the majority of the users rather than its quality. The situation for (b) can be illustrated by a case where earlier comments tend to receive more feedback since they will be displayed at the top of the page, which implies later comments will be ignored irrespective of their quality. In this paper, we directly evaluate the quality of comments separately from user feedback, focusing on their “constructiveness,” as studied in (Napoles et al., 2017; Kolhatkar and Taboada, 2017). This quality measure is reasonable for services in that displaying constructive comments can stimulate discussion on a news article, which makes the user-generated content richer. We use the definition of constructiveness as in the previous studies, but a clear difference from them is that we address a ranking task, whereas the aforementioned sources addressed classification tasks. In a ranking task, we need to rank comments for each article. That is, when we label 1,000 comments, there are many choices, e.g., 200 articles with 5 2619 Proceedings of the 57th Annual Meeting of the Association"
P19-1250,W04-3230,0,0.0804159,"rt vector regression model (Vapnik et al., 1997) trained to directly infer the C-score. We used liblinear-ranksvm6 for RankSVM and SVR. The cost parameter was determined from {20 , . . . , 2−13 } with a validation dataset, where we prepared another 5K comments for each setting for Shallow/Deep. The features for training RankSVM and SVR were made from a comment and the corresponding article. See the next section for the details on preprocessing and the features. 3.2 Preprocessing and Features The preprocessing for training RankSVM and SVR is as follows. We used a morphological analyzer MeCab7 (Kudo et al., 2004), with a neologism dictionary, NEologd8 (Toshinori Sato and Okumura, 2017), for splitting Japanese text into words. We replaced numbers with a special token and standardized letter types, i.e., decapitalization and halfwidth-to-fullwidth.9 We did not remove stop-words because function words would affect the performance in our task, especially for decency. We cut low-frequency words off that appeared only three times or less in each dataset. The dictionary size was about 50,000. The features for a comment (with the corresponding news article) used for RankSVM and SVR are the bag-of-words of the"
P19-1250,P16-3007,0,0.185043,"Missing"
P19-1250,P16-2032,0,0.406471,"d content1 ) for service providers because users can obtain supplementary information about news articles through other users’ opinions. Given that comment visibility is a part of the user experience, ranking comments is practically important. For example, Figure 1 shows a page displaying comments on a Japanese news portal, Yahoo! News.2 The page has a list of comments (displayed below articles), and each comment has buttons for user feedback (“Like,” “Dislike,” and “Reply”). There have been many comment ranking studies (Hsu et al., 2009; Das Sarma et al., 2010; Brand and Van Der Merwe, 2014; Wei et al., 2016) with users’ positive feedback for a comment (e.g., “Like”- or “Upvote”-button clicks) serving as the 1 https://en.wikipedia.org/wiki/ User-generated_content 2 https://news.yahoo.co.jp/ quality measure. However, this type of measurement has two drawbacks: (a) user feedback does not always satisfy the service provider’s needs, such as to create a fair place, and (b) user feedback will be biased by where comments appear in a comment thread. A typical situation for (a) can be seen in political comments, where the “goodness” of the comment will be decided on the basis of the political views of the"
P98-2145,A94-1010,0,0.0133069,"we now plan to calculate the weights for a subset of the texts by clustering the training texts. Since there m a y be some differences among real texts which reflect the differences of their author, their style, their genre, etc., we think that clustering a set of the training texts and calculating the weights for each cluster, rather than calculating the weights for the entire set of texts, might improve the accuracy. In the area of speech recognition, to improve the accuracy of the language models, clustering the training d a t a is considered to be a promising method for automatic training(Carter, 1994; Iyer et al., 1994). Carter presents a method for clustering the sentences in a training corpus automatically into some subcorpora on the criterion of entropy reduction and calculating separate language model parameters for each cluster. He asserts that this kind of clustering offers a way to improve the performance of a model significantly. Acknowledgments The authors would like to express our gratitude to Kadokawa publisher for allowing us to use their thesaurus, and Dr.Shigenobu Aoki of G u n m a Univ. and Dr.Teruo Matsuzawa of J A I S T for their suggestions of statistical analysis, and D"
P98-2145,J87-1002,0,0.0164681,"e the accuracy. In this paper, we describe a method for identifying segment boundaries of a Japanese text with the aid of multiple surface linguistic cues, such as conjunctives, ellipsis, types of sentences, and lexical cohesion. There are a variety of methods for combining multiple knowledge sources (linguistic cues)(McRoy, 1992). Among them, a weighted sum of the scores for all cues that reflects their contribution to identifying the correct segment boundaries is often used as the 881 overall measure to rank the possible segment boundaries. In the past researches (Kurohashi and Nagao, 1994; Cohen, 1987), the weights for each cue tend to be determined by intuition or trial and error. Since determining weights by hand is a labor-intensive task and the weights do not always to achieve optimal or even near-optimal performance(Rayner et al., 1994), we think it is better to determine the weights automatically in order to both avoid the need for expert hand tuning and achieve performance that is at least locally optimal. We begin by assuming the existence of training texts with the correct segment boundaries and use the method of multiple regression analysis for automatically training the weights."
P98-2145,P92-1032,0,0.022714,"r test. Changing the group for the test, we evaluate the performance by the cross validation(Weiss and Kulikowski, 1991). 5. Use only selected cues by applying the stepwise method. As mentioned in section 4, we use the stepwise method for selecting useful cues for training sets. The condition is the same as for the case 4 except for the cue selection. 6. Answer from five human subjects. By this experiment, we try to clarify the upper bound of the performance of the text segmentation task, which can be considered to indicate the degree of the difficulty of the task(Passonneau and Litman, 1993; Gale et al., 1992). Figure 1,2 and table 1 show the results of the experiments. Two figures show the system's mean performance of 14 texts. Table 1 shows the 5 subjects' mean performance of 14 texts (experiment 6). We think table 1 shows the upper bound of the performance of the text segmentation task. We also calculate the lower bound of the performance of the task(""lowerbound"" in figure 2). It can be calculated by considering the case where the system selects boundary candidates at random. In the case, the precision equals to the mean probability that each candidate will be a correct boundary. The recall is e"
P98-2145,J86-3001,0,0.0262544,"iple surface linguistic cues, though our experiments might be small-scale. We also present a method of training the weights for multiple linguistic cues automatically without the overfitting problem. 1 Introduction A text consists of multiple sentences that have semantic relations with each other. They form semantic units which are usually called discourse segments. The global discourse structure of a text can be constructed by relating the discourse segments with each other. Therefore, identifying segment boundaries in a text is considered as a first step to construct the discourse structure(Grosz and Sidner, 1986). The use of surface linguistic cues in a text for identification of segment boundaries has been extensively researched, since it is impractical to assume the use of world knowledge for discourse analysis of real texts. Among a variety of surface cues, lexical cohesion(Halliday and Hasan, 1976), the surface relationship among words that are semantically similar, has recently received much attention and has been widely used for text segmentation(Morris and Hirst, 1991; Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994). Okumura and Honda (Okumura and Honda, 1994) found that the information of"
P98-2145,H94-1014,0,0.0206805,"calculate the weights for a subset of the texts by clustering the training texts. Since there m a y be some differences among real texts which reflect the differences of their author, their style, their genre, etc., we think that clustering a set of the training texts and calculating the weights for each cluster, rather than calculating the weights for the entire set of texts, might improve the accuracy. In the area of speech recognition, to improve the accuracy of the language models, clustering the training d a t a is considered to be a promising method for automatic training(Carter, 1994; Iyer et al., 1994). Carter presents a method for clustering the sentences in a training corpus automatically into some subcorpora on the criterion of entropy reduction and calculating separate language model parameters for each cluster. He asserts that this kind of clustering offers a way to improve the performance of a model significantly. Acknowledgments The authors would like to express our gratitude to Kadokawa publisher for allowing us to use their thesaurus, and Dr.Shigenobu Aoki of G u n m a Univ. and Dr.Teruo Matsuzawa of J A I S T for their suggestions of statistical analysis, and D r . T h a n a r u k"
P98-2145,P93-1041,0,0.477521,"tifying segment boundaries in a text is considered as a first step to construct the discourse structure(Grosz and Sidner, 1986). The use of surface linguistic cues in a text for identification of segment boundaries has been extensively researched, since it is impractical to assume the use of world knowledge for discourse analysis of real texts. Among a variety of surface cues, lexical cohesion(Halliday and Hasan, 1976), the surface relationship among words that are semantically similar, has recently received much attention and has been widely used for text segmentation(Morris and Hirst, 1991; Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994). Okumura and Honda (Okumura and Honda, 1994) found that the information of lexical cohesion is not enough and incorporation of other surface information may improve the accuracy. In this paper, we describe a method for identifying segment boundaries of a Japanese text with the aid of multiple surface linguistic cues, such as conjunctives, ellipsis, types of sentences, and lexical cohesion. There are a variety of methods for combining multiple knowledge sources (linguistic cues)(McRoy, 1992). Among them, a weighted sum of the scores for all cues that ref"
P98-2145,P95-1015,0,0.601371,"Missing"
P98-2145,J92-1001,0,0.0243847,"been widely used for text segmentation(Morris and Hirst, 1991; Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994). Okumura and Honda (Okumura and Honda, 1994) found that the information of lexical cohesion is not enough and incorporation of other surface information may improve the accuracy. In this paper, we describe a method for identifying segment boundaries of a Japanese text with the aid of multiple surface linguistic cues, such as conjunctives, ellipsis, types of sentences, and lexical cohesion. There are a variety of methods for combining multiple knowledge sources (linguistic cues)(McRoy, 1992). Among them, a weighted sum of the scores for all cues that reflects their contribution to identifying the correct segment boundaries is often used as the 881 overall measure to rank the possible segment boundaries. In the past researches (Kurohashi and Nagao, 1994; Cohen, 1987), the weights for each cue tend to be determined by intuition or trial and error. Since determining weights by hand is a labor-intensive task and the weights do not always to achieve optimal or even near-optimal performance(Rayner et al., 1994), we think it is better to determine the weights automatically in order to b"
P98-2145,C94-2121,1,0.725184,"n a text is considered as a first step to construct the discourse structure(Grosz and Sidner, 1986). The use of surface linguistic cues in a text for identification of segment boundaries has been extensively researched, since it is impractical to assume the use of world knowledge for discourse analysis of real texts. Among a variety of surface cues, lexical cohesion(Halliday and Hasan, 1976), the surface relationship among words that are semantically similar, has recently received much attention and has been widely used for text segmentation(Morris and Hirst, 1991; Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994). Okumura and Honda (Okumura and Honda, 1994) found that the information of lexical cohesion is not enough and incorporation of other surface information may improve the accuracy. In this paper, we describe a method for identifying segment boundaries of a Japanese text with the aid of multiple surface linguistic cues, such as conjunctives, ellipsis, types of sentences, and lexical cohesion. There are a variety of methods for combining multiple knowledge sources (linguistic cues)(McRoy, 1992). Among them, a weighted sum of the scores for all cues that reflects their contribution to identifying"
P98-2145,P93-1020,0,0.189206,"Missing"
P98-2145,H94-1040,0,0.0684039,"Missing"
P98-2145,C96-2164,0,0.022328,"-boundary 4We use the Kadokawa Ruigo Shin Jiten(Oono and Hamanishi, 1981) as Japanese thesaurus. points in the multiple regression analysis. If we can give the better value to S(n, n + 1) that reflects the real phenomena in the texts more precisely, we think we can expect the better performance. However, since we have only the correct segment boundaries that are tagged to the training texts, we decide to give 10 each S(n, n + 1) of the segment boundary point and - 1 to the non-boundary point. These values were decided by the results of the preliminary experiment with four types of S. Watanabe(Watanabe, 1996) can be considered as a related work. He describes a system which automatically creates an abstract of a newspaper article by selecting important sentences of a given text. He applies the multiple regression analysis for weighting the surface features of a sentence in order to determine the importance of sentences. Each S of a sentence in training texts is given a score that the number of human subjects who judge the sentence as important, divided by the number of all subjects. We do not adopt the same method for giving a value to S, because we think that such a task by human subjects is labor"
P98-2145,J91-1002,0,\N,Missing
P98-2145,C94-2183,0,\N,Missing
P98-2145,P94-1002,0,\N,Missing
R09-1052,P04-1085,0,0.235831,"Missing"
R09-1052,W00-1303,0,0.0337202,"Missing"
R09-1052,W04-2319,0,0.0224548,"Missing"
R09-1052,P08-1103,0,0.0651582,"Missing"
R09-1052,P05-1012,0,0.135188,"Missing"
R19-1059,N18-1155,1,0.612594,"hical tree structure inherent in the document. Nallapati et al. (2016) and Yang et al. (2016) also used a hierarchical attention that consists of two simple attention modules; one is for words and the other is for sentences. Our attention mechanism differs from them in that ours captures discourse tree structures by new hierarchical attention networks, inspired by the models for capturing sentence-level dependency structures, e.g. machine translation (Hashimoto and Tsuruoka, 2017), dependency parsing (Zhang et al., 2017), constituency parsing (Kamigaito et al., 2017) and sentence compression (Kamigaito et al., 2018). Note that these models were designed for sentence-level tasks while we focus on the document-level summarization task. Sentence selection modules that consider discourse structures of documents have been shown to be useful in ILP-based summarizers. Hirao et al. (2013) attempted to incorporate discourse information in ILP-based sentence extractors. Kikuchi et al. (2014) later proposed another ILP model that takes into account the discourse structure. Their model jointly selects and compresses sentences in To effectively avoid the influence of parse errors and take advantage of the recent adva"
R19-1059,P16-1046,0,0.108004,"nt from the one which they were trained on. Introduction Document summarization is the task of automatically shortening a source document while retaining its salient information. In this paper, we present a recurrent neural network (RNN)-based extractive summarizer taking into account the discourse structure inherent in the source document. 497 Proceedings of Recent Advances in Natural Language Processing, pages 497–506, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_059 2 RNN-based approaches have achieved the stateof-the-art performance in document summarization (Cheng and Lapata, 2016; Nallapati et al., 2017). However, RNN-based summarizers treat the source document just as a sequence of sentences, and ignore the discourse tree structure inherent in the document. The lack of such information limits the ability to correctly compute relative importance between sentences and reduces the coherence of output summaries. Cohan et al. (2018) might be the only exception to the above, showing that the effectiveness of incorporating discourse information into an RNN-based summarizer for scientific papers by treating the source document as a sequence of sections such as “Introduction”"
R19-1059,I17-2002,1,0.785576,"r attention module explicitly captures the hierarchical tree structure inherent in the document. Nallapati et al. (2016) and Yang et al. (2016) also used a hierarchical attention that consists of two simple attention modules; one is for words and the other is for sentences. Our attention mechanism differs from them in that ours captures discourse tree structures by new hierarchical attention networks, inspired by the models for capturing sentence-level dependency structures, e.g. machine translation (Hashimoto and Tsuruoka, 2017), dependency parsing (Zhang et al., 2017), constituency parsing (Kamigaito et al., 2017) and sentence compression (Kamigaito et al., 2018). Note that these models were designed for sentence-level tasks while we focus on the document-level summarization task. Sentence selection modules that consider discourse structures of documents have been shown to be useful in ILP-based summarizers. Hirao et al. (2013) attempted to incorporate discourse information in ILP-based sentence extractors. Kikuchi et al. (2014) later proposed another ILP model that takes into account the discourse structure. Their model jointly selects and compresses sentences in To effectively avoid the influence of"
R19-1059,N18-2097,0,0.169967,"of Recent Advances in Natural Language Processing, pages 497–506, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_059 2 RNN-based approaches have achieved the stateof-the-art performance in document summarization (Cheng and Lapata, 2016; Nallapati et al., 2017). However, RNN-based summarizers treat the source document just as a sequence of sentences, and ignore the discourse tree structure inherent in the document. The lack of such information limits the ability to correctly compute relative importance between sentences and reduces the coherence of output summaries. Cohan et al. (2018) might be the only exception to the above, showing that the effectiveness of incorporating discourse information into an RNN-based summarizer for scientific papers by treating the source document as a sequence of sections such as “Introduction” or “Conclusion”. However, they were not able to show how the tree-like discourse structure is effective in RNN-based approaches for extractive single-document summarization. Related Work There have long been many attempts at tackling extractive single-document summarization (Luhn, 1958), but there is still room for improvements in terms of ROUGE scores"
R19-1059,P14-2052,1,0.885116,"Figure 1. In the figure, each node corresponds to a sentence. Regarding the relations between the sentences, sentence S2 elaborates the fact mentioned in sentence S1. In addition, S2 is further elaborated by S3. S4 is a contrast to the mention S1. Such relations are essential cues for generating a concise and coherent summary. For example, elaborated sentences tend to be more important than elaborating sentences, and the elaborated sentences should be included in the summary while the elaborating sentences are not. Several Integer Linear Programming (ILP)based summarizers (Hirao et al., 2013; Kikuchi et al., 2014) use the discourse information given by a discourse parser (Hernault et al., 2010). Thus, the performance of the summarizers is strongly affected by the performance of the discourse parsers. The performance of the parsers deteriorates especially when they are applied to documents of a domain different from the one which they were trained on. Introduction Document summarization is the task of automatically shortening a source document while retaining its salient information. In this paper, we present a recurrent neural network (RNN)-based extractive summarizer taking into account the discourse"
R19-1059,P14-1048,0,0.0304477,"of the source document when calculating the probability distribution p(yi |x, θ). an ILP summarizer. Unlike the researches above, our focus is on incorporating discourse information into RNN-based summarizers. Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) and RST are the most commonly used framework to represent a discourse structure. PDTB focuses on the relation between two sentences, and the annotated structure for a document is not necessarily a tree. In contrast, RST is forced to represent a document as a tree. Discourse parsers for both schema are available (Hernault et al., 2010; Feng and Hirst, 2014; Wang and Lan, 2015). There are at least two methods to convert an RST-based tree structure to a dependency structure (Hirao et al., 2002; Li et al., 2014). Hayashi et al. (2016) compared these methods and mentioned that DEP-DT by Hirao et al. (2002) has an advantage for applying to summarization tasks. We use DEP-DT for this research since we focus on integrating the tree structure into a summarizer. We found only one model that jointly learns RST parsing and document summarization (Goyal and Eisenstein, 2016). They used the SampleRank algorithm (Wick et al., 2011), a stochastic structure pr"
R19-1059,W16-5903,0,0.0184934,"cument as a tree. Discourse parsers for both schema are available (Hernault et al., 2010; Feng and Hirst, 2014; Wang and Lan, 2015). There are at least two methods to convert an RST-based tree structure to a dependency structure (Hirao et al., 2002; Li et al., 2014). Hayashi et al. (2016) compared these methods and mentioned that DEP-DT by Hirao et al. (2002) has an advantage for applying to summarization tasks. We use DEP-DT for this research since we focus on integrating the tree structure into a summarizer. We found only one model that jointly learns RST parsing and document summarization (Goyal and Eisenstein, 2016). They used the SampleRank algorithm (Wick et al., 2011), a stochastic structure prediction model, while our main focus is to take into account discourse structures in RNNbased summarizers. 3 4 RNN-Based Extractive Summarizer In this section, we first explain the base model and give the details of our proposed attention module in the following section. The base model is composed of two main components: a neural network-based hierarchical document encoder and a decoder-based sentence scorer. The document encoder is further split into two components; a sentence reader and a document reader. The"
R19-1059,P14-1003,0,0.0151844,"discourse information into RNN-based summarizers. Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) and RST are the most commonly used framework to represent a discourse structure. PDTB focuses on the relation between two sentences, and the annotated structure for a document is not necessarily a tree. In contrast, RST is forced to represent a document as a tree. Discourse parsers for both schema are available (Hernault et al., 2010; Feng and Hirst, 2014; Wang and Lan, 2015). There are at least two methods to convert an RST-based tree structure to a dependency structure (Hirao et al., 2002; Li et al., 2014). Hayashi et al. (2016) compared these methods and mentioned that DEP-DT by Hirao et al. (2002) has an advantage for applying to summarization tasks. We use DEP-DT for this research since we focus on integrating the tree structure into a summarizer. We found only one model that jointly learns RST parsing and document summarization (Goyal and Eisenstein, 2016). They used the SampleRank algorithm (Wick et al., 2011), a stochastic structure prediction model, while our main focus is to take into account discourse structures in RNNbased summarizers. 3 4 RNN-Based Extractive Summarizer In this secti"
R19-1059,W04-1013,0,0.0349711,"t. LREG is a feature-rich logistic regression based approach used as a baseline in Cheng and Lapata (2016). ILP is a phrasebased extraction system proposed by Woodsend and Lapata (2010). The approach extracts the phrases and recombines them subject to the constraints in the ILP such as length, coverage or grammaticality. Both TGRAPH (Parveen et al., 2015) and URANK (Wan, 2010) are graph-based sentence extraction approaches, that perform well on the DUC2002 corpus. Evaluation Metrics: We conducted both automatic evaluation and human evaluation. In automatic evaluation, we adopted ROUGE scores (Lin, 2004). We specifically calculated ROUGE1, ROUGE-2 and ROUGE-L by using the Pyrouge library6 . The highlights in the Dailymail dataset were treated as reference summaries when we calculated the scores. We used three length constraints; 75 bytes, 275 bytes (Nallapati et al., 2017; Cheng and Lapata, 2016) and the bytes of reference summaries. We truncated generated summaries in the middle to conform to the length constraints. We adopted the last constraint to evaluate whether a model can include sufficient information within the ideal summary length. For the evaluation on out-of-domain data, we report"
R19-1059,D17-1012,0,0.0282195,"ness of incorporating discourse information into RNN-based summarizers. Unlike their model, our attention module explicitly captures the hierarchical tree structure inherent in the document. Nallapati et al. (2016) and Yang et al. (2016) also used a hierarchical attention that consists of two simple attention modules; one is for words and the other is for sentences. Our attention mechanism differs from them in that ours captures discourse tree structures by new hierarchical attention networks, inspired by the models for capturing sentence-level dependency structures, e.g. machine translation (Hashimoto and Tsuruoka, 2017), dependency parsing (Zhang et al., 2017), constituency parsing (Kamigaito et al., 2017) and sentence compression (Kamigaito et al., 2018). Note that these models were designed for sentence-level tasks while we focus on the document-level summarization task. Sentence selection modules that consider discourse structures of documents have been shown to be useful in ILP-based summarizers. Hirao et al. (2013) attempted to incorporate discourse information in ILP-based sentence extractors. Kikuchi et al. (2014) later proposed another ILP model that takes into account the discourse structure. Their"
R19-1059,W16-3616,0,0.0196532,"ion into RNN-based summarizers. Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) and RST are the most commonly used framework to represent a discourse structure. PDTB focuses on the relation between two sentences, and the annotated structure for a document is not necessarily a tree. In contrast, RST is forced to represent a document as a tree. Discourse parsers for both schema are available (Hernault et al., 2010; Feng and Hirst, 2014; Wang and Lan, 2015). There are at least two methods to convert an RST-based tree structure to a dependency structure (Hirao et al., 2002; Li et al., 2014). Hayashi et al. (2016) compared these methods and mentioned that DEP-DT by Hirao et al. (2002) has an advantage for applying to summarization tasks. We use DEP-DT for this research since we focus on integrating the tree structure into a summarizer. We found only one model that jointly learns RST parsing and document summarization (Goyal and Eisenstein, 2016). They used the SampleRank algorithm (Wick et al., 2011), a stochastic structure prediction model, while our main focus is to take into account discourse structures in RNNbased summarizers. 3 4 RNN-Based Extractive Summarizer In this section, we first explain th"
R19-1059,D15-1166,0,0.137181,"Missing"
R19-1059,C02-1053,0,0.150408,"is on incorporating discourse information into RNN-based summarizers. Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) and RST are the most commonly used framework to represent a discourse structure. PDTB focuses on the relation between two sentences, and the annotated structure for a document is not necessarily a tree. In contrast, RST is forced to represent a document as a tree. Discourse parsers for both schema are available (Hernault et al., 2010; Feng and Hirst, 2014; Wang and Lan, 2015). There are at least two methods to convert an RST-based tree structure to a dependency structure (Hirao et al., 2002; Li et al., 2014). Hayashi et al. (2016) compared these methods and mentioned that DEP-DT by Hirao et al. (2002) has an advantage for applying to summarization tasks. We use DEP-DT for this research since we focus on integrating the tree structure into a summarizer. We found only one model that jointly learns RST parsing and document summarization (Goyal and Eisenstein, 2016). They used the SampleRank algorithm (Wick et al., 2011), a stochastic structure prediction model, while our main focus is to take into account discourse structures in RNNbased summarizers. 3 4 RNN-Based Extractive Summar"
R19-1059,P17-2043,0,0.0218393,"might be the only exception to the above, showing that the effectiveness of incorporating discourse information into an RNN-based summarizer for scientific papers by treating the source document as a sequence of sections such as “Introduction” or “Conclusion”. However, they were not able to show how the tree-like discourse structure is effective in RNN-based approaches for extractive single-document summarization. Related Work There have long been many attempts at tackling extractive single-document summarization (Luhn, 1958), but there is still room for improvements in terms of ROUGE scores (Hirao et al., 2017). The recent focus has been on RNN-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018). We further extend the attention mechanism used in RNN-based summarizers to capture a discourse structure. RNN-based approaches were introduced to natural language processing tasks by the pioneering work by Bahdanau et al. (2015) and Luong et al. (2015), originally for machine translation. Rush et al. (2015) applied the approach to a sentence compression task. Nallapati et al. (2016) extended the model to abstractive document summarization. The DailyMail dataset (Hermann e"
R19-1059,N18-1158,0,0.0861849,"RNN-based summarizer for scientific papers by treating the source document as a sequence of sections such as “Introduction” or “Conclusion”. However, they were not able to show how the tree-like discourse structure is effective in RNN-based approaches for extractive single-document summarization. Related Work There have long been many attempts at tackling extractive single-document summarization (Luhn, 1958), but there is still room for improvements in terms of ROUGE scores (Hirao et al., 2017). The recent focus has been on RNN-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018). We further extend the attention mechanism used in RNN-based summarizers to capture a discourse structure. RNN-based approaches were introduced to natural language processing tasks by the pioneering work by Bahdanau et al. (2015) and Luong et al. (2015), originally for machine translation. Rush et al. (2015) applied the approach to a sentence compression task. Nallapati et al. (2016) extended the model to abstractive document summarization. The DailyMail dataset (Hermann et al., 2015) has been commonly used for training abstractive summarizers. Cheng and Lapata (2016) and Nallapati et al. (20"
R19-1059,D13-1158,0,0.0504508,"Missing"
R19-1059,D15-1226,0,0.0544244,"Missing"
R19-1059,prasad-etal-2008-penn,0,0.0459578,"tter than state-of-the-art neural network-based extractive summarizers. 498 resent the discourse dependency tree of x. Specifically, element Ek,l equals 1 if the edge from xk to xl exists in the discourse tree; otherwise Ek,l = 0. Note that we use the discourse structure matrices E only in the training phase. The model does not require the RST annotations of the source document when calculating the probability distribution p(yi |x, θ). an ILP summarizer. Unlike the researches above, our focus is on incorporating discourse information into RNN-based summarizers. Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) and RST are the most commonly used framework to represent a discourse structure. PDTB focuses on the relation between two sentences, and the annotated structure for a document is not necessarily a tree. In contrast, RST is forced to represent a document as a tree. Discourse parsers for both schema are available (Hernault et al., 2010; Feng and Hirst, 2014; Wang and Lan, 2015). There are at least two methods to convert an RST-based tree structure to a dependency structure (Hirao et al., 2002; Li et al., 2014). Hayashi et al. (2016) compared these methods and mentioned that DEP-DT by Hirao et a"
R19-1059,D15-1044,0,0.0277343,"ere have long been many attempts at tackling extractive single-document summarization (Luhn, 1958), but there is still room for improvements in terms of ROUGE scores (Hirao et al., 2017). The recent focus has been on RNN-based approaches (Cheng and Lapata, 2016; Nallapati et al., 2017; Narayan et al., 2018). We further extend the attention mechanism used in RNN-based summarizers to capture a discourse structure. RNN-based approaches were introduced to natural language processing tasks by the pioneering work by Bahdanau et al. (2015) and Luong et al. (2015), originally for machine translation. Rush et al. (2015) applied the approach to a sentence compression task. Nallapati et al. (2016) extended the model to abstractive document summarization. The DailyMail dataset (Hermann et al., 2015) has been commonly used for training abstractive summarizers. Cheng and Lapata (2016) and Nallapati et al. (2017) later proposed the methods to automatically annotate the binary labels, enabling us to train extractive models. Cohan et al. (2018) demonstrated the usefulness of incorporating discourse information into RNN-based summarizers. Unlike their model, our attention module explicitly captures the hierarchical t"
R19-1059,C10-1128,0,0.0361701,"orks in the encoder. Refresh is a state-of-the-art method using reinforcement learning (Narayan et al., 2018) 5 . In addition to the above methods, we compared our models with previously reported performances on the DUC2002 test set. LREG is a feature-rich logistic regression based approach used as a baseline in Cheng and Lapata (2016). ILP is a phrasebased extraction system proposed by Woodsend and Lapata (2010). The approach extracts the phrases and recombines them subject to the constraints in the ILP such as length, coverage or grammaticality. Both TGRAPH (Parveen et al., 2015) and URANK (Wan, 2010) are graph-based sentence extraction approaches, that perform well on the DUC2002 corpus. Evaluation Metrics: We conducted both automatic evaluation and human evaluation. In automatic evaluation, we adopted ROUGE scores (Lin, 2004). We specifically calculated ROUGE1, ROUGE-2 and ROUGE-L by using the Pyrouge library6 . The highlights in the Dailymail dataset were treated as reference summaries when we calculated the scores. We used three length constraints; 75 bytes, 275 bytes (Nallapati et al., 2017; Cheng and Lapata, 2016) and the bytes of reference summaries. We truncated generated summaries"
R19-1059,K15-2002,0,0.0247434,"when calculating the probability distribution p(yi |x, θ). an ILP summarizer. Unlike the researches above, our focus is on incorporating discourse information into RNN-based summarizers. Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) and RST are the most commonly used framework to represent a discourse structure. PDTB focuses on the relation between two sentences, and the annotated structure for a document is not necessarily a tree. In contrast, RST is forced to represent a document as a tree. Discourse parsers for both schema are available (Hernault et al., 2010; Feng and Hirst, 2014; Wang and Lan, 2015). There are at least two methods to convert an RST-based tree structure to a dependency structure (Hirao et al., 2002; Li et al., 2014). Hayashi et al. (2016) compared these methods and mentioned that DEP-DT by Hirao et al. (2002) has an advantage for applying to summarization tasks. We use DEP-DT for this research since we focus on integrating the tree structure into a summarizer. We found only one model that jointly learns RST parsing and document summarization (Goyal and Eisenstein, 2016). They used the SampleRank algorithm (Wick et al., 2011), a stochastic structure prediction model, while"
R19-1059,P10-1058,0,0.0401252,"re of the target sentence, while our approach incorporates the information on the parent sentence of the target sentence. NeuralSum is also a neural network-based summarizer which uses convolutional neural networks in the encoder. Refresh is a state-of-the-art method using reinforcement learning (Narayan et al., 2018) 5 . In addition to the above methods, we compared our models with previously reported performances on the DUC2002 test set. LREG is a feature-rich logistic regression based approach used as a baseline in Cheng and Lapata (2016). ILP is a phrasebased extraction system proposed by Woodsend and Lapata (2010). The approach extracts the phrases and recombines them subject to the constraints in the ILP such as length, coverage or grammaticality. Both TGRAPH (Parveen et al., 2015) and URANK (Wan, 2010) are graph-based sentence extraction approaches, that perform well on the DUC2002 corpus. Evaluation Metrics: We conducted both automatic evaluation and human evaluation. In automatic evaluation, we adopted ROUGE scores (Lin, 2004). We specifically calculated ROUGE1, ROUGE-2 and ROUGE-L by using the Pyrouge library6 . The highlights in the Dailymail dataset were treated as reference summaries when we ca"
R19-1059,N16-1174,0,0.0544093,"l. (2016) extended the model to abstractive document summarization. The DailyMail dataset (Hermann et al., 2015) has been commonly used for training abstractive summarizers. Cheng and Lapata (2016) and Nallapati et al. (2017) later proposed the methods to automatically annotate the binary labels, enabling us to train extractive models. Cohan et al. (2018) demonstrated the usefulness of incorporating discourse information into RNN-based summarizers. Unlike their model, our attention module explicitly captures the hierarchical tree structure inherent in the document. Nallapati et al. (2016) and Yang et al. (2016) also used a hierarchical attention that consists of two simple attention modules; one is for words and the other is for sentences. Our attention mechanism differs from them in that ours captures discourse tree structures by new hierarchical attention networks, inspired by the models for capturing sentence-level dependency structures, e.g. machine translation (Hashimoto and Tsuruoka, 2017), dependency parsing (Zhang et al., 2017), constituency parsing (Kamigaito et al., 2017) and sentence compression (Kamigaito et al., 2018). Note that these models were designed for sentence-level tasks while"
R19-1059,E17-1063,0,0.0318384,"N-based summarizers. Unlike their model, our attention module explicitly captures the hierarchical tree structure inherent in the document. Nallapati et al. (2016) and Yang et al. (2016) also used a hierarchical attention that consists of two simple attention modules; one is for words and the other is for sentences. Our attention mechanism differs from them in that ours captures discourse tree structures by new hierarchical attention networks, inspired by the models for capturing sentence-level dependency structures, e.g. machine translation (Hashimoto and Tsuruoka, 2017), dependency parsing (Zhang et al., 2017), constituency parsing (Kamigaito et al., 2017) and sentence compression (Kamigaito et al., 2018). Note that these models were designed for sentence-level tasks while we focus on the document-level summarization task. Sentence selection modules that consider discourse structures of documents have been shown to be useful in ILP-based summarizers. Hirao et al. (2013) attempted to incorporate discourse information in ILP-based sentence extractors. Kikuchi et al. (2014) later proposed another ILP model that takes into account the discourse structure. Their model jointly selects and compresses sent"
S07-1069,S07-1012,0,0.0614994,"sly updated, the reliability of Wikipedia as a source of seed pages will be promising in the future. Moreover, observing the results of each person in detail, we found that the purity values are improved when we use a seed page that describes the person using more than about 200 words. On the other hand, in the case where a seed page describes a person with less than 150 words, or describes not only the target person but also some other persons, we could not obtain high purity values. 5 Conclusion In this paper, we described our participating system in the SemEval-2007 Web People Search Task (Artiles et al., 2007). Our system used a semisupervised clustering which controls the fluctuation of the centroid of a cluster. The evaluation results showed that our proposed method achieves high scores in inverse purity, with the lower scores in purity. This fact indicates that our proposed method tends to integrate search-result Web pages into a seed page. This clustering result makes it easier for a user to browse the results of a person Web search. However, in the generated cluster with a seed page, irrelevant search-result Web pages are also contained. This problem can be solved by in321 Purity Inverse purit"
S07-1069,W03-0405,0,0.179586,"ges, while others have only one or two. In light of these facts, if a labeled Web page that describes a person is introduced, clustering for personal name disambiguation would be much more accurate. In the following, we refer to such a labeled Web page as the “seed page.” Then, in order to disambiguate personal names in Web search results, we introduce semi-supervised clustering that uses the seed page to aid the clustering of unlabeled search-result Web pages. Our semi-supervised clustering approach is characterized by controlling the fluctuation of the centroid of a cluster. 2 Related Work (Mann and Yarowsky, 2003) first extract biographical information, such as birthdates, birthplaces, occupations, and so on. Then, for each document, they generate a feature vector composed of the extracted biographical information, proper nouns, and the TF-IDF score computed from the documents in the search results. Finally, using this feature vector, they disambiguate personal names by generating clusters based on a bottom-up centroid agglomera318 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 318–321, c Prague, June 2007. 2007 Association for Computational Linguistics tive"
S07-1069,N01-1004,0,0.0746836,"Missing"
S07-1069,W06-0803,0,\N,Missing
S10-1012,P97-1008,0,0.0749596,"ological information (word boundaries, a partof-speech (POS) tag, a base form, and a reading) for all words. All the morphological information was automatically annotated using chasen2 with unidic and was manually postedited. • Genre code As in the training data, each document was assigned a code indicating its genre from the aforementioned list. 1 Due to space limits, we unfortunately cannot present the statistics of the training and test data, such as the number of instances in different genres, the number of instances for a new word sense, and the Jensen Shannon (JS) divergence (Lin, 1991; Dagan et al., 1997) between the word sense distributions of two different genres. We hope we will present them in another paper in the near future. 2 http://chasen-legacy.sourceforge.jp/ • Word sense IDs Word sense IDs were manually annotated for 3 The word sense IDs for them were hidden from the participants. 70 • The “new word sense” tag was to be chosen only when all sense IDs were not absolutely applicable. the target words4 . The number of target words was 50, with 22 nouns, 23 verbs, and 5 adjectives. Fifty instances of each target word were provided, consisting of a total of 2,500 instances for the evalua"
S10-1012,kilgarriff-rosenzweig-2000-english,0,0.0429634,"easure for supervised word sense disambiguation systems was computed using the scorer. The Iwanami Kokugo Jiten has three levels for sense IDs, and we used the middle-level sense in the task. Therefore, the scoring in the task was ‘middle-grained scoring.’ • The POSs of target words were either a noun, a verb, or an adjective. • We chose words that occurred more than 50 times in the training data. • The relative “difficulty” in disambiguating the sense of words was taken into account. The difficulty of the word w was defined by the entropy of the word sense distribution E(w) in the test data (Kilgarriff and Rosenzweig, 2000). Obviously, the higher E(w) is, the more difficult the WSD for w is. 2. The ability of finding the instances of new senses was evaluated, assuming the task as classifying each instance into a ‘known sense’ or ‘new sense’ class. The outputted sense IDs (same as in 1.) were compared to the given gold standard word senses, and the usual accuracy for binary classification was computed, assuming all sense IDs in the dictionary were in the ‘known sense’ class. • The number of instances for a new sense was also taken into account. 3.2 Manual Annotation Nine annotators assigned the correct word sense"
S10-1012,I08-7018,0,0.029099,"rds as test data. Participants were requested to assign one or more sense IDs to each target word, optionally with associated probabilities. The number of target words was 50, with 22 nouns, 23 verbs, and 5 adjectives. Fifty instances of each target word were provided, con1. All previous Japanese sense-tagged corpora were from newspaper articles, while sensetagged corpora were constructed in English on balanced corpora, such as Brown corpus and BNC corpus. The first balanced corpus of contemporary written Japanese (BCCWJ corpus) is now being constructed as part of a national project in Japan (Maekawa, 2008), and we are now constructing a sense-tagged corpus based on it. Therefore, the task will use the first balanced Japanese sense-tagged corpus. 69 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 69–74, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics • Genre code Each document was assigned a code indicating its genre from the aforementioned list. sisting of a total of 2,500 instances for the evaluation. In what follows, section two describes the details of the data used in the Japanese WSD task. Section three describes the"
S10-1012,S01-1008,1,0.203162,"pear after the dictionary has been compiled. Therefore, some instances might have a sense that cannot be found in the dictionary’s set. The task will take into account not only the instances that have a sense in the given set but also the instances that have a sense that cannot be found in the set. In the latter case, systems should output that the instances have a sense that is not in the set. Introduction This paper reports an overview of the SemEval2 Japanese Word Sense Disambiguation (WSD) task. It can be considered an extension of the SENSEVAL-2 Japanese monolingual dictionarybased task (Shirai, 2001), so it is a lexical sample task. Word senses are defined according to the Iwanami Kokugo Jiten (Nishio et al., 1994), a Japanese dictionary published by Iwanami Shoten. It was distributed to participants as a sense inventory. Our task has the following two new characteristics: Training data, a corpus that consists of three genres (books, newspaper articles, and white papers) and is manually annotated with sense IDs, was also distributed to participants. For the evaluation, we distributed a corpus that consists of four genres (books, newspaper articles, white papers, and documents from a Q&A s"
S10-1012,C08-1003,0,\N,Missing
S10-1012,P06-1012,0,\N,Missing
senda-etal-2006-automatic,C04-1023,1,\N,Missing
W03-2007,W03-2003,1,0.710838,"tory, Tokyo Institute of Technology, and Hitachi, Ltd. iwayama@pi.titech.ac.jp of patent specification is where the claims are written, because “the claims specify the boundaries of the legal monopoly created by the patent” (Burgunder, 1995). Therefore, we believe that patent corpus processing should be centered around patent claim processing. It is common that Japanese patent claims are described in one sentence with peculiar style and wording and that they are difficult to read and understand for ordinary people. After surveying related literature and investigating NTCIR3 patent collection (Iwayama et al., 2003), we found the difficulty has two aspects: structural difficulty and term difficulty. In this paper, we first present the characteristics of patent claims. Next, we present our work on the structure analysis of patent claims. Third, we introduce our on-going research on term explanation for patent claims. 2 Characteristics of Patent Claim Typical Japanese patent claims taken from two patents are shown in Figure 1 and 2. In general, Japanese sentences are inserted with the touten “、” or “，” (comma) and end with the kuten “。” or “．” (period) . The touten plays a role of segmenting the sentence f"
W03-2007,J01-3004,0,\N,Missing
W05-0306,W03-1210,0,0.0854416,"ted corpus, we investigated the causal relation instances from some viewpoints. Our quantitative study shows that what amount of causal relation instances are present, where these relation instances are present, and which types of linguistic expressions are used for expressing these relation instances in text. 1 Introduction For many applications of natural language techniques such as question-answering systems and dialogue systems, acquiring knowledge about causal relations is one central issue. In recent researches, some automatic acquisition methods for causal knowledge have been proposed (Girju, 2003; Sato et al., 1999; Inui, 2004). They have used as knowledge resources a large amount of electric text documents: newspaper articles and Web documents. To realize their knowledge acquisition methods accurately and efficiently, it is important to knowing the characteristics of presence of in-text causal relations. However, while the acquisition methods have been improved by some researches, the characteristics of presence of in-text causal relations are still unclear: we have no empirical study about what amount of causal relation instances exist in text and where in text causal relation insta"
W05-0306,W04-2703,0,0.0206984,"Liu conducted no quantitative studies. Marcu (1997) investigated the frequency distribution of English connectives including “because” and “since” for implementation of rhetorical parsing. However, although Marcu’s study was quantitative one, Marcu treated only explicit linguistic expressions with connectives. In the Timebank corpus (Pustejovsky et al., 2003), the causal relation information is included. However, the information is optional for implicit linguistic expressions. Although both explicit expressions and implicit expressions are treated in the Penn Discourse Treebank (PDTB) corpus (Miltsakaki et al., 2004), no information on causal relations is contained in this corpus. Altenberg (1984) investigated the frequency distribution of causal relation instances from some viewpoints such as document style and the syntactic form in English dialog data. Nishizawa (1997) also conducted a similar work using Japanese dialog data. Some parts of their viewpoints are overlapping 38 with ours. However, while their studies focused on dialog data, our target is text documents. In fact, Altenberg treated also English text documents. However, our focus in this work is Japanese. 3 Annotated information 3.1 Causal re"
W05-0306,J05-1004,0,0.0655603,"Missing"
W06-0507,N03-1011,0,0.0346998,"Missing"
W06-0507,C92-2082,0,0.0689687,"d Yarowsky, 2005). New types of web content such as blogs and wikis, are also a 2 Related work Extracting information using Machine Learning algorithms has received much attention since the nineties, mainly motivated by the Message Understanding Conferences. From the midnineties, there are systems that learn extraction patterns from partially annotated and unannotated data (Huffman, 1995; Riloff, 1996; Riloff and Schmelzenbach, 1998; Soderland, 1999). Generalising textual patterns (both manually and automatically) for the identification of relations has been proposed since the early nineties (Hearst, 1992), and it has been applied to extending ontologies with hyperonymy and holonymy relations (Morin and Jacquemin, 1999; Kietz et al., 2000; Cimiano et al., 2004; Berland and Charniak, 1999). Finkelstein-Landau and Morin (1999) learn patterns for company merging relations with exceedingly good accuracies. Recently, kernel ∗ This work has been sponsored by MEC, project number TIN-2005-06885. 49 Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 49–56, c Sydney, July 2006. 2006 Association for Computational Linguistics 5. Download a separate corpus, called hook corpus, contai"
W06-0507,W03-0405,0,0.014351,"substring to produce the following patterns: methods are also becoming widely used for relation extraction (Bunescu and Mooney, 2005; Zhao and Grishman, 2005). Concerning rote extractors from the web, they have the advantage that the training corpora can be collected easily and automatically, so they are useful in discovering many different relations from text. Several similar approaches have been proposed (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002), with various applications: Question-Answering (Ravichandran and Hovy, 2002), multi-document Named Entity Coreference (Mann and Yarowsky, 2003), and generating biographical information (Mann and Yarowsky, 2005). Szpektor et al. (2004) applies a similar, with no seed lists, to extract automatically entailment relationships between verbs, and Etzioni et al. (2005) report very good results extracting Named Entities and relationships from the web. 2.1 Rote extractors <hook> was born in <target> <hook> ( <target> - 1870 ) Rote extractors (Mann and Yarowsky, 2005) estimate the probability of a relation r(p, q) given the surrounding context A1 pA2 qA3 . This is calculated, with a training corpus T , as the number of times that two related e"
W06-0507,P05-1060,0,0.049626,"web, a task known as Web Mining. In the particular case of web content mining, the aim is automatically mining data from textual web documents that can be represented with machine-readable semantic formalisms such as ontologies and semanticweb languages. Recently, there is an increasing interest in automatically extracting structured information from large corpora and, in particular, from the Web (Craven et al., 1999). Because of the characteristics of the web, it is necessary to develop efficient algorithms able to learn from unannotated data (Riloff and Schmelzenbach, 1998; Soderland, 1999; Mann and Yarowsky, 2005). New types of web content such as blogs and wikis, are also a 2 Related work Extracting information using Machine Learning algorithms has received much attention since the nineties, mainly motivated by the Message Understanding Conferences. From the midnineties, there are systems that learn extraction patterns from partially annotated and unannotated data (Huffman, 1995; Riloff, 1996; Riloff and Schmelzenbach, 1998; Soderland, 1999). Generalising textual patterns (both manually and automatically) for the identification of relations has been proposed since the early nineties (Hearst, 1992), an"
W06-0507,P99-1050,0,0.0266347,"ng information using Machine Learning algorithms has received much attention since the nineties, mainly motivated by the Message Understanding Conferences. From the midnineties, there are systems that learn extraction patterns from partially annotated and unannotated data (Huffman, 1995; Riloff, 1996; Riloff and Schmelzenbach, 1998; Soderland, 1999). Generalising textual patterns (both manually and automatically) for the identification of relations has been proposed since the early nineties (Hearst, 1992), and it has been applied to extending ontologies with hyperonymy and holonymy relations (Morin and Jacquemin, 1999; Kietz et al., 2000; Cimiano et al., 2004; Berland and Charniak, 1999). Finkelstein-Landau and Morin (1999) learn patterns for company merging relations with exceedingly good accuracies. Recently, kernel ∗ This work has been sponsored by MEC, project number TIN-2005-06885. 49 Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 49–56, c Sydney, July 2006. 2006 Association for Computational Linguistics 5. Download a separate corpus, called hook corpus, containing just the hook (in the example, Dickens). 6. Apply the previous patterns to the hook corpus, calculate the prec"
W06-0507,P02-1006,0,0.320182,"Large-scale Non-taxonomic Relation Extraction: Estimating the Precision of Rote Extractors∗ Enrique Alfonseca∗† Maria Ruiz-Casado∗† Manabu Okumura∗ Pablo Castells† ∗ † Precision and Intelligence Laboratory Computer Science Department Tokyo Institute of Techonology Universidad Autonoma de Madrid enrique@lr.pi.titech.ac.jp enrique.alfonseca@uam.es oku@pi.titech.ac.jp maria.ruiz@uam.es pablo.castells@uam.es Abstract source of textual information that contain an underlying structure from which specialist systems can benefit. Consequently, rote extractors (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) have been identified as an appropriate method to look for textual contexts that happen to convey a certain relation between two concepts. In this paper, we describe a new procedure for estimating the precision of the patterns learnt by a rote extractor, and how it compares to previous approaches. The solution proposed opens new possibilities for improving the precision of the generated patterns, as described below. This paper is structured as follows: Section 2 describe related work; Section 3 and 4 describe the proposed procedure and its evaluation, and Section 5 presents the conclusions and"
W06-0507,W98-1106,0,0.0218608,"ating patterns and implicit information from the web, a task known as Web Mining. In the particular case of web content mining, the aim is automatically mining data from textual web documents that can be represented with machine-readable semantic formalisms such as ontologies and semanticweb languages. Recently, there is an increasing interest in automatically extracting structured information from large corpora and, in particular, from the Web (Craven et al., 1999). Because of the characteristics of the web, it is necessary to develop efficient algorithms able to learn from unannotated data (Riloff and Schmelzenbach, 1998; Soderland, 1999; Mann and Yarowsky, 2005). New types of web content such as blogs and wikis, are also a 2 Related work Extracting information using Machine Learning algorithms has received much attention since the nineties, mainly motivated by the Message Understanding Conferences. From the midnineties, there are systems that learn extraction patterns from partially annotated and unannotated data (Huffman, 1995; Riloff, 1996; Riloff and Schmelzenbach, 1998; Soderland, 1999). Generalising textual patterns (both manually and automatically) for the identification of relations has been proposed"
W06-0507,P06-2002,1,0.907082,"of the relation, Entity, if the pattern can extract as hook or target only things of the same entity type as the words in the seed list (as annotated by the NERC module), or PoS, if the pattern can extract as hook or target any sequence of words whose sequence of PoS labels was seen in the training corpus; and (e) a sequence of queries that could be used to check, using the web, whether an extracted pair is correct or not. We assume that the system has used the seed list to extract and generalise a set of patterns for each of the relations using training corpora (Ravichandran and Hovy, 2002; Alfonseca et al., 2006a). Our procedure for calculating the patterns’ precisions is as follows: 1. For every relation, (a) For every hook, collect a hook corpus from the web. Suggested improvements Therefore, we propose the following three improvements to this procedure: 1. Collecting not only a hook corpus but also a target corpus should help in calculating the precision. In the example of the Chrysler building, we have seen that in most cases that we look for the pattern ‘s Chrysler building the previous words are New York, and so the pattern is considered accurate. However, if we look for the pattern New York’s,"
W06-0507,P99-1008,0,0.0863722,"tention since the nineties, mainly motivated by the Message Understanding Conferences. From the midnineties, there are systems that learn extraction patterns from partially annotated and unannotated data (Huffman, 1995; Riloff, 1996; Riloff and Schmelzenbach, 1998; Soderland, 1999). Generalising textual patterns (both manually and automatically) for the identification of relations has been proposed since the early nineties (Hearst, 1992), and it has been applied to extending ontologies with hyperonymy and holonymy relations (Morin and Jacquemin, 1999; Kietz et al., 2000; Cimiano et al., 2004; Berland and Charniak, 1999). Finkelstein-Landau and Morin (1999) learn patterns for company merging relations with exceedingly good accuracies. Recently, kernel ∗ This work has been sponsored by MEC, project number TIN-2005-06885. 49 Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 49–56, c Sydney, July 2006. 2006 Association for Computational Linguistics 5. Download a separate corpus, called hook corpus, containing just the hook (in the example, Dickens). 6. Apply the previous patterns to the hook corpus, calculate the precision of each pattern in the following way: the number of times it iden"
W06-0507,W04-3206,0,0.014708,"unescu and Mooney, 2005; Zhao and Grishman, 2005). Concerning rote extractors from the web, they have the advantage that the training corpora can be collected easily and automatically, so they are useful in discovering many different relations from text. Several similar approaches have been proposed (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002), with various applications: Question-Answering (Ravichandran and Hovy, 2002), multi-document Named Entity Coreference (Mann and Yarowsky, 2003), and generating biographical information (Mann and Yarowsky, 2005). Szpektor et al. (2004) applies a similar, with no seed lists, to extract automatically entailment relationships between verbs, and Etzioni et al. (2005) report very good results extracting Named Entities and relationships from the web. 2.1 Rote extractors <hook> was born in <target> <hook> ( <target> - 1870 ) Rote extractors (Mann and Yarowsky, 2005) estimate the probability of a relation r(p, q) given the surrounding context A1 pA2 qA3 . This is calculated, with a training corpus T , as the number of times that two related elements r(x, y) from T appear with that same context A1 xA2 yA3 , divided by the total numb"
W06-0507,H05-1091,0,0.0254858,"7. Repeat the procedure for other examples of the same relation. To illustrate this process, let us suppose that we want to learn patterns to identify birth years. We may start with the pair (Dickens, 1812). From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their longest common substring to produce the following patterns: methods are also becoming widely used for relation extraction (Bunescu and Mooney, 2005; Zhao and Grishman, 2005). Concerning rote extractors from the web, they have the advantage that the training corpora can be collected easily and automatically, so they are useful in discovering many different relations from text. Several similar approaches have been proposed (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002), with various applications: Question-Answering (Ravichandran and Hovy, 2002), multi-document Named Entity Coreference (Mann and Yarowsky, 2003), and generating biographical information (Mann and Yarowsky, 2005). Szpektor et al. (2004) applies a simila"
W06-0507,P05-1052,0,0.0142738,"or other examples of the same relation. To illustrate this process, let us suppose that we want to learn patterns to identify birth years. We may start with the pair (Dickens, 1812). From the downloaded corpus, we extract sentences such as Dickens was born in 1812 Dickens (1812 - 1870) was an English writer Dickens (1812 - 1870) wrote Oliver Twist The system identifies that the contexts of the last two sentences are very similar and chooses their longest common substring to produce the following patterns: methods are also becoming widely used for relation extraction (Bunescu and Mooney, 2005; Zhao and Grishman, 2005). Concerning rote extractors from the web, they have the advantage that the training corpora can be collected easily and automatically, so they are useful in discovering many different relations from text. Several similar approaches have been proposed (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002), with various applications: Question-Answering (Ravichandran and Hovy, 2002), multi-document Named Entity Coreference (Mann and Yarowsky, 2003), and generating biographical information (Mann and Yarowsky, 2005). Szpektor et al. (2004) applies a similar, with no seed lists, to"
W06-0507,alfonseca-etal-2006-wraetlic,1,\N,Missing
W11-1710,S07-1022,0,0.050743,"Missing"
W11-1710,baccianella-etal-2010-sentiwordnet,0,0.442447,"Missing"
W11-1710,banea-etal-2008-bootstrapping,0,0.0606121,"Missing"
W11-1710,esuli-sebastiani-2006-sentiwordnet,0,0.559548,"Missing"
W11-1710,P97-1023,0,0.0953214,"e rest of the paper is organized as follows. Different developmental phases of the Japanese WordNet Affect are described in Section 3. Prepa3 http://wordnet.princeton.edu/wordnet/download/ http://sentiwordnet.isti.cnr.it/ 5 http://mecab.sourceforge.net/ 6 http://translate.google.com/# 4 81 ration of the translated Japanese corpus, different experiments and evaluations based on morphology and the annotated emotion scores are elaborated in Section 4. Finally Section 5 concludes the paper. 2 Related Works The extraction and annotation of subjective terms started with machine learning approaches (Hatzivassiloglou and McKeown, 1997). Some well known sentiment lexicons have been developed, such as subjective adjective list (Baroni and Vegnaduzzo, 2004), English SentiWordNet (Esuli et. al., 2006), Taboada’s adjective list (Voll and Taboada, 2007), SubjectivityWord List (Banea et al., 2008) etc. Andreevskaia and Bergler (2006) present a method for extracting positive or negative sentiment bearing adjectives from WordNet using the Sentiment Tag Extraction Program (STEP). The proposed methods in (Wiebe and Riloff, 2006) automatically generate resources for subjectivity analysis for a new target language from the available res"
W11-1710,W10-0204,0,0.0222294,"y in the experiments on English as well as Japanese lexicons. But, it was also aimed for sentiment bearing words. Instead of English WordNet Affect (Strapparava and Valitutti, 2004), there are a few attempts in other languages such as, Russian and Romanian (Bobicev et al., 2010), Bengali (Das and Bandyopadhyay, 2010) etc. Our present approach is similar to some of these approaches but in contrast, we have evaluated our Japanese WordNet Affect on the SemEval 2007 affect sensing corpus translated into Japanese. In recent trends, the application of mechanical turk for generating emotion lexicon (Mohammad and Turney, 2010) shows promising results. In the present task, we have incorporated the open source, available and accessible resources to achieve our goals. 3 3.1 Developmental Phases WordNet Affect The English WordNet Affect, based on Ekman’s six emotion types is a small lexical resource compared to the complete WordNet but its affective annotation helps in emotion analysis. Some collection of WordNet Affect synsets was provided as a resource for the shared task of Affective Text in SemEval2007. The whole data is provided in six files named by the six emotions. Each file contains a list of synsets and one s"
W11-1710,strapparava-valitutti-2004-wordnet,0,0.671974,"Missing"
W11-1710,W09-3401,0,\N,Missing
W11-1710,S07-1013,0,\N,Missing
W11-1710,P05-1017,1,\N,Missing
W18-6510,W17-3541,0,0.0167717,"so play an important role in delivering a more interesting conversation. Recent studies addressed the response diversity and engagement issues and have attempted to generate responses better than the common and general ones. Some tackled this issue by defining and emphasizing context; previous utterances are commonly used as context in a conversation (Sordoni et al., 2015; Li et al., 2016a). Other studies have attempted to diversify or manipulate responses using specific attributes such as user identification (Li et al., 2016b), profile information sets (Zhang et al., 2018; Wang et al., 2017; Herzig et al., 2017), topics (Xing et al., 2017), and speciIntroduction Human-computer conversation is a challenging task in Natural Language Processing (NLP). The aim of conversation models is to generate fluent and relevant responses given an input in a free format, i.e., not just in the form of a question. A large amount of available data on the Internet has sparked the shift in conversation models. Starting with Ritter et al. (2011), completely datadriven models are now commonly used to generate responses. Furthermore, the sequence-tosequence (seq2seq) model initiated by Sutskever et al. (2014) has been adapt"
W18-6510,N16-1014,0,0.0620091,"Missing"
W18-6510,P16-1094,0,0.0349368,"Missing"
W18-6510,D16-1127,0,0.175075,"1 how are you ? good morning how are you i’m doing ok i’m good ! ! ! not really good i am excited ! are you sure ? ! come to the party ? yay ! ! ! are you gonna do it ? Table 1: Sample responses from our proposed model involving four different users. notably to machine translation (MT) and response generation. Actual conversations involving humans would be more engaging and the responses are not always general and monotonic. However, neural conversation models tend to generate safe, general, and uninteresting responses, e.g., I don’t know or I’m OK (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016b). We argue that, aside from adding or understanding the context of a conversation, speaking style and response diversity also play an important role in delivering a more interesting conversation. Recent studies addressed the response diversity and engagement issues and have attempted to generate responses better than the common and general ones. Some tackled this issue by defining and emphasizing context; previous utterances are commonly used as context in a conversation (Sordoni et al., 2015; Li et al., 2016a). Other studies have attempted to diversify or manipulate responses using specific"
W18-6510,D16-1230,0,0.0526098,"Missing"
W18-6510,D15-1166,0,0.532562,"e now commonly used to generate responses. Furthermore, the sequence-tosequence (seq2seq) model initiated by Sutskever et al. (2014) has been adapted to many NLP tasks, 89 Proceedings of The 11th International Natural Language Generation Conference, pages 89–98, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics et al., 2015) follow the encoder-decoder framework of Sutskever et al. (2014). For response generation, the encoder-decoder models are usually supplemented by the attention mechanism, following the implementation of Bahdanau et al. (2015) or Luong et al. (2015). fied mechanisms (Zhou et al., 2017). In this study, we focus on the issue of “response style.” We intend to let the model learn to generate responses that resemble those of a real person. Given an input utterance and user-specific information, the model will generate a response relevant to the input utterance based on the given userspecific information. The existing methods that exhibit the use of user-specific information (Li et al., 2016b; Zhang et al., 2018), usually require that the users appear in the training data. Therefore, these existing methods cannot handle the unseen users, i.e.,"
W18-6510,D17-1228,0,0.0256033,"Missing"
W18-6510,P02-1040,0,0.100678,"embeddings 93 6.3 cannot cover unseen users. Our model overcomes that issue by using user-info embeddings. The decoder input of both the models can be represented by Equation (6). Since the baseline model does not have user-info embeddings, our model’s atten˜ t is different from theirs. The attional hidden h tentional hidden of the baseline model would be ˜ t is the same as Equation (3), while our model’s h represented by Equation (7). Evaluation Setup Many previous studies on dialogue or response generation models (Li et al., 2016b,a; Sordoni et al., 2015; Xing et al., 2017) relied on BLEU (Papineni et al., 2002) as their automatic evaluation metric. To compute the score, BLEU measures the overlapping words or n-grams between the generated output (hypothesis) and the target output (reference). BLEU was initially intended for machine translation, which tends to have a finite target; therefore, it might not be suitable for evaluating conversation models. According to Liu et al. (2016), BLEU is lowly correlated with human judgments of dialogue systems. Additionally, some other work on response generation (Shang et al., 2015; Li et al., 2016c; Wang et al., 2017; Zhou et al., 2017) did not use BLEU for the"
W18-6510,P18-1205,0,0.12842,"peaking style and response diversity also play an important role in delivering a more interesting conversation. Recent studies addressed the response diversity and engagement issues and have attempted to generate responses better than the common and general ones. Some tackled this issue by defining and emphasizing context; previous utterances are commonly used as context in a conversation (Sordoni et al., 2015; Li et al., 2016a). Other studies have attempted to diversify or manipulate responses using specific attributes such as user identification (Li et al., 2016b), profile information sets (Zhang et al., 2018; Wang et al., 2017; Herzig et al., 2017), topics (Xing et al., 2017), and speciIntroduction Human-computer conversation is a challenging task in Natural Language Processing (NLP). The aim of conversation models is to generate fluent and relevant responses given an input in a free format, i.e., not just in the form of a question. A large amount of available data on the Internet has sparked the shift in conversation models. Starting with Ritter et al. (2011), completely datadriven models are now commonly used to generate responses. Furthermore, the sequence-tosequence (seq2seq) model initiated"
W18-6510,D14-1162,0,0.0835477,"Missing"
W18-6510,D11-1054,0,0.102165,"Missing"
W18-6510,P15-1152,0,0.0508841,"Missing"
W18-6510,N15-1020,0,0.0556654,"Missing"
W19-8641,P18-1015,0,0.106173,"Missing"
W19-8641,N18-1154,0,0.0117554,"s. In this example, ‘電動車’(Electric cars) and ‘全’(all) are represented by red letters and are not included in the 24character headline. These tokens cannot be evaluated by 24-character headlines. The blue tokens are not included in 9- and 13-character headlines. These tokens should not be included in shorter headlines. allowed because of limitations in the space where the headline appears. The technology of automatic headline generation has the potential to contribute greatly to this domain, and the problems of news headline generation have motivated a wide range of studies (Wang et al., 2018; Chen et al., 2018; Kiyono et al., 2018; Zhou et al., 2018; Cao et al., 2018; Wang et al., 2019). Table 1 shows sample headlines in three different lengths written by professional editors of a media company for the same news article: The length of the first headline for the digital media is restricted to 10 characters, the second to 13 characIntroduction The news media publish newspapers in print form and in electronic form. In the electric form, articles might be read on various types of devices using any application; thus, news media companies have an increasing need to produce multiple headlines for the same"
W19-8641,N16-1012,0,0.0569508,"racters long covered the content of those generated to be 13 characters long. These facts suggest that we should explore in further research a method not only trained by generic supervision data (print headlines) but also tuned for the desired length. 5 Related Work Rush et al. (2015) created the first approach to neural abstractive summarization. They generated a headline from the first sentence of a news article in the AEG (Napoles et al., 2012), which contains an enormous number of pairs of headlines and articles. After their study, a number of researchers addressed this task: For example, Chopra et al. (2016) used the encoderdecoder framework (Sutskever et al., 2014; Bahdanau et al., 2015) and Nallapati et al. (2016) incorporated additional features into the model, such as parts-of-speech tags and named entities. Suzuki and Nagata (2017) proposed word-frequency estimation to reduce the repeated phrases being generated. Zhou et al. (2017) proposed a gating mechanism (sGate) to ensure that important information is selected at each decoding step. Furthermore, attempts to control the output How Do Length Control Mechanisms Work? We wondered whether a method that could control the output length would p"
W19-8641,P19-1099,1,0.834743,"lengths could be controlled by embedding special tokens given to an input sequence. These two studies used DUC 2004 (Over et al., 2007), which comprises only 75byte summaries, to evaluate the outputs in multiple lengths. Liu et al. (2018) also proposed a method for controlling the number of output tokens in the ConvS2S model. In Transformer (Vaswani et al., 2017), Takase and Okazaki (2019) proposed two length control methods by extending positional embedding. They additionally evaluated the system outputs by using the reconstructed test set of AEG which consists of the fixed length headlines. Makino et al. (2019) proposed a global optimization method under a length constraint. Sun et al. (2019) examined how to compare summarizers by considering the length bias of generated summaries in the test set that includes various length summaries. However, no previous work built a dataset for evaluating headlines of multiple lengths or reported an in-depth perspective on this task during the process of new production in the real world. However, a single length reference that could appropriately evaluate multiple length summaries in multiple document summarization was reported (Shapira et al., 2018). In that stu"
W19-8641,K16-1028,0,0.470615,"th International Conference on Natural Language Generation, pages 333–343, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics ters, and the third to 26 characters. From a practical perspective, headlines must be generated under a rigid length constraint. The first study to consider the length of system outputs in the encoder-decoder framework was Rush et al. (2015). This study controlled the length of an output sequence by reducing the score of the end-of-sentence token to −∞ until the method generated the desired number of words. Subsequently, Kikuchi et al. (2016) and Fan et al. (2018) proposed mechanisms for length control; however, these studies produced summaries of 30, 50, and 75 bytes, and the studies evaluated the summaries by using the reference summaries of a single length (approximately 75 bytes long) in DUC 20041 . In addition, Takase and Okazaki (2019) proposed the mechanism for length control and evaluated their method with part of the test set which is consisted by summaries satisfying some length constraints in Annotated English Gigaword corpus (AEG) (Napoles et al., 2012). Thus, some questions can be posed: (1) Can previous evaluation se"
W19-8641,W12-3018,0,0.135595,"Missing"
W19-8641,P82-1020,0,0.722983,"Missing"
W19-8641,D16-1140,1,0.940941,"dings of The 12th International Conference on Natural Language Generation, pages 333–343, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics ters, and the third to 26 characters. From a practical perspective, headlines must be generated under a rigid length constraint. The first study to consider the length of system outputs in the encoder-decoder framework was Rush et al. (2015). This study controlled the length of an output sequence by reducing the score of the end-of-sentence token to −∞ until the method generated the desired number of words. Subsequently, Kikuchi et al. (2016) and Fan et al. (2018) proposed mechanisms for length control; however, these studies produced summaries of 30, 50, and 75 bytes, and the studies evaluated the summaries by using the reference summaries of a single length (approximately 75 bytes long) in DUC 20041 . In addition, Takase and Okazaki (2019) proposed the mechanism for length control and evaluated their method with part of the test set which is consisted by summaries satisfying some length constraints in Annotated English Gigaword corpus (AEG) (Napoles et al., 2012). Thus, some questions can be posed: (1) Can previous evaluation se"
W19-8641,N19-4009,0,0.0236053,"ormance differences between the methods. Although Transformer + SP-token remained the clear winner, the ranking in ROUGE scores of the other methods are flipped. We also computed rank correlation coefficients (Kendall’s τ ) to assess the discrepancy in the ranking among the methods presented in Table 7 and Table 8. The last row of Table 8 reveals that the rank correlation is not perfect (lower than one) but moderate. We understand that τ is maintained high to some extent because of two reasons: (1) Most of the Implementation We employed OpenNMT8 (Klein et al., 2017) for Seq2Seq, and fairseq9 (Ott et al., 2019) for ConvS2S and Transformer. We extended the implementations to realize LenEmb, LenInit, and LC. We set the dimensions for the token and length embeddings to 512, those for hidden states to 512, and the beam width to 5. These parameters are common in all the models. Table 6 summarizes other parameters specific to each sequenceto-sequence model. We used Nesterov’s accelerated gradient method (NAG) (Sutskever et al., 2013) with a momentum of 0.99 in ConvS2S. In Transformer, we set the number of attention heads to 8, the dimensions for the feed-forward network to 2,048, Adam’s β to 0.98, the war"
W19-8641,W18-5410,1,0.851959,"rence on Natural Language Generation, pages 333–343, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics ters, and the third to 26 characters. From a practical perspective, headlines must be generated under a rigid length constraint. The first study to consider the length of system outputs in the encoder-decoder framework was Rush et al. (2015). This study controlled the length of an output sequence by reducing the score of the end-of-sentence token to −∞ until the method generated the desired number of words. Subsequently, Kikuchi et al. (2016) and Fan et al. (2018) proposed mechanisms for length control; however, these studies produced summaries of 30, 50, and 75 bytes, and the studies evaluated the summaries by using the reference summaries of a single length (approximately 75 bytes long) in DUC 20041 . In addition, Takase and Okazaki (2019) proposed the mechanism for length control and evaluated their method with part of the test set which is consisted by summaries satisfying some length constraints in Annotated English Gigaword corpus (AEG) (Napoles et al., 2012). Thus, some questions can be posed: (1) Can previous evaluation settings adequately eval"
W19-8641,P17-4012,0,0.012013,"adlines. This evaluation setup reduced the performance differences between the methods. Although Transformer + SP-token remained the clear winner, the ranking in ROUGE scores of the other methods are flipped. We also computed rank correlation coefficients (Kendall’s τ ) to assess the discrepancy in the ranking among the methods presented in Table 7 and Table 8. The last row of Table 8 reveals that the rank correlation is not perfect (lower than one) but moderate. We understand that τ is maintained high to some extent because of two reasons: (1) Most of the Implementation We employed OpenNMT8 (Klein et al., 2017) for Seq2Seq, and fairseq9 (Ott et al., 2019) for ConvS2S and Transformer. We extended the implementations to realize LenEmb, LenInit, and LC. We set the dimensions for the token and length embeddings to 512, those for hidden states to 512, and the beam width to 5. These parameters are common in all the models. Table 6 summarizes other parameters specific to each sequenceto-sequence model. We used Nesterov’s accelerated gradient method (NAG) (Sutskever et al., 2013) with a momentum of 0.99 in ConvS2S. In Transformer, we set the number of attention heads to 8, the dimensions for the feed-forwar"
W19-8641,D15-1044,0,0.405379,"an array of devices. All devices and applications used for viewing articles have strict upper bounds regarding the number of characters ∗ This work was done at Retrieva, Inc. within Project. 333 Proceedings of The 12th International Conference on Natural Language Generation, pages 333–343, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics ters, and the third to 26 characters. From a practical perspective, headlines must be generated under a rigid length constraint. The first study to consider the length of system outputs in the encoder-decoder framework was Rush et al. (2015). This study controlled the length of an output sequence by reducing the score of the end-of-sentence token to −∞ until the method generated the desired number of words. Subsequently, Kikuchi et al. (2016) and Fan et al. (2018) proposed mechanisms for length control; however, these studies produced summaries of 30, 50, and 75 bytes, and the studies evaluated the summaries by using the reference summaries of a single length (approximately 75 bytes long) in DUC 20041 . In addition, Takase and Okazaki (2019) proposed the mechanism for length control and evaluated their method with part of the tes"
W19-8641,D18-2012,0,0.0327774,"Missing"
W19-8641,W04-3230,0,0.373641,"Missing"
W19-8641,D16-1248,0,0.0286382,"length. In the analysis, the existing methods could not take into account the word selection according to length constraint. We also found it difficult to evaluate methods to controlling output length, because headlines of different lengths are written based on different goals, and because the training data does not necessarily reflect the goal of the headlines of a specific length. In the future, we plan to explore an approach to adapt a model trained on print headlines to those which dedicated to a different length. length in neural abstractive summarization have been gradually increasing. Shi et al. (2016) reported that hidden states in recurrent neural networks in the encoder-decoder framework could implicitly model the length of the output sequences. Kikuchi et al. (2016) was the first to propose the idea of controlling the output length in the encoder-decoder framework. Their approach inserts length information for the output length into the decoder. Additionally, Fan et al. (2018) reported that output lengths could be controlled by embedding special tokens given to an input sequence. These two studies used DUC 2004 (Over et al., 2007), which comprises only 75byte summaries, to evaluate the"
W19-8641,W04-1013,0,0.0269053,"Missing"
W19-8641,W19-2303,0,0.0153687,"ese two studies used DUC 2004 (Over et al., 2007), which comprises only 75byte summaries, to evaluate the outputs in multiple lengths. Liu et al. (2018) also proposed a method for controlling the number of output tokens in the ConvS2S model. In Transformer (Vaswani et al., 2017), Takase and Okazaki (2019) proposed two length control methods by extending positional embedding. They additionally evaluated the system outputs by using the reconstructed test set of AEG which consists of the fixed length headlines. Makino et al. (2019) proposed a global optimization method under a length constraint. Sun et al. (2019) examined how to compare summarizers by considering the length bias of generated summaries in the test set that includes various length summaries. However, no previous work built a dataset for evaluating headlines of multiple lengths or reported an in-depth perspective on this task during the process of new production in the real world. However, a single length reference that could appropriately evaluate multiple length summaries in multiple document summarization was reported (Shapira et al., 2018). In that study, the authors confirmed the correlation coefficient of ROUGE scores between the s"
W19-8641,D18-1444,0,0.0233964,"in recurrent neural networks in the encoder-decoder framework could implicitly model the length of the output sequences. Kikuchi et al. (2016) was the first to propose the idea of controlling the output length in the encoder-decoder framework. Their approach inserts length information for the output length into the decoder. Additionally, Fan et al. (2018) reported that output lengths could be controlled by embedding special tokens given to an input sequence. These two studies used DUC 2004 (Over et al., 2007), which comprises only 75byte summaries, to evaluate the outputs in multiple lengths. Liu et al. (2018) also proposed a method for controlling the number of output tokens in the ConvS2S model. In Transformer (Vaswani et al., 2017), Takase and Okazaki (2019) proposed two length control methods by extending positional embedding. They additionally evaluated the system outputs by using the reconstructed test set of AEG which consists of the fixed length headlines. Makino et al. (2019) proposed a global optimization method under a length constraint. Sun et al. (2019) examined how to compare summarizers by considering the length bias of generated summaries in the test set that includes various length"
W19-8641,E17-2047,0,0.0123436,"or the desired length. 5 Related Work Rush et al. (2015) created the first approach to neural abstractive summarization. They generated a headline from the first sentence of a news article in the AEG (Napoles et al., 2012), which contains an enormous number of pairs of headlines and articles. After their study, a number of researchers addressed this task: For example, Chopra et al. (2016) used the encoderdecoder framework (Sutskever et al., 2014; Bahdanau et al., 2015) and Nallapati et al. (2016) incorporated additional features into the model, such as parts-of-speech tags and named entities. Suzuki and Nagata (2017) proposed word-frequency estimation to reduce the repeated phrases being generated. Zhou et al. (2017) proposed a gating mechanism (sGate) to ensure that important information is selected at each decoding step. Furthermore, attempts to control the output How Do Length Control Mechanisms Work? We wondered whether a method that could control the output length would produce similar headlines even for different lengths for the same news article. To confirm this suspicion, we reported ROUGE-1 recall scores in Figure 4 with three different configurations: (a) evaluating the first 13 characters of he"
W19-8641,N19-1401,1,0.906635,"t. The first study to consider the length of system outputs in the encoder-decoder framework was Rush et al. (2015). This study controlled the length of an output sequence by reducing the score of the end-of-sentence token to −∞ until the method generated the desired number of words. Subsequently, Kikuchi et al. (2016) and Fan et al. (2018) proposed mechanisms for length control; however, these studies produced summaries of 30, 50, and 75 bytes, and the studies evaluated the summaries by using the reference summaries of a single length (approximately 75 bytes long) in DUC 20041 . In addition, Takase and Okazaki (2019) proposed the mechanism for length control and evaluated their method with part of the test set which is consisted by summaries satisfying some length constraints in Annotated English Gigaword corpus (AEG) (Napoles et al., 2012). Thus, some questions can be posed: (1) Can previous evaluation settings adequately evaluate system outputs in headline generation task? (2) What type of problem should we solve in this task according to the target length? (3) How well do systems solve the problems? In this study, we present novel corpora to investigate these research questions. The contributions of th"
W19-8641,P19-1207,0,0.195072,"letters and are not included in the 24character headline. These tokens cannot be evaluated by 24-character headlines. The blue tokens are not included in 9- and 13-character headlines. These tokens should not be included in shorter headlines. allowed because of limitations in the space where the headline appears. The technology of automatic headline generation has the potential to contribute greatly to this domain, and the problems of news headline generation have motivated a wide range of studies (Wang et al., 2018; Chen et al., 2018; Kiyono et al., 2018; Zhou et al., 2018; Cao et al., 2018; Wang et al., 2019). Table 1 shows sample headlines in three different lengths written by professional editors of a media company for the same news article: The length of the first headline for the digital media is restricted to 10 characters, the second to 13 characIntroduction The news media publish newspapers in print form and in electronic form. In the electric form, articles might be read on various types of devices using any application; thus, news media companies have an increasing need to produce multiple headlines for the same news article based on what would be most appropriate and most compelling on a"
W19-8641,P17-1101,0,0.0581178,"mmarization. They generated a headline from the first sentence of a news article in the AEG (Napoles et al., 2012), which contains an enormous number of pairs of headlines and articles. After their study, a number of researchers addressed this task: For example, Chopra et al. (2016) used the encoderdecoder framework (Sutskever et al., 2014; Bahdanau et al., 2015) and Nallapati et al. (2016) incorporated additional features into the model, such as parts-of-speech tags and named entities. Suzuki and Nagata (2017) proposed word-frequency estimation to reduce the repeated phrases being generated. Zhou et al. (2017) proposed a gating mechanism (sGate) to ensure that important information is selected at each decoding step. Furthermore, attempts to control the output How Do Length Control Mechanisms Work? We wondered whether a method that could control the output length would produce similar headlines even for different lengths for the same news article. To confirm this suspicion, we reported ROUGE-1 recall scores in Figure 4 with three different configurations: (a) evaluating the first 13 characters of headlines generated to be 26 characters long on 13char-ref headlines (blue); (b) evaluating headlines ge"
W97-0106,P95-1038,0,0.0271846,"Missing"
W97-0106,H91-1060,0,0.0429897,"Missing"
W97-0106,H92-1026,0,0.0365738,"Missing"
W97-0106,H92-1030,0,0.0549738,"Missing"
W97-0106,P96-1025,0,0.0368283,"Missing"
W97-0106,H90-1021,0,0.0175993,"Missing"
W97-0106,E91-1004,0,0.0599211,"Missing"
W97-0106,P95-1037,0,0.0567278,"Missing"
W97-0106,1995.iwpt-1.22,0,0.0442288,"Missing"
W97-0106,P92-1017,0,0.047747,"Missing"
W97-0106,P93-1024,0,0.155119,"Missing"
W97-0106,E93-1040,0,0.0561511,"Missing"
W97-0106,W96-0114,1,0.874417,"Missing"
W97-0106,1991.iwpt-1.22,0,\N,Missing
W97-1511,A94-1012,0,0.229128,"c e d I n s t i t u t e of J a p a n A d v a n c e d I n s t i t u t e of J a p a n A d v a n c e d I n s t i t u t e of Science and Technology Science and Technology Science and Technology 1-1 A s a h i d a i T a t s u n o k u c h i 1-1 A s a h i d a i T a t s u n o k u c h i 1-1 A s a h i d a i T a t s u n o k u c h i N o m i I s h i ka w a J a p a n N o m i Ishi kaw a J a p a n Nomi Ishikawa J a p a n ping~j aist. ac. j p kawagut i©j aist. ac. j p ploited. In this framework, a whole grammar is not acquired from scratch(Mori and Nagao, 1995) or an initial grammar does not need to be assumed(Kiyono and Tsujii, 1994a). Instead, a rough but effective grammar is learned, in the first place, from a large corpus based on a corpus-based method and then later refined by the way of the combination of rulebased and corpus-based methods. We call the former step of the framework partial grammar acquisition and the latter grammar refinement. For the partial grammar acquisition, in our previous works, we have proposed a mechanism to acquire a partial grammar automatically from a bracketed corpus based on local contextual information(Theeramunkong and Okumura, 1996) and have shown the effectiveness of the derived gra"
W97-1511,C94-2134,0,0.189941,"c e d I n s t i t u t e of J a p a n A d v a n c e d I n s t i t u t e of J a p a n A d v a n c e d I n s t i t u t e of Science and Technology Science and Technology Science and Technology 1-1 A s a h i d a i T a t s u n o k u c h i 1-1 A s a h i d a i T a t s u n o k u c h i 1-1 A s a h i d a i T a t s u n o k u c h i N o m i I s h i ka w a J a p a n N o m i Ishi kaw a J a p a n Nomi Ishikawa J a p a n ping~j aist. ac. j p kawagut i©j aist. ac. j p ploited. In this framework, a whole grammar is not acquired from scratch(Mori and Nagao, 1995) or an initial grammar does not need to be assumed(Kiyono and Tsujii, 1994a). Instead, a rough but effective grammar is learned, in the first place, from a large corpus based on a corpus-based method and then later refined by the way of the combination of rulebased and corpus-based methods. We call the former step of the framework partial grammar acquisition and the latter grammar refinement. For the partial grammar acquisition, in our previous works, we have proposed a mechanism to acquire a partial grammar automatically from a bracketed corpus based on local contextual information(Theeramunkong and Okumura, 1996) and have shown the effectiveness of the derived gra"
W97-1511,E91-1004,0,0.0300239,"it does not exist in the current grammar. Thus we estimate its probability by using the formula (1) in section 4. Similar to most probabilistic models, there is a problem of low-frequency events in this model. Although some statistical NL applications apply backing-off estimation techniques to handle lowfrequency events, our model uses a simple interpolation estimation by adding a uniform probability to every events. Moreover, we make use of the geometric mean of the probability instead of the original probability in order to eliminate the effect of the number of rule applications as done in (Magerman and Marcus, 1991). The modified model is: P(T]S) = H Evaluation Some evaluation experiments and their results are described. For the experiments, we use texts from the EDR corpus, where bracketings are given. The subject is 48,100 sentences including around 510,000 words. Figure 3 shows some example sentences in the EDR corpus (rl, ,I,,ri)ET ( Experimental (~*p(rl""l""r')+(1-c~)*N-'~))ff'I (rll ,l, ,r~)ET (3) Here, o~ is a balancing weight between the observed distribution and the uniform distribution. It is assigned with 0.8 in our experiments. Nrl is the number of rules and Nc is the number of possible context"
W97-1511,P95-1037,0,0.0621731,"Missing"
W97-1511,1995.iwpt-1.22,0,0.107986,"Missing"
W97-1511,W96-0114,1,0.875519,"gao, 1995) or an initial grammar does not need to be assumed(Kiyono and Tsujii, 1994a). Instead, a rough but effective grammar is learned, in the first place, from a large corpus based on a corpus-based method and then later refined by the way of the combination of rulebased and corpus-based methods. We call the former step of the framework partial grammar acquisition and the latter grammar refinement. For the partial grammar acquisition, in our previous works, we have proposed a mechanism to acquire a partial grammar automatically from a bracketed corpus based on local contextual information(Theeramunkong and Okumura, 1996) and have shown the effectiveness of the derived grammar(Theeramunkong and Okumura, 1997). Through some preliminary experiments, we found out that it seems difficult to learn grammar rules which are seldom used in the corpus. This causes by the fact that rarely used rules occupy too few events for us to catch their properties. Therefore in the first step, only grammar rules with relatively high occurrence are first learned. In this paper, we focus on the second step, grammar refinement, where some new rules can be added to the current grammar in order to accept unparsable sentences. This task"
W97-1511,P96-1025,0,0.0292192,"Missing"
W97-1511,H92-1030,0,\N,Missing
W97-1511,1991.iwpt-1.22,0,\N,Missing
Y10-1049,J96-2004,0,0.0317846,"Missing"
Y10-1049,C04-1088,0,0.0358961,"bility of annotation using kappa coefficients and correlation coefficients. Keywords: text type, register, corpus, kappa coefficient 1 Introduction As the volume and the variations of texts compiled in language corpora are greatly increasing in recent years, providing an appropriate and informative classification code is becoming more and more important. Among many classification measures that have been proposed and employed so far, register classification, or text type annotation, has attracted a particular attention of many researchers (Biber, 1988; Tambouratzis et al., 2001; Portele, 2002; Gamon 2004; Sato, Matsuyoshi and Kondoh 2008; Biber and Conrad, 2009; among others). With appropriate classification codes, users will be able to extract on a particular set of the texts according to their objectives. For example, in many cases, usage of a word tends to depend strongly on the register and/or genre, and therefore, it is important for language researchers or learners to refer to register information of the text. In addition, appropriate register information is also crucial for creating automatic text classification systems as a source of baselines. Hence, we propose a new method of text t"
Y10-1049,C02-1081,0,0.0347268,"in terms of stability of annotation using kappa coefficients and correlation coefficients. Keywords: text type, register, corpus, kappa coefficient 1 Introduction As the volume and the variations of texts compiled in language corpora are greatly increasing in recent years, providing an appropriate and informative classification code is becoming more and more important. Among many classification measures that have been proposed and employed so far, register classification, or text type annotation, has attracted a particular attention of many researchers (Biber, 1988; Tambouratzis et al., 2001; Portele, 2002; Gamon 2004; Sato, Matsuyoshi and Kondoh 2008; Biber and Conrad, 2009; among others). With appropriate classification codes, users will be able to extract on a particular set of the texts according to their objectives. For example, in many cases, usage of a word tends to depend strongly on the register and/or genre, and therefore, it is important for language researchers or learners to refer to register information of the text. In addition, appropriate register information is also crucial for creating automatic text classification systems as a source of baselines. Hence, we propose a new meth"
Y10-1049,sato-etal-2008-automatic,0,0.0666943,"Missing"
Y10-1049,J08-4004,0,\N,Missing
Y10-1049,maekawa-etal-2010-design,1,\N,Missing
Y12-1008,E09-1006,0,0.0334745,"Missing"
Y12-1008,W06-1615,0,0.398803,"proposed an adaptive kernel approach that mapped the marginal distribution of source and target data into a common kernel space. They also conducted sample selection to make the conditional probabilities between the two domains closer. Raina et al. (2007) proposed self-taught learning that utilized sparse coding to construct higher level features from the unlabeled data collected from the Web. This method was based on unsupervised learning. Tur (2009) proposed a co-adaptation algorithm where both co-training and DA techniques were used to improve the performance of the model. The research by (Blitzer et al., 2006) involved work on semi-supervised DA, where they calculated the weight of words around the pivot features (words that frequently appeared both in source and target data and behaved similarly in both) to model some words in one domain that behaved similarly in another. They applied SVD to the matrix of the weights, generated a new feature space, and used the new features with the original features. McClosky et al. (2010) focused on the problem where the best model for each document is not obvious when parsing a document collection of heterogeneous domains. They studied it as a new task of multi"
Y12-1008,P06-1012,0,0.0985539,"data better than a classiﬁer developed only from the target data. A classiﬁer in a semi-supervised approach is developed from a large amount of labeled source data and unlabeled target data with the aim of classifying target data better than a classiﬁer developed only from the source data. Finally, a classiﬁer is developed from a large amount of labeled source data with the aim of classifying target data accurately in an unsupervised approach. We focused on the supervised DA for WSD in this paper. Many researchers have investigated DA within or outside the area of natural language processing. Chan and Ng (2006) carried out the DA of WSD by estimating class priors using an EM algorithm. Chan and Ng (2007) also conducted the DA of WSD by estimating class priors using the EM algorithm, but this was supervised DA using active learning. In addition, Daum´e III (2007) worked on the supervised DA. He augmented an input space and made triple length features that were general, source-speciﬁc, and target-speciﬁc. This was easy to implement, could be used with various DA methods, and could easily be extended to multi-DA problems. Daum´e III et al. (2010) extended the work in (Daum´e III, 2007) to semi-supervis"
Y12-1008,P07-1007,0,0.0270082,"sed approach is developed from a large amount of labeled source data and unlabeled target data with the aim of classifying target data better than a classiﬁer developed only from the source data. Finally, a classiﬁer is developed from a large amount of labeled source data with the aim of classifying target data accurately in an unsupervised approach. We focused on the supervised DA for WSD in this paper. Many researchers have investigated DA within or outside the area of natural language processing. Chan and Ng (2006) carried out the DA of WSD by estimating class priors using an EM algorithm. Chan and Ng (2007) also conducted the DA of WSD by estimating class priors using the EM algorithm, but this was supervised DA using active learning. In addition, Daum´e III (2007) worked on the supervised DA. He augmented an input space and made triple length features that were general, source-speciﬁc, and target-speciﬁc. This was easy to implement, could be used with various DA methods, and could easily be extended to multi-DA problems. Daum´e III et al. (2010) extended the work in (Daum´e III, 2007) to semi-supervised DA. It inherited the advantages of the supervised version and outperformed it by using unlab"
Y12-1008,W10-2608,0,0.0308719,"Missing"
Y12-1008,P07-1033,0,0.627487,"Missing"
Y12-1008,P07-1034,0,0.0889076,"0) extended the work in (Daum´e III, 2007) to semi-supervised DA. It inherited the advantages of the supervised version and outperformed it by using unlabeled target data. Agirre and de Lacalle (2008) worked on the semisupervised DA for WSD. They applied singular value decomposition (SVD) to a matrix of unlabeled target data and a large amount of unlabeled source data, and trained a classiﬁer with them. Agirre and de Lacalle (2009) worked on the supervised DA using almost the same method, but they used a small amount of labeled source data instead of the large amount of unlabeled source data. Jiang and Zhai (2007) demonstrated that performance increased as examples were weighted when DA was applied. This method could be used with various other supervised or semi-supervised DA methods. In addition, they tried to identify and remove source data that misled DA, but they concluded that it was only effective if examples were 81 not weighted. Zhong et al. (2009) proposed an adaptive kernel approach that mapped the marginal distribution of source and target data into a common kernel space. They also conducted sample selection to make the conditional probabilities between the two domains closer. Raina et al. ("
Y12-1008,I11-1124,1,0.141155,"tance. Harimoto et al. (2010) measured the distance between domains to conduct DA using a suitable corpus in parsing. In addition, van Asch and Daelemans (2010) reported that performance in DA could be predicted depending on the similarity between source and target data using automatically annotated corpus in parsing. They focused on how corpora were selected for use as source data according to the distance between domains, but here we have focused on how to select a method of DA depending on the degrees of conﬁdence of multiple classiﬁers. The closest work to this work is our previous work: (Komiya and Okumura, 2011) which determined an optimal DA method using decision tree learning given a triple of the target word type of WSD, source data, and target data. It discussed what features affected how the best method was determined. The main difference was that (Komiya and Okumura, 2011) determined the optimal DA method for each triple of the target word type of WSD, source data, and target data, but this paper determined the method for each instance. 3 Automatic determination of DA method for each instance We assumed that the optimal method would vary according to each instance. The DA method is automaticall"
Y12-1008,I08-7018,0,0.0299993,"eft) of the target word. POS and ﬁner subcategory of POS can be obtained using a morphological analyzer. We used ChaSen 1 as a morphological analyzer, the Bunruigoihyo thesaurus (National Institute for Japanese Language and Linguistics, 1964) for semantic classiﬁcation codes (e.g. The code of ‘program’ is 1.3162.), and CaboCha 2 as a syntactic parser. Five-fold cross validation was used in the experiments. 5 Data Three data which are the same as (Komiya and Okumura, 2011) were used for the experiments: (1) the sub-corpus of white papers in the Balanced Corpus of Contemporary Japanese (BCCWJ) (Maekawa, 2008), (2) the sub-corpus of documents from a Q&A site on the WWW in BCCWJ, and (3) Real World Computing (RWC) text databases (newspaper articles) (Hashida et al., 1998). DAs were conducted in six directions according to different source and target data. Word senses were annotated in these corpora according to a Japanese dictionary, i.e., the Iwanami Kokugo Jiten (Nishio et al., 1994). It has three levels for sense IDs, and we used the ﬁne-level 2 Min. 58 82 50 Table 1: Minimum, maximum, and average number of instances of each word type for each corpus • Syntactic feature 1 Genre BCCWJ white papers"
Y12-1008,N10-1004,0,0.025542,"unsupervised learning. Tur (2009) proposed a co-adaptation algorithm where both co-training and DA techniques were used to improve the performance of the model. The research by (Blitzer et al., 2006) involved work on semi-supervised DA, where they calculated the weight of words around the pivot features (words that frequently appeared both in source and target data and behaved similarly in both) to model some words in one domain that behaved similarly in another. They applied SVD to the matrix of the weights, generated a new feature space, and used the new features with the original features. McClosky et al. (2010) focused on the problem where the best model for each document is not obvious when parsing a document collection of heterogeneous domains. They studied it as a new task of multiple source parser adaptation. They proposed a method of parsing a sentence that ﬁrst predicts accuracies for various parsing models using a regression model, and then uses the parsing model with the highest predicted accuracy. The main difference is that their work was about parsing but ours discussed here is about Japanese WSD. They also assumed that they had labeled corpora in heterogeneous domains but we have not. We"
Y12-1008,W10-2605,0,0.143735,"Missing"
Y12-1008,C08-1003,0,\N,Missing
