2020.louhi-1.12,araki-etal-2014-detecting,0,0.0186208,"cal, non-identical, or mereologically (part-whole) related. In keeping with our primary goal of enabling timeline extraction, we implemented CON-SUB as a TLINK since it conveys true temporal containment. CON-SUB, however, differs from other TLINKs, which were constrained by proximity and lexical cues, as discussed in section 2. The fact that CON-SUB also represents structural information allowed us to treat it like a coreference/bridging relation in terms of permissible textual evidence for link creation: namely, semantic scripts. These may be defined as: “a stereotypical sequence of events” (Araki et al., 2014) or “prototypical schematic sequences of events” (Chambers and Jurafsky, 2008). We can expect a surgery, for example, to consist of certain, typical subevents (incisions, subprocedures, anesthesia administration, etc.), which therefore enables annotators to look throughout the whole document for lexical items with meanings that fit those subevents. The concept of semantic scripts is what facilitates attainable long-distance coreference /bridging linking, and therefore, long-distance CON-SUB linking. This is obviously not the case for nonsubevent CONTAINS relations. 4.2 Inter-annotator Agreemen"
2020.louhi-1.12,S17-2093,1,0.860183,"3 School of Information, University of Arizona, Tucson, AZ 4 Department of Computer Science, Loyola University Chicago, Chicago, IL 1 {first.last}@childrens.harvard.edu 2 {first.last}@colorado.edu 3 bethard@arizona.edu 4 ddligach@luc.edu 017 018 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 leading US medical center. This dataset has previously undergone a variety of annotation efforts, most notably temporal annotation (Styler IV et al., 2014). It has been part of several SemEval shared tasks such as Clinical TempEval (Bethard et al., 2017) where state-of-the-art results have been established. Our goal was to utilize this THYME corpus to enable the extraction of more extensive patient timelines by manually creating cross-document links that built off the pre-existing single file annotations. (Wright-Bettner et al., 2019) discuss that a subset of the THYME temporal annotations contributed to incompatible temporal inferences, thus reducing their ability to support meaningful temporal reasoning. Accuracy and informativeness of temporal relation gold annotations are essential for their effectiveness in training a system for temporal"
2020.louhi-1.12,P08-1090,0,0.0789589,"g with our primary goal of enabling timeline extraction, we implemented CON-SUB as a TLINK since it conveys true temporal containment. CON-SUB, however, differs from other TLINKs, which were constrained by proximity and lexical cues, as discussed in section 2. The fact that CON-SUB also represents structural information allowed us to treat it like a coreference/bridging relation in terms of permissible textual evidence for link creation: namely, semantic scripts. These may be defined as: “a stereotypical sequence of events” (Araki et al., 2014) or “prototypical schematic sequences of events” (Chambers and Jurafsky, 2008). We can expect a surgery, for example, to consist of certain, typical subevents (incisions, subprocedures, anesthesia administration, etc.), which therefore enables annotators to look throughout the whole document for lexical items with meanings that fit those subevents. The concept of semantic scripts is what facilitates attainable long-distance coreference /bridging linking, and therefore, long-distance CON-SUB linking. This is obviously not the case for nonsubevent CONTAINS relations. 4.2 Inter-annotator Agreement While the gold intra-document CON-SUB relations enabled high cross-document"
2020.louhi-1.12,N19-1423,0,0.0715314,"(different narratives have different goals, which in turn influences meaning interpretation). This is discussed in detail in Section 4. We empirically found it essential to take changes in discourse context into account and suggest the same would be true for any annotation project that is interested in temporal reasoning, particularly those dealing with longer timelines (i.e., beyond the single-document level). Recent developments in natural language processing establish neural approaches and more specifically transformer-based methods as the state of the art. Pre-trained models such as BERT (Devlin et al., 2019), BioBERT (Lee et al., 2020), Xlnet (Yang et al., 2019), ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019), BART (Lewis et al., 2019), and SpanBERT (Joshi et al., 2020) report significant gains on multiple tasks. Thus, we demonstrate the learnability of the refined temporal relations in the context of these recent methodological developments. 2 prepositions and adjectives (e.g., during, subsequent to, prior to), chronological narrative progression, and so forth. Additionally, the notes had been separately annotated for intra-document coreference (IDENTICAL) and bridging (SET-SUBSET, WHOLEP"
2020.louhi-1.12,W13-1203,0,0.191253,"r. ii. cancer CONTAINS tumor iii. cancer CONTAINS adenocarcinoma The merged THYME and coreference annotations2 for (1) and (2) were as follows:      January 17, 2009 CONTAINS CT CT CONTAINS metastases February 20, 2009 CONTAINS resected metastases OVERLAPS resected metastases IDENTICAL metastases Both sets of intra-document links are pragmatically appropriate. Discourse contexts can expand or reduce the level of granularity at which a sense is interpreted (Recasens et al., 2011; also see Hobbs, 1985). In Note A, the text supports a coarse-grained interpretation of adenocarcinoma, or what Hovy et al., 2013 term a “wide” reading; it refers generally to the patient’s cancer. Note B, however, requires a fine-grained (“narrow”) interpretation – adenocarcinoma here refers specifically to the new, inoperable tumor and is contrasted with the original, resected tumor. The quandary for the cross-document task lies in whether to link adenocarcinoma in A as IDENTICAL to adenocarcinoma in B. An IDENTICAL relation entails logical impossibilities: assuming we also link cancerA as IDENT to cancerB, the combined within- and crossdocument relations now say the recurrent adenocarcinoma temporally contains itself"
2020.louhi-1.12,2020.tacl-1.5,0,0.153352,"take changes in discourse context into account and suggest the same would be true for any annotation project that is interested in temporal reasoning, particularly those dealing with longer timelines (i.e., beyond the single-document level). Recent developments in natural language processing establish neural approaches and more specifically transformer-based methods as the state of the art. Pre-trained models such as BERT (Devlin et al., 2019), BioBERT (Lee et al., 2020), Xlnet (Yang et al., 2019), ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019), BART (Lewis et al., 2019), and SpanBERT (Joshi et al., 2020) report significant gains on multiple tasks. Thus, we demonstrate the learnability of the refined temporal relations in the context of these recent methodological developments. 2 prepositions and adjectives (e.g., during, subsequent to, prior to), chronological narrative progression, and so forth. Additionally, the notes had been separately annotated for intra-document coreference (IDENTICAL) and bridging (SET-SUBSET, WHOLEPART) relations, which were later merged with the temporal annotations (Wright-Bettner et al., 2019). Temporal relations alone are insufficient for timeline extraction; core"
2020.louhi-1.12,P14-1094,0,0.029063,"these pre-existing within-note annotations by manually adding coreference and bridging links across each set of three notes. In the process, we discovered a subset of the original CONTAINS relations contributed to temporally-conflicting information, which led to the addition of two new TLINKs: NOTED-ON and CON-SUB (Wright-Bettner et al., 2019). We discuss below how these updates contribute to more accurate and comprehensive temporal relations which facilitated cross-document linking. As such, this is one of the few studies in clinical NLP for cross-document temporal relation annotations (see Raghavan et al., 2014 and Wright-Bettner et al., 2019; also see Song et al., 2018 for general domain cross-document temporal annotation discussions). Dataset The 594 notes that make up the colon cancer part of the THYME corpus are grouped into sets, each set pertaining to a single patient and consisting of three notes written at different times during the patient’s course of care. These notes had been previously annotated for five different intradocument temporal relations (BEFORE, OVERLAP, BEGINS-ON, ENDS-ON and CONTAINS), a subset of the ISO-TimeML temporal link (TLINK) types (Pustejovsky et al., 2010, Styler IV"
2020.louhi-1.12,L18-1558,0,0.035104,"Missing"
2020.louhi-1.12,W19-1908,1,0.922329,"e refinements of the THYME+ annotations. Splitting CONTAINS into CONTAINS and CON-SUB relations and OVERLAP into OVERLAP and NOTEDON relations leads to better learnability: CONTAINS goes from 0.664 F1 on THYME to 0.748 F1 on THYME+, and OVERLAP goes from 0.179 on THYME to 0.416 on THYME+. The best results for the new categories of CON-SUB and NOTED-ON are 0.072 F1 and 0.744 F1 respectively – results that establish baselines for these two new temporal relations. The performance on all types of relations for THYME+ is 0.625 F1 compared to 0.548 for THYME (Table 2, Overall column, rows 1 and 2). Lin et al., 2019 report 0.684 F1 for THYME CONTAINS, however the result is achieved when training on and evaluating for only the CONTAINS links, and augmenting the training data with automatically generated CONTAINS relations. Thus, it is not a fair comparison to use for the results reported in Table 2. Of the models beyond BioBERT that we explored, BART-large was the most successful. The result with BART-large was 0.748 F1 (Table 2, CONTAINS column, row 3). In general, certain pre-trained models, like BioBERT and BART, yield better results than the other models. BioBERT is pre-trained on biomedical text and"
2020.louhi-1.12,W16-5706,1,0.878583,"Missing"
2020.louhi-1.12,pustejovsky-etal-2010-iso,0,0.037864,"otations (see Raghavan et al., 2014 and Wright-Bettner et al., 2019; also see Song et al., 2018 for general domain cross-document temporal annotation discussions). Dataset The 594 notes that make up the colon cancer part of the THYME corpus are grouped into sets, each set pertaining to a single patient and consisting of three notes written at different times during the patient’s course of care. These notes had been previously annotated for five different intradocument temporal relations (BEFORE, OVERLAP, BEGINS-ON, ENDS-ON and CONTAINS), a subset of the ISO-TimeML temporal link (TLINK) types (Pustejovsky et al., 2010, Styler IV et al., 2014)1. To keep annotation manageable and circumvent massively inferential temporal linking, the THYME guidelines constrained TLINK creation to events within the same sentence or adjacent sentences, and specifically prohibited TLINKing across sections (these are clinically-delineated sections separated from each other by numerical section IDs – History of Present Illness is section 20103, Vital Signs is section 20110, etc.) Linguistic evidence for creating these TLINKs included local cues such as temporal 3 Refined Temporal Relation: NOTEDON The THYME guidelines specified t"
2020.louhi-1.12,D19-6201,1,0.888309,"Missing"
A97-1025,W95-0104,0,0.62845,"this work focused on detecting misspelled words, not contextual spelling errors. The approach taken used letter n-grams to build the semantic space. In this work, we use the words directly. Yarowsky (1994) notes that conceptual spelling correction is part of a closely related class of problems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration. This class of problems has been attacked by many others. A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992). Recently, Golding and Schabes (1996) described a system, Tribayes, that combines a trigram model of the words&apos; parts of speech with a Bayesian classifier. The trigram component of the system is used to make decisions for those confusion sets that documents ~ DO terms TO X rxr txd rxd txr Figure 1: Singular value decomposition (SVD) of matrix X produces matrices T, S and D&apos;. contain words with different parts of speech. The Bayesian component is used to predict t h e correct word from among same part-of-speech word"
A97-1025,P96-1010,0,0.830591,"letter n-grams to build the semantic space. In this work, we use the words directly. Yarowsky (1994) notes that conceptual spelling correction is part of a closely related class of problems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration. This class of problems has been attacked by many others. A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992). Recently, Golding and Schabes (1996) described a system, Tribayes, that combines a trigram model of the words&apos; parts of speech with a Bayesian classifier. The trigram component of the system is used to make decisions for those confusion sets that documents ~ DO terms TO X rxr txd rxd txr Figure 1: Singular value decomposition (SVD) of matrix X produces matrices T, S and D&apos;. contain words with different parts of speech. The Bayesian component is used to predict t h e correct word from among same part-of-speech words. Golding and Schabes selected 18 confusion sets from a list of commonly confused words plus a few that represent ty"
A97-1025,J92-1001,0,0.0106006,"The approach taken used letter n-grams to build the semantic space. In this work, we use the words directly. Yarowsky (1994) notes that conceptual spelling correction is part of a closely related class of problems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration. This class of problems has been attacked by many others. A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992). Recently, Golding and Schabes (1996) described a system, Tribayes, that combines a trigram model of the words&apos; parts of speech with a Bayesian classifier. The trigram component of the system is used to make decisions for those confusion sets that documents ~ DO terms TO X rxr txd rxd txr Figure 1: Singular value decomposition (SVD) of matrix X produces matrices T, S and D&apos;. contain words with different parts of speech. The Bayesian component is used to predict t h e correct word from among same part-of-speech words. Golding and Schabes selected 18 confusion sets from a list of commonly confu"
A97-1025,P94-1013,0,0.0303629,"from a confusion set to a given context. The experimental results from LSA prediction are then compared to both a baseline predictor and a hybrid predictor based on trigrams and a Bayesian classifier. 1Homophones are words that sound the same, but are spelled differently. 166 2 Related Work Latent Semantic Analysis has been applied to the problem of spelling correction previously (Kukich, 1992b). However, this work focused on detecting misspelled words, not contextual spelling errors. The approach taken used letter n-grams to build the semantic space. In this work, we use the words directly. Yarowsky (1994) notes that conceptual spelling correction is part of a closely related class of problems which include word sense disambiguation, word choice selection in machine translation, and accent and capitalization restoration. This class of problems has been attacked by many others. A number of feature-based methods have been tried, including Bayesian classifiers (Gale, Church, and Yarowsky, 1992; Golding, 1995), decision lists (Yarowsky, 1994), and knowledge-based approaches (McRoy, 1992). Recently, Golding and Schabes (1996) described a system, Tribayes, that combines a trigram model of the words&apos;"
bethard-etal-2008-building,kingsbury-palmer-2002-treebank,0,\N,Missing
bethard-etal-2008-building,J93-2004,0,\N,Missing
bethard-etal-2008-building,W04-2703,0,\N,Missing
bethard-etal-2008-building,S07-1014,0,\N,Missing
bethard-etal-2008-building,W06-1618,1,\N,Missing
bethard-etal-2008-building,S07-1025,1,\N,Missing
bethard-etal-2008-building,S07-1085,0,\N,Missing
bethard-etal-2008-building,S07-1108,0,\N,Missing
bethard-etal-2008-building,P06-1095,0,\N,Missing
bethard-etal-2008-building,W03-1210,0,\N,Missing
bethard-etal-2008-building,P00-1043,0,\N,Missing
bethard-etal-2008-building,S07-1003,0,\N,Missing
C08-1023,W04-1013,0,0.0404396,"Missing"
C08-1023,N03-1020,0,0.165926,"Missing"
C08-1023,C02-1144,0,0.0316345,"Missing"
C08-1023,W01-0100,0,0.33468,"ce domain. The algorithms pursue a hybrid approach integrating both domain independent bottom-up sentence scoring features and domain-aware top-down features. Evaluation results indicate that COGENT outperforms existing summarizers and generates summaries that closely resemble those generated by human experts. COGENT concept inventories appear to also support the computational identification of student misconceptions about earthquakes and plate tectonics. 1 Introduction Multidocument summarization (MDS) research efforts have resulted in significant advancements in algorithm and system design (Mani, 2001). Many of these efforts have focused on summarizing news articles, but not significantly explored the research issues arising from processing educational content to support pedagogical applications. This paper describes our research into the application of MDS techniques to educational © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/). Some rights reserved. science content to generate pedagogically useful summaries. Knowledge maps are graphical representations of domain information laid out"
C08-1023,J04-2002,0,0.0864179,"n platform (Radev et al., 2000). MEAD research efforts have resulted in significant contributions to support the development of summarization applications (Radev et al., 2000). While all these systems have produced promising results in automated evaluations, none have directly targeted educational content as input or the generation of pedagogically useful summaries. We are directly building upon MEAD due its focus on sentence extraction and its high degree of modularization. 3 2 Related Work Our work is informed by efforts to automate the acquisition of ontology concepts from text. OntoLearn (Navigli and Velardi, 2004) extracts domain terminology from a collection of texts using a syntactic parse to identify candidate terms that are filtered based on domain relevance and connected using a semantic interpretation based on word sense disambiguation. The newly identified concepts and relationships are used to update an existing ontology. Knowledge Puzzle focuses on n-grams to produce candidate terms filtered based on term frequency in the input documents and on the number of relationships associated with a given term (Zouaq et al., 2007). This approach leverages pattern extraction techniques to identify concep"
C08-1023,W00-0403,0,0.137925,"Missing"
C08-1023,N06-2046,0,0.0355155,"Missing"
corvey-etal-2012-foundations,N06-4006,0,\N,Missing
corvey-etal-2012-foundations,P11-2121,1,\N,Missing
D07-1020,P98-1012,0,0.933299,"s. Without some means of separating documents that contain mentions of distinct entities, most of these applications will produce incorrect results. The work presented here, therefore, addresses the problem of automatically problem of automatically separating sets of news documents generated by queries containing personal names into coherent partitions. The approach we present here combines unsupervised clustering methods with robust syntactic and semantic processing to automatically cluster returned news documents (and thereby entities) into homogeneous sets. This work follows on the work of Bagga & Baldwin (1998), Mann & Yarowsky (2003), Niu et al. (2004), Li et al. (2004), Pedersen et al. (2005), and Malin (2005). The results described here advance this work through the use of syntactic and semantic features that can be robustly extracted from the kind of arbitrary news texts typically returned from opendomain sources. The specific contributions reported here fall into two general areas related to robustness. In the first, we explore the use of features extracted from syntactic and semantic processing at a level that is robust to changes in genre and language. In particular, we seek to go beyond the"
D07-1020,N04-1002,0,0.866477,"nd Malin (2005). The results described here advance this work through the use of syntactic and semantic features that can be robustly extracted from the kind of arbitrary news texts typically returned from opendomain sources. The specific contributions reported here fall into two general areas related to robustness. In the first, we explore the use of features extracted from syntactic and semantic processing at a level that is robust to changes in genre and language. In particular, we seek to go beyond the kind of bag of local words features employed in earlier systems (Bagga & Baldwin, 1998; Gooi & Allan, 2004; Pedersen et al., 2005) that did not attempt to exploit deep semantic features that are difficult to extract, and to go beyond the kind of biographical information (Mann & Yarowsky, 2003) that is unlikely to occur with great frequency (such as place of birth, or family relationships) in many of the documents returned by typical search engines. The second contribution involves the application of these techniques to both English and Chinese news collections. As we’ll see, the methods are effective with both, but error analyses reveal interesting differences between the two languages. 190 Procee"
D07-1020,N04-1003,0,0.826057,"of distinct entities, most of these applications will produce incorrect results. The work presented here, therefore, addresses the problem of automatically problem of automatically separating sets of news documents generated by queries containing personal names into coherent partitions. The approach we present here combines unsupervised clustering methods with robust syntactic and semantic processing to automatically cluster returned news documents (and thereby entities) into homogeneous sets. This work follows on the work of Bagga & Baldwin (1998), Mann & Yarowsky (2003), Niu et al. (2004), Li et al. (2004), Pedersen et al. (2005), and Malin (2005). The results described here advance this work through the use of syntactic and semantic features that can be robustly extracted from the kind of arbitrary news texts typically returned from opendomain sources. The specific contributions reported here fall into two general areas related to robustness. In the first, we explore the use of features extracted from syntactic and semantic processing at a level that is robust to changes in genre and language. In particular, we seek to go beyond the kind of bag of local words features employed in earlier syste"
D07-1020,W03-0405,0,0.870813,"separating documents that contain mentions of distinct entities, most of these applications will produce incorrect results. The work presented here, therefore, addresses the problem of automatically problem of automatically separating sets of news documents generated by queries containing personal names into coherent partitions. The approach we present here combines unsupervised clustering methods with robust syntactic and semantic processing to automatically cluster returned news documents (and thereby entities) into homogeneous sets. This work follows on the work of Bagga & Baldwin (1998), Mann & Yarowsky (2003), Niu et al. (2004), Li et al. (2004), Pedersen et al. (2005), and Malin (2005). The results described here advance this work through the use of syntactic and semantic features that can be robustly extracted from the kind of arbitrary news texts typically returned from opendomain sources. The specific contributions reported here fall into two general areas related to robustness. In the first, we explore the use of features extracted from syntactic and semantic processing at a level that is robust to changes in genre and language. In particular, we seek to go beyond the kind of bag of local wor"
D07-1020,P04-1076,0,0.82391,"at contain mentions of distinct entities, most of these applications will produce incorrect results. The work presented here, therefore, addresses the problem of automatically problem of automatically separating sets of news documents generated by queries containing personal names into coherent partitions. The approach we present here combines unsupervised clustering methods with robust syntactic and semantic processing to automatically cluster returned news documents (and thereby entities) into homogeneous sets. This work follows on the work of Bagga & Baldwin (1998), Mann & Yarowsky (2003), Niu et al. (2004), Li et al. (2004), Pedersen et al. (2005), and Malin (2005). The results described here advance this work through the use of syntactic and semantic features that can be robustly extracted from the kind of arbitrary news texts typically returned from opendomain sources. The specific contributions reported here fall into two general areas related to robustness. In the first, we explore the use of features extracted from syntactic and semantic processing at a level that is robust to changes in genre and language. In particular, we seek to go beyond the kind of bag of local words features employe"
D07-1020,N07-1016,0,0.018001,"Missing"
D07-1020,C98-1012,0,\N,Missing
D07-1020,H05-1048,1,\N,Missing
J08-2006,W04-2412,0,0.104375,"Missing"
J08-2006,W05-0620,0,0.488033,"Missing"
J08-2006,P05-1022,0,0.154686,"Missing"
J08-2006,J02-3001,0,0.954218,"labeled with a preﬁx “R-” appended. We follow the standard convention of using Section 02 to Section 21 as the training set, Section 00 as the development set, and Section 23 as the test set. The training set comprises about 90,000 predicates instantiating about 250,000 arguments and the test set comprises about 5,000 predicates instantiating about 12,000 arguments. 3. Task Description In ASSERT, the task of semantic role labeling is implemented by assigning role labels to constituents of a syntactic parse. Parts of the overall process can be analyzed as three different tasks as introduced by Gildea and Jurafsky (2002): 1. Argument Identiﬁcation—This is the process of identifying parsed constituents in the sentence that represent semantic arguments of Figure 1 Syntax tree for a sentence illustrating the PropBank tags. 291 Computational Linguistics Volume 34, Number 2 Figure 2 Syntax tree for a sentence illustrating the PropBank arguments. a given predicate. Each node in a parse tree can be classiﬁed (with respect to a given predicate) as either one that represents a semantic argument (i.e., a NON -N ULL node) or one that does not represent any semantic argument (i.e., a N ULL node). 2. Argument Classiﬁcatio"
J08-2006,W04-2416,1,0.599093,"feature. 4.2.8 Head Word. Syntactic head of the constituent. 4.2.9 Head Word POS. Part of speech of the head word. 4.2.10 Named Entities in Constituents. Binary features for seven named entities (P ERSON , O RGANIZATION , L OCATION , P ERCENT, M ONEY, T IME , D ATE) tagged by IdentiFinder (Bikel, Schwartz, and Weischedel 1999). 4.2.11 Path Generalizations. 1. Partial Path—Path from the constituent to the lowest common ancestor of the predicate and the constituent. 2. Clause-based path variations—Position of the clause node (S, SBAR) seems to be an important feature in argument identiﬁcation (Hacioglu et al. 2004). Therefore we experimented with four clause-based path feature variations. (a) 294 Replacing all the nodes in a path other than clause nodes with an asterisk. For example, the path NP↑S↑VP↑SBAR↑NP↑VP↓VBD becomes NP↑S↑*S↑*↑*↓VBD. Pradhan, Ward, and Martin (b) (c) (d) Towards Robust Semantic Role Labeling Retaining only the clause nodes in the path, which for the given example would produce NP↑S↑S↓VBD. Adding a binary feature that indicates whether the constituent is in the same clause as the predicate. Collapsing the nodes between S nodes, which gives NP↑S↑NP↑VP↓VBD. 3. Path n-grams—This featu"
J08-2006,W05-0625,0,0.0245547,"rg. ∗∗ The Center for Spoken Language Research, Campus Box 594, Boulder, CO 80309. E-mail: whw@colorado.edu. † The Center for Spoken Language Research, Campus Box 594, Boulder, CO 80309. E-mail: martin@colorado.edu. Submission received: 15 July 2006; revised submission received: 3 May 2007; accepted for publication: 19 June 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 2 been possible to achieve accuracies within the range of human inter-annotator agreement. More recent approaches have involved using improved features such as n-best parses (Koomen et al. 2005; Toutanova, Haghighi, and Manning 2005); exploiting argument interdependence (Jiang, Li, and Ng 2005); using information from fundamentally different, and complementary syntactic, views (Pradhan, Ward et al. 2005); combining hypotheses from different labeling systems using inference (M`arquez et al. 2005); as well as applying novel learning paradigms (Punyakanok et al. 2005; Toutanova, Haghighi, and Manning 2005; Moschitti 2006) that try to capture more sequence and contextual information. Some have also tried to jointly decode the syntactic and semantic structures (Yi and Palmer 2005; Musill"
J08-2006,W00-0730,0,0.040706,"assignment. The basic inputs are a sentence and a syntactic parse of the sentence. For each constituent in the parse tree, the system extracts a set of features and uses a classiﬁer to assign a label to the constituent. The set of labels used are the PropBank argument labels plus NULL, which means no argument is assigned to that constituent for the predicate under consideration. Support vector machines (SVMs) (Burges 1998; Vapnik 1998) have been shown to perform well on text classiﬁcation tasks, where data is represented in a high dimensional space using sparse feature vectors (Joachims 1998; Kudo and Matsumoto 2000; Lodhi et al. 2002). We formulate the problem as a multi-class classiﬁcation problem using an SVM classiﬁer. We employ a O NE vs A LL (OVA) approach to train n classiﬁers for a multi-class problem. The classiﬁers are trained to discriminate between examples 1 www.cemantix.org/assert. 292 Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling of each class, and those belonging to all other classes combined. During testing, the classiﬁer scores on an example are combined to predict its class label. ASSERT was developed using TinySVM2 along with YamCha3 (Kudo and Matsumoto 2000, 2001) a"
J08-2006,N01-1025,0,0.160766,"Missing"
J08-2006,P98-2127,0,0.213323,"s used to identify passives. 4.2.6 SubCategorization. This is the phrase structure rule expanding the predicate’s parent node in the parse tree. For example, in Figure 3, the subcategorization for the predicate “went” is VP→VBD-PP-NP. 4.2.7 Predicate Cluster. The distance function used for clustering is based on the intuition that verbs with similar semantics will tend to have similar direct objects. For example, verbs such as eat, devour, and savor will tend to all occur with direct objects describing food. The clustering algorithm uses a database of verb–direct-object relations extracted by Lin (1998). The verbs were clustered into 64 classes using the probabilistic cooccurrence model of Hofmann and Puzicha (1998). We then use the verb class of the current predicate as a feature. 4.2.8 Head Word. Syntactic head of the constituent. 4.2.9 Head Word POS. Part of speech of the head word. 4.2.10 Named Entities in Constituents. Binary features for seven named entities (P ERSON , O RGANIZATION , L OCATION , P ERCENT, M ONEY, T IME , D ATE) tagged by IdentiFinder (Bikel, Schwartz, and Weischedel 1999). 4.2.11 Path Generalizations. 1. Partial Path—Path from the constituent to the lowest common ance"
J08-2006,J93-2004,0,0.031636,"Missing"
J08-2006,N06-1020,0,0.0841384,"Missing"
J08-2006,P06-1043,0,0.0441905,"Missing"
J08-2006,N06-2025,0,0.0152797,"ible to achieve accuracies within the range of human inter-annotator agreement. More recent approaches have involved using improved features such as n-best parses (Koomen et al. 2005; Toutanova, Haghighi, and Manning 2005); exploiting argument interdependence (Jiang, Li, and Ng 2005); using information from fundamentally different, and complementary syntactic, views (Pradhan, Ward et al. 2005); combining hypotheses from different labeling systems using inference (M`arquez et al. 2005); as well as applying novel learning paradigms (Punyakanok et al. 2005; Toutanova, Haghighi, and Manning 2005; Moschitti 2006) that try to capture more sequence and contextual information. Some have also tried to jointly decode the syntactic and semantic structures (Yi and Palmer 2005; Musillo and Merlo 2006). This problem has also been the subject of two CoNLL shared tasks (Carreras and M`arquez 2004; Carreras and M`arquez 2005). Although all of these systems perform quite well on the standard test data, they show signiﬁcant performance degradation when applied to test data drawn from a genre different from the data on which the system was trained. The focus of this article is to present results from an examination"
J08-2006,N06-2026,0,0.0261916,". 2005; Toutanova, Haghighi, and Manning 2005); exploiting argument interdependence (Jiang, Li, and Ng 2005); using information from fundamentally different, and complementary syntactic, views (Pradhan, Ward et al. 2005); combining hypotheses from different labeling systems using inference (M`arquez et al. 2005); as well as applying novel learning paradigms (Punyakanok et al. 2005; Toutanova, Haghighi, and Manning 2005; Moschitti 2006) that try to capture more sequence and contextual information. Some have also tried to jointly decode the syntactic and semantic structures (Yi and Palmer 2005; Musillo and Merlo 2006). This problem has also been the subject of two CoNLL shared tasks (Carreras and M`arquez 2004; Carreras and M`arquez 2005). Although all of these systems perform quite well on the standard test data, they show signiﬁcant performance degradation when applied to test data drawn from a genre different from the data on which the system was trained. The focus of this article is to present results from an examination into the primary causes of the lack of portability across genres of data. To set the stage for these experiments we ﬁrst describe the operation of ASSERT, our state-of-the art SRL syst"
J08-2006,C04-1100,0,0.0309742,"entiﬁcation transfer relatively well to a new corpus, argument classiﬁcation does not. An analysis of the reasons for this is presented and these generally point to the nature of the more lexical/semantic features dominating the classiﬁcation task where more general structural features are dominant in the argument identiﬁcation task. 1. Introduction Automatic, accurate, and wide-coverage techniques that can annotate naturally occurring text with semantic structure can play a key role in NLP applications such as information extraction (Harabagiu, Bejan, and Morarescu 2005), question answering (Narayanan and Harabagiu 2004), and summarization. Semantic role labeling (SRL) is one method for producing such semantic structure. When presented with a sentence, a semantic role labeler should, for each predicate in the sentence, ﬁrst identify and then label its semantic arguments. This process entails identifying groups of words in a sentence that represent these semantic arguments and assigning speciﬁc labels to them. In the bulk of recent work, this problem has been cast as a problem in supervised machine learning. Using these techniques with hand-corrected syntactic parses, it has ∗ Department of Speech and Language"
J08-2006,J05-1004,0,0.791424,"Missing"
J08-2006,N04-1030,1,0.707395,"ypotheses for each node in the syntax tree. A Viterbi search through the lattice uses the probabilities assigned by the sigmoid as the observation probabilities, along with the argument sequence language model probabilities, to ﬁnd the maximum likelihood path such that each node is either assigned a value belonging to the PropBank arguments, or N ULL. The search is also constrained so that no two nodes that overlap are both assigned NON -N ULL labels. 4.2 Features The feature set used in ASSERT is a combination of features described in Gildea and Jurafsky (2002) as well as those introduced in Pradhan et al. (2004), Surdeanu et al. (2003), and the syntactic-frame feature proposed in (Xue and Palmer 2004). Following is the list of features used. 4.2.1 Predicate. This is the predicate whose arguments are being identiﬁed. The surface form as well as the lemma are added as features. 4.2.2 Path. The syntactic path through the parse tree from the parse constituent to the predicate being classiﬁed. For example, in Figure 3, the path from A RG 0 (The lawyers) to the predicate went is represented with the string NP↑S↓VP↓VBD. ↑ and ↓ represent upward and downward movement in the tree, respectively. 4.2.3 Phrase T"
J08-2006,P05-1072,1,0.910047,"Missing"
J08-2006,P03-1002,0,0.0610249,"in the syntax tree. A Viterbi search through the lattice uses the probabilities assigned by the sigmoid as the observation probabilities, along with the argument sequence language model probabilities, to ﬁnd the maximum likelihood path such that each node is either assigned a value belonging to the PropBank arguments, or N ULL. The search is also constrained so that no two nodes that overlap are both assigned NON -N ULL labels. 4.2 Features The feature set used in ASSERT is a combination of features described in Gildea and Jurafsky (2002) as well as those introduced in Pradhan et al. (2004), Surdeanu et al. (2003), and the syntactic-frame feature proposed in (Xue and Palmer 2004). Following is the list of features used. 4.2.1 Predicate. This is the predicate whose arguments are being identiﬁed. The surface form as well as the lemma are added as features. 4.2.2 Path. The syntactic path through the parse tree from the parse constituent to the predicate being classiﬁed. For example, in Figure 3, the path from A RG 0 (The lawyers) to the predicate went is represented with the string NP↑S↓VP↓VBD. ↑ and ↓ represent upward and downward movement in the tree, respectively. 4.2.3 Phrase Type. Syntactic category"
J08-2006,P05-1073,0,0.063777,"Missing"
J08-2006,W04-3212,0,0.402624,"obabilities assigned by the sigmoid as the observation probabilities, along with the argument sequence language model probabilities, to ﬁnd the maximum likelihood path such that each node is either assigned a value belonging to the PropBank arguments, or N ULL. The search is also constrained so that no two nodes that overlap are both assigned NON -N ULL labels. 4.2 Features The feature set used in ASSERT is a combination of features described in Gildea and Jurafsky (2002) as well as those introduced in Pradhan et al. (2004), Surdeanu et al. (2003), and the syntactic-frame feature proposed in (Xue and Palmer 2004). Following is the list of features used. 4.2.1 Predicate. This is the predicate whose arguments are being identiﬁed. The surface form as well as the lemma are added as features. 4.2.2 Path. The syntactic path through the parse tree from the parse constituent to the predicate being classiﬁed. For example, in Figure 3, the path from A RG 0 (The lawyers) to the predicate went is represented with the string NP↑S↓VP↓VBD. ↑ and ↓ represent upward and downward movement in the tree, respectively. 4.2.3 Phrase Type. Syntactic category (NP, PP, etc.) of the constituent. 4.2.4 Position. Whether the cons"
J08-2006,W05-0639,0,0.0234221,"parses (Koomen et al. 2005; Toutanova, Haghighi, and Manning 2005); exploiting argument interdependence (Jiang, Li, and Ng 2005); using information from fundamentally different, and complementary syntactic, views (Pradhan, Ward et al. 2005); combining hypotheses from different labeling systems using inference (M`arquez et al. 2005); as well as applying novel learning paradigms (Punyakanok et al. 2005; Toutanova, Haghighi, and Manning 2005; Moschitti 2006) that try to capture more sequence and contextual information. Some have also tried to jointly decode the syntactic and semantic structures (Yi and Palmer 2005; Musillo and Merlo 2006). This problem has also been the subject of two CoNLL shared tasks (Carreras and M`arquez 2004; Carreras and M`arquez 2005). Although all of these systems perform quite well on the standard test data, they show signiﬁcant performance degradation when applied to test data drawn from a genre different from the data on which the system was trained. The focus of this article is to present results from an examination into the primary causes of the lack of portability across genres of data. To set the stage for these experiments we ﬁrst describe the operation of ASSERT, our"
J88-4003,J88-4003,1,0.0513221,"Missing"
J88-4003,P84-1063,0,0.285638,"e other hand, 'rm' should be connected to the goal of deleting a file by a planfor relation, since this goal is what 'rm' is typically used for. Traditional approaches to dialog understanding have focused on the process of plan inference. Under this approach, utterances are viewed as steps of plans. Such plans may themselves be parts of higher-level plans, and so on. Allen and Perrault (1980) developed a system that exemplifies this approach. Their system handled direct and indirect speech acts by plan analysis. Carberry (1983) extended this paradigm to deal more thoroughly with domain plans. Litman and Allen (1984) used the notion of metaplans (Wilensky 1983) to facilitate the comprehension of subdialogs. Grosz and Sidner (1985) pointed out the need for attentional knowledge in understanding discourse. One problem that has persisted in the literature is an inadequate representation of the relationship between goals and plans. Planfors provide such a representation. Planfors allow a goal analysis mechanism to combine certain inferences that should be kept together. First, inferences about plans may be made at the same time as those about goals. This is in contrast with systems such as Wilensky's PAM syst"
J92-1011,J76-2003,0,0.105041,"hanisms are quite similar to those used by Fass to implement his theory of Collative Semantics in the Meta5 system (Fass 1988) and the learning mechanisms in the MIDAS system (Martin 1990). Chapter 6, entitled &quot;Computational Theories of Metaphor,&quot; reviews a number of more modern theories of metaphor and compares them to the DTH approach. The theories presented here are mostly from the field of cognitive science, and are more 112 Book Reviews formal than those discussed in Chapter 2. Unfortunately, Way fails to discuss any implemented systems designed to interpret or generate metaphor, such as Russell (1976), Wilks (1978), Weiner (1984), Jacobs (1985), Fass (1988, 1991), and Martin (1990). Chapter 7 presents a discussion of the logical foundations of inheritance hierarchies. Chapter 8 is a broad discussion relating the DTH theory to issues such as prototype theory and Wittgenstein&apos;s notions of family resemblance. The final chapter, entitled &quot;Programming the Dynamic Type Hierarchy,&quot; promises to fill in the implementation details missing from Chapter 5. It begins with a detailed description of the CGEN system, a semantic interpreter based on Sowa&apos;s theories implemented at IBM. This system does not"
J92-1011,J84-1001,0,0.026221,"hose used by Fass to implement his theory of Collative Semantics in the Meta5 system (Fass 1988) and the learning mechanisms in the MIDAS system (Martin 1990). Chapter 6, entitled &quot;Computational Theories of Metaphor,&quot; reviews a number of more modern theories of metaphor and compares them to the DTH approach. The theories presented here are mostly from the field of cognitive science, and are more 112 Book Reviews formal than those discussed in Chapter 2. Unfortunately, Way fails to discuss any implemented systems designed to interpret or generate metaphor, such as Russell (1976), Wilks (1978), Weiner (1984), Jacobs (1985), Fass (1988, 1991), and Martin (1990). Chapter 7 presents a discussion of the logical foundations of inheritance hierarchies. Chapter 8 is a broad discussion relating the DTH theory to issues such as prototype theory and Wittgenstein&apos;s notions of family resemblance. The final chapter, entitled &quot;Programming the Dynamic Type Hierarchy,&quot; promises to fill in the implementation details missing from Chapter 5. It begins with a detailed description of the CGEN system, a semantic interpreter based on Sowa&apos;s theories implemented at IBM. This system does not implement the DTH theory and"
J95-1002,C92-1043,0,0.0204203,"h actions are viewed as being related hierarchically, that is, in which one higher level action is realized by the execution of a set of lower level actions. 8 As Section 4 indicated, there are a large number of lexical and grammatical forms in which such procedural relations are typically expressed, each used in a particular functional context. This section discusses the systems that have been included in IMAGENE to distinguish among these contexts. There are other studies of purpose expressions from the point of view of representation and understanding that are of use here (Di Eugenio 1992; Balkanski 1992). Di Eugenio, for example, has worked with by purposes and to infinitive (TNF) purposes in the context of understanding, but does not appear to have distinguished the two forms in her analysis of the procedural relations between actions. The current study is critically interested in discerning principled reasons for choosing between these sorts of expressions. The issues of slot and form of purpose expressions are treated largely independently by IMAGENE.The slot is determined by the sub-network shown in Figure 5. The form is determined by the sub-networks shown later in Figures 7 and 8. This"
J95-1002,J93-1001,0,0.0143976,"esses like cooking and craft making. The one common feature is that they all involve the expression of actions and of the procedural relations between them. 1 As an example of the nature of this text, consider the following excerpt from the instructions for the GTE Airfone (Airfone 1991), which will be called the Remove-Phone text: (2) When instructed (approx. 10 sec.) remove phone by firmly grasping top of handset and pulling out. Return to seat to place calls. (Airfone 1991) 2 1 It should be noted that this corpus is much smaller than the language corpora used in larger statistical studies (Church and Mercer 1993). The deep semantic and pragmatic knowledge that was required for the current study has necessitated this. 2 This paper will add a reference to the end of all examples that have come directly from the corpus, indicating the manual from which they were taken. Examples of actual IMAGENE output will be fully italicized. All other examples are contrived for explanatory purposes. 32 Keith Vander Linden and James H. Martin Expressing Rhetorical Relations This passage gives an example of the variation of expressional form that is common in instructional text. It contains, among other things, two expr"
J95-1002,W90-0106,0,0.0195007,"Missing"
J95-1002,W93-0202,0,0.0244264,"make it possible or sensible to carry out N R recognizes that situation S must be realized in order to successfully carry out action N The Precondition relation is a simple amalgam of the standard RST relations Circumstance and Condition. It has been taken from R6sner and Stede&apos;s work on generating multilingual instructions (R6sner and Stede 1992a). This particular combination has proven useful in analyzing various kinds of conditions and circumstances that 4 There is some question as to w h e t h e r these semantically based relations should be termed rhetorical in the classic sense at all (Dale 1993). Because of the prevalence of this use of the term, however, it will be retained in this paper. 35 Computational Linguistics Volume 21, Number 1 frequently arise in instructions, such as the precondition found in the Remove-Phone text: &quot;When instructed (approx. 10 sec.) remove phone . . . . &quot; Here, the removal of the phone must not be attempted until after the device has instructed the user to do so. RESULT (adapted from Mann and Thompson 1987) constraints o n N: constraints o n S: none presents either a volitional or non-volitional action or the situation that could have arisen from one cons"
J95-1002,W93-0203,0,0.0569271,"Missing"
J95-1002,P92-1016,0,0.0608017,"Missing"
J95-1002,J86-3001,0,0.0273986,"t include specification of the constraints on the nuclei and the combination of nuclei, as well as a specification of the effect of the expression. The Sequence and Concurrent relations are such relations. RST was attractive for the IMAGENEproject because of its ability to represent the hierarchical structure of text with rhetorical structures that matched the level of analysis required for the study of expressions of procedural relations. There is considerable debate in the field of discourse analysis concerning the relative importance of intentional structure and rhetorical relations (e.g., Grosz and Sidner 1986; Moore and Pollack 1992), most systems focusing on one or the other. The current study has conflated them, as the instructional texts have not tended to display the complex intentional structure common to persuasive texts and interactive discourses (Vander Linden 1993b). Finally, RST has been used by many researchers for the purpose of text generation (e.g., Moore and Paris 1988; Hovy and McCoy 1989; Scott and Souza 1990; R6sner 33 Computational Linguistics Volume 21, Number 1 Precondition[ (1)Instruct ~ (2) Remove Purpose 151Return 1 (6)Place ~uence (3)Grasp (4)Pull Figure 1 The RST represen"
J95-1002,P88-1020,0,0.244458,"3c]). It will conclude with a discussion of how well IMAGENE&apos;Spredictions match the text in the training and the testing portions of the corpus. 2. Related Work in Natural Language Generation In pursuit of other issues, many studies have adopted a temporary solution to the problem of managing diverse forms of expression, namely, that of choosing a single lexical and grammatical form to express each of the relevant types of information dealt with by the system. It is then a simple matter of allowing the type of information to determine the appropriate expressional form. Hovy&apos;s text structurer (Hovy 1988b), for example, uses rhetorical relations as defined in Rhetorical Structure Theory (RST) (Mann and Thompson 1987) to order a set of propositions to be expressed. It does not make any decision as to how a chosen relation is to be expressed, but rather leaves this task to the rudimentary implementation of the rhetorical relations provided by the Penman text generation system (Mann 1985). In the case of a purpose, for example, Penman will produce a non-fronted in order to infinitive clause, as in the following output of the structurer (Hovy 1988b, p. 167): &quot;Knox is en route in order to rendezvo"
J95-1002,P87-1029,0,0.053131,"Missing"
J95-1002,H89-1022,0,0.0296008,"ental source of procedural information for the text inquiries. Currently, however, it is simply used by the final component of the architecture, the Sentence Builder, to specify the appropriate lexical items and case structures for the action input to the text-level inquiries. The Sentence Builder uses the lexical information given in the Process Structure just described, to translate the Text Structure, described above, into the appropriate sentence specification to be passed to Penman for surface realization. This specification is coded in terms of Penman&apos;s Sentence Planning Language (SPL) (Kasper 1989), a language that allows the specification of the lexical items and gram42 Keith Vander Linden and James H. Martin Expressing Rhetorical Relations matical structures to be generated by Penman. The translation process is performed by a recursive descent of the Text Structure hierarchy. One SPL command is produced for each sentence in the Text Structure. 5.2 Purpose Relations in Instructional Text Purpose expressions arise in the context in which actions are viewed as being related hierarchically, that is, in which one higher level action is realized by the execution of a set of lower level acti"
J95-1002,J89-4002,0,0.167987,"o be expressed, but rather leaves this task to the rudimentary implementation of the rhetorical relations provided by the Penman text generation system (Mann 1985). In the case of a purpose, for example, Penman will produce a non-fronted in order to infinitive clause, as in the following output of the structurer (Hovy 1988b, p. 167): &quot;Knox is en route in order to rendezvous with CTG 070.10.&quot; A similar approach to expressing rhetorical relations was taken in McKeown&apos;s TEXT system (McKeown 1985). Text generators specifically designed for instructional text, such as Mellish and Evans&apos; generator (Mellish and Evans 1989), EPICURE (Dale 1992), COMET (McKeown et al. 1990), and T E C H D O C (R6sner and Stede 1992b), display similar characteristics. (See the analytical work of Delin, Scott, and Hartley [1993] for a notable exception to this pattern.) Mellish and Evans&apos; generator, for example, uses the output of NONLIN, a nonlinear planner (Tate 1976), as the preliminary rhetorical structure for the text. Because this often produced text that was monotonous or hard to understand, they included what was termed a message optimization phase that specifies rules for removing or modifying certain elements of the plan"
J95-1002,J92-4007,0,0.119205,"of the constraints on the nuclei and the combination of nuclei, as well as a specification of the effect of the expression. The Sequence and Concurrent relations are such relations. RST was attractive for the IMAGENEproject because of its ability to represent the hierarchical structure of text with rhetorical structures that matched the level of analysis required for the study of expressions of procedural relations. There is considerable debate in the field of discourse analysis concerning the relative importance of intentional structure and rhetorical relations (e.g., Grosz and Sidner 1986; Moore and Pollack 1992), most systems focusing on one or the other. The current study has conflated them, as the instructional texts have not tended to display the complex intentional structure common to persuasive texts and interactive discourses (Vander Linden 1993b). Finally, RST has been used by many researchers for the purpose of text generation (e.g., Moore and Paris 1988; Hovy and McCoy 1989; Scott and Souza 1990; R6sner 33 Computational Linguistics Volume 21, Number 1 Precondition[ (1)Instruct ~ (2) Remove Purpose 151Return 1 (6)Place ~uence (3)Grasp (4)Pull Figure 1 The RST representation of the Remove-Phon"
J95-1002,J88-3006,0,0.213753,"the study of expressions of procedural relations. There is considerable debate in the field of discourse analysis concerning the relative importance of intentional structure and rhetorical relations (e.g., Grosz and Sidner 1986; Moore and Pollack 1992), most systems focusing on one or the other. The current study has conflated them, as the instructional texts have not tended to display the complex intentional structure common to persuasive texts and interactive discourses (Vander Linden 1993b). Finally, RST has been used by many researchers for the purpose of text generation (e.g., Moore and Paris 1988; Hovy and McCoy 1989; Scott and Souza 1990; R6sner 33 Computational Linguistics Volume 21, Number 1 Precondition[ (1)Instruct ~ (2) Remove Purpose 151Return 1 (6)Place ~uence (3)Grasp (4)Pull Figure 1 The RST representation of the Remove-Phone text. and Stede 1992a). This testifies not only to RST&apos;s usefulness, but also to the direct applicability of the results of the current study to the field of natural language generation. Because of the common use of RST, the results can be more easily applied to other work in this area. In particular, the focus on the precise forms of expression of rhet"
J95-1002,W93-0237,1,0.862998,"Missing"
J95-1002,P94-1007,1,0.727964,"Missing"
L16-1558,C08-1006,0,0.216907,"esources. We cite briefly some relevant work in creating such corpora. Lying Words: (Newman et al., 2003) created a labeled corpus of elicited narratives marked as lie or true, then applied machine learning techniques (logistic regression) to rank the contribution of these linguistic categories. Critical Segments: (Enos et al., 2007), employed CSC (Columbia SRI Colorado) Deception Corpus and hypothesized that there exists a class of speech segments, critical segments, whose truth or falsity can be used to compute the overall truth or falsity of the entire communication. Deceptive Indicators: (Bachenko et al., 2008) describe a system built using NLP techniques and linguistic features as deceptive indicators for classifying truth-verifiable statements in civil and criminal transcripts as lie or true. The labelled corpus used in these experiments are real-world transcripts from actual criminal statements, police interrogations, and legal testimony. The Lie Detector: (Mihalcea and Strapparava, 2009) used Amazon Mechanical Turk (AMT) as a source of annotations to build an annotated data set of true and lie texts on three topics (abortion, death penalty, and best friend). This was further extended in (Perez-R"
L16-1558,P14-1147,0,0.0636495,"ts on three topics (abortion, death penalty, and best friend). This was further extended in (Perez-Rosas et al., 2014) to include thermal, and visual responses of human subjects under three deceptive scenarios Differences in Deception: (Conroy et al., 2015) present three types of fake news and a set of requirements for building a fake news detection corpus. Deceptive Opinion Spam: (Ott et al., 2011) describe an experiment in automatic detection of deceptive opinion spam. The authors built positive and negative sets of opinion spam, using on-line hotel reviews and AMT. Following on this work, (Li et al., 2014) addressed issues arising from the use of AMT by broadening the set of review writers to include domain experts and by adding new domains. Boulder Lies and Truth Corpus: (Salvetti, 2012), presented here, emphasized the potential for linguistic and extralinguistic features to bias the results of machine-learned classifiers, describes the process of building a balanced corpus of deceptive and non-deceptive texts that controls for possible confounders. A particular focus of this work is to avoid confounding the truth status of the reviews with the source of the reviews (i.e, truthful reviews from"
L16-1558,P09-2078,0,0.0372503,"tion Corpus and hypothesized that there exists a class of speech segments, critical segments, whose truth or falsity can be used to compute the overall truth or falsity of the entire communication. Deceptive Indicators: (Bachenko et al., 2008) describe a system built using NLP techniques and linguistic features as deceptive indicators for classifying truth-verifiable statements in civil and criminal transcripts as lie or true. The labelled corpus used in these experiments are real-world transcripts from actual criminal statements, police interrogations, and legal testimony. The Lie Detector: (Mihalcea and Strapparava, 2009) used Amazon Mechanical Turk (AMT) as a source of annotations to build an annotated data set of true and lie texts on three topics (abortion, death penalty, and best friend). This was further extended in (Perez-Rosas et al., 2014) to include thermal, and visual responses of human subjects under three deceptive scenarios Differences in Deception: (Conroy et al., 2015) present three types of fake news and a set of requirements for building a fake news detection corpus. Deceptive Opinion Spam: (Ott et al., 2011) describe an experiment in automatic detection of deceptive opinion spam. The authors"
L16-1558,P11-1032,0,0.45435,"tements, police interrogations, and legal testimony. The Lie Detector: (Mihalcea and Strapparava, 2009) used Amazon Mechanical Turk (AMT) as a source of annotations to build an annotated data set of true and lie texts on three topics (abortion, death penalty, and best friend). This was further extended in (Perez-Rosas et al., 2014) to include thermal, and visual responses of human subjects under three deceptive scenarios Differences in Deception: (Conroy et al., 2015) present three types of fake news and a set of requirements for building a fake news detection corpus. Deceptive Opinion Spam: (Ott et al., 2011) describe an experiment in automatic detection of deceptive opinion spam. The authors built positive and negative sets of opinion spam, using on-line hotel reviews and AMT. Following on this work, (Li et al., 2014) addressed issues arising from the use of AMT by broadening the set of review writers to include domain experts and by adding new domains. Boulder Lies and Truth Corpus: (Salvetti, 2012), presented here, emphasized the potential for linguistic and extralinguistic features to bias the results of machine-learned classifiers, describes the process of building a balanced corpus of decept"
L16-1558,perez-rosas-etal-2014-multimodal,0,0.0312583,", 2008) describe a system built using NLP techniques and linguistic features as deceptive indicators for classifying truth-verifiable statements in civil and criminal transcripts as lie or true. The labelled corpus used in these experiments are real-world transcripts from actual criminal statements, police interrogations, and legal testimony. The Lie Detector: (Mihalcea and Strapparava, 2009) used Amazon Mechanical Turk (AMT) as a source of annotations to build an annotated data set of true and lie texts on three topics (abortion, death penalty, and best friend). This was further extended in (Perez-Rosas et al., 2014) to include thermal, and visual responses of human subjects under three deceptive scenarios Differences in Deception: (Conroy et al., 2015) present three types of fake news and a set of requirements for building a fake news detection corpus. Deceptive Opinion Spam: (Ott et al., 2011) describe an experiment in automatic detection of deceptive opinion spam. The authors built positive and negative sets of opinion spam, using on-line hotel reviews and AMT. Following on this work, (Li et al., 2014) addressed issues arising from the use of AMT by broadening the set of review writers to include domai"
N04-1030,A00-2031,0,0.0077464,"Missing"
N04-1030,P01-1017,0,0.09119,"Missing"
N04-1030,W03-1006,0,0.234797,"Missing"
N04-1030,C92-3145,0,0.0180452,"Missing"
N04-1030,N03-2008,0,0.0468064,"Missing"
N04-1030,W03-1008,0,0.271799,"Missing"
N04-1030,P00-1065,0,0.0435523,"Missing"
N04-1030,J02-3001,0,0.853591,"Missing"
N04-1030,P02-1031,0,0.133044,"Missing"
N04-1030,N03-2009,1,0.474549,"Missing"
N04-1030,W00-0730,0,0.418325,"Missing"
N04-1030,N01-1025,0,0.381966,"Missing"
N04-1030,P98-2127,0,0.297893,"Missing"
N04-1030,H94-1020,0,0.017141,"Missing"
N04-1030,P03-1002,0,0.75586,"Missing"
N04-1030,J03-4003,0,\N,Missing
N04-1030,C98-2122,0,\N,Missing
N04-4036,P01-1017,0,0.047049,"Missing"
N04-4036,P00-1065,1,0.910949,"Missing"
N04-4036,J02-3001,1,0.584303,"Missing"
N04-4036,P02-1031,0,0.086694,"Missing"
N04-4036,N03-2009,1,0.830112,"Missing"
N04-4036,kingsbury-palmer-2002-treebank,0,0.0553927,"Missing"
N04-4036,P98-1013,0,0.180142,"Missing"
N04-4036,W00-0730,0,0.0872495,"Missing"
N04-4036,N01-1025,0,0.0931378,"Missing"
N04-4036,C98-1013,0,\N,Missing
N04-4036,J02-3004,0,\N,Missing
N04-4036,P03-1002,0,\N,Missing
N07-1070,boas-2002-bilingual,0,0.0319352,"Missing"
N07-1070,W05-0620,0,0.647894,"Missing"
N07-1070,P05-1022,0,0.0107265,"Missing"
N07-1070,N04-4008,0,0.0164569,"Missing"
N07-1070,J02-3001,0,0.908723,"language studies. The Release 3 of the Penn Treebank contains the hand parsed syntactic trees of a subset of the Brown Corpus – sections F, G, K, L, M, N, P and R. Palmer et al., (2005) have recently PropBanked a significant portion of this Treebanked Brown corpus. In all, about 17,500 predicates are tagged with their semantic arguments. For these experiments we used a limited release of PropBank dated September 2005. A small portion of the predicates – about 8,000 have also been tagged with frame sense information. 2 We formulate the labeling task as a classification problem as initiated by Gildea and Jurafsky (2002) and use Support Vector Machine (SVM) classifiers (2005). We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and classification software. The system uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C=1; and, tolerance of the termination criterion, e=0.001. More details of this system can be found in Pradhan et al., (2005). The performance of this system on section 23 of the WSJ when trained on sections 02-21 is shown in Table 1 Semantic Annotation and Corpora PropBank1 In the corpus (Palmer et al., 2005"
N07-1070,W01-0521,0,0.158443,"Missing"
N07-1070,W00-0730,0,0.0457092,"Corpus – sections F, G, K, L, M, N, P and R. Palmer et al., (2005) have recently PropBanked a significant portion of this Treebanked Brown corpus. In all, about 17,500 predicates are tagged with their semantic arguments. For these experiments we used a limited release of PropBank dated September 2005. A small portion of the predicates – about 8,000 have also been tagged with frame sense information. 2 We formulate the labeling task as a classification problem as initiated by Gildea and Jurafsky (2002) and use Support Vector Machine (SVM) classifiers (2005). We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and classification software. The system uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C=1; and, tolerance of the termination criterion, e=0.001. More details of this system can be found in Pradhan et al., (2005). The performance of this system on section 23 of the WSJ when trained on sections 02-21 is shown in Table 1 Semantic Annotation and Corpora PropBank1 In the corpus (Palmer et al., 2005), predicate argument relations are marked for the verbs in the text. PropBank was constructed by assigning semantic ar"
N07-1070,N01-1025,0,0.0294945,"L, M, N, P and R. Palmer et al., (2005) have recently PropBanked a significant portion of this Treebanked Brown corpus. In all, about 17,500 predicates are tagged with their semantic arguments. For these experiments we used a limited release of PropBank dated September 2005. A small portion of the predicates – about 8,000 have also been tagged with frame sense information. 2 We formulate the labeling task as a classification problem as initiated by Gildea and Jurafsky (2002) and use Support Vector Machine (SVM) classifiers (2005). We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and classification software. The system uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C=1; and, tolerance of the termination criterion, e=0.001. More details of this system can be found in Pradhan et al., (2005). The performance of this system on section 23 of the WSJ when trained on sections 02-21 is shown in Table 1 Semantic Annotation and Corpora PropBank1 In the corpus (Palmer et al., 2005), predicate argument relations are marked for the verbs in the text. PropBank was constructed by assigning semantic arguments to constituents of"
N07-1070,H94-1020,0,0.0142693,"constructed by assigning semantic arguments to constituents of the handcorrected Treebank parses. The arguments of a verb are labeled A RG 0 to A RG 5, where A RG 0 is the P ROTO -AGENT (usually the subject of a transitive verb) A RG 1 is the P ROTO -PATIENT (usually its direct object), etc. In addition to these C ORE A RGU MENTS , 16 additional A DJUNCTIVE A RGUMENTS , referred to as A RG Ms are also marked. More recently the PropBanking effort has been extended to encompass multiple corpora. In this study we use PropBanked versions of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1994) and part of the Brown portion of the Penn Treebank. The WSJ PropBank data comprise 24 sections of the WSJ, each section representing about 100 documents. PropBank release 1.0 contains about 114,000 predicates instantiating about 250,000 arguments and covering about 3,200 verb lemmas. Section 23, which is a standard test set and a test set in some of our experiments, comprises 5,400 predicates instantiating about 12,000 arguments. The Brown corpus is a Standard Corpus of American English that consists of about one million words of English text printed in the calendar year 1961 1 3 SRL System D"
N07-1070,N06-1020,0,0.0211819,"rather than gold standard ones. For this experiment, we used the same amount of training data from WSJ as available in the Brown training set – that is about 14,000 predicates. The examples from WSJ were selected randomly. The Brown test set is the same as used in the previous experiment, and the WSJ test set is the entire section 23. Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and selftraining and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al., 2006a; McClosky et al., 2006b). The performance of these parsers as reported in the respective literature are shown in Table 6 shows the performance (as reported in the literature) of the Charniak parser: when trained and tested on WSJ, when trained on WSJ and tested on Brown, When trained and tested on Brown, and when trained on WSJ and adapted with NANC. Train WSJ WSJ Brown WSJ+NANC Test WSJ Brown Brown Brown 2. F 91.0 85.2 88.4 87.9 Table 6: Charniak parser performance. We describe the results of Semantic Role Labeling under the following five conditions: 1. The SRL system is trained on feature"
N07-1070,P06-1043,0,0.0431706,"rather than gold standard ones. For this experiment, we used the same amount of training data from WSJ as available in the Brown training set – that is about 14,000 predicates. The examples from WSJ were selected randomly. The Brown test set is the same as used in the previous experiment, and the WSJ test set is the entire section 23. Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and selftraining and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al., 2006a; McClosky et al., 2006b). The performance of these parsers as reported in the respective literature are shown in Table 6 shows the performance (as reported in the literature) of the Charniak parser: when trained and tested on WSJ, when trained on WSJ and tested on Brown, When trained and tested on Brown, and when trained on WSJ and adapted with NANC. Train WSJ WSJ Brown WSJ+NANC Test WSJ Brown Brown Brown 2. F 91.0 85.2 88.4 87.9 Table 6: Charniak parser performance. We describe the results of Semantic Role Labeling under the following five conditions: 1. The SRL system is trained on feature"
N07-1070,C04-1100,0,0.0309553,"Missing"
N07-1070,J05-1004,0,0.830264,"7 Association for Computational Linguistics Surprisingly, it does not matter much whether the data is from another newswire, or a completely different type of text – as in the Brown corpus. These results indicate that the systems are being over-fit to the specific genre of text. Many performance improvements on the WSJ PropBank corpus may reflect tuning to the corpus. For the technology to be widely accepted and useful, it must be robust to change in genre of the data. Until recently, data tagged with similar semantic argument structure was not available for multiple genres of text. Recently, Palmer et al., (2005), have PropBanked a significant portion of the Treebanked Brown corpus which enables us to perform experiments to analyze the reasons behind the performance degradation, and suggest potential solutions. (Kuˇcera and Francis, 1967). The corpus contains about 500 samples of 2000+ words each. The idea behind creating this corpus was to create a heterogeneous sample of English text so that it would be useful for comparative language studies. The Release 3 of the Penn Treebank contains the hand parsed syntactic trees of a subset of the Brown Corpus – sections F, G, K, L, M, N, P and R. Palmer et al"
N07-1070,N04-1030,1,0.620811,"he argument identification task. So far, most of the work on SRL systems has been focused on improving the labeling performance on a test set belonging to the same genre of text as the training set. Both the Treebank on which the syntactic parser is trained and the PropBank on which the SRL systems are trained represent articles from the year 1989 of the WSJ. While all these systems perform quite well on the WSJ test data, they show significant performance degradation (approximately 10 point drop in F-score) when applied to label test data that is different than the genre that WSJ represents (Pradhan et al., 2004; Carreras and M`arquez, 2005). 556 Proceedings of NAACL HLT 2007, pages 556–563, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics Surprisingly, it does not matter much whether the data is from another newswire, or a completely different type of text – as in the Brown corpus. These results indicate that the systems are being over-fit to the specific genre of text. Many performance improvements on the WSJ PropBank corpus may reflect tuning to the corpus. For the technology to be widely accepted and useful, it must be robust to change in genre of the data. Until recent"
N07-1070,P05-1072,1,0.579955,"05. A small portion of the predicates – about 8,000 have also been tagged with frame sense information. 2 We formulate the labeling task as a classification problem as initiated by Gildea and Jurafsky (2002) and use Support Vector Machine (SVM) classifiers (2005). We use TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000) (Kudo and Matsumoto, 2001) as the SVM training and classification software. The system uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C=1; and, tolerance of the termination criterion, e=0.001. More details of this system can be found in Pradhan et al., (2005). The performance of this system on section 23 of the WSJ when trained on sections 02-21 is shown in Table 1 Semantic Annotation and Corpora PropBank1 In the corpus (Palmer et al., 2005), predicate argument relations are marked for the verbs in the text. PropBank was constructed by assigning semantic arguments to constituents of the handcorrected Treebank parses. The arguments of a verb are labeled A RG 0 to A RG 5, where A RG 0 is the P ROTO -AGENT (usually the subject of a transitive verb) A RG 1 is the P ROTO -PATIENT (usually its direct object), etc. In addition to these C ORE A RGU MENTS"
N07-1070,P03-1002,0,0.0943289,"Missing"
N07-1070,A00-2018,0,\N,Missing
N07-1070,W04-2416,1,\N,Missing
N07-1070,J93-2004,0,\N,Missing
N07-1070,N06-2025,0,\N,Missing
N07-1070,H05-1081,0,\N,Missing
N07-1070,W03-1008,0,\N,Missing
N07-1070,W04-3212,0,\N,Missing
N07-1070,W05-0639,0,\N,Missing
N07-1070,W05-0623,0,\N,Missing
N07-1070,hockenmaier-steedman-2002-acquiring,0,\N,Missing
N07-1070,W05-0625,0,\N,Missing
N07-1070,P02-1043,0,\N,Missing
N07-1070,P98-2184,0,\N,Missing
N07-1070,C98-2179,0,\N,Missing
N07-1070,N06-2026,0,\N,Missing
N07-1070,P05-1073,0,\N,Missing
N07-1070,P98-2127,0,\N,Missing
N07-1070,C98-2122,0,\N,Missing
N07-1070,W04-2412,0,\N,Missing
N07-1070,W06-2303,0,\N,Missing
nielsen-etal-2008-annotating,W06-2933,0,\N,Missing
nielsen-etal-2008-annotating,W03-0210,0,\N,Missing
nielsen-etal-2008-annotating,W07-1405,1,\N,Missing
nielsen-etal-2008-annotating,C04-1010,0,\N,Missing
nielsen-etal-2008-annotating,J02-3001,0,\N,Missing
nielsen-etal-2008-annotating,J05-1004,1,\N,Missing
nielsen-etal-2008-annotating,W05-0202,0,\N,Missing
P05-1072,P00-1065,1,0.378562,"Missing"
P05-1072,J02-3001,1,0.885159,"/software/yamcha/ 582 (hhhh ( (( NP VP ``` ((((hhhh VBD `VP T he acquisition ARG1 XXX was NULL VBN PP completed predicate in September ARGM−TMP ``` ` [ARG1 The acquisition] was [predicate completed] [ARGM−TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al., (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Table 1 lists the features used. P REDICATE L EMMA PATH: Path from the constituent to the predicate in the parse tree. P OSITION: Whether the constituent is before or after the predicate. VOICE P REDICATE SUB - CATEGORIZATION P REDICATE C LUSTER H EAD W ORD: Head word of the constituent. H EAD W ORD POS: POS of the head word NAMED E NTITIES IN C ONSTITUENTS: 7 named entities as 7 binary features. PARTIAL PATH: Path from the constituent to the lowest common a"
P05-1072,P02-1031,0,0.536652,"Missing"
P05-1072,C04-1186,1,0.578802,"man, 2002). In the second set of experiments, we explore approaches to identify optimal subsets of features for each argument class, and to calibrate the classifier probabilities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled A RG 0 to A RG 5, where A RG 0 is the P ROTO -AGENT, A RG 1 is the P ROTO -PATIENT, etc. In addition to"
P05-1072,N03-2009,1,0.791276,"Missing"
P05-1072,W04-2416,1,0.736909,"Missing"
P05-1072,hockenmaier-steedman-2002-acquiring,0,0.0421125,"organized as follows. We first describe a baseline system based on the best published techniques. We then report on two sets of experiments using techniques that improve performance on the problem of finding arguments when they are present in the syntactic analysis. In the first set of experiments we explore new 581 Proceedings of the 43rd Annual Meeting of the ACL, pages 581–588, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics S features, including features extracted from a parser that provides a different syntactic view – a Combinatory Categorial Grammar (CCG) parser (Hockenmaier and Steedman, 2002). In the second set of experiments, we explore approaches to identify optimal subsets of features for each argument class, and to calibrate the classifier probabilities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu,"
P05-1072,kingsbury-palmer-2002-treebank,0,0.0227165,"lities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled A RG 0 to A RG 5, where A RG 0 is the P ROTO -AGENT, A RG 1 is the P ROTO -PATIENT, etc. In addition to these C ORE A RGUMENTS, additional A DJUNCTIVE A RGUMENTS, referred to as A RG Ms are also marked. Some examples are A RG M-L OC, for locatives; A RG M-T MP, for temporals; A RG MM NR"
P05-1072,W00-0730,0,0.0185823,"tion. Table 9: Head-word based performance using Charniak and Minipar parses. 5.2 Chunk-based Semantic Labeler Hacioglu has previously described a chunk based semantic labeling method (Hacioglu et al., 2004). This system uses SVM classifiers to first chunk input text into flat chunks or base phrases, each labeled with a syntactic tag. A second SVM is trained to assign semantic labels to the chunks. The system is trained 586 SVMs were trained for begin (B) and inside (I) classes of all arguments and outside (O) class for a total of 78 one-vs-all classifiers. Again, TinySVM5 along with YamCha6 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used as the SVM training and test software. Table 11 presents the system performances on the PropBank test set for the chunk-based system. 5 6 http://chasen.org/˜taku/software/TinySVM/ http://chasen.org/˜taku/software/yamcha/ 6 Combining Semantic Labelers We combined the semantic parses as follows: i) scores for arguments were converted to calibrated probabilities, and arguments with scores below a threshold value were deleted. Separate thresholds were used for each parser. ii) For the remaining arguments, the more probable ones among overlapping ones were selec"
P05-1072,N01-1025,0,0.456011,"lti-class classification problem using Support Vector Machine (SVM) classifier (Hacioglu et al., 2003; Pradhan et al., 2003; Pradhan et al., 2004) TinySVM2 along with YamCha3 (Kudo and Mat1 2 3 http://www.cis.upenn.edu/˜ace/ http://chasen.org/˜taku/software/TinySVM/ http://chasen.org/˜taku/software/yamcha/ 582 (hhhh ( (( NP VP ``` ((((hhhh VBD `VP T he acquisition ARG1 XXX was NULL VBN PP completed predicate in September ARGM−TMP ``` ` [ARG1 The acquisition] was [predicate completed] [ARGM−TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al., (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Table 1 lists the features used. P REDICATE L EMMA PATH: Path from the constituent to the predicate in the parse tree. P OSITION: Whether the constituent is before or afte"
P05-1072,H94-1020,0,0.0271304,"ntic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled A RG 0 to A RG 5, where A RG 0 is the P ROTO -AGENT, A RG 1 is the P ROTO -PATIENT, etc. In addition to these C ORE A RGUMENTS, additional A DJUNCTIVE A RGUMENTS, referred to as A RG Ms are also marked. Some examples are A RG M-L OC, for locatives; A RG M-T MP, for temporals; A RG MM NR, for manner, etc. Figure 1 shows a syntax tree along with the argument labels for an example extracted from PropBank. We use Sections 02-21 for training, Section 00 for de"
P05-1072,J05-1004,0,0.248645,"periments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses. Arguments of a verb are labeled A RG 0 to A RG 5, where A RG 0 is the P ROTO -AGENT, A RG 1 is the P ROTO -PATIENT, etc. In addition to these C ORE A RGUMENTS, additional A DJUNCTIVE A RGUMENTS, referred to as A RG Ms are also marked. Some examples are A RG M-L OC, for locatives; A RG M-T MP, for temporals; A RG MM NR, for manner, etc. Fig"
P05-1072,N04-1030,1,0.905723,"O -PATIENT, etc. In addition to these C ORE A RGUMENTS, additional A DJUNCTIVE A RGUMENTS, referred to as A RG Ms are also marked. Some examples are A RG M-L OC, for locatives; A RG M-T MP, for temporals; A RG MM NR, for manner, etc. Figure 1 shows a syntax tree along with the argument labels for an example extracted from PropBank. We use Sections 02-21 for training, Section 00 for development and Section 23 for testing. We formulate the semantic labeling problem as a multi-class classification problem using Support Vector Machine (SVM) classifier (Hacioglu et al., 2003; Pradhan et al., 2003; Pradhan et al., 2004) TinySVM2 along with YamCha3 (Kudo and Mat1 2 3 http://www.cis.upenn.edu/˜ace/ http://chasen.org/˜taku/software/TinySVM/ http://chasen.org/˜taku/software/yamcha/ 582 (hhhh ( (( NP VP ``` ((((hhhh VBD `VP T he acquisition ARG1 XXX was NULL VBN PP completed predicate in September ARGM−TMP ``` ` [ARG1 The acquisition] was [predicate completed] [ARGM−TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers"
P05-1072,P03-1002,0,0.108903,"tion ARG1 XXX was NULL VBN PP completed predicate in September ARGM−TMP ``` ` [ARG1 The acquisition] was [predicate completed] [ARGM−TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al., (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Table 1 lists the features used. P REDICATE L EMMA PATH: Path from the constituent to the predicate in the parse tree. P OSITION: Whether the constituent is before or after the predicate. VOICE P REDICATE SUB - CATEGORIZATION P REDICATE C LUSTER H EAD W ORD: Head word of the constituent. H EAD W ORD POS: POS of the head word NAMED E NTITIES IN C ONSTITUENTS: 7 named entities as 7 binary features. PARTIAL PATH: Path from the constituent to the lowest common ancestor of the predicate and the constituent. V ERB S ENSE I NFORMATI"
P05-1072,W04-3212,0,0.685533,"M−TMP ``` ` [ARG1 The acquisition] was [predicate completed] [ARGM−TMP in September]. Figure 1: Syntax tree for a sentence illustrating the PropBank tags. sumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al., (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). Table 1 lists the features used. P REDICATE L EMMA PATH: Path from the constituent to the predicate in the parse tree. P OSITION: Whether the constituent is before or after the predicate. VOICE P REDICATE SUB - CATEGORIZATION P REDICATE C LUSTER H EAD W ORD: Head word of the constituent. H EAD W ORD POS: POS of the head word NAMED E NTITIES IN C ONSTITUENTS: 7 named entities as 7 binary features. PARTIAL PATH: Path from the constituent to the lowest common ancestor of the predicate and the constituent. V ERB S ENSE I NFORMATION: Oracle verb sense information from PropBank H EAD W ORD OF PP:"
P05-1072,A00-2018,0,0.16996,"luding features extracted from a parser that provides a different syntactic view – a Combinatory Categorial Grammar (CCG) parser (Hockenmaier and Steedman, 2002). In the second set of experiments, we explore approaches to identify optimal subsets of features for each argument class, and to calibrate the classifier probabilities. We then report on experiments that address the problem of arguments missing from a given syntactic analysis. We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). We show that these three views complement each other to improve performance. 2 Baseline System For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994). PropBank was constructed by assigning semantic arguments to constituents of handcorrected Tr"
P05-1072,W03-1006,0,0.252892,"Missing"
P05-1072,W03-1008,0,\N,Missing
P05-1072,P02-1043,0,\N,Missing
P08-2005,W04-1013,0,0.00707021,"Missing"
P08-2005,C02-1144,0,0.0203537,"promise as domain and student knowledge representations to support personalized learning interactions (de la Chica et al., 2008). Related Work Our work is informed by efforts to automate the acquisition of ontology concepts from text. OntoLearn extracts candidate domain terms from texts using a syntactic parse and updates an existing ontology with the identified concepts and relationships (Navigli and Velardi, 2004). Knowledge Puzzle focuses on n-gram identification to produce a list of candidate terms pruned using information extraction techniques to derive the ontology (Zouaq et al., 2007). Lin and Pantel (2002) discover concepts using clustering by committee to group terms into conceptually related clusters. These approaches produce ontologies of very fine granularity and therefore graphs that may not be suitable for presentation to a student. Multi-document summarization (MDS) research also informs our work. XDoX analyzes large document sets to extract important themes using n-gram scoring and clustering (Hardy et al., 2002). Topic representation and topic themes have also served as the basis for the exploration of promising MDS techniques (Harabagiu and Lacatusu, 2005). Finally, MEAD is a widely u"
P08-2005,W00-0403,0,0.251497,"ng by committee to group terms into conceptually related clusters. These approaches produce ontologies of very fine granularity and therefore graphs that may not be suitable for presentation to a student. Multi-document summarization (MDS) research also informs our work. XDoX analyzes large document sets to extract important themes using n-gram scoring and clustering (Hardy et al., 2002). Topic representation and topic themes have also served as the basis for the exploration of promising MDS techniques (Harabagiu and Lacatusu, 2005). Finally, MEAD is a widely used MDS and evaluation platform (Radev et al., 2000). While all these systems have produced promising results in automated evaluations, none have directly targeted educational content collections. 17 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 17–20, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 3 Empirical Study We have conducted a study to capture how human experts processed digital library resources to create a domain knowledge map. Four geology and instructional design experts selected 20 resources from DLESE to construct a knowledge map on earthquakes and plates tectonics for high s"
P08-2005,J04-2002,0,\N,Missing
P08-2045,W06-1618,1,0.880455,"Missing"
P08-2045,bethard-etal-2008-building,1,0.809728,"Missing"
P08-2045,S07-1003,0,0.0539268,"Missing"
P08-2045,W03-1210,0,0.380817,"Missing"
P08-2045,kingsbury-palmer-2002-treebank,0,0.125292,"Missing"
P08-2045,S07-1014,0,0.0554537,"Missing"
P08-2061,J02-3001,0,0.047315,"ings of ACL-08: HLT, Short Papers (Companion Volume), pages 241–244, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 2.1 Knowledge Representation The ASK assessments included a reference answer for each constructed response question. These reference answers were manually decomposed into fine-grained facets, roughly extracted from the relations in a syntactic dependency parse and a shallow semantic parse. The decomposition is based closely on these well-established frameworks, since the representations have been shown to be learnable by automatic systems (c.f., Gildea and Jurafsky, 2002; Nivre et al., 2006). Figure 1 illustrates the process of deriving the constituent facets that comprise the representation of the final reference answer. We begin by determining the dependency parse following the style of MaltParser (Nivre et al., 2006). This dependency parse was then modified in several ways. The rationale for the modifications, which we elaborate below, is to increase the semantic content of facets. These more expressive facets are used later to generate features for the assessment classification task. These types of modifications to the parser output address known limitati"
P08-2061,nielsen-etal-2008-annotating,1,0.880816,"Missing"
P08-2061,W06-2933,0,0.0351744,"Papers (Companion Volume), pages 241–244, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 2.1 Knowledge Representation The ASK assessments included a reference answer for each constructed response question. These reference answers were manually decomposed into fine-grained facets, roughly extracted from the relations in a syntactic dependency parse and a shallow semantic parse. The decomposition is based closely on these well-established frameworks, since the representations have been shown to be learnable by automatic systems (c.f., Gildea and Jurafsky, 2002; Nivre et al., 2006). Figure 1 illustrates the process of deriving the constituent facets that comprise the representation of the final reference answer. We begin by determining the dependency parse following the style of MaltParser (Nivre et al., 2006). This dependency parse was then modified in several ways. The rationale for the modifications, which we elaborate below, is to increase the semantic content of facets. These more expressive facets are used later to generate features for the assessment classification task. These types of modifications to the parser output address known limitations of current statis"
P08-2061,J05-1004,1,0.355102,"Missing"
P17-1043,D15-1198,0,0.241917,"Missing"
P17-1043,P14-1134,0,0.634072,"entations including named entities (Nadeau and Sekine, 2007), word senses (Banerjee and Pedersen, 2002), coreference relations, and predicate-argument structures (Kingsbury and Palmer, 2002; Palmer et al., 2005). More recent innovations include wikification of named entities and normalization of temporal expressions (Verhagen et al., 2010; Str¨otgen and Gertz, 2010). (2016) provides an insightful discussion of the relationship between AMR and other formal representations including first order logic. The process of creating AMR’s for sentences is called AMR Parsing and was first introduced in (Flanigan et al., 2014). A key factor driving the development of AMR systems has been the increasing availability of training resources in the form of corpora where each sentence is paired with a corresponding AMR representation 2 . A consistent framework for evaluating AMR parsers was defined by the Semeval-2016 Meaning Representation Parsing Task3 . Standard training, development and test splits for the AMR Annotation Release 1 corpus are provided, as well as an additional out-of-domain test dataset, for system comparisons. 4 Viewed as a structured prediction task, AMR parsing poses some difficult challenges not f"
P17-1043,W03-0425,0,0.0198069,"Missing"
P17-1043,S16-1185,1,0.866601,"Missing"
P17-1043,S16-1176,0,0.252031,"Missing"
P17-1043,J16-3006,0,0.0050613,"Missing"
P17-1043,S16-1180,0,0.253325,"Missing"
P17-1043,S16-1179,0,0.085998,"Missing"
P17-1043,P13-2131,0,0.164986,"s found by experimenting with the development data set). From then on, only the most probable edges which span graph components are chosen, until the graph contains a single component. Expressed as a step by step procedure, we first define pconnect as the probability threshold at which to require graph component spanning, and we repeat the following, until any two subgraphs in the graph are connected by at least one path. 5 Semantic graph comparison can be tricky because direct graph alignment fails in the presence of just a few miscompares. A practical graph comparison program called Smatch (Cai and Knight, 2013) is used to consistently evaluate AMR parsers. The smatch python script provides an F1 evaluation metric for whole-sentence semantic graph analysis by comparing sets of triples which describe portions of the graphs, and uses a hill climbing algorithm for efficiency. All networks, including SG, were trained using stochastic gradient descent (SGD) with a fixed learning rate. We tried sentence level loglikelihood, which trains a viterbi decoder, as a training objective, but found no improvement over word-level likelihood (cross entropy). After all LSTM and linear layers, we added dropout to minim"
P17-1043,N15-3006,0,0.0262954,"Missing"
P17-1043,kingsbury-palmer-2002-treebank,0,0.294471,"Missing"
P17-1043,S16-1181,0,0.568629,"Missing"
P17-1043,S16-1166,0,0.15933,"Missing"
P17-1043,N15-1040,0,0.290385,"e and system presented in the following sections is largely motivated by these two challenges. 2 2.1 Related Work AMR Parsers Most current AMR parsers are constructed using some form of supervised machine learning that exploits existing AMR corpora. In general, these systems make use of features derived from various forms of syntactic analysis, ranging from partof-speech tagging to more complex dependency or phrase-structure analysis. Currently, most systems fall into two classes: (1) systems that incrementally transform a dependency parse into an AMR 464 graph using transition-based systems (Wang et al., 2015, 2016), and (2) graph-oriented approaches that use syntactic features to score edges between all concept pairs, and then use a maximum spanning connected subgraph (MSCG) algorithm to select edges that will constitute the graph (Flanigan et al., 2014; Werling et al., 2015). As expected, there are exceptions to these general approaches. The largely rule-based approach of (2015) converts logical forms from an existing semantic analyzer into AMR graphs. They demonstrate the ability to use their existing system to generate AMRs in German, French, Spanish and Japanese without the need for a native"
P17-1043,J05-1004,0,0.159236,"Missing"
P17-1043,P15-1095,0,0.456143,"ystems make use of features derived from various forms of syntactic analysis, ranging from partof-speech tagging to more complex dependency or phrase-structure analysis. Currently, most systems fall into two classes: (1) systems that incrementally transform a dependency parse into an AMR 464 graph using transition-based systems (Wang et al., 2015, 2016), and (2) graph-oriented approaches that use syntactic features to score edges between all concept pairs, and then use a maximum spanning connected subgraph (MSCG) algorithm to select edges that will constitute the graph (Flanigan et al., 2014; Werling et al., 2015). As expected, there are exceptions to these general approaches. The largely rule-based approach of (2015) converts logical forms from an existing semantic analyzer into AMR graphs. They demonstrate the ability to use their existing system to generate AMRs in German, French, Spanish and Japanese without the need for a native AMR corpus. (2015) proposes a synchronous hyperedge replacement grammar solution, (2015) uses syntaxbased machine translation techniques to create tree structures similar to AMR, while (2015) creates logical form representations of sentences and then converts these to AMR."
P17-1043,K15-1004,0,0.314627,"Missing"
P17-1043,P15-1109,0,0.0162258,"Missing"
P17-1043,D14-1162,0,0.111457,"eatures All input features for the five networks correspond to the sequence of words in the input sentence, and are presented to the networks as indices into lookup tables. With the exception of pre-trained word embeddings, these lookup tables are randomly initialized prior to training and representations are created during the training process. Detailed Parser Architecture 4.2.1 AMR Spans, Subgraphs, and Subgraph Decoding Word Embeddings The use of distributed word representations generated from large text corpora is pervasive in modern NLP. We start with 300 dimension GloVe representations (Pennington et al., 2014) trained on the 840 billion word common crawl (Smith et al., 2013). We added two binary dimensions: one for out of vocabulary words, and one for padding, resulting in vectors with a width of 302. These embeddings are mapped from the words in the sentence, and are then trained using back propagation just like other parameters in the network. Mapping the words in a sentence to AMR concepts is a critical first step in the parsing process, and can influence the performance of all subsequent processing. Although the most common mapping is one word to one concept, a series of consecutive words, or s"
P17-1043,D14-1048,0,0.137418,"Missing"
P17-1043,D15-1136,0,0.261887,"Missing"
P17-1043,P11-1138,0,0.0041543,"h the words and reference AMR graphs, to identify a subgraph type to associate with each span. Each word in the sentence is then associated with an IOBES subgraph type tag. We call the algorithm which defines span 4.2.2 Wikifier The AMR standard was expanded to include the annotation of named entities with a canonical form, using Wikipedia as the standard (see France in Figure 1a). The wiki link associated with this ”wikification” is expressed using the :wiki attribute, which requires some kind of global external knowledge of the Wikipedia ontology. We use the University of Illinois Wikifier (Ratinov et al., 2011; Cheng and Roth, 2013) to identify the :link directly, and use the possible categories output from the wikifier as feature inputs to the NCat Network. 466 Sentence Alignment AMR AMR Expert Span Identifier Alignment Expert Span Identifier Subgraph Spans Expert Subgraph Spans Subgraph Expander Sentence Feature Extraction Word Features UofI Wikifier NER SG Compare Subgraph Accuracy Predicted Subgraph Spans Backpropagation cross entropy (a) Expert System and Subgraph Expander Development. The alignment between the words in the sentence and elements of the AMR is provided by an automatic aligner."
P17-1043,P13-1135,0,0.00493305,"ence of words in the input sentence, and are presented to the networks as indices into lookup tables. With the exception of pre-trained word embeddings, these lookup tables are randomly initialized prior to training and representations are created during the training process. Detailed Parser Architecture 4.2.1 AMR Spans, Subgraphs, and Subgraph Decoding Word Embeddings The use of distributed word representations generated from large text corpora is pervasive in modern NLP. We start with 300 dimension GloVe representations (Pennington et al., 2014) trained on the 840 billion word common crawl (Smith et al., 2013). We added two binary dimensions: one for out of vocabulary words, and one for padding, resulting in vectors with a width of 302. These embeddings are mapped from the words in the sentence, and are then trained using back propagation just like other parameters in the network. Mapping the words in a sentence to AMR concepts is a critical first step in the parsing process, and can influence the performance of all subsequent processing. Although the most common mapping is one word to one concept, a series of consecutive words, or span, can also be associated with an AMR concept. Likewise, a span"
P17-1043,S10-1071,0,0.00764896,"Missing"
P17-1043,S10-1010,0,\N,Missing
S07-1024,S07-1012,0,0.214448,"Missing"
S07-1024,D07-1020,1,0.416226,"eature vector and a Full token feature vector. Both of them may contain URL and TTRP tokens. Given feature vectors, we need to find a way to learn the similarity matrix. Here, we choose the standard TF-IDF method to calculate the similarity matrix. 2.2.2 Phrase-based features Since considerable information related to the ambiguous object resides in the noun phrases in a web page, such as the person’s job and the person’s location, we attempt to capture this noun phrase information. The following section briefly describes how to extract and use the noun phrase information. For more detail, see Chen & Martin (2007). Contextual base noun phrase feature: With the syntactic phrase chunker, we extract all base noun phrases (non-overlapping syntactic phrases) occurring in the local sentences, which usually include some useful information about the ambiguous object. A base noun phrase of interest serves as an element in the feature vector. Document named-entity feature: Given the EXERT system, a direct and simple way to use the semantic information is to extract all named 127 entities in a web page. Since a given entity can be represented by many mentions in a document, we choose a single representative menti"
S07-1024,W03-0405,0,0.208424,"er Markham,” “Markham, Alexander,” and “Alexander .. Markham” (“..” can match a middle name). Web pages without any mention of the ambiguous personal name of interest are discarded and receive no further processing. Since it is common for a single document to contain one or more mentions of the ambiguous personal name of interest, there is a need to define the object to be disambiguated. Here, we adopt the policy of “one person per document” (all mentions of the ambiguous personal name in one web page are assumed to refer to the same personal entity in reality) as in Bagga & Baldwin (1998), Mann & Yarowsky (2003) and Gooi & Allan (2004). We therefore define an object as a single entity with the ambiguous personal name in a given web page. This definition of the object (document-level object) might be mistaken, because the mentions of the ambiguous personal name in a web page may refer to multiple entities, but we found that this is a rare case (most of those cases occur in genealogy web pages). On the other hand, a document-level object can include much information derived from that web page, so that it can be represented by rich features. Given this definition of an object, we define a target entity"
S07-1024,N04-1002,0,\N,Missing
S07-1024,P98-1012,0,\N,Missing
S07-1024,C98-1012,0,\N,Missing
S07-1024,H05-1048,1,\N,Missing
S07-1025,N01-1025,0,0.0223916,"the Iran-Iraq [EVENT war]. If Task B said (war BEFORE 08−15−90) then since 08−15−90=1990−08−15=today, the relation (war BEFORE today) must hold. 3 Models Using the features described in the previous section, each temporal relation — an event paired with a time or another event — was translated into a set of feature values. Pairing those feature values with the TempEval labels (BEFORE, AFTER, etc.) we trained a statistical classifier for each task. We chose support vector machines3 (SVMs) for our classifiers as they have shown good performance on a variety of natural language processing tasks (Kudo and Matsumoto, 2001; Pradhan et al., 2005). Using cross-validations on the training data, we performed a simple feature selection where any feature whose removal improved the cross-validation F-score was discarded. The resulting features for each task are listed in Table 1. After feature selection, we set the SVM free parameters, e.g. the kernel degree and cost of misclassification, by performing additional cross-validations on the training data, and selecting the model parameters which yielded the highest F-score for each task4 . 3 We used the TinySVM implementation from http://chasen.org/%7Etaku/software/TinyS"
S07-1025,P06-1095,0,0.125556,"Missing"
S07-1025,S07-1014,0,0.164462,"sider a sentence like: (1) The top commander of a Cambodian resistance force said Thursday he has sent a team to recover the remains of a British mine removal expert kidnapped and presumed killed by Khmer Rouge guerrillas almost two years ago. English speakers immediately recognize that kidnapping came first, then sending, and finally saying, even though before and after never appeared in the text. How can machines learn to do the same? The 2007 TempEval competition tries to address this question by establishing a common corpus on which research systems can compete to find temporal relations (Verhagen et al., 2007). TempEval considers the following types of event-time temporal relations: In each of these tasks, systems attempt to annotate pairs with one of the following relations: BEFORE, BEFORE - OR - OVERLAP , OVERLAP, OVERLAP - OF AFTER , AFTER or VAGUE. Competing systems are instructed to find all temporal relations of these types in a corpus of newswire documents. We approach these tasks as pair-wise classification problems, where each event/time pair is assigned one of the TempEval relation classes (BEFORE, AFTER, etc.). Event/time pairs are encoded using syntactically and semantically motivated f"
S15-1013,S10-1041,0,0.199113,"Missing"
S15-1013,W03-1028,0,0.782863,"Missing"
S15-1013,S10-1036,0,0.0235026,"re comprised of the longest n-grams that do not 118 contain a stop word or punctuation mark, occur for the first time within the first 400 words of the document and have a term frequency above a minimum threshold determined by document length. KP-Miner also boosts the weights of multiword candidates in proportion to the ratio of the frequencies of single word candidates to all candidates. In a reranking step, the tf-idf of each term is recalculated based on the number of times it is subsumed by other candidates in the top 15 candidates list. Another tf-idf based unsupervised system is KX-FBK (Pianta & Tonelli, 2010) which uses some of the same heuristics as KP-Miner but with different formulations and was shown to underperform in comparison in the Semeval 2010 keyphrase extraction task. An approach fundamentally different from tf-idf and its family of algorithms is TextRank. It is based on the intuition that 1) keywords in a document are more semantically interrelated as they are generally about related topics and 2) that semantic relatedness can be estimated using cooccurrence relations. Therefore in TextRank a graphical representation of the text is constructed in which edges connect words co-occurring"
S15-1013,S10-1004,0,0.685407,"Missing"
S15-1013,Y09-2035,0,0.0200109,"n TextRank a graphical representation of the text is constructed in which edges connect words co-occurring in a window of a certain length. The PageRank algorithm is then applied to this network of words to distinguish the important ones which are then reassembled into phrases wherever they occur next to each other in the text. TopicRank (Bougouin et al., 2013) which to the best of our knowledge is the state of the art in graph-based keyphrase extraction, is an enhancement of TextRank. Here, nodes represent topics which consist of sets of candidate terms clustered around shared sub-terms. In (Liang et al., 2009) Chinese search engine query logs are used to extract candidate terms which are used as nodes in the graph. Edges are weighted based on cooccurrence count. Also candidate terms which are longer or whose first occurrence is in the title or first paragraph have boosted edge weights. SingleRank (Wan & Xiao, 2008) also uses cooccurrence counts as edge weights. It ranks noun phrases in the text based on the sum of their word weights. ExpandRank (Wan & Xiao, 2008) builds upon SingleRank by incorporating neighboring documents but without significant performance improvements (Hasan & NG, 2010). 3 Meth"
S15-1013,D09-1137,0,0.133237,"Missing"
S15-1013,W04-3252,0,\N,Missing
S15-1013,C10-2042,0,\N,Missing
S15-1033,W04-2412,0,0.177277,"Missing"
S15-1033,W05-0620,0,0.365669,"Missing"
S15-1033,C14-1008,0,0.0652808,"Missing"
S15-1033,J02-3001,0,0.562652,"Missing"
S15-1033,W09-1201,0,0.2492,"Missing"
S15-1033,W14-1501,0,0.0226511,"Missing"
S15-1033,N13-1090,0,0.0142949,"Missing"
S15-1033,J05-1004,0,0.700424,"he benefits of adding increasingly more complex dependency-based features to the system; results are presented for both in-domain and out-of-domain datasets. 1 Introduction Semantic role labeling (Gildea and Jurafsky [2002]), the task of identifying and classifying the semantic arguments of verbal and nominal predicates in text, represents one of the most complex NLP tasks to be addressed by supervised machine learning techniques. In the standard supervised approach to building SRL systems, collections of multiway classifiers are trained using annotated corpora such as PropBank (Palmer et al. [2005]). In this approach, classifiers are trained using features derived directly from the original source text, as well as from subsequent syntactic and semantic processing. As reported in several shared tasks (Carreras and M`arquez [2004],Carreras and M`arquez [2005],Hajiˇc et al. [2009]), SRL systems trained in this manner can achieve high performance. State-of-the-art systems employ classifiers such as support vector machines trained with large numbers of relatively complex combinations of features, often combined with 279 James H. Martin Department of Computer Science and Institute of Cognitiv"
S15-1033,D13-1170,0,0.00650918,"Missing"
S15-1033,W13-3512,0,\N,Missing
S16-1185,D15-1198,0,0.0765011,"rative, greedy algorithm to compute the set of relations in the AMR. Another two 1 2 James H. Martin Department of Computer Science and Institute of Cognitive Science University of Colorado Boulder, CO 80309 James.Martin@colorado.edu http://amr.isi.edu/language.html http://alt.qcri.org/semeval2016/task8/# Related Work Most current AMR parsers assume input that has undergone varying degrees of syntactic analysis, ranging from simple part-of-speech tagging to more complex dependency or phrase-structure analysis. (Wang et al., 2015; Vanderwende et al., 2015; Peng et al., 2015; Pust et al., 2015; Artzi et al., 2015; Flanigan et al., 2014; Werling et al., 2015). In contrast, we follow the spirit of minimal feature extraction using pre-trained word embeddings, as in (Collobert et al., 2011) and a recurrent network architecture similar to that described in (Zhou and Xu, 2015). 3 3.1 System Architecture Feature Extraction In our system, all features are represented by embedding vectors, trained and stored in lookup tables. Word feature embeddings are mapped from the words in the sentence, and are trained with back propagation just like other parameters in the network. They are initialized with vectors which"
S16-1185,D13-1184,0,0.039268,"Missing"
S16-1185,P14-1134,0,0.167588,"ithm to compute the set of relations in the AMR. Another two 1 2 James H. Martin Department of Computer Science and Institute of Cognitive Science University of Colorado Boulder, CO 80309 James.Martin@colorado.edu http://amr.isi.edu/language.html http://alt.qcri.org/semeval2016/task8/# Related Work Most current AMR parsers assume input that has undergone varying degrees of syntactic analysis, ranging from simple part-of-speech tagging to more complex dependency or phrase-structure analysis. (Wang et al., 2015; Vanderwende et al., 2015; Peng et al., 2015; Pust et al., 2015; Artzi et al., 2015; Flanigan et al., 2014; Werling et al., 2015). In contrast, we follow the spirit of minimal feature extraction using pre-trained word embeddings, as in (Collobert et al., 2011) and a recurrent network architecture similar to that described in (Zhou and Xu, 2015). 3 3.1 System Architecture Feature Extraction In our system, all features are represented by embedding vectors, trained and stored in lookup tables. Word feature embeddings are mapped from the words in the sentence, and are trained with back propagation just like other parameters in the network. They are initialized with vectors which are pre-trained on lar"
S16-1185,K15-1004,0,0.0507507,"ncepts are then connected using an iterative, greedy algorithm to compute the set of relations in the AMR. Another two 1 2 James H. Martin Department of Computer Science and Institute of Cognitive Science University of Colorado Boulder, CO 80309 James.Martin@colorado.edu http://amr.isi.edu/language.html http://alt.qcri.org/semeval2016/task8/# Related Work Most current AMR parsers assume input that has undergone varying degrees of syntactic analysis, ranging from simple part-of-speech tagging to more complex dependency or phrase-structure analysis. (Wang et al., 2015; Vanderwende et al., 2015; Peng et al., 2015; Pust et al., 2015; Artzi et al., 2015; Flanigan et al., 2014; Werling et al., 2015). In contrast, we follow the spirit of minimal feature extraction using pre-trained word embeddings, as in (Collobert et al., 2011) and a recurrent network architecture similar to that described in (Zhou and Xu, 2015). 3 3.1 System Architecture Feature Extraction In our system, all features are represented by embedding vectors, trained and stored in lookup tables. Word feature embeddings are mapped from the words in the sentence, and are trained with back propagation just like other parameters in the network."
S16-1185,D15-1136,0,0.0609994,"nected using an iterative, greedy algorithm to compute the set of relations in the AMR. Another two 1 2 James H. Martin Department of Computer Science and Institute of Cognitive Science University of Colorado Boulder, CO 80309 James.Martin@colorado.edu http://amr.isi.edu/language.html http://alt.qcri.org/semeval2016/task8/# Related Work Most current AMR parsers assume input that has undergone varying degrees of syntactic analysis, ranging from simple part-of-speech tagging to more complex dependency or phrase-structure analysis. (Wang et al., 2015; Vanderwende et al., 2015; Peng et al., 2015; Pust et al., 2015; Artzi et al., 2015; Flanigan et al., 2014; Werling et al., 2015). In contrast, we follow the spirit of minimal feature extraction using pre-trained word embeddings, as in (Collobert et al., 2011) and a recurrent network architecture similar to that described in (Zhou and Xu, 2015). 3 3.1 System Architecture Feature Extraction In our system, all features are represented by embedding vectors, trained and stored in lookup tables. Word feature embeddings are mapped from the words in the sentence, and are trained with back propagation just like other parameters in the network. They are initialize"
S16-1185,P11-1138,0,0.105915,"Missing"
S16-1185,N15-3006,0,0.0419803,"tion probabilities. All concepts are then connected using an iterative, greedy algorithm to compute the set of relations in the AMR. Another two 1 2 James H. Martin Department of Computer Science and Institute of Cognitive Science University of Colorado Boulder, CO 80309 James.Martin@colorado.edu http://amr.isi.edu/language.html http://alt.qcri.org/semeval2016/task8/# Related Work Most current AMR parsers assume input that has undergone varying degrees of syntactic analysis, ranging from simple part-of-speech tagging to more complex dependency or phrase-structure analysis. (Wang et al., 2015; Vanderwende et al., 2015; Peng et al., 2015; Pust et al., 2015; Artzi et al., 2015; Flanigan et al., 2014; Werling et al., 2015). In contrast, we follow the spirit of minimal feature extraction using pre-trained word embeddings, as in (Collobert et al., 2011) and a recurrent network architecture similar to that described in (Zhou and Xu, 2015). 3 3.1 System Architecture Feature Extraction In our system, all features are represented by embedding vectors, trained and stored in lookup tables. Word feature embeddings are mapped from the words in the sentence, and are trained with back propagation just like other paramete"
S16-1185,N15-1040,0,0.111083,"rks to compute relation probabilities. All concepts are then connected using an iterative, greedy algorithm to compute the set of relations in the AMR. Another two 1 2 James H. Martin Department of Computer Science and Institute of Cognitive Science University of Colorado Boulder, CO 80309 James.Martin@colorado.edu http://amr.isi.edu/language.html http://alt.qcri.org/semeval2016/task8/# Related Work Most current AMR parsers assume input that has undergone varying degrees of syntactic analysis, ranging from simple part-of-speech tagging to more complex dependency or phrase-structure analysis. (Wang et al., 2015; Vanderwende et al., 2015; Peng et al., 2015; Pust et al., 2015; Artzi et al., 2015; Flanigan et al., 2014; Werling et al., 2015). In contrast, we follow the spirit of minimal feature extraction using pre-trained word embeddings, as in (Collobert et al., 2011) and a recurrent network architecture similar to that described in (Zhou and Xu, 2015). 3 3.1 System Architecture Feature Extraction In our system, all features are represented by embedding vectors, trained and stored in lookup tables. Word feature embeddings are mapped from the words in the sentence, and are trained with back propagatio"
S16-1185,P15-1095,0,0.156336,"of relations in the AMR. Another two 1 2 James H. Martin Department of Computer Science and Institute of Cognitive Science University of Colorado Boulder, CO 80309 James.Martin@colorado.edu http://amr.isi.edu/language.html http://alt.qcri.org/semeval2016/task8/# Related Work Most current AMR parsers assume input that has undergone varying degrees of syntactic analysis, ranging from simple part-of-speech tagging to more complex dependency or phrase-structure analysis. (Wang et al., 2015; Vanderwende et al., 2015; Peng et al., 2015; Pust et al., 2015; Artzi et al., 2015; Flanigan et al., 2014; Werling et al., 2015). In contrast, we follow the spirit of minimal feature extraction using pre-trained word embeddings, as in (Collobert et al., 2011) and a recurrent network architecture similar to that described in (Zhou and Xu, 2015). 3 3.1 System Architecture Feature Extraction In our system, all features are represented by embedding vectors, trained and stored in lookup tables. Word feature embeddings are mapped from the words in the sentence, and are trained with back propagation just like other parameters in the network. They are initialized with vectors which are pre-trained on large corpora of english t"
S16-1185,P15-1109,0,0.235239,"tml http://alt.qcri.org/semeval2016/task8/# Related Work Most current AMR parsers assume input that has undergone varying degrees of syntactic analysis, ranging from simple part-of-speech tagging to more complex dependency or phrase-structure analysis. (Wang et al., 2015; Vanderwende et al., 2015; Peng et al., 2015; Pust et al., 2015; Artzi et al., 2015; Flanigan et al., 2014; Werling et al., 2015). In contrast, we follow the spirit of minimal feature extraction using pre-trained word embeddings, as in (Collobert et al., 2011) and a recurrent network architecture similar to that described in (Zhou and Xu, 2015). 3 3.1 System Architecture Feature Extraction In our system, all features are represented by embedding vectors, trained and stored in lookup tables. Word feature embeddings are mapped from the words in the sentence, and are trained with back propagation just like other parameters in the network. They are initialized with vectors which are pre-trained on large corpora of english text, we use the word embeddings from (Collobert et al., 2011). The only explicit features not derived from the raw input are features based on named entity recognition (NER). We first use the Univ. of Illinois Wikifie"
W04-2416,W04-2412,0,0.211539,"Missing"
W04-2416,W95-0107,0,0.0497133,"(i.e. base phrases) instead of words or the constituents derived from syntactic trees. This system is referred to as the phraseby-phrase (P-by-P) semantic role classifier. We participate in the “closed challenge” of the CoNLL-2004 shared task and report results on both development and test sets. A detailed description of the task, data and related work can be found in (Carreras and M`arquez, 2004). 2 System Description 2.1 Data Representation In this paper, we change the representation of the original data as follows: • Bracketed representation of roles is converted into IOB2 representation (Ramhsaw and Marcus, 1995; Sang and Veenstra, 1995) • Word tokens are collapsed into base phrase (BP) tokens. Since the semantic annotation in the PropBank corpus does not have any embedded structure there is no loss of information in the first change. However, this results in a simpler representation with a reduced set of tagging labels. In the second change, it is possible to miss some information in cases where the semantic chunks do not align with the sequence of BPs. However, in Section 3.2 we show that the loss in performance due to the misalignment is much less than the gain in performance that can be achieved"
W04-2416,E99-1023,0,0.098933,"Missing"
W04-2416,N04-1030,1,\N,Missing
W05-0634,W05-0620,0,0.305505,"Missing"
W05-0634,J02-3001,1,0.761681,"Missing"
W05-0634,N03-2009,1,0.749207,"tracted from the input. This approach is neutral to the particular set of labels used, and will learn to tag input according We have previously reported on using SVM classifiers for semantic role labeling. In this work, we formulate the semantic labeling problem as a multiclass classification problem using Support Vector Machine (SVM) classifiers. Some of these systems use features based on syntactic constituents produced by a Charniak parser (Pradhan et al., 2003; Pradhan et al., 2004) and others use only a flat syntactic representation produced by a syntactic chunker (Hacioglu et al., 2003; Hacioglu and Ward, 2003; Hacioglu, 2004; Hacioglu et al., 2004). The latter approach lacks the information provided by the hierarchical syntactic structure, and the former imposes a limitation that the possible candidate roles should be one of the nodes already present in the syntax tree. We found that, while the chunk based systems are very efficient and robust, the systems that use features based on full syntactic parses are generally more accurate. Analysis of the source of errors for the parse constituent based systems showed that incorrect parses were a major source of error. The syntactic parser did not produc"
W05-0634,W04-2416,1,0.861755,"neutral to the particular set of labels used, and will learn to tag input according We have previously reported on using SVM classifiers for semantic role labeling. In this work, we formulate the semantic labeling problem as a multiclass classification problem using Support Vector Machine (SVM) classifiers. Some of these systems use features based on syntactic constituents produced by a Charniak parser (Pradhan et al., 2003; Pradhan et al., 2004) and others use only a flat syntactic representation produced by a syntactic chunker (Hacioglu et al., 2003; Hacioglu and Ward, 2003; Hacioglu, 2004; Hacioglu et al., 2004). The latter approach lacks the information provided by the hierarchical syntactic structure, and the former imposes a limitation that the possible candidate roles should be one of the nodes already present in the syntax tree. We found that, while the chunk based systems are very efficient and robust, the systems that use features based on full syntactic parses are generally more accurate. Analysis of the source of errors for the parse constituent based systems showed that incorrect parses were a major source of error. The syntactic parser did not produce any constituent that corresponded to t"
W05-0634,kingsbury-palmer-2002-treebank,0,0.0855298,"Missing"
W05-0634,W00-0730,0,0.0185759,"erent syntactic views. Our goal is to preserve the robustness and flexibility of the segmentation of the phrase-based chunker, but to take advantage of features from full syntactic parses. We also want to combine features from different syntactic parses to gain additional robustness. To this end, we use features generated from a Charniak parser and a Collins parser, as supplied for the CoNLL-2005 closed task. 2 System Description We again formulate the semantic labeling problem as a multi-class classification problem using Support Vector Machine (SVM) classifiers. TinySVM1 along with YamCha2 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The general framework is to train separate semantic role labeling systems for each of the parse tree views, and then to use the role arguments output by these systems as additional features in a semantic role classifier using a flat syntactic view. The constituent based classifiers walk a syntactic parse tree and classify each node as N ULL (no role) or as one of the set of"
W05-0634,N01-1025,0,0.0421455,"goal is to preserve the robustness and flexibility of the segmentation of the phrase-based chunker, but to take advantage of features from full syntactic parses. We also want to combine features from different syntactic parses to gain additional robustness. To this end, we use features generated from a Charniak parser and a Collins parser, as supplied for the CoNLL-2005 closed task. 2 System Description We again formulate the semantic labeling problem as a multi-class classification problem using Support Vector Machine (SVM) classifiers. TinySVM1 along with YamCha2 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the O NE VS A LL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a N ULL class. The general framework is to train separate semantic role labeling systems for each of the parse tree views, and then to use the role arguments output by these systems as additional features in a semantic role classifier using a flat syntactic view. The constituent based classifiers walk a syntactic parse tree and classify each node as N ULL (no role) or as one of the set of semantic roles. Chunk base"
W05-0634,N04-1030,1,0.950041,"e in that sentence. Our approach is to use supervised machine learning classifiers to produce the role labels based on features extracted from the input. This approach is neutral to the particular set of labels used, and will learn to tag input according We have previously reported on using SVM classifiers for semantic role labeling. In this work, we formulate the semantic labeling problem as a multiclass classification problem using Support Vector Machine (SVM) classifiers. Some of these systems use features based on syntactic constituents produced by a Charniak parser (Pradhan et al., 2003; Pradhan et al., 2004) and others use only a flat syntactic representation produced by a syntactic chunker (Hacioglu et al., 2003; Hacioglu and Ward, 2003; Hacioglu, 2004; Hacioglu et al., 2004). The latter approach lacks the information provided by the hierarchical syntactic structure, and the former imposes a limitation that the possible candidate roles should be one of the nodes already present in the syntax tree. We found that, while the chunk based systems are very efficient and robust, the systems that use features based on full syntactic parses are generally more accurate. Analysis of the source of errors fo"
W05-0634,P05-1072,1,0.75393,"y the hierarchical syntactic structure, and the former imposes a limitation that the possible candidate roles should be one of the nodes already present in the syntax tree. We found that, while the chunk based systems are very efficient and robust, the systems that use features based on full syntactic parses are generally more accurate. Analysis of the source of errors for the parse constituent based systems showed that incorrect parses were a major source of error. The syntactic parser did not produce any constituent that corresponded to the correct segmentation for the semantic argument. In Pradhan et al. (2005), we reported on a first attempt to overcome this problem by combining semantic role labels produced from different syntactic parses. The hope is that the syntactic parsers will make different errors, and that combining their outputs will improve on 217 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 217–220, Ann Arbor, June 2005. 2005 Association for Computational Linguistics either system alone. This initial attempt used features from a Charniak parser, a Minipar parser and a chunk based parser. It did show some improvement from the combination,"
W05-0634,W95-0107,0,0.0248543,"The general framework is to train separate semantic role labeling systems for each of the parse tree views, and then to use the role arguments output by these systems as additional features in a semantic role classifier using a flat syntactic view. The constituent based classifiers walk a syntactic parse tree and classify each node as N ULL (no role) or as one of the set of semantic roles. Chunk based systems classify each base phrase as being the B(eginning) of a semantic role, I(nside) a semantic role, or O(utside) any semantic role (ie. N ULL). This is referred to as an IOB representation (Ramshaw and Marcus, 1995). The constituent level roles are mapped to the IOB representation used by the chunker. The IOB tags are then used as features for a separate base-phase semantic role labeler (chunker), in addition to the standard set of features used by the chunker. An n-fold cross-validation paradigm is used to train the constituent based role classifiers 1 2 http://chasen.org/˜taku/software/TinySVM/ http://chasen.org/˜taku/software/yamcha/ 218 and the chunk based classifier. For the system reported here, two full syntactic parsers were used, a Charniak parser and a Collins parser. Features were extracted by"
W05-0634,P03-1002,0,0.0642219,"Missing"
W05-0634,W04-3212,0,0.189369,"Missing"
W06-1618,W95-0107,0,0.0109457,"events and the temporal relations between these events. (1) Identifying which words and phrases are EVENTs, and (2) Identifying their semantic classes. The next section describes how we turn these tasks into machine learning problems. 4 1 Event Identification as Classification These examples are derived from (Pustejovsky, et. al. 2003b) 147 Word Event Label Event Semantic Class Label The company ’s sales force applauded the shake up . O O O O O B O B I O O O O O O B_I_ACTION O B_OCCURRENCE I_OCCURRENCE O Table 1: Event chunks for sentence (1) is (B)eginning, (I)nside or (O)utside of a chunk (Ramshaw & Marcus, 1995). So, for example, under this scheme, sentence (1) would have its words labeled as in Table 1. (1) The company’s sales force [EVENT(I_ACTION) applauded] the [EVENT(OCCURRENCE) shake up] The two columns of labels in Table 1 show how the class labels differ depending on our task. If we’re interested only in the simple event identification task, it’s sufficient to know that applauded and shake both begin events (and so have the label B), up is inside an event (and so has the label I), and all other words are outside events (and so have the label O). These labels are shown in the column labeled Ev"
W06-1618,H05-1088,0,0.171473,"Missing"
W06-1618,W02-2004,0,0.024117,"ast holds true for a time span much longer than the typical newswire document and would therefore not be labeled as an EVENT. In addition to identifying which words in the TimeBank are EVENTs, the TimeBank also provides a semantic class label for each EVENT. The possible labels include OCCURRENCE, PERCEPTION, REPORTING, ASPECTUAL, STATE, I_STATE, I_ACTION, and MODAL, and are described in more detail in (Pustejovsky, et. al. 2003a). We consider two tasks on this data: 3 We view event identification as a classification task using a word-chunking paradigm similar to that used by Carreras et. al. (2002). For each word in a document, we assign a label indicating whether the word is inside or outside of an event. We use the standard B-I-O formulation of the word-chunking task that augments each class label with an indicator of whether the given word Events in the TimeBank TimeBank (Pustejovsky, et. al. 2003b) consists of just under 200 documents containing 70,000 words; it is drawn from news texts from a variety of different domains, including newswire and transcribed broadcast news. These documents are annotated using the TimeML annotation scheme (Pustejovsky, et. al. 2003a), which aims to id"
W06-1618,N01-1025,0,0.01564,"ince there are no WordNet senses labeled in our data, we accept a word as falling into one of the above hierarchies if any of its senses fall into that hierarchy. 6 Classifier Parameters The features described in the previous section give us a way to provide the learning algorithm with the necessary information to make a classification decision. The next step is to convert our training data into sets of features, and feed these classification instances to the learning algorithm. For the learning task, we use the TinySVM6 support vector machine (SVM) implementation in conjunction with YamCha7 (Kudo & Matsumoto, 2001), a suite for general-purpose chunking. YamCha has a number of parameters that define how it learns. The first of these is the window width of the “sliding window” that it uses. 5 We also considered the reverse classifiers, which classified all words in the hierarchy as non-events and all words outside the hierarchy as events. 6 http://chasen.org/~taku/software/TinySVM/ 7 http://chasen.org/~taku/software/yamcha/ Word The company ’s sales force applauded The shake up . POS DT NN POS NNS NN VBD DT NN RP . Stem the company ’s sale force applaud the shake up . Label O O O O O B O B Table 2: A wind"
W06-1618,N04-1030,1,0.870127,"Missing"
W06-1618,W96-0213,0,\N,Missing
W06-1618,J00-4004,0,\N,Missing
W06-1618,M98-1002,0,\N,Missing
W08-0902,W05-1210,0,0.0616381,"Missing"
W08-0902,C04-1051,0,0.0287715,"Missing"
W08-0902,J02-3001,0,0.0149882,". 15400) of the students’ handwritten responses. 2.2 Knowledge Representation The ASK assessments included a reference answer for each of their constructed response questions. We decomposed these reference answers into low-level facets, roughly extracted from the relations in a syntactic dependency parse and a shallow semantic parse. However, we use the word facet to refer to any fine-grained component of the reference answer semantics. The decomposition is based closely on these well-established frameworks, since the representations have been shown to be learnable by automatic systems (c.f., Gildea and Jurafsky, 2002; Nivre et al., 2006). These facets are the basis for assessing learner answers. See (Nielsen et al., 2008b) for details on extracting the facets; here we simply sketch the makeup of the final assessed reference answer facets. Example 1 presents a reference answer from the Magnetism and Electricity module and illustrates the facets derived from its dependency parse (shown in Figure 1), along with their glosses. These facets represent the fine-grained knowledge the student is expected to address in their response. (1) (1a) (1a’) (1b) (1b’) (1c) (1c’) (1d) (1d’) (1e) (1e’) The brass ring would n"
W08-0902,nielsen-etal-2008-annotating,1,0.652847,"ference answer for each of their constructed response questions. We decomposed these reference answers into low-level facets, roughly extracted from the relations in a syntactic dependency parse and a shallow semantic parse. However, we use the word facet to refer to any fine-grained component of the reference answer semantics. The decomposition is based closely on these well-established frameworks, since the representations have been shown to be learnable by automatic systems (c.f., Gildea and Jurafsky, 2002; Nivre et al., 2006). These facets are the basis for assessing learner answers. See (Nielsen et al., 2008b) for details on extracting the facets; here we simply sketch the makeup of the final assessed reference answer facets. Example 1 presents a reference answer from the Magnetism and Electricity module and illustrates the facets derived from its dependency parse (shown in Figure 1), along with their glosses. These facets represent the fine-grained knowledge the student is expected to address in their response. (1) (1a) (1a’) (1b) (1b’) (1c) (1c’) (1d) (1d’) (1e) (1e’) The brass ring would not stick to the nail because the ring is not iron. NMod(ring, brass) The ring is brass. Theme_not(stick, r"
W08-0902,P08-2061,1,0.774294,"ference answer for each of their constructed response questions. We decomposed these reference answers into low-level facets, roughly extracted from the relations in a syntactic dependency parse and a shallow semantic parse. However, we use the word facet to refer to any fine-grained component of the reference answer semantics. The decomposition is based closely on these well-established frameworks, since the representations have been shown to be learnable by automatic systems (c.f., Gildea and Jurafsky, 2002; Nivre et al., 2006). These facets are the basis for assessing learner answers. See (Nielsen et al., 2008b) for details on extracting the facets; here we simply sketch the makeup of the final assessed reference answer facets. Example 1 presents a reference answer from the Magnetism and Electricity module and illustrates the facets derived from its dependency parse (shown in Figure 1), along with their glosses. These facets represent the fine-grained knowledge the student is expected to address in their response. (1) (1a) (1a’) (1b) (1b’) (1c) (1c’) (1d) (1d’) (1e) (1e’) The brass ring would not stick to the nail because the ring is not iron. NMod(ring, brass) The ring is brass. Theme_not(stick, r"
W08-0902,C04-1010,0,0.0242852,"Missing"
W08-0902,W06-2933,0,0.16605,"andwritten responses. 2.2 Knowledge Representation The ASK assessments included a reference answer for each of their constructed response questions. We decomposed these reference answers into low-level facets, roughly extracted from the relations in a syntactic dependency parse and a shallow semantic parse. However, we use the word facet to refer to any fine-grained component of the reference answer semantics. The decomposition is based closely on these well-established frameworks, since the representations have been shown to be learnable by automatic systems (c.f., Gildea and Jurafsky, 2002; Nivre et al., 2006). These facets are the basis for assessing learner answers. See (Nielsen et al., 2008b) for details on extracting the facets; here we simply sketch the makeup of the final assessed reference answer facets. Example 1 presents a reference answer from the Magnetism and Electricity module and illustrates the facets derived from its dependency parse (shown in Figure 1), along with their glosses. These facets represent the fine-grained knowledge the student is expected to address in their response. (1) (1a) (1a’) (1b) (1b’) (1c) (1c’) (1d) (1d’) (1e) (1e’) The brass ring would not stick to the nail"
W08-0902,J05-1004,0,0.0220798,"Missing"
W08-0902,W05-0202,0,0.0209779,"not. There is no indication of exactly what facets of the concept a student contradicted or failed to express. Furthermore, virtually all ITSs are developed in a very domain-specific way, with each new question requiring the handcrafting of new semantic extraction frames, parsers, logic representations, or knowledge-based ontologies (c.f., Graesser et al., 2001; Jordan et al., 2004; Peters et al., 2004; Roll et al., 2005; VanLehn et al., 2005). This is also true of research in the area of scoring constructed response questions (e.g., Callear et al., 2001; Leacock, 2004; Mitchell et al., 2002; Pulman and Sukkarieh, 2005). The present paper analyzes the errors of a system that was designed to address these limitations. Rather than have a single expressed versus notexpressed assessment of the reference answer as a whole, we instead break the reference answer down into what we consider to be approximately its lowest level compositional facets. This roughly translates to the set of triples composed of labeled (typed) dependencies in a dependency parse of the reference answer. Breaking the reference answer down into fine-grained facets permits a more focused assessment of the student’s response, but a simple yes o"
W09-2002,E06-1042,0,0.157995,"Missing"
W09-2002,W06-3506,0,0.188576,"Missing"
W09-2002,W07-0103,0,0.111579,"Missing"
W12-2002,C08-1023,1,0.848158,"Missing"
W12-2002,P11-2098,0,0.0402386,"Missing"
W12-2002,J10-2002,0,0.0449821,"Missing"
W12-2002,W07-1401,0,\N,Missing
