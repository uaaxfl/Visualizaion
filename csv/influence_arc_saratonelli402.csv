2020.findings-emnlp.84,S19-2007,0,0.0758306,"Missing"
2020.findings-emnlp.84,2020.semeval-1.201,1,0.797376,"Missing"
2020.findings-emnlp.84,N19-1423,0,0.241595,"we propose a hybrid emoji-based Masked Language Model (MLM) to leverage the common information conveyed by emojis across different languages and improve the learned cross-lingual representation of short text messages, with the goal to perform zeroshot abusive language detection. We compare the results obtained with the original MLM to the ones obtained by our method, showing improved performance on German, Italian and Spanish. 1 Introduction The extensive use of large-scale self-supervised pretraining has greatly contributed to recent progress in many Natural Language Processing (NLP) tasks (Devlin et al., 2019; Liu et al., 2019; Conneau and Lample, 2019). In this context, masked language modelling objectives represent one of the main novelties of these approaches, where some tokens of an input sequence are randomly masked, and the objective is to predict these masked positions taking the corrupted sequence as input. Still, little attention has been devoted to the adaptation of these techniques to tasks dealing with social media data, probably because they are characterized by a very domain-specific language, with high variability and instability. Nevertheless, all these challenges make social media"
2020.findings-emnlp.84,P07-2045,0,0.00717236,"Missing"
2020.findings-emnlp.84,W18-5113,0,0.0173731,"and Wiegand, 2017) and (Fortuna and Nunes, 2018)). 943 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 943–949 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Abusive language detection is usually framed as a supervised learning problem, built using a combination of manually crafted features such as n-grams (Wulczyn et al., 2017), syntactic features (Nobata et al., 2016), and linguistic features (Yin et al., 2009), to more recent neural networks (Park and Fung, 2017; Zhang and Tepper, 2018; Agrawal and Awekar, 2018; Corazza et al., 2018). (Lee et al., 2018) address a comparative study of various learning models on the Hate and Abusive Speech on Twitter dataset (Founta et al., 2018), while (Zampieri et al., 2019a) build the Offensive Language Identification Dataset and experiment with SVMs, BiLSTM and CNN both on the binary abusive language classification and on a more fine-grained categorization. Our work deals with the same task, addressed from a cross-lingual perspective. In recent years, some proposals have been made to tackle abusive language detection in a cross-lingual framework (Sohn and Lee, 2019; Pamungkas and Patti, 2019; Casula et al."
2020.findings-emnlp.84,2021.ccl-1.108,0,0.0992648,"Missing"
2020.findings-emnlp.84,D19-1474,0,0.0202797,"Missing"
2020.findings-emnlp.84,P19-2051,0,0.0304355,"Missing"
2020.findings-emnlp.84,W17-3006,0,0.0155339,"ches has been proposed to detect this kind of messages (for a survey on the task, see (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018)). 943 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 943–949 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Abusive language detection is usually framed as a supervised learning problem, built using a combination of manually crafted features such as n-grams (Wulczyn et al., 2017), syntactic features (Nobata et al., 2016), and linguistic features (Yin et al., 2009), to more recent neural networks (Park and Fung, 2017; Zhang and Tepper, 2018; Agrawal and Awekar, 2018; Corazza et al., 2018). (Lee et al., 2018) address a comparative study of various learning models on the Hate and Abusive Speech on Twitter dataset (Founta et al., 2018), while (Zampieri et al., 2019a) build the Offensive Language Identification Dataset and experiment with SVMs, BiLSTM and CNN both on the binary abusive language classification and on a more fine-grained categorization. Our work deals with the same task, addressed from a cross-lingual perspective. In recent years, some proposals have been made to tackle abusive language detecti"
2020.findings-emnlp.84,scheffler-2014-german,0,0.0728246,"Missing"
2020.findings-emnlp.84,W17-1101,0,0.0190024,"ents the experimental setup. Section 5 reports on the evaluation results, while Section 6 summarizes our findings. 2 Related work The focus of this paper is the abusive language detection task, which has been widely explored in the last years thanks to numerous datasets, approaches and shared tasks (Waseem et al., 2017; Fiˇser et al., 2018; Carmona et al., 2018; Wiegand et al., 2018; Bosco et al., 2018; Zampieri et al., 2019b; Roberts et al., 2019) covering different languages. An increasing number of approaches has been proposed to detect this kind of messages (for a survey on the task, see (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018)). 943 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 943–949 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Abusive language detection is usually framed as a supervised learning problem, built using a combination of manually crafted features such as n-grams (Wulczyn et al., 2017), syntactic features (Nobata et al., 2016), and linguistic features (Yin et al., 2009), to more recent neural networks (Park and Fung, 2017; Zhang and Tepper, 2018; Agrawal and Awekar, 2018; Corazza et al., 2018). (Lee et al., 2018)"
2020.findings-emnlp.84,P16-1162,0,0.0614193,"Missing"
2020.findings-emnlp.84,N16-2013,0,0.0648766,"Missing"
2020.findings-emnlp.84,N19-1060,0,0.0488008,"may have inherent characteristics that make it more challenging to classify for abusive language detection, for example the presence of compound words makes hashtag splitting more error-prone. On the other hand, the Germeval dataset was built by sampling data from specific users and avoiding keyword-based queries, so to obtain the highest possible variability in the offensive language. This led to the creation of a very challenging dataset, where lexical overlap between training and test data is limited and where hate speech is not associated with specific topics or keywords, as suggested in (Wiegand et al., 2019). 6 when used on social media data: first of all, when using emojis, the pre-training step is aimed at predicting tokens that are inherently more relevant for the final abusive language detection task whenever possible, as opposed to random tokens. Secondly, emojis convey similar meaning in the languages that we consider, serving as a common trait between languages during pre-training. We also use <emoji> tokens around emojis to help the system discriminate between the two training objectives when using HE–MLM. The proposed methods represent a novel contribution with respect to social media da"
2020.findings-emnlp.84,N19-1144,0,0.030501,"ther languages. In the following, Section 2 discusses the related work. Section 3 describes our approach to train cross-lingual models for social media data classification, while Section 4 presents the experimental setup. Section 5 reports on the evaluation results, while Section 6 summarizes our findings. 2 Related work The focus of this paper is the abusive language detection task, which has been widely explored in the last years thanks to numerous datasets, approaches and shared tasks (Waseem et al., 2017; Fiˇser et al., 2018; Carmona et al., 2018; Wiegand et al., 2018; Bosco et al., 2018; Zampieri et al., 2019b; Roberts et al., 2019) covering different languages. An increasing number of approaches has been proposed to detect this kind of messages (for a survey on the task, see (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018)). 943 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 943–949 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Abusive language detection is usually framed as a supervised learning problem, built using a combination of manually crafted features such as n-grams (Wulczyn et al., 2017), syntactic features (Nobata et al."
2020.findings-emnlp.84,S19-2010,0,0.0412612,"ther languages. In the following, Section 2 discusses the related work. Section 3 describes our approach to train cross-lingual models for social media data classification, while Section 4 presents the experimental setup. Section 5 reports on the evaluation results, while Section 6 summarizes our findings. 2 Related work The focus of this paper is the abusive language detection task, which has been widely explored in the last years thanks to numerous datasets, approaches and shared tasks (Waseem et al., 2017; Fiˇser et al., 2018; Carmona et al., 2018; Wiegand et al., 2018; Bosco et al., 2018; Zampieri et al., 2019b; Roberts et al., 2019) covering different languages. An increasing number of approaches has been proposed to detect this kind of messages (for a survey on the task, see (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018)). 943 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 943–949 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Abusive language detection is usually framed as a supervised learning problem, built using a combination of manually crafted features such as n-grams (Wulczyn et al., 2017), syntactic features (Nobata et al."
2020.gamnlp-1.6,S19-2007,0,0.0611069,"Missing"
2020.gamnlp-1.6,D17-2002,0,0.0696055,"involve users more easily and increase dissemination is a demanding yet interesting ground to explore. In this paper we present a 3D role-playing game for abusive language annotation that is currently under development. Keywords: games with a purpose, game design, linguistic annotation, abusive language 1. Introduction (Poesio et al., 2013), The Knowledge Towers (Vannella et al., 2014) and Puzzle Racer (Jurgens and Navigli, 2014) for concept-image linking, Infection (Vannella et al., 2014), OnToGalaxy (Krause et al., 2010) and JeuxDeMots (Joubert et al., 2018) for semantic linking, Argotario (Habernal et al., 2017) for fallacious argumentation identification, Zombilingo (Fort et al., 2014) for dependecy syntax anno¨ tation, Sentimentator (Ohman and Kajava, 2018) for sentiment annotation, Wordrobe (Venhuizen et al., 2013) and Ka-Boom! (Jurgens and Navigli, 2014) for sense annotation. Researchers stress the fact that GWAPs should be designed in such a way that they integrate the task without sacrificing their ‘gamefulness’, otherwise the tasks may be perceived as work (Vannella et al., 2014). Some of these games try to exploit disjoint design (Krause et al., 2010), i.e. a technique by which the goal of th"
2020.gamnlp-1.6,Q14-1035,0,0.636083,"Purpose (GWAPs) tend to lack important elements that we commonly see in commercial games, such as 2D and 3D worlds or a story. Making GWAPs more similar to full-fledged video games in order to involve users more easily and increase dissemination is a demanding yet interesting ground to explore. In this paper we present a 3D role-playing game for abusive language annotation that is currently under development. Keywords: games with a purpose, game design, linguistic annotation, abusive language 1. Introduction (Poesio et al., 2013), The Knowledge Towers (Vannella et al., 2014) and Puzzle Racer (Jurgens and Navigli, 2014) for concept-image linking, Infection (Vannella et al., 2014), OnToGalaxy (Krause et al., 2010) and JeuxDeMots (Joubert et al., 2018) for semantic linking, Argotario (Habernal et al., 2017) for fallacious argumentation identification, Zombilingo (Fort et al., 2014) for dependecy syntax anno¨ tation, Sentimentator (Ohman and Kajava, 2018) for sentiment annotation, Wordrobe (Venhuizen et al., 2013) and Ka-Boom! (Jurgens and Navigli, 2014) for sense annotation. Researchers stress the fact that GWAPs should be designed in such a way that they integrate the task without sacrificing their ‘gamefulne"
2020.gamnlp-1.6,W18-5107,1,0.733251,"el No Word level Yes Word level Yes Word level Yes Word level Yes Word level Yes Word level goal of the first one is to collect a set of abusive and not abusive sentences. The goal of the second task is to identify, in an abusive sentence, which expressions or words are offensive, so to have a fine-grained annotation of the sentence, isolating only the offensive strings. For both cases, the game takes in input a corpus of sentences that may contain abusive language, with the goal to annotate them. For our first experimentation, we use the Italian WhatsApp corpus of cyberbullying interactions (Sprugnoli et al., 2018), containing 10 chats for a total of 14,600 tokens. The messages had been manually annotated as offensive or not, and the semantic type of the offense was also specified (e.g. body shame, sexism, blackmail, etc.). For our game, the existing annotation has not been taken into account, but it can be used to check whether the information on offensive messages collected through the game matches the gold standard. Since users are exposed to vulgar language in this game, a disclamer is put at the beginning where they are informed about the potential harm. The input format for the game is rather stra"
2020.gamnlp-1.6,P14-1122,0,0.624449,"ll much has to be explored. Games with a Purpose (GWAPs) tend to lack important elements that we commonly see in commercial games, such as 2D and 3D worlds or a story. Making GWAPs more similar to full-fledged video games in order to involve users more easily and increase dissemination is a demanding yet interesting ground to explore. In this paper we present a 3D role-playing game for abusive language annotation that is currently under development. Keywords: games with a purpose, game design, linguistic annotation, abusive language 1. Introduction (Poesio et al., 2013), The Knowledge Towers (Vannella et al., 2014) and Puzzle Racer (Jurgens and Navigli, 2014) for concept-image linking, Infection (Vannella et al., 2014), OnToGalaxy (Krause et al., 2010) and JeuxDeMots (Joubert et al., 2018) for semantic linking, Argotario (Habernal et al., 2017) for fallacious argumentation identification, Zombilingo (Fort et al., 2014) for dependecy syntax anno¨ tation, Sentimentator (Ohman and Kajava, 2018) for sentiment annotation, Wordrobe (Venhuizen et al., 2013) and Ka-Boom! (Jurgens and Navigli, 2014) for sense annotation. Researchers stress the fact that GWAPs should be designed in such a way that they integrate"
2020.gamnlp-1.6,W13-0215,0,0.456736,"velopment. Keywords: games with a purpose, game design, linguistic annotation, abusive language 1. Introduction (Poesio et al., 2013), The Knowledge Towers (Vannella et al., 2014) and Puzzle Racer (Jurgens and Navigli, 2014) for concept-image linking, Infection (Vannella et al., 2014), OnToGalaxy (Krause et al., 2010) and JeuxDeMots (Joubert et al., 2018) for semantic linking, Argotario (Habernal et al., 2017) for fallacious argumentation identification, Zombilingo (Fort et al., 2014) for dependecy syntax anno¨ tation, Sentimentator (Ohman and Kajava, 2018) for sentiment annotation, Wordrobe (Venhuizen et al., 2013) and Ka-Boom! (Jurgens and Navigli, 2014) for sense annotation. Researchers stress the fact that GWAPs should be designed in such a way that they integrate the task without sacrificing their ‘gamefulness’, otherwise the tasks may be perceived as work (Vannella et al., 2014). Some of these games try to exploit disjoint design (Krause et al., 2010), i.e. a technique by which the goal of the player and the goal of the task are kept separate. In particular, in OnToGalaxy players control a spaceship and have to shoot other spaceships with a certain label that does not satisfy the condition expresse"
2020.lrec-1.532,bartolini-etal-2014-synsets,0,0.0289125,"few corpora in the political domain are available. Indeed according to LRE Map3 there are currently 24 monolingual corpora for Italian, two for the spoken language, i.e. VoLIP (Alfano et al., 2014) and the LUNA corpus (Dinarelli et al., 2009), and the other accounting for written documents. However, none of them pertains to the political domain. Furthermore, between the 286 multimodal resources certified for all the languages by the LRE map, only one is in Italian, IMAGACT, a corpus-based ontology of action concepts, derived from English and Italian spontaneous speech (Moneglia et al., 2014; Bartolini et al., 2014). So both from the political and the multimodal point of view, this language is not well represented. Although some studies related to corpus-based analysis of political discourse do exist also for Italian, they mainly deal with monological discourse (Bolasco et al., 2006; Cedroni, 2010; Longobardi, 2010; Catellani et al., 2010; Bongelli et al., 2010; Zurloni and Anolli, 2010; Sprugnoli et al., 2016; Moretti et al., 2016) and do not make the data available for further studies. Concerning political corpora developed specifically for conversation analysis, other languages have been more extensiv"
2020.lrec-1.532,W09-0505,1,0.666212,"riptions from UK and US government portals and personal foundation websites such as the White House portal, William J. Clinton Foundation, Margaret Thatcher Foundation. This has fostered research on political and media communication and persuasion strategies (Guerini et al., 2010; Esposito et al., 2015). As regards Italian, which is the language of interest for this study, only few corpora in the political domain are available. Indeed according to LRE Map3 there are currently 24 monolingual corpora for Italian, two for the spoken language, i.e. VoLIP (Alfano et al., 2014) and the LUNA corpus (Dinarelli et al., 2009), and the other accounting for written documents. However, none of them pertains to the political domain. Furthermore, between the 286 multimodal resources certified for all the languages by the LRE map, only one is in Italian, IMAGACT, a corpus-based ontology of action concepts, derived from English and Italian spontaneous speech (Moneglia et al., 2014; Bartolini et al., 2014). So both from the political and the multimodal point of view, this language is not well represented. Although some studies related to corpus-based analysis of political discourse do exist also for Italian, they mainly d"
2020.lrec-1.532,moneglia-etal-2014-imagact,0,0.0227023,"st for this study, only few corpora in the political domain are available. Indeed according to LRE Map3 there are currently 24 monolingual corpora for Italian, two for the spoken language, i.e. VoLIP (Alfano et al., 2014) and the LUNA corpus (Dinarelli et al., 2009), and the other accounting for written documents. However, none of them pertains to the political domain. Furthermore, between the 286 multimodal resources certified for all the languages by the LRE map, only one is in Italian, IMAGACT, a corpus-based ontology of action concepts, derived from English and Italian spontaneous speech (Moneglia et al., 2014; Bartolini et al., 2014). So both from the political and the multimodal point of view, this language is not well represented. Although some studies related to corpus-based analysis of political discourse do exist also for Italian, they mainly deal with monological discourse (Bolasco et al., 2006; Cedroni, 2010; Longobardi, 2010; Catellani et al., 2010; Bongelli et al., 2010; Zurloni and Anolli, 2010; Sprugnoli et al., 2016; Moretti et al., 2016) and do not make the data available for further studies. Concerning political corpora developed specifically for conversation analysis, other language"
2020.semeval-1.201,S19-2007,0,0.047937,"fensive, abusive and hateful thoughts on the Internet. Therefore, the task of identifying offensive language on social media has increasingly gained attention in recent years, since the manual identification and deletion of such messages can be very costly and time-consuming, given the ever-increasing quantity of user-generated content online. In order to foster the development of offensive speech detection systems, more and more shared tasks have been organized on the detection and identification of offensive language in the past few years, covering several languages (Zampieri et al., 2019b; Basile et al., 2019; Wiegand et al., 2018; Bosco et al., 2018). In this paper, we present our submission to SemEval 2020 task 12, Multilingual Offensive Language Identification in Social Media (OffensEval 2) (Zampieri et al., 2020). We participated in subtask A, focused on the identification of offensive messages in 5 languages: English, Danish, Turkish, Arabic and Greek. In the SemEval 2019 edition of OffensEval, the task was focused on Offensive Language Identification in English (Zampieri et al., 2019a). The vast majority of the teams who obtained the highest macro-F1 scores in subtask A (focused on the binar"
2020.semeval-1.201,N19-1423,0,0.225405,"fensive Language Identification in Social Media (OffensEval 2) (Zampieri et al., 2020). We participated in subtask A, focused on the identification of offensive messages in 5 languages: English, Danish, Turkish, Arabic and Greek. In the SemEval 2019 edition of OffensEval, the task was focused on Offensive Language Identification in English (Zampieri et al., 2019a). The vast majority of the teams who obtained the highest macro-F1 scores in subtask A (focused on the binary identification of offensive language) used systems based on Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). Since the 2020 edition of the task covers five languages, we aim at improving the performance of a multilingual pre-trained BERT system by using transfer learning. This is achieved by artificially creating more data for fine-tuning two parallel pre-trained BERT models by translating existing corpora in a different language and feeding the parallel data to a multiple input model. 2 Related Work The increased interest in the detection of offensive language on social media has entailed an increase in shared tasks on the topic (Wiegand et al., 2018; Bosco et al., 2018; Zampieri et al., 2019b), a"
2020.semeval-1.201,2020.lrec-1.629,0,0.0210559,"Missing"
2020.semeval-1.201,W17-1101,0,0.0255956,"data being annotated for this task (Waseem and Hovy, 2016; Davidson et al., 2017; Golbeck et This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/. 1539 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1539–1545 Barcelona, Spain (Online), December 12, 2020. License details: al., 2017; Founta et al., 2018). The topic of offensive language detection has been explored in relation to different subtasks, such as the detection of cyberbullying, hate speech, abusive language, aggression, and more (Schmidt and Wiegand, 2017; Sprugnoli et al., 2018; Zampieri et al., 2019b). In general, the methods used for identifying offensive language follow supervised learning approaches. While support vector machines perform well on the task (Schmidt and Wiegand, 2017), deep learning approaches are becoming increasingly popular (Corazza et al., 2020), in particular transformer-based ones (Zampieri et al., 2019b). More specifically, in the 2019 instance of the OffensEval task, 7 out of the 10 best performing systems used BERT-based approaches (Zampieri et al., 2019b; Devlin et al., 2019). BERT models are typically pre-trained"
2020.semeval-1.201,2020.lrec-1.430,0,0.036013,"Missing"
2020.semeval-1.201,W18-5107,1,0.732124,"is task (Waseem and Hovy, 2016; Davidson et al., 2017; Golbeck et This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/. 1539 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1539–1545 Barcelona, Spain (Online), December 12, 2020. License details: al., 2017; Founta et al., 2018). The topic of offensive language detection has been explored in relation to different subtasks, such as the detection of cyberbullying, hate speech, abusive language, aggression, and more (Schmidt and Wiegand, 2017; Sprugnoli et al., 2018; Zampieri et al., 2019b). In general, the methods used for identifying offensive language follow supervised learning approaches. While support vector machines perform well on the task (Schmidt and Wiegand, 2017), deep learning approaches are becoming increasingly popular (Corazza et al., 2020), in particular transformer-based ones (Zampieri et al., 2019b). More specifically, in the 2019 instance of the OffensEval task, 7 out of the 10 best performing systems used BERT-based approaches (Zampieri et al., 2019b; Devlin et al., 2019). BERT models are typically pre-trained on large unannotated cor"
2020.semeval-1.201,N16-2013,0,0.0382618,"nguages, we aim at improving the performance of a multilingual pre-trained BERT system by using transfer learning. This is achieved by artificially creating more data for fine-tuning two parallel pre-trained BERT models by translating existing corpora in a different language and feeding the parallel data to a multiple input model. 2 Related Work The increased interest in the detection of offensive language on social media has entailed an increase in shared tasks on the topic (Wiegand et al., 2018; Bosco et al., 2018; Zampieri et al., 2019b), as well as more data being annotated for this task (Waseem and Hovy, 2016; Davidson et al., 2017; Golbeck et This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/. 1539 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1539–1545 Barcelona, Spain (Online), December 12, 2020. License details: al., 2017; Founta et al., 2018). The topic of offensive language detection has been explored in relation to different subtasks, such as the detection of cyberbullying, hate speech, abusive language, aggression, and more (Schmidt and Wiegand, 2017; Sprugnoli et al., 2018; Zampie"
2020.semeval-1.201,W16-5618,0,0.0607046,"Missing"
2020.semeval-1.201,N19-1144,0,0.0756831,"Missing"
2020.semeval-1.201,S19-2010,0,0.0443938,"Missing"
2021.emnlp-main.822,N13-1132,0,0.0374426,"riments showing that selecting training and test data according to different levels of annotators’ agreement has a strong effect on classifiers performance and robustness. Our findings are further validated in cross-domain experiments and studied using a popular benchmark dataset. We show that such hard cases, where low agreement is present, are not necessarily due to poor-quality annotation and we advocate for a higher presence of ambiguous cases in future datasets, particularly in test sets, to better account for the different points of view expressed online. 1 Introduction and reliability (Hovy et al., 2013), trying to identify spammers and mitigate their effect on annotation, or by repeating labeling on targeted examples (Sheng et al., 2008). However, not all tasks are the same: while in some cases, like for instance PoStagging or parsing, disagreement among annotators is more likely due to unclear annotation guidelines and can usually be reconciled through adjudication, full annotators’ agreement should not be necessarily enforced in social computing tasks, whose goal is to study and manage social behavior and organizational dynamics, especially in virtual worlds built over the Internet (Wang,"
2021.emnlp-main.822,P16-2096,0,0.0295192,"ture through a language model. of each class of agreement, we projected the distribution of This corpus, together with the experiments preannotators’ agreement level for each ensemble class, using the confusion matrix reported in Figure 1. sented in this paper, will hopefully shed light onto 10535 the important role played by annotators’ disagreement, something that we need to understand better and to see as a novel perspective on data. Indeed, if we want to include diversity in the process of data creation and reduce both the exclusion of minorities’ voices and demographic misrepresentation (Hovy and Spruit, 2016), disagreement should be seen as a signal and not as noise. 7 Ethics Statement The tweets in this dataset have been annotated by crowd-workers using Amazon Mechanical Turk. All requirements introduced by the platform for tasks containing adult content were implemented, for example adding a warning in the task title. We further avoid to put any constraints on the minimum length of sessions or on the minimum amount of data to be labeled by each crowd-worker, therefore they were not forced to prolonged exposure to offensive content. Indeed, we observed that crowdworkers tended to annotate for sho"
2021.emnlp-main.822,W09-1904,0,0.068763,"d for. NOTE: This paper contains examples of language which may be offensive to some readers. They do not represent the views of the authors. 2 Related Work While there has been an extensive discussion on minimal standards for inter-annotator agreement to ensure data quality (Di Eugenio and Glass, 2004; Passonneau, 2004; Artstein and Poesio, 2008), recently an increasing number of works argue that disagreement is unavoidable because language is inherently ambiguous (Aroyo and Welty, 2015), proposing ways to tackle annotators’ disagreement when building training sets (Dumitrache et al., 2019). Hsueh et al. (2009), for example, identify a set of criteria to select informative yet unambiguous examples for predictive modeling in a sentiment classification task. Rehbein and Ruppenhofer (2011) analyse the impact that annotation noise can have on active learning approaches. Other works along this line investigate the impact of uncertain or difficult instances on supervised classification (Peterson et al., 2019), while Beigman Klebanov and Beigman (2014) show that including hard cases in training data results in poorer classification of easy data in a word classification task. Along the same lines, Jamison a"
2021.emnlp-main.822,D15-1035,0,0.149418,"l. (2009), for example, identify a set of criteria to select informative yet unambiguous examples for predictive modeling in a sentiment classification task. Rehbein and Ruppenhofer (2011) analyse the impact that annotation noise can have on active learning approaches. Other works along this line investigate the impact of uncertain or difficult instances on supervised classification (Peterson et al., 2019), while Beigman Klebanov and Beigman (2014) show that including hard cases in training data results in poorer classification of easy data in a word classification task. Along the same lines, Jamison and Gurevych (2015) show that filtering instances with low agreement improve classifier performance in four out of five tasks. Both works observe that the presence of such instances lead to misclassifications. Several approaches have been presented that implement strategies to deal with disagreement when training classifiers for diverse tasks. In most cases, disagreement has been treated as a consequence of low annotation quality, and addressed through 1 See Ethics Statement section for further details methodologies aimed at minimising the effects of noisy crowdsourced data. Simpson et al. (2020), for example, p"
2021.emnlp-main.822,passonneau-2004-computing,0,0.104724,"ge, this represents the first dataset explicitly created to cover different agreement levels in a balanced way.We also advocate for the release of more datasets like the one we propose, especially for highly subjective tasks, where the need to include different points of view should be accounted for. NOTE: This paper contains examples of language which may be offensive to some readers. They do not represent the views of the authors. 2 Related Work While there has been an extensive discussion on minimal standards for inter-annotator agreement to ensure data quality (Di Eugenio and Glass, 2004; Passonneau, 2004; Artstein and Poesio, 2008), recently an increasing number of works argue that disagreement is unavoidable because language is inherently ambiguous (Aroyo and Welty, 2015), proposing ways to tackle annotators’ disagreement when building training sets (Dumitrache et al., 2019). Hsueh et al. (2009), for example, identify a set of criteria to select informative yet unambiguous examples for predictive modeling in a sentiment classification task. Rehbein and Ruppenhofer (2011) analyse the impact that annotation noise can have on active learning approaches. Other works along this line investigate t"
2021.emnlp-main.822,E14-1078,0,0.0266269,"low annotation quality, and addressed through 1 See Ethics Statement section for further details methodologies aimed at minimising the effects of noisy crowdsourced data. Simpson et al. (2020), for example, present a Bayesian sequence combination approach to train a model directly from crowdsourced labels rather than aggregating them. They test their approach on tasks such as NER where disagreement is mainly due to poor annotation quality. Other works have focused instead on uncertainty in PoS-tagging, integrating annotators’ agreement in the modified loss function of a structured perceptron (Plank et al., 2014). Also Rodrigues and Pereira (2018) propose an approach to automatically distinguish the good and the unreliable annotators and capture their individual biases. They propose a novel crowd layer in deep learning classifiers to train neural networks directly from the noisy labels of multiple annotators, using only backpropagation. Other researchers have suggested to remove hard cases from the training set (Beigman Klebanov and Beigman, 2009) because they may potentially lead to poor classification of easy cases in the test set. We argue instead that disagreement is inherent to the kind of task w"
2021.emnlp-main.822,P11-1005,0,0.0405344,"has been an extensive discussion on minimal standards for inter-annotator agreement to ensure data quality (Di Eugenio and Glass, 2004; Passonneau, 2004; Artstein and Poesio, 2008), recently an increasing number of works argue that disagreement is unavoidable because language is inherently ambiguous (Aroyo and Welty, 2015), proposing ways to tackle annotators’ disagreement when building training sets (Dumitrache et al., 2019). Hsueh et al. (2009), for example, identify a set of criteria to select informative yet unambiguous examples for predictive modeling in a sentiment classification task. Rehbein and Ruppenhofer (2011) analyse the impact that annotation noise can have on active learning approaches. Other works along this line investigate the impact of uncertain or difficult instances on supervised classification (Peterson et al., 2019), while Beigman Klebanov and Beigman (2014) show that including hard cases in training data results in poorer classification of easy data in a word classification task. Along the same lines, Jamison and Gurevych (2015) show that filtering instances with low agreement improve classifier performance in four out of five tasks. Both works observe that the presence of such instance"
2021.emnlp-main.822,W08-1203,0,0.0798904,"Missing"
2021.emnlp-main.822,P19-1163,0,0.0236453,"tation of some crowd-workers, but inspired by the supplementary training approach rather from genuine differences in the interpretation from Phang et al. (2018). BERT is used with the of the tweets. Additionally, BLM and US American same parameters of the ensemble classifiers, reelections are recent events and annotators may have ported in Section 3.1. The domain data used for been biased by their personal opinion on the topic fine-tuning are built starting from the training data during annotation, an effect that has already been described above divided into different agreement highlighted in Sap et al. (2019, 2020). levels (A++ , A+ , A0 and their combinations). 10532 Training split A++ A+ A0 A++/+ A+/0 A++/0 A++/+/0 Training size 900 900 900 1800 1800 1800 2700 All domains 0.746 0.734 0.639 0.755 0.728 0.723 0.745 Founta + all domains 0.757 0.753 0.683 0.756 0.724 0.730 0.752 Baseline - training only on Founta et al. data: 0.667 F1 Table 2: Performance (F1) when training on data with different levels of human agreement (rows), fine-tuned either on domain data or using the dataset from Founta et al. (2018) and domain data. Results are reported in Table 2. Note that, for training, the tweets in a"
2021.emnlp-main.822,2020.acl-main.486,0,0.0439088,"Missing"
2021.emnlp-main.822,N19-1144,0,0.0633207,"Missing"
2021.findings-emnlp.250,C18-1012,0,0.0233016,"Missing"
2021.findings-emnlp.250,2020.acl-main.747,0,0.0208614,"this approach. However, results obtained using XLM-RoBERTa are largely outperformed by the monolingual BERT model (Table 4), confirming the findings already reported in studies on other NLP tasks (Nozza et al., 2020). Given that ItaCoLA has been created following the same principles of English CoLA and that monolingual results on Italian are in line with results obtained on the English dataset using a similar BERT-based approach, we perform a first set of cross-lingual classification experiments, to serve as baseline results for future improvements. We rely 7 Conclusions on XLM-RoBERTa-base (Conneau et al., 2020), a large multi-lingual language model, trained on In this paper we present the Italian Corpus of Lin2.5TB of filtered CommonCrawl data. We exper- guistic Acceptability, a novel dataset including al2936 Training and validation ItaCoLA and CoLA only CoLA only ItaCoLA Acc. 0.88 0.82 0.86 Test: ItaCoLA MCC 0.517 ± 0.044 (best: 0.553) 0.114 ± 0.027 (best:0.142) 0.440 ± 0.054 (best: 0.497) Acc.* 0.82 0.81 0.76 Test: CoLA MCC 0.508 ± 0.029 (best:0.535) 0.453 ± 0.04 (best:0.494) 0.114 ± 0.136 (best:0.211) Table 5: Monlingual and cross-lingual classification results using XLM-RoBERTa. MCC is the avera"
2021.findings-emnlp.250,N19-1423,0,0.00540042,"available, we select Bert-base-italian-xxl-cased, available on Huggingface.5 It is a model pretrained on a total general-purpose corpus of 81GB. After randomizing the order of instances in our training set, we fine-tune the model using PyThe monolingual experiments are aimed at presenting the first classification results on ItaCoLA and at defining standard training, validation and test split, to be used also in future experiments with the corpus. We compare two classifiers: one using LSTM and FastText embeddings, which we consider our baseline, and the other using an Italian version of BERT (Devlin et al., 2019), which we fine-tune using ItaCoLA training dataset. The two classifiers are evaluated in an in-domain and an out-of-domain setting, similar to the evaluation performed on English CoLA. (Warstadt et al., 2019). For the in-domain evaluation, we divide the ItaCoLA corpus into a training, a validation and a test split, including respectively 7,801, 946 and 975 4 https://github.com/facebookresearch/ examples. We create the splits so that each source is equally represented in each split and the accept- fastText/blob/master/docs/crawl-vectors. md 5 ability/not acceptability ratio is preserved. For t"
2021.findings-emnlp.250,L18-1550,0,0.0208334,"ing set released for the Evalita shared task and for testing we use the official AcComplIt test set. We consider this dataset out-of-domain not only because it comes from different sources compared to ItaCoLA, but also because it was created using crowd-sourcing, i.e. following a completely different approach than ours, which relies on linguistic literature. Baseline LSTM: As baseline classifier, we implement a bidirectional LSTM with two layers (64 and 32 neurons) and a dropout of 0.3. Each sentence is represented as a sequence of word embeddings, obtained with the Italian model of FastText (Grave et al., 2018) trained on Common Crawl and Wikipedia with size 300.4 The network is implemented with Keras (Chollet, 2017) (Adam optimizer, learning rate 0.01, loss function: binary crossentropy, 15 epochs). We perform 10 restarts. Reported results represent the mean performance obtained over the restarts. BERT: Among the Italian BERT-like versions available, we select Bert-base-italian-xxl-cased, available on Huggingface.5 It is a model pretrained on a total general-purpose corpus of 81GB. After randomizing the order of instances in our training set, we fine-tune the model using PyThe monolingual experimen"
2021.findings-emnlp.250,N18-1108,0,0.0346117,"Missing"
2021.findings-emnlp.250,N19-1419,0,0.0277574,"Missing"
2021.findings-emnlp.250,P19-1356,0,0.0270358,"Missing"
2021.findings-emnlp.250,2020.tacl-1.20,0,0.0408357,"Missing"
2021.findings-emnlp.250,P15-1156,0,0.175449,"its content, and we present the first experiments on this new resource. We compare in-domain and out-of-domain classification, and perform a specific evaluation of nine linguistic phenomena. We also present the first cross-lingual experiments, aimed at assessing whether multilingual transformerbased approaches can benefit from using sentences in two languages during fine-tuning. 1 Introduction (Jawahar et al., 2019; McCoy et al., 2020). Acceptability judgments have proven to be a promising area to test the acquisition of linguistic knowledge by neural language models (Gulordava et al., 2018; Lau et al., 2015). In particular, with the creation of the Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019) several approaches have been proposed that cast acceptability as a binary classification task and address it by finetuning transformer-based models on the corpus (Yang et al., 2019; Warstadt and Bowman, 2019; Raffel et al., 2020). Unfortunately, most classification experiments on acceptability judgments have focused on English, mainly because of the lack of large corpora in other languages. In this work, we therefore describe the creation of a novel corpus of acceptability judgments in I"
2021.findings-emnlp.250,Q16-1037,0,0.0220115,"arncategory includes sentences that are strictly ing approaches to acceptability judgment, while its domain-dependent, for instance statements of structure paves the way to cross-language comparlinguistic rules such as “Una testa lessicale -N ative analyses. assegna Caso al SN che essa regge” (En. A Concerning acceptability annotation, for the lexical head -N assigns Case to the SN that creation of ItaCoLA we have chosen to keep a it holds) or sentences extracted from novels, Boolean judgment in line with several previous films or newspapers. works (Lawrence et al., 2000; Wagner et al., 2009; Linzen et al., 2016). This choice ensures robust• Sentences with an extremely twisted syntax ness and simplifies classification, while allowing and a very high number of nested subordius to keep the original judgments as formulated nates. The latter are often used as borderline 2931 Source Graffi (1994) Label 0 Graffi (1994) 1 Graffi (1994) 0 Simone and Masini (2013) 1 Sentence *Edoardo è tornato nella sua l’anno scorso città. (*Edoardo returned to his last year city) Ho voglia di salutare Maria (I want to greet Maria) *Questa donna mi hanno colpito. (*This woman have impressed me) Questa donna mi ha colpito. (Th"
2021.findings-emnlp.250,2020.tacl-1.9,0,0.0238778,"Missing"
2021.findings-emnlp.250,N18-1202,0,0.0253704,"Missing"
2021.findings-emnlp.250,2021.nlp4call-1.3,0,0.063866,"), ranging from 1 specific phenomena (Marvin and Linzen, 2019; Available at https://github.com/dhfbk/ Goldberg, 2019) to a general grammar knowledge ItaCoLA-dataset 2929 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2929–2940 November 7–11, 2021. ©2021 Association for Computational Linguistics research perspectives. 2 2.1 Related Work Acceptability corpora et al., 2020). Both studies use sentences extracted from textbooks. To our knowledge, only for Swedish there is a freely-available corpus whose size is comparable to CoLA and ItaCoLA. The corpus, presented in (Volodina et al., 2021), contains around 9,600 sentences extracted from language learner data. Concerning Italian, only one dataset has been released to date, in the context of Evalita 2020 evaluation campaign on complexity and acceptability (AcComplIt task) (Brunato et al., 2020). The dataset presents several differences w.r.t. ItaCoLA in terms of size, annotation approach and linguistic phenomena, which we detail in Section 4.2. In recent years, studies on automatic assessment of acceptability have become very popular thanks to the release of the CoLa corpus (Warstadt et al., 2019), the first large-scale corpus of"
2021.findings-emnlp.250,W18-5446,0,0.0463286,"Missing"
2021.findings-emnlp.250,Q19-1040,0,0.263962,"of-domain classification, and perform a specific evaluation of nine linguistic phenomena. We also present the first cross-lingual experiments, aimed at assessing whether multilingual transformerbased approaches can benefit from using sentences in two languages during fine-tuning. 1 Introduction (Jawahar et al., 2019; McCoy et al., 2020). Acceptability judgments have proven to be a promising area to test the acquisition of linguistic knowledge by neural language models (Gulordava et al., 2018; Lau et al., 2015). In particular, with the creation of the Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019) several approaches have been proposed that cast acceptability as a binary classification task and address it by finetuning transformer-based models on the corpus (Yang et al., 2019; Warstadt and Bowman, 2019; Raffel et al., 2020). Unfortunately, most classification experiments on acceptability judgments have focused on English, mainly because of the lack of large corpora in other languages. In this work, we therefore describe the creation of a novel corpus of acceptability judgments in Italian, following the methodology used in CoLA for English. We collect 10k sentences extracted from linguis"
2021.hcinlp-1.10,Q14-1035,0,0.0154081,"nels, while only a small fraction of them is visible in public accounts. This makes the analysis of cyberbullying phenomena difficult, since few data of this kind are accessible, making it hard to study the behaviour of adolescents online, especially if they are underage. The few existing works dealing with NLP and cyberbullying 2 Related work To date, there have been several attempts to gamify a wide range of linguistic annotation tasks. These include, among others, Phrase Detectives for anaphora resolution (Poesio et al., 2013), The Knowledge Towers (Vannella et al., 2014) and Puzzle Racer (Jurgens and Navigli, 2014) for concept60 Proceedings of the First Workshop on Bridging Human–Computer Interaction and Natural Language Processing, pages 60–65 April 20, 2021. ©2021 Association for Computational Linguistics image linking, Infection (Vannella et al., 2014), OnToGalaxy (Krause et al., 2010) and JeuxDeMots (Joubert et al., 2018) for semantic linking, Argotario (Habernal et al., 2017) for fallacious argumentation identification, Zombilingo (Fort et al., 2014) for dependecy syntax annotation, Sentimentator ¨ (Ohman and Kajava, 2018) for sentiment annotation, WordClicker (Madge et al., 2018) for part-ofspeech"
2021.hcinlp-1.10,2020.gamnlp-1.6,1,0.643529,"the behaviour of teenagers with respect to verbal abuse online are therefore needed. Also, it is important to understand their perspective on different types of hate messages, so to develop systems that recognise cyberbullying by applying adolescents’ point of view. Indeed, adolescents being part of virtual communities may share communication habits and slang that are different from those of adults, making it difficult for people outside these communities to judge the messages’ offensiveness. In order to deal with this lack of resources, we are developing a game called High School Superhero (Bonetti and Tonelli, 2020), whose goal is two-fold: on the one hand, it is meant to be used by young people to raise awareness on cyberbullying and on the use of offensive language, by playing the role of bystander in offensive interactions among peers. On the other hand, it is designed to collect annotations on hate speech texts, in particular whether players consider a message offensive or not, and which text span should be replaced or removed to make it not offensive. Accomodating these two tasks poses a series of challenges that we discuss in the remainder of this paper. In this paper we discuss several challenges"
2021.hcinlp-1.10,P14-1122,0,0.0201386,"ks are frequent in private chats and channels, while only a small fraction of them is visible in public accounts. This makes the analysis of cyberbullying phenomena difficult, since few data of this kind are accessible, making it hard to study the behaviour of adolescents online, especially if they are underage. The few existing works dealing with NLP and cyberbullying 2 Related work To date, there have been several attempts to gamify a wide range of linguistic annotation tasks. These include, among others, Phrase Detectives for anaphora resolution (Poesio et al., 2013), The Knowledge Towers (Vannella et al., 2014) and Puzzle Racer (Jurgens and Navigli, 2014) for concept60 Proceedings of the First Workshop on Bridging Human–Computer Interaction and Natural Language Processing, pages 60–65 April 20, 2021. ©2021 Association for Computational Linguistics image linking, Infection (Vannella et al., 2014), OnToGalaxy (Krause et al., 2010) and JeuxDeMots (Joubert et al., 2018) for semantic linking, Argotario (Habernal et al., 2017) for fallacious argumentation identification, Zombilingo (Fort et al., 2014) for dependecy syntax annotation, Sentimentator ¨ (Ohman and Kajava, 2018) for sentiment annotation, WordC"
2021.hcinlp-1.10,I13-1066,0,0.0800007,"Missing"
2021.hcinlp-1.10,W13-0215,0,0.0199913,"edings of the First Workshop on Bridging Human–Computer Interaction and Natural Language Processing, pages 60–65 April 20, 2021. ©2021 Association for Computational Linguistics image linking, Infection (Vannella et al., 2014), OnToGalaxy (Krause et al., 2010) and JeuxDeMots (Joubert et al., 2018) for semantic linking, Argotario (Habernal et al., 2017) for fallacious argumentation identification, Zombilingo (Fort et al., 2014) for dependecy syntax annotation, Sentimentator ¨ (Ohman and Kajava, 2018) for sentiment annotation, WordClicker (Madge et al., 2018) for part-ofspeech tagging, Wordrobe (Venhuizen et al., 2013) and Ka-Boom! (Jurgens and Navigli, 2014) for sense annotation. Researchers stress the fact that games with a purpose (GWAPs) should be designed in such a way that they integrate the task without sacrificing their ‘gamefulness’, otherwise the tasks may be perceived as work (Vannella et al., 2014). Concerning the use of gamification to raise awareness against cyberbullying, past works showed that increasing empathy is crucial to con´ trol cyberbullying (Barreda-Angeles et al., 2021; Del Rey et al., 2016) and games can help in this sense as shown in Calvo-Morata et al. (2019). They tested Conect"
2021.hcinlp-1.10,2020.trac-1.23,0,0.091476,"Missing"
2021.hcinlp-1.10,W18-5107,1,0.841065,"Missing"
2021.latechclfl-1.2,D14-1160,0,0.0711701,"Missing"
2021.latechclfl-1.2,L18-1521,0,0.0402633,"Missing"
2021.latechclfl-1.2,tonelli-pianta-2008-frame,1,0.67226,"gs through the lens of smell descriptions. Although this analysis is carried out on English texts, our guidelines have been designed to be languageindependent, and the preparation of LU lists covering different European languages is ongoing. As a next step, we plan to extend the annotation with emotion information, so to capture the effect or reactions triggered by smells. Once a multilingual benchmark with manually annotated olfactory information is available, we will also investigate different approaches to perform the task automatically, exploring cross-language frame information transfer (Tonelli and Pianta, 2008) as well as semisupervised and few-shot classification. More broadly, analysing how olfactory experiences are described in texts can offer fresh perspectives for humanities research in this field. Through the connection of different, multilingual sources, especially historical ones, we can promote scholarship in the field of sensory history, and engage audiences with more (nuanced) stories about their olfactory past. Once information extraction sysOverall, travelers describe their experiences as full of excitement for being on classic soil and involving all senses. Indeed, smell is often menti"
2021.latechclfl-1.2,L16-1360,0,0.0692584,"Missing"
2021.latechclfl-1.2,W14-4716,0,0.0607706,"Missing"
2021.woah-1.9,baccianella-etal-2010-sentiwordnet,0,0.196218,"Missing"
2021.woah-1.9,S19-2007,0,0.0478605,"Missing"
2021.woah-1.9,2020.acl-main.442,0,0.15541,"t al., 2019), with the awareness that a single definition is not sufficient to address the multi-faceted problem of fairness in its entirety. In this work, we adopt a definition for fairness that is strongly contextual to abusive language detection. We define Current abusive language detection systems have demonstrated unintended bias towards sensitive features such as nationality or gender. This is a crucial issue, which may harm minorities and underrepresented groups if such systems were integrated in real-world applications. In this paper, we create ad hoc tests through the CheckList tool (Ribeiro et al., 2020) to detect biases within abusive language classifiers for English. We compare the behaviour of two BERT-based models, one trained on a generic abusive language dataset and the other on a dataset for misogyny detection. Our evaluation shows that, although BERT-based classifiers achieve high accuracy levels on a variety of natural language processing tasks, they perform very poorly as regards fairness and bias, in particular on samples involving implicit stereotypes, expressions of hate towards minorities and protected attributes such as race or sexual orientation. We release both the notebooks"
2021.woah-1.9,2021.ccl-1.108,0,0.0707583,"Missing"
2021.woah-1.9,C18-1228,0,0.0202794,"red to CheckList, however, it is more complicated to handle and deploy for users with little NLP skills. An interesting aspect is that TextAttack includes in the package the so-called “recipes”, i.e. attacks from the literature ready to run, that build a common ground for the assessment and comparison of models’ performances. As outlined in (Ribeiro et al., 2020), some methods to identify errors by NLP systems are taskspecific, such as (Ribeiro et al., 2019) or (Belinkov and Bisk, 2018), while others focus on particular NLP components such as word embeddings, as in (Tsvetkov et al., 2016) or (Rogers et al., 2018). Compared to existing approaches, one of CheckList’s major strengths lies in including the testing phase within a comprehensive framework. The evaluation, conducted through adaptable templates and a range of relevant linguistic capabilities, is on one hand more granular than overall measures such as accuracy; on the other hand it is more versatile, because it leaves liberty to the developer to enrich and expand the tests within new and more suitable capabilities, depending on the task and model under consideration. On the topic of fairness and biases, (Kiritchenko et al., 2020) conduct an in-"
2021.woah-1.9,P19-1163,0,0.0278999,"lease both the notebooks implemented to extend the Fairness tests and the synthetic datasets usable to evaluate systems bias independently of CheckList. 1 Introduction At every stage of a supervised learning process, biases can arise and be introduced in the pipeline, ultimately leading to harm (Suresh and Guttag, 2020; Dixon et al., 2018). When it comes to systems whose goal is to automatically detect abusive language, this issue becomes particularly serious, since unintended bias towards sensitive attributes such as gender, sexual orientation or nationality can harm underrepresented groups. Sap et al. (2019), for example, show that annotators tend to label messages in Afro-American English more frequently than when annotating other messages, which could lead to the training of a system reproducing the same kind of bias. 81 Proceedings of the Fifth Workshop on Online Abuse and Harms, pages 81–91 August 6, 2021. ©2021 Association for Computational Linguistics unfairness as the sensitivity of an abusive language detection classifier with respect to the presence in the record to be classified of entities belonging to protected groups or minorities. Specifically, a classifier is considered unfair or b"
2021.woah-1.9,2020.acl-main.486,0,0.0640842,"Missing"
2021.woah-1.9,2020.emnlp-demos.16,0,0.0935509,"Missing"
2021.woah-1.9,W16-2520,0,0.024449,"adversarial attacks. Compared to CheckList, however, it is more complicated to handle and deploy for users with little NLP skills. An interesting aspect is that TextAttack includes in the package the so-called “recipes”, i.e. attacks from the literature ready to run, that build a common ground for the assessment and comparison of models’ performances. As outlined in (Ribeiro et al., 2020), some methods to identify errors by NLP systems are taskspecific, such as (Ribeiro et al., 2019) or (Belinkov and Bisk, 2018), while others focus on particular NLP components such as word embeddings, as in (Tsvetkov et al., 2016) or (Rogers et al., 2018). Compared to existing approaches, one of CheckList’s major strengths lies in including the testing phase within a comprehensive framework. The evaluation, conducted through adaptable templates and a range of relevant linguistic capabilities, is on one hand more granular than overall measures such as accuracy; on the other hand it is more versatile, because it leaves liberty to the developer to enrich and expand the tests within new and more suitable capabilities, depending on the task and model under consideration. On the topic of fairness and biases, (Kiritchenko et"
2021.woah-1.9,N16-2013,0,0.0912651,"Missing"
2021.woah-1.9,N18-1202,0,0.11748,"Missing"
2021.woah-1.9,2020.semeval-1.213,0,0.0496086,"Missing"
2021.woah-1.9,P19-1621,0,0.0205409,"loys CheckList – is a modelagnostic framework useful for the expansion of the datasets and the increase of models robustness through adversarial attacks. Compared to CheckList, however, it is more complicated to handle and deploy for users with little NLP skills. An interesting aspect is that TextAttack includes in the package the so-called “recipes”, i.e. attacks from the literature ready to run, that build a common ground for the assessment and comparison of models’ performances. As outlined in (Ribeiro et al., 2020), some methods to identify errors by NLP systems are taskspecific, such as (Ribeiro et al., 2019) or (Belinkov and Bisk, 2018), while others focus on particular NLP components such as word embeddings, as in (Tsvetkov et al., 2016) or (Rogers et al., 2018). Compared to existing approaches, one of CheckList’s major strengths lies in including the testing phase within a comprehensive framework. The evaluation, conducted through adaptable templates and a range of relevant linguistic capabilities, is on one hand more granular than overall measures such as accuracy; on the other hand it is more versatile, because it leaves liberty to the developer to enrich and expand the tests within new and m"
2021.woah-1.9,N19-1060,0,0.0184758,"satonelli@fbk.eu Abstract The role of the datasets used to train these models is crucial: as pointed out by (Wiegand et al., 2019a), there may be multiple reasons why a dataset is biased, e.g. due to skewed sampling strategies, prevalence of a specific subject (topic bias) or of content written by a specific author (author bias). Mitigation strategies may involve assessing which terms are frequent in the presence of certain labels and implementing techniques to balance the data by including neutral samples containing those same terms to prevent the model from learning inaccurate correlations (Wiegand et al., 2019a). Furthermore, it is important to distinguish between different types of hatred, depending on the target group addressed: for example, misogynistic expressions show different linguistic peculiarities than racist ones. It is therefore crucial to create specialised datasets addressing different phenomena of abusive language, so that systems can be tuned to the complex and nuanced scenario of online speech. Given the sensitive context in which abusive language detection systems are deployed, a robust value-oriented evaluation of the model’s fairness is necessary, in order to assess unintended b"
2021.woah-1.9,N19-1211,0,0.0179348,"satonelli@fbk.eu Abstract The role of the datasets used to train these models is crucial: as pointed out by (Wiegand et al., 2019a), there may be multiple reasons why a dataset is biased, e.g. due to skewed sampling strategies, prevalence of a specific subject (topic bias) or of content written by a specific author (author bias). Mitigation strategies may involve assessing which terms are frequent in the presence of certain labels and implementing techniques to balance the data by including neutral samples containing those same terms to prevent the model from learning inaccurate correlations (Wiegand et al., 2019a). Furthermore, it is important to distinguish between different types of hatred, depending on the target group addressed: for example, misogynistic expressions show different linguistic peculiarities than racist ones. It is therefore crucial to create specialised datasets addressing different phenomena of abusive language, so that systems can be tuned to the complex and nuanced scenario of online speech. Given the sensitive context in which abusive language detection systems are deployed, a robust value-oriented evaluation of the model’s fairness is necessary, in order to assess unintended b"
2021.woah-1.9,P19-1073,0,0.0532518,"Missing"
2021.woah-1.9,2020.semeval-1.188,0,0.0390357,"Missing"
2021.woah-1.9,N18-2003,0,0.0550217,"Missing"
C12-1162,J05-3002,0,0.0420473,"ipants to develop a system able to detect an inference relation between T-H pairs. In this applied framework, inferences are performed directly over lexical-syntactic representations of the texts. Current systems mainly rely on Machine Learning techniques (typically SVM), logical inference, cross-pair similarity measures between T and H, and word alignment. The definition of TE captures quite broadly the reasoning about language variability needed by different applications for natural language understanding and processing, e.g. information extraction (Romano et al., 2006), text summarization (Barzilay and McKeown, 2005), and reading comprehension systems (Nielsen et al., 2009). Following this rationale, the data sets provided by the challenge organizers are composed of T-H pairs collected from several applicative scenarios (e.g. Question Answering, Information Extraction, Information Retrieval, Summarization), reflecting the way by which the corresponding application could take advantage of automated entailment judgement.4 3 Restatement relations in the Penn Discourse Treebank The Penn Discourse Treebank (PDTB) is a resource built on top of the Wall Street Journal (WSJ), in which discourse relations have bee"
C12-1162,bentivogli-etal-2010-building,1,0.637383,"Discourse Treebank, relazioni implicite. Proceedings of COLING 2012: Technical Papers, pages 2653–2668, COLING 2012, Mumbai, December 2012. 2653 1 Introduction Given the growing amount of resources and automatic systems developed in the NLP community, it is crucial to guarantee the highest possible compatibility among them, and to exploit as much as possible annotated data and tools across different research domains.1 Past works on discourse analysis have been conducted in parallel with research on semantic inference (in particular, on textual entailment phenomena, see Sammons et al. (2010), Bentivogli et al. (2010)) and, although the two fields of study seem to be intertwined, no effort has been made into the reuse of annotated data and processing tools across both domains. With this work, we address the issue of interoperability by investigating the connection between implicit Restatement relations in the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) and the Textual Entailment (TE) relation as defined by Dagan et al. (2009). Consider for instance the following sentences extracted from the PDTB and annotated as being in a Restatement relation: (1) Because hurricanes can change course rapidly, the"
C12-1162,I11-1120,1,0.757336,"draw some conclusions and discuss future work in Section 5. 2 Related work A number of approaches have been proposed for annotating explicit discourse relations following the PDTB paradigm. While the first attempts were limited to retrieving the heads (usually the main verb) of discourse arguments (Elwell and Baldridge, 2008; Wellner and Pustejovsky, 2007), or to extracting only the sentences containing the arguments (Prasad et al., 2010), more recent works have focused on the identification of the exact arguments spans and on the development of end-to-end discourse parsers (Lin et al., 2010; Ghosh et al., 2011b,a). These works rely on the information conveyed by explicit connectives, which proved quite easy to classify using syntactic information (Pitler et al., 2008; Pitler and Nenkova, 2009). If the connective is not overtly expressed, however, the task is more challenging and requires different features compared to explicit relations. Experiments by Lin et al. (2009), Pitler et al. (2009) and Lin et al. (2010) showed that, despite the promising results and the progress with respect to their baselines, there is still room for improvement. As introduced before (Section 1), the notion of textual en"
C12-1162,P10-4008,0,0.0615022,"esent the two sets of experiments we carried out. The first was performed to verify if implicit Restatement relations can be detected using current TE systems (Section 4.2). The second was run on a subset of manually re-annotated sentences (Section 4.3), to further verify the relationship between entailment and Restatement relations on a controlled data set. 2658 4.1 TE systems description In order to analyze the correspondence between the Restatement and the textual entailment relation, we run different experiments using two off-the-shelf TE systems: VENSES (Delmonte et al., 2009) and EDITS (Kouylekov and Negri, 2010). We choose these two systems because: i) they are freely available, ii) they obtained similar performances at the last RTE campaigns, and iii) they rely on different NLP approaches: VENSES is a rule-based system incorporating and combining different levels of linguistic information, from lexical to semantic knowledge. EDITS, instead, is a supervised TE system implementing a distance-based framework, whose modular architecture combines distance and similarity algorithms. 4.1.1 The VENSES system VENSES is a rule-based system for recognizing textual entailment based on a linguistic analyzer and"
C12-1162,D09-1036,0,0.110869,"tracting only the sentences containing the arguments (Prasad et al., 2010), more recent works have focused on the identification of the exact arguments spans and on the development of end-to-end discourse parsers (Lin et al., 2010; Ghosh et al., 2011b,a). These works rely on the information conveyed by explicit connectives, which proved quite easy to classify using syntactic information (Pitler et al., 2008; Pitler and Nenkova, 2009). If the connective is not overtly expressed, however, the task is more challenging and requires different features compared to explicit relations. Experiments by Lin et al. (2009), Pitler et al. (2009) and Lin et al. (2010) showed that, despite the promising results and the progress with respect to their baselines, there is still room for improvement. As introduced before (Section 1), the notion of textual entailment has been proposed as an applied framework to capture major semantic inference needs across applications in NLP (Dagan and Glickman, 2004), (Dagan et al., 2009). Given a pair of textual fragments, it considers if a competent speaker with basic knowledge of the world would typically infer the second from the first one. To promote the development of general T"
C12-1162,J93-2004,0,0.0389783,"Missing"
C12-1162,C10-1087,0,0.103838,"n recently debated during the Collaboratively Constructed Semantic Resources Workshop’s panel at ACL2012, where the importance of the development of functional resources strongly connected with NLP systems was underlined, to allow for resources reusability in different tasks. 2 http://aclweb.org/aclwiki/index.php?title=Recognizing_Textual_Entailment 2654 While the importance of discourse information in TE has already been discussed in relation to anaphora and bridging phenomena (in particular starting from RTE-5, where the T was composed by longer paragraphs, requiring coreference resolution (Mirkin et al., 2010b) (Mirkin et al., 2010a)), little attention has been paid to discourse relations holding between sentences, although this is strictly connected to the problem of coreference (for further discussion on this topic, see Section 3). In this work, we address the following research questions: • What are the main differences between Restatement and TE relations, given that they are very similar from a theoretical point of view? • Is it possible to use RTE systems to identify implicit Restatement relations, since current approaches have proved to have some limitations and achieve poor performance? •"
C12-1162,P10-1123,0,0.147827,"n recently debated during the Collaboratively Constructed Semantic Resources Workshop’s panel at ACL2012, where the importance of the development of functional resources strongly connected with NLP systems was underlined, to allow for resources reusability in different tasks. 2 http://aclweb.org/aclwiki/index.php?title=Recognizing_Textual_Entailment 2654 While the importance of discourse information in TE has already been discussed in relation to anaphora and bridging phenomena (in particular starting from RTE-5, where the T was composed by longer paragraphs, requiring coreference resolution (Mirkin et al., 2010b) (Mirkin et al., 2010a)), little attention has been paid to discourse relations holding between sentences, although this is strictly connected to the problem of coreference (for further discussion on this topic, see Section 3). In this work, we address the following research questions: • What are the main differences between Restatement and TE relations, given that they are very similar from a theoretical point of view? • Is it possible to use RTE systems to identify implicit Restatement relations, since current approaches have proved to have some limitations and achieve poor performance? •"
C12-1162,P09-1077,0,0.0181461,"entences containing the arguments (Prasad et al., 2010), more recent works have focused on the identification of the exact arguments spans and on the development of end-to-end discourse parsers (Lin et al., 2010; Ghosh et al., 2011b,a). These works rely on the information conveyed by explicit connectives, which proved quite easy to classify using syntactic information (Pitler et al., 2008; Pitler and Nenkova, 2009). If the connective is not overtly expressed, however, the task is more challenging and requires different features compared to explicit relations. Experiments by Lin et al. (2009), Pitler et al. (2009) and Lin et al. (2010) showed that, despite the promising results and the progress with respect to their baselines, there is still room for improvement. As introduced before (Section 1), the notion of textual entailment has been proposed as an applied framework to capture major semantic inference needs across applications in NLP (Dagan and Glickman, 2004), (Dagan et al., 2009). Given a pair of textual fragments, it considers if a competent speaker with basic knowledge of the world would typically infer the second from the first one. To promote the development of general TE recognition engines,"
C12-1162,P09-2004,0,0.0775788,"paradigm. While the first attempts were limited to retrieving the heads (usually the main verb) of discourse arguments (Elwell and Baldridge, 2008; Wellner and Pustejovsky, 2007), or to extracting only the sentences containing the arguments (Prasad et al., 2010), more recent works have focused on the identification of the exact arguments spans and on the development of end-to-end discourse parsers (Lin et al., 2010; Ghosh et al., 2011b,a). These works rely on the information conveyed by explicit connectives, which proved quite easy to classify using syntactic information (Pitler et al., 2008; Pitler and Nenkova, 2009). If the connective is not overtly expressed, however, the task is more challenging and requires different features compared to explicit relations. Experiments by Lin et al. (2009), Pitler et al. (2009) and Lin et al. (2010) showed that, despite the promising results and the progress with respect to their baselines, there is still room for improvement. As introduced before (Section 1), the notion of textual entailment has been proposed as an applied framework to capture major semantic inference needs across applications in NLP (Dagan and Glickman, 2004), (Dagan et al., 2009). Given a pair of t"
C12-1162,C08-2022,0,0.0994194,"s following the PDTB paradigm. While the first attempts were limited to retrieving the heads (usually the main verb) of discourse arguments (Elwell and Baldridge, 2008; Wellner and Pustejovsky, 2007), or to extracting only the sentences containing the arguments (Prasad et al., 2010), more recent works have focused on the identification of the exact arguments spans and on the development of end-to-end discourse parsers (Lin et al., 2010; Ghosh et al., 2011b,a). These works rely on the information conveyed by explicit connectives, which proved quite easy to classify using syntactic information (Pitler et al., 2008; Pitler and Nenkova, 2009). If the connective is not overtly expressed, however, the task is more challenging and requires different features compared to explicit relations. Experiments by Lin et al. (2009), Pitler et al. (2009) and Lin et al. (2010) showed that, despite the promising results and the progress with respect to their baselines, there is still room for improvement. As introduced before (Section 1), the notion of textual entailment has been proposed as an applied framework to capture major semantic inference needs across applications in NLP (Dagan and Glickman, 2004), (Dagan et al"
C12-1162,prasad-etal-2008-penn,0,0.181343,"Missing"
C12-1162,prasad-etal-2010-exploiting,0,0.0184035,"ns. In Section 4 we present the experimental setting, introducing the TE systems we used, and then we detail both the first and the second experiment we carried out. Finally, we draw some conclusions and discuss future work in Section 5. 2 Related work A number of approaches have been proposed for annotating explicit discourse relations following the PDTB paradigm. While the first attempts were limited to retrieving the heads (usually the main verb) of discourse arguments (Elwell and Baldridge, 2008; Wellner and Pustejovsky, 2007), or to extracting only the sentences containing the arguments (Prasad et al., 2010), more recent works have focused on the identification of the exact arguments spans and on the development of end-to-end discourse parsers (Lin et al., 2010; Ghosh et al., 2011b,a). These works rely on the information conveyed by explicit connectives, which proved quite easy to classify using syntactic information (Pitler et al., 2008; Pitler and Nenkova, 2009). If the connective is not overtly expressed, however, the task is more challenging and requires different features compared to explicit relations. Experiments by Lin et al. (2009), Pitler et al. (2009) and Lin et al. (2010) showed that,"
C12-1162,robaldo-etal-2010-corpus,0,0.0680069,"Missing"
C12-1162,E06-1052,0,0.0275513,"55 evaluation campaigns3 have asked participants to develop a system able to detect an inference relation between T-H pairs. In this applied framework, inferences are performed directly over lexical-syntactic representations of the texts. Current systems mainly rely on Machine Learning techniques (typically SVM), logical inference, cross-pair similarity measures between T and H, and word alignment. The definition of TE captures quite broadly the reasoning about language variability needed by different applications for natural language understanding and processing, e.g. information extraction (Romano et al., 2006), text summarization (Barzilay and McKeown, 2005), and reading comprehension systems (Nielsen et al., 2009). Following this rationale, the data sets provided by the challenge organizers are composed of T-H pairs collected from several applicative scenarios (e.g. Question Answering, Information Extraction, Information Retrieval, Summarization), reflecting the way by which the corresponding application could take advantage of automated entailment judgement.4 3 Restatement relations in the Penn Discourse Treebank The Penn Discourse Treebank (PDTB) is a resource built on top of the Wall Street Jou"
C12-1162,P10-1122,0,0.0325678,"icazione testuale, Penn Discourse Treebank, relazioni implicite. Proceedings of COLING 2012: Technical Papers, pages 2653–2668, COLING 2012, Mumbai, December 2012. 2653 1 Introduction Given the growing amount of resources and automatic systems developed in the NLP community, it is crucial to guarantee the highest possible compatibility among them, and to exploit as much as possible annotated data and tools across different research domains.1 Past works on discourse analysis have been conducted in parallel with research on semantic inference (in particular, on textual entailment phenomena, see Sammons et al. (2010), Bentivogli et al. (2010)) and, although the two fields of study seem to be intertwined, no effort has been made into the reuse of annotated data and processing tools across both domains. With this work, we address the issue of interoperability by investigating the connection between implicit Restatement relations in the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) and the Textual Entailment (TE) relation as defined by Dagan et al. (2009). Consider for instance the following sentences extracted from the PDTB and annotated as being in a Restatement relation: (1) Because hurricanes can"
C12-1162,D07-1010,0,0.0264717,"troduced. In Section 3 the PDTB is described, with a focus on implicit and Restatement relations. In Section 4 we present the experimental setting, introducing the TE systems we used, and then we detail both the first and the second experiment we carried out. Finally, we draw some conclusions and discuss future work in Section 5. 2 Related work A number of approaches have been proposed for annotating explicit discourse relations following the PDTB paradigm. While the first attempts were limited to retrieving the heads (usually the main verb) of discourse arguments (Elwell and Baldridge, 2008; Wellner and Pustejovsky, 2007), or to extracting only the sentences containing the arguments (Prasad et al., 2010), more recent works have focused on the identification of the exact arguments spans and on the development of end-to-end discourse parsers (Lin et al., 2010; Ghosh et al., 2011b,a). These works rely on the information conveyed by explicit connectives, which proved quite easy to classify using syntactic information (Pitler et al., 2008; Pitler and Nenkova, 2009). If the connective is not overtly expressed, however, the task is more challenging and requires different features compared to explicit relations. Exper"
C12-1162,W05-1206,0,0.0285061,"y” co-refers with the “bankers” in T, according to the definition of TE this is not a positive example, since the amount of background knowledge to be assumed to judge this pair asides from information provided by T. Quite a lot of the pairs tagged as Restatement in the PDTB and present in our data set fall into that category, and cannot therefore be considered as positive textual entailment pairs. At the same time, what the literature assumes as background knowledge to be introduced in the inference process is not completely clear (see the debate among Dagan et al. (2006), Manning (2006) and Zaenen et al. (2005)), making the assignment of the entailment judgment to such pairs particularly difficult. In order to understand and prove if the low performances obtained by the TE systems in our first experiment are due to the presence of Restatement sentences that are negative entailment pairs, we conduct a second experiment on a reduced data set. 4.3 Experiment 2: reduced data set To verify the correctness of our initial hypothesis, i.e. that the sentences annotated as being into a Restatement relation express entailment, we run a second experiment focusing on a manually-annotated subset of pairs, as desc"
C14-1198,bartalesi-lenzi-etal-2012-cat,0,0.053442,"Missing"
C14-1198,bethard-etal-2008-building,0,0.0507966,"Missing"
C14-1198,S13-2002,0,0.124399,"Missing"
C14-1198,P07-2009,0,0.0163953,"Missing"
C14-1198,W08-1301,0,0.0283938,"Missing"
C14-1198,D11-1027,0,0.266255,"Missing"
C14-1198,S07-1003,0,0.0096638,"ill an open issue. However, information on causality could be beneficial to a number of natural language processing tasks such as question answering, text summarization, decision support, etc. The lack of information extraction systems focused on causality may depend also on the lack of unified annotation guidelines and standard benchmarks, which usually foster the comparison of different systems performances. Specific phenomena related to causality, such as causal arguments (Bonial et al., 2010), causal discourse relations (The PDTB Research Group, 2008) or causal relations between nominals (Girju et al., 2007), have been investigated, but no unified framework has been proposed to capture causal relations between events, as opposed to the existing TimeML standard for temporal relations (Pustejovsky et al., 2010). The work presented in this paper copes with this issue by i) proposing an annotation framework to model causal relations between events and ii) detailing the development and the evaluation of a supervised system based on such framework. We take advantage of the formalization work carried out for the TimeML standard, in which events, temporal relations and temporal signals have been carefull"
C14-1198,E14-1033,1,0.821716,"and testing, we implement two different classifiers: the first one is a CSIGNAL labeler, that takes in input information on events and temporal expressions as annotated in the original TimeBank, and classifies whether a token is part of a causal signal or not (Section 4.1). The second one is a CLINK classifier, which given an event pair detects whether they are connected by an explicit causal link (Section 4.2). Both experiments are carried out based on five-fold cross-validation. The overall approach is largely inspired by our existing framework for the classification of temporal relations (Mirza and Tonelli, 2014). 4.1 Automatic Extraction of CSIGNALs The task of recognizing CSIGNALs can be seen as a text chunking task, i.e. using a classifier to determine whether a token is part of a causal signal or not. Since the extent of causal signals can be expressed by multi-word expressions, we employ the IOB tagging convention to annotate the data, where each token can either be classified into B - CSIGNAL, I - CSIGNAL or O (for other). We build our classification model using the Support Vector Machine (SVM) implementation provided by YamCha3 , a generic, customizable, and open source text chunker. In order t"
C14-1198,W14-0702,1,0.795911,"-fold cross-validation setting, and we compare the performance of our classifier with a baseline rule-based system. This relies on an algorithm that, given a term t belonging to affect, link, causative verbs (basic and periphrastic constructions) or causal signals (as listed in the annotation guidelines), looks for specific dependency constructions where t is connected to two events. If such dependencies are found, a CLINK is automatically set between the two events identifying the source and the target of the relation. Further details on the baseline system and its evaluation can be found in Mirza et al. (2014). In our experimental setting, we evaluate two versions of the CLINK classifier: the first includes as features the gold annotated CSIGNALs in the classification model, while the second takes in input the CSIGNALs automatically annotated by the classifier described in Section 4.1. We also evaluate the contribution of dependency, CSIGNAL and TLINK features by excluding each of them from the classification model. Evaluation results are reported in Table 2. We observe that the baseline is always outperformed by the other classifiers. CSIGNAL is the most important feature, with a particularly high"
C14-1198,pianta-etal-2008-textpro,0,0.0278985,"g a classifier to determine whether a token is part of a causal signal or not. Since the extent of causal signals can be expressed by multi-word expressions, we employ the IOB tagging convention to annotate the data, where each token can either be classified into B - CSIGNAL, I - CSIGNAL or O (for other). We build our classification model using the Support Vector Machine (SVM) implementation provided by YamCha3 , a generic, customizable, and open source text chunker. In order to provide the classifier a feature vector to learn from, we perform the two following steps: 1. Run the TextPro tool (Pianta et al., 2008) to get information on base NP chunking and whether a token is part of named entity or not. 2. Run Stanford CoreNLP tool4 to get information on lemma, part-of-speech (PoS) tags and dependency relations between tokens. In the end, the feature vector includes token, lemma, PoS tag, NP chunking, dependency path, and several binary features, indicating whether a token is: i) an event or part of a temporal expression, 2 The annotated data set is available at http://hlt.fbk.eu/technologies/causal-timebank http://chasen.org/∼taku/software/yamcha/ 4 http://nlp.stanford.edu/software/corenlp.shtml 3 210"
C14-1198,P09-2004,0,0.01647,"Missing"
C14-1198,pustejovsky-etal-2010-iso,0,0.0217119,"ck of information extraction systems focused on causality may depend also on the lack of unified annotation guidelines and standard benchmarks, which usually foster the comparison of different systems performances. Specific phenomena related to causality, such as causal arguments (Bonial et al., 2010), causal discourse relations (The PDTB Research Group, 2008) or causal relations between nominals (Girju et al., 2007), have been investigated, but no unified framework has been proposed to capture causal relations between events, as opposed to the existing TimeML standard for temporal relations (Pustejovsky et al., 2010). The work presented in this paper copes with this issue by i) proposing an annotation framework to model causal relations between events and ii) detailing the development and the evaluation of a supervised system based on such framework. We take advantage of the formalization work carried out for the TimeML standard, in which events, temporal relations and temporal signals have been carefully defined and annotated. We propose to model causal relations in a similar way to temporal relations, inheriting from TimeML the notion of event, relation and signal, even though our approach to causality"
C14-1198,W13-4004,0,0.372675,"Missing"
C14-1198,S13-2001,0,0.0372008,"Missing"
C16-1007,bethard-etal-2008-building,0,0.0585506,"nce. Licence details: http:// creativecommons.org/licenses/by/4.0/ 1 The system is made available at https://github.com/paramitamirza/CATENA. 64 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 64–75, Osaka, Japan, December 11-17 2016. Figure 1: System architecture of CATENA The problem of detecting causality between events is as challenging as recognizing their temporal order, but less analyzed from an NLP perspective. Besides, previous works have mostly focused on specific types of event pairs and causal expressions in text (Bethard et al., 2008; Do et al., 2011; Riaz and Girju, 2013). Several works, relying on corpus of parallel temporal and causal relations developed with specific connectives in mind (Bethard et al., 2008), have presented analyses on the interaction between temporal and causal relations (Bethard and Martin, 2008; Rink et al., 2010). Exploiting gold temporal labels as features for the causal relation classifier is shown to be beneficial. Mirza et al. (2014) presented some annotation guidelines to capture explicit causality between event pairs, inspired by TimeML. The resulting corpus, Causal-TimeBank, is then used t"
C16-1007,S13-2002,0,0.072394,"given gold entities) and Task C ‘relation type only’ (relation annotation given gold entities and related pairs). We also compare the results on the second task with the results of Laokulrat et al. (2015), who recently presented a state-of-the-art system for relation classification based on timegraphs and stacked learning. In CATENA, Task C ‘relation type only’ is performed by disabling the module for identifying temporal links described in Section 4.1. The evaluation shows that CATENA is the best performing system in both tasks, even if in Task C best precision and best recall are yielded by Bethard (2013) and Laokulrat et al. (2013), respectively. The recall drop (from .613 to .595) in Task C is because we remove the timex-timex pairs from the final 8 Available at https://www.cs.york.ac.uk/semeval-2013/task1/index.php%3Fid=data.html. Available at http://www.usna.edu/Users/cs/nchamber/caevo/. 10 Available at http://hlt-nlp.fbk.eu/technologies/causal-timebank. 11 Available at https://github.com/paramitamirza/CATENA/data/. 12 Some relation types are not used, and the VAGUE relation introduced in the first TempEval task (Verhagen et al., 2007) is adopted to cope with ambiguous temporal relations,"
C16-1007,C10-3009,0,0.0377217,"use and efficient performances (Westphal and W¨olfl, 2009). 4.2.3 Temporal Supervised Classifiers We build three supervised classification models, one for event-DCT (E-D), one for event-timex (E-T) and one for event-event (E-E) pairs. We use LIBLINEAR (Fan et al., 2008) L2-loss linear SVM (default parameters), and one-vs-rest strategy for multi-class classification. Tools and Resources Several external tools and resources are used to extract features from each temporal entity pair, including: • • • • MorphoPro (Pianta et al., 2008), to get PoS tags and phrase chunk for each token. Mate tools (Bjorkelund et al., 2010) to extract the dependency path between words. WordNet similarity module4 to compute semantic similarity (Lin, 1998) between words. Temporal signal lists from Mirza and Tonelli (2014b), further expanded using the Paraphrase Database (Ganitkevitch et al., 2013), and manually clustered e.g. {before, prior to, in advance of }. Feature Set We implemented a set of features, listed in Table 1, largely inspired by the best performing systems in TempEval-2 (Verhagen et al., 2010) and TempEval-3 (UzZaman et al., 2013) campaigns. We simplified the possible values of some features as follows: 4 http://ws"
C16-1007,Q14-1022,0,0.575612,"and the causal dimension of texts have been scarcely explored, especially from an empirical point of view. In this work, we face this challenge by presenting CATENA (CAusal and TEmporal relation extraction from NAtural language texts),1 a multi-sieve architecture for the extraction and classification of both relation types from English documents, which are pre-annotated with temporal entities, namely events and time expressions. 2 Related Work Our proposed approach for relation extraction is inspired by recent works on hybrid approaches for temporal relation extraction (D’Souza and Ng, 2013; Chambers et al., 2014). D’Souza and Ng (2013) introduce 437 hand-coded rules along with supervised classification models using lexical relation, semantic and discourse features. CAEVO, a CAscading EVent Ordering architecture by Chambers et al. (2014), combines rule-based and data-driven classifiers in a sieve-based architecture for temporal ordering. The classifiers (sieves) are ordered by their individual precision, and transitive closure is applied after each sieve to ensure consistent temporal graph. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// cr"
C16-1007,D11-1027,0,0.19586,"http:// creativecommons.org/licenses/by/4.0/ 1 The system is made available at https://github.com/paramitamirza/CATENA. 64 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 64–75, Osaka, Japan, December 11-17 2016. Figure 1: System architecture of CATENA The problem of detecting causality between events is as challenging as recognizing their temporal order, but less analyzed from an NLP perspective. Besides, previous works have mostly focused on specific types of event pairs and causal expressions in text (Bethard et al., 2008; Do et al., 2011; Riaz and Girju, 2013). Several works, relying on corpus of parallel temporal and causal relations developed with specific connectives in mind (Bethard et al., 2008), have presented analyses on the interaction between temporal and causal relations (Bethard and Martin, 2008; Rink et al., 2010). Exploiting gold temporal labels as features for the causal relation classifier is shown to be beneficial. Mirza et al. (2014) presented some annotation guidelines to capture explicit causality between event pairs, inspired by TimeML. The resulting corpus, Causal-TimeBank, is then used to build supervise"
C16-1007,N13-1112,0,0.33319,"Missing"
C16-1007,N13-1092,0,0.0219196,"Missing"
C16-1007,S13-2015,0,0.0664777,") and Task C ‘relation type only’ (relation annotation given gold entities and related pairs). We also compare the results on the second task with the results of Laokulrat et al. (2015), who recently presented a state-of-the-art system for relation classification based on timegraphs and stacked learning. In CATENA, Task C ‘relation type only’ is performed by disabling the module for identifying temporal links described in Section 4.1. The evaluation shows that CATENA is the best performing system in both tasks, even if in Task C best precision and best recall are yielded by Bethard (2013) and Laokulrat et al. (2013), respectively. The recall drop (from .613 to .595) in Task C is because we remove the timex-timex pairs from the final 8 Available at https://www.cs.york.ac.uk/semeval-2013/task1/index.php%3Fid=data.html. Available at http://www.usna.edu/Users/cs/nchamber/caevo/. 10 Available at http://hlt-nlp.fbk.eu/technologies/causal-timebank. 11 Available at https://github.com/paramitamirza/CATENA/data/. 12 Some relation types are not used, and the VAGUE relation introduced in the first TempEval task (Verhagen et al., 2007) is adopted to cope with ambiguous temporal relations, or to indicate pairs for wh"
C16-1007,W06-2106,0,0.00946001,"mex relations, we take into account temporal expressions of types DATE and TIME, and determine the relation types based on their normalized values. For example, “7 PM tonight” (2015-12-12T19:00) IS INCLUDED in “today” (2015-12-12). Event-DCT Rules The rules for labelling E-D pairs are based on the tense and/or aspect of the event word. For example, for the event mention “(had) fallen”, which is in the past tense with perfective aspect, its relation with the DCT is labelled as BEFORE. Event-timex Rules As for E-T pairs, we build a set of rules based on the temporal senses of some prepositions (Litkowski and Hargraves, 2006; Litkowski, 2014).3 In particular we assign a label whenever a temporal preposition establishes a dependency path between an event (E) and a timex (T), in which T acts as the temporal modifier of E. For example, if T is introduced by a temporal prepositions expressing a S TART T IME sense such as from or since, the relation is labelled as B EGUN BY. 2 3 Note that this is not included in the enumerated possible TLINKs in the TempEval-3 task description. We took the list of temporal prepositions from http://www.clres.com/db/classes/ClassTemporal.php. 66 In the absence of a temporal preposition,"
C16-1007,P14-1120,0,0.0194366,"ount temporal expressions of types DATE and TIME, and determine the relation types based on their normalized values. For example, “7 PM tonight” (2015-12-12T19:00) IS INCLUDED in “today” (2015-12-12). Event-DCT Rules The rules for labelling E-D pairs are based on the tense and/or aspect of the event word. For example, for the event mention “(had) fallen”, which is in the past tense with perfective aspect, its relation with the DCT is labelled as BEFORE. Event-timex Rules As for E-T pairs, we build a set of rules based on the temporal senses of some prepositions (Litkowski and Hargraves, 2006; Litkowski, 2014).3 In particular we assign a label whenever a temporal preposition establishes a dependency path between an event (E) and a timex (T), in which T acts as the temporal modifier of E. For example, if T is introduced by a temporal prepositions expressing a S TART T IME sense such as from or since, the relation is labelled as B EGUN BY. 2 3 Note that this is not included in the enumerated possible TLINKs in the TempEval-3 task description. We took the list of temporal prepositions from http://www.clres.com/db/classes/ClassTemporal.php. 66 In the absence of a temporal preposition, T might simply be"
C16-1007,S15-2134,0,0.0418402,"ghlighting not only the temporal connections between events but also cause-effect chains, would be very beneficial for providing information that the readers need. Besides, this kind of knowledge, derived from structured information about events and their temporal-causal relations, could be used in a number of applications, from tools for automated generation of timelines to question answering and decision support systems. While temporal relation classification is a well-studied task with a number of systems participating in the TempEval campaigns (Verhagen et al., 2010; UzZaman et al., 2013; Llorens et al., 2015), less attention has been devoted by the NLP community to the detection of causal links between events. Although recent attempts have tried to settle an annotation standard for causality inspired by TimeML (Mirza et al., 2014), the interactions between the temporal and the causal dimension of texts have been scarcely explored, especially from an empirical point of view. In this work, we face this challenge by presenting CATENA (CAusal and TEmporal relation extraction from NAtural language texts),1 a multi-sieve architecture for the extraction and classification of both relation types from Engl"
C16-1007,C14-1198,1,0.904643,"of parallel temporal and causal relations developed with specific connectives in mind (Bethard et al., 2008), have presented analyses on the interaction between temporal and causal relations (Bethard and Martin, 2008; Rink et al., 2010). Exploiting gold temporal labels as features for the causal relation classifier is shown to be beneficial. Mirza et al. (2014) presented some annotation guidelines to capture explicit causality between event pairs, inspired by TimeML. The resulting corpus, Causal-TimeBank, is then used to build supervised classification models for extracting causal relations (Mirza and Tonelli, 2014a). None of the above systems presents a hybrid approach in a sieve-based architecture to deal with this task. CATENA is at present the first integrated system available performing temporal and causal relation extraction. 3 System architecture The CATENA system includes two main classification modules, one for temporal and the other for causal relations between events. As shown in Figure 1, they both take as input a document annotated with the so-called temporal entities according to TimeML guidelines (Pustejovsky et al., 2003), including the document creation time (DCT), events and time expre"
C16-1007,E14-1033,1,0.87262,"of parallel temporal and causal relations developed with specific connectives in mind (Bethard et al., 2008), have presented analyses on the interaction between temporal and causal relations (Bethard and Martin, 2008; Rink et al., 2010). Exploiting gold temporal labels as features for the causal relation classifier is shown to be beneficial. Mirza et al. (2014) presented some annotation guidelines to capture explicit causality between event pairs, inspired by TimeML. The resulting corpus, Causal-TimeBank, is then used to build supervised classification models for extracting causal relations (Mirza and Tonelli, 2014a). None of the above systems presents a hybrid approach in a sieve-based architecture to deal with this task. CATENA is at present the first integrated system available performing temporal and causal relation extraction. 3 System architecture The CATENA system includes two main classification modules, one for temporal and the other for causal relations between events. As shown in Figure 1, they both take as input a document annotated with the so-called temporal entities according to TimeML guidelines (Pustejovsky et al., 2003), including the document creation time (DCT), events and time expre"
C16-1007,W14-0702,1,0.859361,"tion about events and their temporal-causal relations, could be used in a number of applications, from tools for automated generation of timelines to question answering and decision support systems. While temporal relation classification is a well-studied task with a number of systems participating in the TempEval campaigns (Verhagen et al., 2010; UzZaman et al., 2013; Llorens et al., 2015), less attention has been devoted by the NLP community to the detection of causal links between events. Although recent attempts have tried to settle an annotation standard for causality inspired by TimeML (Mirza et al., 2014), the interactions between the temporal and the causal dimension of texts have been scarcely explored, especially from an empirical point of view. In this work, we face this challenge by presenting CATENA (CAusal and TEmporal relation extraction from NAtural language texts),1 a multi-sieve architecture for the extraction and classification of both relation types from English documents, which are pre-annotated with temporal entities, namely events and time expressions. 2 Related Work Our proposed approach for relation extraction is inspired by recent works on hybrid approaches for temporal rela"
C16-1007,pianta-etal-2008-textpro,0,0.0606324,"ean Satisfiability Problem (SAT) solvers, is due to its scalability, simplicity of use and efficient performances (Westphal and W¨olfl, 2009). 4.2.3 Temporal Supervised Classifiers We build three supervised classification models, one for event-DCT (E-D), one for event-timex (E-T) and one for event-event (E-E) pairs. We use LIBLINEAR (Fan et al., 2008) L2-loss linear SVM (default parameters), and one-vs-rest strategy for multi-class classification. Tools and Resources Several external tools and resources are used to extract features from each temporal entity pair, including: • • • • MorphoPro (Pianta et al., 2008), to get PoS tags and phrase chunk for each token. Mate tools (Bjorkelund et al., 2010) to extract the dependency path between words. WordNet similarity module4 to compute semantic similarity (Lin, 1998) between words. Temporal signal lists from Mirza and Tonelli (2014b), further expanded using the Paraphrase Database (Ganitkevitch et al., 2013), and manually clustered e.g. {before, prior to, in advance of }. Feature Set We implemented a set of features, listed in Table 1, largely inspired by the best performing systems in TempEval-2 (Verhagen et al., 2010) and TempEval-3 (UzZaman et al., 2013"
C16-1007,W13-4004,0,0.102531,"ommons.org/licenses/by/4.0/ 1 The system is made available at https://github.com/paramitamirza/CATENA. 64 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 64–75, Osaka, Japan, December 11-17 2016. Figure 1: System architecture of CATENA The problem of detecting causality between events is as challenging as recognizing their temporal order, but less analyzed from an NLP perspective. Besides, previous works have mostly focused on specific types of event pairs and causal expressions in text (Bethard et al., 2008; Do et al., 2011; Riaz and Girju, 2013). Several works, relying on corpus of parallel temporal and causal relations developed with specific connectives in mind (Bethard et al., 2008), have presented analyses on the interaction between temporal and causal relations (Bethard and Martin, 2008; Rink et al., 2010). Exploiting gold temporal labels as features for the causal relation classifier is shown to be beneficial. Mirza et al. (2014) presented some annotation guidelines to capture explicit causality between event pairs, inspired by TimeML. The resulting corpus, Causal-TimeBank, is then used to build supervised classification models"
C16-1007,S13-2001,0,0.281638,"been scarcely explored, especially from an empirical point of view. In this work, we face this challenge by presenting CATENA (CAusal and TEmporal relation extraction from NAtural language texts),1 a multi-sieve architecture for the extraction and classification of both relation types from English documents, which are pre-annotated with temporal entities, namely events and time expressions. 2 Related Work Our proposed approach for relation extraction is inspired by recent works on hybrid approaches for temporal relation extraction (D’Souza and Ng, 2013; Chambers et al., 2014). D’Souza and Ng (2013) introduce 437 hand-coded rules along with supervised classification models using lexical relation, semantic and discourse features. CAEVO, a CAscading EVent Ordering architecture by Chambers et al. (2014), combines rule-based and data-driven classifiers in a sieve-based architecture for temporal ordering. The classifiers (sieves) are ordered by their individual precision, and transitive closure is applied after each sieve to ensure consistent temporal graph. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licen"
C16-1007,S07-1014,0,0.032215,"Missing"
C16-1232,W11-0702,0,0.0235997,"ce by emphasizing the role of sentiment polarity in a statement, or using topic modeling to define each position. In (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010) the authors propose a way to classify stances. They gather posts on different topics from online forums, and classify the statements from these debates as in favor or against the debated issue. They use the MPQA corpus to automatically generate a lexicon of entries indicative of a positive and negative argument and add information about the use of modal verbs and sentiment-based features. A similar task is proposed in Abbott et al. (2011) to recognize disagreement in online political forums between quoted text and a given response. The goal was achieved by using word-based features (for example discourse markers) as well as meta-information. Another work investigating stance classification of online posts was presented in Anand et al. (2011): there, the authors attempt to improve the unigram baseline by adding more cognitive-motivated features such as contextual information and opinion dependencies to define the target of opinion words. The results show that the use of these features improves classification results for many to"
C16-1232,W11-1701,0,0.0435906,"from these debates as in favor or against the debated issue. They use the MPQA corpus to automatically generate a lexicon of entries indicative of a positive and negative argument and add information about the use of modal verbs and sentiment-based features. A similar task is proposed in Abbott et al. (2011) to recognize disagreement in online political forums between quoted text and a given response. The goal was achieved by using word-based features (for example discourse markers) as well as meta-information. Another work investigating stance classification of online posts was presented in Anand et al. (2011): there, the authors attempt to improve the unigram baseline by adding more cognitive-motivated features such as contextual information and opinion dependencies to define the target of opinion words. The results show that the use of these features improves classification results for many topics. Another work from Gottipati et al. (2013) proposes to learn topics and support/opposition from discussions in Debatepedia by using a model based on the probabilistic distribution of the terms over the topics and the sides of the debate. A similar work on Debatepedia has been proposed by Awadallah et al"
C16-1232,P12-2041,0,0.0947457,"the topic. From each snippet in the pair, we extract two types of semantic vectors based on the co-occurrences (Turney et al., 2010) of topic keywords: one is computed over the entire snippet, while the other considering only the topic subtree. Co-occurrences are extracted from a window of 8 tokens and weighed using local pointwise mutual information. We then compute the cosine similarity between the vectors of the two sides of the snippet. Entailment: The presence of entailment between the two snippets can be relevant to define if the position expressed by a speaker is accepted by the other (Cabrio and Villata, 2012). For this feature, we use the Excitement Open Platform (Magnini et al., 2014). For each pair, we use information about the entailment between the two snippets (in both the directions) and between the text in the subtrees related to the topic (in both the directions). Lemma overlap: Past works showed that lexical overlap contributes to determining topical alignment between two texts (Somasundaran et al., 2009). Therefore, we compute lemma overlap of nouns, verbs and adjectives between two snippets. Although lexical overlap is already integrated in the textual entailment features, we believe th"
C16-1232,W10-3110,0,0.0190081,"the subtrees related to the topic (in both the directions). Lemma overlap: Past works showed that lexical overlap contributes to determining topical alignment between two texts (Somasundaran et al., 2009). Therefore, we compute lemma overlap of nouns, verbs and adjectives between two snippets. Although lexical overlap is already integrated in the textual entailment features, we believe that this information can provide useful information also in isolation. Negation: For each snippet, we extract two features related to explicit negation cues (e.g. not, don’t, never), adopting the list used in Councill et al. (2010). Using the parse tree of the snippets, we identify the words under the scope of a negation, and then consider as features i) the number of negated words in each snippet (normalized to its length) and ii) the percentage of the overlapping lemmas that in one snippet are under a negation. We expect that, if the same words are negated in a snippet and not in the other, this information can shed light on the relation between them. 5.2 Datasets We evaluate our approach on three different datasets. The first is the 1960 Presidential Campaign dataset presented in the pilot study (300 snippet pairs)."
C16-1232,de-marneffe-etal-2006-generating,0,0.0216798,"Missing"
C16-1232,P04-1085,0,0.530547,"ution of the terms over the topics and the sides of the debate. A similar work on Debatepedia has been proposed by Awadallah et al. (2012), to classify quotes as belonging to a topic and supporting or opposing it. Other approaches to classification rely on corpus-specific features, as in Thomas et al. (2006), who detect support and opposition to legislation in congressional debates by using speech transcriptions as well as records on voting, information about the speakers and the relations among them. Other works focus on the identification of agreement and disagreement in dialogues, such as (Galley et al., 2004; Hillard et al., 2003). They classify consecutive speech transcription segments produced by different speakers as positive or negative with respect to the discussed topic by using lexical, structural, and prosodic features. Compared to previous works, our task is different in that we perform pairwise agreement/disagreement detection between two points of view: our focus is on the relation between the two rather than the single stance. Another difference lies in the types of textual units we want to classify: we do not work on single statements but rather on longer snippets including several s"
C16-1232,D13-1191,0,0.0801189,"nt in online political forums between quoted text and a given response. The goal was achieved by using word-based features (for example discourse markers) as well as meta-information. Another work investigating stance classification of online posts was presented in Anand et al. (2011): there, the authors attempt to improve the unigram baseline by adding more cognitive-motivated features such as contextual information and opinion dependencies to define the target of opinion words. The results show that the use of these features improves classification results for many topics. Another work from Gottipati et al. (2013) proposes to learn topics and support/opposition from discussions in Debatepedia by using a model based on the probabilistic distribution of the terms over the topics and the sides of the debate. A similar work on Debatepedia has been proposed by Awadallah et al. (2012), to classify quotes as belonging to a topic and supporting or opposing it. Other approaches to classification rely on corpus-specific features, as in Thomas et al. (2006), who detect support and opposition to legislation in congressional debates by using speech transcriptions as well as records on voting, information about the"
C16-1232,N03-2012,0,0.661069,"er the topics and the sides of the debate. A similar work on Debatepedia has been proposed by Awadallah et al. (2012), to classify quotes as belonging to a topic and supporting or opposing it. Other approaches to classification rely on corpus-specific features, as in Thomas et al. (2006), who detect support and opposition to legislation in congressional debates by using speech transcriptions as well as records on voting, information about the speakers and the relations among them. Other works focus on the identification of agreement and disagreement in dialogues, such as (Galley et al., 2004; Hillard et al., 2003). They classify consecutive speech transcription segments produced by different speakers as positive or negative with respect to the discussed topic by using lexical, structural, and prosodic features. Compared to previous works, our task is different in that we perform pairwise agreement/disagreement detection between two points of view: our focus is on the relation between the two rather than the single stance. Another difference lies in the types of textual units we want to classify: we do not work on single statements but rather on longer snippets including several sentences, based on the"
C16-1232,P14-1105,0,0.021982,"08). Each pair is represented by four sentiment scores, two scores for each snippet in the pair. We rely on the sentiment analysis module in the Stanford CoreNLP (Socher et al., 2013). We use a global sentiment score for each snippet (obtained by the average of the sentiment score of each sentence in it), and a score for the sentiment in 2465 the subtrees related to the topic (obtained by the average of the sentiment score of each content word in the subtrees). Word embeddings: Past works showed that word embeddings are an effective tool to define ideological positions in political documents (Iyyer et al., 2014). In our case, we do not focus on the sentence level, but rather on the keywords defining the topics debated in our pairs. We treat each snippet separately and we obtain two vectors for each pair: one vector representing the keywords of the topic in the first snippet and one vector for the topic in the second snippet (e.g. a vector for Castro Regime in Kennedy and a vector for Castro Regime in Nixon). The vectors are extracted using Word2vec (Mikolov et al., 2013) on each snippet (425 words on average with the topic occurring multiple times), with continuous bag-of-word algorithm, a windows si"
C16-1232,W11-1902,0,0.0550403,"Missing"
C16-1232,P14-5008,0,0.0142094,"based on the co-occurrences (Turney et al., 2010) of topic keywords: one is computed over the entire snippet, while the other considering only the topic subtree. Co-occurrences are extracted from a window of 8 tokens and weighed using local pointwise mutual information. We then compute the cosine similarity between the vectors of the two sides of the snippet. Entailment: The presence of entailment between the two snippets can be relevant to define if the position expressed by a speaker is accepted by the other (Cabrio and Villata, 2012). For this feature, we use the Excitement Open Platform (Magnini et al., 2014). For each pair, we use information about the entailment between the two snippets (in both the directions) and between the text in the subtrees related to the topic (in both the directions). Lemma overlap: Past works showed that lexical overlap contributes to determining topical alignment between two texts (Somasundaran et al., 2009). Therefore, we compute lemma overlap of nouns, verbs and adjectives between two snippets. Although lexical overlap is already integrated in the textual entailment features, we believe that this information can provide useful information also in isolation. Negation"
C16-1232,D13-1170,0,0.00420567,"the keywords representing the topic, and then we extract the features from them. With this pruning, we focus less on the context and more on the information which is directly related to the topic. For each pair of snippets, we extract the following features: Sentiment information: These features are inspired by previous works using sentiment information to predict a speaker’s opinion on a topic (Pang and Lee, 2008; Abbasi et al., 2008). Each pair is represented by four sentiment scores, two scores for each snippet in the pair. We rely on the sentiment analysis module in the Stanford CoreNLP (Socher et al., 2013). We use a global sentiment score for each snippet (obtained by the average of the sentiment score of each sentence in it), and a score for the sentiment in 2465 the subtrees related to the topic (obtained by the average of the sentiment score of each content word in the subtrees). Word embeddings: Past works showed that word embeddings are an effective tool to define ideological positions in political documents (Iyyer et al., 2014). In our case, we do not focus on the sentence level, but rather on the keywords defining the topics debated in our pairs. We treat each snippet separately and we o"
C16-1232,P09-1026,0,0.0660244,"proach is effective on each of them: the one created for the feasibility study presented in Section 4.1, an extended version of the same dataset, and a larger one extracted from Debatepedia presented in Section 5.2. 2 Related Work Given the highly polarized nature of political debates, we can find in the literature many works focused on classifying political statements as supporting or opposing a debated topic. This classification can be approached in different ways, for instance by emphasizing the role of sentiment polarity in a statement, or using topic modeling to define each position. In (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010) the authors propose a way to classify stances. They gather posts on different topics from online forums, and classify the statements from these debates as in favor or against the debated issue. They use the MPQA corpus to automatically generate a lexicon of entries indicative of a positive and negative argument and add information about the use of modal verbs and sentiment-based features. A similar task is proposed in Abbott et al. (2011) to recognize disagreement in online political forums between quoted text and a given response. The goal was achieved by using"
C16-1232,W10-0214,0,0.492076,"ed classification. We do not focus on debates in dialogical form, but we rather consider sets of documents, in which politicians may express their position with respect to different topics in an implicit or explicit way, like during an electoral campaign. We create and make available three different datasets. 1 Introduction When it comes to evaluate whether the statements of two persons are in agreement or not about a topic, several past works approach the problem by classifying the single statements as supporting or opposing the topic, considering the task as a variant of sentiment analysis (Somasundaran and Wiebe, 2010). These approaches proved to be reliable in specific settings, where the goal of the statements was to express support or opposition w.r.t. the topic. However, when applied to the political domain, they often result into an oversimplified representation of the dynamics involved in the comparison of two positions. In our view, several aspects contribute to the assessment of agreement and disagreement in the political domain, requiring to be properly addressed. As an example, let us consider two excerpts uttered by Kennedy and Nixon in 1960 about the situation in Cuba under the Castro regime: Ke"
C16-1232,W09-3210,0,0.0179792,"of the two sides of the snippet. Entailment: The presence of entailment between the two snippets can be relevant to define if the position expressed by a speaker is accepted by the other (Cabrio and Villata, 2012). For this feature, we use the Excitement Open Platform (Magnini et al., 2014). For each pair, we use information about the entailment between the two snippets (in both the directions) and between the text in the subtrees related to the topic (in both the directions). Lemma overlap: Past works showed that lexical overlap contributes to determining topical alignment between two texts (Somasundaran et al., 2009). Therefore, we compute lemma overlap of nouns, verbs and adjectives between two snippets. Although lexical overlap is already integrated in the textual entailment features, we believe that this information can provide useful information also in isolation. Negation: For each snippet, we extract two features related to explicit negation cues (e.g. not, don’t, never), adopting the list used in Councill et al. (2010). Using the parse tree of the snippets, we identify the words under the scope of a negation, and then consider as features i) the number of negated words in each snippet (normalized t"
C16-1232,W06-1639,0,0.116287,"s to define the target of opinion words. The results show that the use of these features improves classification results for many topics. Another work from Gottipati et al. (2013) proposes to learn topics and support/opposition from discussions in Debatepedia by using a model based on the probabilistic distribution of the terms over the topics and the sides of the debate. A similar work on Debatepedia has been proposed by Awadallah et al. (2012), to classify quotes as belonging to a topic and supporting or opposing it. Other approaches to classification rely on corpus-specific features, as in Thomas et al. (2006), who detect support and opposition to legislation in congressional debates by using speech transcriptions as well as records on voting, information about the speakers and the relations among them. Other works focus on the identification of agreement and disagreement in dialogues, such as (Galley et al., 2004; Hillard et al., 2003). They classify consecutive speech transcription segments produced by different speakers as positive or negative with respect to the discussed topic by using lexical, structural, and prosodic features. Compared to previous works, our task is different in that we perf"
C16-1265,S13-2002,0,0.086137,"n is often modelled as a graph, with times and events/states as the nodes and temporal relations holding between them as the arcs. The details of how these three primitives are expressed in English, as well as their conceptual background (Allen, 1984; Moens and Steedman, 1987) have been discussed in Setzer (2001), and formalized in the TimeML annotation standard (Pustejovsky et al., 2003). In this work we focus on the task of ordering temporal entities, i.e., the classification of temporal relation types. Current state-of the-art systems for temporal ordering resort to data-driven approaches (Bethard, 2013; Laokulrat et al., 2013; Mirza and Tonelli, 2014) or hybrid approaches combining rules and supervised classifiers (D’Souza and Ng, 2013; Chambers et al., 2014; Mirza and Tonelli, 2016). In building the classification models, most approaches rely primarily on morpho-syntactic features as well as lexical semantic information derived from WordNet synsets (Chambers et al., 2007; Laokulrat et al., 2013; Chambers et al., 2014) and VerbOcean semantic relations between verbs (Mani et al., 2006; D’Souza and Ng, 2013). Other approaches exploit sentence-level semantic information, i.e. predicate-argumen"
C16-1265,C10-3009,0,0.0166011,"ement a simple set of rules based on the values of time expressions, which proved to be effective for most T-T edges. E-D, E-T and E-E Classifiers A set of features, listed in Table 1, is used for each type of edge, largely inspired by the best performing systems in TempEval-2 (Verhagen et al., 2010) and TempEval-3 (UzZaman et al., 2013) campaigns. We assume that pairs of temporal entities are given, and we rely on EVENT and TIMEX 3 attributes in annotated TimeML documents, morphosyntactic information generated by MorphoPro (Pianta et al., 2008) and dependency information from the Mate tools (Bjorkelund et al., 2010). Derczynski and Gaizauskas (2013) show the importance of temporal signals in temporal relation labelling, hence, we include also a similar set of features. However, we take the list of temporal signals from the TimeBank corpus, further expand it using the Paraphrase Database (Ganitkevitch et al., 2013), and manually cluster synonymous signals together, e.g. {before, prior to, in advance of }. The cluster ID is then included in the feature set instead of the signal text. Note that the only lexical semantic information we include in the feature set is the Wordnet semantic similarity/relatedness"
C16-1265,D14-1199,0,0.0683374,"Missing"
C16-1265,D15-1262,0,0.0194538,"event is related to the DCT. Such implicit relations are probably covered by hand-crafted rules or features based on the tense, aspect and modality of event words (Chambers et al., 2014), but sometimes such an overt indicator is lacking, as exemplified in previous examples (Section 1). Most works on implicit discourse relations focused on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), in which relations are annotated at the discourse level and organized into a three-level hier2819 archy. The top level relations, for example, include Temporal, Contingency, Comparison and Expansion. Braud and Denis (2015) presented a detailed comparative studies for assessing the benefit of unsupervised word representations, i.e. one-hot word pair representations against low-dimensional ones based on Brown cluster (Brown et al., 1992) and word embeddings, for identifying implicit discourse relations in PDTB. However, only the top level relations are considered, for instance, whether there exists a Temporal relation without investigating further into the more fine-grained temporal ordering. 3 Classifying Temporal Relations Temporal relations, or temporal links, are annotations that connect markables bearing tem"
C16-1265,J92-4003,0,0.281834,"Missing"
C16-1265,P07-2044,0,0.0326574,"tejovsky et al., 2003). In this work we focus on the task of ordering temporal entities, i.e., the classification of temporal relation types. Current state-of the-art systems for temporal ordering resort to data-driven approaches (Bethard, 2013; Laokulrat et al., 2013; Mirza and Tonelli, 2014) or hybrid approaches combining rules and supervised classifiers (D’Souza and Ng, 2013; Chambers et al., 2014; Mirza and Tonelli, 2016). In building the classification models, most approaches rely primarily on morpho-syntactic features as well as lexical semantic information derived from WordNet synsets (Chambers et al., 2007; Laokulrat et al., 2013; Chambers et al., 2014) and VerbOcean semantic relations between verbs (Mani et al., 2006; D’Souza and Ng, 2013). Other approaches exploit sentence-level semantic information, i.e. predicate-argument structure, as features for the classifiers (Llorens et al., 2010; Laokulrat et al., 2013; D’Souza and Ng, 2013). However, the evaluation results of TempEval-3 (UzZaman et al., 2013) show that a system with basic morphosyntactic and lexical semantic features, such as ClearTK (Bethard, 2013), is hard to beat even if using more sophisticated semantic features. Indeed, ClearTK"
C16-1265,Q14-1022,0,0.238409,"assification, as detailed in Section 3. Specifically, we want to establish (i) which vector combination schemes are more suitable for classifying pairs of events, (ii) how well word embeddings can be used for this particular task compared to traditional features (Section 4.2), and finally, (iii) whether the combination of traditional features and word embeddings yields a better performance than using the two components in isolation. To the latter purpose, we compare vector concatenation and stacked learning (Section 4.3). Experiments and evaluations are performed on the TimeBank-Dense corpus (Chambers et al., 2014), which was designed to address the sparsity issue in existing corpora with temporal annotation. We also compare our approach with CAEVO, a CAscading EVent Ordering system evaluated on the same corpus (Section 5). 2 Related Work Many natural language processing applications such as information extraction (IE), question answering (QA), topic detection and tracking require understanding about temporally located events, i.e., to anchor events in time and order them. This temporal information is often modelled as a graph, with times and events/states as the nodes and temporal relations holding bet"
C16-1265,W04-3205,0,0.0726775,"edings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2818–2828, Osaka, Japan, December 11-17 2016. over-fitting. It is also very challenging to achieve any interesting semantic generalization with this representation. Consider for instance, (attack, injured) that would be at equal distance from a synonymic pair (raid, hurt) and an antonymic pair (died, shooting). Other approaches make use of semantic features extracted from external knowledge bases such as WordNet synsets (Fellbaum, 1998) and VerbOcean semantic relations between verbs (Chklovski and Pantel, 2004), capturing for instance that marriage happens-before divorce. Mirza and Tonelli (2014) exploit the list of event duration distribution from Gusev et al. (2011) for temporal relation classification, showing that it gives no benefit to classifier performance. The problem with such knowledge bases is that they have limited coverage, while approaches based on distributional semantics require no supervision and have a much better coverage. The main goal of this work is to assess the contribution of dense vector representations of words and word pairs to temporal relation type classification, as de"
C16-1265,P13-2114,0,0.0160644,"s based on the values of time expressions, which proved to be effective for most T-T edges. E-D, E-T and E-E Classifiers A set of features, listed in Table 1, is used for each type of edge, largely inspired by the best performing systems in TempEval-2 (Verhagen et al., 2010) and TempEval-3 (UzZaman et al., 2013) campaigns. We assume that pairs of temporal entities are given, and we rely on EVENT and TIMEX 3 attributes in annotated TimeML documents, morphosyntactic information generated by MorphoPro (Pianta et al., 2008) and dependency information from the Mate tools (Bjorkelund et al., 2010). Derczynski and Gaizauskas (2013) show the importance of temporal signals in temporal relation labelling, hence, we include also a similar set of features. However, we take the list of temporal signals from the TimeBank corpus, further expand it using the Paraphrase Database (Ganitkevitch et al., 2013), and manually cluster synonymous signals together, e.g. {before, prior to, in advance of }. The cluster ID is then included in the feature set instead of the signal text. Note that the only lexical semantic information we include in the feature set is the Wordnet semantic similarity/relatedness (Lin, 1998) between event words."
C16-1265,N13-1112,0,0.305784,"Missing"
C16-1265,N13-1092,0,0.0627213,"Missing"
C16-1265,W11-0116,0,0.0212496,"g. It is also very challenging to achieve any interesting semantic generalization with this representation. Consider for instance, (attack, injured) that would be at equal distance from a synonymic pair (raid, hurt) and an antonymic pair (died, shooting). Other approaches make use of semantic features extracted from external knowledge bases such as WordNet synsets (Fellbaum, 1998) and VerbOcean semantic relations between verbs (Chklovski and Pantel, 2004), capturing for instance that marriage happens-before divorce. Mirza and Tonelli (2014) exploit the list of event duration distribution from Gusev et al. (2011) for temporal relation classification, showing that it gives no benefit to classifier performance. The problem with such knowledge bases is that they have limited coverage, while approaches based on distributional semantics require no supervision and have a much better coverage. The main goal of this work is to assess the contribution of dense vector representations of words and word pairs to temporal relation type classification, as detailed in Section 3. Specifically, we want to establish (i) which vector combination schemes are more suitable for classifying pairs of events, (ii) how well wo"
C16-1265,K15-1027,0,0.0361605,"Missing"
C16-1265,S13-2015,0,0.212511,"lled as a graph, with times and events/states as the nodes and temporal relations holding between them as the arcs. The details of how these three primitives are expressed in English, as well as their conceptual background (Allen, 1984; Moens and Steedman, 1987) have been discussed in Setzer (2001), and formalized in the TimeML annotation standard (Pustejovsky et al., 2003). In this work we focus on the task of ordering temporal entities, i.e., the classification of temporal relation types. Current state-of the-art systems for temporal ordering resort to data-driven approaches (Bethard, 2013; Laokulrat et al., 2013; Mirza and Tonelli, 2014) or hybrid approaches combining rules and supervised classifiers (D’Souza and Ng, 2013; Chambers et al., 2014; Mirza and Tonelli, 2016). In building the classification models, most approaches rely primarily on morpho-syntactic features as well as lexical semantic information derived from WordNet synsets (Chambers et al., 2007; Laokulrat et al., 2013; Chambers et al., 2014) and VerbOcean semantic relations between verbs (Mani et al., 2006; D’Souza and Ng, 2013). Other approaches exploit sentence-level semantic information, i.e. predicate-argument structure, as features"
C16-1265,Q15-1016,0,0.0473845,"arris, 1954), which states that words which are similar in meaning occur in similar contexts. Baroni et al. (2014) divide approaches based on this principle into two categories: (i) count-based models and (ii) predictive models. They also provide a systematic comparison of word vectors from the two models, on a wide range of lexical semantic tasks, including semantic relatedness, synonym detection, concept categorization, selectional preferences and analogy. The main takeaway is that predictive models, such as Word2Vec (Mikolov et al., 2013), are shown to perform better than count-based ones. Levy et al. (2015) reveal that much of the performance gains of word embeddings are due to hyperparameter optimizations rather than the embedding algorithms themselves, thus refuting the claim that prediction-based methods are superior to count-based approaches. However, they also state that the Skip-Gram model with Negative Sampling (SGNS), which is used to build Word2Vec pre-trained word vectors, can be a robust baseline since it does not significantly underperform in any scenario. In this work, we explore how well word embeddings can be used—as lexical semantic features— to capture the temporal order of even"
C16-1265,S10-1063,0,0.113349,"014) or hybrid approaches combining rules and supervised classifiers (D’Souza and Ng, 2013; Chambers et al., 2014; Mirza and Tonelli, 2016). In building the classification models, most approaches rely primarily on morpho-syntactic features as well as lexical semantic information derived from WordNet synsets (Chambers et al., 2007; Laokulrat et al., 2013; Chambers et al., 2014) and VerbOcean semantic relations between verbs (Mani et al., 2006; D’Souza and Ng, 2013). Other approaches exploit sentence-level semantic information, i.e. predicate-argument structure, as features for the classifiers (Llorens et al., 2010; Laokulrat et al., 2013; D’Souza and Ng, 2013). However, the evaluation results of TempEval-3 (UzZaman et al., 2013) show that a system with basic morphosyntactic and lexical semantic features, such as ClearTK (Bethard, 2013), is hard to beat even if using more sophisticated semantic features. Indeed, ClearTK indirectly uses distributed lexical semantic features in the form of context (tokens appearing) between events. As far as we know, there is no work on the task of ordering/anchoring temporal entities which specifically addresses the issue of implicit relations often recurring when two ev"
C16-1265,P06-1095,0,0.0353781,"emporal relation types. Current state-of the-art systems for temporal ordering resort to data-driven approaches (Bethard, 2013; Laokulrat et al., 2013; Mirza and Tonelli, 2014) or hybrid approaches combining rules and supervised classifiers (D’Souza and Ng, 2013; Chambers et al., 2014; Mirza and Tonelli, 2016). In building the classification models, most approaches rely primarily on morpho-syntactic features as well as lexical semantic information derived from WordNet synsets (Chambers et al., 2007; Laokulrat et al., 2013; Chambers et al., 2014) and VerbOcean semantic relations between verbs (Mani et al., 2006; D’Souza and Ng, 2013). Other approaches exploit sentence-level semantic information, i.e. predicate-argument structure, as features for the classifiers (Llorens et al., 2010; Laokulrat et al., 2013; D’Souza and Ng, 2013). However, the evaluation results of TempEval-3 (UzZaman et al., 2013) show that a system with basic morphosyntactic and lexical semantic features, such as ClearTK (Bethard, 2013), is hard to beat even if using more sophisticated semantic features. Indeed, ClearTK indirectly uses distributed lexical semantic features in the form of context (tokens appearing) between events. A"
C16-1265,P02-1047,0,0.36823,"Missing"
C16-1265,E14-1033,1,0.908797,"hnical Papers, pages 2818–2828, Osaka, Japan, December 11-17 2016. over-fitting. It is also very challenging to achieve any interesting semantic generalization with this representation. Consider for instance, (attack, injured) that would be at equal distance from a synonymic pair (raid, hurt) and an antonymic pair (died, shooting). Other approaches make use of semantic features extracted from external knowledge bases such as WordNet synsets (Fellbaum, 1998) and VerbOcean semantic relations between verbs (Chklovski and Pantel, 2004), capturing for instance that marriage happens-before divorce. Mirza and Tonelli (2014) exploit the list of event duration distribution from Gusev et al. (2011) for temporal relation classification, showing that it gives no benefit to classifier performance. The problem with such knowledge bases is that they have limited coverage, while approaches based on distributional semantics require no supervision and have a much better coverage. The main goal of this work is to assess the contribution of dense vector representations of words and word pairs to temporal relation type classification, as detailed in Section 3. Specifically, we want to establish (i) which vector combination sc"
C16-1265,C16-1007,1,0.760014,"re expressed in English, as well as their conceptual background (Allen, 1984; Moens and Steedman, 1987) have been discussed in Setzer (2001), and formalized in the TimeML annotation standard (Pustejovsky et al., 2003). In this work we focus on the task of ordering temporal entities, i.e., the classification of temporal relation types. Current state-of the-art systems for temporal ordering resort to data-driven approaches (Bethard, 2013; Laokulrat et al., 2013; Mirza and Tonelli, 2014) or hybrid approaches combining rules and supervised classifiers (D’Souza and Ng, 2013; Chambers et al., 2014; Mirza and Tonelli, 2016). In building the classification models, most approaches rely primarily on morpho-syntactic features as well as lexical semantic information derived from WordNet synsets (Chambers et al., 2007; Laokulrat et al., 2013; Chambers et al., 2014) and VerbOcean semantic relations between verbs (Mani et al., 2006; D’Souza and Ng, 2013). Other approaches exploit sentence-level semantic information, i.e. predicate-argument structure, as features for the classifiers (Llorens et al., 2010; Laokulrat et al., 2013; D’Souza and Ng, 2013). However, the evaluation results of TempEval-3 (UzZaman et al., 2013) s"
C16-1265,P87-1001,0,0.385207,"ading EVent Ordering system evaluated on the same corpus (Section 5). 2 Related Work Many natural language processing applications such as information extraction (IE), question answering (QA), topic detection and tracking require understanding about temporally located events, i.e., to anchor events in time and order them. This temporal information is often modelled as a graph, with times and events/states as the nodes and temporal relations holding between them as the arcs. The details of how these three primitives are expressed in English, as well as their conceptual background (Allen, 1984; Moens and Steedman, 1987) have been discussed in Setzer (2001), and formalized in the TimeML annotation standard (Pustejovsky et al., 2003). In this work we focus on the task of ordering temporal entities, i.e., the classification of temporal relation types. Current state-of the-art systems for temporal ordering resort to data-driven approaches (Bethard, 2013; Laokulrat et al., 2013; Mirza and Tonelli, 2014) or hybrid approaches combining rules and supervised classifiers (D’Souza and Ng, 2013; Chambers et al., 2014; Mirza and Tonelli, 2016). In building the classification models, most approaches rely primarily on morp"
C16-1265,P14-2012,0,0.0614707,"Missing"
C16-1265,pianta-etal-2008-textpro,0,0.0169348,"for event-event (E-E) edges. For timex-timex (T-T) relations, we implement a simple set of rules based on the values of time expressions, which proved to be effective for most T-T edges. E-D, E-T and E-E Classifiers A set of features, listed in Table 1, is used for each type of edge, largely inspired by the best performing systems in TempEval-2 (Verhagen et al., 2010) and TempEval-3 (UzZaman et al., 2013) campaigns. We assume that pairs of temporal entities are given, and we rely on EVENT and TIMEX 3 attributes in annotated TimeML documents, morphosyntactic information generated by MorphoPro (Pianta et al., 2008) and dependency information from the Mate tools (Bjorkelund et al., 2010). Derczynski and Gaizauskas (2013) show the importance of temporal signals in temporal relation labelling, hence, we include also a similar set of features. However, we take the list of temporal signals from the TimeBank corpus, further expand it using the Paraphrase Database (Ganitkevitch et al., 2013), and manually cluster synonymous signals together, e.g. {before, prior to, in advance of }. The cluster ID is then included in the feature set instead of the signal text. Note that the only lexical semantic information we"
C16-1265,prasad-etal-2008-penn,0,0.0256514,"s. As far as we know, there is no work on the task of ordering/anchoring temporal entities which specifically addresses the issue of implicit relations often recurring when two events are in different sentences, or when an event is related to the DCT. Such implicit relations are probably covered by hand-crafted rules or features based on the tense, aspect and modality of event words (Chambers et al., 2014), but sometimes such an overt indicator is lacking, as exemplified in previous examples (Section 1). Most works on implicit discourse relations focused on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), in which relations are annotated at the discourse level and organized into a three-level hier2819 archy. The top level relations, for example, include Temporal, Contingency, Comparison and Expansion. Braud and Denis (2015) presented a detailed comparative studies for assessing the benefit of unsupervised word representations, i.e. one-hot word pair representations against low-dimensional ones based on Brown cluster (Brown et al., 1992) and word embeddings, for identifying implicit discourse relations in PDTB. However, only the top level relations are considered, for instance, whether there e"
C16-1265,S14-2033,0,0.0664227,"Missing"
C16-1265,S13-2001,0,0.0517817,"ime expressions, which proved to be effective for most T-T edges. E-D, E-T and E-E Classifiers A set of features, listed in Table 1, is used for each type of edge, largely inspired by the best performing systems in TempEval-2 (Verhagen et al., 2010) and TempEval-3 (UzZaman et al., 2013) campaigns. We assume that pairs of temporal entities are given, and we rely on EVENT and TIMEX 3 attributes in annotated TimeML documents, morphosyntactic information generated by MorphoPro (Pianta et al., 2008) and dependency information from the Mate tools (Bjorkelund et al., 2010). Derczynski and Gaizauskas (2013) show the importance of temporal signals in temporal relation labelling, hence, we include also a similar set of features. However, we take the list of temporal signals from the TimeBank corpus, further expand it using the Paraphrase Database (Ganitkevitch et al., 2013), and manually cluster synonymous signals together, e.g. {before, prior to, in advance of }. The cluster ID is then included in the feature set instead of the signal text. Note that the only lexical semantic information we include in the feature set is the Wordnet semantic similarity/relatedness (Lin, 1998) between event words."
C16-1265,S07-1014,0,0.226095,"Missing"
D09-1029,W08-2208,0,0.183762,"Missing"
D09-1029,miguel-buitelaar-2008-domain,0,0.0261028,"example sentences also for other languages. This would represent the starting point towards the creation of FrameNet for new languages. Indeed, FrameNet structure comprises a language-independent level of information, namely frame and frame element definitions, and a language-dependent one, i.e. the lexical units and the example sentences. This makes the resource particularly suitable to corpus-based (semi) automatic creation of FrameNet for new languages, because the descriptive part can be pre282 served and the language-dependent layer can be populated with new instances in other languages (Crespo and Buitelaar, 2008). We apply our extraction algorithm to the Italian Wikipedia. Since several approaches have been experimented to (semi) automatically build Italian FrameNet using WordNet (De Cao et al. (2008) and Tonelli and Pighin (2009)), we believe that our new proposal to exploit Wikipedia may be of interest in the research community. Anyhow, the approach can be exploited in principle for every language available in Wikipedia. 7.1 Linked Wikipages in Italian N. of extracted sents Avg. sents per Italian sense Italian Wikipedia 371 23,078 62 Table 3: Extracted data from Italian Wikipedia ber of sentences ex"
D09-1029,D07-1074,0,0.00869874,"o automatically collect the missing sentences. Anyhow, the algorithm we propose is suitable also for expanding sentence sets already present in FrameNet. 1 3 The Mapping Algorithm In this section, we describe how to map a frame – lexical unit pair (F, l) into the Wikipedia article that best captures the sense of l as defined in F . The mapping problem is casted as a supervised WSD problem, in which l must be disambiguated using F to provide the context and Wikipedia to provide the sense inventory and the training data. Even if the idea of using Wikipedia links for disambiguation is not novel (Cucerzan, 2007), it is applied for the first time to FrameNet lexical units, considering a frame as a sense definition. The proposed algorithm is summarized as follows: 2 http://framenet.icsi.berkeley.edu 277 http://en.wikipedia.org Step 1 For each lexical unit l, we collect from the English Wikipedia dump3 all contexts4 where l is the anchor of an internal link (wiki link). The set of targets represents the senses of l in Wikipedia and the contexts are used as labelled training examples. For example, the lexical unit building.n in the frame Buildings is an anchor in 708 different contexts that point to 42 d"
D09-1029,P06-1117,0,0.0226785,"Missing"
D09-1029,P05-1050,1,0.802536,"similar works in the past have mainly proposed to automatically extend the FrameNet database by mapping frames and WordNet synsets (Shi and Mihalcea (2005), Johansson and Nugues (2007), and Tonelli and Pighin (2009)), we present an explorative approach that for the first time exploits Wikipedia to this purpose. In particular, given a lexical unit l belonging to a frame F , we devise a strategy to link l to the Wikipedia article that best captures the sense of l in F . This is basically a word disambiguation (WSD) problem (Erk, 2004) and to this purpose we employ a state-of-the-art WSD system (Gliozzo et al., 2005). The mapping between (F, l) pairs and Wikipedia pages could then be exploited for three further subtasks: (a) automatically extract from Wikipedia all sentences pointing to the Wikipage mapped with (F, l) and assign them to F ; (b) automatically expand the lexical units sets in the English FrameNet by exploiting the redirecting and linking strategy of Wikipedia; and (c) since Wikipedia is available in 260 languages, use the English Wikipedia article linked to (F, l) as a bridge to carry out sentence and lexical unit retrieval in other languages. The set of automatically collected data would r"
D09-1029,S07-1018,0,0.0715906,"K(x Evaluation K(xi ,xj ) 6 K(xj ,xj )K(xi ,xi ) 280 http://tedlab.mit.edu/˜dr/svdlibc/ tuition that FrameNet and Wikipedia are linkable resources to a large extent and that our task is wellfounded. Baseline System output Upper bound is already divided by sense and can significantly speed-up manual annotation. On the other hand, the extracted sentences could enrich the training set of machine learning systems for frame annotation to improve the frame identification step. In fact, this task has raised growing interest in the NLP community, with a devoted challenge at the last SemEval campaign (Baker et al., 2007). This retrieval process allows also to extract from Cs all words Ws that have an embedded reference to s in the form <a href=“/wiki/Wiki Sense”...>word</a>. In this way, Ws are automatically included in F as new lexical units. In this phase, redirecting links are very useful because they automatically connect a word or expression to its nearest sense in case there is no specific page for this word. The information about redirecting allows also to account for orthographic variations of the same lexical unit, for example collectible is redirected to collectable. We explain the data extraction p"
D09-1029,S07-1016,0,0.0290782,"s the domain similarity between the contexts of the word to be disambiguated. The simplest method to estimate the domain similarity between two texts is to compute the cosine similarity of their vector representations in the vector space model (VSM). The VSM is a k-dimensional space Rk , in which the text tj is represented by a vector t~j , where the ith component is the term Gliozzo et al. (2005) proposed an elegant approach to WSD based on kernel methods. The algorithm proved effective at Senseval-3 (Mihalcea and Edmonds, 2004) and, nowadays, it still represents the state-of-the-art in WSD (Pradhan et al., 2007). Specifically, they addressed these issues: (i) independently modeling domain and syntagmatic aspects of sense distinction to improve feature representativeness; and (ii) exploiting external knowledge acquired from unlabeled data, with the purpose of drastically reducing the amount of labeled 3 http://download.wikimedia.org/enwiki/ 20090306 4 A context corresponds to a line of text in the Wikipedia dump and it is represented as a paragraph in a Wikipedia article. 278 advance. Under this setting, the domain matrix D is defined by frequency of the term wi in tj . However, such an approach does"
D09-1029,N04-3006,0,0.0349329,"Missing"
D09-1029,W09-1127,1,0.905852,"ssible to easily acquire good-quality data as a starting point for the creation of FrameNet in new languages. The evaluation reported both for the monolingual and the multilingual expansion of FrameNet shows that the approach is promising. 1 In this work, we focus on the automatic enrichment of the FrameNet database for English and we propose a new framework to extend this procedure to new languages. While similar works in the past have mainly proposed to automatically extend the FrameNet database by mapping frames and WordNet synsets (Shi and Mihalcea (2005), Johansson and Nugues (2007), and Tonelli and Pighin (2009)), we present an explorative approach that for the first time exploits Wikipedia to this purpose. In particular, given a lexical unit l belonging to a frame F , we devise a strategy to link l to the Wikipedia article that best captures the sense of l in F . This is basically a word disambiguation (WSD) problem (Erk, 2004) and to this purpose we employ a state-of-the-art WSD system (Gliozzo et al., 2005). The mapping between (F, l) pairs and Wikipedia pages could then be exploited for three further subtasks: (a) automatically extract from Wikipedia all sentences pointing to the Wikipage mapped"
D15-1095,P08-1092,0,0.0197605,"it completely automatically. We first extract from our collection of 10,475 pages the subset of pages containing a section called Life or Biography, which amount to 2,547. We consider such pages as a gold standard, since the presence of Life or Biography shows that their editors paid attention to the structure of the page, distinguishing between what belonged to the person’s biography and what not. Therefore, all subsecRelated Work To our knowledge, this is the first attempt to extract biographical sections from Wikipedia. Other past works focused on the recognition of biographical sentences (Biadsy et al., 2008; Zhou et al., 2004; Biryukov et al., 2005). However the two tasks have different goals: in our case, we aim at extracting all biographical sections, so that all events of a person’s life from birth to death are present. The other approach, instead, is used to generate biography summaries, which was a task of the DUC2004 evaluation exercise4 . Besides, while approaches for sentence selection look for textual features such as typical unigrams or bigrams that characterize biographical descriptions (Filatova and Prager, 2005), we adopt a much simpler approach by considering only section titles. O"
D15-1095,H05-1015,0,0.0295379,"ipedia. Other past works focused on the recognition of biographical sentences (Biadsy et al., 2008; Zhou et al., 2004; Biryukov et al., 2005). However the two tasks have different goals: in our case, we aim at extracting all biographical sections, so that all events of a person’s life from birth to death are present. The other approach, instead, is used to generate biography summaries, which was a task of the DUC2004 evaluation exercise4 . Besides, while approaches for sentence selection look for textual features such as typical unigrams or bigrams that characterize biographical descriptions (Filatova and Prager, 2005), we adopt a much simpler approach by considering only section titles. Other works focused on the analysis of typical events in selected articles from Wikipedia biographies by looking for a particular list of predefined events (Bamman and Smith, 2014). Our approach may complement such works by introducing a preprocessing step that extracts all and only the sections describing the biographies, upon which event extraction experiments can be performed. This would increase both the precision and the recall of the extracted information. 3 Experimental Setup In this section we detail the data used f"
D15-1095,J13-3002,0,0.0193562,"e Exact setting, a true positive is scored when all and only those sections with biographical information in a Wikipedia page are extracted. This measure is useful to understand how often it is possible to extract the complete and exact biographical text concerning a person. R F1 0.662 0.863 0.677 0.897 0.630 0.942 0.617 0.898 0.548 0.809 0.566 0.844 Table 1: Classification results using the Exact and Intersection settings • The Intersection measure, instead, assigns a score between 0 and 1 for every predicted sequence of sections based on how much it overlaps with the gold standard sequence (Johansson and Moschitti, 2013). 5 Discussion We manually inspected the output of the classifier to identify possible issues. Apart from single classification mistakes, mainly due to unusual section titles that do not appear in the training set (such as “Anathematization” in Pope Honorius I page)9 , we found that some wrong classifications depended on specific types of persons in our data set. In particular, the classifier tends to assign a positive label to sections in the pages of mythological characters, even if they cannot have a biography because they did not exist. For instance, in the page of Apollo10 there are secti"
D15-1095,N01-1025,0,0.024403,"Missing"
D15-1095,W04-3256,0,0.0643803,"ically. We first extract from our collection of 10,475 pages the subset of pages containing a section called Life or Biography, which amount to 2,547. We consider such pages as a gold standard, since the presence of Life or Biography shows that their editors paid attention to the structure of the page, distinguishing between what belonged to the person’s biography and what not. Therefore, all subsecRelated Work To our knowledge, this is the first attempt to extract biographical sections from Wikipedia. Other past works focused on the recognition of biographical sentences (Biadsy et al., 2008; Zhou et al., 2004; Biryukov et al., 2005). However the two tasks have different goals: in our case, we aim at extracting all biographical sections, so that all events of a person’s life from birth to death are present. The other approach, instead, is used to generate biography summaries, which was a task of the DUC2004 evaluation exercise4 . Besides, while approaches for sentence selection look for textual features such as typical unigrams or bigrams that characterize biographical descriptions (Filatova and Prager, 2005), we adopt a much simpler approach by considering only section titles. Other works focused"
D15-1095,Q14-1029,0,\N,Missing
D17-1318,P04-1085,0,0.0314735,"i et al., 2016; Glavaˇs et al., 2017). Measuring Agreement. Automatically measuring the level of agreement in political documents (Gottipati et al., 2013; Menini and Tonelli, 2016) has the potential of supporting political analyses such as the comparisons between campaign strategies (Burton et al., 2015), the study of promises kept and broken after elections (Naurin, 2011), the formation of coalitions (Debus, 2009) and the interactions between government and opposition (Hix and Noury, 2016). However, previous work relies on the availability of pre-defined topics, including supervised methods (Galley et al., 2004; Hillard et al., 2003), approaches leveraging collaboratively generated resources (Gottipati et al., 2013; Awadallah et al., 2012) or pairwise agreement detection from political debates (Menini and Tonelli, 2016). Our Contributions. a) New task: Given a collection of political documents such as, e.g., electoral manifestos, we look at ways to perform an automatic, topic-based agreement-disagreement classification. b) New approach: We first segment the texts into coarse-grained domains. Next, coarse domains are used to extract a fine-grained list of topic-based points of view which, in turn, ar"
D17-1318,S16-2016,1,0.880807,"Missing"
D17-1318,E17-2109,1,0.44123,"Missing"
D17-1318,D13-1191,0,0.0161033,"ical proportions (Sim et al., 2013) and the scaling on a left-right spectrum of politicians’ speeches (Slapin and Proksch, 2008). More recently, researchers looked at topic-centered approaches to provide finer-grained analyses, including segmentation methods for topic-labeled manifestos (Glavaˇs et al., 2016), supporting manual coders in identifying coarse-grained political topics (Zirn et al., 2016), as well as topic-based and cross-lingual political scaling (Nanni et al., 2016; Glavaˇs et al., 2017). Measuring Agreement. Automatically measuring the level of agreement in political documents (Gottipati et al., 2013; Menini and Tonelli, 2016) has the potential of supporting political analyses such as the comparisons between campaign strategies (Burton et al., 2015), the study of promises kept and broken after elections (Naurin, 2011), the formation of coalitions (Debus, 2009) and the interactions between government and opposition (Hix and Noury, 2016). However, previous work relies on the availability of pre-defined topics, including supervised methods (Galley et al., 2004; Hillard et al., 2003), approaches leveraging collaboratively generated resources (Gottipati et al., 2013; Awadallah et al., 2012) or"
D17-1318,N03-2012,0,0.0715586,"ˇs et al., 2017). Measuring Agreement. Automatically measuring the level of agreement in political documents (Gottipati et al., 2013; Menini and Tonelli, 2016) has the potential of supporting political analyses such as the comparisons between campaign strategies (Burton et al., 2015), the study of promises kept and broken after elections (Naurin, 2011), the formation of coalitions (Debus, 2009) and the interactions between government and opposition (Hix and Noury, 2016). However, previous work relies on the availability of pre-defined topics, including supervised methods (Galley et al., 2004; Hillard et al., 2003), approaches leveraging collaboratively generated resources (Gottipati et al., 2013; Awadallah et al., 2012) or pairwise agreement detection from political debates (Menini and Tonelli, 2016). Our Contributions. a) New task: Given a collection of political documents such as, e.g., electoral manifestos, we look at ways to perform an automatic, topic-based agreement-disagreement classification. b) New approach: We first segment the texts into coarse-grained domains. Next, coarse domains are used to extract a fine-grained list of topic-based points of view which, in turn, are used to perform class"
D17-1318,S10-1004,0,0.0705934,"Missing"
D17-1318,C16-1232,1,0.914464,"al., 2013) and the scaling on a left-right spectrum of politicians’ speeches (Slapin and Proksch, 2008). More recently, researchers looked at topic-centered approaches to provide finer-grained analyses, including segmentation methods for topic-labeled manifestos (Glavaˇs et al., 2016), supporting manual coders in identifying coarse-grained political topics (Zirn et al., 2016), as well as topic-based and cross-lingual political scaling (Nanni et al., 2016; Glavaˇs et al., 2017). Measuring Agreement. Automatically measuring the level of agreement in political documents (Gottipati et al., 2013; Menini and Tonelli, 2016) has the potential of supporting political analyses such as the comparisons between campaign strategies (Burton et al., 2015), the study of promises kept and broken after elections (Naurin, 2011), the formation of coalitions (Debus, 2009) and the interactions between government and opposition (Hix and Noury, 2016). However, previous work relies on the availability of pre-defined topics, including supervised methods (Galley et al., 2004; Hillard et al., 2003), approaches leveraging collaboratively generated resources (Gottipati et al., 2013; Awadallah et al., 2012) or pairwise agreement detecti"
D17-1318,D14-1162,0,0.08393,"Missing"
D17-1318,D13-1010,0,0.021268,"henomena has gained considerable momentum (Grimmer and Stewart, 2013), arguably because of both the availability of parliamentary proceedings (van Aggelen et al., 2017), electoral manifestos (Volkens et al., 2011) and campaign debates (Woolley and Peters, 2008), and the interest of the computational social science (CSS) community in the potential of text mining methods for advancing political science research (Lazer et al., 2009). Previous work focused on the automatic detection of sentiment expressions in political news (Young and Soroka, 2012), the identification of ideological proportions (Sim et al., 2013) and the scaling on a left-right spectrum of politicians’ speeches (Slapin and Proksch, 2008). More recently, researchers looked at topic-centered approaches to provide finer-grained analyses, including segmentation methods for topic-labeled manifestos (Glavaˇs et al., 2016), supporting manual coders in identifying coarse-grained political topics (Zirn et al., 2016), as well as topic-based and cross-lingual political scaling (Nanni et al., 2016; Glavaˇs et al., 2017). Measuring Agreement. Automatically measuring the level of agreement in political documents (Gottipati et al., 2013; Menini and"
E14-1033,S13-2015,0,0.300863,"e authors perform the same task on the full set of temporal relations, but adopt a much more complex approach. They utilize lexical relations extracted from the Merriam-Webster dictionary and WordNet (Fellbaum, 1998), as well as semantic and discourse features. They also introduce 437 hand-coded rules to build a hybrid classification model. Since we conduct our experiments based on TempEval-3 task setup, this work is also comparable with the systems participating in the task. UzZaman et al. (2013) report that three groups submitted at least one system run to the task. The best performing one (Laokulrat et al., 2013) uses, among others, sentence-level semantic information from a deep syntactic parser, namely predicate-argument structure features. Another system (Chambers, 2013) is composed of four MaxEnt classifiers, two of which have been trained for event-event links (inter- and intrasentence) and two for event-time links. The thirdranked system (Kolya et al., 2013), instead, implements a much simpler set of features accounting for event tense, modality and aspect, event and timex context, etc. sists throughout a temporal duration (e.g. John drove for 5 hours), while IS INCLUDED relation is specified wh"
E14-1033,P07-2044,0,0.554348,"reduced set, increasing the task complexity. This specific temporal relation classification task becomes the main focus of this paper. Supervised classification of temporal relation types has already been explored in some earlier works. Mani et al. (2006) built a MaxEnt classifier to label the temporal links using training data 308 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 308–317, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics which were bootstrapped by applying temporal closure. Chambers et al. (2007) focused on classifying the temporal relation type of event-event pairs using previously learned event attributes as features. However, both works use a reduced set of temporal relations, obtained by collapsing the relation types that inverse each other into a single type. Our work is most similar to the recent work by D’Souza and Ng (2013). The authors perform the same task on the full set of temporal relations, but adopt a much more complex approach. They utilize lexical relations extracted from the Merriam-Webster dictionary and WordNet (Fellbaum, 1998), as well as semantic and discourse fe"
E14-1033,P06-1095,0,0.351161,"t we can compare our system with other systems participating in the challenge. Recent 2 Related Work The task we deal with in this paper was proposed as part of the TempEval-3 shared task (UzZaman et al., 2012). Compared to previous TempEval campaigns, the TempEval-3 task involved recognizing the full set of temporal relations in TimeML (14 types) instead of a reduced set, increasing the task complexity. This specific temporal relation classification task becomes the main focus of this paper. Supervised classification of temporal relation types has already been explored in some earlier works. Mani et al. (2006) built a MaxEnt classifier to label the temporal links using training data 308 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 308–317, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics which were bootstrapped by applying temporal closure. Chambers et al. (2007) focused on classifying the temporal relation type of event-event pairs using previously learned event attributes as features. However, both works use a reduced set of temporal relations, obtained by collapsing the relation types that"
E14-1033,S13-2012,0,0.0619013,"ster dictionary and WordNet (Fellbaum, 1998), as well as semantic and discourse features. They also introduce 437 hand-coded rules to build a hybrid classification model. Since we conduct our experiments based on TempEval-3 task setup, this work is also comparable with the systems participating in the task. UzZaman et al. (2013) report that three groups submitted at least one system run to the task. The best performing one (Laokulrat et al., 2013) uses, among others, sentence-level semantic information from a deep syntactic parser, namely predicate-argument structure features. Another system (Chambers, 2013) is composed of four MaxEnt classifiers, two of which have been trained for event-event links (inter- and intrasentence) and two for event-time links. The thirdranked system (Kolya et al., 2013), instead, implements a much simpler set of features accounting for event tense, modality and aspect, event and timex context, etc. sists throughout a temporal duration (e.g. John drove for 5 hours), while IS INCLUDED relation is specified when an event happens within a temporal expression (e.g. John arrived on Tuesday). a |———| b |———| a |———| b |———| a |——| b |————| a |——| b |————| a |——| b |——————| a"
E14-1033,P09-2004,0,0.0443534,"ween DURING and IS INCLUDED (also their inverses) is that DURING relation is specified when an event perFeature set We implement a number of features for temporal relation classification. Some of them are basic ones which take into account morpho-syntactic information on events and time expressions, their textual context and their attributes. Others rely on semantic information such as typical event durations and connective type. However, we avoid complex processing of data. Such semantic information is based on external lists of lexical items 309 and on the output of the addDiscourse tagger (Pitler and Nenkova, 2009). Some features are computed independently based on either e1 or e2 , while some others are pairwise features, which are computed based on both elements. Some pairwise features are only relevant for event-event pairs, for example, the information on discourse connectives and the binary features representing whether two events have the same event attributes or not. Similarly, the features related to time expression attributes are only relevant for event-timex pairs, since this information can only be obtained if e2 is a time expression. The selection of features that contribute to the improveme"
E14-1033,P11-2061,0,0.126504,"arn from using the inverse relations and closure-based inferred relations. There are six pairs of relation types in TimeML that inverse each other (see Table 1). By switching the order of the entities in a given pair and labelling the pair with the inverse relation type, we basically multiply the number of training data. As for temporal closure, there have been attempts to apply it to improve temporal relation classification. Mani et al. (2006) use SputLink (Verhagen, 2005), which was developed based on Allen’s closure inference (Allen, 1983), to infer the relations based on temporal closure. UzZaman and Allen (2011b) employ Timegraph (Gerevini et al., 1995) to implement the scorer for TempEval-3 evaluation, since precision and recall for temporal relation classification are computed based on the closure graph. We use a simpler approach to obtain the closure graph of temporal relations, by applying the transitive closure only within the same relation type, e.g. e1 BEFORE e2 ∧ e2 BEFORE e3 → e1 BEFORE e3. It can be seen as partial temporal closure since it produces only a subset of the relations produced by temporal closure, which covers more complex cases, e.g. e1 BEFORE e2 ∧ e2 INCLUDES e3 → e1 BEFORE e"
E14-1033,W11-0116,0,0.0513793,"ation connected to the meaning of a predicate. The typical event duration allows us to infer, for instance, that a punctual event is more likely to be contained in a durative one. If we consider the sentence “State-run television broadcast footage of Cuban exiles protesting in Miami”, this feature would tell us that broadcast lasts for hours while protesting lasts for days, thus contributing in determining the direction of DURING relation between the events. The approximate duration for an event is obtained from the list of 1000 most frequent verbs and their duration distributions compiled by Gusev et al. (2011).2 The types of duration include seconds, minutes, hours, days, weeks, months, years and decades. We also add the duration difference between e1 and e2 as a feature 1 http://nlp.stanford.edu/software/ corenlp.shtml 2 The list is available at http://cs.stanford.edu/ people/agusev/durations/ Textual context. The textual order, sentence distance and entity distance of e1 and e2 . Textual order is the appearance order of e1 and e2 in the text, while sentence distance measures how far e1 and e2 are from each other in terms of sentences, i.e. 0 if they are in the same sentence. The entity distance i"
E14-1033,S13-2001,0,0.694529,"in a better performance than using more sophisticated features based on semantic role labelling and deep semantic parsing. We also investigate the impact of extracting new training instances using inverse relations and transitive closure, and gain insight into the impact of this bootstrapping methodology on classifying the full set of TempEval-3 relations. 1 Introduction In recent years, temporal processing has gained increasing attention within the NLP community, in particular since TempEval evaluation campaigns have been organized on this topic (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). In particular, the classification of temporal relations holding between entities such as events and temporal expressions (timex) is crucial to build event timelines and to reconstruct the plot of a story. This could be exploited in decision support systems and document archiving applications, among others. In this work we focus on the problem of classifying temporal relation types, assuming that the links between events and time expressions are already established. This task is part of Tempeval-3 evaluation campaign, hence we follow the guidelines and the dataset provided by the organizers,"
E14-1033,S13-2011,0,0.0208417,"Missing"
E14-1033,S07-1014,0,0.575176,"find that using a simple feature set results in a better performance than using more sophisticated features based on semantic role labelling and deep semantic parsing. We also investigate the impact of extracting new training instances using inverse relations and transitive closure, and gain insight into the impact of this bootstrapping methodology on classifying the full set of TempEval-3 relations. 1 Introduction In recent years, temporal processing has gained increasing attention within the NLP community, in particular since TempEval evaluation campaigns have been organized on this topic (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). In particular, the classification of temporal relations holding between entities such as events and temporal expressions (timex) is crucial to build event timelines and to reconstruct the plot of a story. This could be exploited in decision support systems and document archiving applications, among others. In this work we focus on the problem of classifying temporal relation types, assuming that the links between events and time expressions are already established. This task is part of Tempeval-3 evaluation campaign, hence we follow the guideline"
E14-1033,N13-1112,0,\N,Missing
E14-1033,S10-1010,0,\N,Missing
E17-2042,P10-2013,0,0.0350608,". 2017 Association for Computational Linguistics News Evaluative Descriptive Expository Instructive Narrative None Other 0.82 0.84 0.86 1.0 - Travel Reports 0.90 0.86 0.93 0.65 0.88 1.0 0.92 To test the comprehensiveness of this scheme, we annotate English texts from two different genres and periods of publication: namely, contemporary news and travel reports published between the end of the XIX Century and the beginning of the XX Century. While the former are taken from already available datasets, i.e., TempEval-3, Penn Discourse Treebank, and MASC (UzZaman et al., 2013; Prasad et al., 2008; Ide et al., 2010), the latter constitute a novel set of texts extracted from the Gutenberg project2 . The corpus is released under the name of Content Types Dataset version 1.0 (CTD v1). The resource is still being extended with new annotated texts, but in the remainder of the paper we will refer to this first version. The annotation was conducted by two expert linguists following a multi-step process and using the web-based tool CAT (Bartalesi Lenzi et al., 2012). In the first phase, annotators were allowed to discuss disagreements based on a trial corpus suggesting revisions to improve the guidelines. In the"
E17-2042,J08-4004,0,0.0992312,"l CAT (Bartalesi Lenzi et al., 2012). In the first phase, annotators were allowed to discuss disagreements based on a trial corpus suggesting revisions to improve the guidelines. In the second phase, inter-annotator agreement was calculated on a subset of the CTD v1 (a total of 5,328 tokens and 526 clauses, with 2,500 tokens and about 250 clauses per genre). Table 1 reports the Cohen’s kappa on the number of tokens for both text genres. With the exception of the INSTRUCTIVE CT, all the classes have high scores, exceeding 0.8, usually set as a threshold that guarantees good annotation quality (Artstein and Poesio, 2008). In the final phase, the whole dataset was annotated using the latest version of the guidelines which includes detailed descriptions of the classes, examples for both genres, and priority rules discriminating when more than one CT class may apply to clauses. Table 2 illustrates the composition of CTD v1. The two genres of texts show, for almost all the CT classes, a statistically significant difference (at p<0.01 and calculated with the z test) in their distribution. Table 1: Inter Annotator Agreement: Cohen’s kappa calculated at token level. scheme to account for undefined or unclear cases:"
E17-2042,baccianella-etal-2010-sentiwordnet,0,0.0169125,"mma of clause adverb, coarse tense values (present, past, future), fine-grained tense values (present perfect, etc.), voice, grammatical aspect (progressive, perfect), WordNet sense and supersense, WordNet hypernyms, length of path to the top node in WordNet, head POS Table 3: Features of the clause components. 3.1 Feature Sets other syntactic relation, and (iv) the clause verb. Details for noun phrase and verb phrase components are reported in Table 3. We extended the basic features with prior sentiment polarity scores for nouns, verbs, adjectives, and adverbs in the clause via SentiWordNet (Baccianella et al., 2010). For each target POS, polarity scores are aggregated per lemma and averaged by the number of senses, thus providing a lemma-based prior polarity. Finally, the lemma-based polarity scores are normalized by the clause length and scaled between 0 and 1. Finally, we introduced a binary feature to mark the presence/absence of a temporal expression in a clause. These two additional blocks of features have been selected following the definition of the CTs in the annotation guidelines. In particular, the presence of temporal expressions in a clause can facilitate the distinction between the NARRATIVE"
E17-2042,bartalesi-lenzi-etal-2012-cat,1,0.813257,"the former are taken from already available datasets, i.e., TempEval-3, Penn Discourse Treebank, and MASC (UzZaman et al., 2013; Prasad et al., 2008; Ide et al., 2010), the latter constitute a novel set of texts extracted from the Gutenberg project2 . The corpus is released under the name of Content Types Dataset version 1.0 (CTD v1). The resource is still being extended with new annotated texts, but in the remainder of the paper we will refer to this first version. The annotation was conducted by two expert linguists following a multi-step process and using the web-based tool CAT (Bartalesi Lenzi et al., 2012). In the first phase, annotators were allowed to discuss disagreements based on a trial corpus suggesting revisions to improve the guidelines. In the second phase, inter-annotator agreement was calculated on a subset of the CTD v1 (a total of 5,328 tokens and 526 clauses, with 2,500 tokens and about 250 clauses per genre). Table 1 reports the Cohen’s kappa on the number of tokens for both text genres. With the exception of the INSTRUCTIVE CT, all the classes have high scores, exceeding 0.8, usually set as a threshold that guarantees good annotation quality (Artstein and Poesio, 2008). In the f"
E17-2042,2007.sigdial-1.16,0,0.040168,"re set extended with the doc2vec clause embeddings. 4 The classification of text passages has been studied in previous works considering different textual units (e.g., clauses, sentences, and paragraphs) or language patterns (Kaufer et al., 2004). Several annotation schemes, often based on genre-specific taxonomies, have been proposed. This is the case, for example, of the detection of the main components in scholarly publications (Teufel et al., 2009; Liakata et al., 2012; De Waard and Maat, 2012; Burns et al., 2016) or the annotation of content zones, i.e., functional constituents of texts (Bieler et al., 2007; Stede and Kuhn, 2009; Baiamonte et al., 2016). On the contrary, the notion of Content Types that we have adopted applies across genres. CTs are based on linguistic theories on discourse/rhetorical strategies but differ from discourse relations. Over the years, different typologies have been proposed (Werlich, 1976; Biber, The SVM models have been implemented using LIBSVM (Chang and Lin, 2011) with Linear Kernel. The CRF models have been implemented with CRF++ toolkit 4 with default parameters. Content and Functional Structure Classification This set of experiments assumes an alternative mode"
E17-2042,P14-5010,0,0.00259154,"We experiment two different types of features: the first relies on distributional information extracted through sentence embeddings (Le and Mikolov, 2014), while the second is linguistically motivated and focuses on syntactic and semantic properties of the main components of the clause, i.e. the noun phrase(s) and the verb phrase. For the first type, we extracted embeddings for each clause using the doc2vec (Le and Mikolov, 2014) implementation in gensim, with vector size = 50 and window = 5. For the second feature type, all documents were pre-processed at clause level with Stanford CoreNLP (Manning et al., 2014), performing tokenization, lemmatization, POS tagging, Named Entity recognition. The extraction of basic syntactic and semantic properties of the clause components has been performed with a syntactic-semantic features toolkit (Friedrich and Pinkal, 2015). This has allowed us to identify four blocks of features for: (i) the noun phrase in subject position (i.e. nsubj and nsubjpass), (ii) the noun phrase in direct object position (i.e. dobj and agent), (iii) the noun phrase in any 262 3.2 Classification Experiments Results are illustrated in Table 4. The content-based classification experiments"
E17-2042,W15-2702,0,0.0145443,"butions of the CTs in the dataset. Furthermore, we plan to study whether information on content types can contribute to other NLP tasks. For example, we believe that identifying NARRATIVE and EVALUATIVE CTs may contribute to discriminating between clauses useful to build a storyline or a timeline of events (the former) and clauses bearing sentiment information (the latter). 1989; Chatman, 1990; Adam, 1985; Longacre, 2013) but have been rarely treated computationally, with the exception of the work by Cocco et al. (2011). The theory of Discourse Modes (DMs) (Smith, 2003) is instead followed by Mavridou et al. (2015) that apply it to a paragraph-based pilot annotation of a variety of documents such as novels, news and European Parliament proceedings. Annotators intuitively labeled DMs relying on a very short manual: as a consequence, no formal guidelines were made available and only a moderate agreement was achieved. Moreover, the final dataset is not publicly available and the recognition of DMs has not been automated yet. Our approach is different: we rely on Werlich’s typology, we provide complete annotation guidelines, we make available the annotated dataset, and we experiment automatic classification"
E17-2042,R11-1059,0,0.0330243,"Missing"
E17-2042,W12-4306,0,0.0583861,"Missing"
E17-2042,P15-1123,0,0.0237978,"es of the main components of the clause, i.e. the noun phrase(s) and the verb phrase. For the first type, we extracted embeddings for each clause using the doc2vec (Le and Mikolov, 2014) implementation in gensim, with vector size = 50 and window = 5. For the second feature type, all documents were pre-processed at clause level with Stanford CoreNLP (Manning et al., 2014), performing tokenization, lemmatization, POS tagging, Named Entity recognition. The extraction of basic syntactic and semantic properties of the clause components has been performed with a syntactic-semantic features toolkit (Friedrich and Pinkal, 2015). This has allowed us to identify four blocks of features for: (i) the noun phrase in subject position (i.e. nsubj and nsubjpass), (ii) the noun phrase in direct object position (i.e. dobj and agent), (iii) the noun phrase in any 262 3.2 Classification Experiments Results are illustrated in Table 4. The content-based classification experiments show that CTs are subject to the functional structure of the sentence and, more generally, of the document. Only the CRF classifiers, i.e. sequence labeling models, can beat the baseline, providing balanced results for Precision and Recall, and improving"
E17-2042,prasad-etal-2008-penn,0,0.016121,"pain, April 3-7, 2017. 2017 Association for Computational Linguistics News Evaluative Descriptive Expository Instructive Narrative None Other 0.82 0.84 0.86 1.0 - Travel Reports 0.90 0.86 0.93 0.65 0.88 1.0 0.92 To test the comprehensiveness of this scheme, we annotate English texts from two different genres and periods of publication: namely, contemporary news and travel reports published between the end of the XIX Century and the beginning of the XX Century. While the former are taken from already available datasets, i.e., TempEval-3, Penn Discourse Treebank, and MASC (UzZaman et al., 2013; Prasad et al., 2008; Ide et al., 2010), the latter constitute a novel set of texts extracted from the Gutenberg project2 . The corpus is released under the name of Content Types Dataset version 1.0 (CTD v1). The resource is still being extended with new annotated texts, but in the remainder of the paper we will refer to this first version. The annotation was conducted by two expert linguists following a multi-step process and using the web-based tool CAT (Bartalesi Lenzi et al., 2012). In the first phase, annotators were allowed to discuss disagreements based on a trial corpus suggesting revisions to improve the"
E17-2042,D09-1155,0,0.0118741,"se model has only basic clause features plus the polarity scores and the presence/absence of temporal expressions. • clause+doc2vec model has the clause model feature set extended with the doc2vec clause embeddings. 4 The classification of text passages has been studied in previous works considering different textual units (e.g., clauses, sentences, and paragraphs) or language patterns (Kaufer et al., 2004). Several annotation schemes, often based on genre-specific taxonomies, have been proposed. This is the case, for example, of the detection of the main components in scholarly publications (Teufel et al., 2009; Liakata et al., 2012; De Waard and Maat, 2012; Burns et al., 2016) or the annotation of content zones, i.e., functional constituents of texts (Bieler et al., 2007; Stede and Kuhn, 2009; Baiamonte et al., 2016). On the contrary, the notion of Content Types that we have adopted applies across genres. CTs are based on linguistic theories on discourse/rhetorical strategies but differ from discourse relations. Over the years, different typologies have been proposed (Werlich, 1976; Biber, The SVM models have been implemented using LIBSVM (Chang and Lin, 2011) with Linear Kernel. The CRF models hav"
E17-2042,S13-2001,0,0.0242654,"260–266, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics News Evaluative Descriptive Expository Instructive Narrative None Other 0.82 0.84 0.86 1.0 - Travel Reports 0.90 0.86 0.93 0.65 0.88 1.0 0.92 To test the comprehensiveness of this scheme, we annotate English texts from two different genres and periods of publication: namely, contemporary news and travel reports published between the end of the XIX Century and the beginning of the XX Century. While the former are taken from already available datasets, i.e., TempEval-3, Penn Discourse Treebank, and MASC (UzZaman et al., 2013; Prasad et al., 2008; Ide et al., 2010), the latter constitute a novel set of texts extracted from the Gutenberg project2 . The corpus is released under the name of Content Types Dataset version 1.0 (CTD v1). The resource is still being extended with new annotated texts, but in the remainder of the paper we will refer to this first version. The annotation was conducted by two expert linguists following a multi-step process and using the web-based tool CAT (Bartalesi Lenzi et al., 2012). In the first phase, annotators were allowed to discuss disagreements based on a trial corpus suggesting rev"
E17-2042,W15-4507,1,0.721559,"rd discourse relations, either based on rhetorical structures or lexically-grounded approaches. Content Types provide cues to access the structure of a document’s types of functional content. They contribute to the overall message or purpose of a text and make explicit the functional role of a discourse segment with respect to its content, i.e. meaning. Their identification may improve the performance of more complex NLP tasks by targeting the portions of the documents that are more relevant. For example, when building a storyline it may be useful to focus on the narrative segments of a text (Vossen et al., 2015), while for sentiment analysis the identification of evaluative clauses may be beneficial (Liu, 2015). Our contribution is threefold: i) we make available annotation guidelines with high reliability in terms of inter-annotator agreement and applicable to texts of different genres and period of publication; ii) we release the first version of a new dataset (whose annotation is still in progress) that takes into consideration both contemporary and historical texts, paving the way to a new NLP task, i.e. 2 Dataset Construction Content Types (henceforth CTs) are text passages with specific semanti"
E17-3020,S10-1063,0,0.0279989,"focus on improving the system coverage. Currently, missing trajectories are mainly due to (i) the presence of predicates not recognized as lexical units in FrameNet, e.g. exile; (ii) the lack of information in the English Wikipedia biography, and (iii) the presence of sentences with complex temporal structures, e.g., Cummings returned to Paris in 1921 and remained there for two years before returning to New York. These issues can be dealt with by adding missing predicates to FrameNet, extend Pikes to other languages and experimenting with different systems for temporal information processing (Llorens et al., 2010). We also plan to apply the methodology presented in (Aprosio and Tonelli, 2015) to automatically recognize the Wikipedia text passages dealing with biographical information, so to discard sections containing useless information. Case Study We relied on the Pantheon dataset (Yu et al., 2016) to identify a list of notable figures to be used in our case study. We chose Pantheon since it provides a ready-to-use set of people already classified into categories based on their domain occupation (e.g., Arts, Sports), birth year, nationality and gender. More specifically, we considered 2,407 individua"
E17-3020,D15-1095,1,0.830399,"mainly due to (i) the presence of predicates not recognized as lexical units in FrameNet, e.g. exile; (ii) the lack of information in the English Wikipedia biography, and (iii) the presence of sentences with complex temporal structures, e.g., Cummings returned to Paris in 1921 and remained there for two years before returning to New York. These issues can be dealt with by adding missing predicates to FrameNet, extend Pikes to other languages and experimenting with different systems for temporal information processing (Llorens et al., 2010). We also plan to apply the methodology presented in (Aprosio and Tonelli, 2015) to automatically recognize the Wikipedia text passages dealing with biographical information, so to discard sections containing useless information. Case Study We relied on the Pantheon dataset (Yu et al., 2016) to identify a list of notable figures to be used in our case study. We chose Pantheon since it provides a ready-to-use set of people already classified into categories based on their domain occupation (e.g., Arts, Sports), birth year, nationality and gender. More specifically, we considered 2,407 individuals from Europe and North America living between 1900 and 1955. First we download"
E17-3020,P14-5010,0,0.00259586,"s are cleaned up by removing infoboxes, tables and tags, keeping only the main body as raw text. Pre-processing Raw text is processed using PIKES (Corcoglioniti et al., 2015), a suite of tools for extracting frame oriented knowledge from English texts. PIKES integrates Semafor (Das et al., 2014), a system for Semantic Role Labeling based on FrameNet (Baker et al., 1998), whose output is used to identify predicates related to movements and their arguments because its high-level organization in semantic frames is an useful way to generalize over predicates. PIKES also includes Stanford CoreNLP (Manning et al., 2014). Its modules for Named Entity Recognition and Classification (NERC), coreference resolution and recognition of time expressions are used to detect for each text: (i) mentions related to the person who is the subject of the biography; (ii) locations and organizations that can be movement destinations; (iii) dates. Georeferencing To georeference all the destinations mentioned in the candidate sentences RAMBLE ON uses Nominatim5 . Due to errors by the NERC module (e.g., Artaman League annotated as geographical entity), some destinations can lack coordinates and thus are discarded. Moreover, for"
E17-3020,P98-1013,0,0.132924,"ment of the subject together with a destination. These represent the geographical position of a person at a certain time. Input Data In our approach information extraction is performed on Wikipedia biographical pages. In the first step, these pages are cleaned up by removing infoboxes, tables and tags, keeping only the main body as raw text. Pre-processing Raw text is processed using PIKES (Corcoglioniti et al., 2015), a suite of tools for extracting frame oriented knowledge from English texts. PIKES integrates Semafor (Das et al., 2014), a system for Semantic Role Labeling based on FrameNet (Baker et al., 1998), whose output is used to identify predicates related to movements and their arguments because its high-level organization in semantic frames is an useful way to generalize over predicates. PIKES also includes Stanford CoreNLP (Manning et al., 2014). Its modules for Named Entity Recognition and Classification (NERC), coreference resolution and recognition of time expressions are used to detect for each text: (i) mentions related to the person who is the subject of the biography; (ii) locations and organizations that can be movement destinations; (iii) dates. Georeferencing To georeference all"
E17-3020,C98-1013,0,\N,Missing
edouard-etal-2017-graph,W11-0705,0,\N,Missing
edouard-etal-2017-graph,W13-1103,0,\N,Missing
edouard-etal-2017-graph,E17-1076,0,\N,Missing
ghosh-etal-2012-improving,J93-2004,0,\N,Missing
ghosh-etal-2012-improving,W10-2910,1,\N,Missing
ghosh-etal-2012-improving,W05-1506,0,\N,Missing
ghosh-etal-2012-improving,J08-2005,0,\N,Missing
ghosh-etal-2012-improving,prasad-etal-2008-penn,0,\N,Missing
ghosh-etal-2012-improving,P09-2004,0,\N,Missing
ghosh-etal-2012-improving,I08-7009,0,\N,Missing
ghosh-etal-2012-improving,I11-1120,1,\N,Missing
ghosh-etal-2012-improving,I08-7010,0,\N,Missing
ghosh-etal-2012-improving,D07-1010,0,\N,Missing
girardi-etal-2014-cromer,J00-4005,0,\N,Missing
girardi-etal-2014-cromer,D12-1045,0,\N,Missing
girardi-etal-2014-cromer,P12-2045,0,\N,Missing
girardi-etal-2014-cromer,P10-1143,0,\N,Missing
girardi-etal-2014-cromer,bartalesi-lenzi-etal-2012-cat,1,\N,Missing
girardi-etal-2014-cromer,cybulska-vossen-2014-using,0,\N,Missing
girardi-etal-2014-cromer,day-etal-2008-corpus,0,\N,Missing
girardi-etal-2014-cromer,pianta-etal-2008-textpro,1,\N,Missing
girardi-etal-2014-cromer,W13-1202,1,\N,Missing
I11-1120,W05-0305,0,0.333358,"results. However, their task was limited to retrieving the argument heads. In contrast, we integrate discourse segmentation in the parsing pipeline because we believe that spans are necessary when using the discourse arguments as input to applications such as opinion mining, where attributions need to be explicitly marked. Besides, no gold data are available for head-based discourse parsing evaluation and they have to be automatically derived from parse trees with a further processing step. With our approach, instead, we can directly use PDTB argument spans both for training and for testing. Dinesh et al. (2005) extracted complete arguments with boundaries, but only for a restricted class of connectives. The recent work by Prasad et al. (2010) is also limited, since their system only extracts the sentences containing the arguments. In our work, we assume that explicit discourse connectives are given beforehand, either taken directly from a gold standard or automatically identified. The second task based on PDTB was tackled among others by Pitler et al. (2008) and Pitler and Nenkova (2009). In addition to the work on finding explicit connectives and their arguments, there has been recent work on class"
I11-1120,W10-2910,1,0.667362,"out explicit relations and Arg1 extension. combinations are also represented. We used this tool because the output of CRF++ is compatible to CoNLL 2000 chunking shared task, and we view our task as a discourse chunking task. On the other hand, linear-chain CRFs for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Also Sha and Pereira (2003) claim that, as a single model, CRFs outperform other models for shallow parsing. 6.1 Evaluation methodology We present our results using precision, recall and F1 measures. Following Johansson and Moschitti (2010), we use three scoring schemes: exact, intersection (or partial), and overlap scoring. In the exact scoring scheme, a span extracted by the system is counted as correct if its extent exactly coincides with one in the gold standard. However, we also use the two other scoring schemes since exact scoring may be uninformative in some situations where it is enough to have a rough approximation of the argument spans. In the overlap scheme, an expression is counted as correctly detected if it overlaps with a gold standard argument, i.e. if their intersection is nonempty. The intersection scheme assig"
I11-1120,D09-1036,0,0.133389,"a restricted class of connectives. The recent work by Prasad et al. (2010) is also limited, since their system only extracts the sentences containing the arguments. In our work, we assume that explicit discourse connectives are given beforehand, either taken directly from a gold standard or automatically identified. The second task based on PDTB was tackled among others by Pitler et al. (2008) and Pitler and Nenkova (2009). In addition to the work on finding explicit connectives and their arguments, there has been recent work on classification of implicit discourse relations, see for instance Lin et al. (2009). In a similar classification experiment, Pitler et al. (2009) investigated features ranging from low-level word pairs to high-level linguistic cues, and demonstrated that it is useful to model the sequence of discourse relations using a sequence labeler. Although they both outperformed their respective baselines, this task is very difficult and performances are still very low. 3 The Penn Discourse Treebank (PDTB) The Penn Discourse Treebank (Prasad et al., 2008) is a resource including one million words from the Wall Street Journal (Marcus et al., 1993), annotated with discourse relations. Ba"
I11-1120,J93-2004,0,0.045018,"icit discourse relations, see for instance Lin et al. (2009). In a similar classification experiment, Pitler et al. (2009) investigated features ranging from low-level word pairs to high-level linguistic cues, and demonstrated that it is useful to model the sequence of discourse relations using a sequence labeler. Although they both outperformed their respective baselines, this task is very difficult and performances are still very low. 3 The Penn Discourse Treebank (PDTB) The Penn Discourse Treebank (Prasad et al., 2008) is a resource including one million words from the Wall Street Journal (Marcus et al., 1993), annotated with discourse relations. Based on the observation that “no discourse connective has yet been identified in any language that has other than two arguments” (Webber et al. (2010), p. 15), connectives in the PTDB are treated as discourse predicates taking two text spans as arguments, i.e. parts of the text that describe events, propositions, facts, situations. Such two arguments in the PDTB are just called Arg1 and Arg2 and are chosen according to syntactic criteria: Arg2 is the argument syntactically bound to the connective, while Arg1 is the other one. This means that the numbering"
I11-1120,prasad-etal-2008-penn,0,0.873646,"extraction of discourse arguments for given ex1071 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1071–1079, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP plicit discourse connectives – has been attempted a number of times. Soon after the initial release of the PDTB, it was realized that sentence-internal arguments may be located and classified using techniques similar to semantic role detection and classification methods. Wellner and Pustejovsky (2007) were the first to carry out such an experiment on the PDTB, and Elwell and Baldridge (2008) later improved over their results. However, their task was limited to retrieving the argument heads. In contrast, we integrate discourse segmentation in the parsing pipeline because we believe that spans are necessary when using the discourse arguments as input to applications such as opinion mining, where attributions need to be explicitly marked. Besides, no gold data are available for head-based discourse parsing evaluation and they have to be automatically derived from parse trees with a further processing step. With our approach, instead, we can directly use PDTB argument spans both for"
I11-1120,prasad-etal-2010-exploiting,0,0.494984,"empts have already been made in the direction of automatic classification of connectives, while token-level argument segmentation has not been explored. Therefore in this paper we will focus on the segmentation and labeling of discourse arguments (Arg1 and Arg2) with full spans, as defined in the annotation protocol of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). We present a methodology that, given explicit discourse connectives, automatically extracts discourse arguments by identifying Arg1 and Arg2 including the corresponding text spans. We call this approach shallow following Prasad et al. (2010) as opposed to tree-like representations of discourse, as in Rhetorical Structure Theory (Mann and Thompson, 1988). Indeed, we provide a flat chunk classification of discourse relations, building a non-hierarchical representation of the relations in a text. The discourse parser is designed as a cascade of argument-specific CRFs trained on different sets of lexical, syntactic and semantic features. The evaluation is made in terms of exact and partial match of arguments. The partial match condition may be useful in the case of noisy input or for applications that do not require exact alignment."
I11-1120,N03-1028,0,0.452917,"Missing"
I11-1120,D09-1018,0,0.0106177,"e show that the best combination of features includes syntactic and semantic features. The comparative error analysis investigates the performance variability over connective types and argument positions. 1 Introduction Automatic discourse processing is considered one of the most challenging NLP tasks due to its dependency on lexical and syntactic features and on the inter-sentential relations. While automatic discourse processing of structured documents or free text is still in its infancy, a number of applications of this technology in practical NLP systems have been proposed. For instance, Somasundaran et al. (2009) describe the use of discourse structure for opinion analysis. Other applications include conversational analysis and dialog systems (Tonelli et al., 2010). In this work we divide the whole task of discourse parsing into two sub-tasks: connective classification and argument segmentation and classification. Several successful attempts have already been made in the direction of automatic classification of connectives, while token-level argument segmentation has not been explored. Therefore in this paper we will focus on the segmentation and labeling of discourse arguments (Arg1 and Arg2) with fu"
I11-1120,tonelli-etal-2010-annotation,1,0.786179,"ver connective types and argument positions. 1 Introduction Automatic discourse processing is considered one of the most challenging NLP tasks due to its dependency on lexical and syntactic features and on the inter-sentential relations. While automatic discourse processing of structured documents or free text is still in its infancy, a number of applications of this technology in practical NLP systems have been proposed. For instance, Somasundaran et al. (2009) describe the use of discourse structure for opinion analysis. Other applications include conversational analysis and dialog systems (Tonelli et al., 2010). In this work we divide the whole task of discourse parsing into two sub-tasks: connective classification and argument segmentation and classification. Several successful attempts have already been made in the direction of automatic classification of connectives, while token-level argument segmentation has not been explored. Therefore in this paper we will focus on the segmentation and labeling of discourse arguments (Arg1 and Arg2) with full spans, as defined in the annotation protocol of the Penn Discourse Treebank (PDTB) (Prasad et al., 2008). We present a methodology that, given explicit"
I11-1120,D07-1010,0,0.697978,". Finally, we draw some conclusions in Section 7. 2 Related Work The task that we address in this paper – automatic extraction of discourse arguments for given ex1071 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 1071–1079, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP plicit discourse connectives – has been attempted a number of times. Soon after the initial release of the PDTB, it was realized that sentence-internal arguments may be located and classified using techniques similar to semantic role detection and classification methods. Wellner and Pustejovsky (2007) were the first to carry out such an experiment on the PDTB, and Elwell and Baldridge (2008) later improved over their results. However, their task was limited to retrieving the argument heads. In contrast, we integrate discourse segmentation in the parsing pipeline because we believe that spans are necessary when using the discourse arguments as input to applications such as opinion mining, where attributions need to be explicitly marked. Besides, no gold data are available for head-based discourse parsing evaluation and they have to be automatically derived from parse trees with a further pr"
I11-1120,W03-3023,0,0.0709142,"ation. It includes for example the -ing and -ed suffixes in verb endings as well as the -s to form the plural of nouns. In our 2 We extracted this feature using the Chunklink.pl script made available by Sabine Buchholz at http://ilk.uvt. nl/team/sabine/chunklink/README.html 1074 example sentence, this feature would be for example s for “traders” and “heads”, etc. As for features (F7) and (F8), they rely on information about the main verb of the current sentence. More specifically, feature (F7) is the main verb token (i.e. shook in our example), extracted following the head-finding strategy by Yamada and Matsumoto (2003), while feature (F8) is a boolean feature that indicates for each token if it is the main verb in the sentence or not.3 The previous sentence feature “Prev” (F9) is a connective-surface feature and is used to capture if the following sentence begins with a connective. Our intuition is that it may be relevant to detect Arg1 boundaries in inter-sentential relations. The feature value for each candidate token of a sentence corresponds to the connective token that appears at the beginning of the following sentence, if any. Otherwise, it is equal to 0. We also add gold-standard Arg2 labels (F10) as"
I11-1120,P09-2004,0,0.636093,"rocessing step. With our approach, instead, we can directly use PDTB argument spans both for training and for testing. Dinesh et al. (2005) extracted complete arguments with boundaries, but only for a restricted class of connectives. The recent work by Prasad et al. (2010) is also limited, since their system only extracts the sentences containing the arguments. In our work, we assume that explicit discourse connectives are given beforehand, either taken directly from a gold standard or automatically identified. The second task based on PDTB was tackled among others by Pitler et al. (2008) and Pitler and Nenkova (2009). In addition to the work on finding explicit connectives and their arguments, there has been recent work on classification of implicit discourse relations, see for instance Lin et al. (2009). In a similar classification experiment, Pitler et al. (2009) investigated features ranging from low-level word pairs to high-level linguistic cues, and demonstrated that it is useful to model the sequence of discourse relations using a sequence labeler. Although they both outperformed their respective baselines, this task is very difficult and performances are still very low. 3 The Penn Discourse Treeban"
I11-1120,C08-2022,0,0.0290911,"se trees with a further processing step. With our approach, instead, we can directly use PDTB argument spans both for training and for testing. Dinesh et al. (2005) extracted complete arguments with boundaries, but only for a restricted class of connectives. The recent work by Prasad et al. (2010) is also limited, since their system only extracts the sentences containing the arguments. In our work, we assume that explicit discourse connectives are given beforehand, either taken directly from a gold standard or automatically identified. The second task based on PDTB was tackled among others by Pitler et al. (2008) and Pitler and Nenkova (2009). In addition to the work on finding explicit connectives and their arguments, there has been recent work on classification of implicit discourse relations, see for instance Lin et al. (2009). In a similar classification experiment, Pitler et al. (2009) investigated features ranging from low-level word pairs to high-level linguistic cues, and demonstrated that it is useful to model the sequence of discourse relations using a sequence labeler. Although they both outperformed their respective baselines, this task is very difficult and performances are still very low"
I11-1120,P09-1077,0,0.0761964,"d et al. (2010) is also limited, since their system only extracts the sentences containing the arguments. In our work, we assume that explicit discourse connectives are given beforehand, either taken directly from a gold standard or automatically identified. The second task based on PDTB was tackled among others by Pitler et al. (2008) and Pitler and Nenkova (2009). In addition to the work on finding explicit connectives and their arguments, there has been recent work on classification of implicit discourse relations, see for instance Lin et al. (2009). In a similar classification experiment, Pitler et al. (2009) investigated features ranging from low-level word pairs to high-level linguistic cues, and demonstrated that it is useful to model the sequence of discourse relations using a sequence labeler. Although they both outperformed their respective baselines, this task is very difficult and performances are still very low. 3 The Penn Discourse Treebank (PDTB) The Penn Discourse Treebank (Prasad et al., 2008) is a resource including one million words from the Wall Street Journal (Marcus et al., 1993), annotated with discourse relations. Based on the observation that “no discourse connective has yet b"
I17-3007,D14-1082,0,0.0105669,"tep was to design general-purpose simplification rules which will later be specialised for the domain under consideration (PA). This solution led to the development of MUSST, which includes SS modules for three languages. MUSST is based on the framework proposed by Siddharthan (2004) and is available as an open source Python implementation. Our rules split conjoint clauses, relative clauses and appositive phrases, and change sentences from passive into active voice. These are arguably the most widely applicable simplification operations across languages. We use the Stanford dependency parser (Chen and Manning, 2014) for the three languages, which enabled us to build a consistent multilingual tool. MUSST is evaluated using corpora extracted from the SIMPATICO use cases data. Such corpora (one for each language) were checked and – where applicable – syntactically simplified by experts in the area. Inspired by the work of Gasperin et al. (2009), we also developed a complexity checker module in order to select sentences that should be simplified. In addition, we implemented a confidence model in order to predict whether or not a simplification produced by MUSST is good enough to be shown to the end-user. Dev"
I17-3007,W11-2123,0,0.0097727,"a sentence is “good enough” for a user, we trained a confidence model to classify a simplification as acceptable or not. Using the 292 sentences simplified by the English system and evaluated in Section 3, we built a confidence model for this language. The 70 sentences classified as incorrect (Section 3) were used as negative examples, whilst the remaining sentences received the positive label. As features, we used the same basic counts as for the complexity checker (Section 4.1) along with language model (LM) probabilities and perplexity and grammar checking on the simplifications. KenLM11 (Heafield, 2011) was used to extract LM features. A Python grammar checker was used for evaluating grammaticality12 . The model was trained using the Random Forest implementation from scikit-learn with 10fold cross-validation and achieved 0.80 of accuracy (F1/Precision/recall = 0.60/0.69/0.53), outperforming the MC classifier (accuracy = 0.61). For Italian and Spanish, we also experimented with the datasets presented in Section 3, but the performance is worse because of the significantly smaller training sets. Nevertheless, both models outperform the majority class baseline in terms of accuracy. For Italian,"
I17-3007,L16-1491,1,0.821273,"ing the lexical and/or syntactic complexity of a text (Siddharthan, 2004). It is common to divide this task in two subtasks: lexical simplification (LS) and syntactic simplification (SS). Whilst LS deals with the identification and replacement of difficult words or phrases, SS focuses on making complex syntactic constructions simpler. It is known, for instance, that passive voice constructions are more complex than active voice, and that long sentences with multiple clauses are more difficult to be understood than short sentences with a single clause. Several tools have been developed for LS (Paetzold and Specia, 2016). However, we are not aware of freely available tools for SS. 1 https://www.simpatico-project.eu/ 25 The Companion Volume of the IJCNLP 2017 Proceedings: System Demonstrations, pages 25–28, c Taipei, Taiwan, November 27 – December 1, 2017. 2017 AFNLP in Tint4 (Palmero Aprosio and Moretti, 2016) (an adapted version of CoreNLP for Italian). Figure 1 shows the parser output for the sentence “These organisations have been checked by us and should provide you with a quality service.”, as an example. The sentence is first sent to the Analysis module that will search for discourse markers. In this ca"
J19-2002,W14-2907,0,0.0694473,"Missing"
J19-2002,J08-4004,0,0.142752,"Corpus Annotation The Histo Corpus was annotated following the guidelines described in Section 3 and using the Web-based CAT annotation tool (Bartalesi Lenzi, Moretti, and Sprugnoli 2012). This subsection contains description and results of the inter-annotator agreement performed to check the soundness of the guidelines and the feasibility of the proposed tasks. Then we give details on the annotated data with an analysis of the main differences between events annotated in the two genres forming the Histo Corpus. 4.2.1 Inter-Annotator Agreement. We measured the inter-annotator agreement (IAA) (Artstein and Poesio 2008) on a subset of the Histo Corpus, balanced between the two genres in terms of token number: one travel narrative and four news pieces about different topics (national and foreign policy, sport, scientific discoveries) were selected for a total of 1,200 tokens. Two annotators performed the work independently, using the guidelines reported in Section 3: One was one of the authors of the paper, and the other was not involved in the development of the guidelines. Both annotators have very good English proficiency and expertise in linguistic annotations. Results of the IAA are reported here with di"
J19-2002,bartalesi-lenzi-etal-2012-cat,1,0.879288,"Missing"
J19-2002,S15-2136,0,0.0301266,"tection and classification: both a conditional random fields classifier and a neural architecture are tested. Results are compared and discussed in Section 6. Finally, Section 7 provides a summary of the paper and discusses the lessons learned from this research work. 230 Sprugnoli and Tonelli Novel Event Detection and Classification for Historical Texts 2. Related Work Most existing works on event extraction have focused on contemporary texts in domains such as news, biomedicine, and medicine (Filatova and Hovy 2001; Katz and ¨ Arosio 2001; Schilder and Habel 2003; Bjorne and Salakoski 2011; Bethard et al. 2015). Research in the aforementioned domains has been fostered by the organization of many evaluation campaigns and shared tasks, which revised the notion of event to tailor it to the domain of interest. For example, news are the focus of the “Scenario Template” task within the Message Understanding Conference (MUC) (Sundheim 1991) and of the Automatic Content Extraction (ACE), TempEval and the Text Analysis Conference (TAC) Event Nugget series (Doddington et al. 2004; Verhagen et al. 2007, 2010; UzZaman et al. 2013; Mitamura, Liu, and Hovy 2016, 2017). The i2b2 challenge and ClinicalTempEval are"
J19-2002,W11-1828,0,0.0237604,"tomatic system for event detection and classification: both a conditional random fields classifier and a neural architecture are tested. Results are compared and discussed in Section 6. Finally, Section 7 provides a summary of the paper and discusses the lessons learned from this research work. 230 Sprugnoli and Tonelli Novel Event Detection and Classification for Historical Texts 2. Related Work Most existing works on event extraction have focused on contemporary texts in domains such as news, biomedicine, and medicine (Filatova and Hovy 2001; Katz and ¨ Arosio 2001; Schilder and Habel 2003; Bjorne and Salakoski 2011; Bethard et al. 2015). Research in the aforementioned domains has been fostered by the organization of many evaluation campaigns and shared tasks, which revised the notion of event to tailor it to the domain of interest. For example, news are the focus of the “Scenario Template” task within the Message Understanding Conference (MUC) (Sundheim 1991) and of the Automatic Content Extraction (ACE), TempEval and the Text Analysis Conference (TAC) Event Nugget series (Doddington et al. 2004; Verhagen et al. 2007, 2010; UzZaman et al. 2013; Mitamura, Liu, and Hovy 2016, 2017). The i2b2 challenge and"
J19-2002,Q17-1010,0,0.0073013,"oVe, with both 300 and 100 dimensions (GloVe300 - GloVe100)20 (Pennington, Socher, and Manning 2014), trained on a corpus of 6 billion tokens consisting of the 2014 English Wikipedia and Gigaword 5; • GoogleNews, with 300 dimensions and trained on a subset of the Google News corpus (about 100 billion words)21 (Mikolov et al. 2013); • Levy and Goldberg embeddings (Levy),22 with 300 dimensions and produced from the English Wikipedia on the basis of dependency-based contexts (Levy and Goldberg 2014); • fastText, with 300 dimensions and trained on the English Wikipedia using character n-grams 23 (Bojanowski et al. 2017). By taking into consideration the previously listed pre-trained embeddings, we cover different types of word representation: GloVe and GoogleNews are based on linear bag-of-words contexts, Levy and Komn on dependency parse-trees, and fastText on a bag of character n-grams. We also created additional historical word embeddings by processing a subset of the Corpus of Historical American English (COHA) (Davies 2012) with GloVe, fastText, and Levy and Goldberg’s code. The subset of COHA we have chosen contains 36,856 texts published between 1860 and 1939 for a total of more than 19 20 21 22 23 ht"
J19-2002,J96-2004,0,0.602166,"t or not of an event mention. For event classification, we calculated the Cohen’s kappa (Cohen 1960) on mentions detected by both annotators, so as to also measure the pairwise agreement taking into consideration agreement that would be obtained by chance: • EVENT MENTION DETECTION: – Dice Coefficient macro-average at tag level (perfect match): 0.85 – Dice Coefficient macro-average at token level: 0.87 • EVENT CLASSIFICATION: – Cohen’s kappa: 0.71 In linguistic annotation, a kappa score of 0.80 is considered the minimum threshold for data annotated with good reliability (Landis and Koch 1977; Carletta 1996). Our results on event mention detection are particularly good given the presence of multitoken and discontinuous mentions: The agreement on perfect match is only slightly lower than the one at token level (0.85 vs. 0.87), meaning that mentions can be detected in a consistent way. Disagreements were due to differences in the inclusion of prepositions in the event extent (“twister over”) and to the non-identification of copular constructions (“the average speed was 44 miles per hour”). Another problematic case is given by polysemous event nominals like “story” in the following sentence, which m"
J19-2002,W03-1022,0,0.111124,"physical existence and those having a social dimension: Due to this subtle difference an event of movement can belong to the TRAVEL AND TRAVELING, the SPACE, or the MOVEMENT category. In other words, discerning between physical and social dimensions is ambiguous. Therefore, starting from the original complex and extremely fine-grained classification, we worked to find an appropriate level of granularity by merging categories with a common conceptual core.6 This choice led us to create a unique class for events related to the 5 http://historicalthesaurus.arts.gla.ac.uk/. 6 WordNet supersenses (Ciaramita and Johnson 2003) have partial overlap with the HTOED and HISTO categories (see for example noun.possession/verb.possession in WordNet and the class POSSESSION in both HISTO and HTOED). However, there are several differences to highlight: First of all, supersenses are strongly based on PoS categories, since words that are semantically similar but grammatically different 236 Sprugnoli and Tonelli Novel Event Detection and Classification for Historical Texts concept of space (SPACE-MOVEMENT) and for those involving forces beyond scientific understanding or the laws of nature (RELIGION-SUPERNATURAL). In addition,"
J19-2002,J81-4005,0,0.718581,"Missing"
J19-2002,cybulska-vossen-2010-event,0,0.0172944,"nd of investigation, taking advantage of existing systems for event detection (Llorens, Saquete, and Navarro 2010; Atefeh and Khreich 2015; Reimers and Gurevych 2015; Derczynski et al. 2016) and of the results of past evaluation campaigns (Verhagen et al. 2007; UzZaman et al. 2013; Minard et al. 2015). This cross-fertilization has already been applied to the clinical, chemical, and biomedical domains also for tasks other than event processing (N´edellec et al. 2006; Teufel, Siddharthan, and Batchelor 2009; Sohn et al. 2010), but only limited efforts have been devoted to the historical domain (Cybulska and Vossen 2010; Fokkens et al. 2014). In this work, we present a novel contribution to analyze and automatically classify event mentions in historical documents. In particular, we develop annotation guidelines and an annotated corpus for the domain, and then we present some experiments for the automatic detection and classification of event mentions in historical texts. This study is built upon the findings in Sprugnoli and Tonelli (2017), where we performed an analysis of historians’ requirements regarding linguistic annotation of events in text. Based on the outcome of that previous survey, we present in"
J19-2002,W11-1506,0,0.0203324,"cessing. Studies in this field have focused more on the modeling of historical events through the development and use of ontologies (Raimond and Abdallah 2007; Shaw, Troncy, and Hardman 2009; Van Hage et al. 2011; The European Union 2012; Le Boeuf et al. 2017) without fully exploiting NLP methods. Past approaches to event extraction from historical texts have dealt only with verbal events or named events (Ide and Woolner 2007; Segers et al. 2011). Other efforts have been directed to the identification of specific types of events, such as conflict or communication events (Ide and Woolner 2004; Cybulska and Vossen 2011). Finally, more recent works have adopted crowdsourcing techniques or a 232 Sprugnoli and Tonelli Novel Event Detection and Classification for Historical Texts frame-based approach (De Boer et al. 2015; Fokkens et al. 2018). Additionally, data annotated with events in this domain and publicly released are scarce: Exceptions are the ModeS TimeBank (Guerrero Nieto, Saur`ı, and Bernab´e Poveda 2011) with Spanish texts from the eighteenth century and the De Gasperi corpus (Speranza and Sprugnoli 2018), a collection of documents written by the Italian statesman Alcide de Gasperi at the beginning of"
J19-2002,L16-1587,0,0.0560596,"ct their system of ideas about the past (Oakeshott 2015; Shaw 2010), a systematic and consistent analysis of events mentioned in historical texts would greatly contribute to a better understanding of large archives in this domain. Furthermore, the extensive work done in the Natural Language Processing (NLP) community to manually annotate and automatically detect event mentions could represent a valuable starting point for this kind of investigation, taking advantage of existing systems for event detection (Llorens, Saquete, and Navarro 2010; Atefeh and Khreich 2015; Reimers and Gurevych 2015; Derczynski et al. 2016) and of the results of past evaluation campaigns (Verhagen et al. 2007; UzZaman et al. 2013; Minard et al. 2015). This cross-fertilization has already been applied to the clinical, chemical, and biomedical domains also for tasks other than event processing (N´edellec et al. 2006; Teufel, Siddharthan, and Batchelor 2009; Sohn et al. 2010), but only limited efforts have been devoted to the historical domain (Cybulska and Vossen 2010; Fokkens et al. 2014). In this work, we present a novel contribution to analyze and automatically classify event mentions in historical documents. In particular, we"
J19-2002,doddington-etal-2004-automatic,0,0.123089,"Missing"
J19-2002,W01-1313,0,0.331729,"Missing"
J19-2002,fokkens-etal-2014-biographynet,0,0.0717197,"Missing"
J19-2002,P16-1141,0,0.0786927,"Missing"
J19-2002,ide-woolner-2004-exploiting,0,0.0961104,"mporal Information Processing. Studies in this field have focused more on the modeling of historical events through the development and use of ontologies (Raimond and Abdallah 2007; Shaw, Troncy, and Hardman 2009; Van Hage et al. 2011; The European Union 2012; Le Boeuf et al. 2017) without fully exploiting NLP methods. Past approaches to event extraction from historical texts have dealt only with verbal events or named events (Ide and Woolner 2007; Segers et al. 2011). Other efforts have been directed to the identification of specific types of events, such as conflict or communication events (Ide and Woolner 2004; Cybulska and Vossen 2011). Finally, more recent works have adopted crowdsourcing techniques or a 232 Sprugnoli and Tonelli Novel Event Detection and Classification for Historical Texts frame-based approach (De Boer et al. 2015; Fokkens et al. 2018). Additionally, data annotated with events in this domain and publicly released are scarce: Exceptions are the ModeS TimeBank (Guerrero Nieto, Saur`ı, and Bernab´e Poveda 2011) with Spanish texts from the eighteenth century and the De Gasperi corpus (Speranza and Sprugnoli 2018), a collection of documents written by the Italian statesman Alcide de"
J19-2002,S13-2004,0,0.0432385,"Missing"
J19-2002,W01-1315,0,0.0794938,"Missing"
J19-2002,W09-1401,0,0.0638534,"ple, news are the focus of the “Scenario Template” task within the Message Understanding Conference (MUC) (Sundheim 1991) and of the Automatic Content Extraction (ACE), TempEval and the Text Analysis Conference (TAC) Event Nugget series (Doddington et al. 2004; Verhagen et al. 2007, 2010; UzZaman et al. 2013; Mitamura, Liu, and Hovy 2016, 2017). The i2b2 challenge and ClinicalTempEval are instead about the processing of clinical documents (Sun, Rumshisky, and Uzuner 2013; Bethard et al. 2015, 2016, 2017), whereas the Bio-NLP campaign has tasks on event processing in the domain of biomedicine (Kim et al. 2009, 2011; N´edellec et al. 2013; Sun, Rumshisky, and Uzuner 2013; Kim et al. 2016). Within these initiatives, several corpora have been annotated following annotation guidelines subsuming different event definitions and, as a consequence, proposing different mark-up rules in terms of event taggability, linguistic realization, extent and classification.2 The most used scheme for the annotation of temporal information is TimeML (Pustejovsky et al. 2003), which is at the basis of the TempEval evaluation campaigns and was also consolidated as an ISO-standard (Iso, SemAf/Time Working Group 2008). For"
J19-2002,W11-1801,0,0.0974111,"Missing"
J19-2002,W16-3003,0,0.0146292,"rstanding Conference (MUC) (Sundheim 1991) and of the Automatic Content Extraction (ACE), TempEval and the Text Analysis Conference (TAC) Event Nugget series (Doddington et al. 2004; Verhagen et al. 2007, 2010; UzZaman et al. 2013; Mitamura, Liu, and Hovy 2016, 2017). The i2b2 challenge and ClinicalTempEval are instead about the processing of clinical documents (Sun, Rumshisky, and Uzuner 2013; Bethard et al. 2015, 2016, 2017), whereas the Bio-NLP campaign has tasks on event processing in the domain of biomedicine (Kim et al. 2009, 2011; N´edellec et al. 2013; Sun, Rumshisky, and Uzuner 2013; Kim et al. 2016). Within these initiatives, several corpora have been annotated following annotation guidelines subsuming different event definitions and, as a consequence, proposing different mark-up rules in terms of event taggability, linguistic realization, extent and classification.2 The most used scheme for the annotation of temporal information is TimeML (Pustejovsky et al. 2003), which is at the basis of the TempEval evaluation campaigns and was also consolidated as an ISO-standard (Iso, SemAf/Time Working Group 2008). For TimeML, events can be linguistically realized by many expressions corresponding"
J19-2002,N16-1175,0,0.0251802,"Missing"
J19-2002,N16-1030,0,0.0736949,"Missing"
J19-2002,S16-1201,0,0.0267853,"sed too, being ranked second in the identification of localization events of bacteria (Mehryary et al. 2016). In the clinical domain, the BluLab was the best performing system in the event expression task of Clinical TempEval 2015: It uses SVM algorithms and linguistic features obtaining an F-score of 0.87 on span identification and of 0.82 on class assignment (Velupillai et al. 2015). In 2016, the UTHealth system adopted SVM but added more features, embeddings, and information from domain-specific dictionaries, achieving an F-score of 0.93 on span identification and 0.88 on class assignment (Lee et al. 2016). In 2017, the focus of Clinical TempEval has shifted toward domain adaptation: Systems were trained on a clinical condition (colon cancer data) and tested on another clinical condition (brain cancer data) using an unsupervised approach and also a supervised one but with a limited quantity of training in-domain data. The best system, LIMSICOT, proposed a deep learning approach for event detection using long short-term memory networks (LSTMs) and a linear SVM for classification. The system obtained an F-score of around 0.70 in the unsupervised setting for both span identification and class assi"
J19-2002,P14-2050,0,0.0151131,"been released. Beyond Komninos and Manandhar embeddings (Komn),19 we tested other resources available online, namely: • GloVe, with both 300 and 100 dimensions (GloVe300 - GloVe100)20 (Pennington, Socher, and Manning 2014), trained on a corpus of 6 billion tokens consisting of the 2014 English Wikipedia and Gigaword 5; • GoogleNews, with 300 dimensions and trained on a subset of the Google News corpus (about 100 billion words)21 (Mikolov et al. 2013); • Levy and Goldberg embeddings (Levy),22 with 300 dimensions and produced from the English Wikipedia on the basis of dependency-based contexts (Levy and Goldberg 2014); • fastText, with 300 dimensions and trained on the English Wikipedia using character n-grams 23 (Bojanowski et al. 2017). By taking into consideration the previously listed pre-trained embeddings, we cover different types of word representation: GloVe and GoogleNews are based on linear bag-of-words contexts, Levy and Komn on dependency parse-trees, and fastText on a bag of character n-grams. We also created additional historical word embeddings by processing a subset of the Corpus of Historical American English (COHA) (Davies 2012) with GloVe, fastText, and Levy and Goldberg’s code. The subs"
J19-2002,W16-3004,0,0.0273417,"Missing"
J19-2002,S10-1063,0,0.0706848,"Missing"
J19-2002,P16-1101,0,0.0942859,"Missing"
J19-2002,P14-5010,0,0.00264272,"org/software/crfsuite/. 248 Sprugnoli and Tonelli Novel Event Detection and Classification for Historical Texts algorithm of CRFSuite (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) with L1 regularization. In addition, we put a threshold to ignore features whose frequency of occurrence in the training data is less than 2 and made CRFsuite generate both state and transition features. As for features, we chose a simple set of three beyond the token itself: (i) lemma, (ii) PoS, and (iii) text genre. The first two were extracted by processing the texts in the Histo Corpus with Stanford CoreNLP (Manning et al. 2014) and the third marks the opposition between news and travel narratives at document level. Although other, more semantically rich features could be used, we limit our feature set to a few basic ones, in line with the setting adopted with the BiLSTM approach (Section 5.3), which relies only on word embeddings. In the next subsection we present the results of several experiments carried out on the development set: In particular, we analyze the impact of the features and of the size of the context window on the performance of the classifiers. 5.2.1 Feature Selection. We adopted a backward selectio"
J19-2002,W16-3009,0,0.0148438,", taking advantage of morphosyntactic and semantic features (Llorens, Saquete, and Navarro 2010; Jung and Stent 2013; Kolomiyets and Moens 2013). Best results in event extraction are around 0.80 F1-score: results drop significantly in class assignment, with the best score below 0.72. Support Vector Machine (SVM) is the machine learning approach used by the best systems in the Bio-NLP 2016 biomedical event extraction task (Li, Rao, and Zhang 2016; Lever and Jones 2016). A deep learning approach has been proposed too, being ranked second in the identification of localization events of bacteria (Mehryary et al. 2016). In the clinical domain, the BluLab was the best performing system in the event expression task of Clinical TempEval 2015: It uses SVM algorithms and linguistic features obtaining an F-score of 0.87 on span identification and of 0.82 on class assignment (Velupillai et al. 2015). In 2016, the UTHealth system adopted SVM but added more features, embeddings, and information from domain-specific dictionaries, achieving an F-score of 0.93 on span identification and 0.88 on class assignment (Lee et al. 2016). In 2017, the focus of Clinical TempEval has shifted toward domain adaptation: Systems were"
J19-2002,W13-2001,0,0.0659088,"Missing"
J19-2002,D14-1162,0,0.0816963,"Missing"
J19-2002,J91-4003,0,0.70029,"audience that is smiling and applauding”; (3) a smiling and applauding audience • adjectives in predicative position; (4) the museum itself was damp • nouns that can realize eventualities in different ways: – deverbal nouns denoting an activity or an action; (5) the running of these ferries – nouns that have an eventive meaning in their lexical properties even if they do not derive from verbs; (6) delegates of Russia against the war – post-copular nouns; (7) it was a lie – nouns that normally denote objects but that are assigned an eventive reading either through the process of type-coercion (Pustejovsky 1991), or through the processes of logical metonymy and coercion induced by temporal prepositions. (8) I am finishing this letter rather hurriedly • pronouns related to previously mentioned events. Differently from the Rich ERE and Event Nugget annotation, we do not annotate implied events indicated by nouns like murderer and protestor so as to make a clear distinction between events and entities and avoid confusion. Indeed, the annotation of implied events is a case of annotators’ disagreement on event nugget tagging reported in Mitamura et al. (2015). The factuality status of events does not impa"
J19-2002,D17-1035,0,0.0271195,"Missing"
J19-2002,E95-1040,0,0.254521,"HES RELIGION-SUPERNATURAL* EDUCATION TOTAL* NEWS TRAVEL TOTAL 791 571 516 420 239 360 200 215 173 260 119 205 103 115 67 96 37 23 13 37 2 16 963 377 315 419 450 296 324 144 166 25 120 9 68 46 67 32 86 71 56 21 27 7 1,754 948 831 839 689 656 524 359 339 285 239 214 171 161 134 128 123 94 69 58 29 23 4,578 4,089 8,667 statistically significant difference (at p < 0.05 and calculated with the z test13 ) in their distribution. The high occurrence of events belonging to the SPACE-MOVEMENT class in both genres is due to the broad definition of the class that covers the three main concepts of motion (Sablayrolles 1995)—namely, locations, positions, and postures—and both factive and fictive motions. Examples of change of location (78), position (79), and posture 80 are given here. These are cases of factive motions, whereas Example (81) contains events of fictive motions, that is, “linguistic instances that depict motion with no physical occurrence” (Talmy 1996): (78) Marcel Renault arrived first (79) the pigs used to run about in the principal streets of Naples (80) a man lay in one of the entrances to the Union Station (81) a deep ravine surrounded by mountains 13 The z test is a parametric statistical tes"
J19-2002,E99-1023,0,0.446239,"Missing"
J19-2002,W16-1005,0,0.0191888,"E-MOVEMENT (44 occurrences) and EXISTENCE-CAUSATION (20 occurrences) with an agreement of 0.69 and 0.81, respectively. By comparing the IAA on the Histo Corpus with the agreement reported for other schemes dealing with event annotation, it is worth noticing higher results both for the extent and the class of event mentions in our corpus. In TimeBank 1.2, the agreement is 0.81 on partial match, 0.71 on perfect match, and 0.67 on class assignment.12 For the data used in the Event Nugget task in 2015, the agreement on event detection does not reach 0.80 and is below 0.70 on event classification (Song et al. 2016). 4.2.2 Annotated Data. Table 3 reports the number of annotated events in HC per class and text genre. News and travel narratives show, for almost all the event classes, a 12 Data reported in the TimeBank 1.2 documentation: http://www.timeml.org/timebank/documentation-1.2.html. 245 Computational Linguistics Volume 45, Number 2 Table 3 Annotated events per class and text genre together with the total amount of annotations. The asterisk indicates whether the class has a statistically significant difference in the distribution over the two genres. CLASS SPACE-MOVEMENT* COMMUNICATION* ACTION* MENT"
J19-2002,W15-0812,0,0.14352,"ates. Furthermore, we assume that events can be realized with different parts of speech and syntactic constructions. Because historical texts are a rather general category spanning diverse topics and genres, we put particular effort into developing a set of semantic classes that offer an exhaustive categorization of events, avoiding too much granularity for annotation purposes but also ensuring informativeness. This led to the definition of 22 semantic classes, thus overcoming the limited classifications proposed by other initiatives such as ACE (Linguistic Data Consortium 2005) and Rich ERE (Song et al. 2015).4 We summarize below the guidelines developed for the annotation, including information on event extent, linguistic realization, and types. The complete version of the guidelines is available at this link: https://github.com/dhfbk/Histo/blob/master/Guidelines.pdf. 3.1 Event Linguistic Realization In our annotation scheme for historical texts, the linguistic elements that may realize an event are the following: • verbs in both finite and non-finite form; (1) she expected to be attacked 3 Hereafter we will use the terms “event” and “eventuality” interchangeably. 4 For a detailed comparison of e"
J19-2002,M91-1001,0,0.45292,"for Historical Texts 2. Related Work Most existing works on event extraction have focused on contemporary texts in domains such as news, biomedicine, and medicine (Filatova and Hovy 2001; Katz and ¨ Arosio 2001; Schilder and Habel 2003; Bjorne and Salakoski 2011; Bethard et al. 2015). Research in the aforementioned domains has been fostered by the organization of many evaluation campaigns and shared tasks, which revised the notion of event to tailor it to the domain of interest. For example, news are the focus of the “Scenario Template” task within the Message Understanding Conference (MUC) (Sundheim 1991) and of the Automatic Content Extraction (ACE), TempEval and the Text Analysis Conference (TAC) Event Nugget series (Doddington et al. 2004; Verhagen et al. 2007, 2010; UzZaman et al. 2013; Mitamura, Liu, and Hovy 2016, 2017). The i2b2 challenge and ClinicalTempEval are instead about the processing of clinical documents (Sun, Rumshisky, and Uzuner 2013; Bethard et al. 2015, 2016, 2017), whereas the Bio-NLP campaign has tasks on event processing in the domain of biomedicine (Kim et al. 2009, 2011; N´edellec et al. 2013; Sun, Rumshisky, and Uzuner 2013; Kim et al. 2016). Within these initiatives"
J19-2002,D09-1155,0,0.0506555,"Missing"
J19-2002,S17-2098,0,0.0335675,"Missing"
J19-2002,S13-2001,0,0.234724,"t analysis of events mentioned in historical texts would greatly contribute to a better understanding of large archives in this domain. Furthermore, the extensive work done in the Natural Language Processing (NLP) community to manually annotate and automatically detect event mentions could represent a valuable starting point for this kind of investigation, taking advantage of existing systems for event detection (Llorens, Saquete, and Navarro 2010; Atefeh and Khreich 2015; Reimers and Gurevych 2015; Derczynski et al. 2016) and of the results of past evaluation campaigns (Verhagen et al. 2007; UzZaman et al. 2013; Minard et al. 2015). This cross-fertilization has already been applied to the clinical, chemical, and biomedical domains also for tasks other than event processing (N´edellec et al. 2006; Teufel, Siddharthan, and Batchelor 2009; Sohn et al. 2010), but only limited efforts have been devoted to the historical domain (Cybulska and Vossen 2010; Fokkens et al. 2014). In this work, we present a novel contribution to analyze and automatically classify event mentions in historical documents. In particular, we develop annotation guidelines and an annotated corpus for the domain, and then we present s"
J19-2002,S15-2137,0,0.0184737,"w 0.72. Support Vector Machine (SVM) is the machine learning approach used by the best systems in the Bio-NLP 2016 biomedical event extraction task (Li, Rao, and Zhang 2016; Lever and Jones 2016). A deep learning approach has been proposed too, being ranked second in the identification of localization events of bacteria (Mehryary et al. 2016). In the clinical domain, the BluLab was the best performing system in the event expression task of Clinical TempEval 2015: It uses SVM algorithms and linguistic features obtaining an F-score of 0.87 on span identification and of 0.82 on class assignment (Velupillai et al. 2015). In 2016, the UTHealth system adopted SVM but added more features, embeddings, and information from domain-specific dictionaries, achieving an F-score of 0.93 on span identification and 0.88 on class assignment (Lee et al. 2016). In 2017, the focus of Clinical TempEval has shifted toward domain adaptation: Systems were trained on a clinical condition (colon cancer data) and tested on another clinical condition (brain cancer data) using an unsupervised approach and also a supervised one but with a limited quantity of training in-domain data. The best system, LIMSICOT, proposed a deep learning"
J19-2002,S13-2010,0,0.0316823,"Missing"
J19-2002,S15-2132,0,\N,Missing
J19-2002,S16-1165,0,\N,Missing
J19-2002,W16-3005,0,\N,Missing
J19-2002,S17-2093,0,\N,Missing
J19-2002,W15-0809,0,\N,Missing
L16-1063,pianta-etal-2008-textpro,0,0.0371484,"Description In this Section we describe data pre-processing together with PIERINO structure and functionalities. Examples taken from the #buonascuola use case and screenshots are provided. 2.1. Data Selection and Preparation The platform takes in input a list of natural language comments/answers, grouped by question. In our use case, the analysis focused on a set of 10 questions, which were considered as highly relevant by MIUR collaborators. Overall, we processed more than 270,000 free text comments, containing almost 5,000,000 tokens. Each group of comments is first processed with TextPro2 (Pianta et al., 2008), an NLP suite performing several linguistic analyses, and with Mallet3 (McCallum, 2002), a system for the extraction of topic models. Then, we implemented a Web interface to easily navigate the data and query them. The analyses and the visualization strategies were discussed with MIUR and improved iteratively to meet their requirements, which were basically: • for each question, provide a concise representation of the main topics mentioned in the replies; • provide an interface which can be intuitively used without the need of technical support; • allow users to easily make queries to the dat"
L16-1063,rosell-velupillai-2008-revealing,0,0.0120733,"he literature reports the use of different techniques. For example, supervised approaches (Giorgetti et al., 2003) and active learning (Patil and Ravindran, 2015) are employed to perform survey coding, that is matching open-ended answers with a short description associated to a code, thus converting qualitative information (text) into a quantitative format (code). In addition, text clustering and topic modeling methods are applied to explore information contained in open answers, e.g. to summarize and classify them (Wang and Mulrow, 2014) and to identify the intentions of survey participants (Rosell and Velupillai, 2008). In this work, we present the first attempt to apply stateof-the-art Italian Natural Language Processing (NLP) techniques to the outcome of an online public consultation for policy making, with the goal to integrate citizens’ contributions in the Italian school reform. The work is a joint initiative among the Digital Humanites Group at Fondazione Bruno Kessler (FBK), the Vrije Universiteit Amsterdam (VUA) and the Italian Ministry of Education, Universities and Research (MIUR). The goal of the work is the automatic analysis of linguistic data contained in the answers given by the participants"
L16-1141,P98-1013,0,0.837324,"neously represent data from the various predicate models; and, (ii) the PreMOn Dataset, a collection of RDF datasets integrating various versions of the aforementioned predicate models and mapping resources. PreMOn is freely available and accessible online in different ways, including through a dedicated SPARQL endpoint. Keywords: Predicate Models; Predicate Model Mappings; Lemon; Semantic Web; Linguistic Linked Open Data. 1. Introduction Predicate models such as PropBank (herafter, PB) (Palmer et al., 2005), NomBank (NB) (Meyers et al., 2004), VerbNet (VN) (Schuler, 2005), and FrameNet (FN) (Baker et al., 1998) provide rich descriptions of predicate semantic classes — i.e., rolesets in NB and PB, verb classes in VN, and frames in FN (e.g., “Commerce Sell”) — and their semantic roles (e.g., “Seller” and “Buyer”), abstracting from a number of linguistic phenomena related to their realization in text. Thanks to the mappings of different predicate models, such as SemLink (Palmer, 2009) and the Predicate Matrix (Lacalle et al., 2014), and to their integration in Semantic Role Labeling (SRL) tools, they have become central to a number of tasks such as information extraction, question answering and natural"
L16-1141,N06-2015,0,0.0615047,"verb roleset exactly one local to semantic class verb verb (sub-)class zero or more global Types of semantic roles numbered, modifier Semantic class relations Semantic role relations Additional features – – mappings to PB, VN numbered, modifier, secondary agent – – mappings to VN thematic role (hierarchy) subclass – selectional restrictions, syntactic frames any (9 total) frame zero or more local to semantic class (core roles) core + other 3 types 2. Background (PB)3 PropBank PropBank by Palmer et al. (2005) is a predicate model for verbs, later extended to other parts-ofspeech in OntoNotes (Hovy et al., 2006). PB associates a lexical entry to one or more semantic classes called rolesets that are not shared in general with other entries. Semantic roles are defined locally to each semantic class and categorized as numbered arguments (e.g., Arg0 and Arg1, usually the proto-agent and proto-patient), modifiers (ArgM plus a function tag, such as LOC for location), and secondary agent (ArgA in OntoNotes). Annotated examples are provided for each semantic class. A summary of PB features, compared to other predicate models, is reported in Table 1. NomBank NomBank (NB)4 by Meyers et al. (2004) is a model fo"
L16-1141,lopez-de-lacalle-etal-2014-predicate,0,0.0726873,"a. 1. Introduction Predicate models such as PropBank (herafter, PB) (Palmer et al., 2005), NomBank (NB) (Meyers et al., 2004), VerbNet (VN) (Schuler, 2005), and FrameNet (FN) (Baker et al., 1998) provide rich descriptions of predicate semantic classes — i.e., rolesets in NB and PB, verb classes in VN, and frames in FN (e.g., “Commerce Sell”) — and their semantic roles (e.g., “Seller” and “Buyer”), abstracting from a number of linguistic phenomena related to their realization in text. Thanks to the mappings of different predicate models, such as SemLink (Palmer, 2009) and the Predicate Matrix (Lacalle et al., 2014), and to their integration in Semantic Role Labeling (SRL) tools, they have become central to a number of tasks such as information extraction, question answering and natural language generation. In particular, due to their laying at the syntactic-semantics interface, predicate models are increasingly used within the Semantic Web (SW) community, for knowledge extraction in tools such as NewsReader (Rospocher et al., 2016) and PIKES (Corcoglioniti et al., 2016), or as the starting point for deriving general-domain ontologies grounded in natural language, such as FrameBase (Rouces et al., 2015)"
L16-1141,W04-2705,0,0.216685,"odel by the W3C Ontology-Lexica Community Group, that enables to homogeneously represent data from the various predicate models; and, (ii) the PreMOn Dataset, a collection of RDF datasets integrating various versions of the aforementioned predicate models and mapping resources. PreMOn is freely available and accessible online in different ways, including through a dedicated SPARQL endpoint. Keywords: Predicate Models; Predicate Model Mappings; Lemon; Semantic Web; Linguistic Linked Open Data. 1. Introduction Predicate models such as PropBank (herafter, PB) (Palmer et al., 2005), NomBank (NB) (Meyers et al., 2004), VerbNet (VN) (Schuler, 2005), and FrameNet (FN) (Baker et al., 1998) provide rich descriptions of predicate semantic classes — i.e., rolesets in NB and PB, verb classes in VN, and frames in FN (e.g., “Commerce Sell”) — and their semantic roles (e.g., “Seller” and “Buyer”), abstracting from a number of linguistic phenomena related to their realization in text. Thanks to the mappings of different predicate models, such as SemLink (Palmer, 2009) and the Predicate Matrix (Lacalle et al., 2014), and to their integration in Semantic Role Labeling (SRL) tools, they have become central to a number o"
L16-1141,J05-1004,0,0.777142,"ntology, an extension of the lemon model by the W3C Ontology-Lexica Community Group, that enables to homogeneously represent data from the various predicate models; and, (ii) the PreMOn Dataset, a collection of RDF datasets integrating various versions of the aforementioned predicate models and mapping resources. PreMOn is freely available and accessible online in different ways, including through a dedicated SPARQL endpoint. Keywords: Predicate Models; Predicate Model Mappings; Lemon; Semantic Web; Linguistic Linked Open Data. 1. Introduction Predicate models such as PropBank (herafter, PB) (Palmer et al., 2005), NomBank (NB) (Meyers et al., 2004), VerbNet (VN) (Schuler, 2005), and FrameNet (FN) (Baker et al., 1998) provide rich descriptions of predicate semantic classes — i.e., rolesets in NB and PB, verb classes in VN, and frames in FN (e.g., “Commerce Sell”) — and their semantic roles (e.g., “Seller” and “Buyer”), abstracting from a number of linguistic phenomena related to their realization in text. Thanks to the mappings of different predicate models, such as SemLink (Palmer, 2009) and the Predicate Matrix (Lacalle et al., 2014), and to their integration in Semantic Role Labeling (SRL) tools, th"
L16-1141,C98-1013,0,\N,Missing
P13-2130,burchardt-etal-2006-salto,0,0.0381148,"Missing"
P13-2130,W09-3309,0,0.0244021,"Missing"
P13-2130,D08-1027,0,0.210916,"Missing"
P13-2130,fillmore-etal-2002-framenet,0,0.117716,"Missing"
P13-2130,W11-0404,0,0.471244,"Missing"
P13-2130,D11-1062,0,0.0445991,"Missing"
P13-2130,J05-1004,0,0.173783,"Missing"
R09-1079,burchardt-etal-2006-salsa,0,0.03224,"00 lexical units, with a rich repository of semantic roles (the frame elements) and almost 900 situation descriptions (the frames). FrameNet has proved to be useful in a number of NLP tasks, from textual entailment [3] to question answering [16], and the development of systems for frame recognition has become a topic of great interest for the NLP community, with a devoted task at the last SemEval workshop1. Given the success of the English FrameNet initiative, many researchers have focused on the development of FrameNet-like resources for other languages through manual annotation, for example [4] for German and [17] for Spanish. Manual annotation guarantees high accuracy but requires trained annotators and is expensive and time-consuming. For this reason, a second approach has been investigated, which is based on the automatic projection of frame information from 1 http://framenet.icsi.berkeley.edu/semeval/FSSE.html English texts into a new language using bilingual parallel corpora and possibly carrying out automatic annotation of frame information on the English side. If no parallel corpora are available, manually translating an annotated English corpus and automatically transferring"
R09-1079,P06-2057,0,0.0185922,"process. The most convenient alternative to manual annotation seems to be the import of English FrameNet annotation into another language exploiting a parallel corpus. [13] proposed a method to transfer frame annotation from English to German starting from parallel texts with the English side annotated with frame information. They proposed a model based on alignment at constituent level obtained through word overlap similarity. [14] tested a similar approach on a parallel English-French corpus, showing that the transfer framework can get promising results also if applied to Romance languages. [9] applied the transfer method to English-Swedish parallel texts with the English side being automatically annotated with a semantic role labeller trained on the English FrameNet database. As for Italian, a few projects are currently aimed at developing FrameNet for Italian and at exploring new approaches to speed up manual annotation or convey fully automatic annotation. [1] have proposed a methodology to automatically transfer frame information on an English-Italian parallel corpus based on a statistical machine translation step augmented with a rule-based post-processing. [5] have trained and"
R09-1079,2005.mtsummit-papers.11,0,0.044142,"hat we assume to be the target lexical unit lexunitit . If no alignment is available, the transfer fails, otherwise the English frame label is assigned to the Italian lexunitit . Then, for every English frame element f een , we take all syntactic dependents Dit of lexunitit and compute the number of aligned words between f een and dit ∈ Dit . We consider the Italian dependent with most aligned words Candbest as the best candidate for annotation projection. As an example, we report in Figure 1 the output of the first transfer algorithm applied to two parallel sentences from the Europarl corpus [10]. Dotted arrows connect aligned tokens. In this example, “demonstrated” is the target of the Reasoning frame, and two frame elements are present, namely content and arguer. Both frame elements point to the correct constituent nodes in Italian, that are the syntactic dependents of the target “dimostrato”. The content frame element is correctly transferred even if only one word (dialogue dialogo), which is not the semantic head of the constituent, has been aligned. This algorithm can cope with a different syntactic structure of the sentence in Italian, where the English secondary clause “that we"
R09-1079,P07-2045,0,0.014277,"Berkeley than for Europarl. This means that in MultiBerkeley parsing problems are the main source of error, whereas in the Europarl corpus also other factors have a significant impact on the al445 gorithm performance, for instance free translations. In general, we notice that the transfer approach performs better on a corpus like MultiBerkeley, where syntactic complexity is limited by the sentence length and the faithful translation of the parallel sentences enhances the performance of the aligner. 5.2 Evaluation 2 [1] presented a fully automatic transfer process based on alignment with Moses [11] at chunk level between English and Italian parallel sentences and a selection of the best candidate segment for semantic transfer according to some ranking and post-processing criteria. The algorithm was evaluated on the same subset of Europarl corpus that we used. However, they apply an evaluation framework that is different from that of [12] presented in the previous section. In fact, they consider each FE and target annotation as independent and include in the testset only those FEs having the same label both in the Italian and in the English gold standard. In order to compare this approac"
R09-1079,H05-1108,0,0.294536,"instances. In some cases, new frame definitions may be required for the new language. The first step towards the creation of a FrameNet database for a new language should be the annotation of frame information on a corpus of sentences in the new language. Since manual annotation is time-consuming and requires relevant financial efforts, several approaches have been proposed in the past to automatically carry out the annotation process. The most convenient alternative to manual annotation seems to be the import of English FrameNet annotation into another language exploiting a parallel corpus. [13] proposed a method to transfer frame annotation from English to German starting from parallel texts with the English side annotated with frame information. They proposed a model based on alignment at constituent level obtained through word overlap similarity. [14] tested a similar approach on a parallel English-French corpus, showing that the transfer framework can get promising results also if applied to Romance languages. [9] applied the transfer method to English-Swedish parallel texts with the English side being automatically annotated with a semantic role labeller trained on the English F"
R09-1079,C04-1156,1,0.816774,"en automatically annotated with part of speech and syntactic information and manually enriched with frame-semantic information as described in [13] in the context of transfer experiments between English and German. The same sentences were used also for the English-Italian transfer. The Italian sentences were parsed with Bikel’s phrase-based statistical parser trained for Italian [6], which obtained the best score in the EVALITA evaluation campaign for Italian NLP tools with 70.79 f-measure. Then the English-Italian corpus was aligned at word level with KNOWA (KNowledge-intensive Word Aligner) [15]. The coverage of the word alignment process reached 65.1 coverage on the whole corpus. The Italian side of the corpus was manually annotated with frame information in order to build a gold standard to assess transfer quality. The gold standard turns out to include instances of 158 frames, mainly connected to the communication and the political scenarios, with the great majority of lexical units being verbs. This means that the variability of frames was limited, with about 6 instances for every frame. Another characteristic of the Europarl corpus is the presence of extremely free translations."
R09-1079,D07-1002,0,0.0238174,"ears, the creation of annotated lexical resources has become crucial to the development of text processing systems, especially to train supervised learning systems and evaluate unsupervised or handcrafted systems. The FrameNet database [8] clearly exemplifies this trend. This resource contains more than 135,000 annotated sentences pointing to more than 10,000 lexical units, with a rich repository of semantic roles (the frame elements) and almost 900 situation descriptions (the frames). FrameNet has proved to be useful in a number of NLP tasks, from textual entailment [3] to question answering [16], and the development of systems for frame recognition has become a topic of great interest for the NLP community, with a devoted task at the last SemEval workshop1. Given the success of the English FrameNet initiative, many researchers have focused on the development of FrameNet-like resources for other languages through manual annotation, for example [4] for German and [17] for Spanish. Manual annotation guarantees high accuracy but requires trained annotators and is expensive and time-consuming. For this reason, a second approach has been investigated, which is based on the automatic projec"
R09-1079,tonelli-pianta-2008-frame,1,0.82004,"based only on word alignment or also, when available, on syntac442 tic structure information. [13] carried out experiments with both approaches and proved that exploiting constituent information yields substantial improvements over relying on word alignment alone. The methodology was then further optimized by [12] and applied to English-German and English-French corpora in order to transfer FE information via constituent alignment. We explored two variants of the constituentbased strategy applied to frame information transfer from English to Italian. The first variant, which was presented in [18], requires full parsing on both source and target corpus. Given an English constituent, annotated as FE, the algorithm extracts its head, aligns it with the corresponding Italian head, then looks for the maximal syntactic projection of the Italian semantic head, and transfers the English FE annotation to such constituent. In this approach, the correct alignment of the head is enough to carry out the FE transfer. However, this feature may also turn in a disadvantage, because if the semantic head is not aligned, there will be no transfer. We present here a second version of the transfer algorith"
R13-1039,C04-1200,0,0.0150887,"urse relations to achieve their results, conducting a survey on a small amount of data that showed that the contrast relation was the most frequent one. However, the survey presented in Hatzivassiloglou and McKeown (1997) on the WSJ corpus showed that contrast is actually the third most important relation in the corpus. Therefore the hypothesis made by Zirn et al. (2011) may be data specific. The framework of Heerschop et al. (2011) achieved even better results than Zirn et al. (2011). The system uses deep discourse structure as well as SentiWordNet and WordNet in order to disambiguate words. Kim and Hovy (2004) define opinion as a quadruple composed by topic, holder, claim and sentiment. The authors use a Named Entity tagger to identify the potential holder of the opinion. Later Stoyanov and Cardie (2008) argue that in fine grained subjectivity analysis, topic identification is very relevant, and treat the task from the perspective of topic coreference resolution. The authors use named entities beside other topic based features to represent the topical structure of text. Johansson and Moschitti (2013) developed a joint model-based sequence labeler for finegrained opinion expression using relational"
R13-1039,P06-2063,0,0.0223551,"authors try alternative machine learning approaches with combinations of supervised and unsupervised methods for the same task. However, they do not automatically identify discourse relations, but used task-specific manual annotations. 3 Data Resources In order to test our hypothesis we used 80 Wall Street Journal documents Marcus et al. (1993) that are part both of the Penn Discourse TreeBank (PDTB) and of the Multi-Perspective QuestionAnswer (MPQA) bank. Polanyi and Zaenen (2006) investigate the usage of contextual valence shifters and discourse connectives inside a text. In the approach of Kim and Hovy (2006) the system makes use of conjunctions like “and” to infer polarities and applies a specific rule to sentences including the word “but”: if no polarity can be identified for the clause containing “but”, the polarity of the previous phrase is negated. In a more recent system, 3.1 Penn Discourse TreeBank (PDTB) 2.0 The Penn Discourse Treebank (PDTB) is a resource containing one million words from the Wall Street Journal corpus Marcus et al. (1993) annotated with discourse relations. Connectives in the PTDB are treated as discourse predicates taking two text spans as arguments (Arg), i.e. parts of"
R13-1039,P10-2050,0,0.0145931,"ral building block is opinion expression. Opinion expressions belong to two categories: Direct subjective expressions (DSEs) are explicit mentions of opinion, whereas expressive subjective elements (ESEs) signal the attitude of the speaker by the choice of words, other than these there are Objective Speech Events (OSEs). Opinions have two features: polarity and intensity, and most expressions are also associated with a holder, also called source. In this work, we only consider polarities, not intensities or holders. Polarity can be POSITIVE, NEUTRAL, NEGATIVE, and BOTH; for compatibility with Choi and Cardie (2010), we mapped BOTH to NEUTRAL. 4 Experiments Our Approach The goal of our first experiment is to observe the effect of a limited number of gold label features from PDTB. Since no previous work documented the effect of PDTB senses on the task of opinion expression mining using MPQA, we use four PDTB surface senses (described in the Subsection 3.1) as one of the features in this experiment. We then run the second experiment in order to observe 2 304 (http://crfpp.sourceforge.net/) 5.3 Since the dataset is fairly small, we perform a 5-fold cross validation over the dataset to have a rough estimatio"
R13-1039,J93-2004,0,0.0456841,"PQA) scheme Wiebe et al. (2005). We perform two different experiments sets. We first exploit gold features based on shallow discourse structure1 to classify fine-grained opinion expressions. In a second experiment, we use some syntax based features, those are found useful on a shallow discourse structure classification task, along with the named entities. Both of the experiments are found to be useful at different levels of fine-grained opinion expression mining. We use conditional random fields for this entire shallow parsing task. A set of documents from the Wall Street Journal (WSJ) corpus Marcus et al. (1993) annotated both in the Penn Discourse Treebank Prasad et al. (2008) and MPQA corpus is used. We also take advantage of the availability of several robust natural language processing tools pretrained on WSJ data. Opinion analysis deals with public opinions and trends, but subjective language is highly ambiguous. In this paper, we follow a simple data-driven technique to learn fine-grained opinions. We select an intersection set of Wall Street Journal documents that is included both in the Penn Discourse Tree Bank (PDTB) and in the Multi-Perspective Question Answering (MPQA) corpus. This is done"
R13-1039,H05-1045,0,0.0476439,"007), among others. The first approach focuses on conjoined adjectives (i.e. the adjectives which are joined with discourse connectives) within the WSJ corpus. While the second one operates at the sentence level, the third one extracts 1 By shallow discourse structure we mean the explicit discourse connective sense and its two argument spans Ghosh (2012). 302 Proceedings of Recent Advances in Natural Language Processing, pages 302–310, Hissar, Bulgaria, 7-13 September 2013. opinion phrases at the subsentence level for product features. Rich sets of linguistic features are used in the works of Choi et al. (2005), Wilson et al. (2005a), Breck et al. (2007). The first use conditional random models with information extraction patterns; the second is more focused on the classification of opinion phrases using contextual polarity; the third approach improved the performance of Wilson et al. (2005a), using conditional random fields and external knowledge sources. Zirn et al. (2011) incorporated this information using discourse relations. Zirn et al. (2011) studied a fully automatic framework for fine-grained sentiment analysis at sub-sentence level, combining multiple sentiment lexicons and neighbourhood a"
R13-1039,P05-1045,0,0.00972738,"a and the part-of-speech (PoS) tag of the token. The fourth one is the polarity value of the current token taken from a standard subjectivity lexicon maintained by Wilson et al. (2005b). The selection of baseline features is motivated by the work of Breck et al. (2007). The features are listed in the Table 1. Features used to prepare the baseline. BF1. Token (T) BF2. Lemma (L) BF3. PoS tag BF4. Polarity Values (POLV) 3 (http://ilk.uvt.nl/team/sabine/ homepage/software.html) Table 1: Baseline Feature sets opinion expression labeling. 305 Matsumoto4 . We used the Stanford Named Entity tagger by Finkel et al. (2005) to tag the named entities. This tagger is a three-class (viz. PERSON, ORGANISATION, LOCATION) tagger for English. The pre-trained models are trained both on CoNLL 2003 and MUC data, for the intersection of those class sets. NEs are included as a feature following the previous work by Stoyanov and Cardie (2008), where the authors show that information from NEs contribute to the entity relation structure in a discourse. Partial Metric Metrics P J&M 0.547 Baseline 0.628 Discourse based 0.596 NE&Syntax based 0.658 F1 0.497 0.313 0.209 0.339 Table 5: Results for identifying Polarity expressions wi"
R13-1039,I11-1120,1,0.941385,"the top-level classes are the most generic ones and include TEMPORAL, CONTINGENCY, COMPARISON and EXPANSION labels. We used these four surface senses only in our task. We define our discourse structure as shallow since it includes only the discourse connective senses and its two argument spans, excluding other types of hierarchical annotation. 3.2 the effect of named entities with the mentioned feature bundle. This set of features encoding some syntactic-level information may improve the overall classification performance like the same features facilitated a shallow discourse parsing task by Ghosh et al. (2011); in addition to the feature bundle, the named entities might reflect some information about distribution of discourse entities. 5 We perform our experiments at two different stages: (1) we first draw a baseline using basic features from the previous work and a standard sentiment lexicon by Wilson et al. (2005b), then (2) we run further experiments to improve the baseline with additional features. Our goal is to investigate possible improvements using discourse features or some other features that may encode discourse information via shallow parsing. The experiments are entirely run using cond"
R13-1039,prasad-etal-2008-penn,0,0.0136057,"ts sets. We first exploit gold features based on shallow discourse structure1 to classify fine-grained opinion expressions. In a second experiment, we use some syntax based features, those are found useful on a shallow discourse structure classification task, along with the named entities. Both of the experiments are found to be useful at different levels of fine-grained opinion expression mining. We use conditional random fields for this entire shallow parsing task. A set of documents from the Wall Street Journal (WSJ) corpus Marcus et al. (1993) annotated both in the Penn Discourse Treebank Prasad et al. (2008) and MPQA corpus is used. We also take advantage of the availability of several robust natural language processing tools pretrained on WSJ data. Opinion analysis deals with public opinions and trends, but subjective language is highly ambiguous. In this paper, we follow a simple data-driven technique to learn fine-grained opinions. We select an intersection set of Wall Street Journal documents that is included both in the Penn Discourse Tree Bank (PDTB) and in the Multi-Perspective Question Answering (MPQA) corpus. This is done in order to explore the usefulness of discourselevel structure to"
R13-1039,E99-1023,0,0.0602402,"tool because the output of CRF++ is compatible with CoNLL 2000 chunking shared task, and we view our task as an opinion expression chunking task. On the other hand, linearchain CRFs for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Also Sha and Pereira (2003) claim that, as a single model, CRFs outperform other models for shallow parsing. We use conditional random fields to classify subjective (any of direct or expressive) and objective expressions. We encode the opinion expression spans by means of the IOB2 scheme Sang et al. (1999). In order to represent MPQA opinion expressions with IOB2 tags, we remove the expressions where the expression spans are overlapping expressions (i.e. an opinion expression span can be overlapped by another opinion expression span), though overlapping expressions are rare in MPQA [ Johansson and Moschitti (2013)]. Multi-Perspective Question Answering (MPQA) We use the version 2.0 of the MPQA corpus, whose central building block is opinion expression. Opinion expressions belong to two categories: Direct subjective expressions (DSEs) are explicit mentions of opinion, whereas expressive subjecti"
R13-1039,P97-1023,0,0.716719,"ogs, online forums, Facebook, Twitter and other social media channels has given an opportunity of unprecedented reach to publicly sharing thoughts on events, products and services. However, there are some open issues related to this research area, commonly known as Opinion Mining, which can be summarized as follows: (1) Opinions are potentially ambiguous, and (2) Contextual interpretation of polarity is hard to achieve. Subsidiary important problem is the non-availability of large corpora with good annotation quality. Related Work Fine-grained sentiment analysis methods have been developed by Hatzivassiloglou and McKeown (1997), Hu and Liu (2004) and Popescu and Etzioni (2007), among others. The first approach focuses on conjoined adjectives (i.e. the adjectives which are joined with discourse connectives) within the WSJ corpus. While the second one operates at the sentence level, the third one extracts 1 By shallow discourse structure we mean the explicit discourse connective sense and its two argument spans Ghosh (2012). 302 Proceedings of Recent Advances in Natural Language Processing, pages 302–310, Hissar, Bulgaria, 7-13 September 2013. opinion phrases at the subsentence level for product features. Rich sets of"
R13-1039,N03-1028,0,0.100759,"CRF++ tool 2 for sequence labeling classification by Lafferty et al. (2001), with second-order Markov dependency between tags. Beside the individual specification of a feature in the feature description template, the features in various combinations are also represented. We used this tool because the output of CRF++ is compatible with CoNLL 2000 chunking shared task, and we view our task as an opinion expression chunking task. On the other hand, linearchain CRFs for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Also Sha and Pereira (2003) claim that, as a single model, CRFs outperform other models for shallow parsing. We use conditional random fields to classify subjective (any of direct or expressive) and objective expressions. We encode the opinion expression spans by means of the IOB2 scheme Sang et al. (1999). In order to represent MPQA opinion expressions with IOB2 tags, we remove the expressions where the expression spans are overlapping expressions (i.e. an opinion expression span can be overlapped by another opinion expression span), though overlapping expressions are rare in MPQA [ Johansson and Moschitti (2013)]. Mul"
R13-1039,P09-1026,0,0.0159257,"ald (2011) combine fully and partially supervised structured conditional models for a joint classification of the polarity of whole reviews and review sentences. The impact of discourse relations for sentiment analysis is investigated in Asher et al. (2009). The authors conduct a manual study in which they represent opinions in text as shallow semantic feature structures. These are combined with overall opinion using hand-written rules based on manually annotated discourse relations. An interdependent classification scenario to determine polarity as well as discourse relations is presented in Somasundaran and Wiebe (2009). In their approach, text is modeled as opinion graphs including discourse information. In Somasundaran and Wiebe (2009) the authors try alternative machine learning approaches with combinations of supervised and unsupervised methods for the same task. However, they do not automatically identify discourse relations, but used task-specific manual annotations. 3 Data Resources In order to test our hypothesis we used 80 Wall Street Journal documents Marcus et al. (1993) that are part both of the Penn Discourse TreeBank (PDTB) and of the Multi-Perspective QuestionAnswer (MPQA) bank. Polanyi and Za"
R13-1039,C08-1103,0,0.0871378,"vassiloglou and McKeown (1997) on the WSJ corpus showed that contrast is actually the third most important relation in the corpus. Therefore the hypothesis made by Zirn et al. (2011) may be data specific. The framework of Heerschop et al. (2011) achieved even better results than Zirn et al. (2011). The system uses deep discourse structure as well as SentiWordNet and WordNet in order to disambiguate words. Kim and Hovy (2004) define opinion as a quadruple composed by topic, holder, claim and sentiment. The authors use a Named Entity tagger to identify the potential holder of the opinion. Later Stoyanov and Cardie (2008) argue that in fine grained subjectivity analysis, topic identification is very relevant, and treat the task from the perspective of topic coreference resolution. The authors use named entities beside other topic based features to represent the topical structure of text. Johansson and Moschitti (2013) developed a joint model-based sequence labeler for finegrained opinion expression using relational features except discourse-level features, beside a set of classifier to determine opinion holder and also a multi-class classifier that assigns polarity to a given opinion expression. These classifi"
R13-1039,J13-3002,1,0.897822,". The system uses deep discourse structure as well as SentiWordNet and WordNet in order to disambiguate words. Kim and Hovy (2004) define opinion as a quadruple composed by topic, holder, claim and sentiment. The authors use a Named Entity tagger to identify the potential holder of the opinion. Later Stoyanov and Cardie (2008) argue that in fine grained subjectivity analysis, topic identification is very relevant, and treat the task from the perspective of topic coreference resolution. The authors use named entities beside other topic based features to represent the topical structure of text. Johansson and Moschitti (2013) developed a joint model-based sequence labeler for finegrained opinion expression using relational features except discourse-level features, beside a set of classifier to determine opinion holder and also a multi-class classifier that assigns polarity to a given opinion expression. These classifiers were further used to generate the hypothesis sets for a re-ranking system that further improved the performance of the classification. T¨ackstr¨om and McDonald (2011) combine fully and partially supervised structured conditional models for a joint classification of the polarity of whole reviews an"
R13-1039,P11-2100,0,0.0255746,"Missing"
R13-1039,H05-1044,0,0.193979,"The first approach focuses on conjoined adjectives (i.e. the adjectives which are joined with discourse connectives) within the WSJ corpus. While the second one operates at the sentence level, the third one extracts 1 By shallow discourse structure we mean the explicit discourse connective sense and its two argument spans Ghosh (2012). 302 Proceedings of Recent Advances in Natural Language Processing, pages 302–310, Hissar, Bulgaria, 7-13 September 2013. opinion phrases at the subsentence level for product features. Rich sets of linguistic features are used in the works of Choi et al. (2005), Wilson et al. (2005a), Breck et al. (2007). The first use conditional random models with information extraction patterns; the second is more focused on the classification of opinion phrases using contextual polarity; the third approach improved the performance of Wilson et al. (2005a), using conditional random fields and external knowledge sources. Zirn et al. (2011) incorporated this information using discourse relations. Zirn et al. (2011) studied a fully automatic framework for fine-grained sentiment analysis at sub-sentence level, combining multiple sentiment lexicons and neighbourhood as well as discourse r"
R13-1039,I11-1038,0,0.0139238,"gs of Recent Advances in Natural Language Processing, pages 302–310, Hissar, Bulgaria, 7-13 September 2013. opinion phrases at the subsentence level for product features. Rich sets of linguistic features are used in the works of Choi et al. (2005), Wilson et al. (2005a), Breck et al. (2007). The first use conditional random models with information extraction patterns; the second is more focused on the classification of opinion phrases using contextual polarity; the third approach improved the performance of Wilson et al. (2005a), using conditional random fields and external knowledge sources. Zirn et al. (2011) incorporated this information using discourse relations. Zirn et al. (2011) studied a fully automatic framework for fine-grained sentiment analysis at sub-sentence level, combining multiple sentiment lexicons and neighbourhood as well as discourse relations. They used Markov logic to integrate polarity scores from different sentiment lexicons with information about relations between neighbouring segments, and evaluate the approach on product reviews. The authors used only contrast and no contrast discourse relations to achieve their results, conducting a survey on a small amount of data that"
R13-1039,H05-2017,0,\N,Missing
R13-1039,H05-1043,0,\N,Missing
S07-1040,W97-0109,0,\N,Missing
S07-1040,S07-1005,0,\N,Missing
S07-1040,J01-3001,0,\N,Missing
S07-1040,P97-1009,0,\N,Missing
S07-1040,W06-2106,0,\N,Missing
S07-1040,W93-0102,0,\N,Missing
S10-1036,pianta-etal-2008-textpro,1,\N,Missing
S10-1065,P98-1013,0,0.298433,"we will discuss how we mapped the VENSES analysis to the representation of frame information in order to identify null instantiations in the text. 1 Introduction The SemEval-2010 task for linking events and their participants in discourse (Ruppenhofer et al., 2009) introduced a new issue w.r.t. the SemEval-2007 task “Frame Semantic Structure Extraction” (Baker et al., 2007), in that it focused on linking local semantic argument structures across sentence boundaries. Specifically, the task included first the identification of frames and frame elements in a text following the FrameNet paradigm (Baker et al., 1998), then the identification of locally uninstantiated roles (NIs). If these roles are indefinite (INI), they have to be marked as such and no antecedent has to be found. On the contrary, if they are definite (DNI), their coreferents have to be found in the wider discourse context. The challenge comprised two tasks, namely the full task (semantic role recognition and labelling + NI linking) and the NIs only task, i.e. the identification of null instantiations and their referents given a test set with gold standard local semantic argument structure. We took part to the NIs only task by modifying t"
S10-1065,W06-2302,1,0.839869,"at this stage of computation may not be effective. 296 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 296–299, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics 2.2 The anaphora resolution module The AHDS structure is passed to and used by a full-fledged module for pronominal and anaphora resolution, which is in turn split into two submodules. The resolution procedure takes care only of third person pronouns of all kinds – reciprocals, reflexives, possessive and personal. Its mechanisms are quite complex, as described in (Delmonte et al., 2006). The first submodule basically treats all pronouns at sentence level – that is, taking into account their position – and if they are left free, they receive the annotation “external”. If they are bound, they are associated to an antecedent’s index; else they might also be interpreted as expletives, i.e. they receive a label that prevents the following submodule to consider them for further computation. The second submodule receives as input the external pronouns, and tries to find an antecedent in the previous stretch of text or discourse. To do that, the systems computes a topic hierarchy th"
S10-1065,J86-3001,0,0.291777,"Missing"
S10-1065,W09-2417,0,0.263895,"azione Bruno Kessler Trento, Italy. satonelli@fbk.eu Abstract The system to spot INIs, DNIs and their antecedents is an adaptation of VENSES, a system for semantic evaluation that has been used for RTE challenges in the last 6 years. In the following we will briefly describe the system and then the additions we made to cope with the new task. In particular, we will discuss how we mapped the VENSES analysis to the representation of frame information in order to identify null instantiations in the text. 1 Introduction The SemEval-2010 task for linking events and their participants in discourse (Ruppenhofer et al., 2009) introduced a new issue w.r.t. the SemEval-2007 task “Frame Semantic Structure Extraction” (Baker et al., 2007), in that it focused on linking local semantic argument structures across sentence boundaries. Specifically, the task included first the identification of frames and frame elements in a text following the FrameNet paradigm (Baker et al., 1998), then the identification of locally uninstantiated roles (NIs). If these roles are indefinite (INI), they have to be marked as such and no antecedent has to be found. On the contrary, if they are definite (DNI), their coreferents have to be foun"
S10-1065,S10-1008,0,\N,Missing
S10-1065,C98-1013,0,\N,Missing
S13-2077,esuli-sebastiani-2006-sentiwordnet,0,0.112993,"dalities are possible: (1) Constrained (using the provided training data only; other resources, such as lexica, are allowed; however, it is not allowed to use additional tweets/SMS messages or additional sentences with sentiment annotations); and (2) Unconstrained (using additional data for training, e.g., additional tweets/SMS messages or additional sentences annotated for sentiment). We participated in the Constrained modality. We adopted a supervised machine learning (ML) approach based on various contextual and semantic features. In particular, we exploited resources such as SentiWordNet (Esuli and Sebastiani, 2006), LIWC (Pennebaker and Francis, 2001), and the lexicons described in Mohammad et al. (2009). Critical features include: whether the message contains intensifiers, adjectives, interjections, presence of positive or negative emoticons, possible message polarity based on SentiWordNet scores (Esuli and Sebastiani, 2006; Gatti and Guerini, 2012), scores based on LIWC categories (Pennebaker and Francis, 2001), negated words, etc. 2 System Description Our supervised ML-based approach relies on Support Vector Machines (SVMs). The SVM implementation used in the system is LIBSVM (Chang 466 Second Joint"
S13-2077,C12-2036,1,0.507685,"itional sentences annotated for sentiment). We participated in the Constrained modality. We adopted a supervised machine learning (ML) approach based on various contextual and semantic features. In particular, we exploited resources such as SentiWordNet (Esuli and Sebastiani, 2006), LIWC (Pennebaker and Francis, 2001), and the lexicons described in Mohammad et al. (2009). Critical features include: whether the message contains intensifiers, adjectives, interjections, presence of positive or negative emoticons, possible message polarity based on SentiWordNet scores (Esuli and Sebastiani, 2006; Gatti and Guerini, 2012), scores based on LIWC categories (Pennebaker and Francis, 2001), negated words, etc. 2 System Description Our supervised ML-based approach relies on Support Vector Machines (SVMs). The SVM implementation used in the system is LIBSVM (Chang 466 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 466–470, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics and Lin, 2001) for training SVM models and testing. Moreover, in the preprocessing phase we used TweetNL"
S13-2077,D09-1063,0,0.0989609,"such as lexica, are allowed; however, it is not allowed to use additional tweets/SMS messages or additional sentences with sentiment annotations); and (2) Unconstrained (using additional data for training, e.g., additional tweets/SMS messages or additional sentences annotated for sentiment). We participated in the Constrained modality. We adopted a supervised machine learning (ML) approach based on various contextual and semantic features. In particular, we exploited resources such as SentiWordNet (Esuli and Sebastiani, 2006), LIWC (Pennebaker and Francis, 2001), and the lexicons described in Mohammad et al. (2009). Critical features include: whether the message contains intensifiers, adjectives, interjections, presence of positive or negative emoticons, possible message polarity based on SentiWordNet scores (Esuli and Sebastiani, 2006; Gatti and Guerini, 2012), scores based on LIWC categories (Pennebaker and Francis, 2001), negated words, etc. 2 System Description Our supervised ML-based approach relies on Support Vector Machines (SVMs). The SVM implementation used in the system is LIBSVM (Chang 466 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International W"
S13-2077,N13-1039,0,0.071406,"Missing"
S13-2077,S13-2052,0,0.0804079,"Missing"
tonelli-etal-2008-enriching,J03-4003,0,\N,Missing
tonelli-etal-2010-annotation,pareti-prodanof-2010-annotating,0,\N,Missing
tonelli-etal-2010-annotation,J93-2004,0,\N,Missing
tonelli-etal-2010-annotation,mladova-etal-2008-sentence,0,\N,Missing
tonelli-etal-2010-annotation,W05-0312,0,\N,Missing
tonelli-etal-2010-annotation,W09-3029,1,\N,Missing
tonelli-etal-2010-annotation,W09-0505,1,\N,Missing
tonelli-etal-2010-annotation,prasad-etal-2008-penn,1,\N,Missing
tonelli-etal-2010-annotation,I08-7009,0,\N,Missing
tonelli-etal-2010-venpro,W09-0714,0,\N,Missing
tonelli-etal-2010-venpro,pianta-etal-2008-textpro,1,\N,Missing
tonelli-pianta-2008-frame,burchardt-etal-2006-salto,0,\N,Missing
tonelli-pianta-2008-frame,C04-1156,1,\N,Missing
tonelli-pianta-2008-frame,P98-1013,0,\N,Missing
tonelli-pianta-2008-frame,C98-1013,0,\N,Missing
tonelli-pianta-2008-frame,P06-2057,0,\N,Missing
tonelli-pianta-2008-frame,2005.mtsummit-papers.11,0,\N,Missing
W06-2302,W04-2005,1,0.82968,"Missing"
W06-2302,C90-2047,0,0.060047,"Missing"
W06-2302,C96-1021,0,0.112441,"Missing"
W06-2302,qiu-etal-2004-public,0,0.0349918,"Missing"
W06-2302,P98-2143,0,0.0565723,"Missing"
W06-2302,J98-2001,0,0.0795361,"Missing"
W06-2302,poesio-kabadjov-2004-general,0,0.0286773,"Missing"
W06-2302,J86-3001,0,\N,Missing
W06-2302,C98-2138,0,\N,Missing
W06-2302,1991.iwpt-1.8,0,\N,Missing
W07-1408,C04-1180,0,\N,Missing
W07-1408,W04-0913,1,\N,Missing
W07-1408,J86-3001,0,\N,Missing
W07-1408,W06-2302,1,\N,Missing
W09-0505,P98-1013,0,0.607831,"ion levels of words, turns1 , attribute-value pairs, dialog acts, predicate argument structures. The annotation at word level is made with part-of-speech and morphosyntactic information following the recommendations of EAGLES corpora annotation (Leech and Wilson, 2006). The attribute-value annotation uses a predefined domain ontology to specify concepts and their relations. Dialog acts are used to annotate intention in an utterance and can be useful to find relations between different utterances as the next section will show. For predicate structure annotation, we followed the FrameNet model (Baker et al., 1998) (see Section 2.2). 2.1 2. Conventional/Discourse management acts, which maintain dialog cohesion and delimit specific phases, such as opening, continuation, closing, and apologizing; 3. Feedback/Grounding acts,used to elicit and provide feedback in order to establish or restore a common ground in the conversation. Our taxonomy, following the same three-fold partition, is summarized in Table 1. Table 1: Dialog act taxonomy Core dialog acts Speaker wants information from addressee Action-request Speaker wants addressee to perform an action Yes-answer Affirmative answer No-answer Negative answer"
W09-0505,W04-3218,1,0.880684,"lli, Alessandro Moschitti, Giuseppe Riccardi∗ University of Trento 38050 Povo - Trento, Italy {dinarelli,silviaq,moschitti,riccardi}@disi.unitn.it, satonelli@fbk.eu Abstract tities) within one or more frames (frame-slot semantics) that is defined by the application. While this model is simple and clearly insufficient to cope with interpretation and reasoning, it has supported the first generation of spoken dialog systems. Such dialog systems are thus limited by the ability to parse semantic features such as predicates and to perform logical computation in the context of a specific dialog act (Bechet et al., 2004). This limitation is reflected in the type of human-machine interactions which are mostly directed at querying the user for specific slots (e.g. “What is the departure city?”) or implementing simple dialog acts (e.g. confirmation). We believe that an important step in overcoming such limitation relies on the study of models of human-human dialogs at different levels of representation: lexical, syntactic, semantic and discourse. In this paper, we present our results in addressing the above issues in the context of the LUNA research project for next-generation spoken dialog interfaces (De Mori e"
W09-0505,burchardt-etal-2006-salto,0,0.0299062,"Missing"
W09-0505,W03-2117,0,0.0692443,"Missing"
W09-0505,W08-0109,1,\N,Missing
W09-0505,C98-1013,0,\N,Missing
W09-1127,P98-1013,0,0.215609,"o NLP systems has received 219 In this paper, we present an approach using Support Vector Machines (SVM) to map FrameNet lexical units to WordNet synsets. The proposed approach addresses some of the limitations of previous works on the same task (see for example De Cao et al. (2008) and Johansson and Nugues (2007)). Most notably, as we do not train the SVM on a perProceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 219–227, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics 2 FrameNet and WordNet The FrameNet database (Baker et al. (1998), Fillmore et al. (2003)) is an English lexical resource based on the description of some prototypical situations, the frames, and the frame-evoking words or expressions associated to them, the lexical units (LU). Every frame corresponds to a scenario involving a set of participants, the frame elements (FEs), that are typically the semantic arguments shared by all LUs in a frame. We report in Table 1 the information recorded in FrameNet for the CAUSE TO WAKE frame. In the first row there is the frame definition with the relevant frame elements, namely AGENT, C AUSE, S LEEPER and S LEEP S TATE."
W09-1127,S07-1018,0,0.0176025,"ardt et al., 2007) to cope with sparsedata problems in the automatic assignment of frame labels. 221 Johansson and Nugues (2007) created a feature representation for every WordNet lemma and used it to train an SVM classifier for each frame that tells whether a lemma belongs to the frame or not. The best-performing feature representation was built using the sequence of unique identifiers for each synset in its hypernym tree and weigthing the synsets according to their relative frequency in the SemCor corpus. They used the mapping in the Semeval-2007 task on frame-semantic structure extraction (Baker et al., 2007) in order to find target words in open text and assign frames. Crespo and Buitelaar (2008) carried out an automatic mapping of medical-oriented frames to WordNet synsets applying a Statistical Hypothesis Testing to select synsets attached to a lexical unit that were statistically significant using a given reference corpus. The mapping obtained was used to expand Spanish FrameNet using EuroWordNet (Vossen, 1998) and evaluation was carried out on the Spanish lexical units obtained after mapping. Given a set of lexical units, De Cao et al. (2008) propose a method to detect the set of suitable Wor"
W09-1127,W07-1402,0,0.0125844,"lop a rulebased semantic parser (Shi and Mihalcea, 2004) as well as to detect target words and assign frames for verbs in an open text (Honnibal and Hawker, 2005). Burchardt et al. (2005) presented a rule-based system for the assignment of FrameNet frames by way of a “detour via WordNet”. They applied a WordNet-based WSD system to annotate lexical units in unseen texts with their contextually determined WordNet synsets and then exploited synonyms and hypernyms information to assign the best frame to the lexical units. The system was integrated into the SALSA RTE system for textual entailment (Burchardt et al., 2007) to cope with sparsedata problems in the automatic assignment of frame labels. 221 Johansson and Nugues (2007) created a feature representation for every WordNet lemma and used it to train an SVM classifier for each frame that tells whether a lemma belongs to the frame or not. The best-performing feature representation was built using the sequence of unique identifiers for each synset in its hypernym tree and weigthing the synsets according to their relative frequency in the SemCor corpus. They used the mapping in the Semeval-2007 task on frame-semantic structure extraction (Baker et al., 2007"
W09-1127,W08-2208,0,0.506257,"Missing"
W09-1127,miguel-buitelaar-2008-domain,0,0.0548542,"frame labels. 221 Johansson and Nugues (2007) created a feature representation for every WordNet lemma and used it to train an SVM classifier for each frame that tells whether a lemma belongs to the frame or not. The best-performing feature representation was built using the sequence of unique identifiers for each synset in its hypernym tree and weigthing the synsets according to their relative frequency in the SemCor corpus. They used the mapping in the Semeval-2007 task on frame-semantic structure extraction (Baker et al., 2007) in order to find target words in open text and assign frames. Crespo and Buitelaar (2008) carried out an automatic mapping of medical-oriented frames to WordNet synsets applying a Statistical Hypothesis Testing to select synsets attached to a lexical unit that were statistically significant using a given reference corpus. The mapping obtained was used to expand Spanish FrameNet using EuroWordNet (Vossen, 1998) and evaluation was carried out on the Spanish lexical units obtained after mapping. Given a set of lexical units, De Cao et al. (2008) propose a method to detect the set of suitable WordNet senses able to evoke a frame by applying a similarity function that exploits differen"
W09-1127,P06-1117,0,0.0958195,"Missing"
W09-1127,U05-1028,0,0.106436,"effort and near-manual quality by importing all lemmas from the mapped synsets. 3 Related work Several experiments have been carried out to develop a FrameNet-WordNet mapping and test its applications. Shi and Mihalcea (2005) described a semi-automatic approach to exploit VerbNet as a bridge between FrameNet and WordNet for verbs, using synonym and hyponym relations and similarity between Levin’s verb classes and FrameNet frames. Their mapping was used to develop a rulebased semantic parser (Shi and Mihalcea, 2004) as well as to detect target words and assign frames for verbs in an open text (Honnibal and Hawker, 2005). Burchardt et al. (2005) presented a rule-based system for the assignment of FrameNet frames by way of a “detour via WordNet”. They applied a WordNet-based WSD system to annotate lexical units in unseen texts with their contextually determined WordNet synsets and then exploited synonyms and hypernyms information to assign the best frame to the lexical units. The system was integrated into the SALSA RTE system for textual entailment (Burchardt et al., 2007) to cope with sparsedata problems in the automatic assignment of frame labels. 221 Johansson and Nugues (2007) created a feature representa"
W09-1127,magnini-cavaglia-2000-integrating,0,0.131227,"Missing"
W09-1127,J05-1004,0,0.181461,"Missing"
W09-1127,D07-1002,0,0.211258,"Missing"
W09-1127,N04-3006,0,0.0310368,"d EuroWordNet (Vossen, 1998) allows to easily populate frame sets for new languages with reduced human effort and near-manual quality by importing all lemmas from the mapped synsets. 3 Related work Several experiments have been carried out to develop a FrameNet-WordNet mapping and test its applications. Shi and Mihalcea (2005) described a semi-automatic approach to exploit VerbNet as a bridge between FrameNet and WordNet for verbs, using synonym and hyponym relations and similarity between Levin’s verb classes and FrameNet frames. Their mapping was used to develop a rulebased semantic parser (Shi and Mihalcea, 2004) as well as to detect target words and assign frames for verbs in an open text (Honnibal and Hawker, 2005). Burchardt et al. (2005) presented a rule-based system for the assignment of FrameNet frames by way of a “detour via WordNet”. They applied a WordNet-based WSD system to annotate lexical units in unseen texts with their contextually determined WordNet synsets and then exploited synonyms and hypernyms information to assign the best frame to the lexical units. The system was integrated into the SALSA RTE system for textual entailment (Burchardt et al., 2007) to cope with sparsedata problems"
W09-1127,tonelli-pianta-2008-frame,1,0.841295,"allelism Our idea is that, if an English lexical unit and its Italian translation belong to the same frame, they are likely to appear also in the same MultiWordNet synset, and the latter would be a good candidate for mapping. In fact, in MultiWordNet the Italian WordNet is strictly aligned with the Princeton WordNet 1.6, with synsets having the same id for both languages, and also semantic relations are preserved in the multilingual hierarchy. Since no Italian FrameNet is available yet, we extended the parallel English-Italian corpus annotated on both sides with frame information described in Tonelli and Pianta (2008) by adding and annotating 400 new parallel sentences. The final corpus contains about 1,000 pairs of parallel sentences where the English and the Italian lexical unit belong to the same frame. Given a pair hl, si, we check if l appears also in the corpus with the frame label Fi and extract its Italian translation lit . If lit appears also in the Italian version of synset s in MultiWordNet, we consider s as a good candidate for the mapping of l and encode this information as a binary feature. Simple synset-frame overlap Intuitively, the more lemmas a frame and a synset have in common, the more"
W09-1127,C98-1013,0,\N,Missing
W09-3740,P98-1013,0,0.0609818,", Emanuele Pianta Abstract In this paper we present a novel approach to mapping FrameNet lexical units to WordNet synsets in order to automatically enrich the lexical unit set of a given frame. While the mapping approaches proposed in the past mainly rely on the semantic similarity between lexical units in a frame and lemmas in a synset, we exploit the definition of the lexical entries in FrameNet and the WordNet glosses to find the best candidate synset(s) for the mapping. Evaluation results are also reported and discussed. 1 FrameNet and the existing mapping approaches The FrameNet database [1] is a lexical resource of English describing some prototypical situations, the frames, and the frame-evoking words or expressions associated with them, the lexical units (LU). Every frame corresponds to a scenario involving a set of participants, the frame elements, that are typically the syntactic dependents of the lexical units. The FrameNet resource is corpus-based, i.e. every lexical unit should be instantiated by at least one example sentence, even if at the moment the definition and annotation step is still incomplete for several LUs. FrameNet has proved to be useful in a number of NLP t"
W09-3740,W08-2208,0,0.515931,"omputational Semantics, pages 342–345, c Tilburg, January 2009. 2009 International Conference on Computational Semantics Several experiments have been carried out in this direction. Johansson and Nugues [5] created a feature representation for every WordNet lemma and used it to train an SVM classifier for each frame that tells whether a lemma belongs to the frame or not. Crespo and Buitelaar [3] carried out an automatic mapping of medical-oriented frames to WordNet synsets, trying to select synsets attached to a LU that were statistically significant in a given reference corpus. De Cao et al. [2] proposed a method to detect the set of suitable WordNet senses able to evoke the same frame by exploiting the hypernym hierarchies that capture the largest number of LUs in the frame. For all above mentioned approaches, a real evaluation on randomly selected frames is missing, and accuracy was mainly computed over the new lexical units obtained for a frame, not on a gold standard where one or more synsets are assigned to every lexical unit in a frame. Besides, it seems that the most common approach to carry out the mapping relies on some similarity measures that perform better on richer sets"
W09-3740,miguel-buitelaar-2008-domain,0,0.0316609,"nyms from the mapped synset, and would allow to exploit the semantic and lexical relations in WordNet to enrich the information encoded in FrameNet. 342 Proceedings of the 8th International Conference on Computational Semantics, pages 342–345, c Tilburg, January 2009. 2009 International Conference on Computational Semantics Several experiments have been carried out in this direction. Johansson and Nugues [5] created a feature representation for every WordNet lemma and used it to train an SVM classifier for each frame that tells whether a lemma belongs to the frame or not. Crespo and Buitelaar [3] carried out an automatic mapping of medical-oriented frames to WordNet synsets, trying to select synsets attached to a LU that were statistically significant in a given reference corpus. De Cao et al. [2] proposed a method to detect the set of suitable WordNet senses able to evoke the same frame by exploiting the hypernym hierarchies that capture the largest number of LUs in the frame. For all above mentioned approaches, a real evaluation on randomly selected frames is missing, and accuracy was mainly computed over the new lexical units obtained for a frame, not on a gold standard where one o"
W09-3740,C98-1013,0,\N,Missing
W11-0908,P98-1013,0,0.371949,"or not, and on the other hand to precision problems, i.e. if an implicit entity is accessible to the reader from the discourse or its context, an appropriate antecedent has to be found. However, a system able to derive the presence of IEs may be a determining factor in improving performance of QA systems and, in general, in Informations Retrieval and Extraction systems. The current computational scene has witnessed an increased interest in the creation and use of semantically annotated computational lexica and their associated annotated corpora, like PropBank (Palmer et al., 2005), FrameNet (Baker et al., 1998) and NomBank (Meyers, 2007), where the proposed annotation scheme has been applied in real contexts. In all these cases, what has been addressed is a basic semantic issue, i.e. labeling PAS associated to semantic predicates like adjectives, verbs and nouns. However, what these corpora have not made available is information related to IEs. For example, in the case of eventive deverbal nominals, information about the subject/object of the nominal predicate is often implicit and has to be understood from the previous 54 Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS"
W11-0908,S07-1018,0,0.0315457,"ent identification over a common test set and considering different kinds of predicates was made by Ruppenhofer et al. (2010). Further details are given in the following section. Data set Train Test Sentences 438 525 Frame inst. 1,370 1,703 Frame types 317 452 Overt FEs 2,526 3,141 DNIs (resolved) 303 (245) 349 (259) INIs 277 361 Table 1: Data set statistics from SemEval task 10 3 SemEval 2010 task 10 The SemEval-2010 task for linking events and their participants in discourse (Ruppenhofer et al., 2010) introduced a new issue w.r.t. the SemEval-2007 task ‘Frame Semantic Structure Extraction’ (Baker et al., 2007), in that it focused on linking local semantic argument structures across sentence boundaries. Specifically, the task included first the identification of frames and frame elements in a text following the FrameNet paradigm (Baker et al., 1998), then the identification of locally uninstantiated roles (NIs). If these roles are indefinite (INI), they have to be marked as such and no antecedent has to be found. On the contrary, if they are definite (DNI), their coreferents have to be found in the wider discourse context. The challenge comprised two tasks, namely the full task (semantic role recogn"
W11-0908,S10-1059,0,0.463373,"d with gold standard frame information. The participants had to i) assess if a local argument is implicit; ii) decide whether it is an INI or a DNI and iii) in the second case, find the antecedent of the implicit argument. We report in Table 1 some statistics about the provided data sets from Ruppenhofer et al. (2010). Note that overt FEs are the explicit frame elements annotated in the data set. Although 26 teams downloaded the data sets, there were only two submissions, probably depending on the intrinsic difficulties of the task (see dis57 cussion in Section 5). The best performing system (Chen et al., 2010) is based on a supervised learning approach using, among others, distributional semantic similarity between the heads of candidate referents and role fillers in the training data, but its performance is strongly affected by data sparseness. Indeed, only 438 sentences with annotated NIs were made available in the training set, which is clearly insufficient to capture such a multifaceted phenomenon with a supervised approach. The second system participating in the task (Tonelli and Delmonte, 2010) was an adaptation of an existing LFG-based system for deep semantic analysis (Delmonte, 2009), whos"
W11-0908,P10-1160,0,0.217117,"aluated on a data set, so we cannot directly compare its performance with other approaches. Furthermore, it is strongly domain-dependent. In a case study, Burchardt et al. (2005) propose to identify implicit arguments exploiting contextual relations from deep-parsing and lexico-semantic frame relations encoded in FrameNet. In particular, they suggest converting a text into a network of lexico-semantic predicate-argument relations connected through frame-to-frame relations and recurrent anaphoric linking patterns. However, the authors do not implement and evaluate this approach. Most recently, Gerber and Chai (2010) have presented a supervised classification model for the recovery of implicit arguments of nominal predicates in NomBank. The model features are quite different from those usually considered in standard SRL tasks and include among others information from VerbNet classes, pointwise mutual information between semantic arguments, collocation and frequency information about the predicates, information about parent nodes and siblings of the predicates and discourse information. The authors show the feasibility of their approach, which however relies on a selected group of nominal predicates with a"
W11-0908,P86-1004,0,0.882878,"gent is licensed by the passive construction: (3) One of them was suddenly shut off ∅. (4) I am reckoned fleet of foot ∅. Cases of INI were annotated by the organizers of the SemEval task 10 also with nominal predicates, as shown in the example below, where the perceiver of the odour is left unspecified: (5) Rank reeds and lush, slimy water-plants sent an odour ∅ of decay and a heavy miasmatic vapour. Few attempts have been done so far to automatically deal with the recovery of implicit information 56 in text. One of the earliest systems for identifying extra-sentential arguments is PUNDIT by Palmer et al. (1986). This Prolog-based system comprises a syntactic component for parsing, a semantic component, which decomposes predicates into component meanings and fills their semantic roles with syntactic constituents based on a domain-specific model, and a reference resolution component, which is called both for explicit constituents and for obligatory implicit constituents. The reference resolution process is based on a focus list with all potential pronominal referents identified by the semantic component. The approach, however, has not been evaluated on a data set, so we cannot directly compare its per"
W11-0908,J05-1004,0,0.120682,"ding whether an item is implicit or not, and on the other hand to precision problems, i.e. if an implicit entity is accessible to the reader from the discourse or its context, an appropriate antecedent has to be found. However, a system able to derive the presence of IEs may be a determining factor in improving performance of QA systems and, in general, in Informations Retrieval and Extraction systems. The current computational scene has witnessed an increased interest in the creation and use of semantically annotated computational lexica and their associated annotated corpora, like PropBank (Palmer et al., 2005), FrameNet (Baker et al., 1998) and NomBank (Meyers, 2007), where the proposed annotation scheme has been applied in real contexts. In all these cases, what has been addressed is a basic semantic issue, i.e. labeling PAS associated to semantic predicates like adjectives, verbs and nouns. However, what these corpora have not made available is information related to IEs. For example, in the case of eventive deverbal nominals, information about the subject/object of the nominal predicate is often implicit and has to be understood from the previous 54 Proceedings of the ACL 2011 Workshop on Relati"
W11-0908,S10-1008,0,0.738999,"ommitment, for example “I can promise ∅ that one of you will be troubled [→ unexpressed Addressee]” and “I dare swear ∅ that before tomorrow night he will be fluttering in our net [→ unexpressed Addressee]”. In this paper we discuss the issues related to the identification of implicit entities in text, focussing in particular on omissions of core arguments of predicates. We investigate the topic from the perspective proposed by (Fillmore, 1986) and base our observations on null instantiated arguments annotated for the SemEval 2010 Task 10, ‘Linking Events and Their Participants in Discourse’ (Ruppenhofer et al., 2010)2 . The paper is structured as follows: in Section 2 we detail the task of identifying null instantiated arguments from a theoretical perspective and describe related work. In Section 3 we briefly introduce the SemEval task 10 for identifying implicit arguments in text, while in Section 4 we detail our proposal for NI identification and binding. In Section 5 we give a thorough description of the types of null instantiations annotated in the SemEval data set and we explain the behavior of our algorithm w.r.t. such cases. We also compare our results with the output of the systems participating t"
W11-0908,S10-1065,1,0.681685,"ding on the intrinsic difficulties of the task (see dis57 cussion in Section 5). The best performing system (Chen et al., 2010) is based on a supervised learning approach using, among others, distributional semantic similarity between the heads of candidate referents and role fillers in the training data, but its performance is strongly affected by data sparseness. Indeed, only 438 sentences with annotated NIs were made available in the training set, which is clearly insufficient to capture such a multifaceted phenomenon with a supervised approach. The second system participating in the task (Tonelli and Delmonte, 2010) was an adaptation of an existing LFG-based system for deep semantic analysis (Delmonte, 2009), whose output was mapped to FrameNet-style annotation. In this case, the major challenge was to cope with the classification of some NI phenomena which are very much dependent on frame specific information, and can hardly be generalized in the LFG framework. 4 A linguistically motivated proposal for NI identification and binding In this section, we describe our proposal for dealing with INI/DNI identification and evaluate our output against SemEval gold standard data. As discussed in the previous sec"
W11-0908,H86-1011,0,\N,Missing
W11-0908,C98-1013,0,\N,Missing
W12-1102,chrupala-etal-2008-learning,0,0.0491824,"Missing"
W12-1102,S10-1004,0,0.190972,"Missing"
W12-1102,pianta-etal-2008-textpro,1,0.882203,"Missing"
W12-1102,S10-1036,1,0.881043,"Missing"
W12-2206,W10-1001,0,0.178667,"years, research on English readability has progressed toward more sophisticated models that take into account difficulty at syntactic, semantic and discourse level thanks to advances in psycholinguistic accounts of text processing (Graesser et al., 2004) and to the availability of a wide range of NPL tools (e.g. dependency and constituency parsers, anaphora resolution systems, etc.) and resources (e.g. WordNet). However, for many other languages current approaches for readability assessment still rely on few basic factors. A notable exception is the Coh-Metrix-PORT tool (Scarton et al., 2009; Aluisio et al., 2010), which includes 60 readability measures for Brazilian Portuguese inspired by the Coh-Metrix (Graesser et al., 2004). A different approach has been followed by the developers of the DeLite system for German (Gl¨ockner 41 et al., 2006; von der Br¨uck et al., 2008): the tool computes a set of indices measuring the linguistic complexity of a document through deep parsing and outputs a final readability score obtained by applying the k-nearest neighbor algorithm based on 3,000 ratings from 300 users. As for Italian, the only work aimed at improving on the performance of standard readability indice"
W12-2206,J08-1001,0,0.0499367,"the cohesion of the text. These indices have been devised based on psycholinguistic studies on the mental representation of textual content (McNamara et al., 1996) and address various characteristics of explicit text, from lexicon to syntax, semantics and discourse, that contribute to the creation of this representation. Although the tool relies on widely used NLP techniques such as PoS tagging and parsing, there have been limited attempts to employ it in studies on automatic assessment of text cohesion. Nevertheless, recent works in the NLP community investigating the impact of entity grids (Barzilay and Lapata, 2008) or of discourse relations (Pitler and Nenkova, 2008) on text coherence and readability go in the same direction as research on Coh-Metrix, in that they aim at identifying the linguistic features that best express readability at syntactic, semantic and discourse level. The indices belonging to Coh-Metrix are divided into five main classes: • General Word and Text Information: The indices in this class capture the correlation between brain’s processing time and word-level information. For example, many syllables in a word or many words in a sentence are likely to make a document more difficult"
W12-2206,W11-2308,0,0.193083,"Missing"
W12-2206,P81-1022,0,0.0595419,"Missing"
W12-2206,pianta-etal-2008-textpro,1,0.827793,"eference corpus, which is supposed to be a general corpus representing many domains. Index 8 is the logarithm of raw frequency of content words, because logarithm proved to be compatible with reading time (Haberlandt and Graesser, 1985). Index 9 is obtained by computing first the lowest frequency score among all the content words in each sentence, and then calculating the mean. Index 10 is obtained by computing first the lowest log frequency score among all content words in each sentence, and then calculating the mean. Content words were extracted by running the TextPro NLP suite for Italian (Pianta et al., 2008)6 and keeping only words tagged with one of WordNet PoS, namely v, a, n and r. Indices 11 and 12 compute the abstractness of nouns and verbs by measuring the distance between the WordNet synset containing the lemma (most frequent sense) and the root. Then, the mean distance of all nouns and verbs in the text is computed. We obtain this index using MultiWordNet (Pianta et al., 2002), the Italian version of WordNet, aligned at synset level with the English one. Indices from 13 to 17 measure the syntactic complexity of sentences based on parsing output. Indices 13-15 are computed after parsing ea"
W12-2206,D08-1020,0,0.316139,"sed based on psycholinguistic studies on the mental representation of textual content (McNamara et al., 1996) and address various characteristics of explicit text, from lexicon to syntax, semantics and discourse, that contribute to the creation of this representation. Although the tool relies on widely used NLP techniques such as PoS tagging and parsing, there have been limited attempts to employ it in studies on automatic assessment of text cohesion. Nevertheless, recent works in the NLP community investigating the impact of entity grids (Barzilay and Lapata, 2008) or of discourse relations (Pitler and Nenkova, 2008) on text coherence and readability go in the same direction as research on Coh-Metrix, in that they aim at identifying the linguistic features that best express readability at syntactic, semantic and discourse level. The indices belonging to Coh-Metrix are divided into five main classes: • General Word and Text Information: The indices in this class capture the correlation between brain’s processing time and word-level information. For example, many syllables in a word or many words in a sentence are likely to make a document more difficult for the brain to process it. Also, if the type/token"
W13-1202,P10-1143,0,0.0191435,"on of the conflict, which enables use cases that require the analysis of the conflict itself. 5 The GAF Annotation Framework This section explains the basic idea behind GAF by using texts on earthquakes in Indonesia. GAF provides a general model for event representation (including textual and extra-textual mentions) as well as exact representation of linguistic annotation or output of NLP tools. Simply put, GAF is the combination of textual analyses and formal semantic representations in RDF. 5.1 A SEM for earthquakes We selected newspaper texts on the January 2009 West Papua earthquakes from Bejan and Harabagiu (2010) to illustrate GAF. This choice was made because the topic “earthquake” illustrates the advantage of sharing URIs across domains. Gao and Hunter (2011) propose a Linked Data model to capture major geological events such as earthquakes, volcano activity and tsunamis. They combine information from different seismological databases with the intention to provide more complete information 15 to experts which may help to predict the occurrence of such events. The information can also be used in text interpretation. We can verify whether interpretations by NLP tools correspond to the data and relatio"
W13-1202,P11-1098,0,0.0122162,"Missing"
W13-1202,C96-1079,0,0.0462667,"ents, temporal relations, etc.) are represented explicitly in the semantic layer. The remainder of this paper is structured as follows. In Section 2, we present related work and explain the motivation behind our approach. Section 3 describes the in-text annotation approach. Our semantic annotation layer is presented in Section 4. Sections 5-7 present GAF through a use case on earthquakes in Indonesia. This is followed by our conclusions and future work in section 8. 2 Motivation and Background Annotation of events and of relations between them has a long tradition in NLP. The MUC conferences (Grishman and Sundheim, 1996) in the 90s did not explicitly annotate events and coreference relations, but the templates used for evaluating the information extraction tasks indirectly can be seen as annotation of events represented in newswires. Such events are not ordered in time or further related to each other. In response, Setzer and Gaizauskas (2000) describe an annotation framework to create coherent temporal orderings of events represented in documents using closure rules. They suggest that reasoning with text independent models, such as a calendar, helps annotating textual representations. More recently, generic"
W13-1202,P12-2045,0,0.0756526,"Missing"
W13-1202,J05-1004,0,0.020106,"tly annotate events and coreference relations, but the templates used for evaluating the information extraction tasks indirectly can be seen as annotation of events represented in newswires. Such events are not ordered in time or further related to each other. In response, Setzer and Gaizauskas (2000) describe an annotation framework to create coherent temporal orderings of events represented in documents using closure rules. They suggest that reasoning with text independent models, such as a calendar, helps annotating textual representations. More recently, generic corpora, such as Propbank (Palmer et al., 2005) and the Framenet corpus (Baker et al., 2003) have been built according to linguistic principles. The annotations aim at properly representing verb structures within a sentence context, focusing on verb arguments, semantic roles and other elements. In ACE 2004 (Linguistic Data Consortium, 2004b), event detection and linking is included as a pilot task for the first time, inspired by annotation schemes developed for named entities. They distinguish between event mentions and the trigger event, which is the mention that most clearly expresses its occurrence (Linguistic Data Consortium, 2004a). T"
W13-1202,pustejovsky-etal-2010-iso,0,0.0205156,"between instances and instance mentions avoiding the problem of determining a trigger event. Additionally, it facilitates the integration of information from extra-textual sources and information that can be inferred from texts, but is not explicitly mentioned. Sections 5 to 7 will explain how we can achieve this with GAF. 3 The TERENCE annotation format The TERENCE Annotation Format (TAF) is defined within the TERENCE Project1 with the goal to include event mentions, temporal expressions and participant mentions in a single annotation protocol (Moens et al., 2011). TAF is based on ISOTimeML (Pustejovsky et al., 2010), but introduces several adaptations in order to fit the domain of children’s stories for which it was originally developed. The format has been used to annotate around 30 children stories in Italian and 10 in English. We selected TAF as the basis for our in-text annotation for three reasons. First, it incorporates the (in our opinion crucial) distinction between instances and instance mentions. Second, it adapts some consolidated paradigms for linguistic annotation such as TimeML for events and temporal expressions and ACE for participants and participant mentions (Linguistic Data Consortium,"
W13-1202,setzer-gaizauskas-2000-annotating,0,0.0651309,"4. Sections 5-7 present GAF through a use case on earthquakes in Indonesia. This is followed by our conclusions and future work in section 8. 2 Motivation and Background Annotation of events and of relations between them has a long tradition in NLP. The MUC conferences (Grishman and Sundheim, 1996) in the 90s did not explicitly annotate events and coreference relations, but the templates used for evaluating the information extraction tasks indirectly can be seen as annotation of events represented in newswires. Such events are not ordered in time or further related to each other. In response, Setzer and Gaizauskas (2000) describe an annotation framework to create coherent temporal orderings of events represented in documents using closure rules. They suggest that reasoning with text independent models, such as a calendar, helps annotating textual representations. More recently, generic corpora, such as Propbank (Palmer et al., 2005) and the Framenet corpus (Baker et al., 2003) have been built according to linguistic principles. The annotations aim at properly representing verb structures within a sentence context, focusing on verb arguments, semantic roles and other elements. In ACE 2004 (Linguistic Data Cons"
W13-1202,W11-0116,0,\N,Missing
W13-1202,bartalesi-lenzi-etal-2012-cat,1,\N,Missing
W14-0702,bethard-etal-2008-building,0,0.231471,"between discourse arguments, in the framework of 10 Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 10–19, c Gothenburg, Sweden, April 26, 2014. 2014 Association for Computational Linguistics the Penn Discourse Treebank (PDTB). As opposed to PropBank, this kind of relations holds only between clauses and do not involve predicates and their arguments. In PDTB, the Cause relation type is classified as a subtype of C ONTINGENCY. Causal relations have also been annotated as relations between events in a restricted set of linguistic constructions (Bethard et al., 2008), between clauses in text from novels (Grivaz, 2010), or in noun-noun compounds (Girju et al., 2007). Several types of annotation guidelines for causal relations have been presented, with varying degrees of reliability. One of the simpler approaches asks annotators to check whether the sentence they are reading can be paraphrased using a connective phrase such as and as a result or and as a consequence (Bethard et al., 2008). Another approach to annotate causal relations tries to combine linguistic tests with semantic reasoning tests. In Grivaz (2010), the linguistic paraphrasing suggested by"
W14-0702,W08-1301,0,0.057827,"Missing"
W14-0702,W13-1202,1,0.878655,"Missing"
W14-0702,S07-1003,0,0.218136,"Missing"
W14-0702,grivaz-2010-human,0,0.0379738,"ings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 10–19, c Gothenburg, Sweden, April 26, 2014. 2014 Association for Computational Linguistics the Penn Discourse Treebank (PDTB). As opposed to PropBank, this kind of relations holds only between clauses and do not involve predicates and their arguments. In PDTB, the Cause relation type is classified as a subtype of C ONTINGENCY. Causal relations have also been annotated as relations between events in a restricted set of linguistic constructions (Bethard et al., 2008), between clauses in text from novels (Grivaz, 2010), or in noun-noun compounds (Girju et al., 2007). Several types of annotation guidelines for causal relations have been presented, with varying degrees of reliability. One of the simpler approaches asks annotators to check whether the sentence they are reading can be paraphrased using a connective phrase such as and as a result or and as a consequence (Bethard et al., 2008). Another approach to annotate causal relations tries to combine linguistic tests with semantic reasoning tests. In Grivaz (2010), the linguistic paraphrasing suggested by Bethard et al. (2008) is augmented with rules that t"
W14-0702,S13-2001,0,0.303965,"this annotation, we report some statistics on the behavior of causal cues in text and perform a preliminary investigation on the interaction between causal and temporal relations. 1 Introduction The annotation of events and event relations in natural language texts has gained in recent years increasing attention, especially thanks to the development of TimeML annotation scheme (Pustejovsky et al., 2003), the release of TimeBank (Pustejovsky et al., 2006) and the organization of several evaluation campaigns devoted to automatic temporal processing (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). However, while there is a wide consensus in the NLP community over the modeling of temporal relations between events, mainly based on Allen’s interval algebra (Allen, 1983), the question on how to model other types of event relations is still open. In particular, linguistic annotation of causal relations, which have been widely investigated from a philosophical and logical point of view, are still under debate. This leads, in turn, to the lack of a standard benchmark to evaluate causal relation extraction systems, making it difficult to compare systems performances, and to identify the state"
W14-0702,S07-1014,0,0.0508225,"relations in the TempEval-3 corpus. Based on this annotation, we report some statistics on the behavior of causal cues in text and perform a preliminary investigation on the interaction between causal and temporal relations. 1 Introduction The annotation of events and event relations in natural language texts has gained in recent years increasing attention, especially thanks to the development of TimeML annotation scheme (Pustejovsky et al., 2003), the release of TimeBank (Pustejovsky et al., 2006) and the organization of several evaluation campaigns devoted to automatic temporal processing (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). However, while there is a wide consensus in the NLP community over the modeling of temporal relations between events, mainly based on Allen’s interval algebra (Allen, 1983), the question on how to model other types of event relations is still open. In particular, linguistic annotation of causal relations, which have been widely investigated from a philosophical and logical point of view, are still under debate. This leads, in turn, to the lack of a standard benchmark to evaluate causal relation extraction systems, making it difficult to compare s"
W14-0702,P00-1043,0,0.0901106,"ilosophical and logical point of view, are still under debate. This leads, in turn, to the lack of a standard benchmark to evaluate causal relation extraction systems, making it difficult to compare systems performances, and to identify the state-ofthe-art approach for this particular task. 2 Existing resources on Causality Several attempts have been made to annotate causal relations in texts. A common approach is to look for specific cue phrases like because or since or to look for verbs that contain a cause as part of their meaning, such as break (cause to be broken) or kill (cause to die) (Khoo et al., 2000; Sakaji et al., 2008; Girju et al., 2007). In PropBank (Bonial et al., 2010), causal relations are annotated in the form of predicate-argument relations, where ARGM - CAU is used to annotate “the reason for an action”, for example: “They [PREDICATE moved] to London [ARGM - CAU because of the baby].” Another scheme annotates causal relations between discourse arguments, in the framework of 10 Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 10–19, c Gothenburg, Sweden, April 26, 2014. 2014 Association for Computational Linguistics the Penn Disco"
W14-0702,P09-2004,0,0.0563362,"of automatic annotation may be prone to errors because it takes into account only a limited list of causal connectors. Besides, it only partially accounts for possible ambiguities of causal cues and may suffer from parsing errors. However, this allows us to make some preliminary remarks on the amount of causal information found in the TempEval-3 corpus. Some statistics are reported in the following subsection. 1. The TBAQ-cleaned corpus is PoS-tagged and parsed using the Stanford dependency parser (de Marneffe and Manning, 2008). 2. The corpus is further analyzed with the addDiscourse tagger (Pitler and Nenkova, 2009), which automatically identifies explicit discourse connectives and their sense, i.e. E X PANSION , C ONTINGENCY , C OMPARISON and T EMPORAL. This is used to disambiguate causal connectives (e.g. we consider only the occurrences of since when it is a causal connective, meaning that it falls into C ONTIN GENCY class instead of T EMPORAL ). 4.1 Basic construction. In Table 1 we report some statistics on the non-periphrastic structures identified starting from verbs expressing the three categories of causation. Note that for the verbs have, start, hold and keep, even though they connect two event"
W14-0702,S10-1010,0,\N,Missing
W18-5107,bartalesi-lenzi-etal-2012-cat,1,0.805765,"ccessible from the outside, a preliminary agreement was signed involving students’ parents, teachers and headmasters to allow the activity. The threads were then saved in anonymous form and manually annotated by two expert linguists. The original names were not completely removed, but they were replaced by fictitious names, so that it was still possible to track all the messages exchanged by the same person. 5 Corpus Description The corpus of Whatsapp chats is made of 14,600 tokens divided in 10 chats. All the chats have been annotated by two annotators using the CAT web-based tool (Bartalesi Lenzi et al., 2012) following the same guidelines. Our guidelines are an adaptation to Italian of the “Guidelines for the Fine-Grained Analysis of Cyberbullying” developed for English by the Language and Translation Technology Team of Ghent University (Van Hee et al., 2015c). Following these guidelines, the annotator should identify all the harmful expressions in a conversation and, for each of it, he/she should annotate: (i) the cyberbullying role of the message’s author; (ii) the cyberbullying type of the expression; (iii) the presence of sarcasm in the expression; (iv) whether the expression containing insult"
W18-5107,W18-3501,0,0.0404488,"Missing"
W18-5107,R15-1086,0,0.0583497,"Missing"
W19-2305,W12-2910,0,0.0253778,"nglish Wikipedia (Zhu et al., 2010). These data have become the common benchmark for evaluating new approaches to neural text simplification. These methods rely on the use of deep reinforcement learning (Zhang and Lapata, 2017), memory-augmented neural networks (Vu et al., 2018), the combination of semantic parsing and neural approaches (Sulem et al., 2018) and the personalisation to specific grade levels (Scarton and Specia, 2018). Due to data paucity, none of them can be tested on other languages, for which less data-intensive, rule-based solutions have been proposed (Brouwers et al., 2012; Bott et al., 2012; Barlacchi and Tonelli, 2013). The main disadvantage of such solutions, however, is a reduced portability and scalability to new scenarios, which require the creation of new sets of rules each time a new language (or a new domain with specific idiosyncrasies) has to be covered. Neural text simplification has gained increasing attention in the NLP community thanks to recent advancements in deep sequence-tosequence learning. Most recent efforts with such a data-demanding paradigm have dealt with the English language, for which sizeable training datasets are currently available to deploy competi"
W19-2305,W14-1820,0,0.0657305,"Missing"
W19-2305,F12-2016,0,0.060497,"Missing"
W19-2305,P18-2113,0,0.0437516,"mplex and a simple version of the same document, which are large enough to experiment with deep neural systems, are Newsela (Xu et al., 2015) and the aligned version of simple and standard English Wikipedia (Zhu et al., 2010). These data have become the common benchmark for evaluating new approaches to neural text simplification. These methods rely on the use of deep reinforcement learning (Zhang and Lapata, 2017), memory-augmented neural networks (Vu et al., 2018), the combination of semantic parsing and neural approaches (Sulem et al., 2018) and the personalisation to specific grade levels (Scarton and Specia, 2018). Due to data paucity, none of them can be tested on other languages, for which less data-intensive, rule-based solutions have been proposed (Brouwers et al., 2012; Bott et al., 2012; Barlacchi and Tonelli, 2013). The main disadvantage of such solutions, however, is a reduced portability and scalability to new scenarios, which require the creation of new sets of rules each time a new language (or a new domain with specific idiosyncrasies) has to be covered. Neural text simplification has gained increasing attention in the NLP community thanks to recent advancements in deep sequence-tosequence"
W19-2305,P17-1099,0,0.0370237,"uence of words is fed to the encoder, which maps it into a sequence of continuous representations (the hidden states of the encoder) providing increasing levels of abstraction. At each time step, based on these continuous representations and the generated word in the previous time step, the decoder generates the next word. This process continues until the decoder generates the end-of-sentence symbol. This sequence-to-sequence model is extended by adding a pointer-generator network that allows both copying words via pointing to the source sentence, and generating words from a fixed vocabulary (See et al., 2017). At each time step, the network estimates the probability of generating a word and uses this probability as a gate to decide whether to generate or copy the word. To apply this pointer-generator network, a shared vocabulary containing all the words in the complex and simple training sentences is used. This architecture is implemented in the OpenNMT platform (Klein et al., 2017). • We explore different approaches for augmenting training data for neural text simplification using weak supervision; • We test them in under-resourced conditions on Italian and Spanish. 2 Neural sentence simplificati"
W19-2305,P17-4012,0,0.0413134,"end-of-sentence symbol. This sequence-to-sequence model is extended by adding a pointer-generator network that allows both copying words via pointing to the source sentence, and generating words from a fixed vocabulary (See et al., 2017). At each time step, the network estimates the probability of generating a word and uses this probability as a gate to decide whether to generate or copy the word. To apply this pointer-generator network, a shared vocabulary containing all the words in the complex and simple training sentences is used. This architecture is implemented in the OpenNMT platform (Klein et al., 2017). • We explore different approaches for augmenting training data for neural text simplification using weak supervision; • We test them in under-resourced conditions on Italian and Spanish. 2 Neural sentence simplification system Related work The lack of data for training sequence-to-sequence models is a problem that has been addressed in several NLP tasks. In MT, for instance, synthetic parallel data for low-resource settings have been generated by automatically translating sentences from the target language into the source language (Sennrich et al., 2016b,a). In speech translation, recent wor"
W19-2305,P16-1009,0,0.507026,"lopment of neural solutions also for languages other than English, we explore data augmentation techniques for creating task-specific training data. Our experiments range from simple oversampling techniques to weakly supervised data augmentation methods inspired by recent 37 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (NeuralGen), pages 37–44 c Minneapolis, Minnesota, USA, June 6, 2019. 2019 Association for Computational Linguistics works in other NLP tasks (B´erard et al., 2016; Ding and Balog, 2018), in particular Machine Translation (MT) (Sennrich et al., 2016b). In a nutshell, taking an opposite direction to simplification, we proceed by i) automatically selecting simple sentences from a large pool of monolingual data, and ii) synthetically creating complex sentences. These artificially created sentences will be then used as the “source” side of new difficult– simple training pairs fed into an MT-like encoderdecoder architecture. Our hypothesis is that, though sub-optimal due to possible errors introduced in the automatic generation of complex sentences, these training pairs represent useful material for building our sequence-to-sequence text simp"
W19-2305,W14-0406,0,0.0865816,"Missing"
W19-2305,P18-1016,0,0.0396309,"or this language. Indeed, the only available datasets composed of a complex and a simple version of the same document, which are large enough to experiment with deep neural systems, are Newsela (Xu et al., 2015) and the aligned version of simple and standard English Wikipedia (Zhu et al., 2010). These data have become the common benchmark for evaluating new approaches to neural text simplification. These methods rely on the use of deep reinforcement learning (Zhang and Lapata, 2017), memory-augmented neural networks (Vu et al., 2018), the combination of semantic parsing and neural approaches (Sulem et al., 2018) and the personalisation to specific grade levels (Scarton and Specia, 2018). Due to data paucity, none of them can be tested on other languages, for which less data-intensive, rule-based solutions have been proposed (Brouwers et al., 2012; Bott et al., 2012; Barlacchi and Tonelli, 2013). The main disadvantage of such solutions, however, is a reduced portability and scalability to new scenarios, which require the creation of new sets of rules each time a new language (or a new domain with specific idiosyncrasies) has to be covered. Neural text simplification has gained increasing attention in"
W19-2305,P17-2014,0,0.10099,"e by automatic processing, large “silver” data provide a valuable additional complement to small “gold” training corpora. Regarding neural text simplification, we are not aware of previous work on extending small training corpora with synthetic data. Indeed, the lack of training instances has been a major issue in the development of such applications for languages other than English. 3 Our sentence simplification approach is based on the attentional encoder-decoder model (Bahdanau et al., 2014) initially proposed for MT. It takes as input a complex sentence and outputs its simplified version (Nisioi et al., 2017). Cast as a (monolingual) translation task, it provides a comprehensive solution to address both lexical and structural simplification, since the model does not only learn single term replacements, but also more complex structural changes. Initially, a sequence of words is fed to the encoder, which maps it into a sequence of continuous representations (the hidden states of the encoder) providing increasing levels of abstraction. At each time step, based on these continuous representations and the generated word in the previous time step, the decoder generates the next word. This process contin"
W19-2305,W08-2140,0,0.0441388,"sampling and the use of external word embeddings to be fed to the neural simplification system. Our approach is evaluated on Italian and Spanish, for which few thousand gold sentence pairs are available. The results show that these techniques yield performance improvements over a baseline sequence-to-sequence configuration. 1 Marco Turchi Fondazione Bruno Kessler Trento, Italy turchi@fbk.eu Introduction Text simplification aims at making a text more readable by reducing its lexical and structural complexity while preserving the meaning. (Chandrasekar and Bangalore, 1997; Carroll et al., 1998; Vickrey and Koller, 2008; Crossley et al., 2012; Shardlow, 2014). Neural approaches to the task have gained increasing attention in the NLP community thanks to recent advancements of deep, To alleviate the data bottleneck issue, enabling the development of neural solutions also for languages other than English, we explore data augmentation techniques for creating task-specific training data. Our experiments range from simple oversampling techniques to weakly supervised data augmentation methods inspired by recent 37 Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation (Neura"
W19-2305,L18-1615,0,0.0534213,"e Spanish Billion Word Corpus, that is widely used in NLP experiments on Spanish (Zea et al., 2016; Quir´os et al., 2016).9 To favour the extraction of word embeddings from simple texts, we increase the Spanish Billion Word Corpus by adding our extracted simple Spanish texts. In total, Spanish word embeddings are extracted from a corpus of nearly 1.5B words. Spanish The Spanish gold standard is obtained from the Spanish Newsela corpus,5 containing 1, 221 documents manually annotated by professionals for different proficiency levels. We align complex– ˇ simple pairs using the CATS-Align6 tool (Stajner et al., 2018) and discard the pairs coupled with an alignment accuracy below 0.5. The gold standard contains 55, 890 complex-to-simple pairs. The set of simple sentences used to create the synthetic pairs is extracted from a large monolin5.3 System configuration OpenNMT is run on a Nvidia Tesla K80 GPU using stochastic gradient descent (Robbins and Monro, 1951) optimization with learning rate 1. Each run is repeated three times with different seeds, then the average value is considered. Since the source and target languages are the same, in 2 www.opensubtitles.org www.gazzettaufficiale.it 4 simple.wikipedi"
W19-2305,N18-2013,0,0.0248981,"instances) and sizable datasets have been developed and made available only for this language. Indeed, the only available datasets composed of a complex and a simple version of the same document, which are large enough to experiment with deep neural systems, are Newsela (Xu et al., 2015) and the aligned version of simple and standard English Wikipedia (Zhu et al., 2010). These data have become the common benchmark for evaluating new approaches to neural text simplification. These methods rely on the use of deep reinforcement learning (Zhang and Lapata, 2017), memory-augmented neural networks (Vu et al., 2018), the combination of semantic parsing and neural approaches (Sulem et al., 2018) and the personalisation to specific grade levels (Scarton and Specia, 2018). Due to data paucity, none of them can be tested on other languages, for which less data-intensive, rule-based solutions have been proposed (Brouwers et al., 2012; Bott et al., 2012; Barlacchi and Tonelli, 2013). The main disadvantage of such solutions, however, is a reduced portability and scalability to new scenarios, which require the creation of new sets of rules each time a new language (or a new domain with specific idiosyncrasies) h"
W19-2305,D11-1038,0,0.0689553,"ubtitles,2 the Pais`a corpus (Lyding et al., 2014), Wikipedia and the collection of Italian laws.3 This merging process results in around 1.3B words and 125M sentences. We rank all sentences by readability level according to the best features described in (Dell’Orletta et al., 2014) and keep the 500, 000 most readable (i.e. simplest) sentences to create the synthetic pairs. This process is needed due to the lack of an Italian equivalent of the Simple English Wikipedia,4 which is widely used as a source of simple monolingual data when dealing with English text simplification (Zhu et al., 2010; Woodsend and Lapata, 2011). From the large corpus described above, before filtering only simple sentences, we also create word embeddings with 300 dimensions using word2vec (Mikolov et al., 2013). 5.2 gual corpus covering different domains, obtained from websites written in simple Spanish for language learners.7 The documents are then ranked based on the Flesch-Szigriszt readability score for Spanish (Szigriszt, 1993)8 and all sentences belonging to the most readable ones are included in the set of simple monolingual data (484, 325 simple sentences in total, from a set of about 1.2M sentences). For Spanish, we do not r"
W19-2305,Q15-1021,0,0.106446,"ne Bruno Kessler Trento, Italy negri@fbk.eu Mattia Di Gangi Fondazione Bruno Kessler Trento, Italy digangi@fbk.eu Abstract sequence-to-sequence approaches. However, all recent improvements have dealt with English. The main reason is that such data-hungry approaches require large training sets (in the order of hundred thousand instances) and sizable datasets have been developed and made available only for this language. Indeed, the only available datasets composed of a complex and a simple version of the same document, which are large enough to experiment with deep neural systems, are Newsela (Xu et al., 2015) and the aligned version of simple and standard English Wikipedia (Zhu et al., 2010). These data have become the common benchmark for evaluating new approaches to neural text simplification. These methods rely on the use of deep reinforcement learning (Zhang and Lapata, 2017), memory-augmented neural networks (Vu et al., 2018), the combination of semantic parsing and neural approaches (Sulem et al., 2018) and the personalisation to specific grade levels (Scarton and Specia, 2018). Due to data paucity, none of them can be tested on other languages, for which less data-intensive, rule-based solu"
W19-2305,W16-2705,0,0.0352302,"Missing"
W19-2305,D17-1062,0,0.0238801,"uire large training sets (in the order of hundred thousand instances) and sizable datasets have been developed and made available only for this language. Indeed, the only available datasets composed of a complex and a simple version of the same document, which are large enough to experiment with deep neural systems, are Newsela (Xu et al., 2015) and the aligned version of simple and standard English Wikipedia (Zhu et al., 2010). These data have become the common benchmark for evaluating new approaches to neural text simplification. These methods rely on the use of deep reinforcement learning (Zhang and Lapata, 2017), memory-augmented neural networks (Vu et al., 2018), the combination of semantic parsing and neural approaches (Sulem et al., 2018) and the personalisation to specific grade levels (Scarton and Specia, 2018). Due to data paucity, none of them can be tested on other languages, for which less data-intensive, rule-based solutions have been proposed (Brouwers et al., 2012; Bott et al., 2012; Barlacchi and Tonelli, 2013). The main disadvantage of such solutions, however, is a reduced portability and scalability to new scenarios, which require the creation of new sets of rules each time a new langu"
W19-2305,C10-1152,0,0.323913,"r Trento, Italy digangi@fbk.eu Abstract sequence-to-sequence approaches. However, all recent improvements have dealt with English. The main reason is that such data-hungry approaches require large training sets (in the order of hundred thousand instances) and sizable datasets have been developed and made available only for this language. Indeed, the only available datasets composed of a complex and a simple version of the same document, which are large enough to experiment with deep neural systems, are Newsela (Xu et al., 2015) and the aligned version of simple and standard English Wikipedia (Zhu et al., 2010). These data have become the common benchmark for evaluating new approaches to neural text simplification. These methods rely on the use of deep reinforcement learning (Zhang and Lapata, 2017), memory-augmented neural networks (Vu et al., 2018), the combination of semantic parsing and neural approaches (Sulem et al., 2018) and the personalisation to specific grade levels (Scarton and Specia, 2018). Due to data paucity, none of them can be tested on other languages, for which less data-intensive, rule-based solutions have been proposed (Brouwers et al., 2012; Bott et al., 2012; Barlacchi and To"
W19-3511,S17-2126,0,0.0258161,"As a test set, we manually annotate 900 Instagram comments, randomly extracted from the Manchester network, labeling them as hate speech or not. Overall, the test set contains 787 non-offensive and 113 offensive messages. We preprocess both data sets, given that hashtags, user mentions, links to external media and emojis are common in social media interactions. To normalize the text as much as possible while retaining all relevant semantic information, we first replace URLs with the word “url” and “@” user mentions with “username” by using regular expressions. We also use the Ekphrasis tool (Baziotis et al., 2017) to split hashtags into sequences of words, when possible. The system obtained on the test set a microaveraged F1 of 0.823. We then run the classifier on all messages extracted for the Manchester network, and make the output available through the platform interface. //sigmajs.org). The platform can be used with two settings: in the first one (Figure 3), the Manchester network is displayed, with colors denoting different sub-communities characterised by dense connections. By clicking on a node, the platform displays the cloud of key-concepts automatically extracted from the conversations betwee"
W19-3511,W18-5107,1,0.762502,"Missing"
W19-3511,N16-2013,0,0.0865676,"Missing"
