C14-1163,W02-0606,0,0.0485938,"ell as word sense disambiguation to determine the meaning of ambiguous words in context. Other related work comes from two areas: unsupervised morphology induction and semantic clustering. Unsupervised morphology induction is concerned with the automatic identification of morphological relations (cf. Hammarstr¨om and Borin (2011) for an overview). Most approaches in this area do not differentiate between the inflectional and derivational level of morphology (Gaussier (1999) is an exception) and restrict themselves to the string level. Only a small number of studies (Schone and Jurafsky, 2000; Baroni et al., 2002) take distributional information into account. Semantic clustering is the task of inducing semantic classes from (broadly speaking) distributional information (Turney and Pantel, 2010; im Walde, 2006). Boleda et al. (2012) include derivational properties in their feature set to learn Catalan adjective classes. However, the input to such studies is almost always a set of words from the same part of speech with no prior morphological constraints, while our input lemmas are morphologically preselected (via derivational rules), are often extremely infrequent, and exhibit systematical variation in"
C14-1163,C10-1011,0,0.0137879,"Evert, 2005). The semantic similarity is measured by the cosine similarity between the vectors. Despite the size of the corpus, many lemmas from DE RIV BASE occur very infrequently, and due to the inflection in German, it is important to retrieve as many occurrences of each lemma as possible. 1731 We therefore use a very permissive two-step lemmatization scheme that starts from lemmas from the lexicon-based TreeTagger (Schmid, 1994), which provides reliable lemmas but with relatively low coverage, and supplements them with lemmas and parts of speech produced by the probabilistic MATE toolkit (Bohnet, 2010) when TreeTagger abstains. 3.2 Frequency Considerations The advantage of the string transformation-based construction of DE RIV BASE is its ability to include infrequent lemmas in the lexicon, and in fact DE RIV BASE includes more than 250,000 content lemmas, some of which occur not more than three times in SDeWaC. However, this is a potential problem when we build distributional representations for all lemmas in DE RIV BASE since it is known from the literature that similarity predictions for infrequent lemmas are often unreliable (Bullinaria and Levy, 2007). Our data conform to expectations"
C14-1163,J12-3005,0,0.0267642,"is concerned with the automatic identification of morphological relations (cf. Hammarstr¨om and Borin (2011) for an overview). Most approaches in this area do not differentiate between the inflectional and derivational level of morphology (Gaussier (1999) is an exception) and restrict themselves to the string level. Only a small number of studies (Schone and Jurafsky, 2000; Baroni et al., 2002) take distributional information into account. Semantic clustering is the task of inducing semantic classes from (broadly speaking) distributional information (Turney and Pantel, 2010; im Walde, 2006). Boleda et al. (2012) include derivational properties in their feature set to learn Catalan adjective classes. However, the input to such studies is almost always a set of words from the same part of speech with no prior morphological constraints, while our input lemmas are morphologically preselected (via derivational rules), are often extremely infrequent, and exhibit systematical variation in parts of speech. To our knowledge, this challenging situation has not been addressed in previous studies. Recent work has also considered the opposite problem, namely using derivational morphology for improving distributio"
C14-1163,1993.eamt-1.1,0,0.384086,"Missing"
C14-1163,W99-0904,0,0.103679,"ch is a very productive process of word formation in Slavic languages but also in languages more closely related to English, like German ˇ (Stekauer and Lieber, 2005). Derivation comprises a large number of distinct patterns, many of which cross part of speech boundaries (nominalization, verbalization, adjectivization), but some of which do not (gender indicators like master / mistress, approximations like red / reddish). A simple way to conceptualize derivation is that it partitions a language’s vocabulary into derivational families of derivationally related lemmas (cf. Zeller et al. (2013), Gaussier (1999)). In WordNet, this type of information has been included to some extent by so-called “morpho-semantic” relations (Fellbaum et al., 2009), and the approach has been applied to languages other This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1728 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1728–1739, Dublin, Ireland, August 23-29 2014. lachen to laugh V sfx ‘er’ N"
C14-1163,N03-1013,0,0.101177,"Missing"
C14-1163,J06-2001,0,0.0368891,"logy induction is concerned with the automatic identification of morphological relations (cf. Hammarstr¨om and Borin (2011) for an overview). Most approaches in this area do not differentiate between the inflectional and derivational level of morphology (Gaussier (1999) is an exception) and restrict themselves to the string level. Only a small number of studies (Schone and Jurafsky, 2000; Baroni et al., 2002) take distributional information into account. Semantic clustering is the task of inducing semantic classes from (broadly speaking) distributional information (Turney and Pantel, 2010; im Walde, 2006). Boleda et al. (2012) include derivational properties in their feature set to learn Catalan adjective classes. However, the input to such studies is almost always a set of words from the same part of speech with no prior morphological constraints, while our input lemmas are morphologically preselected (via derivational rules), are often extremely infrequent, and exhibit systematical variation in parts of speech. To our knowledge, this challenging situation has not been addressed in previous studies. Recent work has also considered the opposite problem, namely using derivational morphology for"
C14-1163,jacquemin-2010-derivational,0,0.0526293,"Missing"
C14-1163,W13-2608,0,0.0648607,"bove, the standard distributional approach of using plain cosine scores to measure the absolute amount of co-occurrences does not seem very promising, due to the low absolute numbers of shared dimensions of the two lemmas. We expect other similarity measures, e.g., the Lin measure (Lin, 1998), to perform equally poorly since they do not change the fundamental approach. Also, although using a large corpus for semantic space construction might ameliorate the situation, we would prefer to make improvements on the modeling side of semantic validation. We follow the ideas of Hare et al. (2009) and Lapesa and Evert (2013) who propose to consider semantic similarity in terms of ranks rather than absolute values. The advantage of rank-based similarity is that it takes the density of regions in the semantic space into account. That is, a low cosine value does not necessarily indicate low semantic relatedness – provided that the two words are located in a “sparse” region. Conversely, a high cosine value can be meaningless in a densely populated region. A second conceptual benefit of rank-based similarity is that it is directed: It is possible to distinguish the “forward” rank (the rank of l1 in the neighborhood of"
C14-1163,P13-1149,0,0.181468,"d-alone derivational lexicons such as CatVar (Habash and Dorr, 2003) for English, DE RIV BASE (Zeller et al., 2013) for German, or the multilingual CELEX (Baayen et al., 1996). Recent work has demonstrated that NLP can benefit from derivational knowledge. Shnarch et al. (2011) employ derivational knowledge in recognizing English textual entailment to better gauge the semantic similarity of text and hypothesis. Pad´o et al. (2013) improve the prediction of German semantic similarity judgments for lemma pairs by backing off to derivational families for infrequent lemmas. Luong et al. (2013) and Lazaridou et al. (2013) improve distributional semantic representations. Note that all of these applications make use of derivational knowledge to address various semantic tasks, working on the assumption that derivationally related words, as represented in derivational lexicons, are strongly semantically related. This assumption is not completely warranted, though. The development of wide-coverage derivational lexicons is generally driven by morphological information, using for example finite-state technology (Karttunen and Beesley, 2005) to characterize known derivational patterns in terms of string transformation"
C14-1163,W13-3512,0,0.106005,"nal information are stand-alone derivational lexicons such as CatVar (Habash and Dorr, 2003) for English, DE RIV BASE (Zeller et al., 2013) for German, or the multilingual CELEX (Baayen et al., 1996). Recent work has demonstrated that NLP can benefit from derivational knowledge. Shnarch et al. (2011) employ derivational knowledge in recognizing English textual entailment to better gauge the semantic similarity of text and hypothesis. Pad´o et al. (2013) improve the prediction of German semantic similarity judgments for lemma pairs by backing off to derivational families for infrequent lemmas. Luong et al. (2013) and Lazaridou et al. (2013) improve distributional semantic representations. Note that all of these applications make use of derivational knowledge to address various semantic tasks, working on the assumption that derivationally related words, as represented in derivational lexicons, are strongly semantically related. This assumption is not completely warranted, though. The development of wide-coverage derivational lexicons is generally driven by morphological information, using for example finite-state technology (Karttunen and Beesley, 2005) to characterize known derivational patterns in te"
C14-1163,P13-2128,1,0.86164,"Missing"
C14-1163,W07-1710,0,0.0708195,"Missing"
C14-1163,W00-0712,0,0.295011,"for a specific lemma, as well as word sense disambiguation to determine the meaning of ambiguous words in context. Other related work comes from two areas: unsupervised morphology induction and semantic clustering. Unsupervised morphology induction is concerned with the automatic identification of morphological relations (cf. Hammarstr¨om and Borin (2011) for an overview). Most approaches in this area do not differentiate between the inflectional and derivational level of morphology (Gaussier (1999) is an exception) and restrict themselves to the string level. Only a small number of studies (Schone and Jurafsky, 2000; Baroni et al., 2002) take distributional information into account. Semantic clustering is the task of inducing semantic classes from (broadly speaking) distributional information (Turney and Pantel, 2010; im Walde, 2006). Boleda et al. (2012) include derivational properties in their feature set to learn Catalan adjective classes. However, the input to such studies is almost always a set of words from the same part of speech with no prior morphological constraints, while our input lemmas are morphologically preselected (via derivational rules), are often extremely infrequent, and exhibit syst"
C14-1163,P11-2098,0,0.035295,"Missing"
C14-1163,P13-1118,1,0.905696,"Missing"
C14-1163,faass-etal-2010-design,0,\N,Missing
C14-1163,2003.mtsummit-systems.9,0,\N,Missing
C14-1163,J11-2002,0,\N,Missing
C16-1122,D10-1115,0,0.0881323,"are represented as vectors in some underlying distributional space. The simple additive model predicts the derived word from the base word as d = b + p where p is a vector representing the semantic shift accompanying the derivation pattern. The simple multiplicative model, d = b p is very similar, but uses component-wise multiplication ( ) instead of addition to combine the base and pattern vectors (Mitchell and Lapata, 2010). The third model, the weighted additive model, enables a simple reweighting of the contributions of basis and pattern (d = αb + βp). Finally, the lexical function model (Baroni and Zamparelli, 2010) represents the pattern as a matrix P that is multiplied with the basis vector: d = Pb, essentially modelling derivation as linear mapping. This model is considerably more powerful than the others, however its number of parameters is quadratic in the number of dimensions of the underlying space, whereas the additive and multiplicative models only use a linear number of parameters. For their empirical evaluation, Lazaridou et al. (2013) considered a dataset of 18 English patterns defined as simple affixes – 4 within-POS (such as un-) and 14 across-POS (such as +ment) – and found that the lexica"
C16-1122,P13-4006,0,0.0342734,"hift vector p is computed as the average of the shift vectors across all word pairs from a single pattern, while the weighted additive model additionally optimizes α and β in a subsequent step. Since the lexical function model is more prone to overfitting due to its many parameters, we train it using ridge regression, employing generalized cross-validation to tune the regularization parameter on the training set. As a fifth, baseline model, we use the identity mapping, which simply predicts the basis vector as the vector of the derived word. Our implementation is based on the DISSECT toolkit (Dinu et al., 2013). Evaluation. The performance of the CDSMs is measured by how well the predicted vector aligns with the corpus-observed vector for the derived word. More concretely, we quantify the performance on each word pair by Reciprocal rank (RR), that is, 1 divided by the position of the predicted vector in the similarity-ranked list of the observed vector’s neighbors. Besides being a well-established evaluation measure in Information Retrieval, RR is also more sensitive than the “Recall out of n” measure used previously (Kisselew et al., 2015), which measures the 0–1 loss and also requires fixing a thr"
C16-1122,D08-1094,1,0.843223,"Missing"
C16-1122,P04-1048,0,0.0480596,"(row 1) this means attaching an affix (+it¨at). The other rows show that the pattern can be more complex, involving stem alternation (row 2; note that the infinitive suffix +en is inflectional), deletion of previous affixes (row 3), circumfixation (row 4), or no overt changes, i.e., conversion (row 5).1 Derivation can take place both within parts of speech (row 6) and across parts of speech. Derivation is a very productive process in many languages, notably Slavic languages. Thus, natural language processing (NLP) for these languages can profit from knowledge about derivational relationships (Green et al., 2004; Szpektor and Dagan, 2008; Pad´o et al., 2013). Nevertheless, derivation is a relatively understudied phenomenon in NLP, and few lexicons contain derivational information. For English, there are two main resources. CatVar (Habash and Dorr, 2003) is a database that groups 100K words of all parts of speech into 60K derivational families, i.e., derivationally related sets of words. The other is CELEX (Baayen et al., 1996), a multi-level lexical database for English, German, and Dutch, which covers about 50K English words and contains derivational information in its morphological annotation. For"
C16-1122,N03-1013,0,0.056161,"ation (row 4), or no overt changes, i.e., conversion (row 5).1 Derivation can take place both within parts of speech (row 6) and across parts of speech. Derivation is a very productive process in many languages, notably Slavic languages. Thus, natural language processing (NLP) for these languages can profit from knowledge about derivational relationships (Green et al., 2004; Szpektor and Dagan, 2008; Pad´o et al., 2013). Nevertheless, derivation is a relatively understudied phenomenon in NLP, and few lexicons contain derivational information. For English, there are two main resources. CatVar (Habash and Dorr, 2003) is a database that groups 100K words of all parts of speech into 60K derivational families, i.e., derivationally related sets of words. The other is CELEX (Baayen et al., 1996), a multi-level lexical database for English, German, and Dutch, which covers about 50K English words and contains derivational information in its morphological annotation. For German, DErivBase (Zeller et al., 2013) is a resource focused on derivation that groups 280K lemmas into 17K derivational families. As opposed to CatVar and CELEX, it also provides explicit information about the applicable derivation pattern at t"
C16-1122,P14-1006,0,0.0213232,"istributional semantics, or CDSMs (Mitchell and Lapata, 2010; Erk and Pad´o, 2008; Baroni et al., 2014; Coecke et al., 2010), have established themselves as a standard tool in computational semantics. Building on traditional distributional semantic models for individual words (Turney and Pantel, 2010), they are generally applied to compositionally compute phrase meaning by defining combination operations on the meaning of the phrase’s constituents. CDSMs have also been co-opted by the deep learning community for tasks including sentiment analysis (Socher et al., 2013) and machine translation (Hermann and Blunsom, 2014). A more recent development is the use of CDSMs to model meaning-related phenomena above and below syntactic structure; here, the term “composition” is used more generally to apply to processes of meaning combination from multiple linguistic units, e.g., above and below syntactic structure. Above the sentence level, such models attempt to predict the unfolding of discourse (Kiros et al., 2015). Below the word level, CDSMs have been applied to model word formation processes like compounding (church + tower → church tower) and (morphological) derivation (Lazaridou et al., 2013) (favor + able → f"
C16-1122,W15-0108,1,0.826306,"Missing"
C16-1122,P13-1149,0,0.288633,"e translation (Hermann and Blunsom, 2014). A more recent development is the use of CDSMs to model meaning-related phenomena above and below syntactic structure; here, the term “composition” is used more generally to apply to processes of meaning combination from multiple linguistic units, e.g., above and below syntactic structure. Above the sentence level, such models attempt to predict the unfolding of discourse (Kiros et al., 2015). Below the word level, CDSMs have been applied to model word formation processes like compounding (church + tower → church tower) and (morphological) derivation (Lazaridou et al., 2013) (favor + able → favorable). More concretely, given a distributional representation of a basis and a derivation pattern (typically an affix), the task of the CDSM is to predict a distributional representation of the derived word, without being provided with any additional information. Interest in the use of CDSMs in this context comes from the observation that derived words are often less frequent than their bases (Hay, 2003), and in the extreme case even completely novel; consequently, distributional evidence is often unreliable and sometimes unavailable. This is confirmed by Luong et al. (20"
C16-1122,W13-3512,0,0.128062,"ou et al., 2013) (favor + able → favorable). More concretely, given a distributional representation of a basis and a derivation pattern (typically an affix), the task of the CDSM is to predict a distributional representation of the derived word, without being provided with any additional information. Interest in the use of CDSMs in this context comes from the observation that derived words are often less frequent than their bases (Hay, 2003), and in the extreme case even completely novel; consequently, distributional evidence is often unreliable and sometimes unavailable. This is confirmed by Luong et al. (2013) who compare the performance of different types of word embeddings on a word similarity task and achieve poorer performance on data sets containing rarer and more complex words. Due to the Zipfian distribution there are many more rare than frequent word types in a corpus, which increases the need for methods being able to model derived words. In this paper, we ask to what extent the application of CDSMs to model derivation is a success story. The record is unclear on this point: Lazaridou et al. (2013), after applying a range of CDSMs to an English derivation dataset, report success, while Kis"
C16-1122,N13-1090,0,0.0225336,"ollowing Kisselew et al. (2015), to mitigate sparsity, for out-of-vocabulary words we back off to the lemmas produced by MATE Tools (Bohnet, 2010), which have higher recall but lower precision than TreeTagger. We also use the MATE dependency analysis to reconstruct lemmas for separated prefix verbs. 3 http://goo.gl/tiRJy0 1288 Prediction Models. To obtain the vector representations on which we can train our prediction models, we use CBOW, a state-of-the-art predictive distributional semantics space which been shown particularly effective for modelling word similarity and relational knowledge (Mikolov et al., 2013).4 (Considering the type of semantic space as a parameter is outside the scope of our study.) As both target and context elements, we use all 280K unique POS-disambiguated lemmas (nouns, adjectives, and verbs) from DErivBase. We use a within-sentence context window of size ±2 to either side of the target word, 300 context dimensions, negative sampling set to 15, and no hierarchical softmax. On these vector representations, we train four prediction models (cf. Section 2): the simple additive model, the simple multiplicative model, the weighted additive model, and the lexical function model. Eac"
C16-1122,P13-2128,1,0.862888,"Missing"
C16-1122,D13-1170,0,0.00757296,"rity. 1 Introduction Compositional models of distributional semantics, or CDSMs (Mitchell and Lapata, 2010; Erk and Pad´o, 2008; Baroni et al., 2014; Coecke et al., 2010), have established themselves as a standard tool in computational semantics. Building on traditional distributional semantic models for individual words (Turney and Pantel, 2010), they are generally applied to compositionally compute phrase meaning by defining combination operations on the meaning of the phrase’s constituents. CDSMs have also been co-opted by the deep learning community for tasks including sentiment analysis (Socher et al., 2013) and machine translation (Hermann and Blunsom, 2014). A more recent development is the use of CDSMs to model meaning-related phenomena above and below syntactic structure; here, the term “composition” is used more generally to apply to processes of meaning combination from multiple linguistic units, e.g., above and below syntactic structure. Above the sentence level, such models attempt to predict the unfolding of discourse (Kiros et al., 2015). Below the word level, CDSMs have been applied to model word formation processes like compounding (church + tower → church tower) and (morphological) d"
C16-1122,N10-1091,0,0.0353174,"Missing"
C16-1122,C08-1107,0,0.0148805,"ttaching an affix (+it¨at). The other rows show that the pattern can be more complex, involving stem alternation (row 2; note that the infinitive suffix +en is inflectional), deletion of previous affixes (row 3), circumfixation (row 4), or no overt changes, i.e., conversion (row 5).1 Derivation can take place both within parts of speech (row 6) and across parts of speech. Derivation is a very productive process in many languages, notably Slavic languages. Thus, natural language processing (NLP) for these languages can profit from knowledge about derivational relationships (Green et al., 2004; Szpektor and Dagan, 2008; Pad´o et al., 2013). Nevertheless, derivation is a relatively understudied phenomenon in NLP, and few lexicons contain derivational information. For English, there are two main resources. CatVar (Habash and Dorr, 2003) is a database that groups 100K words of all parts of speech into 60K derivational families, i.e., derivationally related sets of words. The other is CELEX (Baayen et al., 1996), a multi-level lexical database for English, German, and Dutch, which covers about 50K English words and contains derivational information in its morphological annotation. For German, DErivBase (Zeller"
C16-1122,W11-1301,0,0.0173134,"ns. All numeric variables (predictors and dependent variable) are z-scaled; frequency variables are logarithmized. 1289 Baseline Simple Add Weighted Add Mult LexFun Mean Reciprocal Rank 0.271 0.309 0.316 0.272 0.150 # Predictions used by Oracle (Experiment 2) # Predictions used by Regressionbased Ensemble (Experiment 2) 2139 954 1613 532 913 51 2306 3528 190 76 Table 2: Results for individual prediction models on test set • Prediction level predictors describe properties of the vector that the CDSM outputs. Following work on assessing the plausibility of compositionally constructed vectors by Vecchi et al. (2011), we compute the length of the vector (deriv norm) and the similarity of the vector to its nearest neighbors (deriv density), and the similarity between base vector and derived vector (base deriv sim); • Pattern level predictors. We represent the identity of the pattern, which is admissible since we can assume that the DErivBase patterns cover the (vast) majority of German derivation patterns (Clark, 1973). Unfortunately, this excludes a range of other predictors, such as the parts of speech of the base and derived words, due to their perfect collinearity with the pattern predictor. The rest o"
C16-1122,P13-1118,1,0.900912,"Missing"
C16-1122,2003.mtsummit-systems.9,0,\N,Missing
C16-1122,2014.lilt-9.5,0,\N,Missing
glavas-etal-2014-hieve,S10-1062,0,\N,Missing
glavas-etal-2014-hieve,W04-1017,0,\N,Missing
glavas-etal-2014-hieve,E12-1034,0,\N,Missing
glavas-etal-2014-hieve,chambers-jurafsky-2010-database,0,\N,Missing
glavas-etal-2014-hieve,S07-1014,0,\N,Missing
glavas-etal-2014-hieve,W03-0502,0,\N,Missing
glavas-etal-2014-hieve,J05-2005,0,\N,Missing
glavas-etal-2014-hieve,S10-1010,0,\N,Missing
glavas-etal-2014-hieve,P08-1090,0,\N,Missing
glavas-etal-2014-hieve,S13-2001,0,\N,Missing
glavas-etal-2014-hieve,P09-1068,0,\N,Missing
glavas-etal-2014-hieve,W13-0119,0,\N,Missing
glavas-etal-2014-hieve,S13-2002,0,\N,Missing
glavas-etal-2014-hieve,mani-etal-2008-spatialml,0,\N,Missing
glavas-etal-2014-hieve,P11-2061,0,\N,Missing
glavas-etal-2014-hieve,kordjamshidi-etal-2010-spatial,1,\N,Missing
I17-2031,J86-3001,0,0.496988,"tomated scoring of summaries. However, they frame the problem as a classification task and predict a single holistic score, whereas we frame the problem as a regression task and predict the scores for six rubrics. 3 Reading-for-Understanding and Reading-to-Write in L2 Summarization can be perceived as a Reading-forUnderstanding (RU) task as discussed by Madnani et al. (2013) based on (Sabatini et al., 2013). In other words, summarizing includes lower- and higher-level comprehension processes leading to establishing coherence according to the most plausible intended meaning of the source text (Grosz and Sidner, 1986). Meaning is thus actively constructed by selecting and organizing the main ideas of the text into a coherent whole. When the result of comprehension processes is articulated in writing, there is a need to introduce cohesion devices which signal the rhetorical structure of the text and ensure a smooth flow of sentences. Summary writing is thus also a Reading-to-Write (RW) task (e.g., Delaney (2008)) demonstrating the ability to “convey information” as “a central component of real-world skills” (Foltz, 2016). While there is a natural overlap between RU and RW (since RW includes and largely depe"
I17-2031,P06-4018,0,0.0339902,"on, we experiment with feature selection based on the F-test for each feature, retaining all, 10, or 5 top-ranked features, yielding six different models. We use the sklearn implementation of the algorithms (Pedregosa et al., 2011). Features. Each of the six regression models is trained on the same set of features. The features can be grouped into reference-based features (BLEU, ROUGE, and “source-copying” features) inspired by (Madnani et al., 2013), and linguistic features derived from Coh-Metrix indices. For preprocessing (sentence segmentation and tokenization), we use the NLTK toolkit of Bird (2006). • BLEU (Papineni et al., 2002) is a precisionbased metric originally used for comparing machine-generated translations against reference translations. In our case, BLEU measures the n-gram overlap between the student’s summary and the source text. The rationale is that a good Setup. We evaluate the models using a nested 10×5 cross-validation, where the inner five folds 183 2 http://cohmetrix.com Acc Model Acc Cmp Rel Chr Org Chs Baseline 43.5 42.3 46.3 35.8 37.4 36.6 BLEU Rel Chr Org Chs 0.27 −0.38 −0.50 −0.51 −0.49 −0.60 Ridge-all Ridge-10 Ridge-5 42.2 54.1∗ 54.1∗ 51.6 54.1∗ 50.2 46.8 46.8"
I17-2031,N03-1020,0,0.500026,"Missing"
I17-2031,W13-1722,0,0.373745,"Missing"
I17-2031,P02-1040,0,0.0997893,"h feature selection based on the F-test for each feature, retaining all, 10, or 5 top-ranked features, yielding six different models. We use the sklearn implementation of the algorithms (Pedregosa et al., 2011). Features. Each of the six regression models is trained on the same set of features. The features can be grouped into reference-based features (BLEU, ROUGE, and “source-copying” features) inspired by (Madnani et al., 2013), and linguistic features derived from Coh-Metrix indices. For preprocessing (sentence segmentation and tokenization), we use the NLTK toolkit of Bird (2006). • BLEU (Papineni et al., 2002) is a precisionbased metric originally used for comparing machine-generated translations against reference translations. In our case, BLEU measures the n-gram overlap between the student’s summary and the source text. The rationale is that a good Setup. We evaluate the models using a nested 10×5 cross-validation, where the inner five folds 183 2 http://cohmetrix.com Acc Model Acc Cmp Rel Chr Org Chs Baseline 43.5 42.3 46.3 35.8 37.4 36.6 BLEU Rel Chr Org Chs 0.27 −0.38 −0.50 −0.51 −0.49 −0.60 Ridge-all Ridge-10 Ridge-5 42.2 54.1∗ 54.1∗ 51.6 54.1∗ 50.2 46.8 46.8 50.8∗ 42.3 47.7∗ 47.3∗ 39.3 43.2"
I17-2031,W15-0603,0,0.0775143,"le and make available a dataset of expert-rated collegelevel summaries in EL2.1 2 Related Work Automated scoring of student writing has attracted considerable attention due to the opportunity to analyze cognitive aspects of writing as well as a 1 http://takelab.fer.hr/el2-summaries 181 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 181–186, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP need to automate the time-consuming, cognitively demanding, and sometimes insufficiently reliable assessment process, e.g., (Burstein et al., 2013; Rahimi et al., 2015). Much has been done in the area of L1 and L2 essays, e.g., with Coh-Metrix (Crossley and McNamara, 2009; McNamara et al., 2010; Crossley and McNamara, 2011), and some studies have investigated automated scoring also in summaries, e.g., (Madnani et al., 2013). As assignments which demonstrate students’ reading/writing skills and their broader academic abilities, summaries have been studied as part of university-level L2 assessment; e.g., integrated task in (Guo et al., 2013). In such research, holistic scoring mostly supported by well-defined descriptors, e.g., (Rahimi et al., 2015), has predo"
karan-etal-2012-evaluation,ramisch-etal-2010-mwetoolkit,0,\N,Missing
karan-etal-2012-evaluation,J90-1003,0,\N,Missing
karan-etal-2012-evaluation,P08-2046,1,\N,Missing
karan-etal-2012-evaluation,P05-2003,0,\N,Missing
karan-etal-2012-evaluation,P10-1085,0,\N,Missing
karan-etal-2012-evaluation,P06-2084,0,\N,Missing
L16-1267,W09-2420,0,0.0723168,"Missing"
L16-1267,W15-5309,1,0.744887,"Missing"
L16-1267,D07-1007,0,0.104599,"ally, we evaluate three different baseline WSD models on both dataset variants and report on the insights gained. We make both dataset variants freely available. Keywords: word sense disambiguation, lexical sample, multi-label annotation, Croatian language 1. Introduction Word sense disambiguation (WSD), a task of computationally determining the meaning of a word in its context (Navigli, 2009), is one of the longest-standing and most crucial tasks of natural language processing (NLP). Knowing the right sense of a word can be beneficial in various NLP applications, such as machine translation (Carpuat and Wu, 2007), information retrieval (Stokoe et al., 2003), and information extraction (Ciaramita and Altun, 2006). An indispensable ingredient in the development and testing of WSD systems are sense-annotated corpora. Unfortunately, such corpora are extremely costly to produce, mainly because a sufficient number of contexts has to be manually labeled for each polysemous word. Moreover, most work on WSD has focused on English and has relied on WSD datasets built as part of the SemEval (Senseval) evaluation exercises (Navigli et al., 2007; Agirre et al., 2009; Manandhar et al., 2010; Moro and Navigli, 2015)"
L16-1267,W06-1670,0,0.162048,"insights gained. We make both dataset variants freely available. Keywords: word sense disambiguation, lexical sample, multi-label annotation, Croatian language 1. Introduction Word sense disambiguation (WSD), a task of computationally determining the meaning of a word in its context (Navigli, 2009), is one of the longest-standing and most crucial tasks of natural language processing (NLP). Knowing the right sense of a word can be beneficial in various NLP applications, such as machine translation (Carpuat and Wu, 2007), information retrieval (Stokoe et al., 2003), and information extraction (Ciaramita and Altun, 2006). An indispensable ingredient in the development and testing of WSD systems are sense-annotated corpora. Unfortunately, such corpora are extremely costly to produce, mainly because a sufficient number of contexts has to be manually labeled for each polysemous word. Moreover, most work on WSD has focused on English and has relied on WSD datasets built as part of the SemEval (Senseval) evaluation exercises (Navigli et al., 2007; Agirre et al., 2009; Manandhar et al., 2010; Moro and Navigli, 2015). Consequently, WSD datasets for languages other than English are comparably rare. In this paper, we"
L16-1267,S01-1001,0,0.0802418,"xtracted all the polysemous words from the MRD, excluded ones with a frequency lower than 1000, and finally hand-picked 36 words: 12 adjectives, 12 nouns, and 12 verbs. To some extent, we have tried to keep the set of words balanced with respect to both their frequencies and levels of polysemy. For each of the 36 words, we sampled 300 sentences from hrWaC, which amounts to 10,800 word instances (words and their contexts). Note that for each word in the dataset we meet the criterion for the recommended number of instances per word, namely 75 + 15 · n, where n denotes the number of word senses (Edmonds and Cotton, 2001). 2.2. Annotation Task The annotation task was set up rather straightforwardly: each instance to be annotated comprised a sentence containing a target polysemous word, a list of its senses (along with their glosses and usage examples), and an additional “none of the above” (NOTA) option. The annotators were instructed to select all the senses (i.e., multi-label annotation) they deem appropriate for the given word, considering its context. They were also instructed to select the NOTA option in case of an invalid instance, which could occur because of the incorrect lemmatization or spelling erro"
L16-1267,D09-1046,0,0.0260326,", 2007; Agirre et al., 2009; Manandhar et al., 2010; Moro and Navigli, 2015). Consequently, WSD datasets for languages other than English are comparably rare. In this paper, we present Cro36WSD – a medium-sized lexical sample dataset for Croatian WSD, which extends our ˇ Cro6WSD dataset introduced in (Alagi´c and Snajder, 2015). Extensions include a larger number of words in the lexical sample, adoption of the multi-label annotation scheme, and the sense inventory more apt to the task. The latter two extensions are motivated by the often-discussed inadequateness of discrete sense inventories (Erk and McCarthy, 2009) and their granularities (Edmonds and Kilgarriff, 2002; Snyder and Palmer, 2004). We construct two different variants of the dataset according to different label aggregation schemes. We also carry out a correlation analysis of the collected sense-annotated data. Lastly, we evaluate three baseline WSD models and report on the insights gained. We make both dataset variants publicly available,1 in the hope that it 1 http://takelab.fer.hr/cro36wsd will facilitate further research in computational semantics for Croatian language. The rest of the paper is organized as follows. In Section 2, we descr"
L16-1267,C12-2053,0,0.0723226,"Missing"
L16-1267,W14-0405,0,0.0310478,"Missing"
L16-1267,S10-1011,0,0.0352311,"h as machine translation (Carpuat and Wu, 2007), information retrieval (Stokoe et al., 2003), and information extraction (Ciaramita and Altun, 2006). An indispensable ingredient in the development and testing of WSD systems are sense-annotated corpora. Unfortunately, such corpora are extremely costly to produce, mainly because a sufficient number of contexts has to be manually labeled for each polysemous word. Moreover, most work on WSD has focused on English and has relied on WSD datasets built as part of the SemEval (Senseval) evaluation exercises (Navigli et al., 2007; Agirre et al., 2009; Manandhar et al., 2010; Moro and Navigli, 2015). Consequently, WSD datasets for languages other than English are comparably rare. In this paper, we present Cro36WSD – a medium-sized lexical sample dataset for Croatian WSD, which extends our ˇ Cro6WSD dataset introduced in (Alagi´c and Snajder, 2015). Extensions include a larger number of words in the lexical sample, adoption of the multi-label annotation scheme, and the sense inventory more apt to the task. The latter two extensions are motivated by the often-discussed inadequateness of discrete sense inventories (Erk and McCarthy, 2009) and their granularities (Ed"
L16-1267,S15-2049,0,0.0176659,"(Carpuat and Wu, 2007), information retrieval (Stokoe et al., 2003), and information extraction (Ciaramita and Altun, 2006). An indispensable ingredient in the development and testing of WSD systems are sense-annotated corpora. Unfortunately, such corpora are extremely costly to produce, mainly because a sufficient number of contexts has to be manually labeled for each polysemous word. Moreover, most work on WSD has focused on English and has relied on WSD datasets built as part of the SemEval (Senseval) evaluation exercises (Navigli et al., 2007; Agirre et al., 2009; Manandhar et al., 2010; Moro and Navigli, 2015). Consequently, WSD datasets for languages other than English are comparably rare. In this paper, we present Cro36WSD – a medium-sized lexical sample dataset for Croatian WSD, which extends our ˇ Cro6WSD dataset introduced in (Alagi´c and Snajder, 2015). Extensions include a larger number of words in the lexical sample, adoption of the multi-label annotation scheme, and the sense inventory more apt to the task. The latter two extensions are motivated by the often-discussed inadequateness of discrete sense inventories (Erk and McCarthy, 2009) and their granularities (Edmonds and Kilgarriff, 200"
L16-1267,S07-1006,0,0.0587399,"Missing"
L16-1267,W04-0811,0,0.0470635,"onsequently, WSD datasets for languages other than English are comparably rare. In this paper, we present Cro36WSD – a medium-sized lexical sample dataset for Croatian WSD, which extends our ˇ Cro6WSD dataset introduced in (Alagi´c and Snajder, 2015). Extensions include a larger number of words in the lexical sample, adoption of the multi-label annotation scheme, and the sense inventory more apt to the task. The latter two extensions are motivated by the often-discussed inadequateness of discrete sense inventories (Erk and McCarthy, 2009) and their granularities (Edmonds and Kilgarriff, 2002; Snyder and Palmer, 2004). We construct two different variants of the dataset according to different label aggregation schemes. We also carry out a correlation analysis of the collected sense-annotated data. Lastly, we evaluate three baseline WSD models and report on the insights gained. We make both dataset variants publicly available,1 in the hope that it 1 http://takelab.fer.hr/cro36wsd will facilitate further research in computational semantics for Croatian language. The rest of the paper is organized as follows. In Section 2, we describe the manual construction of the sense-annotated dataset for Croatian, while i"
L16-1425,W13-2408,0,0.0489378,"Missing"
L16-1425,P98-1013,0,0.205203,"Missing"
L16-1425,W04-3205,0,0.148356,"tant role in many natural language processing (NLP) tasks. In particular, for applications that involve processing of events and states, resources that model verb semantics are of great importance. A number of such resources have been developed, most notably verb lexica FrameNet (Baker et al., 1998) and VerbNet (KipperSchuler, 2005), which model the predicate-argument relations. More recently, the interest in textual entailment (Dagan et al., 2013) has motivated the construction of largescale verb entailment resources (Pekar, 2006; Szpektor and Dagan, 2008; Hashimoto et al., 2009). VerbOcean (Chklovski and Pantel, 2004) is a broadcoverage repository of fine-grained semantic relations between English verbs. VerbOcean models five semantic relations between verbs: similarity, strength, antonymy, enablement, and happens-before. The resource has been acquired semi-automatically from the web using lexicosyntactic patterns, and covers about 3,500 verbs and 30,000 semantic relations. VerbOcean has been used for many NLP tasks, including event extraction (Mani et al., 2006), paraphrase detection, entailment recognition (Dagan et al., 2010; Mehdad et al., 2011) and contradiction detection (De Marneffe et al., 2008). I"
L16-1425,P08-1118,0,0.0877337,"Missing"
L16-1425,D09-1122,0,0.0462419,"Missing"
L16-1425,P06-1095,0,0.01202,"ted the construction of largescale verb entailment resources (Pekar, 2006; Szpektor and Dagan, 2008; Hashimoto et al., 2009). VerbOcean (Chklovski and Pantel, 2004) is a broadcoverage repository of fine-grained semantic relations between English verbs. VerbOcean models five semantic relations between verbs: similarity, strength, antonymy, enablement, and happens-before. The resource has been acquired semi-automatically from the web using lexicosyntactic patterns, and covers about 3,500 verbs and 30,000 semantic relations. VerbOcean has been used for many NLP tasks, including event extraction (Mani et al., 2006), paraphrase detection, entailment recognition (Dagan et al., 2010; Mehdad et al., 2011) and contradiction detection (De Marneffe et al., 2008). In this paper we apply the VerbOcean methodology to extract a broad-coverage repository of semantic relations between Croatian verbs. We essentially adopt the procedure of Chklovski and Pantel (2004) and evaluate the quality of the resulting resource on a manually annotated sample of semantic verb relations. Based on our insights, we create V ERB CRO CEAN, a freely available repository of finegrained semantic verb relations for Croatian. We make avail"
L16-1425,P11-1134,0,0.0214546,"d Dagan, 2008; Hashimoto et al., 2009). VerbOcean (Chklovski and Pantel, 2004) is a broadcoverage repository of fine-grained semantic relations between English verbs. VerbOcean models five semantic relations between verbs: similarity, strength, antonymy, enablement, and happens-before. The resource has been acquired semi-automatically from the web using lexicosyntactic patterns, and covers about 3,500 verbs and 30,000 semantic relations. VerbOcean has been used for many NLP tasks, including event extraction (Mani et al., 2006), paraphrase detection, entailment recognition (Dagan et al., 2010; Mehdad et al., 2011) and contradiction detection (De Marneffe et al., 2008). In this paper we apply the VerbOcean methodology to extract a broad-coverage repository of semantic relations between Croatian verbs. We essentially adopt the procedure of Chklovski and Pantel (2004) and evaluate the quality of the resulting resource on a manually annotated sample of semantic verb relations. Based on our insights, we create V ERB CRO CEAN, a freely available repository of finegrained semantic verb relations for Croatian. We make available two versions of the resource: a coverage-oriented version containing about 36k rela"
L16-1425,N06-1007,0,0.031883,"n 2. Lexico-semantic resources such as WordNet (Miller, 1995) play an important role in many natural language processing (NLP) tasks. In particular, for applications that involve processing of events and states, resources that model verb semantics are of great importance. A number of such resources have been developed, most notably verb lexica FrameNet (Baker et al., 1998) and VerbNet (KipperSchuler, 2005), which model the predicate-argument relations. More recently, the interest in textual entailment (Dagan et al., 2013) has motivated the construction of largescale verb entailment resources (Pekar, 2006; Szpektor and Dagan, 2008; Hashimoto et al., 2009). VerbOcean (Chklovski and Pantel, 2004) is a broadcoverage repository of fine-grained semantic relations between English verbs. VerbOcean models five semantic relations between verbs: similarity, strength, antonymy, enablement, and happens-before. The resource has been acquired semi-automatically from the web using lexicosyntactic patterns, and covers about 3,500 verbs and 30,000 semantic relations. VerbOcean has been used for many NLP tasks, including event extraction (Mani et al., 2006), paraphrase detection, entailment recognition (Dagan e"
L16-1425,H05-1077,0,0.0884984,"Missing"
L16-1425,P13-2137,1,0.757029,"Missing"
L16-1425,W14-0136,0,0.0684524,"Missing"
L16-1425,C08-1107,0,0.030359,"emantic resources such as WordNet (Miller, 1995) play an important role in many natural language processing (NLP) tasks. In particular, for applications that involve processing of events and states, resources that model verb semantics are of great importance. A number of such resources have been developed, most notably verb lexica FrameNet (Baker et al., 1998) and VerbNet (KipperSchuler, 2005), which model the predicate-argument relations. More recently, the interest in textual entailment (Dagan et al., 2013) has motivated the construction of largescale verb entailment resources (Pekar, 2006; Szpektor and Dagan, 2008; Hashimoto et al., 2009). VerbOcean (Chklovski and Pantel, 2004) is a broadcoverage repository of fine-grained semantic relations between English verbs. VerbOcean models five semantic relations between verbs: similarity, strength, antonymy, enablement, and happens-before. The resource has been acquired semi-automatically from the web using lexicosyntactic patterns, and covers about 3,500 verbs and 30,000 semantic relations. VerbOcean has been used for many NLP tasks, including event extraction (Mani et al., 2006), paraphrase detection, entailment recognition (Dagan et al., 2010; Mehdad et al."
L16-1481,W13-2408,0,0.0645026,"Missing"
L16-1481,S07-1002,0,0.0361347,"s (nouns only) using the similarities between their neighbors. The HyperLex (V´eronis, 2004) algorithm exploits the small world property of co-occurrence graphs and extracts the hubs corresponding to word senses. Agirre et al. (2006) optimize the parameters of the HyperLex algorithm and compare it to a modified version of PageRank. Di Marco and Navigli (2013) study WSI in the context of web search result clustering. The evaluation of WSD and WSI systems has been given a great deal of attention in the literature. Most evaluation methods and resources stem from the SemEval (Senseval) workshops (Agirre and Soroa, 2007; Manandhar and Klapaftis, 2009). WSI systems are typically evaluated on the task of unsupervised WSD, either sense labeling or sense discrimination. In both cases, a sense inventory and a corresponding sense-annotated corpus are required. Unlike previous work, in this paper we focus on an intrinsic evaluation of our WSI system, which does not require a senseannotated corpus. Additionally, we evaluate our system on the task of unsupervised WSD. Our work focuses on WSI for Croatian, which has not yet been addressed in the literature. The existing work for Croatian focuses exclusively on the WSD"
L16-1481,W06-1669,0,0.0154638,"Missing"
L16-1481,W15-5309,1,0.893061,"Missing"
L16-1481,L16-1267,1,0.88638,"Missing"
L16-1481,W06-3812,0,0.0720532,"Missing"
L16-1481,J13-3008,0,0.0486127,"Missing"
L16-1481,E03-1020,0,0.117531,"Missing"
L16-1481,W09-2419,0,0.0244448,"similarities between their neighbors. The HyperLex (V´eronis, 2004) algorithm exploits the small world property of co-occurrence graphs and extracts the hubs corresponding to word senses. Agirre et al. (2006) optimize the parameters of the HyperLex algorithm and compare it to a modified version of PageRank. Di Marco and Navigli (2013) study WSI in the context of web search result clustering. The evaluation of WSD and WSI systems has been given a great deal of attention in the literature. Most evaluation methods and resources stem from the SemEval (Senseval) workshops (Agirre and Soroa, 2007; Manandhar and Klapaftis, 2009). WSI systems are typically evaluated on the task of unsupervised WSD, either sense labeling or sense discrimination. In both cases, a sense inventory and a corresponding sense-annotated corpus are required. Unlike previous work, in this paper we focus on an intrinsic evaluation of our WSI system, which does not require a senseannotated corpus. Additionally, we evaluate our system on the task of unsupervised WSD. Our work focuses on WSI for Croatian, which has not yet been addressed in the literature. The existing work for Croatian focuses exclusively on the WSD task. Bakari´c et al. (2007) an"
L16-1481,J98-1004,0,0.602124,"Missing"
L16-1481,N03-1032,0,0.179921,"Missing"
L16-1481,C02-1114,0,0.0489388,". To the best of our knowledge, this is the first work on WSI for Croatian, and also the first that makes such resources freely available. The rest of the paper is structured as follows. In the next section, we give a brief overview of the related work. In Section 3 we describe the co-occurrence graphs and the graph-based clustering. In Section 4 we describe the manual annotation of a gold standard dataset for the intrinsic WSI evaluation. Section 5 presents the evaluation results. Section 5 concludes the paper. 2. Related Work Studies on graph co-occurrence based WSI date back to the work of Widdows and Dorow (2002), whose algorithm clustered the nodes (nouns only) using the similarities between their neighbors. The HyperLex (V´eronis, 2004) algorithm exploits the small world property of co-occurrence graphs and extracts the hubs corresponding to word senses. Agirre et al. (2006) optimize the parameters of the HyperLex algorithm and compare it to a modified version of PageRank. Di Marco and Navigli (2013) study WSI in the context of web search result clustering. The evaluation of WSD and WSI systems has been given a great deal of attention in the literature. Most evaluation methods and resources stem fro"
N18-2033,W10-2805,0,0.0308562,"the phrase constituents (Baroni et al., 2014). The most basic CDSMs represent words as vectors and compose phrase vectors by component-wise operations of the constituent vectors (Mitchell and Lapata, 2008). More complex models represent predicates with matrices and tensors (Baroni and Zamparelli, 2010; Grefenstette, 2013; Paperno et al., 2014). Given the large number of different CDSMs proposed in the literature (Erk, 2012), meaningful evaluation becomes crucial. The dominant evaluation method, adopted by the majority of CDSM studies, is pairwise phrase similarity (Mitchell and Lapata, 2008; Guevara, 2010; Grefenstette et al., 2012; Grefenstette, 2013; Paperno et al., 2014). Only a handful of studies pursued other evaluation tasks, such as textual entailment (Marelli et al., 2014a,b) or sentiment analysis (Socher et al., 2013). Arguably, phrase similarity evaluation has three major problems. First, the task is affected by the general limitations of rating scales, such as inconsistencies in annotations, scale region bias, and fixed granularity (Schuman and Presser, 1996). Phrase similarity datasets used for CDSM evaluation demonstrate slight to fair inter-annotator agreement, as well as overlap"
N18-2033,S15-1017,1,0.890939,"Missing"
N18-2033,W13-3513,0,0.0213643,"(Mitchell and Lapata, 2008). Secondly, phrase similarity is a task that is rather difficult to put down precisely, especially for long phrases. Generally, phrases can be (dis)similar in any number of ways. Annotators commonly agree that some sentence pairs are semantically highly similar (private company files annual account and private company registers annual account, Pickering and Frisson 2001), and others are semantically unrelated (man waves hand vs. employee leaves company). In contrast, their assessments become less confident for cases like delegate buys land and agent sells property (Kartsaklis et al., 2013), where there is a semantic relation other than synonymy. Similarity is also arguably not a useful measure when sentences are semantically deviant, as it is often the case in the datasets: how similar are private company files annual account and private company smooths annual account? The third problem is that the most widely used phrase similarity datasets are constructed in a balanced fashion along psycholinguistic principles. For instance, the adjective-noun-verb-adjectivenoun (“ANVAN”) dataset (Pickering and Frisson, 2001; Kartsaklis et al., 2013), from which the examples above are drawn,"
N18-2033,E14-1057,1,0.805026,"Missing"
N18-2033,S14-2001,0,0.0254595,"vectors (Mitchell and Lapata, 2008). More complex models represent predicates with matrices and tensors (Baroni and Zamparelli, 2010; Grefenstette, 2013; Paperno et al., 2014). Given the large number of different CDSMs proposed in the literature (Erk, 2012), meaningful evaluation becomes crucial. The dominant evaluation method, adopted by the majority of CDSM studies, is pairwise phrase similarity (Mitchell and Lapata, 2008; Guevara, 2010; Grefenstette et al., 2012; Grefenstette, 2013; Paperno et al., 2014). Only a handful of studies pursued other evaluation tasks, such as textual entailment (Marelli et al., 2014a,b) or sentiment analysis (Socher et al., 2013). Arguably, phrase similarity evaluation has three major problems. First, the task is affected by the general limitations of rating scales, such as inconsistencies in annotations, scale region bias, and fixed granularity (Schuman and Presser, 1996). Phrase similarity datasets used for CDSM evaluation demonstrate slight to fair inter-annotator agreement, as well as overlap between groups of items rated as low and high in similarity (Mitchell and Lapata, 2008). Secondly, phrase similarity is a task that is rather difficult to put down precisely, es"
N18-2033,marelli-etal-2014-sick,0,0.0236963,"vectors (Mitchell and Lapata, 2008). More complex models represent predicates with matrices and tensors (Baroni and Zamparelli, 2010; Grefenstette, 2013; Paperno et al., 2014). Given the large number of different CDSMs proposed in the literature (Erk, 2012), meaningful evaluation becomes crucial. The dominant evaluation method, adopted by the majority of CDSM studies, is pairwise phrase similarity (Mitchell and Lapata, 2008; Guevara, 2010; Grefenstette et al., 2012; Grefenstette, 2013; Paperno et al., 2014). Only a handful of studies pursued other evaluation tasks, such as textual entailment (Marelli et al., 2014a,b) or sentiment analysis (Socher et al., 2013). Arguably, phrase similarity evaluation has three major problems. First, the task is affected by the general limitations of rating scales, such as inconsistencies in annotations, scale region bias, and fixed granularity (Schuman and Presser, 1996). Phrase similarity datasets used for CDSM evaluation demonstrate slight to fair inter-annotator agreement, as well as overlap between groups of items rated as low and high in similarity (Mitchell and Lapata, 2008). Secondly, phrase similarity is a task that is rather difficult to put down precisely, es"
N18-2033,S17-1014,1,0.777868,"Missing"
N18-2033,K16-1006,0,0.0302634,"odel, PLFGupta . All CDSMs rank the four candidates for each target (cf. Table 1) by comparing the vector for the original sentence against four sentences in which the target is replaced by the two correct and two incorrect substitutes. We use the raw dot product as similarity measure, following Roller and Erk (2016), to boost frequent candidates. CDSMs. We consider two component-wise CDSMs: the simple additive and multiplicative models (Mitchell and Lapata, 2008), defined as 208 Lexical Substitution Model. As competitor, we consider a dedicated lexical substitution model, namely context2vec (Melamud et al., 2016). Since it has demonstrated state-of-the-art performance on lexical substitution and word sense disambiguation tasks, it is a suitable competitor model for CDSMs on a similar problem. Context2vec uses word embeddings to compute a set of viable substitutes given a context, using a bidirectional LSTM recurrent neural network to build a sentential context representation. We work with two instantiations: first, using only ANVAN as context (C2VANVAN ), and second, using the full CoInCo sentence from which the ANVAN was extracted (C2VSent ). The first model is directly comparable to the CDSMs in tha"
N18-2033,P08-1028,0,0.23862,"Englishlanguage corpus with manual “all-words” lexical substitution annotation. Our experiments indicate that the Practical Lexical Function CDSM outperforms simple component-wise CDSMs and performs on par with the context2vec lexical substitution model using the same context. 1 Introduction Compositional Distributional Semantics Models (CDSMs) compute phrase meaning in semantic space as a function of the meanings of the phrase constituents (Baroni et al., 2014). The most basic CDSMs represent words as vectors and compose phrase vectors by component-wise operations of the constituent vectors (Mitchell and Lapata, 2008). More complex models represent predicates with matrices and tensors (Baroni and Zamparelli, 2010; Grefenstette, 2013; Paperno et al., 2014). Given the large number of different CDSMs proposed in the literature (Erk, 2012), meaningful evaluation becomes crucial. The dominant evaluation method, adopted by the majority of CDSM studies, is pairwise phrase similarity (Mitchell and Lapata, 2008; Guevara, 2010; Grefenstette et al., 2012; Grefenstette, 2013; Paperno et al., 2014). Only a handful of studies pursued other evaluation tasks, such as textual entailment (Marelli et al., 2014a,b) or sentime"
N18-2033,P14-1009,0,0.0732062,"outperforms simple component-wise CDSMs and performs on par with the context2vec lexical substitution model using the same context. 1 Introduction Compositional Distributional Semantics Models (CDSMs) compute phrase meaning in semantic space as a function of the meanings of the phrase constituents (Baroni et al., 2014). The most basic CDSMs represent words as vectors and compose phrase vectors by component-wise operations of the constituent vectors (Mitchell and Lapata, 2008). More complex models represent predicates with matrices and tensors (Baroni and Zamparelli, 2010; Grefenstette, 2013; Paperno et al., 2014). Given the large number of different CDSMs proposed in the literature (Erk, 2012), meaningful evaluation becomes crucial. The dominant evaluation method, adopted by the majority of CDSM studies, is pairwise phrase similarity (Mitchell and Lapata, 2008; Guevara, 2010; Grefenstette et al., 2012; Grefenstette, 2013; Paperno et al., 2014). Only a handful of studies pursued other evaluation tasks, such as textual entailment (Marelli et al., 2014a,b) or sentiment analysis (Socher et al., 2013). Arguably, phrase similarity evaluation has three major problems. First, the task is affected by the gener"
N18-2033,N16-1131,0,0.0236259,"matrices are learned with ridge regression from unigram and bigram vectors. Gupta et al. (2015) pointed out that the standard PLF “overcounts” the predicate by adding it explicitly, and proposed a rectified variant which simply → − leaves out the function word vector V . We also experiment with this model, PLFGupta . All CDSMs rank the four candidates for each target (cf. Table 1) by comparing the vector for the original sentence against four sentences in which the target is replaced by the two correct and two incorrect substitutes. We use the raw dot product as similarity measure, following Roller and Erk (2016), to boost frequent candidates. CDSMs. We consider two component-wise CDSMs: the simple additive and multiplicative models (Mitchell and Lapata, 2008), defined as 208 Lexical Substitution Model. As competitor, we consider a dedicated lexical substitution model, namely context2vec (Melamud et al., 2016). Since it has demonstrated state-of-the-art performance on lexical substitution and word sense disambiguation tasks, it is a suitable competitor model for CDSMs on a similar problem. Context2vec uses word embeddings to compute a set of viable substitutes given a context, using a bidirectional LS"
N18-2033,D13-1170,0,0.00937345,"x models represent predicates with matrices and tensors (Baroni and Zamparelli, 2010; Grefenstette, 2013; Paperno et al., 2014). Given the large number of different CDSMs proposed in the literature (Erk, 2012), meaningful evaluation becomes crucial. The dominant evaluation method, adopted by the majority of CDSM studies, is pairwise phrase similarity (Mitchell and Lapata, 2008; Guevara, 2010; Grefenstette et al., 2012; Grefenstette, 2013; Paperno et al., 2014). Only a handful of studies pursued other evaluation tasks, such as textual entailment (Marelli et al., 2014a,b) or sentiment analysis (Socher et al., 2013). Arguably, phrase similarity evaluation has three major problems. First, the task is affected by the general limitations of rating scales, such as inconsistencies in annotations, scale region bias, and fixed granularity (Schuman and Presser, 1996). Phrase similarity datasets used for CDSM evaluation demonstrate slight to fair inter-annotator agreement, as well as overlap between groups of items rated as low and high in similarity (Mitchell and Lapata, 2008). Secondly, phrase similarity is a task that is rather difficult to put down precisely, especially for long phrases. Generally, phrases ca"
P08-2046,J90-1003,0,0.230016,"Missing"
P08-2046,P06-2084,0,0.13006,"rrespond to some conventional way of saying things (Manning and Sch¨utze, 1999). Related to the term collocation is the term n-gram, which is used to denote any sequence of n words. There are many possible applications of collocations: automatic language generation, word sense disambiguation, improving text categorization, information retrieval, etc. As different applications require different types of collocations that are often not found in dictionaries, automatic extraction of collocations from large textual corpora has been the focus of much research in the last decade; see, for example, (Pecina and Schlesinger, 2006; Evert and Krenn, 2005). Automatic extraction of collocations is usually performed by employing lexical association measures (AMs) to indicate how strongly the words comprising an n-gram are associated. However, the use of lexical AMs for the purpose of collocation extraction has reached a plateau; recent research in this field has focused on combining the existing AMs in the hope of improving the results (Pecina and Schlesinger, 2006). In this paper, we propose an approach for deriving new AMs for collocation extraction based on genetic programming. A similar approach has been usefully appli"
P13-1118,J12-1003,0,0.0227636,"understood as groups of derivationally related lemmas (Daille et al., 2002; Milin et al., 2009). The lemmas in CatVar come from various open word classes, and multiple words may be listed for the same POS. The above family lists two nouns: an event noun (asking) and an agentive noun (asker). However, CatVar does not consider prefixation, which is why, e.g., the adjective unasked is missing. CatVar has found application in different areas of English NLP. Examples are the acquisition of paraphrases that cut across POS lines, applied, for example, in textual entailment (Szpektor and Dagan, 2008; Berant et al., 2012). Then there is the induction and extension of semantic roles resources for predicates of various parts of speech (Meyers et al., 2004; Green et al., 2004). Finally, CatVar has 1201 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1201–1211, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics been used as a lexical resource to generate sentence intersections (Thadani and McKeown, 2011). In this paper, we describe the project of obtaining derivational knowledge for German to enable similar applications. Even though the"
P13-1118,W99-0904,0,0.272967,"rules starting from seed examples (Piasecki et al., 2012). Hammarström and Borin (2011) give an extensive overview of stateof-the-art unsupervised learning of morphology. Unsupervised approaches operate at the level of word-forms and have complementary strengths and weaknesses to rule-based approaches. On the upside, they do not require linguistic knowledge; on the downside, they have a harder time distinguishing between derivation and inflection, which may result in lower precision, and are not guaranteed to yield analyses that correspond to linguistic intuition. An exception is the work by Gaussier (1999), who applies an unsupervised model to construct derivational families for French. For German, several morphological tools exist. Morphix is a classification-based analyzer and generator of German words on the inflectional level (Finkler and Neumann, 1988). SMOR (Schmid et al., 2004) employs a finite-state transducer to analyze German words at the inflectional, derivational, and compositional level, and has been used in other morphological analyzers, e.g., Morphisto (Zielinski and Simon, 2008). The site canoonet1 offers broad-coverage information about the German language including derivationa"
P13-1118,P04-1048,0,0.129323,"nd multiple words may be listed for the same POS. The above family lists two nouns: an event noun (asking) and an agentive noun (asker). However, CatVar does not consider prefixation, which is why, e.g., the adjective unasked is missing. CatVar has found application in different areas of English NLP. Examples are the acquisition of paraphrases that cut across POS lines, applied, for example, in textual entailment (Szpektor and Dagan, 2008; Berant et al., 2012). Then there is the induction and extension of semantic roles resources for predicates of various parts of speech (Meyers et al., 2004; Green et al., 2004). Finally, CatVar has 1201 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1201–1211, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics been used as a lexical resource to generate sentence intersections (Thadani and McKeown, 2011). In this paper, we describe the project of obtaining derivational knowledge for German to enable similar applications. Even though there are two derivational resources for this language, IMSL EX (Fitschen, 2004) and C ELEX (Baayen et al., 1996), both have shortcomings. The former does not"
P13-1118,C10-1011,0,0.00877358,"ional families of different size (counting 2 up to 114 lemmas), and assigned 55, 79, and 24 rules to L3, L2 and L1, respectively. 4.2 Data and Preprocessing For an accurate application of nominal derivation rules, we need a lemma list with POS and gender information. We POS-tag and lemmatize S DEWAC, a large German-language web corpus from which boilerplate paragraphs, ungrammatical sentences, and duplicate pages were removed (Faaß et al., 2010). For POS tagging and lemmatization, we use TreeTagger (Schmid, 1994) and determine grammatical gender with the morphological layer of the MATE Tools (Bohnet, 2010). We treat proper nouns like common nouns. We apply three language-specific filtering steps based on observations in Section 3.1. First, we discard non-capitalized nominal lemmas. Second, we deleted verbal lemmas not ending in verb suffixes. An additional complication in German concerns prefix verbs, because prefix is separated in tensed instances. For example, the 3rd person male singular of aufhören (to stop) is er hört auf (he stops). Since most prefixes double as prepositions, the correct lemmas can only be reconstructed by parsing. We parse the corpus using the MST parser (McDonald et al."
P13-1118,N03-1013,0,0.229523,"Missing"
P13-1118,H92-1022,0,0.176793,"Missing"
P13-1118,J96-2004,0,0.00789448,"ws us to identify the pairs that are derivationally unrelated, but compositionally related, e.g., EhemannN – EhefrauN (husband – wife). We first carried out a calibration phase in which the annotators double-annotated 200 pairs from each of the two samples and refined the annotation guidelines. In a subsequent validation phase, we computed inter-annotator agreements on the annotations of another 200 pairs each from the P- and the R-samples. Table 4 shows the proportion of identical annotations by both annotators as well as Cohen’s κ score (Cohen, 1968). We achieve substantial agreement for κ (Carletta, 1996). On the P-sample, κ is a little lower because the distribution of the categories is skewed towards R, which makes an agreement by chance more probable. In our opinion, the IAA results were sufficiently high to switch to single annotation for the production phase. Here, each annotator annotated another 1000 pairs from the P-sample and R-sample so that the final test set consists of 2000 pairs from each sample. The P-sample contains 1663 positive (R+M) and 337 negative (N+C+L) pairs, respectively, the R-sample contains 575 positive and 1425 negative pairs. As expected, there are more positive 6"
P13-1118,W98-1239,0,0.0300565,"tion model that is applied to German in Section 4. Sections 5 and 6 present our evaluation setup and results. Section 7 concludes the paper and outlines future work. 2 Related Work Computational models of morphology have a long tradition. Koskenniemi (1983) was the first who analyzed and generated morphological phenomena computationally. His two-level theory has been applied in finite state transducers (FST) for several languages (Karttunen and Beesley, 2005). Many recent approaches automatically induce morphological information from corpora. They are either based solely on corpus statistics (Déjean, 1998), measure semantic similarity between input and output lemma (Schone and Jurafsky, 2000), or bootstrap derivation rules starting from seed examples (Piasecki et al., 2012). Hammarström and Borin (2011) give an extensive overview of stateof-the-art unsupervised learning of morphology. Unsupervised approaches operate at the level of word-forms and have complementary strengths and weaknesses to rule-based approaches. On the upside, they do not require linguistic knowledge; on the downside, they have a harder time distinguishing between derivation and inflection, which may result in lower precisio"
P13-1118,J11-2002,0,0.0467485,"tational models of morphology have a long tradition. Koskenniemi (1983) was the first who analyzed and generated morphological phenomena computationally. His two-level theory has been applied in finite state transducers (FST) for several languages (Karttunen and Beesley, 2005). Many recent approaches automatically induce morphological information from corpora. They are either based solely on corpus statistics (Déjean, 1998), measure semantic similarity between input and output lemma (Schone and Jurafsky, 2000), or bootstrap derivation rules starting from seed examples (Piasecki et al., 2012). Hammarström and Borin (2011) give an extensive overview of stateof-the-art unsupervised learning of morphology. Unsupervised approaches operate at the level of word-forms and have complementary strengths and weaknesses to rule-based approaches. On the upside, they do not require linguistic knowledge; on the downside, they have a harder time distinguishing between derivation and inflection, which may result in lower precision, and are not guaranteed to yield analyses that correspond to linguistic intuition. An exception is the work by Gaussier (1999), who applies an unsupervised model to construct derivational families fo"
P13-1118,W06-2932,0,0.0211716,"(Bohnet, 2010). We treat proper nouns like common nouns. We apply three language-specific filtering steps based on observations in Section 3.1. First, we discard non-capitalized nominal lemmas. Second, we deleted verbal lemmas not ending in verb suffixes. An additional complication in German concerns prefix verbs, because prefix is separated in tensed instances. For example, the 3rd person male singular of aufhören (to stop) is er hört auf (he stops). Since most prefixes double as prepositions, the correct lemmas can only be reconstructed by parsing. We parse the corpus using the MST parser (McDonald et al., 2006) and recover prefix verbs by searching for instances of the dependency relation labeled P TKVZ. Since S DEWAC, as a web corpus, still contains errors, we only take into account lemmas that occur three times or more in the corpus. Considering the size of S DEWAC, we consider this as a conservative filtering step that preserves high recall and provides a comprehensive basis for evaluation. After preprocessing and filtering, we run the induction of the derivational families as explained in Section 3 to obtain the DE RIV BASE resource. 4.3 Statistics on DE RIV BASE The preparation of the S DEWAC c"
P13-1118,meyers-etal-2004-annotating,0,0.0174174,"open word classes, and multiple words may be listed for the same POS. The above family lists two nouns: an event noun (asking) and an agentive noun (asker). However, CatVar does not consider prefixation, which is why, e.g., the adjective unasked is missing. CatVar has found application in different areas of English NLP. Examples are the acquisition of paraphrases that cut across POS lines, applied, for example, in textual entailment (Szpektor and Dagan, 2008; Berant et al., 2012). Then there is the induction and extension of semantic roles resources for predicates of various parts of speech (Meyers et al., 2004; Green et al., 2004). Finally, CatVar has 1201 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1201–1211, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics been used as a lexical resource to generate sentence intersections (Thadani and McKeown, 2011). In this paper, we describe the project of obtaining derivational knowledge for German to enable similar applications. Even though there are two derivational resources for this language, IMSL EX (Fitschen, 2004) and C ELEX (Baayen et al., 1996), both have shortcomings"
P13-1118,W07-1710,0,0.196021,"Missing"
P13-1118,piasecki-etal-2012-recognition,0,0.0563221,"rk. 2 Related Work Computational models of morphology have a long tradition. Koskenniemi (1983) was the first who analyzed and generated morphological phenomena computationally. His two-level theory has been applied in finite state transducers (FST) for several languages (Karttunen and Beesley, 2005). Many recent approaches automatically induce morphological information from corpora. They are either based solely on corpus statistics (Déjean, 1998), measure semantic similarity between input and output lemma (Schone and Jurafsky, 2000), or bootstrap derivation rules starting from seed examples (Piasecki et al., 2012). Hammarström and Borin (2011) give an extensive overview of stateof-the-art unsupervised learning of morphology. Unsupervised approaches operate at the level of word-forms and have complementary strengths and weaknesses to rule-based approaches. On the upside, they do not require linguistic knowledge; on the downside, they have a harder time distinguishing between derivation and inflection, which may result in lower precision, and are not guaranteed to yield analyses that correspond to linguistic intuition. An exception is the work by Gaussier (1999), who applies an unsupervised model to cons"
P13-1118,W11-1606,0,0.0120952,"xamples are the acquisition of paraphrases that cut across POS lines, applied, for example, in textual entailment (Szpektor and Dagan, 2008; Berant et al., 2012). Then there is the induction and extension of semantic roles resources for predicates of various parts of speech (Meyers et al., 2004; Green et al., 2004). Finally, CatVar has 1201 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1201–1211, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics been used as a lexical resource to generate sentence intersections (Thadani and McKeown, 2011). In this paper, we describe the project of obtaining derivational knowledge for German to enable similar applications. Even though there are two derivational resources for this language, IMSL EX (Fitschen, 2004) and C ELEX (Baayen et al., 1996), both have shortcomings. The former does not appear to be publicly available, and the latter has a limited coverage (50k lemmas) and does not explicitly represent derivational relationships within families, which are necessary for fine-grained optimization of families. For this reason, we look into building a novel derivational resource for German. Unf"
P13-1118,schmid-etal-2004-smor,0,0.0231245,"to rule-based approaches. On the upside, they do not require linguistic knowledge; on the downside, they have a harder time distinguishing between derivation and inflection, which may result in lower precision, and are not guaranteed to yield analyses that correspond to linguistic intuition. An exception is the work by Gaussier (1999), who applies an unsupervised model to construct derivational families for French. For German, several morphological tools exist. Morphix is a classification-based analyzer and generator of German words on the inflectional level (Finkler and Neumann, 1988). SMOR (Schmid et al., 2004) employs a finite-state transducer to analyze German words at the inflectional, derivational, and compositional level, and has been used in other morphological analyzers, e.g., Morphisto (Zielinski and Simon, 2008). The site canoonet1 offers broad-coverage information about the German language including derivational word formation. 3 Framework In this section, we describe our rule-based model of derivation, its operation to define derivational families, and the application of the model to German. We note that the model is purely surface-based, i.e., it does not model any semantic regularities"
P13-1118,W00-0712,0,0.11241,"our evaluation setup and results. Section 7 concludes the paper and outlines future work. 2 Related Work Computational models of morphology have a long tradition. Koskenniemi (1983) was the first who analyzed and generated morphological phenomena computationally. His two-level theory has been applied in finite state transducers (FST) for several languages (Karttunen and Beesley, 2005). Many recent approaches automatically induce morphological information from corpora. They are either based solely on corpus statistics (Déjean, 1998), measure semantic similarity between input and output lemma (Schone and Jurafsky, 2000), or bootstrap derivation rules starting from seed examples (Piasecki et al., 2012). Hammarström and Borin (2011) give an extensive overview of stateof-the-art unsupervised learning of morphology. Unsupervised approaches operate at the level of word-forms and have complementary strengths and weaknesses to rule-based approaches. On the upside, they do not require linguistic knowledge; on the downside, they have a harder time distinguishing between derivation and inflection, which may result in lower precision, and are not guaranteed to yield analyses that correspond to linguistic intuition. An"
P13-1118,C08-1107,0,0.0204321,"nal families are commonly understood as groups of derivationally related lemmas (Daille et al., 2002; Milin et al., 2009). The lemmas in CatVar come from various open word classes, and multiple words may be listed for the same POS. The above family lists two nouns: an event noun (asking) and an agentive noun (asker). However, CatVar does not consider prefixation, which is why, e.g., the adjective unasked is missing. CatVar has found application in different areas of English NLP. Examples are the acquisition of paraphrases that cut across POS lines, applied, for example, in textual entailment (Szpektor and Dagan, 2008; Berant et al., 2012). Then there is the induction and extension of semantic roles resources for predicates of various parts of speech (Meyers et al., 2004; Green et al., 2004). Finally, CatVar has 1201 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1201–1211, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics been used as a lexical resource to generate sentence intersections (Thadani and McKeown, 2011). In this paper, we describe the project of obtaining derivational knowledge for German to enable similar applica"
P13-1118,faass-etal-2010-design,0,\N,Missing
P13-1118,2003.mtsummit-systems.9,0,\N,Missing
P13-2128,W98-0705,0,0.190944,"Missing"
P13-2128,J10-4006,0,0.157856,"ector spaces have been applied successfully to many problems in NLP (see Turney and Pantel (2010) or Erk (2012) for current overviews). Most distributional models in computational lexical semantics are either (a) bag-of-words models, where the context features are words within a surface window around the target word, or (b) syntactic models, where context features are typically pairs of dependency relations and context words. The advantage of syntactic models is that they incorporate a richer, structured notion of context. This makes them more versatile; the Distributional Memory framework by Baroni and Lenci (2010) is applicable to a wide range of tasks. It is also able – at least in principle – to capture more fine-grained types of semantic similarity such as predicateargument plausibility (Erk et al., 2010). At the same time, syntactic spaces are much more prone to sparsity problems, as their contexts are sparser. This leads to reliability and coverage problems. In this paper, we propose a novel strategy for combating sparsity in syntactic vector spaces, derivational smoothing. It follows the intuition that derivationally related words (feed – feeder, blocked – blockage) are, as a rule, semantically h"
P13-2128,D07-1060,0,0.0412258,"Missing"
P13-2128,D08-1007,0,0.0126143,"l Linguistics, pages 731–735, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics off from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves. We see two main precursor studies for our work. Bergsma et al. (2008) build models of selectional preferences that include morphological features such as capitalization and the presence of digits. However, their approach is task-specific and requires a (semi-)supervised setting. Allan and Kumaran (2003) make use of morphology by building language models for stemming-based equivalence classes. Our approach also uses morphological processing, albeit more precise than stemming. 3 A Resource for German Derivation Using derivational knowledge for smoothing raises the question of how semantically similar the lemmas within a family really are. Fortunately, DERIV BASE"
P13-2128,C10-1011,0,0.0285386,"-based smoothing triggers), but they did not yield any improvements over the simpler models we present here. 5 Experimental Evaluation Syntactic Distributional Model. The syntactic distributional model that we use represents target words by pairs of dependency relations and context words. More specifically, we use the W × LW matricization of D M .D E, the German version (Pad´o and Utt, 2012) of Distributional Memory (Baroni and Lenci, 2010). D M .D E was created on the basis of the 884M-token SD E WAC web corpus (Faaß et al., 2010), lemmatized, tagged, and parsed with the German MATE toolkit (Bohnet, 2010). Experiments. We evaluate the impact of smoothing on two standard tasks from lexical semantics. The first task is predicting semantic similarity. We lemmatized and POS-tagged the German G UR 350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analogously to the well-known Rubenstein and Goodenough (1965) dataset for English.2 We predict 2 semantic similarity as cosine similarity. We make a prediction for a word pair if both words are represented in the semantic space and their vectors have a non-zero similarity. The second task is synonym choice"
P13-2128,1993.eamt-1.1,0,0.579256,"Missing"
P13-2128,P05-1045,0,0.00756365,"and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003). In lexical semantics, smoothing is often achieved by backing 731 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 731–735, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics off from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves. We see two main precursor studies for our work. Bergsma et al. (2008) build models of selectional preferences that include morphological features such as capitalization and the presence of digits. However, their approach is task-specific and requires a (semi-)supervised setting. Allan and Kumaran (2003) make use of morphology by building language models for stemming-based equivalence classes. Our approach also uses morphological processing, alb"
P13-2128,W05-1516,0,0.0197004,"sion methods in Information Retrieval are also prominent cases of smoothing that addresses the lexical mismatch between query and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003). In lexical semantics, smoothing is often achieved by backing 731 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 731–735, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics off from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves. We see two main precursor studies for our work. Bergsma et al. (2008) build models of selectional preferences that include morphological features such as capitalization and the presence of digits. However, their approach is task-specific and requires a (semi-)supervised setting. Allan and Kumaran (2003) make use of mor"
P13-2128,P13-1118,1,0.82069,"Missing"
P13-2128,N07-2052,0,0.0794613,"Missing"
P13-2128,faass-etal-2010-design,0,\N,Missing
P13-2128,J10-4007,1,\N,Missing
P13-2128,N03-1013,0,\N,Missing
P13-2128,2003.mtsummit-systems.9,0,\N,Missing
P13-2137,C12-2001,0,0.0354826,"Missing"
P13-2137,J10-4006,0,0.567318,"re (K¨ubler et al., 2009). Their obvious problem, of course, is that they require a large parsed corpus. In this paper, we describe the construction of a Distributional Memory for Croatian (D M .H R), a free word order language. To do so, we parse hrWaC (Ljubeˇsi´c and Erjavec, 2011), a 1.2B-token Croatian web corpus. We evaluate D M .H R on a synonym choice task, where it outperforms the standard bag-of-word model for nouns and verbs. We report on the first structured distributional semantic model for Croatian, D M .H R. It is constructed after the model of the English Distributional Memory (Baroni and Lenci, 2010), from a dependencyparsed Croatian web corpus, and covers about 2M lemmas. We give details on the linguistic processing and the design principles. An evaluation shows state-of-theart performance on a semantic similarity task with particularly good performance on nouns. The resource is freely available. 1 Introduction Most current work in lexical semantics is based on the Distributional Hypothesis (Harris, 1954), which posits a correlation between the degree of words’ semantic similarity and the similarity of the contexts in which they occur. Using this hypothesis, word meaning representations"
P13-2137,broda-etal-2008-corpus,0,0.0212408,"syntax-based (cooccurrence defined syntactically, syntactic objects as dimensions). Syntax-based models have several desirable properties. First, they are model to fine-grained types of semantic similarity such as predicate-argument plausibility (Erk et al., 2010). Second, they are more versatile – Baroni and Lenci (2010) have presented a generic framework, the Distributional Memory (DM), which is applicable 2 Related Work Vector space semantic models have been applied to a number of Slavic languages, including Bulgarian (Nakov, 2001a), Czech (Smrˇz and Rychl´y, 2001), Polish (Piasecki, 2009; Broda et al., 2008; Broda and Piasecki, 2008), and Russian (Nakov, 2001b; Mitrofanova et al., 2007). Previous work on distributional semantic models for Croatian dealt with similarity prediction (Ljubeˇsi´c et al., 2008; Jankovi´c et al., 2011) and synonym detection (Karan et al., 2012), however using only wordbased and not syntactic-based models. So far the only DM for a language other than English is the German D M .D E by Pad´o and Utt (2012), who describe the process of building D M .D E and the evaluation on a synonym choice task. Our work is similar, though each language has its own challenges. Croatian,"
P13-2137,1993.eamt-1.1,0,0.518212,"Missing"
P13-2137,P07-2053,0,0.033919,"Missing"
P13-2137,W06-2932,0,0.0123606,". SET IMES .H R consists of 90K tokens and 4K sentences, manually lemmatized and MSD-tagged according to Multext East v4 tagset (Erjavec, 2012), with the help of the Croatian Lemmatization Server (Tadi´c, 2005). It is used also as a basis for a novel formalism for syntactic annotation and dependency parsing of Croatian (Agi´c and Merkler, 2013). On the basis of previous evaluation for Croatian (Agi´c et al., 2008; Agi´c et al., 2009; Agi´c, 2012) and availability and licensing considerations, we chose HunPos tagger (Hal´acsy et al., 2007), CST lemmatizer (Ingason et al., 2008), and MSTParser (McDonald et al., 2006) to process hrWaC. We evaluated the tools on 100-sentence test sets from SET IMES .H R and Wikipedia; performance on Wikipedia should be indicative of the performance on a cross-domain dataset, such as hrWaC. In Table 1 we show lemmatization and tagging accuracy, as well as dependency parsing accuracy in terms of labeled attachment score (LAS). The results show that lemmatization, tagging and parsing accuracy improves on the state of the art for Croatian. The SET IMES .H R dependency parsing models are publicly available.3 Syntactic patterns. We collect the co-occurrence counts of tuples using"
P13-2137,D07-1060,0,0.239074,"Missing"
P13-2137,J10-4007,1,\N,Missing
P13-2139,H05-1016,0,0.0358848,"ches rely on traditional vector space models (Salton et al., 1975), as more sophisticated natural language processing techniques have not yet proven to be useful for this task. On the other hand, significant advances in sentence-level event extraction have been made over the last decade, in particular as the result of 2 Related Work The traditional vector space model (VSM) (Salton et al., 1975) computes the cosine between bag-ofwords representations of documents. The VSM is at the core of most approaches that identify sametopic news stories (Hatzivassiloglou et al., 2000; Brants et al., 2003; Kumaran and Allan, 2005; Atkinson and Van der Goot, 2009). However, it has been observed that some word classes (e.g., named entities, noun phrases, collocations) have more significance than the others. Among them, named entities have been considered as particularly important, as they often identify the participants of an event. In view of this, Hatzivassiloglou et al. (2000) restrict the set of words to be used for document representation to words constituting noun phrases and named entities. Makkonen et 797 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 797–803, c So"
P13-2139,bejan-harabagiu-2008-linguistic,0,0.22822,"Missing"
P13-2139,D12-1045,0,0.0727162,"Missing"
P13-2139,P10-1143,0,0.106609,"Missing"
P13-2139,S07-1014,0,0.0985379,"Missing"
P13-2139,wayne-2000-multilingual,0,0.052955,"oreferent event mentions: (yanked, stolen), (recovered, recovered), and (arrests, arrested). Experiments We conducted two preliminary experiments to investigate whether kernels on event graphs can be used to recognize identical events. 4.1 R Table 1: Results for recognition of identical events (v,v )∈VP 4 P Task 1: Recognizing identical events Dataset. In the first experiment, we classify pairs of news stories as either describing identical real-world events or not. For this we need a collection of stories in which pairs of stories on identical events have been annotated as such. TDT corpora (Wayne, 2000) is not directly usable because it has no such annotations. We therefore decided to build a small annotated dataset.1 To this end, we use the news clusters of the EMM NewsBrief service (Steinberger et al., 2009). EMM clusters news stories from different sources using a document similarity score. We acquired 10 randomly chosen news clusters, manually inspected each of them, and retained in each cluster only the documents that describe the same real-world events. Additionally, we ensured that no documents from Results. For each graph kernel and the VSM baseline, we determine the optimal threshol"
P13-2139,C00-2137,0,0.0712328,"c. Avg. prec. Tensor PGK Conormal PGK WDK 86.7 93.3 86.7 96.8 97.5 95.7 VSM baseline 80.0 77.1 Event-shifting paraphrase “Taliban militants have been arrested in north-west Pakistan. At least 380 militants have been arrested. . . ” Table 3: Results for event-based similarity ranking Table 2: Event paraphrasing example two different rank evaluation metrics: R-precision (precision at rank 30, as there are 30 positive pairs) and average precision. The performance of graph kernel models and the VSM baseline is given in Table 3. We tested the significance of differences using stratified shuffling (Yeh, 2000). When considering average precision, all kernel models significantly (at p &lt; 0.01) outperform the baseline. However, when considering R-precision, only the conormal PGK model significantly (at p &lt; 0.05) outperforms the baseline. There is no statistical significance in performance differences between the considered kernel methods. Inspection of the rankings reveals that graph kernels assign very low scores to negative pairs, i.e., they distinguish well between textual representations of topically similar, but different real-world events. individual kernel scores to an SVM model (with RBF kerne"
P13-2139,S10-1010,0,\N,Missing
Q17-1032,W13-2408,0,0.0630567,"Missing"
Q17-1032,W11-0822,0,0.0571938,"Missing"
Q17-1032,C14-1071,1,0.831411,"exception which uses distributional semantics around the phrase to identify how easily an n-gram could be replaced by a single word. Typically multiword lexicons are created by ranking n-grams according to an association measure and applying a threshold. The algorithm of da Silva and Lopes (1999) is somewhat more sophisticated, in that it identifies the local maxima of association measures across subsuming n-grams within a sentence to identify MWEs of unrestricted length and syntactic composition; its effectiveness beyond noun phrases, however, seems relatively limited (Ramisch et al., 2012). Brooke et al. (2014; 2015) developed a heuristic method intended for general FS extraction in larger corpora, first using conditional probabilities to do an initial (single pass) coarse-grained segmentation of the corpus, followed by a pass through the resulting vocabulary, breaking larger units into smaller ones based on a tradeoff between marginal and conditional statistics. The work of Newman et al. (2012) is an example of an unsupervised approach which does not use association measures: it extends the Bayesian word segmentation approach of Goldwater et al. (2009) to multiword tokenization, applying a generat"
Q17-1032,W15-0915,1,0.0565729,"ample PMI (Church and Hanks, 1990), i.e., the log ratio of the joint probability to the product of the marginal probabilities of the individual words. Another measure which addresses sequences of longer than two words is the C-value (Frantzi et al., 2000) which weights term frequency by the log length of the n-gram while penalizing ngrams that appear in frequent larger ones. Mutual expectation (Dias et al., 1999) involves deriving a normalized statistic that reflects the extent to which a phrase resists the omission of any constituent word. Similarly, the lexical predictability ratio (LPR) of Brooke et al. (2015) is an association measure applicable to any possible syntactic pattern, which is calculated by discounting syntactic predictability from the overall conditional probability for each word given the other words in the phrase. Though most association measures involve only usage statistics of the phrase and its subparts, the DRUID measure (Riedl and Biemann, 2015) is an exception which uses distributional semantics around the phrase to identify how easily an n-gram could be replaced by a single word. Typically multiword lexicons are created by ranking n-grams according to an association measure a"
Q17-1032,J90-1003,0,0.392984,"semantics are accessible by analogy (e.g., glass limb, analogous to wooden leg). We also exclude from the definition of both FS and MWE those named entities which refer to people or places which are little-known and/or whose surface form appears derived (e.g., Mrs. Barbara W. Smith or Smith Garden Supplies Ltd). Figure 1 shows the conception of the relationship between FS, (multiword) constructions, MWE, and (multiword) named entities that we assume for this paper. From a practical perspective, the starting point for multiword lexicon creation has typically been lexical association measures (Church and Hanks, 1990; Dunning, 1993; Schone and Jurafsky, 2001; Evert, 2004; Pecina, 2010; Araujo et al., 2011; Kulkarni and Finlayson, 2011; Ramisch, 2014). When these methods are used to build a lexicon, particular binary syntactic patterns are typically chosen. Only some of these measures generalize tractably beyond two words, for example PMI (Church and Hanks, 1990), i.e., the log ratio of the joint probability to the product of the marginal probabilities of the individual words. Another measure which addresses sequences of longer than two words is the C-value (Frantzi et al., 2000) which weights term frequen"
Q17-1032,P16-1016,0,0.0130188,"014a) distinguishes a full range of MWE sequences in the English Web Treebank, including gapped expressions, using a supervised se457 quence tagging model. Though, in theory, automatic lexical resources could be a useful addition to the Schneider et al. model, which uses only manual lexical resources, attempts to do so have achieved mixed success (Riedl and Biemann, 2016). The motivations for building lexicons of FS naturally overlap with those for MWE: models of distributional semantics, in particular, can benefit from sensitivity to multiword units (Cohen and Widdows, 2009), as can parsing (Constant and Nivre, 2016) and topic models (Lau et al., 2013). One major motivation for looking beyond MWEs is the ability to carry out broader linguistic analyses. Within corpus linguistics, multiword sequences have been studied in the form of lexical bundles (Biber et al., 2004), which are simply n-grams that occur above a certain frequency threshold. Like FS, lexical bundles generally involve larger phrasal chunks that would be missed by traditional MWE extraction, and so research in this area has tended to focus on how particular formulaic phrases (e.g., if you look at) are indicative of particular genres (e.g., u"
Q17-1032,J93-1003,0,0.165092,"e by analogy (e.g., glass limb, analogous to wooden leg). We also exclude from the definition of both FS and MWE those named entities which refer to people or places which are little-known and/or whose surface form appears derived (e.g., Mrs. Barbara W. Smith or Smith Garden Supplies Ltd). Figure 1 shows the conception of the relationship between FS, (multiword) constructions, MWE, and (multiword) named entities that we assume for this paper. From a practical perspective, the starting point for multiword lexicon creation has typically been lexical association measures (Church and Hanks, 1990; Dunning, 1993; Schone and Jurafsky, 2001; Evert, 2004; Pecina, 2010; Araujo et al., 2011; Kulkarni and Finlayson, 2011; Ramisch, 2014). When these methods are used to build a lexicon, particular binary syntactic patterns are typically chosen. Only some of these measures generalize tractably beyond two words, for example PMI (Church and Hanks, 1990), i.e., the log ratio of the joint probability to the product of the marginal probabilities of the individual words. Another measure which addresses sequences of longer than two words is the C-value (Frantzi et al., 2000) which weights term frequency by the log l"
Q17-1032,J09-1005,0,0.0191603,"vocabulary, breaking larger units into smaller ones based on a tradeoff between marginal and conditional statistics. The work of Newman et al. (2012) is an example of an unsupervised approach which does not use association measures: it extends the Bayesian word segmentation approach of Goldwater et al. (2009) to multiword tokenization, applying a generative Dirichlet Process model which jointly constructs a segmentation of the corpus and a corresponding multiword vocabulary. Other research in MWEs has tended to be rather focused on particular syntactic patterns such as verbnoun combinations (Fazly et al., 2009). The system of Schneider et al. (2014a) distinguishes a full range of MWE sequences in the English Web Treebank, including gapped expressions, using a supervised se457 quence tagging model. Though, in theory, automatic lexical resources could be a useful addition to the Schneider et al. model, which uses only manual lexical resources, attempts to do so have achieved mixed success (Riedl and Biemann, 2016). The motivations for building lexicons of FS naturally overlap with those for MWE: models of distributional semantics, in particular, can benefit from sensitivity to multiword units (Cohen a"
Q17-1032,D08-1104,0,0.0313431,"rge, unlabeled corpus of text. Since our primary association measure is an adaption of LPR, our approach in this section mostly follows Brooke et al. (2015) up until the last stage. An initial requirement of any such method is an n-gram frequency threshold, which we set to 1 instance per 10 million words, following Brooke et al. (2015).3 We include gapped or non-contiguous n-grams in our analysis, in acknowledgment of the fact that many languages have MWEs where the components can be “separated”, including verb particle constructions in English (Deh´e, 2002), and noun-verb idioms in Japanese (Hashimoto and Kawahara, 2008). Having said this, there are generally strong syntactic and length restrictions on what can constitute a gap (Wasow, 2002), which we capture in the form of a language-specific POS-based regular expression (see Section 4 for details). This greatly lowers the number of potentially gapped n-gram types, increasing precision and efficiency for negligible loss of recall. We also exclude punctuation and lemmatize the corpus, and enforce an n-gram count threshold. As long as the count threshold is substantially above 1, efficient extraction of all n-grams can be done iteratively: in iteration i, i-gr"
Q17-1032,W11-0818,0,0.0164636,"ion of both FS and MWE those named entities which refer to people or places which are little-known and/or whose surface form appears derived (e.g., Mrs. Barbara W. Smith or Smith Garden Supplies Ltd). Figure 1 shows the conception of the relationship between FS, (multiword) constructions, MWE, and (multiword) named entities that we assume for this paper. From a practical perspective, the starting point for multiword lexicon creation has typically been lexical association measures (Church and Hanks, 1990; Dunning, 1993; Schone and Jurafsky, 2001; Evert, 2004; Pecina, 2010; Araujo et al., 2011; Kulkarni and Finlayson, 2011; Ramisch, 2014). When these methods are used to build a lexicon, particular binary syntactic patterns are typically chosen. Only some of these measures generalize tractably beyond two words, for example PMI (Church and Hanks, 1990), i.e., the log ratio of the joint probability to the product of the marginal probabilities of the individual words. Another measure which addresses sequences of longer than two words is the C-value (Frantzi et al., 2000) which weights term frequency by the log length of the n-gram while penalizing ngrams that appear in frequent larger ones. Mutual expectation (Dias"
Q17-1032,W14-0405,0,0.0270618,"Missing"
Q17-1032,C12-1127,1,0.547831,"word nonetheless stubbornly remains the de facto standard processing unit for most research in modern NLP. The potential of multiword knowledge to improve both the automatic processing of language as well as offer new understanding of human acquisition and usage of language is the primary motivator of this work. Here, we present an effective, expandable, and tractable new approach to comprehensive multiword lexicon acquisition. Our aim is to find a middle ground between standard MWE acquisition approaches based on association measures (Ramisch, 2014) and more sophisticated statistical models (Newman et al., 2012) that do not scale to large corpora, the main source of the distributional information in modern NLP systems. A central challenge in building comprehensive multiword lexicons is paring down the huge space of possibilities without imposing restrictions which disregard a major portion of the multiword vocabulary of a language: allowing for diversity creates significant redundancy among statistically promising candidates. The lattice model proposed here addresses this primarily by having the candidates— contiguous and non-contiguous n-gram types— compete with each other based on subsumption and o"
Q17-1032,W12-3301,0,0.0640475,"Missing"
Q17-1032,D15-1290,0,0.020927,"quent larger ones. Mutual expectation (Dias et al., 1999) involves deriving a normalized statistic that reflects the extent to which a phrase resists the omission of any constituent word. Similarly, the lexical predictability ratio (LPR) of Brooke et al. (2015) is an association measure applicable to any possible syntactic pattern, which is calculated by discounting syntactic predictability from the overall conditional probability for each word given the other words in the phrase. Though most association measures involve only usage statistics of the phrase and its subparts, the DRUID measure (Riedl and Biemann, 2015) is an exception which uses distributional semantics around the phrase to identify how easily an n-gram could be replaced by a single word. Typically multiword lexicons are created by ranking n-grams according to an association measure and applying a threshold. The algorithm of da Silva and Lopes (1999) is somewhat more sophisticated, in that it identifies the local maxima of association measures across subsuming n-grams within a sentence to identify MWEs of unrestricted length and syntactic composition; its effectiveness beyond noun phrases, however, seems relatively limited (Ramisch et al.,"
Q17-1032,W16-1816,0,0.0189816,"ructs a segmentation of the corpus and a corresponding multiword vocabulary. Other research in MWEs has tended to be rather focused on particular syntactic patterns such as verbnoun combinations (Fazly et al., 2009). The system of Schneider et al. (2014a) distinguishes a full range of MWE sequences in the English Web Treebank, including gapped expressions, using a supervised se457 quence tagging model. Though, in theory, automatic lexical resources could be a useful addition to the Schneider et al. model, which uses only manual lexical resources, attempts to do so have achieved mixed success (Riedl and Biemann, 2016). The motivations for building lexicons of FS naturally overlap with those for MWE: models of distributional semantics, in particular, can benefit from sensitivity to multiword units (Cohen and Widdows, 2009), as can parsing (Constant and Nivre, 2016) and topic models (Lau et al., 2013). One major motivation for looking beyond MWEs is the ability to carry out broader linguistic analyses. Within corpus linguistics, multiword sequences have been studied in the form of lexical bundles (Biber et al., 2004), which are simply n-grams that occur above a certain frequency threshold. Like FS, lexical b"
Q17-1032,Q16-1013,0,0.0288207,"in this area has tended to focus on how particular formulaic phrases (e.g., if you look at) are indicative of particular genres (e.g., university lectures). Lexical bundles have been applied, in particular, to learner language: for example, Chen and Baker (2010) show that non-native student writers use a severely restricted range of lexical bundle types, and tend to overuse those types, while Granger and Bestgen (2014) investigate the role of proficiency, demonstrating that intermediate learners underuse lower-frequency bigrams and overuse high-frequency bigrams relative to advanced learners. Sakaguchi et al. (2016) demonstrate that improving fluency (closely linked to the use of linguistic formulas) is more important than improving strict grammaticality with respect to native speaker judgments of non-native productions; Brooke et al. (2015) explicitly argue for FS lexicons as a way to identify, track, and improve learner proficiency. 3 Method Our approach to FS identification involves optimization of the total explanatory power of a lattice, where each node corresponds to an n-gram type. The explanatory power of the whole lattice is defined simply as a product of the explainedness of the individual node"
Q17-1032,Q14-1016,0,0.252037,"computational linguistics, the most common term used to describe multiword lexical units is multiword expression (“MWE”: Sag et al. (2002), Baldwin and Kim (2010)), but here we wish to make a principled distinction between at least somewhat non-compositional, strongly lexicalized MWEs and FS, a near superset which includes many MWEs but also compositional linguistic formulas. This distinction is not a new one; it exists, for example, in the original paper of Sag et al. (2002) in the distinction between lexicalized and institutionalized phrases, and also to some extent in the MWE annotation of Schneider et al. (2014b), who distinguish between weak (collocational)2 and strong (non-compositional) MWEs. It is our contention, however, that separate, precise terminology is useful for research targeted at either class: we need not strain the concept of MWE to include items which do not require special semantics, nor are we inclined to disregard the larger formulaticity of language simply because it is not the dominant focus of MWE 1 Though by this definition individuals or small groups may have their own FS, here we are only interested in FS that are shared by a recognizable language community. 2 Here we avoid"
Q17-1032,schneider-etal-2014-comprehensive,0,0.241709,"computational linguistics, the most common term used to describe multiword lexical units is multiword expression (“MWE”: Sag et al. (2002), Baldwin and Kim (2010)), but here we wish to make a principled distinction between at least somewhat non-compositional, strongly lexicalized MWEs and FS, a near superset which includes many MWEs but also compositional linguistic formulas. This distinction is not a new one; it exists, for example, in the original paper of Sag et al. (2002) in the distinction between lexicalized and institutionalized phrases, and also to some extent in the MWE annotation of Schneider et al. (2014b), who distinguish between weak (collocational)2 and strong (non-compositional) MWEs. It is our contention, however, that separate, precise terminology is useful for research targeted at either class: we need not strain the concept of MWE to include items which do not require special semantics, nor are we inclined to disregard the larger formulaticity of language simply because it is not the dominant focus of MWE 1 Though by this definition individuals or small groups may have their own FS, here we are only interested in FS that are shared by a recognizable language community. 2 Here we avoid"
Q17-1032,W01-0513,0,0.126288,".g., glass limb, analogous to wooden leg). We also exclude from the definition of both FS and MWE those named entities which refer to people or places which are little-known and/or whose surface form appears derived (e.g., Mrs. Barbara W. Smith or Smith Garden Supplies Ltd). Figure 1 shows the conception of the relationship between FS, (multiword) constructions, MWE, and (multiword) named entities that we assume for this paper. From a practical perspective, the starting point for multiword lexicon creation has typically been lexical association measures (Church and Hanks, 1990; Dunning, 1993; Schone and Jurafsky, 2001; Evert, 2004; Pecina, 2010; Araujo et al., 2011; Kulkarni and Finlayson, 2011; Ramisch, 2014). When these methods are used to build a lexicon, particular binary syntactic patterns are typically chosen. Only some of these measures generalize tractably beyond two words, for example PMI (Church and Hanks, 1990), i.e., the log ratio of the joint probability to the product of the marginal probabilities of the individual words. Another measure which addresses sequences of longer than two words is the C-value (Frantzi et al., 2000) which weights term frequency by the log length of the n-gram while p"
Q17-1032,shinzato-etal-2008-large,0,0.0171967,"probing the challenges associated with using an n-gram approach to FS identification in such languages. For Croatian, we used ˇ the 1.2-billion-token fhrWaC corpus (Snajder et al., 2013), a filtered version of the Croatian web corpus hrWaC (Ljubeˇsi´c and Klubiˇcka, 2014), which is POS-tagged and lemmatized using the tools of Agi´c et al. (2013). Similar to English, the POS regex for Croatian includes simple nouns, adjectives and pronouns, but also other elements that regularly appear inside FS, including both adverbs and copulas. For Japanese, we used a subset of the 100M-page web corpus of Shinzato et al. (2008), which was roughly the same token length as the English corpus. We segmented and POS-tagged the corpus with MeCab (Kudo, 2008) using the UNIDIC morphological dictionary (Den et al., 2007). The POS regex for Japanese covers the same basic nominal structures as English, but also includes case markers and adverbials. Though our processing of Japanese includes basic lemmatization related to superficial elements like the choice of writing script and politeness markers, many elements (such as case marking) which are removed by lemmatization in Croatian are segmented into independent morphological u"
Q17-1032,P13-2137,1,0.886577,"Missing"
Q17-1032,C00-2137,0,0.0294829,"0.43 0.46 0.47 0.49 0.49 0.63 0.53 0.78 0.61 0.49 0.60 0.48 0.54 0.55 0.56 0.59 Table 2: Results of FS identification in various test sets: Countrank = ranking with frequency; PMIrank = PMI-based ranking; minLPRrank = ranking with minLPR; LPRseg = the method of Brooke et al. (2015); “−cl” = no clearing; “−ovr” = no penalization of overlaps; “P” = Precision; “R” = Recall; and “F” = F-score. Bold is best in a given column. The performance difference of the Lattice model relative to the best baseline for all test sets considered together is significant at p < 0.01 (based on the permutation test: Yeh (2000)). P R F PMIrank minLPRrank LPR-seg LocalMaxs DP-seg 0.20 0.34 0.42 0.56 0.35 0.29 0.45 0.45 0.39 0.71 0.23 0.39 0.43 0.46 0.47 Lattice 0.47 0.63 0.54 Table 3: Results of FS identification in contiguous BNC test set; LocalMaxs = method of da Silva and Lopes (1999); DP-seg = method of Newman et al. (2012) ble 3), we found that both the LocalMaxs algorithm and the DP-seg method of Newman et al. (2012) were able to beat our other baseline methods with roughly similar F-scores, though both are well below our Lattice method. Some of the difference seems attributable to fairly severe precision/recal"
R09-1074,P00-1026,0,0.0375483,"ly related word-forms into the so-called stem classes. Traditional rule- and dictionary-based stemming requires significant linguistic expertise and resources, which is why, for resourcepoor languages, language-independent approaches are gaining popularity. Among others, string distancebased stemming, in which stem classes are derived by clustering word-forms based on their character structure, has been shown to be a viable alternative. String distance-based clustering was first proposed by Adamson and Boreham [1] for the English language, and similar approaches were later employed for Arabic [12] and Turkish [3]. More recently, Majumder et al. [9] proposed string distance measures for stemming in Bengali, as well as in Hungarian and Czech [8]. The performance of their stemming procedure has been shown to be comparable to traditional rule-based approaches. In this paper, we investigate the applicability of string distance-based stemming to the Croatian language. The Croatian language, much like other Slavic languages, is morphologically complex, especially in the form of inflection. Previous approaches to the stemming in Croatian are rule-based [5, 7] or dictionary-based [14, 16] and a"
S12-1060,N09-1003,0,0.01562,"Missing"
S12-1060,S12-1051,0,0.584667,"l., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher et al., 2011; Wan et al., 2006) is sufficiently similar to reuse some of the techniques. This paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the SemEval-2012 Semantic Text Similarity Task (Agirre et al., 2012). We propose several sentence similarity measures built upon knowledge-based and corpus-based similarity of individual words as well as similarity of dependency parses. Our two systems, simple and syntax, use supervised machine learning, more specifically the support vector regression (SVR), to combine a large amount of features computed from pairs of sentences. The two systems differ in the set of features they employ. Our systems placed in the top 5 (out of 89 submitted systems) for all three aggregate correlation measures: 2nd (syntax) and 3rd (simple) for overall Pearson, 1st (simple) and"
S12-1060,P06-4018,0,0.115304,"d Similarity ws353 PathLen Lin Dist (NYT) Dist (Wikipedia) Knowledge-based Word Similarity Knowledge-based word similarity approaches rely on a semantic network of words, such as WordNet. Given two words, their similarity can be estimated by considering their relative positions within the knowledge base hierarchy. All of our knowledge-based word similarity measures are based on WordNet. Some measures use the concept of a lowest common subsumer (LCS) of concepts c1 and c2 , which represents the lowest node in the WordNet hierarchy that is a hypernym of both c1 and c2 . We use the NLTK library (Bird, 2006) to compute the PathLen similarity (Leacock and Chodorow, 1998) and Lin similarity (Lin, 1998) measures. A single word often denotes several concepts, depending on its context. In order to compute the similarity score for a pair of words, we take the maximum similarity score over all possible pairs of concepts (i.e., WordNet synsets). 2.2 Table 1: Evaluation of word similarity measures Preprocessing We list all of the preprocessing steps our systems perform. If a preprocessing step is executed by only one of our systems, the system’s name is indicated in parentheses. 1. All hyphens and slashes"
S12-1060,J06-1003,0,0.117718,"in the top 5, for the three overall evaluation metrics used (overall Pearson – 2nd and 3rd, normalized Pearson – 1st and 3rd, weighted mean – 2nd and 5th). 1 Introduction Natural language processing tasks such as text classification (Sebastiani, 2002), text summarization (Lin and Hovy, 2003; Aliguliyev, 2009), information retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher et al., 2011; Wan et al., 2006) is sufficiently similar to reuse some of the techniques. This paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the SemEval-2012 Semantic Text Similarity Task (Agirre et al., 2012). We propose several sentence similarity"
S12-1060,W08-1301,0,0.00811189,"Missing"
S12-1060,de-marneffe-etal-2006-generating,0,0.00613622,"Missing"
S12-1060,N03-1020,0,0.112565,"ndensed into short text snippets such as social media posts, image captions, and scientific abstracts. We predict the human ratings of sentence similarity using a support vector regression model with multiple features measuring word-overlap similarity and syntax similarity. Out of 89 systems submitted, our two systems ranked in the top 5, for the three overall evaluation metrics used (overall Pearson – 2nd and 3rd, normalized Pearson – 1st and 3rd, weighted mean – 2nd and 5th). 1 Introduction Natural language processing tasks such as text classification (Sebastiani, 2002), text summarization (Lin and Hovy, 2003; Aliguliyev, 2009), information retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et"
S12-1060,P08-1028,0,0.0519795,"c(S1 , S2 ) = Pw∈S1 ∩S2 0 0 w ∈S2 ic(w ) The weighted word overlap between two sentences is calculated as the harmonic mean of the wwc(S1 , S2 ) and wwc(S2 , S1 ). This measure proved to be very useful, but it could be improved even further. Misspelled frequent words are more frequent than some correctly spelled but rarely used words. Hence dealing with misspelled words would remove the inappropriate heavy penalty for a mismatch between correctly and incorrectly spelled words. Vector Space Sentence Similarity This measure is motivated by the idea of compositionality of distributional vectors (Mitchell and Lapata, 2008). We represent each sentence as a single distributional vector u(·) by summing the distributional (i.e., LSA) vector of each word w in the P sentence S: u(S) = w∈S xw , where xw is the vector representation of the word w. Another similar representation uW (·) uses the information content ic(w) to weigh the LSA vector P of each word before summation: uW (S) = w∈S ic(w)xw . The simple system uses |cos(u(S1 ), u(S2 )) |and |cos(uW (S1 ), uW (S2 )) |for the vector space sentence similarity features. 3.4 Greedy Lemma Aligning Overlap This measure computes the similarity between sentences using the"
S12-1060,J98-1004,0,0.0575789,"Missing"
S12-1060,U06-1019,0,0.291952,"retrieval (Park et al., 2005), and word sense disambiguation (Sch¨utze, 1998) rely on a measure of semantic similarity of textual documents. Research predominantly focused either on the document similarity (Salton et al., 1975; Maguitman et al., 2005) or the word similarity (Budanitsky and Hirst, 2006; Agirre et al., 2009). Evaluating the similarity of short texts such as sentences or paragraphs (Islam and Inkpen, 2008; Mihalcea et al., 2006; Oliva et al., 2011) received less attention from the research community. The task of recognizing paraphrases (Michel et al., 2011; Socher et al., 2011; Wan et al., 2006) is sufficiently similar to reuse some of the techniques. This paper presents the two systems for automated measuring of semantic similarity of short texts which we submitted to the SemEval-2012 Semantic Text Similarity Task (Agirre et al., 2012). We propose several sentence similarity measures built upon knowledge-based and corpus-based similarity of individual words as well as similarity of dependency parses. Our two systems, simple and syntax, use supervised machine learning, more specifically the support vector regression (SVR), to combine a large amount of features computed from pairs of"
S15-2012,S12-1051,0,0.0391829,"r a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 2012 (Agirre et al., 2012). There is now a 1 http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 70–74, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics significant amount of work on this task. The best performing STS systems employ various methods for aligning semantically corresponding words or otherwise quantifying the amount of semantically congruent content between two sentences (Sultan et ˇ c et al., 2012). al., 2014; Sari´ In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011)"
S15-2012,S14-2010,0,0.030874,"Missing"
S15-2012,I13-1041,0,0.0128648,"dditionally provide an analysis of the dataset and point to some peculiarities of the evaluation setup. 1 Introduction Recognizing tweets that convey the same meaning (paraphrases) or similar meaning is useful in applications such as event detection (Petrovi´c et al., 2012), tweet summarization (Yang et al., 2011), and tweet retrieval (Naveed et al., 2011). Paraphrase detection in tweets is a more challenging task than paraphrase detection in other domains such as news (Xu et al., 2013). Besides brevity (max. 140 characters), tweets exhibit all the irregularities typical of social media text (Baldwin et al., 2013), such as informality, ungrammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic texˇ c et al., 2012; Sultan et al., tual similarity (STS) (Sari´ 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in st"
S15-2012,P09-1053,0,0.172779,"ammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic texˇ c et al., 2012; Sultan et al., tual similarity (STS) (Sari´ 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 2012 (Agirre et al., 2012). There is now a 1 http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluatio"
S15-2012,P12-1091,0,0.0512082,"g the amount of semantically congruent content between two sentences (Sultan et ˇ c et al., 2012). al., 2014; Sari´ In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011) detect content redundancy between tweets, where redundant means paraphrased or entailed content. They achieve reasonable performance with SVM using vector-comparison and syntactic tree kernels. Xu et al. (2014) propose M UL TI P, a latent variable model for joint inference of correspondence of words and sentences. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number of features is relatively small, we use SVM with a non-linear (RBF) kernel. Our features can be divided into (1) semantic overlap features, most of which are adaptations of STS features proposed by ˇ c et al. (2012), and (2) word alignment features, Sari´ based on (a) the output of the word alignment model by Sultan et al. (2014) and (b) a re-implementation of the M ULTI P model by Xu et al. (2014). In the dataset prov"
S15-2012,J10-3003,0,0.109662,"es brevity (max. 140 characters), tweets exhibit all the irregularities typical of social media text (Baldwin et al., 2013), such as informality, ungrammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic texˇ c et al., 2012; Sultan et al., tual similarity (STS) (Sari´ 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 201"
S15-2012,N12-1019,0,0.0843001,"ncy, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic texˇ c et al., 2012; Sultan et al., tual similarity (STS) (Sari´ 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 2012 (Agirre et al., 2012). There is now a 1 http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages"
S15-2012,D14-1162,0,0.0807917,"tokens in tweets, and the other taking into account only content words. 2 https://github.com/ma-sultan/ monolingual-word-aligner 72 Anchor count (ANC). We re-implemented the M ULTI P model of Xu et al. (2014).3 As anchor candidates we consider all pairs of content words from the two tweets. We use a minimalistic set of features including (1) Levenshtein distance between candidate words, (2) several binary features indicating relatedness of words (e.g., lowercased tokens match, POStags match), and (3) semantic similarity obtained as the cosine of word embeddings, obtained with the GloVe model (Pennington et al., 2014) trained on Twitter data.4 To account for feature interactions, following (Xu et al., 2014), we also use conjunction features. We use the number of anchors identified by this method for a pair of tweets as a feature for our SVM model. 4 Evaluation Each team was allowed to submit two runs on the test set provided by the task organizers (Xu et al., 2015). Participants were provided with a training set (13,063 pairs) and a development set (4,727 pairs). We used the train and development set to optimize the hyperparameters C and γ of our SVM model with the RBF kernel. For the final evaluation, the"
S15-2012,N12-1034,0,0.314377,"Missing"
S15-2012,S12-1060,1,0.880991,"Missing"
S15-2012,S14-2039,0,0.0567314,"nces. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number of features is relatively small, we use SVM with a non-linear (RBF) kernel. Our features can be divided into (1) semantic overlap features, most of which are adaptations of STS features proposed by ˇ c et al. (2012), and (2) word alignment features, Sari´ based on (a) the output of the word alignment model by Sultan et al. (2014) and (b) a re-implementation of the M ULTI P model by Xu et al. (2014). In the dataset provided by the organizers, each tweet is associated with a topic, with 10 to 100 tweet pairs per topic. An important preprocessing step is to remove tokens that can be found in the name of a topic. For example, for the topic “Roberto Mancini”, we trim the tweets “Roberto Mancini gets the boot from the Man City” and “City sacked Mancini” to “gets the boot from the Man City” and “City sacked”, respectively, and then compute the features on the trimmed tweets. The rationale is that, given a topic, there is an"
S15-2012,W13-2515,0,0.245099,"Missing"
S15-2012,Q14-1034,0,0.531452,"Association for Computational Linguistics significant amount of work on this task. The best performing STS systems employ various methods for aligning semantically corresponding words or otherwise quantifying the amount of semantically congruent content between two sentences (Sultan et ˇ c et al., 2012). al., 2014; Sari´ In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011) detect content redundancy between tweets, where redundant means paraphrased or entailed content. They achieve reasonable performance with SVM using vector-comparison and syntactic tree kernels. Xu et al. (2014) propose M UL TI P, a latent variable model for joint inference of correspondence of words and sentences. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number of features is relatively small, we use SVM with a non-linear (RBF) kernel. Our features can be divided into (1) semantic overlap features, most of which are adaptations of STS features proposed by ˇ c et al. (2"
S15-2012,D11-1061,0,0.158719,"2 (Agirre et al., 2012). There is now a 1 http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 70–74, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics significant amount of work on this task. The best performing STS systems employ various methods for aligning semantically corresponding words or otherwise quantifying the amount of semantically congruent content between two sentences (Sultan et ˇ c et al., 2012). al., 2014; Sari´ In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011) detect content redundancy between tweets, where redundant means paraphrased or entailed content. They achieve reasonable performance with SVM using vector-comparison and syntactic tree kernels. Xu et al. (2014) propose M UL TI P, a latent variable model for joint inference of correspondence of words and sentences. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number"
S15-2012,S15-2001,0,\N,Missing
S16-1075,W11-1701,0,0.0925133,"xical and a number of task-specific features. Our system ranked 3rd among the 19 systems submitted to this task. 1 Introduction Stance is the overall position held by a person towards an object, idea, or proposition (Somasundaran and Wiebe, 2009). The task of stance detection – the automatic classification of stance expressed in text – has been attracting increasing interest, as it is of practical interest to many stakeholders, ranging from political bodies to companies. Most recent work focused on stance detection in online debates (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Anand et al., 2011; Hasan and Ng, 2014; Sridhar et al., 2015). Twitter is an outstanding platform for large-scale stance analysis. However, unlike in the case of dedicated online debate platforms, Twitter data is much less structured and dialogical. Furthermore, as pointed out by Rajadesingan and Liu (2014), processing of tweets poses specific challenges stemming from the high volume of data, brevity of messages, and the use of non-standard language. In this paper, we describe a system for stance classification in tweets, with which we participated in the SemEval-2016 Task 6A. Given a tweet and the topic of the"
S16-1075,P11-1038,0,0.0113372,"ith a number of learning algorithms and also design a number of lexical and task-specific features. Subsequently, we reduce the algorithm and feature space by employing a series of optimization rounds. Finally, we train and fine-tune an ensemble of the chosen classifiers using a genetic algorithm. Following sections describe these steps in more detail. • • • 3.1 Features We first preprocess the data: we tokenize the tweets,1 stem the resulting tokens, and finally eliminate the stop words using the NLTK toolkit (Bird et al., 2009). In some configurations, we use a sentiment lexicon compiled by Han and Baldwin (2011) to replace all positive and negative sentiment-bearing words with dummy labels $POS$ and $NEG$, respectively. We compute two types of features: lexical features and task-specific features. We compute the former after preprocessing and the latter before preprocessing the data. The lexical features are as follows: • Word features – Word unigrams, bigrams, and trigrams, computed as (1) binary vectors, (2) countbased vectors, and (3) tf-idf-weighted vectors. We use a frequency cut-off of 2 and additionally filter based on class entropy with a cut-off set at 1.1; • Character features – Character b"
S16-1075,D14-1083,0,0.0334232,"f task-specific features. Our system ranked 3rd among the 19 systems submitted to this task. 1 Introduction Stance is the overall position held by a person towards an object, idea, or proposition (Somasundaran and Wiebe, 2009). The task of stance detection – the automatic classification of stance expressed in text – has been attracting increasing interest, as it is of practical interest to many stakeholders, ranging from political bodies to companies. Most recent work focused on stance detection in online debates (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Anand et al., 2011; Hasan and Ng, 2014; Sridhar et al., 2015). Twitter is an outstanding platform for large-scale stance analysis. However, unlike in the case of dedicated online debate platforms, Twitter data is much less structured and dialogical. Furthermore, as pointed out by Rajadesingan and Liu (2014), processing of tweets poses specific challenges stemming from the high volume of data, brevity of messages, and the use of non-standard language. In this paper, we describe a system for stance classification in tweets, with which we participated in the SemEval-2016 Task 6A. Given a tweet and the topic of the tweet (the target),"
S16-1075,S12-1060,1,0.659521,"Missing"
S16-1075,P09-1026,0,0.0222593,"nalysis and Knowledge Engineering Lab Unska 3, 10000 Zagreb, Croatia name.surname@fer.hr Abstract This paper describes our system for the detection of stances in tweets submitted to SemEval 2016 Task 6A. The system uses an ensemble of learning algorithms, fine-tuned using a genetic algorithm. We experiment with various offthe-shelf classifiers and build our model using standard lexical and a number of task-specific features. Our system ranked 3rd among the 19 systems submitted to this task. 1 Introduction Stance is the overall position held by a person towards an object, idea, or proposition (Somasundaran and Wiebe, 2009). The task of stance detection – the automatic classification of stance expressed in text – has been attracting increasing interest, as it is of practical interest to many stakeholders, ranging from political bodies to companies. Most recent work focused on stance detection in online debates (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Anand et al., 2011; Hasan and Ng, 2014; Sridhar et al., 2015). Twitter is an outstanding platform for large-scale stance analysis. However, unlike in the case of dedicated online debate platforms, Twitter data is much less structured and dialogic"
S16-1075,W10-0214,0,0.0113352,"ld our model using standard lexical and a number of task-specific features. Our system ranked 3rd among the 19 systems submitted to this task. 1 Introduction Stance is the overall position held by a person towards an object, idea, or proposition (Somasundaran and Wiebe, 2009). The task of stance detection – the automatic classification of stance expressed in text – has been attracting increasing interest, as it is of practical interest to many stakeholders, ranging from political bodies to companies. Most recent work focused on stance detection in online debates (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Anand et al., 2011; Hasan and Ng, 2014; Sridhar et al., 2015). Twitter is an outstanding platform for large-scale stance analysis. However, unlike in the case of dedicated online debate platforms, Twitter data is much less structured and dialogical. Furthermore, as pointed out by Rajadesingan and Liu (2014), processing of tweets poses specific challenges stemming from the high volume of data, brevity of messages, and the use of non-standard language. In this paper, we describe a system for stance classification in tweets, with which we participated in the SemEval-2016 Task 6A. Given a tweet"
S16-1075,P15-1012,0,0.0418238,"ures. Our system ranked 3rd among the 19 systems submitted to this task. 1 Introduction Stance is the overall position held by a person towards an object, idea, or proposition (Somasundaran and Wiebe, 2009). The task of stance detection – the automatic classification of stance expressed in text – has been attracting increasing interest, as it is of practical interest to many stakeholders, ranging from political bodies to companies. Most recent work focused on stance detection in online debates (Somasundaran and Wiebe, 2009; Somasundaran and Wiebe, 2010; Anand et al., 2011; Hasan and Ng, 2014; Sridhar et al., 2015). Twitter is an outstanding platform for large-scale stance analysis. However, unlike in the case of dedicated online debate platforms, Twitter data is much less structured and dialogical. Furthermore, as pointed out by Rajadesingan and Liu (2014), processing of tweets poses specific challenges stemming from the high volume of data, brevity of messages, and the use of non-standard language. In this paper, we describe a system for stance classification in tweets, with which we participated in the SemEval-2016 Task 6A. Given a tweet and the topic of the tweet (the target), the task was to predic"
S16-1075,S15-2113,0,0.0223866,"content of the corresponding word, following Sari´ et al. (2012). We also used the following task-specific features: • Counting features – The average word length, number of retweet symbols, number of hashes, number of emoticons, number of capitalized words, 1 http://sentiment.christopherpotts.net/codedata/happyfuntokenizing.py 2 https://code.google.com/archive/p/word2vec/ • and the number of exclamation marks; Repeated vowels – Whether the tweet contains at least one sequence of the same vowel longer than two characters. This feature is often employed in Twitter sentiment analysis, e.g., in (Xu et al., 2015); Number of misspelled words – The number of misspelled words, determined using the freelyavailable PyEnchant spellchecking library;3 Scripture citation – Our analysis of the dataset revealed that, for two out of five targets, namely Atheism and Legalization of Abortion, the users who quote the scriptures are by and large AGAINST the targets. To capture this regularity, we include a feature that checks whether the tweet matches some of the common scripture citation patterns, such as “Rom. 14:17” in “RT @prayerbullets: Let the righteousness, peace, and joy of the kingdom be established in my li"
S17-1014,D10-1115,0,0.504726,"n dataset. We find that the PLF works about as well for Croatian as for English, but demonstrate that its strength lies in modeling verbs, and that the free word order affects the less robust PLF variant. 1 Introduction Compositional distributional semantic models (CDSMs) represent phrase meaning in a vector space by composing the meanings of individual words. Many CDSMs were proposed, ranging from basic ones that use element-wise operations on word vectors to compute phrase vectors (Mitchell and Lapata, 2008), to more complex models that represent predicate arguments as higher-order tensors (Baroni and Zamparelli, 2010; Guevara, 2010). The latter models assume that predicates in a phrase act as functions that act on other phrase components to yield the final representation of the phrase. For example, an adjective acts as a function on the noun in an adjective-noun phrase, while a transitive verb acts as a binary function on its subject and object. However, since the number of parameters in a tensor grows exponentially with the number of arguments of the function that it models, learning full tensors for predicates with many arguments is tedious to impractical (Grefenstette et al., 2012). The Practical Lexic"
S17-1014,W10-2805,0,0.209741,"LF works about as well for Croatian as for English, but demonstrate that its strength lies in modeling verbs, and that the free word order affects the less robust PLF variant. 1 Introduction Compositional distributional semantic models (CDSMs) represent phrase meaning in a vector space by composing the meanings of individual words. Many CDSMs were proposed, ranging from basic ones that use element-wise operations on word vectors to compute phrase vectors (Mitchell and Lapata, 2008), to more complex models that represent predicate arguments as higher-order tensors (Baroni and Zamparelli, 2010; Guevara, 2010). The latter models assume that predicates in a phrase act as functions that act on other phrase components to yield the final representation of the phrase. For example, an adjective acts as a function on the noun in an adjective-noun phrase, while a transitive verb acts as a binary function on its subject and object. However, since the number of parameters in a tensor grows exponentially with the number of arguments of the function that it models, learning full tensors for predicates with many arguments is tedious to impractical (Grefenstette et al., 2012). The Practical Lexical Function mode"
S17-1014,S15-1017,1,0.848946,"ived vectors for predicate-argument combinations are a key part of the PLF, non-adjacency might make it difficult to estimate its parameters reliably for such languages. Secondly, the evaluation method reported by Paperno et al. (2014) uses a somewhat artificial setup by assuming that all phrase pairs, even ill-formed ones, can be graded for similarity. In this work we consider both of these questions. We investigate the application of PLF to Croatian language, a Slavic language with relatively free word order. We compare PLF with other, simpler CDSMs, as well as PLF modifications proposed by Gupta et al. (2015). In contrast to Paperno et al. (2014), we adopt lexical substitution as evaluation, building a new dataset of Croatian ANVAN phrases, 115 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 115–120, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics together with word substitutes for each word. The PLF model for Croatian performs comparably well to English, outperforming simpler CDSMs in particular at the verb position. Gupta et al. (2015) found both modifications to outperform simple baseline CDSMs for English w"
S17-1014,W13-3513,0,0.742936,"voditi suparniˇcka momˇcad (legendary coach lead opponent team) cijenjen (appreciated), izvanredan (outstanding), poznat (famous), uspješan (successful), znamenit (notable) dobar igraˇc dati pobjedniˇcki gol (good player score winning goal) pogoditi (to hit), posti´ci (to achieve), zabiti (to score), zadati (to give) sportski automobil prije´ci velika udaljenost (sports car travel large distance) dionica (section), dužina (length), put (way), razdaljina (distance) Table 1: Examples of ANVAN phrases with manually collected substitutes for boldfaced targets. pairs rated for semantic similarity (Kartsaklis et al., 2013; Grefenstette, 2013). The phrases in each pair differ only in the verb. Annotators rated the similarity on a scale from 1 to 7, and CDSMs were evaluated by correlating the ratings with the similarity of the predicted phrase vectors. The described approach is not appropriate when one or both ANVAN phrases are ungrammatical or nonsensical. Consider the following phrase pair in the ANVAN dataset by Kartsaklis et al. (2013): ‘dental service file false tooth’ – ‘dental service register false tooth’. While the first sentence is plausible, the second one is arguably somewhere between implausible and"
S17-1014,S07-1009,0,0.0492604,"sider the following phrase pair in the ANVAN dataset by Kartsaklis et al. (2013): ‘dental service file false tooth’ – ‘dental service register false tooth’. While the first sentence is plausible, the second one is arguably somewhere between implausible and nonsensical. We believe that semantic similarity is not a reasonable evaluation criterion for such (relatively frequent) cases. For our experiment, we chose a word-choice evaluation setup, which essentially builds on the idea of lexical substitution. Lexical substitution is the task of identifying a substitute for a word in a given context (McCarthy and Navigli, 2007). Typically, a system is presented with a phrase and candidate substitutes for a target word in the phrase and needs to select one or more adequate substitutes. Systems either have to rank the candidates in the appropriate order (McCarthy and Navigli, 2007; Sinha and Mihalcea, 2009), or just choose one best substitute (Melamud et al., 2016). An additional benefit of a lexical substitution setup is that we can evaluate the predictions of the model not just globally, but at the level of individual words. We will exploit that possibility below. Croatian ANVAN dataset. We constructed individual AN"
S17-1014,K16-1006,0,0.206901,"or such (relatively frequent) cases. For our experiment, we chose a word-choice evaluation setup, which essentially builds on the idea of lexical substitution. Lexical substitution is the task of identifying a substitute for a word in a given context (McCarthy and Navigli, 2007). Typically, a system is presented with a phrase and candidate substitutes for a target word in the phrase and needs to select one or more adequate substitutes. Systems either have to rank the candidates in the appropriate order (McCarthy and Navigli, 2007; Sinha and Mihalcea, 2009), or just choose one best substitute (Melamud et al., 2016). An additional benefit of a lexical substitution setup is that we can evaluate the predictions of the model not just globally, but at the level of individual words. We will exploit that possibility below. Croatian ANVAN dataset. We constructed individual ANVAN phrases for Croatian like in prior English work (Kartsaklis et al., 2013; Grefenstette, 2013). We started by choosing six transitive verbs from the list of polysemous verbs on the Croatian language portal.2 We chose verbs with high polysemy level, while avoiding those that overlap in semantic meaning. The list consist of the following v"
S17-1014,P08-1028,0,0.712906,"ver in free word order languages. We evaluate variants of the PLF for Croatian, using a new lexical substitution dataset. We find that the PLF works about as well for Croatian as for English, but demonstrate that its strength lies in modeling verbs, and that the free word order affects the less robust PLF variant. 1 Introduction Compositional distributional semantic models (CDSMs) represent phrase meaning in a vector space by composing the meanings of individual words. Many CDSMs were proposed, ranging from basic ones that use element-wise operations on word vectors to compute phrase vectors (Mitchell and Lapata, 2008), to more complex models that represent predicate arguments as higher-order tensors (Baroni and Zamparelli, 2010; Guevara, 2010). The latter models assume that predicates in a phrase act as functions that act on other phrase components to yield the final representation of the phrase. For example, an adjective acts as a function on the noun in an adjective-noun phrase, while a transitive verb acts as a binary function on its subject and object. However, since the number of parameters in a tensor grows exponentially with the number of arguments of the function that it models, learning full tenso"
S17-1014,P14-1009,0,0.341511,"tter models assume that predicates in a phrase act as functions that act on other phrase components to yield the final representation of the phrase. For example, an adjective acts as a function on the noun in an adjective-noun phrase, while a transitive verb acts as a binary function on its subject and object. However, since the number of parameters in a tensor grows exponentially with the number of arguments of the function that it models, learning full tensors for predicates with many arguments is tedious to impractical (Grefenstette et al., 2012). The Practical Lexical Function model (PLF, Paperno et al. (2014)) strikes a middle ground by breaking down all tensors with ranks higher than two into multiple matrices, each representing the predicate’s composition with a single argument (cf. Section 2 for details). In the experiments of Paperno et al. (2014), PLF has been shown to work better than some other CDSMs in modeling semantic similarity. Particularly good results were obtained on ANVAN (adjective-noun-verb-adjectivenoun) phrases, where PLF outperformed both simple CDSMs (due to its higher expressiveness) as well as the higher-order Lexical Function model (Baroni and Zamparelli, 2010). Although t"
S17-1014,R09-1073,0,0.0182613,"semantic similarity is not a reasonable evaluation criterion for such (relatively frequent) cases. For our experiment, we chose a word-choice evaluation setup, which essentially builds on the idea of lexical substitution. Lexical substitution is the task of identifying a substitute for a word in a given context (McCarthy and Navigli, 2007). Typically, a system is presented with a phrase and candidate substitutes for a target word in the phrase and needs to select one or more adequate substitutes. Systems either have to rank the candidates in the appropriate order (McCarthy and Navigli, 2007; Sinha and Mihalcea, 2009), or just choose one best substitute (Melamud et al., 2016). An additional benefit of a lexical substitution setup is that we can evaluate the predictions of the model not just globally, but at the level of individual words. We will exploit that possibility below. Croatian ANVAN dataset. We constructed individual ANVAN phrases for Croatian like in prior English work (Kartsaklis et al., 2013; Grefenstette, 2013). We started by choosing six transitive verbs from the list of polysemous verbs on the Croatian language portal.2 We chose verbs with high polysemy level, while avoiding those that overl"
S17-1014,P13-2137,1,0.918073,"guistics together with word substitutes for each word. The PLF model for Croatian performs comparably well to English, outperforming simpler CDSMs in particular at the verb position. Gupta et al. (2015) found both modifications to outperform simple baseline CDSMs for English when evaluated on ANVAN datasets, with test adaptation outperforming the original PLF. 2 PLF for Croatian. We implemented the basic PLF and the two above-mentioned modifications for Croatian following the procedure described by Paperno et al. (2014). As a corpus for building word and phrase lexical vectors we used fHrWaC (Šnajder et al., 2013), a filtered version of Croatian web corpus (Ljubeši´c and Erjavec, 2011), totaling 51M sentences and 1.2B tokens. The corpus has been parsed using the MSTParser for Croatian (Agi´c and Merkler, 2013). As a first step in obtaining word vector representations, we extracted a co-occurrence matrix of 30K most frequent lemmas (nouns, verbs, and adjectives) in corpus, using a window of size 3. Next, the vectors contained in the resulting matrix were transformed using Positive Pointwise Mutual Information (PPMI) and reduced to size 300 using Singular Value Decomposition. Finally, all vectors in the"
S17-2055,J90-1003,0,0.110741,"aum, 1998); 3.6 • Latent Semantic Analysis (Landauer et al., 1998); We follow the work of Severyn and Moschitti (2015). The overall architecture of the network includes two convolutional layers and the corresponding information flow:7 • Latent Dirichlet Allocation (Blei et al., 2003); • The question q and comment c at the input are represented as matrices containing the word2vec embeddings of their words. They are the input of two separate convolutional layers that perform feature extraction; • BLEU (Papineni et al., 2002); • Meteor (Denkowski and Lavie, 2011); • Pointwise Mutual Information (Church and Hanks, 1990). 3.4 CNN • The max-pooling operation is applied to the resulting feature-maps; Other Features In this group we list hand-crafted features, as well as features that do not fit into the previous two groups: • This results in task-specific representation vectors of the question and the comment. These vectors are concatenated and fed into a fully connected hidden layer, along with other features that we wish to include. Note that the extra features could be especially helpful in cases where many words in q and c that are not covered by the word2vec model, which may lead to meaningless features ex"
S17-2055,W11-2107,0,0.0156992,"s relevant, when presented (q, c) as input. • WordNet (Fellbaum, 1998); 3.6 • Latent Semantic Analysis (Landauer et al., 1998); We follow the work of Severyn and Moschitti (2015). The overall architecture of the network includes two convolutional layers and the corresponding information flow:7 • Latent Dirichlet Allocation (Blei et al., 2003); • The question q and comment c at the input are represented as matrices containing the word2vec embeddings of their words. They are the input of two separate convolutional layers that perform feature extraction; • BLEU (Papineni et al., 2002); • Meteor (Denkowski and Lavie, 2011); • Pointwise Mutual Information (Church and Hanks, 1990). 3.4 CNN • The max-pooling operation is applied to the resulting feature-maps; Other Features In this group we list hand-crafted features, as well as features that do not fit into the previous two groups: • This results in task-specific representation vectors of the question and the comment. These vectors are concatenated and fed into a fully connected hidden layer, along with other features that we wish to include. Note that the extra features could be especially helpful in cases where many words in q and c that are not covered by the"
S17-2055,S15-2036,0,0.0213242,"n a question q and a comment list C, the task is to rank the comments in C according to their relevance with respect to q. Datasets for this task were extracted from Qatar Living, a web forum where people pose questions about various aspects of their daily life in Qatar. Following Filice et al. (2016), we framed the task as a binary classification problem. We experimented with two classification approaches – Support Vector Machines (SVM) (Cortes and Vapnik, 1995) and Convolutional Neural Networks (CNN) (Kim, 2014). Most of the features we use follow the work of Barrón-Cedeno et al. (2015) and Nicosia et al. (2015). Moreover, we use embedding-based (Mihaylov and Nakov, 2016) features for both models. The CNN model with the full feature set has proven to be the most successful, ranking 10th in the competition with a MAP-score of 81.14 and an F1-score of 66.99. The rest of this paper is structured as follows. Section 2 gives a brief overview of the data set. A detailed description of the our models is given in Section 3. Section 4 outlines our experiments and results. Finally, we present our conclusions in Section 5. 2 Dataset The dataset we used was provided by the sharedtask organizers. Incoming user qu"
S17-2055,P02-1040,0,0.0977447,"he SVM classifier that the class is relevant, when presented (q, c) as input. • WordNet (Fellbaum, 1998); 3.6 • Latent Semantic Analysis (Landauer et al., 1998); We follow the work of Severyn and Moschitti (2015). The overall architecture of the network includes two convolutional layers and the corresponding information flow:7 • Latent Dirichlet Allocation (Blei et al., 2003); • The question q and comment c at the input are represented as matrices containing the word2vec embeddings of their words. They are the input of two separate convolutional layers that perform feature extraction; • BLEU (Papineni et al., 2002); • Meteor (Denkowski and Lavie, 2011); • Pointwise Mutual Information (Church and Hanks, 1990). 3.4 CNN • The max-pooling operation is applied to the resulting feature-maps; Other Features In this group we list hand-crafted features, as well as features that do not fit into the previous two groups: • This results in task-specific representation vectors of the question and the comment. These vectors are concatenated and fed into a fully connected hidden layer, along with other features that we wish to include. Note that the extra features could be especially helpful in cases where many words i"
S17-2055,S16-1172,0,0.0397591,"Missing"
S17-2055,P13-4028,0,0.0309032,"f both the question and comment by averaging their corresponding content-word vectors. This yields two 300dimensional vectors, which are fed into our models as 600 numerical features; • Word2vec average cosine similarity – we compute vector representations of the question and comment in the same manner as for the previous feature, but using word2vec word vectors. We then introduce into our models a single numerical feature computed as the cosine similarity of the question and comment vector representations. 3.3 SEMILAR Features Features listed under this group were all obtained from SEMILAR5 (Rus et al., 2013). This is a library that implements a multitude of popular semantic similarity measures for text. We use it to calculate the similarity of the question to the candidate comment, and include each measure as a 3 These vectors alone can produce a MAP-score of 78.45 on subtask A, and can be obtained from https://github. com/tbmihailov/semeval2016-task3-cqa 4 We use the SL999 variant available at – http://ttic. uchicago.edu/~wieting/ 5 http://deeptutor2.memphis.edu/ Semilar-Web/ numerical feature. In our experiments we include similarity measures based on: 3.5 • Lexical overlap – two variants, over"
S17-2055,D14-1181,0,0.0287978,"entries for the SemEval 2017 Question-Comment Similarity subtask (Nakov et al., 2017). Given a question q and a comment list C, the task is to rank the comments in C according to their relevance with respect to q. Datasets for this task were extracted from Qatar Living, a web forum where people pose questions about various aspects of their daily life in Qatar. Following Filice et al. (2016), we framed the task as a binary classification problem. We experimented with two classification approaches – Support Vector Machines (SVM) (Cortes and Vapnik, 1995) and Convolutional Neural Networks (CNN) (Kim, 2014). Most of the features we use follow the work of Barrón-Cedeno et al. (2015) and Nicosia et al. (2015). Moreover, we use embedding-based (Mihaylov and Nakov, 2016) features for both models. The CNN model with the full feature set has proven to be the most successful, ranking 10th in the competition with a MAP-score of 81.14 and an F1-score of 66.99. The rest of this paper is structured as follows. Section 2 gives a brief overview of the data set. A detailed description of the our models is given in Section 3. Section 4 outlines our experiments and results. Finally, we present our conclusions i"
S17-2055,S15-2047,0,0.143316,"2017). Given a question q and a comment list C, the task is to rank the comments in C according to their relevance with respect to q. Datasets for this task were extracted from Qatar Living, a web forum where people pose questions about various aspects of their daily life in Qatar. Following Filice et al. (2016), we framed the task as a binary classification problem. We experimented with two classification approaches – Support Vector Machines (SVM) (Cortes and Vapnik, 1995) and Convolutional Neural Networks (CNN) (Kim, 2014). Most of the features we use follow the work of Barrón-Cedeno et al. (2015) and Nicosia et al. (2015). Moreover, we use embedding-based (Mihaylov and Nakov, 2016) features for both models. The CNN model with the full feature set has proven to be the most successful, ranking 10th in the competition with a MAP-score of 81.14 and an F1-score of 66.99. The rest of this paper is structured as follows. Section 2 gives a brief overview of the data set. A detailed description of the our models is given in Section 3. Section 4 outlines our experiments and results. Finally, we present our conclusions in Section 5. 2 Dataset The dataset we used was provided by the sharedtask or"
S17-2055,S16-1136,0,0.0457586,"Missing"
S17-2055,S17-2003,0,0.02624,"th on the SemEval 2017 task 3, subtask A. 1 Introduction The ever-growing Community Question Answering (CQA) on-line services are gaining popularity at an increasing rate. However, there are some problems inherent to question-answer collections created by on-line communities. A major issue is the sheer volume of CQA collections, which makes finding an answer to a user question infeasible without some kind of an automated retrieval system. Consequently, information retrieval on CQA collections has gained increased focus in the research community, giving rise to several shared tasks on SemEval (Nakov et al., 2017). From a natural language processing perspective, this is a difficult task due to high variance in the quality of questions and answers in CQA collections (Màrquez et al., 2015). The cause of this is the self-moderated nature of CQA sites, which implies that there are few restrictions on who is allowed to answer a question. In this paper we describe our entries for the SemEval 2017 Question-Comment Similarity subtask (Nakov et al., 2017). Given a question q and a comment list C, the task is to rank the comments in C according to their relevance with respect to q. Datasets for this task were ex"
S17-2055,S16-1083,0,0.0208273,"ance boosts on the development set: System Description Considering we have class labels available for each question-comment pair, it is natural to frame this ranking task as a supervised classification problem. The input of the classifier is a vector of features that represents the question-comment pair (q, c). The output is a class – relevant or non-relevant – indicating whether c is relevant with respect to q. We have experimented with two supervised approaches for classifying the data, SVM and CNN, which were shown to be very successful in questioncomment re-ranking tasks in previous work (Nakov et al., 2016). Note that both of these variants of our system fall into the pointwise category of the learning-to-rank paradigm (Cao et al., 2007). We next describe all our systems’ components in detail. 3.1 Preprocessing We have preprocessed all entries by tokenizing them, stemming the tokens, and removing stopwords using the NLTK toolkit.2 These tokens were used as input to the feature extraction pipeline. 2 Embedding-Based Features http://www.nltk.org 340 • Question and Answer Embeddings – using the PARAGRAM embeddings, we compute the vector representation of both the question and comment by averaging t"
S17-2055,P14-1146,0,0.0424691,"Missing"
S17-2066,P13-2044,0,0.0310149,"nding humor expressed in text is a challenging natural language problem. Besides standard ambiguity of natural language, humor is also highly subjective and lacks an universal definition (Mihalcea and Strapparava, 2005). Moreover, humor should almost never be taken at face value, as its understanding often requires a broader context – external knowledge and common sense. On top of that, what is funny today might not be funny tomorrow, as humor goes hand in hand with ever-changing trends of popular culture. Even though there has been some work on humor generation (Petrovi´c and Matthews, 2013; Valitutti et al., 2013), most work has been concerned with humor detection, a task of classifying whether a given text snippet is humorous (Mihalcea and Strapparava, 2005; Kiddon and Brun, 2011; Yang et al., 2015; Chen and Lee, 2017). However, this research was mostly focused on a simple binary detection of humor. In this paper, we describe a system for ranking humor in tweets, which we participated with 1 http://www.cc.com/shows/-midnight 396 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 396–400, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computation"
S17-2066,D15-1284,0,0.0686899,"a and Strapparava, 2005). Moreover, humor should almost never be taken at face value, as its understanding often requires a broader context – external knowledge and common sense. On top of that, what is funny today might not be funny tomorrow, as humor goes hand in hand with ever-changing trends of popular culture. Even though there has been some work on humor generation (Petrovi´c and Matthews, 2013; Valitutti et al., 2013), most work has been concerned with humor detection, a task of classifying whether a given text snippet is humorous (Mihalcea and Strapparava, 2005; Kiddon and Brun, 2011; Yang et al., 2015; Chen and Lee, 2017). However, this research was mostly focused on a simple binary detection of humor. In this paper, we describe a system for ranking humor in tweets, which we participated with 1 http://www.cc.com/shows/-midnight 396 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 396–400, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 3 Model Collection Movie titles Song names Book titles TV series titles Cartoon titles People information One-line jokes Curse words We tackle both subtasks with a single bas"
S17-2066,P11-2016,0,0.159684,"sal definition (Mihalcea and Strapparava, 2005). Moreover, humor should almost never be taken at face value, as its understanding often requires a broader context – external knowledge and common sense. On top of that, what is funny today might not be funny tomorrow, as humor goes hand in hand with ever-changing trends of popular culture. Even though there has been some work on humor generation (Petrovi´c and Matthews, 2013; Valitutti et al., 2013), most work has been concerned with humor detection, a task of classifying whether a given text snippet is humorous (Mihalcea and Strapparava, 2005; Kiddon and Brun, 2011; Yang et al., 2015; Chen and Lee, 2017). However, this research was mostly focused on a simple binary detection of humor. In this paper, we describe a system for ranking humor in tweets, which we participated with 1 http://www.cc.com/shows/-midnight 396 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 396–400, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics 3 Model Collection Movie titles Song names Book titles TV series titles Cartoon titles People information One-line jokes Curse words We tackle both subtask"
S17-2066,S17-2132,1,0.761153,"Missing"
S17-2066,H05-1067,0,0.288547,"d into subtasks 6A and 6B. In the first subtask, participants must recognize the more humorous tweet of the two, whereas the second subtask asks for a complete tripartite ranking of all tweets under a given hashtag. This basically means that the order of the tweets is not important as long they are placed in the correct bin. For more details consult (Potash et al., 2017). While extremely interesting, understanding humor expressed in text is a challenging natural language problem. Besides standard ambiguity of natural language, humor is also highly subjective and lacks an universal definition (Mihalcea and Strapparava, 2005). Moreover, humor should almost never be taken at face value, as its understanding often requires a broader context – external knowledge and common sense. On top of that, what is funny today might not be funny tomorrow, as humor goes hand in hand with ever-changing trends of popular culture. Even though there has been some work on humor generation (Petrovi´c and Matthews, 2013; Valitutti et al., 2013), most work has been concerned with humor detection, a task of classifying whether a given text snippet is humorous (Mihalcea and Strapparava, 2005; Kiddon and Brun, 2011; Yang et al., 2015; Chen"
S17-2066,P13-2041,0,0.283707,"Missing"
S17-2066,S17-2004,0,0.070946,"Missing"
S17-2132,C10-2005,0,0.129693,"Missing"
S17-2132,D14-1162,0,0.0937909,"process tweets and extract various features. We use standard features such as bag-of-words (more precisely tf-idf), pre784 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 784–789, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics trained word embeddings, and count-based stylistic features. Additionally, we design task-specific features based on publicly available ratings for certain topics. We next describe the preprocessing and the features in more detail. 2.1 Word embeddings. For word embeddings we use GloVe (Pennington et al., 2014). We use 200dimensional word embeddings, pretrained on 2B tweets. Final vector representation of a tweet is calculated as an average of the sum of vectors of all the words in a tweet. Preprocessing of Tweets Tf–idf. The standard tf–idf vectorizer from Python’s scikit–learn package.2 As a first preprocessing step, we tokenize tweets using a basic Twitter-adapted tokenizer.1 After tokenization, we lemmatize and stem tweets and remove stopwords from each tweet using the NLTK toolkit (Bird et al., 2009). Additionally, since some of our features require recognizing named entities in a tweet, we use"
S17-2132,S17-2088,0,0.0647703,"Missing"
S17-2132,C14-1008,0,0.0436562,"Missing"
S17-2132,S14-2033,0,0.056429,"Missing"
S17-2132,S17-2066,1,0.760525,"Missing"
S17-2132,W02-1011,0,0.0334173,"btasks A, B, and D. Our main focus was topic-based message polarity classification on a two-point scale (subtask B). The system we submitted uses a Support Vector Machine classifier with rich set of features, ranging from standard to more creative, task-specific features, including a series of rating-based features as well as features that account for sentimental reminiscence of past topics and deceased famous people. Our system ranked 14th out of 39 submissions in subtask A, 5th out of 24 submissions in subtask B, and 3rd out of 16 submissions in subtask D. 1 Introduction Sentiment analysis (Pang et al., 2002), a task of determining polarity of text towards some topic, recently gained a lot of interest, mostly due to its applicability in various fields, such as public relations (Pang et al., 2008) and market analysis (He et al., 2013). Following the growing popularity of social networks and an increasing number of user comments that can be found there, sentiment analysis of texts on social networks, such as tweets from Twitter, has been the focus of much research. However, determining the sentiment of a tweet is often not an easy task, since the length of the tweet is limited and language is mostly"
S17-2148,D15-1089,0,0.0213597,"ially within recent SemEval campaigns (Nakov et al., 2016; 866 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 866–871, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics Rosenthal et al., 2015, 2014). A large body of recent work focuses on sentence-level sentiment prediction. Socher et al. (2012) and Socher et al. (2013) reported impressive results working with matrix-vector recursive neural network (MV-RNN) and recursive neural tensor networks models over sentence parse trees. Working with sentence parse trees Kim et al. (2015) and Srivastava et al. (2013) obtained competitive results using tree kernels as an alternative to recursive neural networks. These methods, while producing promising results, are highly dependent on parse trees. In practice, we often work with informal texts, where syntactic parsing often produces inaccurate results, which in turn heavily affects performances of the aforementioned methods. Furthermore, as noted by Le and Mikolov (2014), it is not straightforward how to extend these methods when working with text spans that range over multiple sentences. There has been a growing amount of inte"
S17-2148,S15-2078,0,0.0328476,"lar to social media posts, finance news are short texts, but, unlike social media posts, the text is edited and hence grammatically correct. On the other hand, news headlines are notorious for 2 Related Work There has been considerable research focusing on sentiment analysis of short texts (Thelwall et al., 2010; Kiritchenko et al., 2014), especially within recent SemEval campaigns (Nakov et al., 2016; 866 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 866–871, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics Rosenthal et al., 2015, 2014). A large body of recent work focuses on sentence-level sentiment prediction. Socher et al. (2012) and Socher et al. (2013) reported impressive results working with matrix-vector recursive neural network (MV-RNN) and recursive neural tensor networks models over sentence parse trees. Working with sentence parse trees Kim et al. (2015) and Srivastava et al. (2013) obtained competitive results using tree kernels as an alternative to recursive neural networks. These methods, while producing promising results, are highly dependent on parse trees. In practice, we often work with informal text"
S17-2148,S14-2009,0,0.146358,"Missing"
S17-2148,P11-1015,0,0.435946,"niversity of Zagreb Unska 3, 10000 Zagreb, Croatia {leon.rotim,martin.tutek,jan.snajder}@fer.hr Abstract the use of a specific language (Reah, 2002), which is often elliptical and compressed, and thus differs from the language used in the rest of the news story. Many approaches to sentiment analysis resort to rich, domain-specific, hand-crafted features (Wilson et al., 2009; Abbasi et al., 2008). At the same time, there has been a growing interest in featurelight methods, including kernel-methods (Culotta and Sorensen, 2004; Lodhi et al., 2002a; Srivastava et al., 2013) and neural embeddings (Maas et al., 2011; Socher et al., 2013). These methods alleviate the need for manual creation of domain-specific features, while maintaining high accuracy. Most of the recently published work focuses on sentiment analysis problems that are framed as a classification task, while fine-grained analysis is framed as a regression problem. However, most of the high performing classification methods can be easily tuned to perform regression. In this work we focus on feature-light methods as they do not require complex, time consuming feature engineering. More specifically, we focus on string kernels (Lodhi et al., 20"
S17-2148,D12-1110,0,0.047678,"and hence grammatically correct. On the other hand, news headlines are notorious for 2 Related Work There has been considerable research focusing on sentiment analysis of short texts (Thelwall et al., 2010; Kiritchenko et al., 2014), especially within recent SemEval campaigns (Nakov et al., 2016; 866 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 866–871, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics Rosenthal et al., 2015, 2014). A large body of recent work focuses on sentence-level sentiment prediction. Socher et al. (2012) and Socher et al. (2013) reported impressive results working with matrix-vector recursive neural network (MV-RNN) and recursive neural tensor networks models over sentence parse trees. Working with sentence parse trees Kim et al. (2015) and Srivastava et al. (2013) obtained competitive results using tree kernels as an alternative to recursive neural networks. These methods, while producing promising results, are highly dependent on parse trees. In practice, we often work with informal texts, where syntactic parsing often produces inaccurate results, which in turn heavily affects performances"
S17-2148,P14-5010,0,0.00526081,"popular being the spectrum kernel (SK) (Leslie et al., 2002) and the subsequence kernel (SSK) (Lodhi et al., 2002a). The SSK measures string similarity by first mapping each input string s to: X ϕu (s) = λl(i) (1) since most of the words appearing in news headlines are title-cased. We refer to this method as the word embeddings method (WEM). We further experimented with additional filtering of the word tokens we use for building word embedding vectors. Our motivation was based on the observation that sentiment-bearing words typically exclude the named entities. We therefore used StanfordNLP (Manning et al., 2014) named entity recognition (NER) tools to filter out all named entities before building adding up the word embedding vectors. We refer to this method as the filtered word embeddings method (FWEM). When using word embeddings as features, we experimented with the linear, RBF, and cosine kernel (CK). The latter is defined as: i:u=s[i] where u is a subsequence searched for in s, i is a vector of indices at which u appears in s, l is a function measuring the length of a matched subsequence and λ ≤ 1 is a weighting parameter giving lower weights to longer subsequences. Using (1), the SSK kernel is de"
S17-2148,D13-1170,0,0.0949658,"Unska 3, 10000 Zagreb, Croatia {leon.rotim,martin.tutek,jan.snajder}@fer.hr Abstract the use of a specific language (Reah, 2002), which is often elliptical and compressed, and thus differs from the language used in the rest of the news story. Many approaches to sentiment analysis resort to rich, domain-specific, hand-crafted features (Wilson et al., 2009; Abbasi et al., 2008). At the same time, there has been a growing interest in featurelight methods, including kernel-methods (Culotta and Sorensen, 2004; Lodhi et al., 2002a; Srivastava et al., 2013) and neural embeddings (Maas et al., 2011; Socher et al., 2013). These methods alleviate the need for manual creation of domain-specific features, while maintaining high accuracy. Most of the recently published work focuses on sentiment analysis problems that are framed as a classification task, while fine-grained analysis is framed as a regression problem. However, most of the high performing classification methods can be easily tuned to perform regression. In this work we focus on feature-light methods as they do not require complex, time consuming feature engineering. More specifically, we focus on string kernels (Lodhi et al., 2002b) and methods using"
S17-2148,D13-1144,0,0.182302,"culty of Electrical Engineering and Computing, University of Zagreb Unska 3, 10000 Zagreb, Croatia {leon.rotim,martin.tutek,jan.snajder}@fer.hr Abstract the use of a specific language (Reah, 2002), which is often elliptical and compressed, and thus differs from the language used in the rest of the news story. Many approaches to sentiment analysis resort to rich, domain-specific, hand-crafted features (Wilson et al., 2009; Abbasi et al., 2008). At the same time, there has been a growing interest in featurelight methods, including kernel-methods (Culotta and Sorensen, 2004; Lodhi et al., 2002a; Srivastava et al., 2013) and neural embeddings (Maas et al., 2011; Socher et al., 2013). These methods alleviate the need for manual creation of domain-specific features, while maintaining high accuracy. Most of the recently published work focuses on sentiment analysis problems that are framed as a classification task, while fine-grained analysis is framed as a regression problem. However, most of the high performing classification methods can be easily tuned to perform regression. In this work we focus on feature-light methods as they do not require complex, time consuming feature engineering. More specifically, we"
S17-2148,D14-1162,0,0.102195,"Missing"
S17-2148,P14-1146,0,0.0328321,"ing often produces inaccurate results, which in turn heavily affects performances of the aforementioned methods. Furthermore, as noted by Le and Mikolov (2014), it is not straightforward how to extend these methods when working with text spans that range over multiple sentences. There has been a growing amount of interest in methods that are not based on syntax. The most promising results have been achieved using neural word embeddings (Mikolov et al., 2013a), while string kernels (Zhang et al., 2008; Lodhi et al., 2002a; Leslie et al., 2002) offer a viable alternative. Maas et al. (2011) and Tang et al. (2014) reported promising results by learning sentiment specific word embeddings. By extending word embeddings to more complex paragraph embeddings Le and Mikolov (2014) reported state-of-the-art results on sentiment classification for both short and long English texts. Building on word embeddings, Joulin et al. (2016) developed an end-to-end, domain independent, high-performance text classification model. 3 Dataset Our task was, given a news headline, to predict the sentiment score for a specific company mentioned in the headline. The dataset consisted of the name of the company, the text of the ne"
S17-2148,P12-3020,0,0.030622,"29 teams and 4th out of 45 submissions, with a cosine score of 0.733. 1 Introduction Sentiment analysis (Pang and Lee, 2008) is a task of predicting whether the text expresses a positive, negative, or neutral opinion in general or with respect to an entity of interest. Developing systems capable of performing highly accurate sentiment analysis has attracted considerable attention over the last two decades. The topic has been one of the main research areas in recent shared tasks, with main focus on social media texts, which are of particular interest for social studies (O’Connor et al., 2010; Wang et al., 2012) and marketing analysis (He et al., 2013; Yu et al., 2013). At the same time, social media texts pose a big challenge for sentiment analysis due to their short, informal and often ungrammatical format. This work focuses on the second subtask of SemEval-2017 Task 5, which aims to perform finegrained sentiment analysis of the financial news. Given that sentiments can affect market dynamics (Goonatilake and Herath, 2007; Van de Kauter et al., 2015), sentiment analysis of financial news can be a powerful tool for predicting market reactions. Similar to social media posts, finance news are short te"
S17-2148,J09-3003,0,0.165715,"mEval-2017 Task 5: Linear Aggregation of Word Embeddings for Fine-Grained Sentiment Analysis on Financial News Leon Rotim, Martin Tutek, Jan Šnajder Text Analysis and Knowledge Engineering Lab Faculty of Electrical Engineering and Computing, University of Zagreb Unska 3, 10000 Zagreb, Croatia {leon.rotim,martin.tutek,jan.snajder}@fer.hr Abstract the use of a specific language (Reah, 2002), which is often elliptical and compressed, and thus differs from the language used in the rest of the news story. Many approaches to sentiment analysis resort to rich, domain-specific, hand-crafted features (Wilson et al., 2009; Abbasi et al., 2008). At the same time, there has been a growing interest in featurelight methods, including kernel-methods (Culotta and Sorensen, 2004; Lodhi et al., 2002a; Srivastava et al., 2013) and neural embeddings (Maas et al., 2011; Socher et al., 2013). These methods alleviate the need for manual creation of domain-specific features, while maintaining high accuracy. Most of the recently published work focuses on sentiment analysis problems that are framed as a classification task, while fine-grained analysis is framed as a regression problem. However, most of the high performing cla"
S18-1135,A00-2018,0,0.431103,"hree contexts represent the words before, between, and after the entity mention pair. Every feature describing a word contains the information about which context it is located in. It also additionally contains the position of the word relative to the start of that context. While this increases the total number of parameters, it also provides information about the word ordering in the context independently of the size of other contexts. This encoding scheme is used to create features of word tokens, part-of-speech tags, and named entities. Similarly as Chen et al. (2006), who in turn followed Charniak (2000) and Zhang (2004), we experimented with additional constituency parse features describing grammatical functions and chunk tag information for all five contexts, and IOB-chains of the heads of the two entities. However, as preliminary experiments showed that these additional features do not provide any performance gains, we decided not to include them in our final models, intending to evaluate these results in future work. domain (G´abor et al., 2018) to be used for training the models. All entities representing domain concepts (e.g., word sense disambiguation and translation) are already annot"
S18-1135,S18-1111,0,0.0562093,"Missing"
S18-1135,P17-1007,0,0.0188069,"l. The results obtained this way were better then using a 6-way classification model; while this appears counter-intuitive, we leave a more thorough investigation for future work. Once the predicted classes are obtained, we map regular and reversed relation types into one class, as in the original task. Considering that C OMPARE relation cannot be Word embeddings and OOV words. To capture the semantic meaning of the words we used 300-dimensional pre-trained Google word2vec word embeddings.2 In accordance with standard practice, which leverages the additive compositionality of word embeddings (Gittens et al., 2017), we represent each context as a sum of its word embeddings. As we divided the sentence 1 2 3 A lexical substitute is a meaning-preserving replacement of a word in its context. https://spacy.io/ https://code.google.com/archive/p/word2vec/ 844 reversed, we perform a post-prediction correction if a relation instance has an active reverse flag in the test set but the model predicted the instance to be of the C OMPARE class. In this case, we change the prediction for that instance to be the second most probable class according to the predicted class probabilities. 3.2 distance from the entity ment"
S18-1135,P05-1053,0,0.104663,"lation extraction is typically framed as a classification task: pairs of entities from a document are inspected and the type of relation is predicted by means of local linguistic cues (Culotta et al., 2006). Relation extraction has been extensively studied in the literature; see (Konstantinova, 2014) for a comprehensive overview. Etzioni et al. (2008) group the relation extraction approaches into three classes: (1) knowledge-based methods, (2) supervised methods, and (3) self-supervised methods. Traditional approaches mostly relied on shallow machine learning models with handcrafted features (GuoDong et al., 2005) and specific kernel methods (Zelenko et al., 2003). Some systems 2 Dataset The organizers have provided different training datasets for scenarios 1.1 and 1.2 of subtask 1, each consisting of 350 abstracts of scientific papers from the Natural Language Processing (NLP) 842 Proceedings of the 12th International Workshop on Semantic Evaluation (SemEval-2018), pages 842–847 New Orleans, Louisiana, June 5–6, 2018. ©2018 Association for Computational Linguistics entities involved in the relation, while the remaining three contexts represent the words before, between, and after the entity mention pa"
S18-1135,P04-1053,0,0.216293,"Missing"
S18-1135,W09-2415,0,0.110284,"Missing"
S18-1135,K16-1006,0,0.0337041,"lead to substantial performance improvement. into five context, the result is five vectors, which are subsequently concatenated into a single 1500dimensional vector. Using a pre-trained word2vec model combined with a scientific domain dataset led to many outof-vocabulary (OOV) words. After some very basic word preprocessing, e.g., handling of hyphens and underscores, 1108 out of 9319 unique words from the datasets (combined datasets of subtasks 1.1. and 1.2) remained uncovered by the word2vec model. To tackle the OOV problem, we experimented with a fallback mechanism based on the context2vec (Melamud et al., 2016), an off-theshelf lexical substitution system. Our idea was to retrieve the lexical substitutes for each OOV word3 and retrieve their vectors instead – a technique akin to query expansion in information retrieval. We experimented with a number of variants of this method, however as none led to performance improvements, we decided not to include lexical substitution fallback in the final model. Reversed relations. We use a boolean feature to indicate the reverse direction of a relation in an instance, as provided in the training datasets. With the addition of that feature the model can more eas"
S18-1135,W15-1506,0,0.317964,"CNN model. We next describe these models in more detail. 3.1 SVM Model Our best-performing model uses an SVM classifier with a combination of sparse and dense features. Sparse feature encoding. To encode the sparse features, we adopt the method of Chen et al. (2006) who divide the sentence into contexts. In a nutshell, the method consists in describing the context in which the entities occur by dividing the sentence into five contexts around the entity mentions. Two of these five contexts are related to the Dependency features. Many systems from the literature make use of dependency features (Nguyen and Grishman, 2015a; Xu et al., 2016, 2015), which, intuitively, should be useful for re843 Subtask 1.1 Subtask 1.2 Relation type Regular Reversed All Regular Reversed All Combined U SAGE T OPIC C OMPARE M ODEL -F EATURE R ESULT PART-W HOLE 296 8 95 226 52 158 187 10 – 100 20 76 483 18 95 326 72 234 323 230 41 123 85 117 147 13 – 52 38 79 470 243 41 175 123 196 953 261 136 501 195 430 Total 835 393 1228 919 329 1248 2476 Table 1: Class distribution in subtask 1 training datasets for scenario 1.1 (clean annotation) and scenario 1.2 (noisy annotation). Last column shows the combined counts of both datasets. latio"
S18-1135,C16-1138,0,0.0189576,"these models in more detail. 3.1 SVM Model Our best-performing model uses an SVM classifier with a combination of sparse and dense features. Sparse feature encoding. To encode the sparse features, we adopt the method of Chen et al. (2006) who divide the sentence into contexts. In a nutshell, the method consists in describing the context in which the entities occur by dividing the sentence into five contexts around the entity mentions. Two of these five contexts are related to the Dependency features. Many systems from the literature make use of dependency features (Nguyen and Grishman, 2015a; Xu et al., 2016, 2015), which, intuitively, should be useful for re843 Subtask 1.1 Subtask 1.2 Relation type Regular Reversed All Regular Reversed All Combined U SAGE T OPIC C OMPARE M ODEL -F EATURE R ESULT PART-W HOLE 296 8 95 226 52 158 187 10 – 100 20 76 483 18 95 326 72 234 323 230 41 123 85 117 147 13 – 52 38 79 470 243 41 175 123 196 953 261 136 501 195 430 Total 835 393 1228 919 329 1248 2476 Table 1: Class distribution in subtask 1 training datasets for scenario 1.1 (clean annotation) and scenario 1.2 (noisy annotation). Last column shows the combined counts of both datasets. lation extraction and c"
S18-1135,D15-1206,0,0.0478253,"Missing"
S18-1192,W16-2815,1,0.903108,"Missing"
S18-1192,W14-2107,1,0.897604,"Missing"
S18-1192,D15-1075,0,0.144376,"Missing"
S18-1192,S18-1121,0,0.0732842,"Missing"
S18-1192,C14-1141,0,0.0670189,"Missing"
S18-1192,S14-2001,0,0.0871753,"Missing"
S18-1192,D15-1050,0,0.0412582,"Missing"
S18-1192,D14-1006,0,0.077592,"Missing"
S18-1192,J17-3005,0,0.0418088,"Missing"
S18-1192,D14-1083,0,0.0566188,"Missing"
S19-2172,S19-2145,0,0.0606487,"Missing"
S19-2172,D14-1162,0,0.0837316,"Missing"
seljan-etal-2010-corpus,J93-1004,0,\N,Missing
seljan-etal-2010-corpus,steinberger-etal-2006-jrc,0,\N,Missing
seljan-etal-2010-corpus,P98-1117,0,\N,Missing
seljan-etal-2010-corpus,C98-1113,0,\N,Missing
seljan-etal-2010-corpus,tadic-2000-building,1,\N,Missing
seljan-etal-2010-corpus,ceausu-etal-2006-acquis,0,\N,Missing
snajder-2014-derivbase,piasecki-etal-2012-recognition,0,\N,Missing
snajder-2014-derivbase,W07-1710,0,\N,Missing
snajder-2014-derivbase,W03-2906,0,\N,Missing
snajder-2014-derivbase,W14-0136,0,\N,Missing
snajder-2014-derivbase,N03-1013,0,\N,Missing
snajder-2014-derivbase,2003.mtsummit-systems.9,0,\N,Missing
snajder-2014-derivbase,P04-1048,0,\N,Missing
snajder-2014-derivbase,P11-2098,0,\N,Missing
snajder-2014-derivbase,J11-2002,0,\N,Missing
snajder-2014-derivbase,P13-2137,1,\N,Missing
snajder-2014-derivbase,P13-2128,1,\N,Missing
snajder-2014-derivbase,W13-2408,0,\N,Missing
snajder-2014-derivbase,W13-3512,0,\N,Missing
snajder-2014-derivbase,P13-1118,1,\N,Missing
snajder-2014-derivbase,R09-1074,1,\N,Missing
snajder-2014-derivbase,W99-0904,0,\N,Missing
W12-0501,W06-1669,0,0.0732088,"Missing"
W12-0501,E06-1027,0,0.0272846,"d polarity analysis. Subjectivity analysis answers whether the text unit is subjective or neutral, while polarity analysis determines whether a subjective text unit is positive or negative. The majority of research approaches (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Wilson et al., 2009) see subjectivity and polarity as categorical terms (i.e., classification problems). Intuitively, not all words express the sentiment with the same intensity. Accordingly, there has been some research effort in assessing subjectivity and polarity as graded values (Baccianella et al., 2010; Andreevskaia and Bergler, 2006). Most of the work on sentence or document level sentiment makes usage of sentiment annotated lexicon providing subjectivity and polarity information for individual words (Wilson et al., 2009; Taboada et al., 2011). In this paper we present a hybrid approach for automated acquisition of sentiment lexicon. The method is language independent and corpusbased and therefore suitable for languages lacking general lexical resources such as WordNet (Fellbaum, 2010). The two-step hybrid process combines semi-supervised graph-based algorithms and supervised learning models. We consider three different t"
W12-0501,baccianella-etal-2010-sentiwordnet,0,0.627575,"s subjectivity analysis and polarity analysis. Subjectivity analysis answers whether the text unit is subjective or neutral, while polarity analysis determines whether a subjective text unit is positive or negative. The majority of research approaches (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Wilson et al., 2009) see subjectivity and polarity as categorical terms (i.e., classification problems). Intuitively, not all words express the sentiment with the same intensity. Accordingly, there has been some research effort in assessing subjectivity and polarity as graded values (Baccianella et al., 2010; Andreevskaia and Bergler, 2006). Most of the work on sentence or document level sentiment makes usage of sentiment annotated lexicon providing subjectivity and polarity information for individual words (Wilson et al., 2009; Taboada et al., 2011). In this paper we present a hybrid approach for automated acquisition of sentiment lexicon. The method is language independent and corpusbased and therefore suitable for languages lacking general lexical resources such as WordNet (Fellbaum, 2010). The two-step hybrid process combines semi-supervised graph-based algorithms and supervised learning mode"
W12-0501,J90-1003,0,0.416762,"ompiled lexical resource like WordNet does not exist. In such a case, semantic relations between words may be extracted from corpus. In their pioneering work, Hatzivassiloglou and McKeown (1997) attempt to determine the polarity of adjectives based on their co-occurrences in conjunctions. They start with a small manually labeled seed set and build on the observation that adjectives of the same polarity are often conjoined with the conjunction and, while adjectives of the opposite polarity are conjoined with the conjunction but. Turney and Littman (2003) use pointwise mutual information (PMI) (Church and Hanks, 1990) and latent semantic analysis (LSA) (Dumais, 2004) to determine the similarity of the word of unknown polarity with the words in both positive and negative seed sets. The aforementioned work presumes that there is a correlation between lexical semantics and sentiment. We base our work on the same assumption, but instead of directly comparing the words with the seed sets, we use distributional semantics to build a word similarity graph. In contrast to the approaches above, this allows us to potentially account for similarities between all pairs of words from corpus. To the best of our knowledge"
W12-0501,P07-1054,0,0.420568,"rest of the paper is structured as follows. In Section 2 we present the related work on sentiment lexicon acquisition. Section 3 discusses the semi-supervised step of the hybrid approach. In Section 4 we explain the supervised step in more detail. In Section 5 the experimental setup, the evaluation procedure, and the results of the approach are discussed. Section 6 concludes the paper and outlines future work. 2 Related Work Several approaches have been proposed for determining the prior polarity of words. Most of the approaches can be classified as either dictionarybased (Kamps et al., 2004; Esuli and Sebastiani, 2007; Baccianella et al., 2010) or corpus-based (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Regardless of the resource used, most of the approaches focus on bootstrapping, starting from a small seed set of manually labeled words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Esuli and Sebastiani, 2007). In this paper we also follow this idea of the semi-supervised bootstrapping as the first step of the sentiment lexicon acquisition. Dictionary-based approaches grow the seed sets according to the explicit paradigmatic semantic relations (synonymy, antonymy, hypon"
W12-0501,W06-3808,0,0.0414275,"n the same assumption, but instead of directly comparing the words with the seed sets, we use distributional semantics to build a word similarity graph. In contrast to the approaches above, this allows us to potentially account for similarities between all pairs of words from corpus. To the best of our knowledge, such an approach that combines corpus-based lexical semantics with graph-based propagation has not yet been applied to the task of building sentiment lexicon. However, similar approaches have been proven rather efficient on other tasks such as document level sentiment classification (Goldberg and Zhu, 2006) and word sense disambiguation (Agirre et al., 2006). word into one of the three sentiment classes (positive, negative, or neutral). Accordingly, we evaluate our method using three different measures – one to evaluate the quality of the ordering by positivity and negativity, other to evaluate the absolute sentiment scores assigned to each corpus word, and another to evaluate the classification performance. The rest of the paper is structured as follows. In Section 2 we present the related work on sentiment lexicon acquisition. Section 3 discusses the semi-supervised step of the hybrid approach"
W12-0501,P97-1023,0,0.914895,"e Processing of Textual Data (Hybrid2012), EACL 2012, pages 1–9, c Avignon, France, April 23 2012. 2012 Association for Computational Linguistics for the synsets from the negative seed set. Word’s polarity is then decided based on the difference between its PageRank values of the two runs. We also believe that graph is the appropriate structure for the propagation of sentiment properties of words. Unfortunately, for many languages a precompiled lexical resource like WordNet does not exist. In such a case, semantic relations between words may be extracted from corpus. In their pioneering work, Hatzivassiloglou and McKeown (1997) attempt to determine the polarity of adjectives based on their co-occurrences in conjunctions. They start with a small manually labeled seed set and build on the observation that adjectives of the same polarity are often conjoined with the conjunction and, while adjectives of the opposite polarity are conjoined with the conjunction but. Turney and Littman (2003) use pointwise mutual information (PMI) (Church and Hanks, 1990) and latent semantic analysis (LSA) (Dumais, 2004) to determine the similarity of the word of unknown polarity with the words in both positive and negative seed sets. The"
W12-0501,kamps-etal-2004-using,0,0.0333739,"Missing"
W12-0501,W06-1652,0,0.0181436,"also comparable to SentiWordNet when restricted to monosentimous (all senses carry the same sentiment) words. This is satisfactory, given the absence of explicit semantic relations between words in the corpus. 1 Introduction Knowing someone’s attitude towards events, entities, and phenomena can be very important in various areas of human activity. Sentiment analysis is an area of computational linguistics that aims to recognize the subjectivity and attitude expressed in natural language texts. Applications of sentiment analysis are numerous, including sentiment-based document classification (Riloff et al., 2006), opinion-oriented information extraction (Hu and Liu, 2004), and question answering (Somasundaran et al., 2007). 1. Polarity ranking task – determine the relative rankings of words, i.e., order lexicon items descendingly by positivity and negativity; 2. Polarity regression task – assign each word absolute scores (between 0 and 1) for positivity and negativity; 3. Sentiment classification task – classify each 1 Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 1–9, c Avignon, France, April 23 2012. 2012 Association for"
W12-0501,J11-2001,0,0.0995927,"s (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Wilson et al., 2009) see subjectivity and polarity as categorical terms (i.e., classification problems). Intuitively, not all words express the sentiment with the same intensity. Accordingly, there has been some research effort in assessing subjectivity and polarity as graded values (Baccianella et al., 2010; Andreevskaia and Bergler, 2006). Most of the work on sentence or document level sentiment makes usage of sentiment annotated lexicon providing subjectivity and polarity information for individual words (Wilson et al., 2009; Taboada et al., 2011). In this paper we present a hybrid approach for automated acquisition of sentiment lexicon. The method is language independent and corpusbased and therefore suitable for languages lacking general lexical resources such as WordNet (Fellbaum, 2010). The two-step hybrid process combines semi-supervised graph-based algorithms and supervised learning models. We consider three different tasks, each capturing different aspect of a sentiment lexicon: Numerous sentiment analysis applications make usage of a sentiment lexicon. In this paper we present experiments on hybrid sentiment lexicon acquisition"
W12-0501,J09-3003,0,0.017454,"ents on Hybrid Corpus-Based Sentiment Lexicon Acquisition ˇ Goran Glavaˇs, Jan Snajder and Bojana Dalbelo Baˇsi´c Faculty of Electrical Engineering and Computing University of Zagreb Zagreb, Croatia {goran.glavas, jan.snajder, bojana.dalbelo}@fer.hr Abstract Sentiment analysis combines subjectivity analysis and polarity analysis. Subjectivity analysis answers whether the text unit is subjective or neutral, while polarity analysis determines whether a subjective text unit is positive or negative. The majority of research approaches (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Wilson et al., 2009) see subjectivity and polarity as categorical terms (i.e., classification problems). Intuitively, not all words express the sentiment with the same intensity. Accordingly, there has been some research effort in assessing subjectivity and polarity as graded values (Baccianella et al., 2010; Andreevskaia and Bergler, 2006). Most of the work on sentence or document level sentiment makes usage of sentiment annotated lexicon providing subjectivity and polarity information for individual words (Wilson et al., 2009; Taboada et al., 2011). In this paper we present a hybrid approach for automated acqui"
W13-2404,W06-3808,0,0.03436,"lemmas that property pairs. Opinion classification of reviews frequently co-occur with opinion clues. We then has been approached using supervised text cate- manually filter out the false positives from the lists gorization techniques (Pang et al., 2002; Funk et of candidate clues and aspects. al., 2008) and semi-supervised methods based on Unlike some approaches (Popescu and Etzioni, the similarity between unlabeled documents and a 2007; Kobayashi et al., 2007), we do not require small set of manually labeled documents or clues that clues or aspects belong to certain word cate(Turney, 2002; Goldberg and Zhu, 2006). gories or to a predefined taxonomy. Our approach Sentiment analysis and opinion mining ap- is pragmatic – clues are words that express opinproaches have been proposed for several Slavic ions about aspects, while aspects are words that languages (Chetviorkin et al., 2012; Buczynski and opinion clues target. For example, we treat words Wawer, 2008; Smrž, 2006; Smailovi´c et al., 2012). like sti´ci (to arrive) and sve (everything) as aspects, Methods that rely on translation, using resources because they can be targets of opinion clues, as in developed for major languages, have also been pro- “"
W13-2404,agic-etal-2010-towards,0,0.0435545,"Missing"
W13-2404,P10-1060,0,0.0281604,"Missing"
W13-2404,C12-2001,0,0.0489295,"Missing"
W13-2404,W06-1642,0,0.0799172,"Missing"
W13-2404,W12-1702,0,0.0303502,"occurrence or syntactic criteria in a domainspecific corpus (Kanayama and Nasukawa, 2006; Popescu and Etzioni, 2007; Fahrni and Klenner, 2008; Mukherjee and Liu, 2012). Kobayashi et 18 Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 18–23, c Sofia, Bulgaria, 8-9 August 2013. 2010 Association for Computational Linguistics al. (2007) extract aspect-clue pairs from weblog rating). Analogously, we consider as negative posts using a supervised model with parts of de- clue candidates lemmas that occur much more frependency trees as features. Kelly et al. (2012) quently in negative than in positive reviews. Asuse a semi-supervised SVM model with syntactic suming that opinion clues target product aspects, features to classify the relations between entity- we extract as aspect candidates all lemmas that property pairs. Opinion classification of reviews frequently co-occur with opinion clues. We then has been approached using supervised text cate- manually filter out the false positives from the lists gorization techniques (Pang et al., 2002; Funk et of candidate clues and aspects. al., 2008) and semi-supervised methods based on Unlike some approaches ("
W13-2404,D07-1114,0,0.0227591,"odel with syntactic suming that opinion clues target product aspects, features to classify the relations between entity- we extract as aspect candidates all lemmas that property pairs. Opinion classification of reviews frequently co-occur with opinion clues. We then has been approached using supervised text cate- manually filter out the false positives from the lists gorization techniques (Pang et al., 2002; Funk et of candidate clues and aspects. al., 2008) and semi-supervised methods based on Unlike some approaches (Popescu and Etzioni, the similarity between unlabeled documents and a 2007; Kobayashi et al., 2007), we do not require small set of manually labeled documents or clues that clues or aspects belong to certain word cate(Turney, 2002; Goldberg and Zhu, 2006). gories or to a predefined taxonomy. Our approach Sentiment analysis and opinion mining ap- is pragmatic – clues are words that express opinproaches have been proposed for several Slavic ions about aspects, while aspects are words that languages (Chetviorkin et al., 2012; Buczynski and opinion clues target. For example, we treat words Wawer, 2008; Smrž, 2006; Smailovi´c et al., 2012). like sti´ci (to arrive) and sve (everything) as aspects"
W13-2404,P12-1036,0,0.0243172,"ave been proposed, ranging from domainspecific (Fahrni and Klenner, 2008; Qiu et al., 2009; Choi et al., 2009) to cross-domain approaches (Wilson et al., 2009; Taboada et al., 2011), and from lexicon-based methods (Popescu and Etzioni, 2007; Jijkoun et al., 2010; Taboada et al., 2011) to machine learning approaches (Boiy and Moens, 2009; Go et al., 2009). While early attempts focused on classifying overall document opinion (Turney, 2002; Pang et al., 2002), more recent approaches identify opinions expressed about individual product aspects (Popescu and Etzioni, 2007; Fahrni and Klenner, 2008; Mukherjee and Liu, 2012). Identifying opinionated aspects allows for aspect-based comparison across reviews and enables opinion summarization 2 Related Work Aspect-based opinion mining typically consists of three subtasks: sentiment lexicon acquisition, aspect-clue pair identification, and overall review opinion prediction. Most approaches to domainspecific sentiment lexicon acquisition start from a manually compiled set of aspects and opinion clues and then expand it with words satisfying certain co-occurrence or syntactic criteria in a domainspecific corpus (Kanayama and Nasukawa, 2006; Popescu and Etzioni, 2007; F"
W13-2404,W02-1011,0,0.0181113,"pinion mining is being increasingly used to automatically recognize opinions about products in natural language texts. Numerous approaches to opinion mining have been proposed, ranging from domainspecific (Fahrni and Klenner, 2008; Qiu et al., 2009; Choi et al., 2009) to cross-domain approaches (Wilson et al., 2009; Taboada et al., 2011), and from lexicon-based methods (Popescu and Etzioni, 2007; Jijkoun et al., 2010; Taboada et al., 2011) to machine learning approaches (Boiy and Moens, 2009; Go et al., 2009). While early attempts focused on classifying overall document opinion (Turney, 2002; Pang et al., 2002), more recent approaches identify opinions expressed about individual product aspects (Popescu and Etzioni, 2007; Fahrni and Klenner, 2008; Mukherjee and Liu, 2012). Identifying opinionated aspects allows for aspect-based comparison across reviews and enables opinion summarization 2 Related Work Aspect-based opinion mining typically consists of three subtasks: sentiment lexicon acquisition, aspect-clue pair identification, and overall review opinion prediction. Most approaches to domainspecific sentiment lexicon acquisition start from a manually compiled set of aspects and opinion clues and th"
W13-2404,J11-2001,0,0.0296675,"f reviews. We show that a supervised approach to linking opinion clues to aspects is feasible, and that the extracted clues and aspects improve polarity and rating predictions. 1 Introduction For companies, knowing what customers think of their products and services is essential. Opinion mining is being increasingly used to automatically recognize opinions about products in natural language texts. Numerous approaches to opinion mining have been proposed, ranging from domainspecific (Fahrni and Klenner, 2008; Qiu et al., 2009; Choi et al., 2009) to cross-domain approaches (Wilson et al., 2009; Taboada et al., 2011), and from lexicon-based methods (Popescu and Etzioni, 2007; Jijkoun et al., 2010; Taboada et al., 2011) to machine learning approaches (Boiy and Moens, 2009; Go et al., 2009). While early attempts focused on classifying overall document opinion (Turney, 2002; Pang et al., 2002), more recent approaches identify opinions expressed about individual product aspects (Popescu and Etzioni, 2007; Fahrni and Klenner, 2008; Mukherjee and Liu, 2012). Identifying opinionated aspects allows for aspect-based comparison across reviews and enables opinion summarization 2 Related Work Aspect-based opinion min"
W13-2404,P02-1053,0,0.0267542,"s essential. Opinion mining is being increasingly used to automatically recognize opinions about products in natural language texts. Numerous approaches to opinion mining have been proposed, ranging from domainspecific (Fahrni and Klenner, 2008; Qiu et al., 2009; Choi et al., 2009) to cross-domain approaches (Wilson et al., 2009; Taboada et al., 2011), and from lexicon-based methods (Popescu and Etzioni, 2007; Jijkoun et al., 2010; Taboada et al., 2011) to machine learning approaches (Boiy and Moens, 2009; Go et al., 2009). While early attempts focused on classifying overall document opinion (Turney, 2002; Pang et al., 2002), more recent approaches identify opinions expressed about individual product aspects (Popescu and Etzioni, 2007; Fahrni and Klenner, 2008; Mukherjee and Liu, 2012). Identifying opinionated aspects allows for aspect-based comparison across reviews and enables opinion summarization 2 Related Work Aspect-based opinion mining typically consists of three subtasks: sentiment lexicon acquisition, aspect-clue pair identification, and overall review opinion prediction. Most approaches to domainspecific sentiment lexicon acquisition start from a manually compiled set of aspects and"
W13-2404,J09-3003,0,0.178251,"polarity and rating of reviews. We show that a supervised approach to linking opinion clues to aspects is feasible, and that the extracted clues and aspects improve polarity and rating predictions. 1 Introduction For companies, knowing what customers think of their products and services is essential. Opinion mining is being increasingly used to automatically recognize opinions about products in natural language texts. Numerous approaches to opinion mining have been proposed, ranging from domainspecific (Fahrni and Klenner, 2008; Qiu et al., 2009; Choi et al., 2009) to cross-domain approaches (Wilson et al., 2009; Taboada et al., 2011), and from lexicon-based methods (Popescu and Etzioni, 2007; Jijkoun et al., 2010; Taboada et al., 2011) to machine learning approaches (Boiy and Moens, 2009; Go et al., 2009). While early attempts focused on classifying overall document opinion (Turney, 2002; Pang et al., 2002), more recent approaches identify opinions expressed about individual product aspects (Popescu and Etzioni, 2007; Fahrni and Klenner, 2008; Mukherjee and Liu, 2012). Identifying opinionated aspects allows for aspect-based comparison across reviews and enables opinion summarization 2 Related Work A"
W13-2404,C00-2137,0,0.0487357,"Missing"
W13-2404,H05-2017,0,\N,Missing
W13-2404,H05-1043,0,\N,Missing
W13-2404,W11-1704,0,\N,Missing
W13-2405,P09-1096,0,0.0287886,"exical gap is smoothing via clustering, proposed by Kim and Seo (2006). First, query logs are expanded with word definitions from a machine readable dictionary. Subsequently, query logs are clustered, and query similarity is computed against the clusters, instead of against the individual FAQs. As an alternative to clustering, query expansion is often used to perform lexical smoothing (Voorhees, 1994; Navigli and Velardi, 2003). In some domains a FAQ engine additionally must deal with typing errors and noisy usergenerated content. An example is the FAQ retrieval for SMS messages, described by Kothari et al. (2009) and Contractor et al. (2010). Although low lexical overlap is identified as the primary problem in FAQ retrieval, sometimes it is the high lexical overlap that also presents a 3 Croatian FAQ test collection The standard procedure for IR evaluation requires a test collection consisting of documents, queries, and relevance judgments. We additionally require an annotated dataset to train the model. As there currently exists no standard IR test collection for Croatian, we decided to build a FAQ test collection from scratch. We use this collection for both model training and retrieval evaluation."
W13-2405,S12-1051,0,0.0187539,"ons in advance. Instead, they must guess what the likely questions would be. Thus, it is very common that users’ information needs are not fully covered by the provided questions. Secondly, both FAQs and user queries are generally very short texts, which diminishes the chances of a keyword match. In this paper we describe the design and the evaluation of a FAQ retrieval engine for Croatian. To address the lexical gap problem, we take a supervised learning approach and train a model that predicts the relevance of a FAQ given a query. Motivated by the recent work on semantic textual similarity (Agirre et al., 2012), we use as model features a series of similarity measures based on word overlap and semantic vector space similarity. We train and evaluate the model on a FAQ dataset from a telecommunication domain. On this dataset, our best performing model achieves 0.47 of mean reciprocal rank, i.e., on average ranks the relevant FAQ among the top two results. In summary, the contribution of this paper is twofold. Firstly, we propose and evaluate a FAQ retrieval model based on supervised machine learning. To the best of our knowledge, no previFrequently asked questions (FAQ) are an efficient way of communi"
W13-2405,D10-1009,0,0.0465313,"Missing"
W13-2405,P08-1028,0,0.0222614,"lines of Karan et al. (2012). We build the model from Croatian web corpus HrWaC from Ljubeši´c and Erjavec (2011). For lemmatization, we use the morphological lexicon from Šnajder et al. (2008). Prior to the SVD, we weight the matrix elements with their tf-idf values. Preliminary experiments showed that system performance remained satisfactory when reducing the vector space to only 25 dimensions, but further reduction caused deterioration. We use 25 dimensions in all experiments. LSA represents the meaning of a w by a vector v(w). Motivated by work on distributional semantic compositionality (Mitchell and Lapata, 2008), we compute the semantic representation of text T as the semantic composition (defined as vector addition) of the individual words constituting T : X v(T ) = v(w) where ssim(w1 , w2 ) is the semantic similarity of words w1 and w2 computed as the cosine similarity of their LSA vectors, and ic is the information content given by (2). The overall similarity between two texts is defined as the sum of weighted pair similarities, normalized by the length of the longer text: P (w1 ,w2 )∈P sim(w1 , w2 ) alo(T1 , T2 ) = max(length(T1 ), length(T2 )) (7) where P is the set of aligned lemma pairs. A sim"
W13-2405,S12-1060,1,0.881591,"Missing"
W13-2405,N04-1008,0,0.0415671,"have a high lexical overlap with a query. Moreo et al. (2012) address the problem of false positives using case based reasoning. Rather than considering only the words, they use phrases (“differentiator expressions”) that discriminate well between FAQs. The approaches described so far are essentially unsupervised. A number of supervised FAQ retrieval methods have been described in the literature. To bridge the lexical gap, Xue et al. (2008) use machine translation models to “translate” the user query into a FAQ. Their system is trained on very large FAQ knowledge bases, such as Yahoo answers. Soricut and Brill (2004) describe another large-scale FAQ retrieval system, which uses language and transformation models. A good general overview of supervised approaches to ranking tasks is the work by Liu (2009). Our system falls into the category of supervised methods. In contrast to the above-described approaches, we use a supervised model with word overlap and semantic similarity features. Taking into account that FAQs are short texts, we use features that have been recently proposed for determining the semantic similarity between pairs of sentences (Šari´c et al., 2012). Because we train our model to output a"
W13-2407,S10-1031,0,0.0189028,"ly consist of two steps: candidate extraction and candidate scoring. Supervised approaches include decision tree models (Turney, 1999; Ercan and Cicekli, 2007), na¨ıve Bayes classifier (Witten et al., 1999; McCallum and Nigam, 1998; Frank et al., 1999), and SVM (Zhang et al., 2006). Unsupervised approaches include clustering (Liu et al., 2009), graph-based methods (Mihalcea and Tarau, 2004), and language modeling (Tomokiyo and Hurst, 2003). Many more methods were proposed and evaluated within the SemEval shared task (Kim et al., 2010). Recent approaches (Jiang et al., 2009; Wang and Li, 2011; Eichler and Neumann, 2010) acknowledge keyphrase extraction as a highly subjective task and frame it as a learning-to-rank problem. Keyphrase extraction for Croatian has been addressed in both supervised and unsupervised setting. Ahel et al. (2009) use a na¨ıve Bayes classifier with phrase position and tf-idf (term frequency/inverse document frequency) as features. Saratlija et al. (2011) use distributional semantics to build topically related word clusters, from which they extract keywords and expand them to keyphrases. Miji´c et al. (2010) use filtering based on morphosyntactic tags followed by tf-idf scoring. Introd"
W13-2407,W03-1805,0,0.0285867,"rpretable keyphrase scoring measures that perform comparably to more complex machine learning methods previously developed for Croatian. 1 2 Related Work Keyphrase extraction typically consist of two steps: candidate extraction and candidate scoring. Supervised approaches include decision tree models (Turney, 1999; Ercan and Cicekli, 2007), na¨ıve Bayes classifier (Witten et al., 1999; McCallum and Nigam, 1998; Frank et al., 1999), and SVM (Zhang et al., 2006). Unsupervised approaches include clustering (Liu et al., 2009), graph-based methods (Mihalcea and Tarau, 2004), and language modeling (Tomokiyo and Hurst, 2003). Many more methods were proposed and evaluated within the SemEval shared task (Kim et al., 2010). Recent approaches (Jiang et al., 2009; Wang and Li, 2011; Eichler and Neumann, 2010) acknowledge keyphrase extraction as a highly subjective task and frame it as a learning-to-rank problem. Keyphrase extraction for Croatian has been addressed in both supervised and unsupervised setting. Ahel et al. (2009) use a na¨ıve Bayes classifier with phrase position and tf-idf (term frequency/inverse document frequency) as features. Saratlija et al. (2011) use distributional semantics to build topically rel"
W13-2407,S10-1004,0,0.0174324,"viously developed for Croatian. 1 2 Related Work Keyphrase extraction typically consist of two steps: candidate extraction and candidate scoring. Supervised approaches include decision tree models (Turney, 1999; Ercan and Cicekli, 2007), na¨ıve Bayes classifier (Witten et al., 1999; McCallum and Nigam, 1998; Frank et al., 1999), and SVM (Zhang et al., 2006). Unsupervised approaches include clustering (Liu et al., 2009), graph-based methods (Mihalcea and Tarau, 2004), and language modeling (Tomokiyo and Hurst, 2003). Many more methods were proposed and evaluated within the SemEval shared task (Kim et al., 2010). Recent approaches (Jiang et al., 2009; Wang and Li, 2011; Eichler and Neumann, 2010) acknowledge keyphrase extraction as a highly subjective task and frame it as a learning-to-rank problem. Keyphrase extraction for Croatian has been addressed in both supervised and unsupervised setting. Ahel et al. (2009) use a na¨ıve Bayes classifier with phrase position and tf-idf (term frequency/inverse document frequency) as features. Saratlija et al. (2011) use distributional semantics to build topically related word clusters, from which they extract keywords and expand them to keyphrases. Miji´c et al."
W13-2407,R09-1086,0,0.0329727,"Missing"
W13-2407,D09-1027,0,0.0187949,"G P K EX on Croatian newspaper articles. We show that G P K EX can evolve simple and interpretable keyphrase scoring measures that perform comparably to more complex machine learning methods previously developed for Croatian. 1 2 Related Work Keyphrase extraction typically consist of two steps: candidate extraction and candidate scoring. Supervised approaches include decision tree models (Turney, 1999; Ercan and Cicekli, 2007), na¨ıve Bayes classifier (Witten et al., 1999; McCallum and Nigam, 1998; Frank et al., 1999), and SVM (Zhang et al., 2006). Unsupervised approaches include clustering (Liu et al., 2009), graph-based methods (Mihalcea and Tarau, 2004), and language modeling (Tomokiyo and Hurst, 2003). Many more methods were proposed and evaluated within the SemEval shared task (Kim et al., 2010). Recent approaches (Jiang et al., 2009; Wang and Li, 2011; Eichler and Neumann, 2010) acknowledge keyphrase extraction as a highly subjective task and frame it as a learning-to-rank problem. Keyphrase extraction for Croatian has been addressed in both supervised and unsupervised setting. Ahel et al. (2009) use a na¨ıve Bayes classifier with phrase position and tf-idf (term frequency/inverse document f"
W13-2407,W04-3252,0,\N,Missing
W13-5001,bejan-harabagiu-2008-linguistic,0,0.0696688,"Missing"
W13-5001,P13-2139,1,0.776575,"Missing"
W13-5001,I13-1005,0,0.0286708,". Our approach fits into the latter group but we represent documents as graphs of events rather than graphs of concepts. In NLP, graph kernels have been used for question type classification (Suzuki, 2005), cross-lingual retrieval (Noh et al., 2009), and recognizing news stories on ˇ the same event (Glavaˇs and Snajder, 2013b). Event-based IR is addressed explicitly by Lin et al. (2007), who compare predicate-argument structures extracted from queries to those extracted from documents. However, queries have to be manually decomposed into semantic roles and can contain only a single predicate. Kawahara et al. (2013) propose a similar approach and demonstrate that ranking based on semantic roles outperforms ranking based on syntactic dependencies. Both these approaches target the problem of syntactic alternation but do not consider the queries made of multiple predicates, such as those expressing temporal relations between events. 3 Kernels on Event Graphs Our approach consists of two steps. First, we construct event graphs from both the document and the query. We then use a graph kernel to measure the query-document similarity and rank the documents. 3.1 Event Graphs An event graph is a mixed graph in wh"
W14-2107,P13-2142,0,0.0593771,"Missing"
W14-2107,S12-1051,0,0.0259301,"atter case, the argument may be contradicted or it may simply be a non sequitur. While we might expect these relations to be recognizable in texts from more formal genres, such as legal documents and parliamentary debates, it is questionable to what extent these relations can be detected in user-generated content, where the arguments are stated vaguely and implicitly. 1.0 0.8 0.6 0.4 0.2 0.0 A a N Label s S To account for this, we use a series of argumentcomment comparison features based on semantic textual similarity (STS). STS measures “the degree of semantic equivalence between two texts” (Agirre et al., 2012). It is a looser notion than TE and, unlike TE, it is a non-directional (symmetric) relation. We rely on the freely available TakeLab STS sysˇ c et al. (2012). Given a comment and tem by Sari´ an argument, the STS system outputs a continuous similarity score. We also compute the similarity between the argument and each sentence from the comment, which gives us a vector of similarities. The vector length equals the largest number of sentences in a comment, which in C OM A RG is 29. Additionally, we compute the maximum and the Figure 1: Ratio of positive entailment decisions across labels, scale"
W14-2107,W11-1701,0,0.351565,"Missing"
W14-2107,D07-1114,0,0.0141547,"Missing"
W14-2107,P12-2041,0,0.618602,"ed to argumentation mining, stance classification, and opinion mining. Palau and Moens (2009) approach argumentation mining in three steps: (1) argument identification (determining whether a sentence is argumentative), (2) argument proposition classification (categorize argumentative sentences as premises or conclusions), and (3) detection of argumentation structure or “argumentative parsing” (determining the relations between the arguments). The focus of their work is on legal text: the Araucaria corpus (Reed et al., 2008) and documents from the European Court of Human Rights. More recently, Cabrio and Villata (2012) explored the use of textual entailment for building argumentation networks and determining the acceptability of arguments. Textual entailment (TE) is a generic NLP framework for recognizing inference relations between two natural language texts (Dagan et al., 2006). Cabrio and Villata base their approach on Dung’s argumentation theory (Dung, 1995) and apply it to arguments from online debates. After linking the arguments with support/attack relations using TE, they are able to compute a set of acceptable arguments. Their system helps the participants to get an overview of a debate and the acc"
W14-2107,W06-2915,0,0.0391233,"Missing"
W14-2107,W04-3205,0,0.0113888,"nnotated comment-argument pairs. 3.3 Argument Recognition Model 4.1 Textual Entailment Following the work of Cabrio and Villata (2012), we use textual entailment (TE) to determine whether the comment (the text) entails the argument phrase (the hypothesis). To this end we use the Excitement Open Platform (EOP), a rich suite of textual entailment tools designed for modular use (Pad´o et al., 2014). From EOP we used seven pre-trained entailment decision algorithms (EDAs). Some EDAs contain only syntactical features, whereas others rely on resources such as WordNet (Fellbaum, 1998) and VerbOcean (Chklovski and Pantel, 2004). Each EDA outputs a binary decision (Entailment or NonEntailment) along with the degree of confidence. We use the outputs (decisions and confidences) of all seven EDAs as the features of our classifier (14 features in total). We also experimented with using additional features (the disjunction of all classifier decisions, the maximum confidence value, and the mean confidence value), but using these did not improve the performance. In principle, we expect the comment text (which is usually longer) to entail the argument phrase (which is usually shorter). This is also confirmed by the ratio of"
W14-2107,C10-2100,0,0.187635,"Missing"
W14-2107,reed-etal-2008-language,0,0.0385438,"Missing"
W14-2107,S12-1060,1,0.77623,"Missing"
W14-2107,W10-0214,0,0.384433,"Missing"
W14-3705,N03-1013,0,0.0359899,"entences and the number of tokens. Additionally, we use a feature indicating if the two mentions are adjacent (no mentions occur in between). Syntactic features: All dependency relations on the path between events in the dependency tree and features that indicate whether one of the features syntactically governs the other. We compute the syntactic features only for pairs of event mentions from the same sentence, using the Stanford dependency parser (De Marneffe et al., 2006). Knowledge-based features: Computed using WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), and CatVar (Habash and Dorr, 2003). We use a feature indicating whether one event mention or any of its derivatives (obtained from CatVar) is a WordNet hypernym of (for nominalized mentions) or entailed from (for verb mentions) the other mention (or any of its derivatives). We use an additional feature to indicate the VerbOcean relation between the event mentions, if such exists. Unlike features from previous groups, bloodshed agreed Figure 1: An example of an EHDAG for a narrative not account for global narrative coherence. Chambers and Jurafsky (2008) consider narratives to be chains of temporally ordered events linked by a"
W14-3705,E12-1034,0,0.0492029,"Missing"
W14-3705,W06-1623,0,0.144937,"from previous groups, bloodshed agreed Figure 1: An example of an EHDAG for a narrative not account for global narrative coherence. Chambers and Jurafsky (2008) consider narratives to be chains of temporally ordered events linked by a common protagonist. Limiting a narrative to a sequence of protagonist-sharing events can often be overly restrictive. E.g., an “encounter between Merkel and Holland” may belong to the same “summit” narrative as a “meeting between Obama and Putin,” although they share no protagonists. Several approaches enforce coherence of temporal relations at a document level. Bramsen et al. (2006) represent the temporal structure of a document as a DAG in which vertices denote textual segments and edges temporal precedence. Similarly, Do et al. (2012) enforce coherence using ILP for joint inference on decisions from local event– event and event–time interval decisions. Complementary to Chambers and Jurafsky (2008), who use a linear temporal structure, with EHDAGs we model the hierarchical structure of events with diverse participants. Similarly to Bramsen et al. (2006), we use an ILP formulation of global coherence over local decisions, but consider STC relations between events rather"
W14-3705,P08-1090,0,0.0342476,"ng WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), and CatVar (Habash and Dorr, 2003). We use a feature indicating whether one event mention or any of its derivatives (obtained from CatVar) is a WordNet hypernym of (for nominalized mentions) or entailed from (for verb mentions) the other mention (or any of its derivatives). We use an additional feature to indicate the VerbOcean relation between the event mentions, if such exists. Unlike features from previous groups, bloodshed agreed Figure 1: An example of an EHDAG for a narrative not account for global narrative coherence. Chambers and Jurafsky (2008) consider narratives to be chains of temporally ordered events linked by a common protagonist. Limiting a narrative to a sequence of protagonist-sharing events can often be overly restrictive. E.g., an “encounter between Merkel and Holland” may belong to the same “summit” narrative as a “meeting between Obama and Putin,” although they share no protagonists. Several approaches enforce coherence of temporal relations at a document level. Bramsen et al. (2006) represent the temporal structure of a document as a DAG in which vertices denote textual segments and edges temporal precedence. Similarly"
W14-3705,W04-3205,0,0.192755,"in the document, both in the number of sentences and the number of tokens. Additionally, we use a feature indicating if the two mentions are adjacent (no mentions occur in between). Syntactic features: All dependency relations on the path between events in the dependency tree and features that indicate whether one of the features syntactically governs the other. We compute the syntactic features only for pairs of event mentions from the same sentence, using the Stanford dependency parser (De Marneffe et al., 2006). Knowledge-based features: Computed using WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), and CatVar (Habash and Dorr, 2003). We use a feature indicating whether one event mention or any of its derivatives (obtained from CatVar) is a WordNet hypernym of (for nominalized mentions) or entailed from (for verb mentions) the other mention (or any of its derivatives). We use an additional feature to indicate the VerbOcean relation between the event mentions, if such exists. Unlike features from previous groups, bloodshed agreed Figure 1: An example of an EHDAG for a narrative not account for global narrative coherence. Chambers and Jurafsky (2008) consider narratives to be chains of te"
W14-3705,C04-1197,0,0.0430997,"one event must be in the same relation with all coreferent mentions of the other event (eqs. 6–9). Let coref (ei , ej ) be a predicate that holds iff mentions e1 and e2 corefer. The coreference constraints are as follows: The hierarchy of events induced from the independent pairwise STC decisions may be globally incoherent. We therefore need to optimize the set of pairwise STC classifications with respect to the set of constraints that enforce global coherence. We perform exact inference using Integer Linear Programming (ILP), an approach that has been proven useful in many NLP applications (Punyakanok et al., 2004; Roth and Yih, 2007; Clarke and Lapata, 2008). We use the lp solve1 solver to optimize the objective function with respect to the constraints. Objective function. Let M = {e1 , e2 , . . . , en } be the set of all event mentions in the news story and P be the set of all considered pairs of event mentions, P = {(ei , ej ) |ei , ej ∈ M, i < j}. Let R = {S UPER S UB, S UB S UPER, N O R EL} be the set of spatiotemporal relation types and let C(ei , ej , r) be the probability, produced by the pairwise classifier, of relation r holding between event mentions ei and ej . We maximize the sum of local"
W14-3705,de-marneffe-etal-2006-generating,0,0.059379,"Missing"
W14-3705,D12-1062,0,0.0799773,"onsider narratives to be chains of temporally ordered events linked by a common protagonist. Limiting a narrative to a sequence of protagonist-sharing events can often be overly restrictive. E.g., an “encounter between Merkel and Holland” may belong to the same “summit” narrative as a “meeting between Obama and Putin,” although they share no protagonists. Several approaches enforce coherence of temporal relations at a document level. Bramsen et al. (2006) represent the temporal structure of a document as a DAG in which vertices denote textual segments and edges temporal precedence. Similarly, Do et al. (2012) enforce coherence using ILP for joint inference on decisions from local event– event and event–time interval decisions. Complementary to Chambers and Jurafsky (2008), who use a linear temporal structure, with EHDAGs we model the hierarchical structure of events with diverse participants. Similarly to Bramsen et al. (2006), we use an ILP formulation of global coherence over local decisions, but consider STC relations between events rather than temporal relations between textual segments. 3 Spatiotemporal Containment Classifier Constructing Coherent Hierarchies As an example, consider the follo"
W14-3705,W13-0119,0,0.0370892,"Missing"
W14-3705,S13-2001,0,0.0784021,"Missing"
W14-3705,glavas-etal-2014-hieve,1,0.679039,"Missing"
W14-3705,C00-2137,0,0.0814194,"Missing"
W14-3705,P13-2139,1,0.889441,"Missing"
W14-3705,S10-1010,0,\N,Missing
W14-3705,2003.mtsummit-systems.9,0,\N,Missing
W15-0108,D10-1115,0,0.48573,"the result of a compositional process that combines the meanings of the base term read and the affix +er. This puts derivation into the purview of compositional distributional semantic models (CDSMs). CDSMs are normally used to compute the meaning of phrases and sentences by combining distributional representations of the individual words. A first generation of CDSMs represented all words as vectors and modeled composition as vector combination (Mitchell and Lapata, 2010). A second generation represents the meaning of predicates as higher-order algebraic objects such as matrices and tensors (Baroni and Zamparelli, 2010; Coecke et al., 2010), which are combined using various composition operations. Lazaridou et al. predict vectors for derived terms and evaluate their approach on a set of English derivation patterns. Building on and extending their analysis, we turn to German derivation patterns and offer both qualitative and quantitative analyses of two composition models on a state-of-the-art vector space, with the aim of better understanding where these models work well and where they fail. Our contributions are as follows. First, we perform all analyses in parallel for six derivation patterns (two each fo"
W15-0108,C10-1011,0,0.0148778,"cal function model (L EX F UN) represents the pattern as a matrix P that encodes the linear transformation that maps base onto derived terms: P b ≈ d. The best matrix Pˆ is typically computed via least-squares regression between the predicted vectors dˆi and the actual vectors di . 3 Experimental Setup Distributional model. We build a vector space from the SdeWaC corpus (Faaß and Eckart, 2013), part-of-speech tagged and lemmatized using TreeTagger (Schmid, 1994). To alleviate sparsity arising from TreeTagger’s lexicon-driven lemmatization, we back off for unrecognized words to the MATE Tools (Bohnet, 2010), which have higher recall but lower precision than TreeTagger. We also reconstruct lemmas for separated prefix verbs based on the MATE dependency analysis. Finally, we get a word list with 289,946 types (content words only). From the corpus, we extract lemmatized sentences and train a state-of-the art predictive model, namely CBOW (Mikolov et al., 2013). This model builds distributed word vectors by learning to predict the current word based on a context. We use lemma-POS pairs as both target and context elements, 300 dimensions, negative sampling set to 15, and no hierarchical softmax. Selec"
W15-0108,P13-1149,0,0.341749,"ng base-derived term pairs of the same pattern. A regression analysis shows that semantic coherence of the base and derived terms within a pattern, as well as coherence of the semantic shifts from base to derived terms, all significantly impact prediction quality. 1 Introduction Derivation is a major morphological process of word formation (e.g., read → read+er), which is typically associated with a fairly specific semantic shift (+er: agentivization). It may therefore be surprising that the semantics of derivation is a relatively understudied phenomenon in distributional semantics. Recently, Lazaridou et al. (2013) proposed to consider the semantics of a derived term like read+er as the result of a compositional process that combines the meanings of the base term read and the affix +er. This puts derivation into the purview of compositional distributional semantic models (CDSMs). CDSMs are normally used to compute the meaning of phrases and sentences by combining distributional representations of the individual words. A first generation of CDSMs represented all words as vectors and modeled composition as vector combination (Mitchell and Lapata, 2010). A second generation represents the meaning of predic"
W15-0108,W13-3512,0,0.125158,"a large number of distinct patterns. Some cross part-of-speech boundaries (nominalization, verbalization, adjectivization), but many do not (gender indicators like actor / actress or (de-)intensifiers like red / reddish). In many languages, such as German ˇ or the Slavic languages, derivational morphology is extremely productive (Stekauer and Lieber, 2005). Particularly relevant from a semantic perspective is that the meanings of the base and derived terms are often, but not always, closely related to each other. Consequently, derivational knowledge can be used to improve semantic processing (Luong et al., 2013; Pad´o et al., 2013). However, relatively few databases of derivational relations exist. CELEX (Baayen et al., 1996) contains derivational information for several languages, but was largely hand-written. A recent large-coverage resource for German, DErivBase (Zeller et al., 2013), covers 280k lemmas and was created from a rule-based framework that is fairly portable across languages. It is unique in that each base-derived lemma pair is labeled with a sequence of derivation patterns from a set of 267 patterns, enabling easy access to instances of specific patterns (cf. Section 3). Compositiona"
W15-0108,N13-1090,0,0.267478,"work that is fairly portable across languages. It is unique in that each base-derived lemma pair is labeled with a sequence of derivation patterns from a set of 267 patterns, enabling easy access to instances of specific patterns (cf. Section 3). Compositional models for derivation. Base and derived terms are closely related in meaning. In addition, this relation is coherent to a substantial extent, due to the phenomenon of productivity. In English, for example, the suffix -er generally indicates an agentive nominalization (sleep / sleeper) and un- is a negation prefix (well / unwell). Though Mikolov et al. (2013) address some inflectional patterns, Lazaridou et al. (2013) were the first to use this observation to motivate modeling derivation with CDSMs. Conceptually, the meaning of the base term (represented as a distributional vector) is combined with some distributional representation of the affix to obtain a vector representing the meaning of the derived term. In their experiments, they found that the two best-motivated and best-performing composition models were the full additive model (Zanzotto et al., 2010) and the lexical function model (Baroni and Zamparelli, 2010). Botha and Blunsom (2014) us"
W15-0108,P13-2128,1,0.92115,"Missing"
W15-0108,C10-1142,0,0.0530275,"tive nominalization (sleep / sleeper) and un- is a negation prefix (well / unwell). Though Mikolov et al. (2013) address some inflectional patterns, Lazaridou et al. (2013) were the first to use this observation to motivate modeling derivation with CDSMs. Conceptually, the meaning of the base term (represented as a distributional vector) is combined with some distributional representation of the affix to obtain a vector representing the meaning of the derived term. In their experiments, they found that the two best-motivated and best-performing composition models were the full additive model (Zanzotto et al., 2010) and the lexical function model (Baroni and Zamparelli, 2010). Botha and Blunsom (2014) use a related approach to model morphology for language modeling. The additive model (A DD) (Mitchell and Lapata, 2010) generally represents a derivation pattern p as a vector computed as the shift from base term vector b to the derived term vector d, i.e., b + p ≈ d. Given a set of P base-derived term pairs (b, d) for p, the best pˆ is computed as the average of the vector difference, 1 pˆ = N i (di − bi ).1 The lexical function model (L EX F UN) represents the pattern as a matrix P that encodes the linear"
W15-0108,P13-1118,1,0.897303,"Missing"
W15-0108,P13-4006,0,\N,Missing
W15-0514,S12-1051,0,0.0352507,"arguments in online debates. This is a formidable task, but as a first step towards this goal, we present a cluster analysis of users’ argumentative statements from online debates. The underlying assumption is that statements that express the same argument will be semantically more similar than statements that express different arguments, so that we can group together similar statements into clusters that correspond to arguments. We operationalize this by using hierarchical clustering based on semantic textual similarity (STS), defined as the degree of semantic equivalence between two texts (Agirre et al., 2012). The purpose of our study is twofold. First, we wish to investigate the notion of prominent arguments, considering in particular the variability in expressing arguments, and how well it can be captured by semantic similarity. Secondly, from a more practical perspective, we investigate the possibility of automatically identifying prominent arguments, setting a baseline for the task of unsupervised argument identification. 2 Related Work The pioneering work in argumentation mining is that of Moens et al. (2007), who addressed mining of argumentation from legal documents. Recently, the 110 Proce"
W15-0514,W14-2107,1,0.469086,"Missing"
W15-0514,P12-2041,0,0.125534,"dly, from a more practical perspective, we investigate the possibility of automatically identifying prominent arguments, setting a baseline for the task of unsupervised argument identification. 2 Related Work The pioneering work in argumentation mining is that of Moens et al. (2007), who addressed mining of argumentation from legal documents. Recently, the 110 Proceedings of the 2nd Workshop on Argumentation Mining, pages 110–115, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics focus has also moved to mining from user-generated content, such as online debates (Cabrio and Villata, 2012), discussions on regulations (Park and Cardie, 2014), and product reviews (Ghosh et al., 2014). ˇ Boltuˇzi´c and Snajder (2014) introduced argument recognition as the task of identifying what arguments, from a predefined set of arguments, have been used in users comments, and how. They frame the problem as multiclass classification and describe a model with similarity- and entailment-based features. Essentially the same task of argument recognition, but at the level of sentences, is addressed by Hasan and Ng (2014). They use a probabilistic framework for argument recognition (reason classifica"
W15-0514,W12-3810,0,0.143832,"Missing"
W15-0514,W14-2106,0,0.0366888,"prominent arguments, setting a baseline for the task of unsupervised argument identification. 2 Related Work The pioneering work in argumentation mining is that of Moens et al. (2007), who addressed mining of argumentation from legal documents. Recently, the 110 Proceedings of the 2nd Workshop on Argumentation Mining, pages 110–115, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics focus has also moved to mining from user-generated content, such as online debates (Cabrio and Villata, 2012), discussions on regulations (Park and Cardie, 2014), and product reviews (Ghosh et al., 2014). ˇ Boltuˇzi´c and Snajder (2014) introduced argument recognition as the task of identifying what arguments, from a predefined set of arguments, have been used in users comments, and how. They frame the problem as multiclass classification and describe a model with similarity- and entailment-based features. Essentially the same task of argument recognition, but at the level of sentences, is addressed by Hasan and Ng (2014). They use a probabilistic framework for argument recognition (reason classification) jointly with the related task of stance classification. Similarly, Conrad et al. (2012)"
W15-0514,D14-1083,0,0.146912,"also moved to mining from user-generated content, such as online debates (Cabrio and Villata, 2012), discussions on regulations (Park and Cardie, 2014), and product reviews (Ghosh et al., 2014). ˇ Boltuˇzi´c and Snajder (2014) introduced argument recognition as the task of identifying what arguments, from a predefined set of arguments, have been used in users comments, and how. They frame the problem as multiclass classification and describe a model with similarity- and entailment-based features. Essentially the same task of argument recognition, but at the level of sentences, is addressed by Hasan and Ng (2014). They use a probabilistic framework for argument recognition (reason classification) jointly with the related task of stance classification. Similarly, Conrad et al. (2012) detect spans of text containing arguing subjectivity and label them with argument tags using a model that relies on sentiment, discourse, and similarity features. The above approaches are supervised and rely on datasets manually annotated with arguments from a predefined set of arguments. In contrast, in this work we explore unsupervised argument identification. A similar task is described by Trabelsi and Za¨ıane (2014), w"
W15-0514,W14-2105,0,0.0161611,"the possibility of automatically identifying prominent arguments, setting a baseline for the task of unsupervised argument identification. 2 Related Work The pioneering work in argumentation mining is that of Moens et al. (2007), who addressed mining of argumentation from legal documents. Recently, the 110 Proceedings of the 2nd Workshop on Argumentation Mining, pages 110–115, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics focus has also moved to mining from user-generated content, such as online debates (Cabrio and Villata, 2012), discussions on regulations (Park and Cardie, 2014), and product reviews (Ghosh et al., 2014). ˇ Boltuˇzi´c and Snajder (2014) introduced argument recognition as the task of identifying what arguments, from a predefined set of arguments, have been used in users comments, and how. They frame the problem as multiclass classification and describe a model with similarity- and entailment-based features. Essentially the same task of argument recognition, but at the level of sentences, is addressed by Hasan and Ng (2014). They use a probabilistic framework for argument recognition (reason classification) jointly with the related task of stance classi"
W15-0514,W09-3204,0,0.0525656,"Missing"
W15-0514,W09-3200,0,0.25055,"5 pro and 22 con, on average 12 arguments per topic). The majority of sentences (2028 sentences) is labeled with pro arguments. The average sentence length is 14 words. Argument similarity. We experiment with two approaches for measuring the similarity of arguments. Vector-space similarity: We represent statements as vectors in a semantic space. We use two representations: (1) a bag-of-word (BoW) vector, weighted by inverse sentence frequency, and (2) a distributed representation based on the recently proposed neural network skip-gram model of Mikolov et al. (2013a). As noted by Ramage et al. (2009), BoW has shown to be a powerful baseline for semantic similarity. The rationale for weighting by inverse sentence frequency (akin to inverse document frequency) is that more frequently used words are less argument-specific and hence should contribute less to the similarity. On the other hand, distributed representations have been shown to work exceptionally well (outperforming BoW) for representing the meaning of individual words. Furthermore, they have been shown to model quite well the semantic composition of short phrases via simple vector addition (Mikolov et al., 2013b). To build a vecto"
W15-0514,D07-1043,0,0.0479694,"s 1: Clustering Models Evaluation metrics. A number of clustering evaluation metrics have been proposed in the literature. We adopt the external evaluation approach, which compares the hypothesized clusters against target clusters. We use argument labels of Hasan and Ng (2014) as target clusters. As noted by Amig´o et al. (2009), external cluster evaluation is a non-trivial task and there is no consensus on the best approach. We therefore chose to use two established, but rather different measures: the Adjusted Rand Index (ARI) (Hubert and Arabie, 1985) and the information-theoretic Vmeasure (Rosenberg and Hirschberg, 2007). ARI of 0 indicates clustering expected by chance and 1 indicates perfect clustering. The V-measure trade-offs measures of homogeneity (h) and completeness (c). It ranges from 0 to 1, with 1 being perfect clustering. Results. We cluster the sentences from the four topics separately, using the gold number of clusters 112 for each topic. Results are shown in Table 1. Overall, the best model is skip-gram with Ward’s linkage, generally outperforming the other models considered in terms of both ARI and V-measure. This model also results in the most consistent clusters in terms of balanced homogene"
W15-0514,S12-1060,1,0.910924,"Missing"
W15-0514,W14-1305,0,0.105813,"Missing"
W15-5303,P95-1017,0,0.177682,"ional approaches to coreference resolution for English were rule-based and heavily influenced by computational theories of discourse such as focusing and centering (Sidner, 1979; Grosz et al., 1983). As annotated coreference corpora became available, primarily within the Message Understanding Conferences (MUC-6 and MUC-7), research focus shifted towards supervised machine learning models. The first learningbased coreference resolution approach dates back to Connolly et al. (1997). The mention-pair model is essentially a binary coreference classifier for pairs of entity mentions, introduced by Aone and Bennett (1995) and McCarthy and Lehnert (1995). It is still at the core of most coreference resolution systems, despite its obvious inability to enforce the transitivity inherent to the coreference relation and the fact that it requires an additional clustering algorithm to build the coreference clusters. Interestingly enough, more complex models such as entity-mention model (McCallum and Wellner, 2003; Daum´e III and Marcu, 2005; Yang et al., 2008a) and ranking models (Iida et al., 2003; Yang et al., 2008b), designed to remedy for the shortcomings of the mention-pair model, Introduction Entity coreference"
W15-5303,kopec-ogrodniczuk-2012-creating,0,0.0879632,"n coreference resolution for other major languages, including Spanish (Palomar et al., 2001; Sapena et al., 2010), Italian (Kobdani and Sch¨utze, 2010; Poesio et al., 2010), German (Versley, 2006; Wunsch, 2010), Chinese (Converse, 2006; Kong and Zhou, 2010), Japanese (Iida et al., 2003; Iida, 2007), and Arabic (Zitouni et al., 2005; Luo and Zitouni, 2005). On the other hand, research on coreference resolution for Slavic languages has been quite limited, mainly due to the non-existence of manually annotated corpora. The exceptions are the work done for Polish, (Marciniak, 2002; Matysiak, 2007; Kopec and Ogrodniczuk, 2012), Czech (Linh et al., 2009), and Bulgarian (Zhikov et al., 2013). In particular, Kopec and Ogrodniczuk (2012) demonstrate that a rule-based coreference resolution system for Polish significantly outperforms state-of-the-art machine learning models for English, suggesting that the coreference resolution model benefits from morphological complexity of Polish. In this work, we present a mention-pair coreference resolution model for Croatian. Our model accounts for transitivity of coreference relations by encoding transitivity constraints as an ILP optimization problem. Our constrained mention-pai"
W15-5303,W11-1902,0,0.0292016,"morphological preprocessing (MP-M ORPH). Results show that the supervised mention-pair model significantly outperforms both reasonable rule-based baselines. When morphological features are not used, the model exhibits a slightly lower performance, although the difference is not substantial. Enforcing transitivity in an ILP setting marginally improves the overall MUC score, but yields notable 2-point improvement in B 3 score. Precision is consistently higher than recall for all models and both evaluation metrics, which is consistent with the coreference resolution results for other languages (Lee et al., 2011; Kobdani and Sch¨utze, 2011). Overall, our results are over 10 points higher than the state-of-the-art performance for English (Lee et al., 2011) and comparable (higher MUC and lower B 3 score) to the best results obtained for Polish (Kopec and Ogrodniczuk, 2012), suggesting that coreference resolution may be easier task for morphologically complex languages. 6 Conclusion We presented the first coreference resolution model for Croatian. We built a supervised mention-pair model for recognizing identity coreference relations between entity mentions and augmented it with transitivity constraints"
W15-5303,W09-3939,0,0.0408154,"Missing"
W15-5303,H05-1013,0,0.0756267,"Missing"
W15-5303,H05-1083,0,0.088793,"Missing"
W15-5303,Q14-1037,0,0.0229354,"ous applications that could greatly benefit from the ability to identify different mentions of the same entity, such as relation extraction (Shinyama and Sekine, 2006), question answering (Vicedo and Ferr´andez, 2000; Zheng, 2002), and text summarization (Bergler et al., 2003; Steinberger et al., 2007). Despite being easy to define, coreference resolution is considered to be a rather difficult task, primarily because it heavily relies on external knowledge (e.g., for resolving “U.S. President” and “Barack Obama”, one needs to know that Obama is the president of the USA) (Markert et al., 2003; Durrett and Klein, 2014). Although machine learning-based approaches to anaphora and coreference resolution for English appeared almost two decades ago (Connolly et al., 17 Proceedings of the 5th Workshop on Balto-Slavic Natural Language Processing, pages 17–23, Hissar, Bulgaria, 10–11 September 2015. failed to demonstrate a significant performance improvements over the simple mention-pair model. Besides for English, there is a significant body of work on coreference resolution for other major languages, including Spanish (Palomar et al., 2001; Sapena et al., 2010), Italian (Kobdani and Sch¨utze, 2010; Poesio et al.,"
W15-5303,W03-2606,0,0.0327748,"his attention to numerous applications that could greatly benefit from the ability to identify different mentions of the same entity, such as relation extraction (Shinyama and Sekine, 2006), question answering (Vicedo and Ferr´andez, 2000; Zheng, 2002), and text summarization (Bergler et al., 2003; Steinberger et al., 2007). Despite being easy to define, coreference resolution is considered to be a rather difficult task, primarily because it heavily relies on external knowledge (e.g., for resolving “U.S. President” and “Barack Obama”, one needs to know that Obama is the president of the USA) (Markert et al., 2003; Durrett and Klein, 2014). Although machine learning-based approaches to anaphora and coreference resolution for English appeared almost two decades ago (Connolly et al., 17 Proceedings of the 5th Workshop on Balto-Slavic Natural Language Processing, pages 17–23, Hissar, Bulgaria, 10–11 September 2015. failed to demonstrate a significant performance improvements over the simple mention-pair model. Besides for English, there is a significant body of work on coreference resolution for other major languages, including Spanish (Palomar et al., 2001; Sapena et al., 2010), Italian (Kobdani and Sch¨"
W15-5303,P83-1007,0,0.436491,"rence relation and the fact that it requires an additional clustering algorithm to build the coreference clusters. Interestingly enough, more complex models such as entity-mention model (McCallum and Wellner, 2003; Daum´e III and Marcu, 2005; Yang et al., 2008a) and ranking models (Iida et al., 2003; Yang et al., 2008b), designed to remedy for the shortcomings of the mention-pair model, Introduction Entity coreference resolution, the task of recognizing mentions in text that refer to the same realworld entity, has been one of the central tasks of natural language processing (NLP) for decades (Grosz et al., 1983; Connolly et al., 1997; Ponzetto and Strube, 2006). Coreference resolution owes this attention to numerous applications that could greatly benefit from the ability to identify different mentions of the same entity, such as relation extraction (Shinyama and Sekine, 2006), question answering (Vicedo and Ferr´andez, 2000; Zheng, 2002), and text summarization (Bergler et al., 2003; Steinberger et al., 2007). Despite being easy to define, coreference resolution is considered to be a rather difficult task, primarily because it heavily relies on external knowledge (e.g., for resolving “U.S. Presiden"
W15-5303,W03-2604,0,0.448941,". The mention-pair model is essentially a binary coreference classifier for pairs of entity mentions, introduced by Aone and Bennett (1995) and McCarthy and Lehnert (1995). It is still at the core of most coreference resolution systems, despite its obvious inability to enforce the transitivity inherent to the coreference relation and the fact that it requires an additional clustering algorithm to build the coreference clusters. Interestingly enough, more complex models such as entity-mention model (McCallum and Wellner, 2003; Daum´e III and Marcu, 2005; Yang et al., 2008a) and ranking models (Iida et al., 2003; Yang et al., 2008b), designed to remedy for the shortcomings of the mention-pair model, Introduction Entity coreference resolution, the task of recognizing mentions in text that refer to the same realworld entity, has been one of the central tasks of natural language processing (NLP) for decades (Grosz et al., 1983; Connolly et al., 1997; Ponzetto and Strube, 2006). Coreference resolution owes this attention to numerous applications that could greatly benefit from the ability to identify different mentions of the same entity, such as relation extraction (Shinyama and Sekine, 2006), question"
W15-5303,S10-1018,0,0.0358274,"Missing"
W15-5303,P02-1014,0,0.101915,"ions (as the set of individual binary decisions may conflict the transitivity property of the coreference relation). 4.1 • Indication whether the two mention strings fully match (f1 ); • Indication whether one mention string contains the other (f2 ); • Length of the longest common subsequence between the mentions (f3 ); • Edit distance (i.e., Levenshtein distance) between the mentions (f4 ). Creating Training Instances Overlap features quantify the overlap between the mentions in terms of tokens these mentions share: In this work, we generate training instances using the heuristic proposed by Ng and Cardie (2002), which is, in turn, the extension of the approach by Soon et al. (2001). We thus create a positive 1 Mention-Pair Model • Indications whether there is at least one matching word, lemma, and stem between the tokens of the two mentions (f4 , f5 , and f6 ); • Relative overlap between the mentions, meaA part of this dataset is freely available; cf. Section 5. 19 sured as the number of content lemmas (nouns, adjectives, verbs, and adverbs) found in both mentions, normalized by the token length of both mentions (f7 ). Grammatical features encode some grammatical properties and aim to indicate gramm"
W15-5303,W11-1910,0,0.0539046,"Missing"
W15-5303,D10-1086,0,0.0543677,"Missing"
W15-5303,J01-4005,0,0.112695,"Missing"
W15-5303,P08-1096,0,0.0138681,"ach dates back to Connolly et al. (1997). The mention-pair model is essentially a binary coreference classifier for pairs of entity mentions, introduced by Aone and Bennett (1995) and McCarthy and Lehnert (1995). It is still at the core of most coreference resolution systems, despite its obvious inability to enforce the transitivity inherent to the coreference relation and the fact that it requires an additional clustering algorithm to build the coreference clusters. Interestingly enough, more complex models such as entity-mention model (McCallum and Wellner, 2003; Daum´e III and Marcu, 2005; Yang et al., 2008a) and ranking models (Iida et al., 2003; Yang et al., 2008b), designed to remedy for the shortcomings of the mention-pair model, Introduction Entity coreference resolution, the task of recognizing mentions in text that refer to the same realworld entity, has been one of the central tasks of natural language processing (NLP) for decades (Grosz et al., 1983; Connolly et al., 1997; Ponzetto and Strube, 2006). Coreference resolution owes this attention to numerous applications that could greatly benefit from the ability to identify different mentions of the same entity, such as relation extractio"
W15-5303,poesio-etal-2010-creating,0,0.0429758,"Missing"
W15-5303,N06-1025,0,0.0414381,"s an additional clustering algorithm to build the coreference clusters. Interestingly enough, more complex models such as entity-mention model (McCallum and Wellner, 2003; Daum´e III and Marcu, 2005; Yang et al., 2008a) and ranking models (Iida et al., 2003; Yang et al., 2008b), designed to remedy for the shortcomings of the mention-pair model, Introduction Entity coreference resolution, the task of recognizing mentions in text that refer to the same realworld entity, has been one of the central tasks of natural language processing (NLP) for decades (Grosz et al., 1983; Connolly et al., 1997; Ponzetto and Strube, 2006). Coreference resolution owes this attention to numerous applications that could greatly benefit from the ability to identify different mentions of the same entity, such as relation extraction (Shinyama and Sekine, 2006), question answering (Vicedo and Ferr´andez, 2000; Zheng, 2002), and text summarization (Bergler et al., 2003; Steinberger et al., 2007). Despite being easy to define, coreference resolution is considered to be a rather difficult task, primarily because it heavily relies on external knowledge (e.g., for resolving “U.S. President” and “Barack Obama”, one needs to know that Obama"
W15-5303,J08-3002,0,0.023642,"ach dates back to Connolly et al. (1997). The mention-pair model is essentially a binary coreference classifier for pairs of entity mentions, introduced by Aone and Bennett (1995) and McCarthy and Lehnert (1995). It is still at the core of most coreference resolution systems, despite its obvious inability to enforce the transitivity inherent to the coreference relation and the fact that it requires an additional clustering algorithm to build the coreference clusters. Interestingly enough, more complex models such as entity-mention model (McCallum and Wellner, 2003; Daum´e III and Marcu, 2005; Yang et al., 2008a) and ranking models (Iida et al., 2003; Yang et al., 2008b), designed to remedy for the shortcomings of the mention-pair model, Introduction Entity coreference resolution, the task of recognizing mentions in text that refer to the same realworld entity, has been one of the central tasks of natural language processing (NLP) for decades (Grosz et al., 1983; Connolly et al., 1997; Ponzetto and Strube, 2006). Coreference resolution owes this attention to numerous applications that could greatly benefit from the ability to identify different mentions of the same entity, such as relation extractio"
W15-5303,recasens-etal-2010-typology,0,0.0611384,"Missing"
W15-5303,R13-1098,0,0.0609934,"Missing"
W15-5303,S10-1017,0,0.0612956,"Missing"
W15-5303,W05-0709,0,0.0289008,"Missing"
W15-5303,N06-1039,0,0.0305155,"and ranking models (Iida et al., 2003; Yang et al., 2008b), designed to remedy for the shortcomings of the mention-pair model, Introduction Entity coreference resolution, the task of recognizing mentions in text that refer to the same realworld entity, has been one of the central tasks of natural language processing (NLP) for decades (Grosz et al., 1983; Connolly et al., 1997; Ponzetto and Strube, 2006). Coreference resolution owes this attention to numerous applications that could greatly benefit from the ability to identify different mentions of the same entity, such as relation extraction (Shinyama and Sekine, 2006), question answering (Vicedo and Ferr´andez, 2000; Zheng, 2002), and text summarization (Bergler et al., 2003; Steinberger et al., 2007). Despite being easy to define, coreference resolution is considered to be a rather difficult task, primarily because it heavily relies on external knowledge (e.g., for resolving “U.S. President” and “Barack Obama”, one needs to know that Obama is the president of the USA) (Markert et al., 2003; Durrett and Klein, 2014). Although machine learning-based approaches to anaphora and coreference resolution for English appeared almost two decades ago (Connolly et al"
W15-5303,J01-4004,0,0.385848,"ity property of the coreference relation). 4.1 • Indication whether the two mention strings fully match (f1 ); • Indication whether one mention string contains the other (f2 ); • Length of the longest common subsequence between the mentions (f3 ); • Edit distance (i.e., Levenshtein distance) between the mentions (f4 ). Creating Training Instances Overlap features quantify the overlap between the mentions in terms of tokens these mentions share: In this work, we generate training instances using the heuristic proposed by Ng and Cardie (2002), which is, in turn, the extension of the approach by Soon et al. (2001). We thus create a positive 1 Mention-Pair Model • Indications whether there is at least one matching word, lemma, and stem between the tokens of the two mentions (f4 , f5 , and f6 ); • Relative overlap between the mentions, meaA part of this dataset is freely available; cf. Section 5. 19 sured as the number of content lemmas (nouns, adjectives, verbs, and adverbs) found in both mentions, normalized by the token length of both mentions (f7 ). Grammatical features encode some grammatical properties and aim to indicate grammatical compatibility of the two mentions: • Indication whether the first"
W15-5303,P00-1070,0,0.113559,"Missing"
W15-5309,D07-1007,0,0.031958,"We adopt a lexical sample approach and compile a corresponding senseannotated dataset on which we evaluate our models. We carry out a detailed investigation of the different active learning setups, and show that labeling as few as 100 instances suffices to reach near-optimal performance. 1 Introduction Word sense disambiguation (WSD) is the task of computationally determining the meaning of a word in its context (Navigli, 2009). WSD is considered one of the central tasks of natural language processing (NLP). A number of NLP applications can benefit from WSD, most notably machine translation (Carpuat and Wu, 2007), information retrieval (Stokoe et al., 2003), and information extraction (Markert and Nissim, 2007; Hassan et al., 2006; Ciaramita and Altun, 2006). At the same time, WSD is also considered a very difficult task; the difficulty arises from the fact that WSD relies on human knowledge and that it lends itself to different formalizations (e.g., the choice of a sense inventory) (Navigli, 2009). The two main approaches to WSD are knowledge-based and supervised. Knowledgebased approaches rely on lexical knowledge bases such as WordNet. The drawback of knowledgebased approaches is that the construct"
W15-5309,N06-1016,0,0.375254,"proves classifier performance. Another important issue of AL is the stopping condition. Zhu and Hovy (2007) propose a stopping criterion based on the classifier confidence, Wang et al. (2008) propose a minimum expected error strategy, while Zhu et al. (2008a) propose classifier-change as a stopping criterion. Finally, Zhu et al. (2008b) propose sampling methods for generating a representative initial training set, as well as selective sampling method for alleviating the problem of outliers. All of the above cited work addresses WSD for English, whereas our work focuses on Croatian. Similar to Chen et al. (2006), we use uncertaintybased sampling but combine it with an SVM model. In contrast to Chen et al. (2006), we opt for simple, readily available features derived from cooccurrences. We study three sampling methods in this work, but leave the issues of stopping criterion and class imbalance for future work. Croatian is a Slavic language, and WSD for Slavic languages seems not to have received much attention so far. Notable exceptions are (Ba´s et al., 2008; Broda and Piasecki, 2009) for Polish and (Lyashevskaya et al., 2011) for Russian. WSD for Bulgarian, Czech, Serbian, and Slovene has been consi"
W15-5309,S07-1007,0,0.0347918,"we evaluate our models. We carry out a detailed investigation of the different active learning setups, and show that labeling as few as 100 instances suffices to reach near-optimal performance. 1 Introduction Word sense disambiguation (WSD) is the task of computationally determining the meaning of a word in its context (Navigli, 2009). WSD is considered one of the central tasks of natural language processing (NLP). A number of NLP applications can benefit from WSD, most notably machine translation (Carpuat and Wu, 2007), information retrieval (Stokoe et al., 2003), and information extraction (Markert and Nissim, 2007; Hassan et al., 2006; Ciaramita and Altun, 2006). At the same time, WSD is also considered a very difficult task; the difficulty arises from the fact that WSD relies on human knowledge and that it lends itself to different formalizations (e.g., the choice of a sense inventory) (Navigli, 2009). The two main approaches to WSD are knowledge-based and supervised. Knowledgebased approaches rely on lexical knowledge bases such as WordNet. The drawback of knowledgebased approaches is that the construction of largescale lexical resources requires a tremendous ef49 Proceedings of the 5th Workshop on B"
W15-5309,W06-1670,0,0.10134,"investigation of the different active learning setups, and show that labeling as few as 100 instances suffices to reach near-optimal performance. 1 Introduction Word sense disambiguation (WSD) is the task of computationally determining the meaning of a word in its context (Navigli, 2009). WSD is considered one of the central tasks of natural language processing (NLP). A number of NLP applications can benefit from WSD, most notably machine translation (Carpuat and Wu, 2007), information retrieval (Stokoe et al., 2003), and information extraction (Markert and Nissim, 2007; Hassan et al., 2006; Ciaramita and Altun, 2006). At the same time, WSD is also considered a very difficult task; the difficulty arises from the fact that WSD relies on human knowledge and that it lends itself to different formalizations (e.g., the choice of a sense inventory) (Navigli, 2009). The two main approaches to WSD are knowledge-based and supervised. Knowledgebased approaches rely on lexical knowledge bases such as WordNet. The drawback of knowledgebased approaches is that the construction of largescale lexical resources requires a tremendous ef49 Proceedings of the 5th Workshop on Balto-Slavic Natural Language Processing, pages 49"
W15-5309,P04-1036,0,0.0296446,"d in a cross-lingual setup by Tufi¸s et al. (2004) and Ide et al. (2002). Bakari´c et al. (2007) analyze the discriminative strength of co-occurring words for WSD of Croatian nouns. Additionally, Karan et al. (2012) consider a problem dual to WSD, namely synonymy detection. To the best of our knowledge, our work is the first reported work on active learning for WSD for a Slavic language. Related Work WSD is a long-standing problem in NLP. A number of semi-supervised WSD methods have been proposed in the literature, including the use of external sources for the generation of sense-tagged data (McCarthy et al., 2004), 2004), use of bilingual corpora (Li and Li, 2004), label propagation (Niu et al., 2005), and bootstrapping (Mihalcea, 2004; Park et al., 2000). Focusing on AL approaches to WSD, one of the first attempts is that of Chklovski and Mihalcea (2002). Their Open Mind World Expert system collected sense-annotated data over the web, which were later used for the Senseval-3 English lexical sample task (Mihalcea et al., 2004). The system employs the so-called committee-based sampling: the instances to be labeled are selected based on the disagreement between the labels assigned by two different classi"
W15-5309,W04-0807,0,0.0427681,"nding problem in NLP. A number of semi-supervised WSD methods have been proposed in the literature, including the use of external sources for the generation of sense-tagged data (McCarthy et al., 2004), 2004), use of bilingual corpora (Li and Li, 2004), label propagation (Niu et al., 2005), and bootstrapping (Mihalcea, 2004; Park et al., 2000). Focusing on AL approaches to WSD, one of the first attempts is that of Chklovski and Mihalcea (2002). Their Open Mind World Expert system collected sense-annotated data over the web, which were later used for the Senseval-3 English lexical sample task (Mihalcea et al., 2004). The system employs the so-called committee-based sampling: the instances to be labeled are selected based on the disagreement between the labels assigned by two different classifiers. Chen et al. (2006) experiment with WSD for five frequent English verbs. Unlike Chklovski and Mihalcea, they use uncertainty-based sampling coupled with a maximum entropy learner, and a rich set of topical, collocational, syntactic, and semantic features. Their results show that, given a target accuracy level, AL can reduce the number of training instances by half when compared to labeling randomly selected inst"
W15-5309,S01-1001,0,0.314996,"ts is a well-known issue (Edmonds and Kilgarriff, 2002), and in this study we wanted to avoid the problem by choosing words with as distinct senses as possible.2 Research on sense granularity in the context of AL is warranted but is beyond the scope of this paper. The final list of words is as follows: okvirN (frame), vatraN (fire), brusitiV (to rasp), odlikovatiV (to award), lakA (easy), and prljavA (dirty). For each of these words, we sampled 500 sentences from hrWaC, yielding a total of 3000 word instances. Note that 500 instances per word is well above the 75+15·n instances recommended by Edmonds and Cotton (2001), where n is the number of senses of the word. 3.2 given a list of possible word senses (two or three) and an additional “none of the above” (NOTA) option. They were instructed to select a single sense, unless there is no adequate sense listed or the instance is erroneous (incorrect lemmatization or a spelling error). For each sense, we provided a gloss line and usage examples from CroWN. The annotation guidelines were rather straightforward. In cases when more than a single sense apply, the annotators were asked to choose the one they deem more appropriate. The only issue that we felt deserve"
W15-5309,W04-2405,0,0.0449016,"of co-occurring words for WSD of Croatian nouns. Additionally, Karan et al. (2012) consider a problem dual to WSD, namely synonymy detection. To the best of our knowledge, our work is the first reported work on active learning for WSD for a Slavic language. Related Work WSD is a long-standing problem in NLP. A number of semi-supervised WSD methods have been proposed in the literature, including the use of external sources for the generation of sense-tagged data (McCarthy et al., 2004), 2004), use of bilingual corpora (Li and Li, 2004), label propagation (Niu et al., 2005), and bootstrapping (Mihalcea, 2004; Park et al., 2000). Focusing on AL approaches to WSD, one of the first attempts is that of Chklovski and Mihalcea (2002). Their Open Mind World Expert system collected sense-annotated data over the web, which were later used for the Senseval-3 English lexical sample task (Mihalcea et al., 2004). The system employs the so-called committee-based sampling: the instances to be labeled are selected based on the disagreement between the labels assigned by two different classifiers. Chen et al. (2006) experiment with WSD for five frequent English verbs. Unlike Chklovski and Mihalcea, they use uncer"
W15-5309,W06-3802,0,0.0304156,"carry out a detailed investigation of the different active learning setups, and show that labeling as few as 100 instances suffices to reach near-optimal performance. 1 Introduction Word sense disambiguation (WSD) is the task of computationally determining the meaning of a word in its context (Navigli, 2009). WSD is considered one of the central tasks of natural language processing (NLP). A number of NLP applications can benefit from WSD, most notably machine translation (Carpuat and Wu, 2007), information retrieval (Stokoe et al., 2003), and information extraction (Markert and Nissim, 2007; Hassan et al., 2006; Ciaramita and Altun, 2006). At the same time, WSD is also considered a very difficult task; the difficulty arises from the fact that WSD relies on human knowledge and that it lends itself to different formalizations (e.g., the choice of a sense inventory) (Navigli, 2009). The two main approaches to WSD are knowledge-based and supervised. Knowledgebased approaches rely on lexical knowledge bases such as WordNet. The drawback of knowledgebased approaches is that the construction of largescale lexical resources requires a tremendous ef49 Proceedings of the 5th Workshop on Balto-Slavic Natural L"
W15-5309,W97-0201,0,0.359033,"g Lab Faculty of Electrical Engineering and Computing, University of Zagreb Unska 3, 10000 Zagreb, Croatia {domagoj.alagic,jan.snajder}@fer.hr Abstract fort, rendering such approaches particularly costineffective for smaller languages. On the other hand, supervised approaches do not rely on lexical resources and generally outperform knowledgebased approaches (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). However, supervised methods instead require a large amount of hand-annotated data, which is also extremely expensive and time-consuming to obtain. Interestingly enough, Ng (1997) estimates that a wide coverage WSD system for English would require a sense-tagged corpus of 3200 words with 1000 instances per word. Assuming human throughput of one instance per minute (Edmonds, 2000), this amounts to an immense effort of 27 man-years. One way of addressing the lack of manually sense-tagged data is to rely on semi-supervised learning (Abney, 2007), which, along with a smaller set of labeled data, also makes use of a typically much larger set of unlabeled data. A related technique is that of active learning (Olsson, 2009; Settles, 2010). However, what differentiates active l"
W15-5309,W02-0808,0,0.0720363,"VM model. In contrast to Chen et al. (2006), we opt for simple, readily available features derived from cooccurrences. We study three sampling methods in this work, but leave the issues of stopping criterion and class imbalance for future work. Croatian is a Slavic language, and WSD for Slavic languages seems not to have received much attention so far. Notable exceptions are (Ba´s et al., 2008; Broda and Piasecki, 2009) for Polish and (Lyashevskaya et al., 2011) for Russian. WSD for Bulgarian, Czech, Serbian, and Slovene has been considered in a cross-lingual setup by Tufi¸s et al. (2004) and Ide et al. (2002). Bakari´c et al. (2007) analyze the discriminative strength of co-occurring words for WSD of Croatian nouns. Additionally, Karan et al. (2012) consider a problem dual to WSD, namely synonymy detection. To the best of our knowledge, our work is the first reported work on active learning for WSD for a Slavic language. Related Work WSD is a long-standing problem in NLP. A number of semi-supervised WSD methods have been proposed in the literature, including the use of external sources for the generation of sense-tagged data (McCarthy et al., 2004), 2004), use of bilingual corpora (Li and Li, 2004"
W15-5309,P05-1049,0,0.0341641,"7) analyze the discriminative strength of co-occurring words for WSD of Croatian nouns. Additionally, Karan et al. (2012) consider a problem dual to WSD, namely synonymy detection. To the best of our knowledge, our work is the first reported work on active learning for WSD for a Slavic language. Related Work WSD is a long-standing problem in NLP. A number of semi-supervised WSD methods have been proposed in the literature, including the use of external sources for the generation of sense-tagged data (McCarthy et al., 2004), 2004), use of bilingual corpora (Li and Li, 2004), label propagation (Niu et al., 2005), and bootstrapping (Mihalcea, 2004; Park et al., 2000). Focusing on AL approaches to WSD, one of the first attempts is that of Chklovski and Mihalcea (2002). Their Open Mind World Expert system collected sense-annotated data over the web, which were later used for the Senseval-3 English lexical sample task (Mihalcea et al., 2004). The system employs the so-called committee-based sampling: the instances to be labeled are selected based on the disagreement between the labels assigned by two different classifiers. Chen et al. (2006) experiment with WSD for five frequent English verbs. Unlike Chk"
W15-5309,S01-1005,0,0.169854,"Missing"
W15-5309,P00-1069,0,0.051779,"words for WSD of Croatian nouns. Additionally, Karan et al. (2012) consider a problem dual to WSD, namely synonymy detection. To the best of our knowledge, our work is the first reported work on active learning for WSD for a Slavic language. Related Work WSD is a long-standing problem in NLP. A number of semi-supervised WSD methods have been proposed in the literature, including the use of external sources for the generation of sense-tagged data (McCarthy et al., 2004), 2004), use of bilingual corpora (Li and Li, 2004), label propagation (Niu et al., 2005), and bootstrapping (Mihalcea, 2004; Park et al., 2000). Focusing on AL approaches to WSD, one of the first attempts is that of Chklovski and Mihalcea (2002). Their Open Mind World Expert system collected sense-annotated data over the web, which were later used for the Senseval-3 English lexical sample task (Mihalcea et al., 2004). The system employs the so-called committee-based sampling: the instances to be labeled are selected based on the disagreement between the labels assigned by two different classifiers. Chen et al. (2006) experiment with WSD for five frequent English verbs. Unlike Chklovski and Mihalcea, they use uncertainty-based samplin"
W15-5309,J04-1001,0,0.0239873,"e et al. (2002). Bakari´c et al. (2007) analyze the discriminative strength of co-occurring words for WSD of Croatian nouns. Additionally, Karan et al. (2012) consider a problem dual to WSD, namely synonymy detection. To the best of our knowledge, our work is the first reported work on active learning for WSD for a Slavic language. Related Work WSD is a long-standing problem in NLP. A number of semi-supervised WSD methods have been proposed in the literature, including the use of external sources for the generation of sense-tagged data (McCarthy et al., 2004), 2004), use of bilingual corpora (Li and Li, 2004), label propagation (Niu et al., 2005), and bootstrapping (Mihalcea, 2004; Park et al., 2000). Focusing on AL approaches to WSD, one of the first attempts is that of Chklovski and Mihalcea (2002). Their Open Mind World Expert system collected sense-annotated data over the web, which were later used for the Senseval-3 English lexical sample task (Mihalcea et al., 2004). The system employs the so-called committee-based sampling: the instances to be labeled are selected based on the disagreement between the labels assigned by two different classifiers. Chen et al. (2006) experiment with WSD for f"
W15-5309,W14-0405,0,0.0868749,"Missing"
W15-5309,S07-1016,0,0.0918583,"Missing"
W15-5309,W04-0811,0,0.333091,"Missing"
W15-5309,C04-1192,0,0.0939572,"Missing"
W15-5309,I08-1048,0,0.0189717,"3, we describe the manually sense-annotated dataset for Croatian. In Section 4, we describe the AL-based WSD models, while in Section 5 we present and discuss the experimental results. Lastly, Section 6 concludes the paper and outlines future work. 2 typical for WSD due to skewness in sense distribution. They analyze the effect of resampling techniques and show that bootstrap-based oversampling of underrepresented senses improves classifier performance. Another important issue of AL is the stopping condition. Zhu and Hovy (2007) propose a stopping criterion based on the classifier confidence, Wang et al. (2008) propose a minimum expected error strategy, while Zhu et al. (2008a) propose classifier-change as a stopping criterion. Finally, Zhu et al. (2008b) propose sampling methods for generating a representative initial training set, as well as selective sampling method for alleviating the problem of outliers. All of the above cited work addresses WSD for English, whereas our work focuses on Croatian. Similar to Chen et al. (2006), we use uncertaintybased sampling but combine it with an SVM model. In contrast to Chen et al. (2006), we opt for simple, readily available features derived from cooccurren"
W15-5309,D07-1082,0,0.105992,"as follows. In the next section, we give a brief overview of ALbased WSD. In Section 3, we describe the manually sense-annotated dataset for Croatian. In Section 4, we describe the AL-based WSD models, while in Section 5 we present and discuss the experimental results. Lastly, Section 6 concludes the paper and outlines future work. 2 typical for WSD due to skewness in sense distribution. They analyze the effect of resampling techniques and show that bootstrap-based oversampling of underrepresented senses improves classifier performance. Another important issue of AL is the stopping condition. Zhu and Hovy (2007) propose a stopping criterion based on the classifier confidence, Wang et al. (2008) propose a minimum expected error strategy, while Zhu et al. (2008a) propose classifier-change as a stopping criterion. Finally, Zhu et al. (2008b) propose sampling methods for generating a representative initial training set, as well as selective sampling method for alleviating the problem of outliers. All of the above cited work addresses WSD for English, whereas our work focuses on Croatian. Similar to Chen et al. (2006), we use uncertaintybased sampling but combine it with an SVM model. In contrast to Chen"
W15-5309,C08-1142,0,0.0585973,"Missing"
W15-5309,C08-1143,0,0.0215831,"Section 4, we describe the AL-based WSD models, while in Section 5 we present and discuss the experimental results. Lastly, Section 6 concludes the paper and outlines future work. 2 typical for WSD due to skewness in sense distribution. They analyze the effect of resampling techniques and show that bootstrap-based oversampling of underrepresented senses improves classifier performance. Another important issue of AL is the stopping condition. Zhu and Hovy (2007) propose a stopping criterion based on the classifier confidence, Wang et al. (2008) propose a minimum expected error strategy, while Zhu et al. (2008a) propose classifier-change as a stopping criterion. Finally, Zhu et al. (2008b) propose sampling methods for generating a representative initial training set, as well as selective sampling method for alleviating the problem of outliers. All of the above cited work addresses WSD for English, whereas our work focuses on Croatian. Similar to Chen et al. (2006), we use uncertaintybased sampling but combine it with an SVM model. In contrast to Chen et al. (2006), we opt for simple, readily available features derived from cooccurrences. We study three sampling methods in this work, but leave the i"
W15-5309,W02-0817,0,\N,Missing
W16-2815,S12-1051,0,0.11277,"Missing"
W16-2815,W14-2107,1,0.924868,"Missing"
W16-2815,W15-0514,1,0.870688,"Missing"
W16-2815,L16-1200,0,0.0287945,"Missing"
W16-2815,P12-2041,0,0.183153,"aims. 1 Introduction Argumentation mining aims to extract and analyze argumentation expressed in natural language texts. It is an emerging field at the confluence of natural language processing (NLP) and computational argumentation; see (Moens, 2014; Lippi and Torroni, 2016) for a comprehensive overview. Initial work on argumentation mining has focused on well-structured, edited text, such as legal text (Walton, 2005) or scientific publications (Jim´enez-Aleixandre and Erduran, 2007). Recently, the focus has also shifted to argumentation mining from social media texts, such as online debates (Cabrio and Villata, 2012; Habernal et al., 2014; ˇ Boltuˇzi´c and Snajder, 2014), discussions on regulations (Park and Cardie, 2014), product reviews (Ghosh et al., 2014), blogs (Goudas et al., 2014), and tweets (Llewellyn et al., 2014; Bosc et al., 2016). Mining arguments from social media can uncover valuable insights into peoples’ opinions; in this context, it can be thought of as a sophisticated opinion mining technique – one that seeks to uncover the reasons for opinions and patterns of reasoning. The potential applications of social media mining are numerous, especially when done on a large scale. In comparison"
W16-2815,P11-1099,0,0.129429,"Missing"
W16-2815,W14-2106,0,0.0289846,"confluence of natural language processing (NLP) and computational argumentation; see (Moens, 2014; Lippi and Torroni, 2016) for a comprehensive overview. Initial work on argumentation mining has focused on well-structured, edited text, such as legal text (Walton, 2005) or scientific publications (Jim´enez-Aleixandre and Erduran, 2007). Recently, the focus has also shifted to argumentation mining from social media texts, such as online debates (Cabrio and Villata, 2012; Habernal et al., 2014; ˇ Boltuˇzi´c and Snajder, 2014), discussions on regulations (Park and Cardie, 2014), product reviews (Ghosh et al., 2014), blogs (Goudas et al., 2014), and tweets (Llewellyn et al., 2014; Bosc et al., 2016). Mining arguments from social media can uncover valuable insights into peoples’ opinions; in this context, it can be thought of as a sophisticated opinion mining technique – one that seeks to uncover the reasons for opinions and patterns of reasoning. The potential applications of social media mining are numerous, especially when done on a large scale. In comparison to argumentation mining from edited texts, there are additional challenges involved in mining arguments from social media. First, social media te"
W16-2815,D14-1083,0,0.136117,"rded. Finally, the arguments will often lack a proper structure. This is especially true for short texts, such as microbloging posts, which mostly consist of a single claim. When analyzing short and noisy arguments on a large scale, it becomes crucial to identify identical but differently expressed claims across texts. For example, summarizing and analyzing arguments on a controversial topic presupposes that can identify and aggregate identical claims. This task has been addressed in the literature under the name of ˇ argument recognition (Boltuˇzi´c and Snajder, 2014), reason classification (Hasan and Ng, 2014), argument facet similarity (Swanson et al., 2015; Misra et al., 2015), and argument tagging (Sobhani et al., 2015). The task can be decomposed into two subtasks: (1) identifying the main claims for a topic and (2) matching each claim expressed in text to claims identified as the main claims. The focus of this paper is on the latter. The difficulty of the claim matching task arises from the existence of a gap between the user’s claim and the main claim. Many factors contribute to the gap: linguistic variation, implied commonsense knowledge, or implicit premises from the beliefs and value judgm"
W16-2815,llewellyn-etal-2014-using,0,0.0273253,"onal argumentation; see (Moens, 2014; Lippi and Torroni, 2016) for a comprehensive overview. Initial work on argumentation mining has focused on well-structured, edited text, such as legal text (Walton, 2005) or scientific publications (Jim´enez-Aleixandre and Erduran, 2007). Recently, the focus has also shifted to argumentation mining from social media texts, such as online debates (Cabrio and Villata, 2012; Habernal et al., 2014; ˇ Boltuˇzi´c and Snajder, 2014), discussions on regulations (Park and Cardie, 2014), product reviews (Ghosh et al., 2014), blogs (Goudas et al., 2014), and tweets (Llewellyn et al., 2014; Bosc et al., 2016). Mining arguments from social media can uncover valuable insights into peoples’ opinions; in this context, it can be thought of as a sophisticated opinion mining technique – one that seeks to uncover the reasons for opinions and patterns of reasoning. The potential applications of social media mining are numerous, especially when done on a large scale. In comparison to argumentation mining from edited texts, there are additional challenges involved in mining arguments from social media. First, social media texts are more noisy than edited texts, which makes them less amena"
W16-2815,N15-1046,0,0.0537507,"is especially true for short texts, such as microbloging posts, which mostly consist of a single claim. When analyzing short and noisy arguments on a large scale, it becomes crucial to identify identical but differently expressed claims across texts. For example, summarizing and analyzing arguments on a controversial topic presupposes that can identify and aggregate identical claims. This task has been addressed in the literature under the name of ˇ argument recognition (Boltuˇzi´c and Snajder, 2014), reason classification (Hasan and Ng, 2014), argument facet similarity (Swanson et al., 2015; Misra et al., 2015), and argument tagging (Sobhani et al., 2015). The task can be decomposed into two subtasks: (1) identifying the main claims for a topic and (2) matching each claim expressed in text to claims identified as the main claims. The focus of this paper is on the latter. The difficulty of the claim matching task arises from the existence of a gap between the user’s claim and the main claim. Many factors contribute to the gap: linguistic variation, implied commonsense knowledge, or implicit premises from the beliefs and value judgments of the person making the 124 Proceedings of the 3rd Workshop on A"
W16-2815,W14-2105,0,0.0172774,"age texts. It is an emerging field at the confluence of natural language processing (NLP) and computational argumentation; see (Moens, 2014; Lippi and Torroni, 2016) for a comprehensive overview. Initial work on argumentation mining has focused on well-structured, edited text, such as legal text (Walton, 2005) or scientific publications (Jim´enez-Aleixandre and Erduran, 2007). Recently, the focus has also shifted to argumentation mining from social media texts, such as online debates (Cabrio and Villata, 2012; Habernal et al., 2014; ˇ Boltuˇzi´c and Snajder, 2014), discussions on regulations (Park and Cardie, 2014), product reviews (Ghosh et al., 2014), blogs (Goudas et al., 2014), and tweets (Llewellyn et al., 2014; Bosc et al., 2016). Mining arguments from social media can uncover valuable insights into peoples’ opinions; in this context, it can be thought of as a sophisticated opinion mining technique – one that seeks to uncover the reasons for opinions and patterns of reasoning. The potential applications of social media mining are numerous, especially when done on a large scale. In comparison to argumentation mining from edited texts, there are additional challenges involved in mining arguments fro"
W16-2815,W15-0509,0,0.178001,"microbloging posts, which mostly consist of a single claim. When analyzing short and noisy arguments on a large scale, it becomes crucial to identify identical but differently expressed claims across texts. For example, summarizing and analyzing arguments on a controversial topic presupposes that can identify and aggregate identical claims. This task has been addressed in the literature under the name of ˇ argument recognition (Boltuˇzi´c and Snajder, 2014), reason classification (Hasan and Ng, 2014), argument facet similarity (Swanson et al., 2015; Misra et al., 2015), and argument tagging (Sobhani et al., 2015). The task can be decomposed into two subtasks: (1) identifying the main claims for a topic and (2) matching each claim expressed in text to claims identified as the main claims. The focus of this paper is on the latter. The difficulty of the claim matching task arises from the existence of a gap between the user’s claim and the main claim. Many factors contribute to the gap: linguistic variation, implied commonsense knowledge, or implicit premises from the beliefs and value judgments of the person making the 124 Proceedings of the 3rd Workshop on Argument Mining, pages 124–133, c Berlin, Germ"
W16-2815,C14-1142,0,0.0774584,"Missing"
W16-2815,W15-4631,0,0.0293739,"Missing"
W17-0810,cybulska-vossen-2014-using,0,0.442561,"tion 5 we discuss the results. In Section 6 we describe the comparative analysis. Section 7 concludes the paper. 2 Besides the work at these shared tasks, several authors proposed different schemes for event annotation, considering both the linguistic level and the conceptual one. The NewsReader Project (Vossen et al., 2016; Rospocher et al., 2016; Agerri and Rigau, 2016) is an initiative focused on extracting information about what happened to whom, when, and where, processing a large volume of financial and economic data. Within this project, in addition to description schemes (e.g., ECB+. (Cybulska and Vossen, 2014a)) and multilingual semantically annotated corpus of Wikinews articles (Minard et al., 2016a), van Son et al. (2016) propose a framework for annotating perspectives in texts using four different layers, i.e., events, attribution, factuality, and opinion. In the NewsReader Project the annotation is based on the guidelines to detect and annotate markables and relations among markables (Speranza and Minard, 2014). In the detection and annotation of markables, the authors distinguish among entities and entity mention in order to “handle both the annotation of single mentions and of the coreferenc"
W17-0810,J05-1004,0,0.151362,"ach property is associated with some association rules that specify the constraints related to both its syntactic behaviors and the pertinence and the intension of the property itself. In other words, these association rules contribute to the description of the way in which entity classes can be combined through properties in sentence contexts. To formalize such rules in the form of a set of axioms, we take in consideration the possibility of combining semantic and lexical behaviors, suitable for identifying specific event patterns. Thus, for inOur functional annotation differs from PropBank (Palmer et al., 2005) definitions of semantic roles as we do not delineate our functional roles through a verb-by-verb analysis. More concretely, PropBank adds predicate-argument relations to the syntactic trees of the Penn Treebank, representing these relations as framesets, which describe the different sets of roles required for the different meanings of the verb. In contrast, our analysis aims to describe the focus of an event mention by means of identifying actions, which can involve also other lexical elements in addition to the verb. This is easily demonstrated through the example “fire broke out” from Figur"
W17-0810,L16-1699,0,0.0342139,"Missing"
W17-0810,E12-2021,0,0.0621506,"Missing"
W17-0810,W03-0419,0,0.0269221,"Missing"
W17-0810,L16-1187,0,0.0663059,"Missing"
W17-1403,L16-1267,1,0.894594,"Missing"
W17-1403,biemann-2012-turk,0,0.0242425,"h study so far. We compile a small-scale lexical sample dataset and annotate it using three annotation schemes to gain insights into how they affect the annotations. We analyze the obtained substitutes and report on interesting language-specific details, hoping to facilitate research on this topic for other Slavic languages. Finally, we re-implement one of the best-performing models for English lexical substitution (Melamud et al., 2015b) and evaluate it on our dataset. 2 Related Work Most work on lexical substitution was done for English (McCarthy and Navigli, 2007; Sinha and Mihalcea, 2014; Biemann, 2012; Kremer et al., 2014). A few notable exceptions include German within the G ERM E VAL-2015 (Miller et al., 2015), Italian within the EVALITA-2009 (Toral, 2009), and Spanish within a cross-lingual setup at S E M E VAL -2012 (Mihalcea et al., 2010). Recently, most research on lexical substitution closely relates Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 14–19, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics to the task of learning meaning representations that are able to account for multiple senses of polysemous words (Mel"
W17-1403,D07-1007,0,0.0577,"otations, analyze the inter-annotator agreement, and observe a number of interesting language-specific details in the obtained lexical substitutes. Furthermore, we apply a recently-proposed, dependency-based lexical substitution model to our dataset. The model achieves a P@3 score of 0.35, which indicates the difficulty of the task. 1 Introduction Modeling word meaning is one of the most rewarding challenges of many natural language processing (NLP) applications, including information retrieval (Stokoe et al., 2003), information extraction (Ciaramita and Altun, 2006), and machine translation (Carpuat and Wu, 2007), to name a few. Perhaps the most straightforward task concerned with word senses is word sense disambiguation (WSD), a task of determining the correct sense of a polysemous word in its context (Navigli, 2009). Despite being a straightforward task, WSD has several drawbacks. Most often, it is criticized for relying on a fixed set of senses for each of the words (sense inventory), which – although meticulously compiled by experts – is often of inappropriate coverage or granularity (Edmonds and Kilgarriff, 2002; Snyder and Palmer, 2004). This requirement makes evaluation of WSD models across dif"
W17-1403,W06-1670,0,0.0512232,"hree different annotation schemes. We compare the annotations, analyze the inter-annotator agreement, and observe a number of interesting language-specific details in the obtained lexical substitutes. Furthermore, we apply a recently-proposed, dependency-based lexical substitution model to our dataset. The model achieves a P@3 score of 0.35, which indicates the difficulty of the task. 1 Introduction Modeling word meaning is one of the most rewarding challenges of many natural language processing (NLP) applications, including information retrieval (Stokoe et al., 2003), information extraction (Ciaramita and Altun, 2006), and machine translation (Carpuat and Wu, 2007), to name a few. Perhaps the most straightforward task concerned with word senses is word sense disambiguation (WSD), a task of determining the correct sense of a polysemous word in its context (Navigli, 2009). Despite being a straightforward task, WSD has several drawbacks. Most often, it is criticized for relying on a fixed set of senses for each of the words (sense inventory), which – although meticulously compiled by experts – is often of inappropriate coverage or granularity (Edmonds and Kilgarriff, 2002; Snyder and Palmer, 2004). This requi"
W17-1403,J13-3003,0,0.0133783,"within the G ERM E VAL-2015 (Miller et al., 2015), Italian within the EVALITA-2009 (Toral, 2009), and Spanish within a cross-lingual setup at S E M E VAL -2012 (Mihalcea et al., 2010). Recently, most research on lexical substitution closely relates Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 14–19, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics to the task of learning meaning representations that are able to account for multiple senses of polysemous words (Melamud et al., 2015a; Melamud et al., 2016; Roller and Erk, 2016; Erk et al., 2013). For the experiments, we adopt the work of Melamud et al. (2015b), who proposed a lexical substitution model based on dependency-based embeddings. Their model is easy to implement, yet it performs nearly at the state-of-the-art level. their mind. Thus, by allowing the annotators to use MWEs, they could sometimes reach for a more common MWE instead of thinking a bit harder about single-word substitutes. As an example, consider the word preozbiljan (too serious) in the following sentence: 3 In this case, the annotators might more commonly use the idiomatic phrase smrtno ozbiljan (dead serious)"
W17-1403,E14-1057,0,0.149389,"We compile a small-scale lexical sample dataset and annotate it using three annotation schemes to gain insights into how they affect the annotations. We analyze the obtained substitutes and report on interesting language-specific details, hoping to facilitate research on this topic for other Slavic languages. Finally, we re-implement one of the best-performing models for English lexical substitution (Melamud et al., 2015b) and evaluate it on our dataset. 2 Related Work Most work on lexical substitution was done for English (McCarthy and Navigli, 2007; Sinha and Mihalcea, 2014; Biemann, 2012; Kremer et al., 2014). A few notable exceptions include German within the G ERM E VAL-2015 (Miller et al., 2015), Italian within the EVALITA-2009 (Toral, 2009), and Spanish within a cross-lingual setup at S E M E VAL -2012 (Mihalcea et al., 2010). Recently, most research on lexical substitution closely relates Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 14–19, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics to the task of learning meaning representations that are able to account for multiple senses of polysemous words (Melamud et al., 2015a; Me"
W17-1403,P14-2050,0,0.0400263,"l. (2015b) propose four substitutability measures that combine these two concepts in different ways (Table 3). Whereas Add measure employs an arithmetic mean, Mult measure uses a stricter, geometric mean. Furthermore, they introduce Bal variants that balance out the effect of context size. In addition to these models, we use an out-of-context (OOC) model as a baseline, which calculates the substitute score simply as a cosine between the substitute’s and target word’s embedding (also shown in Table 3). Substitutability measures are calculated using dependency-based word and context embeddings (Levy and Goldberg, 2014), which the authors derived from the original skip-gram negative sampling algorithm (SGNS) (Mikolov et al., 2013). In a nutshell, instead of using models that are based solely on lexical contexts, their model can be trained on arbitrary contexts (in their case, the syntactic contexts derived from dependency parse trees). The rationale behind using dependencybased embeddings is that using only regular SGNS embeddings does not account for substitute’s paradigmatic fit in its context. We train these word-type (lemma and POS-tag) embeddings on hrWaC, a Croatian web corpus (Ljubeši´c and Erjavec, 2"
W17-1403,W04-0811,0,0.0849548,"traction (Ciaramita and Altun, 2006), and machine translation (Carpuat and Wu, 2007), to name a few. Perhaps the most straightforward task concerned with word senses is word sense disambiguation (WSD), a task of determining the correct sense of a polysemous word in its context (Navigli, 2009). Despite being a straightforward task, WSD has several drawbacks. Most often, it is criticized for relying on a fixed set of senses for each of the words (sense inventory), which – although meticulously compiled by experts – is often of inappropriate coverage or granularity (Edmonds and Kilgarriff, 2002; Snyder and Palmer, 2004). This requirement makes evaluation of WSD models across different applications rather difficult. An alternative perspective on modeling word senses is the one of lexical substitution (McCarthy 14 and Navigli, 2007), a task of finding a meaningpreserving replacement of a polysemous target word in context. For instance, in the sentence “It took me around two hours to reach Nagoya from Kyoto by coach”, suitable substitutes for the word coach may be van or bus, whereas the substitute trainer represents a different sense of the word. Note that such a setup circumvents the need of having a fixed se"
W17-1403,S07-1009,0,0.145726,"bstitution task for the Croatian language, a first such study so far. We compile a small-scale lexical sample dataset and annotate it using three annotation schemes to gain insights into how they affect the annotations. We analyze the obtained substitutes and report on interesting language-specific details, hoping to facilitate research on this topic for other Slavic languages. Finally, we re-implement one of the best-performing models for English lexical substitution (Melamud et al., 2015b) and evaluate it on our dataset. 2 Related Work Most work on lexical substitution was done for English (McCarthy and Navigli, 2007; Sinha and Mihalcea, 2014; Biemann, 2012; Kremer et al., 2014). A few notable exceptions include German within the G ERM E VAL-2015 (Miller et al., 2015), Italian within the EVALITA-2009 (Toral, 2009), and Spanish within a cross-lingual setup at S E M E VAL -2012 (Mihalcea et al., 2010). Recently, most research on lexical substitution closely relates Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 14–19, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics to the task of learning meaning representations that are able to account for"
W17-1403,N15-1050,0,0.288653,"ed substitutes in various ways (e.g., variety, count, etc.). In this paper, we report on a preliminary study of the lexical substitution task for the Croatian language, a first such study so far. We compile a small-scale lexical sample dataset and annotate it using three annotation schemes to gain insights into how they affect the annotations. We analyze the obtained substitutes and report on interesting language-specific details, hoping to facilitate research on this topic for other Slavic languages. Finally, we re-implement one of the best-performing models for English lexical substitution (Melamud et al., 2015b) and evaluate it on our dataset. 2 Related Work Most work on lexical substitution was done for English (McCarthy and Navigli, 2007; Sinha and Mihalcea, 2014; Biemann, 2012; Kremer et al., 2014). A few notable exceptions include German within the G ERM E VAL-2015 (Miller et al., 2015), Italian within the EVALITA-2009 (Toral, 2009), and Spanish within a cross-lingual setup at S E M E VAL -2012 (Mihalcea et al., 2010). Recently, most research on lexical substitution closely relates Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 14–19, c Valencia, Spain, 4 Apr"
W17-1403,W15-1501,0,0.0380918,"Missing"
W17-1403,K16-1006,0,0.0127436,"4). A few notable exceptions include German within the G ERM E VAL-2015 (Miller et al., 2015), Italian within the EVALITA-2009 (Toral, 2009), and Spanish within a cross-lingual setup at S E M E VAL -2012 (Mihalcea et al., 2010). Recently, most research on lexical substitution closely relates Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 14–19, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics to the task of learning meaning representations that are able to account for multiple senses of polysemous words (Melamud et al., 2015a; Melamud et al., 2016; Roller and Erk, 2016; Erk et al., 2013). For the experiments, we adopt the work of Melamud et al. (2015b), who proposed a lexical substitution model based on dependency-based embeddings. Their model is easy to implement, yet it performs nearly at the state-of-the-art level. their mind. Thus, by allowing the annotators to use MWEs, they could sometimes reach for a more common MWE instead of thinking a bit harder about single-word substitutes. As an example, consider the word preozbiljan (too serious) in the following sentence: 3 In this case, the annotators might more commonly use the idiomat"
W17-1403,S10-1002,0,0.0174706,"pecific details, hoping to facilitate research on this topic for other Slavic languages. Finally, we re-implement one of the best-performing models for English lexical substitution (Melamud et al., 2015b) and evaluate it on our dataset. 2 Related Work Most work on lexical substitution was done for English (McCarthy and Navigli, 2007; Sinha and Mihalcea, 2014; Biemann, 2012; Kremer et al., 2014). A few notable exceptions include German within the G ERM E VAL-2015 (Miller et al., 2015), Italian within the EVALITA-2009 (Toral, 2009), and Spanish within a cross-lingual setup at S E M E VAL -2012 (Mihalcea et al., 2010). Recently, most research on lexical substitution closely relates Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 14–19, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics to the task of learning meaning representations that are able to account for multiple senses of polysemous words (Melamud et al., 2015a; Melamud et al., 2016; Roller and Erk, 2016; Erk et al., 2013). For the experiments, we adopt the work of Melamud et al. (2015b), who proposed a lexical substitution model based on dependency-based embeddings. Their model is eas"
W17-1403,N16-1131,0,0.144582,"ptions include German within the G ERM E VAL-2015 (Miller et al., 2015), Italian within the EVALITA-2009 (Toral, 2009), and Spanish within a cross-lingual setup at S E M E VAL -2012 (Mihalcea et al., 2010). Recently, most research on lexical substitution closely relates Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 14–19, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics to the task of learning meaning representations that are able to account for multiple senses of polysemous words (Melamud et al., 2015a; Melamud et al., 2016; Roller and Erk, 2016; Erk et al., 2013). For the experiments, we adopt the work of Melamud et al. (2015b), who proposed a lexical substitution model based on dependency-based embeddings. Their model is easy to implement, yet it performs nearly at the state-of-the-art level. their mind. Thus, by allowing the annotators to use MWEs, they could sometimes reach for a more common MWE instead of thinking a bit harder about single-word substitutes. As an example, consider the word preozbiljan (too serious) in the following sentence: 3 In this case, the annotators might more commonly use the idiomatic phrase smrtno ozbil"
W17-1403,P10-1097,0,0.0232746,"pcos(s, t)|C |· c∈C pcos(s, c) OOC cos(s, t) Table 3: The different substitutability measures for a lexical substitute s of a target word t within a context C.6 Metric Models GAP P@3 P@5 Add BalAdd Mult BalMult OOC 0.28 0.26 0.27 0.28 0.26 0.35 0.31 0.28 0.31 0.21 0.28 0.26 0.27 0.28 0.25 Table 4: Model scores on our dataset. annotators for a specific target word (i.e., across all target word’s instances). This enables us to basically evaluate the model’s ability of identifying the viable substitutes and ranking low the ones that bear a sense different of that evoked in a context. Following (Thater et al., 2010), we evaluate the models in terms of generalized average precision (GAP) (Kishida, 2005). GAP is a weighted extension of the mean average precision (MAP) measure, where weights capture how many times the annotators used a certain substitute in a goldset. In line with work of Roller and Erk (2016), we decided not to use the original lexical substitution metrics (oot and best), but standard P@3 and P@5 scores, which we find more interpretable. We report the results in Table 4. We observe that the model based on Add substitutability measure consistently performs best. Usually, out of the top thre"
W17-1409,baccianella-etal-2010-sentiwordnet,0,0.0939463,"to be compiled manually. If there is no sentiment-labeled data available, sentiment lexicons can be used directly for sentiment classification: the text is simply classified as positive if it contains more words from a positive than a negative lexicon, and classified as negative otherwise (we refer to this as lexicon word-counting models). On the other hand, if sentiment-labeled data is available, sentiment lexicons can be used as (additional) features for supervised sentiment classification models. One challenge of sentiment analysis is that the task is highly domain dependent (Turney, 2002; Baccianella et al., 2010). This means that generic sentiment lexicons will often not be useful for a specific domain. A notorious example is the word unpredictable, which is typically positive in the domain of movie and book reviews, but generally negative in other domains. The aim of this paper is to investigate how sentiment lexicons work for domain-specific sentiment classification for Croatian. Our main goal is to find out whether sentiment lexicons can be of use for sentiment classification, either as a part of a simple word-counting model or as an addition to a supervised model using word-representation features"
W17-1409,P07-1124,0,0.0266116,"ctively they can be used in sentiment classification. Our results indicate that, even with as few as 500 labeled instances, a supervised model substantially outperforms a word-counting model. We also observe that adding lexicon-based features does not significantly improve supervised sentiment classification. 1 Introduction Sentiment analysis (Pang et al., 2008) aims to recognize both subjectivity and polarity of texts, information that can be beneficial in various applications, including social studies (O’Connor et al., 2010), marketing analyses (He et al., 2013), and stock price prediction (Devitt and Ahmad, 2007). In general, however, building a well-performing sentiment analysis model requires a fair amount of sentiment-labeled data, whose collection is often costly and time-consuming. A popular annotationlight alternative are sentiment polarity lexicons (Taboada et al., 2011): lists of positive and negative words that most likely induce the correspond54 ing sentiment. The key selling points of sentiment lexicons are that they are interpretable and quite easy to be compiled manually. If there is no sentiment-labeled data available, sentiment lexicons can be used directly for sentiment classification:"
W17-1409,P07-1054,0,0.0237535,"in-specific, corpus-based seed set (DC1) – Starting from the 45 most frequent ngrams, we circularly assigned one n-gram to the positive, negative, and the neutral seed set, until all n-grams were exhausted (a round-robin approach). We used this seed set as a baseline. An example of a domain-specific, humancompiled seed set is shown in Table 1. Sentiment propagation. To propagate sentiment labels across graph nodes, we used the PageRank algorithm (Page et al., 1999). Since PageRank was originally designed to rank web pages by their relevance, we adapted it for sentiment propagation, following (Esuli and Sebastiani, 2007; Glavaš et al., 2012a). In each iteration, node scores were computed using the power iteration method: a(k) = αa(k−1) W + (1 − α)e where W is the weighted adjacency matrix of the graph, a is the computed vector of node scores, e Positive seeds Negative seeds Neutral seeds Croatian Translation hvala, zanimati, nov, dobar, brzina, super, lijepo, zadovoljan, besplatan, ostati, riješiti, biti zadovoljan, uredno, brzi, hvala vi nemati, problem, ne mo´ci, kvar, ne raditi, cˇ ekati, biti problem, prigovor, raskid, katastrofa, sramota, zlo, raskid ugovor, oti´ci, smetnja thanks, to interest, new, goo"
W17-1409,W12-0501,1,0.868168,"Missing"
W17-1409,P97-1023,0,0.860005,"a is lacking. Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 54–59, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics 2 Related Work There has been a lot of research on sentiment lexicon acquisition, covering both corpora- and resource-based approaches across many languages (Taboada et al., 2006; Kaji and Kitsuregawa, 2007; Lu et al., 2010; Rao and Ravichandran, 2009; Turney and Littman, 2003). A common approach includes bootstrapping, a method which constructs a sentiment lexicon starting from a small manuallylabeled seed set (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Moreover, a problem of lexicon domain dependence has also been addressed (Kanayama and Nasukawa, 2006). Even though most research on sentiment lexicon acquisition and lexicon-based sentiment classification deals with English, there has been some work on Slavic languages as well, including Macedonian (Jovanoski et al., 2015), Croatian (Glavaš et al., 2012b), Slovene (Fišer et al., 2016), and Serbian (Mladenovi´c et al., 2016). While we follow the work of Glavaš et al. (2012b), who focused on the task of semi-supervised lexicon acqusition, we turn our attention to ev"
W17-1409,R15-1034,0,0.0222983,"aji and Kitsuregawa, 2007; Lu et al., 2010; Rao and Ravichandran, 2009; Turney and Littman, 2003). A common approach includes bootstrapping, a method which constructs a sentiment lexicon starting from a small manuallylabeled seed set (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Moreover, a problem of lexicon domain dependence has also been addressed (Kanayama and Nasukawa, 2006). Even though most research on sentiment lexicon acquisition and lexicon-based sentiment classification deals with English, there has been some work on Slavic languages as well, including Macedonian (Jovanoski et al., 2015), Croatian (Glavaš et al., 2012b), Slovene (Fišer et al., 2016), and Serbian (Mladenovi´c et al., 2016). While we follow the work of Glavaš et al. (2012b), who focused on the task of semi-supervised lexicon acqusition, we turn our attention to evaluating the so-obtained lexicons on the task of sentiment classification. 3 Lexicon Acquisition 3.1 Dataset For our experiments, we used a large sentimentannotated dataset of user posts gathered from the Facebook pages of various Croatian internet and mobile service providers.1 The dataset comprises 15,718 user posts categorized into three classes: po"
W17-1409,D07-1115,0,0.0373386,"lexicon-based models on the task of domain-specific sentiment classification and compare them against supervised models. Finally, we investigate whether a word-counting model can have an edge over a supervised model when the labeled data is lacking. Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 54–59, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics 2 Related Work There has been a lot of research on sentiment lexicon acquisition, covering both corpora- and resource-based approaches across many languages (Taboada et al., 2006; Kaji and Kitsuregawa, 2007; Lu et al., 2010; Rao and Ravichandran, 2009; Turney and Littman, 2003). A common approach includes bootstrapping, a method which constructs a sentiment lexicon starting from a small manuallylabeled seed set (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Moreover, a problem of lexicon domain dependence has also been addressed (Kanayama and Nasukawa, 2006). Even though most research on sentiment lexicon acquisition and lexicon-based sentiment classification deals with English, there has been some work on Slavic languages as well, including Macedonian (Jovanoski et al., 2015),"
W17-1409,W06-1642,0,0.145384,"Missing"
W17-1409,E09-1077,0,0.0332568,"ecific sentiment classification and compare them against supervised models. Finally, we investigate whether a word-counting model can have an edge over a supervised model when the labeled data is lacking. Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 54–59, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics 2 Related Work There has been a lot of research on sentiment lexicon acquisition, covering both corpora- and resource-based approaches across many languages (Taboada et al., 2006; Kaji and Kitsuregawa, 2007; Lu et al., 2010; Rao and Ravichandran, 2009; Turney and Littman, 2003). A common approach includes bootstrapping, a method which constructs a sentiment lexicon starting from a small manuallylabeled seed set (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Moreover, a problem of lexicon domain dependence has also been addressed (Kanayama and Nasukawa, 2006). Even though most research on sentiment lexicon acquisition and lexicon-based sentiment classification deals with English, there has been some work on Slavic languages as well, including Macedonian (Jovanoski et al., 2015), Croatian (Glavaš et al., 2012b), Slovene (Fiš"
W17-1409,J11-2001,0,0.0836554,"e supervised sentiment classification. 1 Introduction Sentiment analysis (Pang et al., 2008) aims to recognize both subjectivity and polarity of texts, information that can be beneficial in various applications, including social studies (O’Connor et al., 2010), marketing analyses (He et al., 2013), and stock price prediction (Devitt and Ahmad, 2007). In general, however, building a well-performing sentiment analysis model requires a fair amount of sentiment-labeled data, whose collection is often costly and time-consuming. A popular annotationlight alternative are sentiment polarity lexicons (Taboada et al., 2011): lists of positive and negative words that most likely induce the correspond54 ing sentiment. The key selling points of sentiment lexicons are that they are interpretable and quite easy to be compiled manually. If there is no sentiment-labeled data available, sentiment lexicons can be used directly for sentiment classification: the text is simply classified as positive if it contains more words from a positive than a negative lexicon, and classified as negative otherwise (we refer to this as lexicon word-counting models). On the other hand, if sentiment-labeled data is available, sentiment le"
W17-1409,P02-1053,0,0.045247,"nd quite easy to be compiled manually. If there is no sentiment-labeled data available, sentiment lexicons can be used directly for sentiment classification: the text is simply classified as positive if it contains more words from a positive than a negative lexicon, and classified as negative otherwise (we refer to this as lexicon word-counting models). On the other hand, if sentiment-labeled data is available, sentiment lexicons can be used as (additional) features for supervised sentiment classification models. One challenge of sentiment analysis is that the task is highly domain dependent (Turney, 2002; Baccianella et al., 2010). This means that generic sentiment lexicons will often not be useful for a specific domain. A notorious example is the word unpredictable, which is typically positive in the domain of movie and book reviews, but generally negative in other domains. The aim of this paper is to investigate how sentiment lexicons work for domain-specific sentiment classification for Croatian. Our main goal is to find out whether sentiment lexicons can be of use for sentiment classification, either as a part of a simple word-counting model or as an addition to a supervised model using w"
W17-1409,taboada-etal-2006-methods,0,\N,Missing
W17-1411,W13-2408,0,0.0645751,"Missing"
W17-1411,P16-1231,0,0.0709622,"Missing"
W17-1411,I13-1041,0,0.022508,"he more popular applications include political popularity (O’Connor et al., 2010) and stock price prediction (Devitt and Ahmad, 2007). Social media texts, including user reviews (Tang et al., 2009; Pontiki et al., 2014) and microblogs (Nakov et al., 2016; Kouloumpis et al., 2011), are particularly amenable to sentiment analysis, with applications in social studies (O’Connor et al., 2010; Wang et al., 2012) and marketing analyses (He et al., 2013; Yu et al., 2013). At the same time, social media poses a great challenge for sentiment analysis, as such texts are often short, informal, and noisy (Baldwin et al., 2013), and make heavy use of figurative language (Ghosh et al., 2015; Buschmeier et al., 2014). Sentiment analysis is most often framed as 69 a supervised classification task. Many approaches resort to rich, domain-specific features (Wilson et al., 2009; Abbasi et al., 2008), including surfaceform, lexicon-based, and syntactic features. On the other hand, there has been a growing trend in using feature-light methods, including neural word embeddings (Maas et al., 2011; Socher et al., 2013) and kernel-based methods (Culotta and Sorensen, 2004; Lodhi et al., 2002a; Srivastava et al., 2013). In partic"
W17-1411,S15-2080,0,0.0164657,"r et al., 2010) and stock price prediction (Devitt and Ahmad, 2007). Social media texts, including user reviews (Tang et al., 2009; Pontiki et al., 2014) and microblogs (Nakov et al., 2016; Kouloumpis et al., 2011), are particularly amenable to sentiment analysis, with applications in social studies (O’Connor et al., 2010; Wang et al., 2012) and marketing analyses (He et al., 2013; Yu et al., 2013). At the same time, social media poses a great challenge for sentiment analysis, as such texts are often short, informal, and noisy (Baldwin et al., 2013), and make heavy use of figurative language (Ghosh et al., 2015; Buschmeier et al., 2014). Sentiment analysis is most often framed as 69 a supervised classification task. Many approaches resort to rich, domain-specific features (Wilson et al., 2009; Abbasi et al., 2008), including surfaceform, lexicon-based, and syntactic features. On the other hand, there has been a growing trend in using feature-light methods, including neural word embeddings (Maas et al., 2011; Socher et al., 2013) and kernel-based methods (Culotta and Sorensen, 2004; Lodhi et al., 2002a; Srivastava et al., 2013). In particular, two methods that stand out in terms of both their simplic"
W17-1411,W13-2404,1,0.856902,"on informal texts. Word embeddings (Mikolov et al., 2013a) and string kernels (Lodhi et al., 2002b) present an alternative to syntax-based methods. Tang et al. (2014) and Maas et al. (2011) learn sentiment-specific word embeddings, while Le and Mikolov (2014) reach state-of-the-art performance for both short and long sentiment classification of English texts. Zhang et al. (2008) report impressive performance on Chinese reviews using string kernels. There has been limited research on sentiment analysis for Croatian. Bidin ¯ et al. (2014) applied MV-RNN to prediction of phrase sentiment, while Glavaš et al. (2013) addressed aspect-based sentiment analysis using a feature-rich model. More recently, Mozetiˇc et al. (2016) presented a multilingual study of sentiment-labeled tweets and sentiment classification in different languages, including Croatian. However, they experiment only with classifiers using standard bag-of-words features. 3 Datasets We conducted our comparison on three short-text datasets in Croatian.1 The datasets differ in domain, genre, size, and the number of classes. Table 1 summarizes the datasets’ statistics. Game reviews (GR). This dataset originally consisted of longer reviews of co"
W17-1411,D15-1089,0,0.0738304,"Spain, 4 April 2017. 2017 Association for Computational Linguistics e.g., (Thelwall et al., 2010; Kiritchenko et al., 2014), especially within the recent SemEval evaluation campaigns (Nakov et al., 2016; Rosenthal et al., 2015; Rosenthal et al., 2014). Recent research has focused on sentence-level sentiment classification using neural networks: Socher et al. (2012) and Socher et al. (2013) report impressive results using a matrix-vector recursive neural network (MVRNN) and recursive neural tensor networks models over parse trees. Tree kernels present an alternative to neural-based approaches: Kim et al. (2015) and Srivastava et al. (2013) use tree kernels on sentence dependency trees and achieve competitive results. However, as noted by Le and Mikolov (2014), while syntax-based methods work well at the sentence level, it is not straightforward to extend them to fragments spanning multiple sentences. Another downside of these methods is that they rely on parsing, which often fails on informal texts. Word embeddings (Mikolov et al., 2013a) and string kernels (Lodhi et al., 2002b) present an alternative to syntax-based methods. Tang et al. (2014) and Maas et al. (2011) learn sentiment-specific word em"
W17-1411,W14-0405,0,0.0356675,"Missing"
W17-1411,P11-1015,0,0.276977,"the same time, social media poses a great challenge for sentiment analysis, as such texts are often short, informal, and noisy (Baldwin et al., 2013), and make heavy use of figurative language (Ghosh et al., 2015; Buschmeier et al., 2014). Sentiment analysis is most often framed as 69 a supervised classification task. Many approaches resort to rich, domain-specific features (Wilson et al., 2009; Abbasi et al., 2008), including surfaceform, lexicon-based, and syntactic features. On the other hand, there has been a growing trend in using feature-light methods, including neural word embeddings (Maas et al., 2011; Socher et al., 2013) and kernel-based methods (Culotta and Sorensen, 2004; Lodhi et al., 2002a; Srivastava et al., 2013). In particular, two methods that stand out in terms of both their simplicity and effectiveness are word embeddings (Mikolov et al., 2013a) and string kernels (Lodhi et al., 2002b). In this paper we focus on sentiment classification of short text in Croatian, a morphologically complex South Slavic language. We compare two simple yet effective methods – word embeddings and string kernels – which are often used in text classification tasks. While both methods are easy to set"
W17-1411,S14-2004,0,0.028651,"t, on two out of three datasets, word embeddings outperform string kernels, which in turn outperform word and n-gram bag-of-words baselines. 1 Introduction Sentiment analysis (Pang and Lee, 2008) – a task of predicting whether the text expresses a positive, negative, or neutral opinion in general or with respect to an entity – has attracted considerable attention over the last two decades. Some of the more popular applications include political popularity (O’Connor et al., 2010) and stock price prediction (Devitt and Ahmad, 2007). Social media texts, including user reviews (Tang et al., 2009; Pontiki et al., 2014) and microblogs (Nakov et al., 2016; Kouloumpis et al., 2011), are particularly amenable to sentiment analysis, with applications in social studies (O’Connor et al., 2010; Wang et al., 2012) and marketing analyses (He et al., 2013; Yu et al., 2013). At the same time, social media poses a great challenge for sentiment analysis, as such texts are often short, informal, and noisy (Baldwin et al., 2013), and make heavy use of figurative language (Ghosh et al., 2015; Buschmeier et al., 2014). Sentiment analysis is most often framed as 69 a supervised classification task. Many approaches resort to r"
W17-1411,S14-2009,0,0.0161314,"ly labeled for sentiment polarity, using different levels of morphological preprocessing. To spur further research, we make one dataset publicly available. 2 Related Work Sentiment classification for short and informal texts has been the focus of considerable research, Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 69–75, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics e.g., (Thelwall et al., 2010; Kiritchenko et al., 2014), especially within the recent SemEval evaluation campaigns (Nakov et al., 2016; Rosenthal et al., 2015; Rosenthal et al., 2014). Recent research has focused on sentence-level sentiment classification using neural networks: Socher et al. (2012) and Socher et al. (2013) report impressive results using a matrix-vector recursive neural network (MVRNN) and recursive neural tensor networks models over parse trees. Tree kernels present an alternative to neural-based approaches: Kim et al. (2015) and Srivastava et al. (2013) use tree kernels on sentence dependency trees and achieve competitive results. However, as noted by Le and Mikolov (2014), while syntax-based methods work well at the sentence level, it is not straightfor"
W17-1411,S15-2078,0,0.0124235,"sets in Croatian, manually labeled for sentiment polarity, using different levels of morphological preprocessing. To spur further research, we make one dataset publicly available. 2 Related Work Sentiment classification for short and informal texts has been the focus of considerable research, Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 69–75, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics e.g., (Thelwall et al., 2010; Kiritchenko et al., 2014), especially within the recent SemEval evaluation campaigns (Nakov et al., 2016; Rosenthal et al., 2015; Rosenthal et al., 2014). Recent research has focused on sentence-level sentiment classification using neural networks: Socher et al. (2012) and Socher et al. (2013) report impressive results using a matrix-vector recursive neural network (MVRNN) and recursive neural tensor networks models over parse trees. Tree kernels present an alternative to neural-based approaches: Kim et al. (2015) and Srivastava et al. (2013) use tree kernels on sentence dependency trees and achieve competitive results. However, as noted by Le and Mikolov (2014), while syntax-based methods work well at the sentence lev"
W17-1411,P13-2137,1,0.873095,"Missing"
W17-1411,D12-1110,0,0.0115519,"make one dataset publicly available. 2 Related Work Sentiment classification for short and informal texts has been the focus of considerable research, Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 69–75, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics e.g., (Thelwall et al., 2010; Kiritchenko et al., 2014), especially within the recent SemEval evaluation campaigns (Nakov et al., 2016; Rosenthal et al., 2015; Rosenthal et al., 2014). Recent research has focused on sentence-level sentiment classification using neural networks: Socher et al. (2012) and Socher et al. (2013) report impressive results using a matrix-vector recursive neural network (MVRNN) and recursive neural tensor networks models over parse trees. Tree kernels present an alternative to neural-based approaches: Kim et al. (2015) and Srivastava et al. (2013) use tree kernels on sentence dependency trees and achieve competitive results. However, as noted by Le and Mikolov (2014), while syntax-based methods work well at the sentence level, it is not straightforward to extend them to fragments spanning multiple sentences. Another downside of these methods is that they rely on"
W17-1411,D13-1170,0,0.0366698,"ial media poses a great challenge for sentiment analysis, as such texts are often short, informal, and noisy (Baldwin et al., 2013), and make heavy use of figurative language (Ghosh et al., 2015; Buschmeier et al., 2014). Sentiment analysis is most often framed as 69 a supervised classification task. Many approaches resort to rich, domain-specific features (Wilson et al., 2009; Abbasi et al., 2008), including surfaceform, lexicon-based, and syntactic features. On the other hand, there has been a growing trend in using feature-light methods, including neural word embeddings (Maas et al., 2011; Socher et al., 2013) and kernel-based methods (Culotta and Sorensen, 2004; Lodhi et al., 2002a; Srivastava et al., 2013). In particular, two methods that stand out in terms of both their simplicity and effectiveness are word embeddings (Mikolov et al., 2013a) and string kernels (Lodhi et al., 2002b). In this paper we focus on sentiment classification of short text in Croatian, a morphologically complex South Slavic language. We compare two simple yet effective methods – word embeddings and string kernels – which are often used in text classification tasks. While both methods are easy to set up, they differ in ter"
W17-1411,D13-1144,0,0.18659,"l, and noisy (Baldwin et al., 2013), and make heavy use of figurative language (Ghosh et al., 2015; Buschmeier et al., 2014). Sentiment analysis is most often framed as 69 a supervised classification task. Many approaches resort to rich, domain-specific features (Wilson et al., 2009; Abbasi et al., 2008), including surfaceform, lexicon-based, and syntactic features. On the other hand, there has been a growing trend in using feature-light methods, including neural word embeddings (Maas et al., 2011; Socher et al., 2013) and kernel-based methods (Culotta and Sorensen, 2004; Lodhi et al., 2002a; Srivastava et al., 2013). In particular, two methods that stand out in terms of both their simplicity and effectiveness are word embeddings (Mikolov et al., 2013a) and string kernels (Lodhi et al., 2002b). In this paper we focus on sentiment classification of short text in Croatian, a morphologically complex South Slavic language. We compare two simple yet effective methods – word embeddings and string kernels – which are often used in text classification tasks. While both methods are easy to set up, they differ in terms of preprocessing required: word embeddings require a sizable, possibly lemmatized corpus, whereas"
W17-1411,P14-1146,0,0.0234912,"rnels present an alternative to neural-based approaches: Kim et al. (2015) and Srivastava et al. (2013) use tree kernels on sentence dependency trees and achieve competitive results. However, as noted by Le and Mikolov (2014), while syntax-based methods work well at the sentence level, it is not straightforward to extend them to fragments spanning multiple sentences. Another downside of these methods is that they rely on parsing, which often fails on informal texts. Word embeddings (Mikolov et al., 2013a) and string kernels (Lodhi et al., 2002b) present an alternative to syntax-based methods. Tang et al. (2014) and Maas et al. (2011) learn sentiment-specific word embeddings, while Le and Mikolov (2014) reach state-of-the-art performance for both short and long sentiment classification of English texts. Zhang et al. (2008) report impressive performance on Chinese reviews using string kernels. There has been limited research on sentiment analysis for Croatian. Bidin ¯ et al. (2014) applied MV-RNN to prediction of phrase sentiment, while Glavaš et al. (2013) addressed aspect-based sentiment analysis using a feature-rich model. More recently, Mozetiˇc et al. (2016) presented a multilingual study of sent"
W17-1411,P12-3020,0,0.0118404,"8) – a task of predicting whether the text expresses a positive, negative, or neutral opinion in general or with respect to an entity – has attracted considerable attention over the last two decades. Some of the more popular applications include political popularity (O’Connor et al., 2010) and stock price prediction (Devitt and Ahmad, 2007). Social media texts, including user reviews (Tang et al., 2009; Pontiki et al., 2014) and microblogs (Nakov et al., 2016; Kouloumpis et al., 2011), are particularly amenable to sentiment analysis, with applications in social studies (O’Connor et al., 2010; Wang et al., 2012) and marketing analyses (He et al., 2013; Yu et al., 2013). At the same time, social media poses a great challenge for sentiment analysis, as such texts are often short, informal, and noisy (Baldwin et al., 2013), and make heavy use of figurative language (Ghosh et al., 2015; Buschmeier et al., 2014). Sentiment analysis is most often framed as 69 a supervised classification task. Many approaches resort to rich, domain-specific features (Wilson et al., 2009; Abbasi et al., 2008), including surfaceform, lexicon-based, and syntactic features. On the other hand, there has been a growing trend in u"
W17-1411,J09-3003,0,0.0451757,"2016; Kouloumpis et al., 2011), are particularly amenable to sentiment analysis, with applications in social studies (O’Connor et al., 2010; Wang et al., 2012) and marketing analyses (He et al., 2013; Yu et al., 2013). At the same time, social media poses a great challenge for sentiment analysis, as such texts are often short, informal, and noisy (Baldwin et al., 2013), and make heavy use of figurative language (Ghosh et al., 2015; Buschmeier et al., 2014). Sentiment analysis is most often framed as 69 a supervised classification task. Many approaches resort to rich, domain-specific features (Wilson et al., 2009; Abbasi et al., 2008), including surfaceform, lexicon-based, and syntactic features. On the other hand, there has been a growing trend in using feature-light methods, including neural word embeddings (Maas et al., 2011; Socher et al., 2013) and kernel-based methods (Culotta and Sorensen, 2004; Lodhi et al., 2002a; Srivastava et al., 2013). In particular, two methods that stand out in terms of both their simplicity and effectiveness are word embeddings (Mikolov et al., 2013a) and string kernels (Lodhi et al., 2002b). In this paper we focus on sentiment classification of short text in Croatian,"
W17-1411,P04-1054,0,\N,Missing
W17-1411,W14-2608,0,\N,Missing
W17-1411,S13-2052,0,\N,Missing
W17-1412,agic-ljubesic-2014-setimes,0,0.0728339,"Missing"
W17-1412,M98-1001,0,0.552765,"revolving around a certain “focus” entity. The main rationale of such a setup is to foster development of “all-rounder” NER and cross-lingual entity matching solutions that are not tailored to specific, narrow domains. The shared task was organized in the context of the 6th Balto-Slavic Natural Language Processing Workshop co-located with the EACL 2017 conference. Similar shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were carried out in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first shared task focusing on multilingual named entity recognition, which covered some European languages, including Spanish, German, and Dutch, was organized in the context of CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The NE types covered in these campaigns were similar to the NE types covered in our Challenge. Also related to our task is the Entity Discovery and Linking (EDL) track (Ji et al., 2014; Ji et al., 2015) of the NIST Text Analysis Conferences (TAC). EDL aimed to extract entity mentions from"
W17-1412,doddington-etal-2004-automatic,0,0.348774,"ity. The main rationale of such a setup is to foster development of “all-rounder” NER and cross-lingual entity matching solutions that are not tailored to specific, narrow domains. The shared task was organized in the context of the 6th Balto-Slavic Natural Language Processing Workshop co-located with the EACL 2017 conference. Similar shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were carried out in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first shared task focusing on multilingual named entity recognition, which covered some European languages, including Spanish, German, and Dutch, was organized in the context of CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The NE types covered in these campaigns were similar to the NE types covered in our Challenge. Also related to our task is the Entity Discovery and Linking (EDL) track (Ji et al., 2014; Ji et al., 2015) of the NIST Text Analysis Conferences (TAC). EDL aimed to extract entity mentions from a collection of textual documents in multiple l"
W17-1412,W17-1413,0,0.0438103,"Missing"
W17-1412,W17-1414,0,0.0586994,"te a working solution, only two teams submitted results within the deadline. A total of two unique runs were submitted. JHU/APL team attempted the NER and Entity Matching sub-tasks. They employed a statistical 82 tagger called SVMLattice (Mayfield et al., 2003), with NER labels inferred by projecting English tags across bitext. The Illinois tagger (Ratinov and Roth, 2009) was used for English. A rule-based entity clusterer called “kripke” was used for Entity Matching (McNamee et al., 2013). The team (code “jhu”) attempted all languages available in the Challenge. More details can be found in (Mayfield et al., 2017). The G4.19 Research Group adapted Liner2 (Marci´nczuk et al., 2013)—a generic framework which can be used to solve various tasks based on sequence labeling, which is equipped with a set of modules (based on statistical models, dictionaries, rules and heuristics) which recognize and annotate certain types of phrases. The details of tuning Liner2 to tackle the shared task are described in (Marci´nczuk et al., 2017). The team (code “pw”) attempted only the Polish-language Challenge. The above systems met the deadline to participate in the first run of the Challenge—Phase I. Since the Challenge a"
W17-1412,P16-1060,0,0.06896,"Missing"
W17-1412,P13-1069,0,0.0148193,"tance, the inflected form of the Polish proper name Europejskiego Funduszu Rozwoju Regionalnego (EuropeanGEN FundGEN DevelopmentGEN RegionalGEN ) consists of two basic genitive noun phrases, of which only the first one (“European Fund”) needs to be normalized, whereas the second (“Regional Development”) should remain unchanged. The corresponding base form is “Europejski Fundusz Rozwoju Regionalnego”. Since in some Slavic languages adjectives may precede or follow a noun in a noun phrase (like in the example above), detection of inner syntactic structure of complex proper names is not trivial (Radziszewski, 2013), and thus complicates the process of automated lemmatization. Complex person name declension paradigms (Piskorski et al., 2009) add another level of complexity. It is worth mentioning that, for the sake of compliance with the NER guidelines in Section 2, documents that included hard-to-decide entity mention annotations were excluded from the test datasets for the present. A case in point is a document in Croatian that contained the phrase “Zagrebaˇcka, Sisaˇcko-Moslavaˇcka i Karlovaˇcka županija”—a contracted version of 80 three named entities (“Zagrebaˇcka županija”, Genitive Nominative (“ba"
W17-1412,W09-1119,0,0.0219001,"ems Eleven teams from seven countries—Czech Republic, Germany, India, Poland, Russia, Slovenia, and USA—registered for the evaluation task and received the trial datasets. Due to the complexity of the task and relatively short time available to create a working solution, only two teams submitted results within the deadline. A total of two unique runs were submitted. JHU/APL team attempted the NER and Entity Matching sub-tasks. They employed a statistical 82 tagger called SVMLattice (Mayfield et al., 2003), with NER labels inferred by projecting English tags across bitext. The Illinois tagger (Ratinov and Roth, 2009) was used for English. A rule-based entity clusterer called “kripke” was used for Entity Matching (McNamee et al., 2013). The team (code “jhu”) attempted all languages available in the Challenge. More details can be found in (Mayfield et al., 2017). The G4.19 Research Group adapted Liner2 (Marci´nczuk et al., 2013)—a generic framework which can be used to solve various tasks based on sequence labeling, which is equipped with a set of modules (based on statistical models, dictionaries, rules and heuristics) which recognize and annotate certain types of phrases. The details of tuning Liner2 to t"
W17-1412,W03-0419,0,0.841094,"Missing"
W17-1412,W02-2024,0,0.606186,"Balto-Slavic Natural Language Processing Workshop co-located with the EACL 2017 conference. Similar shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were carried out in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first shared task focusing on multilingual named entity recognition, which covered some European languages, including Spanish, German, and Dutch, was organized in the context of CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The NE types covered in these campaigns were similar to the NE types covered in our Challenge. Also related to our task is the Entity Discovery and Linking (EDL) track (Ji et al., 2014; Ji et al., 2015) of the NIST Text Analysis Conferences (TAC). EDL aimed to extract entity mentions from a collection of textual documents in multiple languages (English, Chinese, and Spanish), and to partition the entities into cross-document equivalence classes, by either linking mentions to a knowledge base or directly Proceedings of the 6th Workshop on Balto-Slavic Nat"
W17-1412,W10-2403,0,\N,Missing
W17-1412,W07-1701,0,\N,Missing
W17-1412,W16-2709,0,\N,Missing
W17-1727,W03-1812,0,0.036954,"sider n-grams for which the number of tokens between the first and final constituent is less than or equal to 2n. On the other hand, permutation of MWE constituents is much less frequent, even for a relatively free word order language such as Croatian. Thus, there may be a benefit to capturing which types of MWE – presumably mostly characterized by their POS patterns – allow for permutations; e.g., jednim udarcem ubiti dvije muhe / dvije muhe ubiti jednim udarcem, etc. (to kill two flies with one stone). Finally, inspired by a growing body of research on semantic non-compositionality of MWEs (Baldwin et al., 2003; Kim and Baldwin, 2006; Biemann and Giesbrecht, 2011; Krˇcmáˇr et al., 2013), we introduced a simple semantic opacity feature. We opted for a simple approach proposed by (Mitchell and Lapata, 2008), and computed this feature by deriving distributional vectors from hrWaC for the MWE and the additive composition of its con195 words depends upon both features. Finally, since similes and hyphenated expressions signal a strict word order, we defined interactions between perm, adjac, hyphen, and simile. foreign hyphen fossil caps assoc 3 frozen perm MWE syntax simile comp adjac context partial over"
W17-1727,W11-1304,0,0.0331207,"between the first and final constituent is less than or equal to 2n. On the other hand, permutation of MWE constituents is much less frequent, even for a relatively free word order language such as Croatian. Thus, there may be a benefit to capturing which types of MWE – presumably mostly characterized by their POS patterns – allow for permutations; e.g., jednim udarcem ubiti dvije muhe / dvije muhe ubiti jednim udarcem, etc. (to kill two flies with one stone). Finally, inspired by a growing body of research on semantic non-compositionality of MWEs (Baldwin et al., 2003; Kim and Baldwin, 2006; Biemann and Giesbrecht, 2011; Krˇcmáˇr et al., 2013), we introduced a simple semantic opacity feature. We opted for a simple approach proposed by (Mitchell and Lapata, 2008), and computed this feature by deriving distributional vectors from hrWaC for the MWE and the additive composition of its con195 words depends upon both features. Finally, since similes and hyphenated expressions signal a strict word order, we defined interactions between perm, adjac, hyphen, and simile. foreign hyphen fossil caps assoc 3 frozen perm MWE syntax simile comp adjac context partial overlap Figure 1: Bayesian network for MWE classification"
W17-1727,J90-1003,0,0.339507,"comparing against (semi)naïve Bayes models, we demonstrate that manually modeling feature interactions is indeed important. We make our annotated dataset of Croatian MWEs freely available. 1 Introduction Multiword expressions (MWEs) have attracted a great deal of attention in the natural language processing community. While MWEs span a wide range of types, common to all is the idiosyncrasy at the lexical, syntactic, semantic, pragmatic, or statistical level (Baldwin and Kim, 2010). A variety of models has been proposed for the automatic identification of MWE in corpora, including statistical (Church and Hanks, 1990; Lin, 1999; Pecina, 2010) and linguistic-based approaches (Cook et al., 2007; Baldwin, 2005; Green et al., 2011); see (Ramisch, 2015) for a recent overview. Sag et al. (2002) argued for a combination of the two approaches. Recently, Tsvetkov and Wintner (2014) proposed an approach for the detection of MWE candidates that combines a number of statistical and linguistic features. The most interesting aspect of their work is that they explicitly model the linguistically motivated interactions between the features using a Bayesian network (BN). The advantages of BNs lie in their interpretability"
W17-1727,W07-1106,0,0.0291186,"feature interactions is indeed important. We make our annotated dataset of Croatian MWEs freely available. 1 Introduction Multiword expressions (MWEs) have attracted a great deal of attention in the natural language processing community. While MWEs span a wide range of types, common to all is the idiosyncrasy at the lexical, syntactic, semantic, pragmatic, or statistical level (Baldwin and Kim, 2010). A variety of models has been proposed for the automatic identification of MWE in corpora, including statistical (Church and Hanks, 1990; Lin, 1999; Pecina, 2010) and linguistic-based approaches (Cook et al., 2007; Baldwin, 2005; Green et al., 2011); see (Ramisch, 2015) for a recent overview. Sag et al. (2002) argued for a combination of the two approaches. Recently, Tsvetkov and Wintner (2014) proposed an approach for the detection of MWE candidates that combines a number of statistical and linguistic features. The most interesting aspect of their work is that they explicitly model the linguistically motivated interactions between the features using a Bayesian network (BN). The advantages of BNs lie in their interpretability and the possibility to encode linguistic knowledge in the form of the network"
W17-1727,D11-1067,0,0.0591672,"Missing"
W17-1727,W06-2110,0,0.0419123,"h the number of tokens between the first and final constituent is less than or equal to 2n. On the other hand, permutation of MWE constituents is much less frequent, even for a relatively free word order language such as Croatian. Thus, there may be a benefit to capturing which types of MWE – presumably mostly characterized by their POS patterns – allow for permutations; e.g., jednim udarcem ubiti dvije muhe / dvije muhe ubiti jednim udarcem, etc. (to kill two flies with one stone). Finally, inspired by a growing body of research on semantic non-compositionality of MWEs (Baldwin et al., 2003; Kim and Baldwin, 2006; Biemann and Giesbrecht, 2011; Krˇcmáˇr et al., 2013), we introduced a simple semantic opacity feature. We opted for a simple approach proposed by (Mitchell and Lapata, 2008), and computed this feature by deriving distributional vectors from hrWaC for the MWE and the additive composition of its con195 words depends upon both features. Finally, since similes and hyphenated expressions signal a strict word order, we defined interactions between perm, adjac, hyphen, and simile. foreign hyphen fossil caps assoc 3 frozen perm MWE syntax simile comp adjac context partial overlap Figure 1: Bayesian"
W17-1727,W13-3208,0,0.0310257,"Missing"
W17-1727,P99-1041,0,0.236046,"naïve Bayes models, we demonstrate that manually modeling feature interactions is indeed important. We make our annotated dataset of Croatian MWEs freely available. 1 Introduction Multiword expressions (MWEs) have attracted a great deal of attention in the natural language processing community. While MWEs span a wide range of types, common to all is the idiosyncrasy at the lexical, syntactic, semantic, pragmatic, or statistical level (Baldwin and Kim, 2010). A variety of models has been proposed for the automatic identification of MWE in corpora, including statistical (Church and Hanks, 1990; Lin, 1999; Pecina, 2010) and linguistic-based approaches (Cook et al., 2007; Baldwin, 2005; Green et al., 2011); see (Ramisch, 2015) for a recent overview. Sag et al. (2002) argued for a combination of the two approaches. Recently, Tsvetkov and Wintner (2014) proposed an approach for the detection of MWE candidates that combines a number of statistical and linguistic features. The most interesting aspect of their work is that they explicitly model the linguistically motivated interactions between the features using a Bayesian network (BN). The advantages of BNs lie in their interpretability and the pos"
W17-1727,P08-1028,0,0.0456276,"for a relatively free word order language such as Croatian. Thus, there may be a benefit to capturing which types of MWE – presumably mostly characterized by their POS patterns – allow for permutations; e.g., jednim udarcem ubiti dvije muhe / dvije muhe ubiti jednim udarcem, etc. (to kill two flies with one stone). Finally, inspired by a growing body of research on semantic non-compositionality of MWEs (Baldwin et al., 2003; Kim and Baldwin, 2006; Biemann and Giesbrecht, 2011; Krˇcmáˇr et al., 2013), we introduced a simple semantic opacity feature. We opted for a simple approach proposed by (Mitchell and Lapata, 2008), and computed this feature by deriving distributional vectors from hrWaC for the MWE and the additive composition of its con195 words depends upon both features. Finally, since similes and hyphenated expressions signal a strict word order, we defined interactions between perm, adjac, hyphen, and simile. foreign hyphen fossil caps assoc 3 frozen perm MWE syntax simile comp adjac context partial overlap Figure 1: Bayesian network for MWE classification stituents, and then computing the cosine between the two vectors. For opaque MWEs, we expect the cosine to be lower than for semantically transp"
W17-1727,J14-2007,0,0.484687,"deal of attention in the natural language processing community. While MWEs span a wide range of types, common to all is the idiosyncrasy at the lexical, syntactic, semantic, pragmatic, or statistical level (Baldwin and Kim, 2010). A variety of models has been proposed for the automatic identification of MWE in corpora, including statistical (Church and Hanks, 1990; Lin, 1999; Pecina, 2010) and linguistic-based approaches (Cook et al., 2007; Baldwin, 2005; Green et al., 2011); see (Ramisch, 2015) for a recent overview. Sag et al. (2002) argued for a combination of the two approaches. Recently, Tsvetkov and Wintner (2014) proposed an approach for the detection of MWE candidates that combines a number of statistical and linguistic features. The most interesting aspect of their work is that they explicitly model the linguistically motivated interactions between the features using a Bayesian network (BN). The advantages of BNs lie in their interpretability and the possibility to encode linguistic knowledge in the form of the network structure. Furthermore, unlike most previous work, Tsvetkov and Wintner address MWE of various types and flexible syntactic constructions. They show that the manually-designed BN outp"
W17-4201,balahur-etal-2010-sentiment,0,0.0652338,"Missing"
W17-4201,S07-1072,0,0.0964553,"Missing"
W17-4201,E17-4007,0,0.020231,"basis of two main goals: headlines that represent the abstract of the main event and headlines that promote one of the details in the news story (Bell, 1991; Nir, 1993). Furthermore, Iarovici and Amel 2 Related work Despite the fact that news values has been widely investigated in Social Science and journalism studies, not much attention has been paid to its automatic classification by the NLP community. In fact, even if news value classification may be applied in several user-oriented applications, e.g., news recommendation systems, and web search engines, few scholars (De Nies et al., 2012; Piotrkowicz et al., 2017) have been focused on this particular topic. Related to our work is the work on predicting 1 Proceedings of the 2017 EMNLP Workshop on Natural Language Processing meets Journalism, pages 1–6 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics emotions in news articles and headlines, which has been investigated from different perspectives and by means of different techniques. Strapparava and Mihalcea (2008) describe an experiment devoted to analyze emotion in news headlines, focusing on six basic emotions and proposing knowledgebased and corpus-based approac"
W17-4201,S07-1013,0,0.0580233,"e Cohen’s κ and F1-macro IAA agreement scores for the 11 news value labels. We observe a moderate agreement of κ ≥ 0.4 (Landis and Koch, 1977) only for the “Bad news”, “Celebrity”, and “Entertainment” news values, suggesting that recognizing news values from headlines is a difficult task even for humans. To obtain the final dataset, we adjudicated the annotations of the three annotators my a majority vote. The adjudicated IAA is moderate/substantial, except for “Magnitude”, “Shareability”, and “Surprise”. Dataset As a starting point, we adopt the dataset proposed for the SemEval-2007 Task 14 (Strapparava and Mihalcea, 2007). The dataset consists of 1250 headlines extracted from major newspapers such as New York Times, CNN, BBC News, and Google News. Each headline has been manually annotated for valence and six emotions (Anger, Disgust, Fear, Joy, Sadness, and Surprise) on a scale from 0 to 100. In this work, we use only the emotion labels, and not the valence labels. News values. On top of the emotion annotations, we added an additional layer of news value labels. Our starting point for the annotation was the news values classification scheme proposed by Harcup and O’Neill (2016). This study proposes a set of fi"
W17-4201,C00-2137,0,0.18962,"two positive emotions, joy and surprise, which are the kernels of two sub-groups: joy is related to “Good news”, “Shareability” and, to a lesser extent, to “Magnitude”, while surprise emotions relates to “Entertainment” and “Surprise” news values. 3 SVM News value Bad news Celebrity Conflict Drama Entertainment Good news Magnitude Power elite Shareability T 0.652 0.553 0.526 0.636 0.832 0.414 0.299 0.596 0.309 the models. Models for the remaining nine news values were trained successfully and outperform a random baseline (the differences are significant at p<0.001; two-sided permutation test (Yeh, 2000)). We can make three main observations. First, there is a considerable variance in performance across the news values: “Bad news” and “Entertainment” seems to be the easiest to predict, whereas “Shareability”, “Magnitude”, and “Celebrity” are more difficult. Secondly, by comparing “T” and “T+E” variants of the models, we observe that adding emotions as features improves leads to further improvements for the “Bad news” and “Entertainment” news values (differences are significant at p<0.05) for CNN, and for SVM also for “Magnitude”, but for other news values adding emotions did not improve the p"
W17-5210,W16-2815,1,0.898151,"Missing"
W17-5210,W14-2109,0,0.0151129,"relations, concepts are domain dependent and need to be defined for each new topic. Let R, C, and M denote the set of relations, concepts, and modalities, respectively. Formally, we define a claim microstructure as a quadruple (m1 , m2 , o2 , r), where m1 ∈ M and (optionally) m2 ∈ M ∪ {} are the modalities, o2 ∈ C ∪ {} is the (optional) second opinion holder, and r = (t, c1 , c2 ) ∈ R is the (possibly higherorder) relation between two concepts or relations c1 , c2 ∈ C ∪ R, conveyed by the relation type t. Table 1 defines the relation types used in this work. It should be noted that, unlike Aharoni et al. (2014), who consider as claims only the statements that directly support or contest the debating topic, we consider all statements with propositional content. For example, in the context of gay rights, ‘belief(purpose(Life, Love))’ is a valid claim in our framework, although it does not support nor contest the topic, i.e., the stance of that claim is neutral. Modalities. We furthermore observed that the claims express different modalities, which can roughly be categorized into beliefs, value judgments, and policies. We formalize this via unary relations ‘believes’, ‘approves’, and ‘desires’, corresp"
W17-5210,D15-1255,0,0.0441865,"Missing"
W17-5210,W11-1701,0,0.0359406,"line discussions, users express their opinions using more or less well structured arguments. The building blocks of these arguments are claims: statements that are in dispute and that we are trying to support with reason Govier (2013). Claims can support or attack other claims, giving rise to complex argumentative structures. Thus, the ability to identify and analyze claims in text is a crucial part of argumentation mining (Moens, 2014; Lippi and Torroni, 2016). Outside the realm of wellstructured argumentation, the ability to analyze claims is crucial for tasks such as stance classification (Anand et al., 2011; Hasan and Ng, 2013; Mohammad et al., 2016) and fine-grained opinion analysis (Stoyanov and Cardie, 2008; Yang and Cardie, 2013), as well as the converging task of argument-based opinion mining (Clos et al., 2014; ˇ Boltuˇzi´c and Snajder, 2014), which aims to uncover the reasons underpinning the opinions. Previous research has tackled the claim detection task for diverse domains, including legal docu74 Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 74–80 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for"
W17-5210,I13-1191,0,0.0260641,"ers express their opinions using more or less well structured arguments. The building blocks of these arguments are claims: statements that are in dispute and that we are trying to support with reason Govier (2013). Claims can support or attack other claims, giving rise to complex argumentative structures. Thus, the ability to identify and analyze claims in text is a crucial part of argumentation mining (Moens, 2014; Lippi and Torroni, 2016). Outside the realm of wellstructured argumentation, the ability to analyze claims is crucial for tasks such as stance classification (Anand et al., 2011; Hasan and Ng, 2013; Mohammad et al., 2016) and fine-grained opinion analysis (Stoyanov and Cardie, 2008; Yang and Cardie, 2013), as well as the converging task of argument-based opinion mining (Clos et al., 2014; ˇ Boltuˇzi´c and Snajder, 2014), which aims to uncover the reasons underpinning the opinions. Previous research has tackled the claim detection task for diverse domains, including legal docu74 Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 74–80 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Lingui"
W17-5210,D14-1083,0,0.0467807,"Missing"
W17-5210,E17-1024,0,0.0189198,"ering and Computing Text Analysis and Knowledge Engineering Lab Unska 3, 10000 Zagreb, Croatia {filip.boltuzic,jan.snajder}@fer.hr Abstract ments (Palau and Moens, 2009), microtexts (Peldszus and Stede, 2015), Wikipedia articles (Aharoni et al., 2014; Levy et al., 2014; Rinott et al., 2015), student essays (Stab and Gurevych, 2017), and user-generated web discourse (Habernal and ˇ Gurevych, 2015). Boltuˇzi´c and Snajder (2015) addressed the task of identifying prominent claims in ˇ online debates, while Boltuˇzi´c and Snajder (2016) analyzed the implicit premises between two claims. Recently, Bar-Haim et al. (2017) introduced the claim stance classification task, where classification is done at the claim rather than document level. In this paper, we address the task of claim analysis from a different angle. While prior work has dealt with claims as textual fragments, we study the possibility of a more precise, domain-specific analysis of claims based on their internal logical structure. The work closest to ours is that of Wyner and Van Engers (2010) and Wyner et al. (2016), who explored normalizing claims from the policy making domain by translating them to Attempto Controlled English (Fuchs et al., 200"
W17-5210,C14-1141,0,0.0514256,"Missing"
W17-5210,W14-2107,1,0.900299,"Missing"
W17-5210,W15-0514,1,0.886714,"Missing"
W17-5210,C00-2137,0,0.0748173,"Missing"
W17-5210,S16-1003,0,0.0618907,"Missing"
W17-5210,D15-1110,0,0.053781,"Missing"
W17-5210,D15-1050,0,0.0350719,"Missing"
W17-5210,S16-2021,0,0.0388074,"Missing"
W17-5210,J17-3005,0,0.0295884,"Missing"
W17-5210,C08-1103,0,0.043121,"uilding blocks of these arguments are claims: statements that are in dispute and that we are trying to support with reason Govier (2013). Claims can support or attack other claims, giving rise to complex argumentative structures. Thus, the ability to identify and analyze claims in text is a crucial part of argumentation mining (Moens, 2014; Lippi and Torroni, 2016). Outside the realm of wellstructured argumentation, the ability to analyze claims is crucial for tasks such as stance classification (Anand et al., 2011; Hasan and Ng, 2013; Mohammad et al., 2016) and fine-grained opinion analysis (Stoyanov and Cardie, 2008; Yang and Cardie, 2013), as well as the converging task of argument-based opinion mining (Clos et al., 2014; ˇ Boltuˇzi´c and Snajder, 2014), which aims to uncover the reasons underpinning the opinions. Previous research has tackled the claim detection task for diverse domains, including legal docu74 Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 74–80 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics and show that, even with a simple encoding of microstructures as features, we g"
W18-1112,luyckx-daelemans-2008-personae,0,0.0970194,"Missing"
W18-1112,W15-2913,0,0.0312496,"017). The growing interest in personality prediction gave rise to two shared tasks (Celli et al., 2013; Rangel et al., 2015), which relied on benchmark datasets labeled with Big Five types. The overarching conclusion was that the personality prediction is a challenging task because there are no strongly predictive features. However, the results suggested that n-gram based models consistently yield good performance across the different languages. Presumably due to its controversy, the MBTI model has thus far been less used for personality prediction. This has changed, however, with the work of Plank and Hovy (2015), who made use of the MBTI popularity among general public and collected a dataset of over 1.2 million status updates on Twitter and leveraged users’ self-reported personality types (Plank and Hovy, 2015). Soon thereafter, Verhoeven et al. (2016) published a multilingual dataset TwiSty. Background and Related Work Personality and language are closely related – as a matter of fact, the Big Five model emerged from a statistical analysis of the English lexicon (Digman, 1990). Ensuing research in psychology attempted to establish links between personality and language use (Pennebaker and King, 199"
W18-1112,W17-2903,0,0.0646558,"Missing"
W18-1112,J16-3007,0,0.109515,"Missing"
W18-1112,D15-1309,0,0.0567124,"ish lexicon (Digman, 1990). Ensuing research in psychology attempted to establish links between personality and language use (Pennebaker and King, 1999), setting the ground for research on automated personality prediction. Most early studies in personality predic3 4 Our personality prediction dataset is derived from Reddit. Reddit has previously been used as a source of data for various studies. De Choudhury and De (2014) studied mental health discourse and concluded that Reddit users openly share their experiences and challenges with mental illnesses in their personal and professional lives. Schrading et al. (2015) studied domestic abuse and found that http://www.alexa.com/topsites http://redditmetrics.com 88 abuse-related discussion groups have more tightknit communities, longer posts and comments, and less discourse than non-abusive groups. Wallace et al. (2014) tackled irony detection and concluded that Reddit provides a lot of context, which can help in dealing with the ambiguous cases. Shen and Rudzicz (2017) achieved good results in anxiety classification using the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker et al., 2003), n-grams, and topic modeling. To the best of our knowled"
W18-1112,W17-3107,0,0.0748338,"hury and De (2014) studied mental health discourse and concluded that Reddit users openly share their experiences and challenges with mental illnesses in their personal and professional lives. Schrading et al. (2015) studied domestic abuse and found that http://www.alexa.com/topsites http://redditmetrics.com 88 abuse-related discussion groups have more tightknit communities, longer posts and comments, and less discourse than non-abusive groups. Wallace et al. (2014) tackled irony detection and concluded that Reddit provides a lot of context, which can help in dealing with the ambiguous cases. Shen and Rudzicz (2017) achieved good results in anxiety classification using the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker et al., 2003), n-grams, and topic modeling. To the best of our knowledge, ours it the first work on using Reddit as a source of data for personality prediction. 3 3.1 different, often ambiguous ways. In some cases it may be difficult to determine whether the flair refers to a personality type. For example, “KentJude” is not an MBTI type even though it contains the ENTJ acronym, a clue being that it is not written in all caps. In other cases, determining the type requires s"
W18-1112,L16-1258,0,0.0266364,"ediction is a challenging task because there are no strongly predictive features. However, the results suggested that n-gram based models consistently yield good performance across the different languages. Presumably due to its controversy, the MBTI model has thus far been less used for personality prediction. This has changed, however, with the work of Plank and Hovy (2015), who made use of the MBTI popularity among general public and collected a dataset of over 1.2 million status updates on Twitter and leveraged users’ self-reported personality types (Plank and Hovy, 2015). Soon thereafter, Verhoeven et al. (2016) published a multilingual dataset TwiSty. Background and Related Work Personality and language are closely related – as a matter of fact, the Big Five model emerged from a statistical analysis of the English lexicon (Digman, 1990). Ensuing research in psychology attempted to establish links between personality and language use (Pennebaker and King, 1999), setting the ground for research on automated personality prediction. Most early studies in personality predic3 4 Our personality prediction dataset is derived from Reddit. Reddit has previously been used as a source of data for various studie"
W18-1112,P14-2084,0,0.0408989,"predic3 4 Our personality prediction dataset is derived from Reddit. Reddit has previously been used as a source of data for various studies. De Choudhury and De (2014) studied mental health discourse and concluded that Reddit users openly share their experiences and challenges with mental illnesses in their personal and professional lives. Schrading et al. (2015) studied domestic abuse and found that http://www.alexa.com/topsites http://redditmetrics.com 88 abuse-related discussion groups have more tightknit communities, longer posts and comments, and less discourse than non-abusive groups. Wallace et al. (2014) tackled irony detection and concluded that Reddit provides a lot of context, which can help in dealing with the ambiguous cases. Shen and Rudzicz (2017) achieved good results in anxiety classification using the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker et al., 2003), n-grams, and topic modeling. To the best of our knowledge, ours it the first work on using Reddit as a source of data for personality prediction. 3 3.1 different, often ambiguous ways. In some cases it may be difficult to determine whether the flair refers to a personality type. For example, “KentJude” is no"
W18-4422,W15-4319,0,0.0753075,"Missing"
W18-4422,W17-3007,0,0.0240866,"Missing"
W18-4422,W17-3013,0,0.0402974,"Missing"
W18-4422,gao-huang-2017-detecting,0,0.0135752,"emon...? Not good job.....this guis creating a problem n our socacity pakistani team a world racod 373 run in 20 over. I visited 5 atm but I cont able to withdraw from money..not working.. Your nation is neither islamic nor humane OAG NAG CAG NAG CAG OAG Table 1: An excerpt from the train set (OAG – openly aggressive, CAG – covertly aggressive, NAG – not aggressive). but in a combination with a gated rectified unit (GRU) layer. The work of Potapova and Gordeev (2016) describes an approach to detect aggressive speech using CNN and random forest separately, but not their combination. Similarly, Gao and Huang (2017) explore a logistic regression model with a rich set of features and a bidirectional LSTM, without combining the models. Djuric et al. (2015) employ a logistic regression layer on top of representations learned on a huge hate-speech data set using paragraph2vec (Le and Mikolov, 2014). Several varieties of recurrent neural networks (RNN) as well as a CNN were also evaluated in (Pavlopoulos et al., 2017), yielding good results. A survey of related work indicates that, while both shallow and deep learning models have been extensively tested on this task, the approaches that build on their combina"
W18-4422,D15-1089,0,0.0349732,"Missing"
W18-4422,W18-4401,0,0.0291786,"Missing"
W18-4422,W02-0109,0,0.0727585,". The first model we considered was logistic regression. We explored three variants, each based on unigrams, bigrams, and character n-grams, respectively. For each of these variants we also include the following additional features: • Bad word occurrences – boolean feature which indicates if the text contains a word from a list of inappropriate and likely offensive words, obtained from the web;3 • POS tags – number of occurrences for nouns, verbs, adverbs, adjectives, foreign words, and cardinal numbers (a total of six numerical features). We extract the counts using the POS tagger from NLTK (Loper and Bird, 2002); • Text length in both characters and tokens (two numerical features); • Capitalization features – the number of words that are capitalized and the number of words written in all caps (two numerical features); • Numerical tokens – the number of tokens that represent a number; • Named entities – the number of named entities; three numerical features corresponding to counts of named entities of type person, organization and location, respectively. We used the named entity recognizer (NER) from NLTK (Loper and Bird, 2002) to extract the named entities; • Sentiment polarity – a single numerical f"
W18-4422,malmasi-zampieri-2017-detecting,0,0.0370537,"s. Initial studies dealing with the detection of aggressive speech (or its many subtypes) rely on traditional text classification techniques, such as the naive Bayes classifier (Kwok and Wang, 2013; Chen et al., 2012; Dinakar et al., 2011), logistic regression (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Burnap and Williams, 2015), or support vector machines (SVM) (Xu et al., 2012; Dadvar et al., 2013; Schofield and Davidson, 2017). For example, the work of Van Hee et al. (2015) focuses on classifying different subtypes of cyberbullying using an SVM. In a similar vein, Malmasi and Zampieri (2017) built a system to discern between hate speech and mere profanity using a combination of traditional models. Recent work also features dictionary based approaches for detecting abusive language in languages other than English (Tulkens et al., 2016; Mubarak et al., 2017). Other approaches focus on users instead of single texts, such as the work of Ribeiro et al. (2018), where the goal was to determine which social networks users resort to hate speech. They employed gradient boosting, adaptive boosting, and a semi-supervised learning method. Notable is also the work of Nobata et al. (2016), who"
W18-4422,W16-3638,0,0.0289874,"re few and far between. Approaches most related to ours explore combinations of traditional and deep learning methods. Badjatiya et al. (2017) focus on detecting racism and sexism using a combination of models from different paradigms, specifically LSTM in combination with gradient boosting. Similarly, Park and Fung (2017) use logistic regression in combination with several variants of CNNs in a two step scenario. The first step distinguishes between abusive and non-abusive texts, while the second step distinguishes between different subtypes of abuse. The work most similar to ours is that of Mehdad and Tetreault (2016), where an SVM metaclassifier is trained on outputs of a variant of SVM- and an RNN-based model. 3 Dataset The shared task dataset consists of 15,000 Facebook messages (train and validation portion combined), out of which 3,419 are labelled as openly aggressive, 5,297 as covertly aggressive, and 6,284 as notaggressive. Table 1 lists some examples from the dataset. As mentioned in the introduction, data labeling was a challenging task as there is a certain degree of subjectivity present when determining the aggression type. From our experience, the most problematic are the openly vs. covertly a"
W18-4422,W17-3008,0,0.0181868,"y, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Burnap and Williams, 2015), or support vector machines (SVM) (Xu et al., 2012; Dadvar et al., 2013; Schofield and Davidson, 2017). For example, the work of Van Hee et al. (2015) focuses on classifying different subtypes of cyberbullying using an SVM. In a similar vein, Malmasi and Zampieri (2017) built a system to discern between hate speech and mere profanity using a combination of traditional models. Recent work also features dictionary based approaches for detecting abusive language in languages other than English (Tulkens et al., 2016; Mubarak et al., 2017). Other approaches focus on users instead of single texts, such as the work of Ribeiro et al. (2018), where the goal was to determine which social networks users resort to hate speech. They employed gradient boosting, adaptive boosting, and a semi-supervised learning method. Notable is also the work of Nobata et al. (2016), who employ a rich feature set and the regression model from Vowpal Wabbit (Langford et al., 2007), which outperformed even deep learning-based models. In recent years deep learning-based approaches have become increasingly popular for this task. Pitsilis et al. (2018) worke"
W18-4422,W17-3006,0,0.018377,"urrent neural networks (RNN) as well as a CNN were also evaluated in (Pavlopoulos et al., 2017), yielding good results. A survey of related work indicates that, while both shallow and deep learning models have been extensively tested on this task, the approaches that build on their combination are few and far between. Approaches most related to ours explore combinations of traditional and deep learning methods. Badjatiya et al. (2017) focus on detecting racism and sexism using a combination of models from different paradigms, specifically LSTM in combination with gradient boosting. Similarly, Park and Fung (2017) use logistic regression in combination with several variants of CNNs in a two step scenario. The first step distinguishes between abusive and non-abusive texts, while the second step distinguishes between different subtypes of abuse. The work most similar to ours is that of Mehdad and Tetreault (2016), where an SVM metaclassifier is trained on outputs of a variant of SVM- and an RNN-based model. 3 Dataset The shared task dataset consists of 15,000 Facebook messages (train and validation portion combined), out of which 3,419 are labelled as openly aggressive, 5,297 as covertly aggressive, and"
W18-4422,W17-3004,0,0.0179248,"ted rectified unit (GRU) layer. The work of Potapova and Gordeev (2016) describes an approach to detect aggressive speech using CNN and random forest separately, but not their combination. Similarly, Gao and Huang (2017) explore a logistic regression model with a rich set of features and a bidirectional LSTM, without combining the models. Djuric et al. (2015) employ a logistic regression layer on top of representations learned on a huge hate-speech data set using paragraph2vec (Le and Mikolov, 2014). Several varieties of recurrent neural networks (RNN) as well as a CNN were also evaluated in (Pavlopoulos et al., 2017), yielding good results. A survey of related work indicates that, while both shallow and deep learning models have been extensively tested on this task, the approaches that build on their combination are few and far between. Approaches most related to ours explore combinations of traditional and deep learning methods. Badjatiya et al. (2017) focus on detecting racism and sexism using a combination of models from different paradigms, specifically LSTM in combination with gradient boosting. Similarly, Park and Fung (2017) use logistic regression in combination with several variants of CNNs in a"
W18-4422,D14-1162,0,0.0809015,"Missing"
W18-4422,P13-1147,0,0.0253477,"Missing"
W18-4422,W17-1101,0,0.0271942,"n 6. 2 Related Work Aggressive speech i.e., abusive language on the web, comes in many flavours, including racism, sexism, homophobia, trolling, cyberbullying etc. Waseem et al. (2017) proposed a typology for various sub-types of abusive language. Similarly, an annotation schema for socially unacceptable discourse practices was proposed by Fiˇser et al. (2017). Focusing on microblogging, Founta et al. (2018) propose a characterization of abusive behaviour on Twitter. There is a considerable body of work dealing with detecting various types of abusive language; a good overview can be found in (Schmidt and Wiegand, 2017). While distinguishing between aggressive and non-aggressive speech is a already a challenging task, distinguishing between different subtypes of aggressive speech is of course even more difficult. This was observed for the case of open vs. covert aggression by Malmasi and Zampieri (2018), and served as primary motivation for this shared task (Kumar et al., 2018). A separate issue, which further increases the difficulty of the task, is that there is no universally agreedupon definition of aggressive speech – a situation which negatively affects the reliability of annotated data. A study by Ros"
W18-4422,R15-1086,0,0.0682298,"Missing"
W18-4422,N16-2013,0,0.0350115,"peech definitions to annotators can better align their view with the definition, but this does not positively affect annotation reliability. A related study by Waseem (2016) indicated that having expert knowledge during annotation can result in less annotation effort, but this does not necessarily lead to overall better prediction models. Initial studies dealing with the detection of aggressive speech (or its many subtypes) rely on traditional text classification techniques, such as the naive Bayes classifier (Kwok and Wang, 2013; Chen et al., 2012; Dinakar et al., 2011), logistic regression (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Burnap and Williams, 2015), or support vector machines (SVM) (Xu et al., 2012; Dadvar et al., 2013; Schofield and Davidson, 2017). For example, the work of Van Hee et al. (2015) focuses on classifying different subtypes of cyberbullying using an SVM. In a similar vein, Malmasi and Zampieri (2017) built a system to discern between hate speech and mere profanity using a combination of traditional models. Recent work also features dictionary based approaches for detecting abusive language in languages other than English (Tulkens et al., 2016; Mubarak"
W18-4422,W17-3012,0,0.0173122,"out of 31 teams on the Facebook and Twitter test sets, respectively. The rest of this paper is structured as follows. In Section 2 we give a brief overview of existing work on aggression detection and related tasks. Section 3 presents the data set, while the machine learning models we use are explained in Section 4. In Section 5 we present and discuss the results, followed by a conclusion and ideas for future improvements in Section 6. 2 Related Work Aggressive speech i.e., abusive language on the web, comes in many flavours, including racism, sexism, homophobia, trolling, cyberbullying etc. Waseem et al. (2017) proposed a typology for various sub-types of abusive language. Similarly, an annotation schema for socially unacceptable discourse practices was proposed by Fiˇser et al. (2017). Focusing on microblogging, Founta et al. (2018) propose a characterization of abusive behaviour on Twitter. There is a considerable body of work dealing with detecting various types of abusive language; a good overview can be found in (Schmidt and Wiegand, 2017). While distinguishing between aggressive and non-aggressive speech is a already a challenging task, distinguishing between different subtypes of aggressive s"
W18-4422,W16-5618,0,0.0200075,"observed for the case of open vs. covert aggression by Malmasi and Zampieri (2018), and served as primary motivation for this shared task (Kumar et al., 2018). A separate issue, which further increases the difficulty of the task, is that there is no universally agreedupon definition of aggressive speech – a situation which negatively affects the reliability of annotated data. A study by Ross et al. (2016) has shown that supplying hate-speech definitions to annotators can better align their view with the definition, but this does not positively affect annotation reliability. A related study by Waseem (2016) indicated that having expert knowledge during annotation can result in less annotation effort, but this does not necessarily lead to overall better prediction models. Initial studies dealing with the detection of aggressive speech (or its many subtypes) rely on traditional text classification techniques, such as the naive Bayes classifier (Kwok and Wang, 2013; Chen et al., 2012; Dinakar et al., 2011), logistic regression (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Burnap and Williams, 2015), or support vector machines (SVM) (Xu et al., 2012; Dadvar et al., 2013; Schof"
W18-4422,N12-1084,0,0.0317677,"liability. A related study by Waseem (2016) indicated that having expert knowledge during annotation can result in less annotation effort, but this does not necessarily lead to overall better prediction models. Initial studies dealing with the detection of aggressive speech (or its many subtypes) rely on traditional text classification techniques, such as the naive Bayes classifier (Kwok and Wang, 2013; Chen et al., 2012; Dinakar et al., 2011), logistic regression (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Burnap and Williams, 2015), or support vector machines (SVM) (Xu et al., 2012; Dadvar et al., 2013; Schofield and Davidson, 2017). For example, the work of Van Hee et al. (2015) focuses on classifying different subtypes of cyberbullying using an SVM. In a similar vein, Malmasi and Zampieri (2017) built a system to discern between hate speech and mere profanity using a combination of traditional models. Recent work also features dictionary based approaches for detecting abusive language in languages other than English (Tulkens et al., 2016; Mubarak et al., 2017). Other approaches focus on users instead of single texts, such as the work of Ribeiro et al. (2018), where th"
W18-5117,P07-1033,0,0.513505,"Missing"
W18-5117,W17-3013,0,0.0402303,"Missing"
W18-5117,gao-huang-2017-detecting,0,0.142581,"edictions would in effect turn every false positive prediction into infringement of free speech. This would defeat the 2 Related Work A bewildering plethora of different types of abusive language can be found online. Some of the 1 http://takelab.fer.hr/alfeda 132 Proceedings of the Second Workshop on Abusive Language Online (ALW2), pages 132–137 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics types dealt with in related work include but are not limited to sexism, racism (Waseem and Hovy, 2016; Waseem, 2016), toxicity (Kolhatkar et al., 2018), hatefulness (Gao and Huang, 2017), aggression (Kumar et al., 2018), attack (Wulczyn et al., 2017), obscenity, threats, and insults. A typology of abusive language detection subtasks was recently proposed by Waseem et al. (2017). Traditional machine learning approaches to detecting abusive language include the naive Bayes classifier (Kwok and Wang, 2013; Chen et al., 2012; Dinakar et al., 2011), logistic regression (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Burnap and Williams, 2015), and support vector machines (SVM) (Xu et al., 2012; Dadvar et al., 2013; Schofield and Davidson, 2017). The best perfo"
W18-5117,W02-0109,0,0.24132,"anguage). 2 We do this by labeling all classes typeset in bold in Table 1 as positive and all other classes as negative. There are two exceptions to this rule. First, on the Kol dataset, we consider as positive those examples for which at least one annotator gave a rating higher than 1. Second, on the Kaggle dataset, which uses a multilabeling scheme, we consider as positive all instances annotated with at least one of the six harmful labels, and as negative all instances without any labels. We perform only the very basic preprocessing by lowercasing all words and lemmatizing them using NTLK (Loper and Bird, 2002). While these modifications to original datasets make a comparison to previous work difficult, they allow a direct comparison across the datasets and a straightforward application of FEDA. 4 Exp. 1: Cross-Domain Performance The goal of this experiment is to asses how well the models trained on a particular dataset of abusive language perform on a different dataset. The differences in performance can be traced back to two factors: (1) the difference in the types of abusive language that the dataset was labeled with and (2) the differences in dataset sizes. In this work we observe the joint effe"
W18-5117,W16-3638,0,0.0301856,"Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Burnap and Williams, 2015), and support vector machines (SVM) (Xu et al., 2012; Dadvar et al., 2013; Schofield and Davidson, 2017). The best performance is most often attained by deep learning models, the most popular being convolutional neural networks (Gamb¨ack and Sikdar, 2017; Potapova and Gordeev, 2016; Pavlopoulos et al., 2017) and variants of recurrent neural networks (Pavlopoulos et al., 2017; Gao and Huang, 2017; Pitsilis et al., 2018; Zhang et al., 2018). Some approaches (Badjatiya et al., 2017; Park and Fung, 2017; Mehdad and Tetreault, 2016) also rely on combining different types of models. In this paper we explore combining different datasets from different domains to improve model performance. This idea is well established in the machine learning community under the name of transfer learning; we refer to (Weiss et al., 2016; Lu et al., 2015) for overviews. The work closest to ours is (Waseem et al., 2018), where multi-task learning is used to build robust hate-speech detection models. Our approach is very similar, but we consider more datasets and use a simpler, more easily interpretable transfer learning scheme. 3 Datasets For"
W18-5117,W17-3006,0,0.0359672,"logistic regression (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Burnap and Williams, 2015), and support vector machines (SVM) (Xu et al., 2012; Dadvar et al., 2013; Schofield and Davidson, 2017). The best performance is most often attained by deep learning models, the most popular being convolutional neural networks (Gamb¨ack and Sikdar, 2017; Potapova and Gordeev, 2016; Pavlopoulos et al., 2017) and variants of recurrent neural networks (Pavlopoulos et al., 2017; Gao and Huang, 2017; Pitsilis et al., 2018; Zhang et al., 2018). Some approaches (Badjatiya et al., 2017; Park and Fung, 2017; Mehdad and Tetreault, 2016) also rely on combining different types of models. In this paper we explore combining different datasets from different domains to improve model performance. This idea is well established in the machine learning community under the name of transfer learning; we refer to (Weiss et al., 2016; Lu et al., 2015) for overviews. The work closest to ours is (Waseem et al., 2018), where multi-task learning is used to build robust hate-speech detection models. Our approach is very similar, but we consider more datasets and use a simpler, more easily interpretable transfer le"
W18-5117,W17-3004,0,0.0350239,"oposed by Waseem et al. (2017). Traditional machine learning approaches to detecting abusive language include the naive Bayes classifier (Kwok and Wang, 2013; Chen et al., 2012; Dinakar et al., 2011), logistic regression (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Burnap and Williams, 2015), and support vector machines (SVM) (Xu et al., 2012; Dadvar et al., 2013; Schofield and Davidson, 2017). The best performance is most often attained by deep learning models, the most popular being convolutional neural networks (Gamb¨ack and Sikdar, 2017; Potapova and Gordeev, 2016; Pavlopoulos et al., 2017) and variants of recurrent neural networks (Pavlopoulos et al., 2017; Gao and Huang, 2017; Pitsilis et al., 2018; Zhang et al., 2018). Some approaches (Badjatiya et al., 2017; Park and Fung, 2017; Mehdad and Tetreault, 2016) also rely on combining different types of models. In this paper we explore combining different datasets from different domains to improve model performance. This idea is well established in the machine learning community under the name of transfer learning; we refer to (Weiss et al., 2016; Lu et al., 2015) for overviews. The work closest to ours is (Waseem et al., 2018), w"
W18-5117,W16-5618,0,0.0921165,"umans in the loop is crucial, since blindly relying on model predictions would in effect turn every false positive prediction into infringement of free speech. This would defeat the 2 Related Work A bewildering plethora of different types of abusive language can be found online. Some of the 1 http://takelab.fer.hr/alfeda 132 Proceedings of the Second Workshop on Abusive Language Online (ALW2), pages 132–137 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics types dealt with in related work include but are not limited to sexism, racism (Waseem and Hovy, 2016; Waseem, 2016), toxicity (Kolhatkar et al., 2018), hatefulness (Gao and Huang, 2017), aggression (Kumar et al., 2018), attack (Wulczyn et al., 2017), obscenity, threats, and insults. A typology of abusive language detection subtasks was recently proposed by Waseem et al. (2017). Traditional machine learning approaches to detecting abusive language include the naive Bayes classifier (Kwok and Wang, 2013; Chen et al., 2012; Dinakar et al., 2011), logistic regression (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Burnap and Williams, 2015), and support vector machines (SVM) (Xu et al., 20"
W18-5117,W17-3012,0,0.288647,"eneral abusive language generalize between different datasets labeled with different abusive language types. To this end, we compare the cross-domain performance of simple classification models on nine different datasets, finding that the models fail to generalize to out-domain datasets and that having at least some in-domain data is important. We also show that using the frustratingly simple domain adaptation (Daume III, 2007) in most cases improves the results over indomain training, especially when used to augment a smaller dataset with a larger one. 1 Introduction Abusive language online (Waseem et al., 2017) is an increasing problem in modern society. Although abusive language is undoubtedly not a new phenomenon in human communication, the rise of the internet has made it concerningly prevalent. The main reason behind this is the cloak of relative anonymity offered when commenting online, which lowers the inhibitions of individuals prone to abusive language and removes some of the social mechanisms present in real life that serve to protect potential victims. Moreover, this type of psychological violence can occur at any time and regardless of the physical distance between the persons involved. W"
W18-5117,N16-2013,0,0.43096,"t. However, retaining humans in the loop is crucial, since blindly relying on model predictions would in effect turn every false positive prediction into infringement of free speech. This would defeat the 2 Related Work A bewildering plethora of different types of abusive language can be found online. Some of the 1 http://takelab.fer.hr/alfeda 132 Proceedings of the Second Workshop on Abusive Language Online (ALW2), pages 132–137 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics types dealt with in related work include but are not limited to sexism, racism (Waseem and Hovy, 2016; Waseem, 2016), toxicity (Kolhatkar et al., 2018), hatefulness (Gao and Huang, 2017), aggression (Kumar et al., 2018), attack (Wulczyn et al., 2017), obscenity, threats, and insults. A typology of abusive language detection subtasks was recently proposed by Waseem et al. (2017). Traditional machine learning approaches to detecting abusive language include the naive Bayes classifier (Kwok and Wang, 2013; Chen et al., 2012; Dinakar et al., 2011), logistic regression (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Burnap and Williams, 2015), and support vector machines (SVM)"
W18-5117,N12-1084,0,0.059937,"Waseem, 2016), toxicity (Kolhatkar et al., 2018), hatefulness (Gao and Huang, 2017), aggression (Kumar et al., 2018), attack (Wulczyn et al., 2017), obscenity, threats, and insults. A typology of abusive language detection subtasks was recently proposed by Waseem et al. (2017). Traditional machine learning approaches to detecting abusive language include the naive Bayes classifier (Kwok and Wang, 2013; Chen et al., 2012; Dinakar et al., 2011), logistic regression (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017; Burnap and Williams, 2015), and support vector machines (SVM) (Xu et al., 2012; Dadvar et al., 2013; Schofield and Davidson, 2017). The best performance is most often attained by deep learning models, the most popular being convolutional neural networks (Gamb¨ack and Sikdar, 2017; Potapova and Gordeev, 2016; Pavlopoulos et al., 2017) and variants of recurrent neural networks (Pavlopoulos et al., 2017; Gao and Huang, 2017; Pitsilis et al., 2018; Zhang et al., 2018). Some approaches (Badjatiya et al., 2017; Park and Fung, 2017; Mehdad and Tetreault, 2016) also rely on combining different types of models. In this paper we explore combining different datasets from different"
W18-5427,W17-5221,0,0.0632979,"Missing"
W18-5427,P17-1055,0,0.117151,"he final decision. We test the model on two sentiment analysis tasks and demonstrate its capacity to isolate different task-related aspects of the input, while reaching performance comparable with the state of the art. 2 Related Work Attention (Bahdanau et al., 2014) and its variants (Luong et al., 2015) have initially been proposed for machine translation, but are now widely adopted in NLP. Attention has proven especially useful in tasks that involve long text sequences, such as summarization (Rush et al., 2015; See et al., 2017), question answering (Hermann et al., 2015; Xiong et al., 2016; Cui et al., 2017), and natural language inference (Rockt¨aschel et al., 2015; Yin et al., 2016; Parikh et al., 2016), as well as purely attentional machine translation (Vaswani et al., 2017; Gu et al., 2017). Thus far, there has been a number of interesting and effective approaches for interpreting the in249 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 249–257 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics ner workings of recurrent neural networks through methods such as representing them as finite automata"
W18-5427,P16-5005,0,0.0643921,"Missing"
W18-5427,N16-1082,0,0.0181569,"ani et al., 2017; Gu et al., 2017). Thus far, there has been a number of interesting and effective approaches for interpreting the in249 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 249–257 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics ner workings of recurrent neural networks through methods such as representing them as finite automata (Weiss et al., 2017), extracting inference rules (Zanzotto and Ferrone, 2017), and analyzing saliency of inputs through first-order derivative information (Li et al., 2016; Arras et al., 2017). inter-sentence attention. In contrast to their work, we do not predetermine the level on which the attention is applied – in each iteration the mechanism can focus on any element of the input sequence. Akin to the saliency analysis approaches, we opt not to condense the trained network into a finite set of rules. We differ from (Li et al., 2016; Arras et al., 2017) in that we attempt to decode the steps of the decision process of a recurrent network instead of demonstrating through saliency how the decision changes with respect to the inputs. In the context of sentiment"
W18-5427,D15-1166,0,0.306127,"transformation, along with reusing the representations obtained in previous steps, allows the model to construct a recursive representation and process the input sequence bit by bit. The upshot is that we can inspect how the model weighs the different parts of the sentence and recursively combines them to give the final decision. We test the model on two sentiment analysis tasks and demonstrate its capacity to isolate different task-related aspects of the input, while reaching performance comparable with the state of the art. 2 Related Work Attention (Bahdanau et al., 2014) and its variants (Luong et al., 2015) have initially been proposed for machine translation, but are now widely adopted in NLP. Attention has proven especially useful in tasks that involve long text sequences, such as summarization (Rush et al., 2015; See et al., 2017), question answering (Hermann et al., 2015; Xiong et al., 2016; Cui et al., 2017), and natural language inference (Rockt¨aschel et al., 2015; Yin et al., 2016; Parikh et al., 2016), as well as purely attentional machine translation (Vaswani et al., 2017; Gu et al., 2017). Thus far, there has been a number of interesting and effective approaches for interpreting the i"
W18-5427,P11-1015,0,0.0658042,"sets We test IRAM on two sentiment classification datasets. The first is the Stanford Sentiment Treebank (SST) (Socher et al., 2013), a dataset derived from movie reviews on Rotten Tomatoes and containing 11,855 sentences labeled into five classes at the sentence level and at the level of each node in the constituency parse tree. The binary version with the neutral class removed contains 56,400 instances, while the fine-grained version with scores ranging from 1 (very negative) to 5 (very positive) contains 94,200 text-sentiment pairs. The second dataset is the Internet Movie Database (IMDb) (Maas et al., 2011), containing 22,500 multi-sentence reviews extracted from positive and negative reviews. We truncate each sentence from this dataset to a maximum length of 200 tokens. Firstly, we demonstrate and analyze how each component in the vanilla model contributes to the performance and interpretability. We then analyze the full model and evaluate it on the aforementioned datasets. 4.2 Experimental Setup Unless stated otherwise, all weights are initialized from a Gaussian distribution with zero mean and standard deviation of 0.01. We use the Adam optimizer (Kingma and Ba, 2014) with the AmsGrad modific"
W18-5427,E17-1038,0,0.0563081,"Missing"
W18-5427,D16-1244,0,0.0767091,"Missing"
W18-5427,D14-1162,0,0.0809488,"of the input in each iteration. However, the model is in no way incentivized to learn to propagate information through the sum251 3.3 Vanilla Encoder For training, the inputs of the encoding phase are a sequence of words x = [w1 , . . . , wN ] and a class Figure 2: The full version of the iterative recursive attention model (IRAM). Green-colored components share their parameters with components of the same type; blue-colored components each have their own parameters. label y. The encoder of the vanilla model maps the word indices to dense vector representations using pretrained GloVe vectors (Pennington et al., 2014). The sequence of word vectors is then fed as input to a bidirectional long-short term memory (BiLSTM) network (Hochreiter and Schmidhuber, 1997). The outputs of the BiLSTM are used as the input sequence to the iterative attention step, while the cell state in the last timestep is used as the initial query. 3.4 Full Encoder There are three key differences between the full encoder and the vanilla encoder. The full encoder uses (1) character n-gram embeddings, (2) an additional highway network, whose task is to fine-tune the word embeddings, and (3) an additional layer of BiLSTM, followed by a h"
W18-5427,N18-1202,0,0.0672997,"Missing"
W18-5427,D15-1044,0,0.0477735,"ect how the model weighs the different parts of the sentence and recursively combines them to give the final decision. We test the model on two sentiment analysis tasks and demonstrate its capacity to isolate different task-related aspects of the input, while reaching performance comparable with the state of the art. 2 Related Work Attention (Bahdanau et al., 2014) and its variants (Luong et al., 2015) have initially been proposed for machine translation, but are now widely adopted in NLP. Attention has proven especially useful in tasks that involve long text sequences, such as summarization (Rush et al., 2015; See et al., 2017), question answering (Hermann et al., 2015; Xiong et al., 2016; Cui et al., 2017), and natural language inference (Rockt¨aschel et al., 2015; Yin et al., 2016; Parikh et al., 2016), as well as purely attentional machine translation (Vaswani et al., 2017; Gu et al., 2017). Thus far, there has been a number of interesting and effective approaches for interpreting the in249 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 249–257 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics ner"
W18-5427,P17-1099,0,0.0209027,"eighs the different parts of the sentence and recursively combines them to give the final decision. We test the model on two sentiment analysis tasks and demonstrate its capacity to isolate different task-related aspects of the input, while reaching performance comparable with the state of the art. 2 Related Work Attention (Bahdanau et al., 2014) and its variants (Luong et al., 2015) have initially been proposed for machine translation, but are now widely adopted in NLP. Attention has proven especially useful in tasks that involve long text sequences, such as summarization (Rush et al., 2015; See et al., 2017), question answering (Hermann et al., 2015; Xiong et al., 2016; Cui et al., 2017), and natural language inference (Rockt¨aschel et al., 2015; Yin et al., 2016; Parikh et al., 2016), as well as purely attentional machine translation (Vaswani et al., 2017; Gu et al., 2017). Thus far, there has been a number of interesting and effective approaches for interpreting the in249 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 249–257 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics ner workings of recurr"
W18-5427,D13-1170,0,0.00743107,". This way we force the network to propagate information through the attention steps, and also because the intermediate summaries do not contribute directly toward the classification and hence need not have the same polarity. The last summary vector is fed into a maxout network (Goodfellow et al., 2013) to obtain the class-conditional probabilities. Fig. 2 shows the full version of the iterative at252 tention mechanism with all of the aforementioned components. 4 4.1 Experiments Datasets We test IRAM on two sentiment classification datasets. The first is the Stanford Sentiment Treebank (SST) (Socher et al., 2013), a dataset derived from movie reviews on Rotten Tomatoes and containing 11,855 sentences labeled into five classes at the sentence level and at the level of each node in the constituency parse tree. The binary version with the neutral class removed contains 56,400 instances, while the fine-grained version with scores ranging from 1 (very negative) to 5 (very positive) contains 94,200 text-sentiment pairs. The second dataset is the Internet Movie Database (IMDb) (Maas et al., 2011), containing 22,500 multi-sentence reviews extracted from positive and negative reviews. We truncate each sentence"
W18-5427,N16-1174,0,0.052093,"l to construct its own sequential representation of the input. Our model only connects the last attention step to the output, removing the need for intermediate steps to contain all the information relevant for the final decision. Apart from (Sordoni et al., 2016), related work closest to ours consists of concepts of multi-head attention (Lin et al., 2017; Vaswani et al., 2017), in which all queries are generated at once, pairwise attention (Cui et al., 2017; Xiong et al., 2016), where attention is applied to multiple inputs but is not applied iteratively and hierarchical iterative attention (Yang et al., 2016), where the authors first use a intra-sentence attention mechanism and then combine the intermediate representations with 3 3.1 Model Iterative Recursive Attention Fig. 1 shows the architecture of the iterative attention mechanism. The mechanism uses a recurrent network, dubbed the controller, to refine the attention query throughout T iterations. Inputs to the mechanism are an initial query x ˆ and a set of hidden states H = [hi , . . . , hN ] constituting the input sequence, both obtained from the encoding step. As the controller, we use a gated recurrent unit (GRU) (Cho et al., 2014) cell."
W18-5427,Q16-1019,0,0.0752397,"Missing"
W18-5427,W14-4012,0,\N,Missing
W18-5427,P16-1086,0,\N,Missing
W18-6211,W16-0307,0,0.276742,"Missing"
W18-6211,P14-2030,0,0.0660962,"Missing"
W18-6211,W17-5217,0,0.0277708,"Missing"
W18-6211,W14-3207,0,0.528235,"es with the lowest p-value on the entire dataset are presented in Table 5, together with feature value means for the two groups. The values in the table are percentages of words in text from each category, except Authentic and Clout, which are “summary variables” devised by Pennebaker et al. (2015). Personal pronouns, especially the pronoun I, are used more often by bipolar disorder users. This observation is in accord with past studies on language of depressed people, which we can compare to because a bipolar depressive episode is almost identical to major depression (Anderson et al., 2012). Coppersmith et al. (2014) also report a significant difference in the use of I between Twitter users with bipolar disorder and the control group. The Authentic feature of Newman et al. (2003) reflects the authenticity of the author’s text: a higher value of this feature in bipolar disorder users may perhaps be explained by them speaking about personal issues more sincerely, though further research would be required to confirm this. We also observe a higher use of words associated with feelings (feel), health, and biological processes (bio). Kacewicz et al. (2014) argue that pronoun use reflects standings in social hie"
W18-6211,W15-1201,0,0.606777,"Missing"
W18-6211,W15-1204,0,0.0496209,"77* 0.752* 0.817* 0.882* 0.784* 0.801* 0.837 Table 4: Accuracy of the MCC baseline and our models across topic categories. Accuracies marked with “*” are significantly different from the baseline. Table 3: Prediction accuracy for the different models and feature sets Between-group analysis. Ten LIWC features with the lowest p-value on the entire dataset are presented in Table 5, together with feature value means for the two groups. The values in the table are percentages of words in text from each category, except Authentic and Clout, which are “summary variables” devised by Pennebaker et al. (2015). Personal pronouns, especially the pronoun I, are used more often by bipolar disorder users. This observation is in accord with past studies on language of depressed people, which we can compare to because a bipolar depressive episode is almost identical to major depression (Anderson et al., 2012). Coppersmith et al. (2014) also report a significant difference in the use of I between Twitter users with bipolar disorder and the control group. The Authentic feature of Newman et al. (2003) reflects the authenticity of the author’s text: a higher value of this feature in bipolar disorder users ma"
W18-6211,W18-1112,1,0.755347,"Missing"
W18-6211,W17-3107,0,0.297705,"Missing"
W19-3514,W17-3013,0,0.0286413,"Missing"
W19-3514,gao-huang-2017-detecting,0,0.0372355,"Missing"
W19-3514,W18-1112,1,0.825222,"Missing"
W19-3514,D18-1305,0,0.252737,"es. Furthermore, this dataset was constructed using a very carefully designed methodology for a specific experiment – detecting whether a toxic comment will appear given a courteous initial exchange of two comments. We are interested in a more general case, where conversation threads might be longer and not necessarily start in a courteous manner. Moreover, we aimed at a setting which would better reflect the realistic working conditions in which our models would be used and allow us to measure their practical impact. Consequently, we decided to create a new dataset from the data collected by Hua et al. (2018). It contains the entire conversational history of comments on Wikipedia modeled as a graph of actions. The possible actions are Creation, Addition, Modification, Deletion, and Restoration. Automatically derived toxicity scores are also provided for each example. We apply the following steps to this dataset: Step 1. Filter the data to remove all threads with less than 2 different participants. This leaves ∼8.7M threads. Step 2. Apply all Modification actions, to update the comments to their most recent version. Step 3. Flag comments that were deleted. A comment is considered deleted if there i"
W19-3514,P15-1150,0,0.101747,"Missing"
W19-3514,W16-5618,0,0.0614149,"Missing"
W19-3514,W17-3004,0,0.109364,"Missing"
W19-3514,D14-1162,0,0.0835958,"Missing"
W19-3514,W17-3012,0,0.0248428,"Missing"
W19-3514,N16-2013,0,0.0641145,"Missing"
W19-3514,N12-1084,0,0.0766835,"Missing"
W19-3514,P18-1125,0,0.164525,"2017) and the code is available online.4 5 Results The results are given in Table 4. Each column represents one dataset variant. All differences within the same variant are statistically significant at p&lt;0.05 (tested used bootstrap resampling). While differences across different dataset variants are not directly comparable, there is a tendency for models to perform much better in the post-hoc scenario than in the preemptive scenario, which is expected. Preemptive models are, however, able manually labeled to beat the random baseline and achieve scores that are numerically similar to those of Zhang et al. (2018a) on their data. The BiLSTM-context model performs similarly or worse than the BiLSTM model in all cases except the preemptive real case where context does help, but both LSTM-based models are outperformed by a simple SVM. This indicates that the additional information provided by the thread context is, in this case, not very useful for determining the correct class. Manual inspection of the data set confirms that humans could determine the toxicity of most comments without referring to the thread for additional context.This intuition is invalid in 4 For implementing the encoder, we performed"
W19-4405,W15-0602,0,0.0314594,"a, 2010) which reflects students’ ability to connect ideas in their mind and to convey the same message in essays or summaries (Halliday and Hasan, 2014). Existing approaches to text quality predominantly focus on surface measures for assessment (e.g., number of cohesive devices), which sometimes have little relation either to human judgment, e.g., text length (Mintz et al., 2014), or to textspecific meaning (Rahimi et al., 2017). However, automatic scoring of coherence should ideally provide clear and reliable feedback (Burstein et al., 2013) based on features with cognitive validity, e.g., (Loukina et al., 2015). One way to meet such requirements is to define coherence as the identification of relations between the text’s ideas (Rapp et al., 2007). Such a definition may best be analysed in summaries in which the key ideas of the source text are integrated into a rhetorical structure (RS). In cognitive terms, writing summaries is an exercise in reading-for-understanding (RU) (Sabatini 2 Related Work Automatic assessment of text quality can include content, language accuracy, sophistication and style 46 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Application"
W19-4405,W13-1722,0,0.0735222,"Missing"
W19-4405,W18-0531,0,0.0207408,"ng-for-understanding (RU) (Sabatini 2 Related Work Automatic assessment of text quality can include content, language accuracy, sophistication and style 46 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 46–51 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics as well as sometimes overlapping features such as topic similarity, focus, coherence, cohesion, readability, or text organisation and development, e.g., (Pitler et al., 2010; Yannakoudakis and Briscoe, 2012; Guo et al., 2013; Rahimi et al., 2015; Gao et al., 2018). Coherence is a broad concept assessed by different automatic tools, e.g., (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012; Burstein et al., 2013). Scoring measures may include surface features such as word or text length or the number of pronouns and connectives, e.g., (Yannakoudakis and Briscoe, 2012; MacArthur et al., 2018), which may also be contextualised, e.g., (Pitler et al., 2010). Source overlaps may also be used in scoring such as overlapping n-grams in summaries (Madnani et al., 2013), and semantic similarity (e.g,. LSA) may provide information on relatedness between words,"
W19-4405,O09-4003,0,0.0400829,"98; Higgins et al., 2004; Higgins and Burstein, 2007), or larger text sections (Crossley and McNamara, 2010). Both types of features (surface and LSA) are encompassed by CohMetrix (Graesser et al., 2004; McNamara et al., 2014), a comprehensive computational tool using a range of measures to grasp cognitive aspects of text analysis. Moreover, inter-sentential coherence can be measured using syntax-based entity grids (Barzilay and Lapata, 2008), for example, to distinguish between high- and low-coherence essays (Burstein et al., 2010), or analysing discourse relations (Pitler and Nenkova, 2008; Skoufaki, 2009). relies on summaries as a RU/RW task which consists of detecting and conveying the RS of the source text. Similar to Higgins et al. (2004), we use semantic similarity and rhetorical structure to assess coherence of student summaries against summaries written by experts. While Higgins et al. measured the coherence of functional discourse segments (e.g., thesis, conclusion) via semantic similarity between their respective sentences, in our study coherence is measured via similarity between rhethorical structures. Our intuition relies on the establishment of source macrostructure as a coherence-"
W19-4405,I17-2031,1,0.843456,"sful in modeling both cohesion and coherence – and this in spite of the unavoidable errors of the PDTB parser and errors in similarity computations. m − max(0, n1 − n2 ) n1 m − max(0, n2 − n1 ) R= n2 P = The intuition is that precision is maximized if all relations from S1 are perfectly matched to some relations from S2 , and conversely for recall. The F1-score is the harmonic mean of P and R. Finally, we compute the F1-score of a student’s summary S as the mean of pairwise F1-scores between S and both reference summaries. 4 Evaluation Dataset. For model evaluation, we adopt the ˇ dataset of (Sladoljev-Agejev and Snajder, 2017). The dataset consists of a total of 225 text-present summaries (c. 300 words) of two articles written by 114 first-year business undergraduates in English as L2 (mostly upper intermediate and advanced). Both articles (c. 900 words each) were taken from The Economist, a business magazine. Two expert 1 Comparison with Coh-Metrix indices. As mentioned in the introduction, a number of studies have used Coh-Metrix cohesion indices as predictors of both cohesion and coherence. In particˇ ular, Sladoljev-Agejev and Snajder (2017) found https://code.google.com/archive/p/word2vec/ 48 Class Level Chs C"
W19-4405,P10-1056,0,0.031716,"to a rhetorical structure (RS). In cognitive terms, writing summaries is an exercise in reading-for-understanding (RU) (Sabatini 2 Related Work Automatic assessment of text quality can include content, language accuracy, sophistication and style 46 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 46–51 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics as well as sometimes overlapping features such as topic similarity, focus, coherence, cohesion, readability, or text organisation and development, e.g., (Pitler et al., 2010; Yannakoudakis and Briscoe, 2012; Guo et al., 2013; Rahimi et al., 2015; Gao et al., 2018). Coherence is a broad concept assessed by different automatic tools, e.g., (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012; Burstein et al., 2013). Scoring measures may include surface features such as word or text length or the number of pronouns and connectives, e.g., (Yannakoudakis and Briscoe, 2012; MacArthur et al., 2018), which may also be contextualised, e.g., (Pitler et al., 2010). Source overlaps may also be used in scoring such as overlapping n-grams in summaries (Madnani et al., 2013),"
W19-4405,C14-1090,0,0.0204173,"oncept assessed by different automatic tools, e.g., (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012; Burstein et al., 2013). Scoring measures may include surface features such as word or text length or the number of pronouns and connectives, e.g., (Yannakoudakis and Briscoe, 2012; MacArthur et al., 2018), which may also be contextualised, e.g., (Pitler et al., 2010). Source overlaps may also be used in scoring such as overlapping n-grams in summaries (Madnani et al., 2013), and semantic similarity (e.g,. LSA) may provide information on relatedness between words, e.g., lexical chaining (Somasundaran et al., 2014), sentences (Foltz et al., 1998; Higgins et al., 2004; Higgins and Burstein, 2007), or larger text sections (Crossley and McNamara, 2010). Both types of features (surface and LSA) are encompassed by CohMetrix (Graesser et al., 2004; McNamara et al., 2014), a comprehensive computational tool using a range of measures to grasp cognitive aspects of text analysis. Moreover, inter-sentential coherence can be measured using syntax-based entity grids (Barzilay and Lapata, 2008), for example, to distinguish between high- and low-coherence essays (Burstein et al., 2010), or analysing discourse relation"
W19-4405,D08-1020,0,0.0599338,"entences (Foltz et al., 1998; Higgins et al., 2004; Higgins and Burstein, 2007), or larger text sections (Crossley and McNamara, 2010). Both types of features (surface and LSA) are encompassed by CohMetrix (Graesser et al., 2004; McNamara et al., 2014), a comprehensive computational tool using a range of measures to grasp cognitive aspects of text analysis. Moreover, inter-sentential coherence can be measured using syntax-based entity grids (Barzilay and Lapata, 2008), for example, to distinguish between high- and low-coherence essays (Burstein et al., 2010), or analysing discourse relations (Pitler and Nenkova, 2008; Skoufaki, 2009). relies on summaries as a RU/RW task which consists of detecting and conveying the RS of the source text. Similar to Higgins et al. (2004), we use semantic similarity and rhetorical structure to assess coherence of student summaries against summaries written by experts. While Higgins et al. measured the coherence of functional discourse segments (e.g., thesis, conclusion) via semantic similarity between their respective sentences, in our study coherence is measured via similarity between rhethorical structures. Our intuition relies on the establishment of source macrostructur"
W19-4405,W12-2004,0,0.0291444,"ture (RS). In cognitive terms, writing summaries is an exercise in reading-for-understanding (RU) (Sabatini 2 Related Work Automatic assessment of text quality can include content, language accuracy, sophistication and style 46 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 46–51 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics as well as sometimes overlapping features such as topic similarity, focus, coherence, cohesion, readability, or text organisation and development, e.g., (Pitler et al., 2010; Yannakoudakis and Briscoe, 2012; Guo et al., 2013; Rahimi et al., 2015; Gao et al., 2018). Coherence is a broad concept assessed by different automatic tools, e.g., (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012; Burstein et al., 2013). Scoring measures may include surface features such as word or text length or the number of pronouns and connectives, e.g., (Yannakoudakis and Briscoe, 2012; MacArthur et al., 2018), which may also be contextualised, e.g., (Pitler et al., 2010). Source overlaps may also be used in scoring such as overlapping n-grams in summaries (Madnani et al., 2013), and semantic similarity (e.g,. L"
W19-4405,prasad-etal-2008-penn,0,0.200083,"Missing"
W19-4405,W15-0603,0,0.0231785,"an exercise in reading-for-understanding (RU) (Sabatini 2 Related Work Automatic assessment of text quality can include content, language accuracy, sophistication and style 46 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 46–51 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics as well as sometimes overlapping features such as topic similarity, focus, coherence, cohesion, readability, or text organisation and development, e.g., (Pitler et al., 2010; Yannakoudakis and Briscoe, 2012; Guo et al., 2013; Rahimi et al., 2015; Gao et al., 2018). Coherence is a broad concept assessed by different automatic tools, e.g., (Higgins et al., 2004; Yannakoudakis and Briscoe, 2012; Burstein et al., 2013). Scoring measures may include surface features such as word or text length or the number of pronouns and connectives, e.g., (Yannakoudakis and Briscoe, 2012; MacArthur et al., 2018), which may also be contextualised, e.g., (Pitler et al., 2010). Source overlaps may also be used in scoring such as overlapping n-grams in summaries (Madnani et al., 2013), and semantic similarity (e.g,. LSA) may provide information on relatedn"
W19-5118,C14-1029,0,0.185396,"Missing"
W19-5118,U16-1011,0,0.41029,"Missing"
W19-5118,warburton-2014-narrowing,0,0.0206078,"aim of Automatic Term Extraction (or Recognition) (ATE) is to extract terms – single words or multiword expressions (MWEs) representing domain-specific concepts – from a domainspecific corpus. ATE is widely used in many NLP tasks, such as information retrieval and machine translation. Moreover, Computer Assisted Translation (CAT) tools often use ATE methods to aid translators in finding and extracting translation equivalent terms in the target language (Costa et al., 2016; Oliver, 2017). While corpus-based approaches to terminology extraction are the norm when building large-scale termbases (Warburton, 2014), a survey we conducted1 showed that translators are most often interested in ATE from individual documents of various lengths, rather than entire corpora, since they typically translate on document at a time. 1 Survey 2LwrTkv. results available at 2 Related Work Most ATE methods begin with the extraction and filtering of candidate terms, followed by candidate term scoring and ranking. Because of divergent candidate extraction and filtering step implementations, many existing ATE evaluations are not directly comparable. Zhang et al. (2008) were among the first to compare several scoring and ra"
W19-5118,Y10-1036,0,0.0366375,"rom this group use other ATE methods as features, and attempt to learn the importance of each feature in an unsupervised or supervised setting. Glossary Extraction (Park et al., 2002) extends Weirdness, while Term Extraction (Sclano and Velardi, 2007) further extends Glossary Extraction. SemRe-Rank (Zhang et al., 2018) is a generic approach that incorporates semantic relatedness to re-rank terms. Both da Silva Conrado et al. (2013) and Yuan et al. (2017) use a variety of features in a supervised binary term classifier. A weakly supervised bootstrapping approach called fault tolerant learning (Yang et al., 2010) has been extended for deep learning (Wang et al., 2016). The following methods are the only ones from this group available in ATR4S and therefore the only ones evaluated: PostRankDC (Buitelaar et al., 2013) combines DomainCoherence with Basic, while both PU-ATR (supervised) (Astrakhantsev, 2014) and Voting (unsupervised) (Zhang et al., 2008) use the same five features as implemented in ATR4S. In our study, we distinguish between the original Voting5 and its variant, Voting3 , in which the two Wikipedia-based features are removed to gauge their impact. Term Extraction Methods ATE methods may b"
W19-5118,W14-4807,0,0.0314211,"anking methods, using the same candidate extraction and filtering step and the UAP metric on a custom Wikipedia corpus and GENIA (Kim et al., 2003) corpus. In a followup work, they developed JATE 2.0 (Zhang et al., 2016), with 10 ATE methods available out-of-the-box, that were evaluated http://bit.ly/ 149 Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019), pages 149–154 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics such as DomainCoherence (Buitelaar et al., 2013) and NC-Value (Frantzi et al., 2000). on GENIA and ACL RD-TEC (Zadeh and Handschuh, 2014) using the “precision at K” metric. A similar toolkit, ATR4S (Astrakhantsev, 2018), which implements 15 ATE methods, was evaluated on even more datasets using “average precision at K”. All abovementioned studies were carried out corpus-level, and rely on exact matching between extracted terms and a subset of gold terms. The latter makes such evaluations unrealistic because it disregards the contribution of the candidate extraction and filtering step. The subset is selected by considering only the gold terms that appear in the output above the cut off of at level K, which is used to discriminat"
W19-5118,L16-1359,0,0.0610136,"Knowledge Engineering Lab, Zagreb, Croatia {antonio.sajatovic,maja.buljan,jan.snajder,bojana.dalbelo}@fer.hr Abstract A task related to ATE is Automatic Keyword and Keyphrase Extraction (AKE), which deals with the extraction of single words and MWEs from a single document. Unlike ATE, which aims to capture domain-specific terminology, keywords and keyphrases extracted by AKE should capture the main topics of a document. Consequently, there will only be a handful of representative keyphrases for a document (Turney, 2000). In spite of these differences, several AKE methods were adapted for ATE (Zhang et al., 2016). While corpus-level ATE methods, as well as AKE methods, have been extensively evaluated in the literature, it is not obvious how the results transfer to document-level ATE, which is how ATE is typically used for CAT. In this paper, we aim to close this gap and present an evaluation study that considers both corpus- and documentlevel ATE. We evaluate 16 state-of-the-art ATE methods, including modified AKE methods. Furthermore, addressing another deficiency in existing evaluations, we evaluate the methods using a complete set of gold terms, making the evaluation more realistic. Automatic Term"
W19-5118,C02-1142,0,0.135775,"n can also be used instead of term frequency information, as in NovelTM (Li et al., 2013). Wikipedia. Several methods use Wikipedia instead of term frequency to distinguish between candidate and actual terms, such as LinkProbability (Astrakhantsev, 2014) and KeyConceptRelatedness (Astrakhantsev, 2014). In addition to Wikipedia, KeyConceptRelatedness also relies on keyphrase extraction and semantic relatedness. Re-ranking. Methods from this group use other ATE methods as features, and attempt to learn the importance of each feature in an unsupervised or supervised setting. Glossary Extraction (Park et al., 2002) extends Weirdness, while Term Extraction (Sclano and Velardi, 2007) further extends Glossary Extraction. SemRe-Rank (Zhang et al., 2018) is a generic approach that incorporates semantic relatedness to re-rank terms. Both da Silva Conrado et al. (2013) and Yuan et al. (2017) use a variety of features in a supervised binary term classifier. A weakly supervised bootstrapping approach called fault tolerant learning (Yang et al., 2010) has been extended for deep learning (Wang et al., 2016). The following methods are the only ones from this group available in ATR4S and therefore the only ones eval"
W19-5118,zhang-etal-2008-comparative,0,0.901587,"raction are the norm when building large-scale termbases (Warburton, 2014), a survey we conducted1 showed that translators are most often interested in ATE from individual documents of various lengths, rather than entire corpora, since they typically translate on document at a time. 1 Survey 2LwrTkv. results available at 2 Related Work Most ATE methods begin with the extraction and filtering of candidate terms, followed by candidate term scoring and ranking. Because of divergent candidate extraction and filtering step implementations, many existing ATE evaluations are not directly comparable. Zhang et al. (2008) were among the first to compare several scoring and ranking methods, using the same candidate extraction and filtering step and the UAP metric on a custom Wikipedia corpus and GENIA (Kim et al., 2003) corpus. In a followup work, they developed JATE 2.0 (Zhang et al., 2016), with 10 ATE methods available out-of-the-box, that were evaluated http://bit.ly/ 149 Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019), pages 149–154 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics such as DomainCoherence (Buitelaar et al., 2013) and NC-V"
