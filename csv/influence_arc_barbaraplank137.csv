2020.coling-main.583,W19-6143,1,0.751989,"Missing"
2020.coling-main.583,2020.coling-main.603,1,0.797448,"Missing"
2020.coling-main.583,2020.emnlp-main.431,1,0.80454,"Missing"
2020.coling-main.583,P19-1510,0,0.127027,"Missing"
2020.coling-main.603,P18-1099,0,0.0979845,"d to construct the shared space. SCL uses auxiliary functions inspired by Ando and Zhang (2005), while SFA uses a graph-based spectral learning method. Creating domain-specific and domain-general features is the key idea of EasyAdapt (Daum´e III, 2007), a seminal 6841 Model-centric: (Ziser and Reichart 2017; 2018a; 2018b; 2019) (Miller, 2019) (Glorot et al., 2011) (Chen et al., 2012) (Yang and Eisenstein, 2014) (Clinchant et al., 2016) (Ganin et al., 2016) (Li et al., 2017) (Kim et al., 2017) (Sato et al., 2017) (Wu et al., 2017) (Yasunaga et al., 2018) (Shen et al., 2018) (Li et al., 2018a) (Alam et al., 2018a) (Wang et al., 2019) (Shah et al., 2018) (Fu et al., 2017) (Rios et al., 2018) (Xu et al., 2019) (Shi et al., 2018) (Rocha and Lopes Cardoso, 2019)* (Ghosal et al., 2020) (Naik and Rose, 2020) Data-centric: (Ruder and Plank, 2018) (Lim et al., 2020) (Rotman and Reichart, 2019) (Han and Eisenstein, 2019) (Li et al., 2019) (Gururangan et al., 2020) Hybrid: (Saito et al., 2017) (Desai et al., 2019) (Jia et al., 2019) (Cui and Bollegala, 2019) (Peng and Dredze, 2017) (Guo et al., 2020) (Ben-David et al., 2020) Neural SCL Neural SCL (Joint AE-SCL) SDA MSDA MSDA MSDA DANN DANN+SCL MemNet DANN/DSN"
2020.coling-main.603,2020.acl-main.658,0,0.0273426,"what variety comprises, how covert and overt factors impact results, and take them into consideration in modeling and evaluation. Related problems Following the idea of the variety space, we discuss three related notions: crosslingual learning, domain generalization/robustness, and out-of-distribution generalization. In cross-lingual learning the feature space drastically changes, as alphabets, vocabularies and word order can be different. It can be seen as extreme adaptation scenario, for which parallel data may exist and can be used to build multilingual representations (Ruder et al., 2019; Artetxe et al., 2020). Second, instead of adapting to a particular target, there is some work on domain generalization aimed at building a single system which is robust on several known target domains. One example is the SANCL shared task (Petrov and McDonald, 2012), where participants were asked to build a single system that can robustly parse reviews, weblogs, answers, emails, newsgroups. In this setup, the DA problem boils down to finding a more robust system for given targets. It can be seen as optimizing for both in-domain and out-of-domain(s) accuracy. If domains are unknown a priori, robustness can be taken"
2020.coling-main.603,D11-1033,0,0.256359,"-of-distribution generalization. A categorization of domain adaptation in NLP We categorize research into model-centric, datacentric and hybrid approaches, as shown in Figure 1. Model-centric methods target approaches to augment the feature space, alter the loss function, the architecture or model parameters (Blitzer et al., 2006; Pan et al., 2010; Ganin et al., 2016). Data-centric methods focus on the data aspect and either involve pseudo-labeling (or bootstrapping) to bridge the domain gap (Abney, 2007; Zhu and Goldberg, 2009; Ruder and Plank, 2018; Cui and Bollegala, 2019), data selection (Axelrod et al., 2011; Plank and van Noord, 2011; Ruder and Plank, 2017) and pre-training methods (Han and Eisenstein, 2019; Guo et al., 2020). As some approaches take elements of both, we include a hybrid category.2 A comprehensive overview of UDA methods and the tasks each method is applied to is provided in Table 1. Other surveys Comprehensive reviews on DA exist, each with a different focus: visual applications (Csurka, 2017; Patel et al., 2015; Wilson and Cook, 2020), machine translation (MT) (Chu and Wang, 2018), pre-neural DA methods in NLP (Jiang, 2008; Margolis, 2011). Seminal surveys in machine learning"
2020.coling-main.603,I13-1041,0,0.275939,"t years, leading to relevant research lines. First, the Penn Treebank WSJ corpus (Marcus et al., 1993) and the Brown corpus (Francis and Kucera, 1979) are prototypical examples, with the WSJ being considered widely as the canonical newswire domain. In the recent decade, there has been considerable work on what is considered non-canonical data. The dichotomy between canonical (typically considered well-edited English newswire) and noncanonical data arose with the increasing interest of working with social media with all its challenges related to the ‘noisiness’ of the domain (Eisenstein, 2013; Baldwin et al., 2013). Models trained on canonical data failed in light of the challenges on, e.g., Twitter (Gimpel et al., 2011; Foster et al., 2011). The general quest to understand the implications of variations of language on model performance led to lines of work on how human factors impact data in a covert or overt way, e.g., on how latent socio-demographic factors impact NLP performance (Hovy, 2015; Nguyen et al., 2016), or how direct data collection strategies like crowdsourcing impact corpus composition (Geva et al., 2019) or frequency effects impact NLP performance (Zhang et al., 2019). However, what is"
2020.coling-main.603,Q19-1004,0,0.023395,"(2020); this allows studying diachronic effects, as labeled evaluation data lacks diversity in terms of topics and time (Desai et al., 2019; Derczynski et al., 2016); and c) to release unaggregated, multiple annotations to study divergences in annotations (Plank et al., 2014a). Back to the roots and how knowledge transfers Revisiting classics in neural times is beneficial, as shown for example in recent work which brings back SCL and pseudo-labeling methods (see Table 1), but much is left to see how these methods generalize. This can be linked to the question on what representations capture (Belinkov and Glass, 2019) and how knowledge transfers (Rethmeier et al., 2020). X scarcity Even unlabeled data can be scarce (X scarcity), particularly in highly-specialized language varieties (e.g., clinical data) (Rethmeier and Plank, 2019). This is often due to data sharing restrictions. In some cases, only a trained source model could be available instead of raw or labeled texts (Laparra et al., 2020). Together with the quest for more efficient learning methods, the general question of how to adapt in light of X scarcity or absence becomes important. 8 Conclusion In this survey, we review strands of unsupervised d"
2020.coling-main.603,D19-1371,0,0.0983751,"taBERT, DAPT, TAPT). They differ by the source of unlabeled data: broad-domain  domain-specific  task-specific; (b) Auxiliary-task pre-training: pre-training, followed by (possibly multiple stages of) auxiliarytask pre-training (e.g., supplementary training on intermediate labeled-data tasks, STILTs). Pre-training (option 1) can be seen as straightforward adaptation, analogous to zero-shot in crosslingual learning. The key idea is to train encoders with self-supervised objectives like (masked) language 6845 model and related unsupervised objectives (Peters et al., 2018; Devlin et al., 2019; Beltagy et al., 2019). In light of a domain shift, adaptive pre-training is beneficial, in which in one instantiation contextualized embeddings are adapted to text from the target domain by masked language modeling, as introduced by Han and Eisenstein (2019). More broadly, we distinguish two variants of adaptive pre-training. They differ whether unlabeled data or some form of auxiliary labeled data (or intermediate tasks data) is used. These variants can also be combined, and fine-tuning applies to all setups, if data is available. The key idea of multi-phase pre-training (option 2a) is to use secondary-stage unsu"
2020.coling-main.603,P19-1236,0,0.176769,"nchant et al., 2016) (Ganin et al., 2016) (Li et al., 2017) (Kim et al., 2017) (Sato et al., 2017) (Wu et al., 2017) (Yasunaga et al., 2018) (Shen et al., 2018) (Li et al., 2018a) (Alam et al., 2018a) (Wang et al., 2019) (Shah et al., 2018) (Fu et al., 2017) (Rios et al., 2018) (Xu et al., 2019) (Shi et al., 2018) (Rocha and Lopes Cardoso, 2019)* (Ghosal et al., 2020) (Naik and Rose, 2020) Data-centric: (Ruder and Plank, 2018) (Lim et al., 2020) (Rotman and Reichart, 2019) (Han and Eisenstein, 2019) (Li et al., 2019) (Gururangan et al., 2020) Hybrid: (Saito et al., 2017) (Desai et al., 2019) (Jia et al., 2019) (Cui and Bollegala, 2019) (Peng and Dredze, 2017) (Guo et al., 2020) (Ben-David et al., 2020) Neural SCL Neural SCL (Joint AE-SCL) SDA MSDA MSDA MSDA DANN DANN+SCL MemNet DANN/DSN DANN DANN DANN DANN DANN DANN DANN DANN+Wasserstein DANN DANN DANN DSN (GSN) DANN, Shared encoders DANN (concept embeddings) DANN (context embeddings) 3 3 3 3 SSL, Multitask tri-training SSL Deep self-training AdaptaBERT Adaptive pre-training Adaptive pre-training (incl. multi-phase) 3 Asymmetric tri-training Adaptive (temporal) ensembling Cross-domain LM SelfAdapt (pivots+co-training) Multi-task-DA DistanceNet-Ba"
2020.coling-main.603,P07-1034,0,0.222479,"reconstruction component that leverages both shared and private feature representations in the learning process. As noted also by Han and Eisenstein (2019), a downside of adversarial methods is that they require careful balancing between objectives (Kim et al., 2017; Alam et al., 2018a) to avoid instability during learning (Arjovsky et al., 2017). Reweighting This family of methods is an instance-level adaptation method. The core idea of instance weighting (also known as importance weighting) is to assign a weight to each training instance proportional to its similarity to the target domain (Jiang and Zhai, 2007). We can see instance weighting as an alternative to domain adversaries. While domain adversaries distinguish the domains to learn domain invariant representations in a joint model, instance weighting decouples domain detection for a-priori weight estimation of an instance. Methods that explicitly reweight the loss based on domain discrepancy information include maximum mean discrepancy (MMD) (Gretton et al., 2007) and its more efficient version called kernel mean matching (KMM) (Gretton et al., 2009). KMM reweights the training instances such that the means of the training and test points in"
2020.coling-main.603,P17-1119,0,0.26903,"common across domains by using unlabeled data from both domains. The two approaches differ in the specifics of the method to construct the shared space. SCL uses auxiliary functions inspired by Ando and Zhang (2005), while SFA uses a graph-based spectral learning method. Creating domain-specific and domain-general features is the key idea of EasyAdapt (Daum´e III, 2007), a seminal 6841 Model-centric: (Ziser and Reichart 2017; 2018a; 2018b; 2019) (Miller, 2019) (Glorot et al., 2011) (Chen et al., 2012) (Yang and Eisenstein, 2014) (Clinchant et al., 2016) (Ganin et al., 2016) (Li et al., 2017) (Kim et al., 2017) (Sato et al., 2017) (Wu et al., 2017) (Yasunaga et al., 2018) (Shen et al., 2018) (Li et al., 2018a) (Alam et al., 2018a) (Wang et al., 2019) (Shah et al., 2018) (Fu et al., 2017) (Rios et al., 2018) (Xu et al., 2019) (Shi et al., 2018) (Rocha and Lopes Cardoso, 2019)* (Ghosal et al., 2020) (Naik and Rose, 2020) Data-centric: (Ruder and Plank, 2018) (Lim et al., 2020) (Rotman and Reichart, 2019) (Han and Eisenstein, 2019) (Li et al., 2019) (Gururangan et al., 2020) Hybrid: (Saito et al., 2017) (Desai et al., 2019) (Jia et al., 2019) (Cui and Bollegala, 2019) (Peng and Dredze, 2017) (Guo et al"
2020.coling-main.603,N18-2076,0,0.434546,"of the test distribution. To do so, it assumes a subpopulation shift, where the test population is a subpopulation mix of the training distribution. A model is then trained to do well over a wide range of potential test distributions. Some early work in dialogue (Bod, 1999) and parsing (Plank and Sima’an, 2008) adopted a similar idea of subdomains, however, with manually identified subpopulations. This bears some similarity to early work on leveraging general background knowledge (embeddings trained on general data) for domain adaptation (Plank and Moschitti, 2013; Nguyen and Grishman, 2015; Li et al., 2018a), and also relates to recent work on pre-training (Section 5.3). An alternative and complementary interesting line of research is to predict test set performance for new data varieties (Ravi et al., 2008; Van Asch and Daelemans, 2010; Elsahar and Gall´e, 2019; Xia et al., 2020). 4 Model-centric approaches Model-centric approaches redesign parts of the model: the feature space, the loss function or regularization and the structure of the model. We categorize them into feature-centric and loss-centric methods. 4.1 Feature-centric methods Two lines of work can be found within feature-centric me"
2020.coling-main.603,P19-1229,0,0.101487,"019) (Miller, 2019) (Glorot et al., 2011) (Chen et al., 2012) (Yang and Eisenstein, 2014) (Clinchant et al., 2016) (Ganin et al., 2016) (Li et al., 2017) (Kim et al., 2017) (Sato et al., 2017) (Wu et al., 2017) (Yasunaga et al., 2018) (Shen et al., 2018) (Li et al., 2018a) (Alam et al., 2018a) (Wang et al., 2019) (Shah et al., 2018) (Fu et al., 2017) (Rios et al., 2018) (Xu et al., 2019) (Shi et al., 2018) (Rocha and Lopes Cardoso, 2019)* (Ghosal et al., 2020) (Naik and Rose, 2020) Data-centric: (Ruder and Plank, 2018) (Lim et al., 2020) (Rotman and Reichart, 2019) (Han and Eisenstein, 2019) (Li et al., 2019) (Gururangan et al., 2020) Hybrid: (Saito et al., 2017) (Desai et al., 2019) (Jia et al., 2019) (Cui and Bollegala, 2019) (Peng and Dredze, 2017) (Guo et al., 2020) (Ben-David et al., 2020) Neural SCL Neural SCL (Joint AE-SCL) SDA MSDA MSDA MSDA DANN DANN+SCL MemNet DANN/DSN DANN DANN DANN DANN DANN DANN DANN DANN+Wasserstein DANN DANN DANN DSN (GSN) DANN, Shared encoders DANN (concept embeddings) DANN (context embeddings) 3 3 3 3 SSL, Multitask tri-training SSL Deep self-training AdaptaBERT Adaptive pre-training Adaptive pre-training (incl. multi-phase) 3 Asymmetric tri-training Adaptive (te"
2020.coling-main.603,J93-2004,0,0.0690272,"a domain? From the notion of domain to variety space and related problems Despite the formal definition of domain above, the term is quiet loosely used in NLP and there is no common ground on what constitutes a domain (Plank, 2016). Typically in NLP, domain is meant to refer to some coherent type of corpus, i.e., predetermined by the given dataset (Plank, 2011). This may relate to topic, style, genre, or linguistic register. The notion of domain and what plays into it has though significantly changed over the last years, leading to relevant research lines. First, the Penn Treebank WSJ corpus (Marcus et al., 1993) and the Brown corpus (Francis and Kucera, 1979) are prototypical examples, with the WSJ being considered widely as the canonical newswire domain. In the recent decade, there has been considerable work on what is considered non-canonical data. The dichotomy between canonical (typically considered well-edited English newswire) and noncanonical data arose with the increasing interest of working with social media with all its challenges related to the ‘noisiness’ of the domain (Eisenstein, 2013; Baldwin et al., 2013). Models trained on canonical data failed in light of the challenges on, e.g., Tw"
2020.coling-main.603,P06-1043,0,0.221494,"of pre-training methods. We summarize data-centric strands next, which differ whether they use pseudolabeling, select relevant data or use large unlabeled data or auxiliary tasks for model pre-training. 5.1 Pseudo-labeling The main idea of pseudo-labeling is to apply a trained classifier to predict labels on unlabeled instances, which are then treated as ‘pseudo’ gold labels. Pseudo-labeling applies semi-supervised methods (Abney, 2007; Zhu and Goldberg, 2009) such as bootstrapping methods like self-training, co-training and tri-training or methods such as temporal ensembling (Charniak, 1997; McClosky et al., 2006; Blum and Mitchell, 1998; Steedman et al., 2003; Zhou and Li, 2005; Søgaard and Rishøj, 2010; Saito et 6844 al., 2017; Laine and Aila, 2016) by using either the same model, a teacher model, or multiple bootstrap models which may include slower but more accurate hand-crafted models (Petrov et al., 2010) to guide pseudo-labeling. Most pseudo-labeling works date back to traditional non-neural learning methods. Bootstrapping methods for domain adaptation are well-studied in parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; Yu et al., 2015). They include models trained on other grammar"
2020.coling-main.603,N19-1039,0,0.106656,"spondence learning (SCL) (Blitzer et al., 2006) and spectral feature alignment (SFA) (Pan et al., 2010). They both aim at finding features which are common across domains by using unlabeled data from both domains. The two approaches differ in the specifics of the method to construct the shared space. SCL uses auxiliary functions inspired by Ando and Zhang (2005), while SFA uses a graph-based spectral learning method. Creating domain-specific and domain-general features is the key idea of EasyAdapt (Daum´e III, 2007), a seminal 6841 Model-centric: (Ziser and Reichart 2017; 2018a; 2018b; 2019) (Miller, 2019) (Glorot et al., 2011) (Chen et al., 2012) (Yang and Eisenstein, 2014) (Clinchant et al., 2016) (Ganin et al., 2016) (Li et al., 2017) (Kim et al., 2017) (Sato et al., 2017) (Wu et al., 2017) (Yasunaga et al., 2018) (Shen et al., 2018) (Li et al., 2018a) (Alam et al., 2018a) (Wang et al., 2019) (Shah et al., 2018) (Fu et al., 2017) (Rios et al., 2018) (Xu et al., 2019) (Shi et al., 2018) (Rocha and Lopes Cardoso, 2019)* (Ghosal et al., 2020) (Naik and Rose, 2020) Data-centric: (Ruder and Plank, 2018) (Lim et al., 2020) (Rotman and Reichart, 2019) (Han and Eisenstein, 2019) (Li et al., 2019) (G"
2020.coling-main.603,P10-2041,0,0.0332069,"sentations (Rotman and Reichart, 2019; Lim et al., 2020) and a recent work proposes adaptive ensembling (Desai et al., 2019) as extension of temporal ensembling (see hybrid methods in Section 6). 5.2 Data selection A relatively unexplored area is data selection for adaptation, which is gaining traction again in light of large pre-trained models (which data should they be trained on?) and the related problem of cross-lingual learning (what is/are the best source language(s) to transfer from?). Data selection aims to select the best matching data for a new domain, typically by using perplexity (Moore and Lewis, 2010) or using domain similarity measures such as Jensen-Shannon divergence over term or topic distributions (Plank and van Noord, 2011). This has mostly been studied for MT (Moore and Lewis, 2010; Axelrod et al., 2011; van der Wees et al., 2017; Aharoni and Goldberg, 2020), but also for parsing (Plank and van Noord, 2011; Ruder and Plank, 2017) and sentiment analysis (Remus, 2012) though for supervised domain adaptation setups only. For parsing and sentiment analysis, the simple Jensen-Shannon divergence on term distribution constitutes a strong baseline (Plank, 2011; Ruder and Plank, 2017). Withi"
2020.coling-main.603,2020.acl-main.681,0,0.10835,"eral features is the key idea of EasyAdapt (Daum´e III, 2007), a seminal 6841 Model-centric: (Ziser and Reichart 2017; 2018a; 2018b; 2019) (Miller, 2019) (Glorot et al., 2011) (Chen et al., 2012) (Yang and Eisenstein, 2014) (Clinchant et al., 2016) (Ganin et al., 2016) (Li et al., 2017) (Kim et al., 2017) (Sato et al., 2017) (Wu et al., 2017) (Yasunaga et al., 2018) (Shen et al., 2018) (Li et al., 2018a) (Alam et al., 2018a) (Wang et al., 2019) (Shah et al., 2018) (Fu et al., 2017) (Rios et al., 2018) (Xu et al., 2019) (Shi et al., 2018) (Rocha and Lopes Cardoso, 2019)* (Ghosal et al., 2020) (Naik and Rose, 2020) Data-centric: (Ruder and Plank, 2018) (Lim et al., 2020) (Rotman and Reichart, 2019) (Han and Eisenstein, 2019) (Li et al., 2019) (Gururangan et al., 2020) Hybrid: (Saito et al., 2017) (Desai et al., 2019) (Jia et al., 2019) (Cui and Bollegala, 2019) (Peng and Dredze, 2017) (Guo et al., 2020) (Ben-David et al., 2020) Neural SCL Neural SCL (Joint AE-SCL) SDA MSDA MSDA MSDA DANN DANN+SCL MemNet DANN/DSN DANN DANN DANN DANN DANN DANN DANN DANN+Wasserstein DANN DANN DANN DSN (GSN) DANN, Shared encoders DANN (concept embeddings) DANN (context embeddings) 3 3 3 3 SSL, Multitask tri-training SSL Dee"
2020.coling-main.603,W15-1506,0,0.0255927,"mance without the knowledge of the test distribution. To do so, it assumes a subpopulation shift, where the test population is a subpopulation mix of the training distribution. A model is then trained to do well over a wide range of potential test distributions. Some early work in dialogue (Bod, 1999) and parsing (Plank and Sima’an, 2008) adopted a similar idea of subdomains, however, with manually identified subpopulations. This bears some similarity to early work on leveraging general background knowledge (embeddings trained on general data) for domain adaptation (Plank and Moschitti, 2013; Nguyen and Grishman, 2015; Li et al., 2018a), and also relates to recent work on pre-training (Section 5.3). An alternative and complementary interesting line of research is to predict test set performance for new data varieties (Ravi et al., 2008; Van Asch and Daelemans, 2010; Elsahar and Gall´e, 2019; Xia et al., 2020). 4 Model-centric approaches Model-centric approaches redesign parts of the model: the feature space, the loss function or regularization and the structure of the model. We categorize them into feature-centric and loss-centric methods. 4.1 Feature-centric methods Two lines of work can be found within f"
2020.coling-main.603,D19-1432,0,0.134137,"own target domains. One example is the SANCL shared task (Petrov and McDonald, 2012), where participants were asked to build a single system that can robustly parse reviews, weblogs, answers, emails, newsgroups. In this setup, the DA problem boils down to finding a more robust system for given targets. It can be seen as optimizing for both in-domain and out-of-domain(s) accuracy. If domains are unknown a priori, robustness can be taken a step further towards out-of-domain generalization, to unknown targets, the most challenging setup. A recent solution is distributionally robust optimization (Oren et al., 2019), i.e., optimizing for worst-case performance without the knowledge of the test distribution. To do so, it assumes a subpopulation shift, where the test population is a subpopulation mix of the training distribution. A model is then trained to do well over a wide range of potential test distributions. Some early work in dialogue (Bod, 1999) and parsing (Plank and Sima’an, 2008) adopted a similar idea of subdomains, however, with manually identified subpopulations. This bears some similarity to early work on leveraging general background knowledge (embeddings trained on general data) for domain"
2020.coling-main.603,W17-2612,0,0.196083,"et al., 2017) (Kim et al., 2017) (Sato et al., 2017) (Wu et al., 2017) (Yasunaga et al., 2018) (Shen et al., 2018) (Li et al., 2018a) (Alam et al., 2018a) (Wang et al., 2019) (Shah et al., 2018) (Fu et al., 2017) (Rios et al., 2018) (Xu et al., 2019) (Shi et al., 2018) (Rocha and Lopes Cardoso, 2019)* (Ghosal et al., 2020) (Naik and Rose, 2020) Data-centric: (Ruder and Plank, 2018) (Lim et al., 2020) (Rotman and Reichart, 2019) (Han and Eisenstein, 2019) (Li et al., 2019) (Gururangan et al., 2020) Hybrid: (Saito et al., 2017) (Desai et al., 2019) (Jia et al., 2019) (Cui and Bollegala, 2019) (Peng and Dredze, 2017) (Guo et al., 2020) (Ben-David et al., 2020) Neural SCL Neural SCL (Joint AE-SCL) SDA MSDA MSDA MSDA DANN DANN+SCL MemNet DANN/DSN DANN DANN DANN DANN DANN DANN DANN DANN+Wasserstein DANN DANN DANN DSN (GSN) DANN, Shared encoders DANN (concept embeddings) DANN (context embeddings) 3 3 3 3 SSL, Multitask tri-training SSL Deep self-training AdaptaBERT Adaptive pre-training Adaptive pre-training (incl. multi-phase) 3 Asymmetric tri-training Adaptive (temporal) ensembling Cross-domain LM SelfAdapt (pivots+co-training) Multi-task-DA DistanceNet-Bandit PERL (pivots+context embeddings) 3 RE NER DEP"
2020.coling-main.603,N18-1202,0,0.141338,"s-lingual work, ¨ un et al., 2019; Lin et al., 2019). Another line explores whether simple overlap metrics are indicative (Ust¨ tailoring large pre-trained models to the domain of a target task is still beneficial, and use of data selection to overcome costly expert selection. They propose two multi-phase pre-training methods (Gururangan et al., 2020) (as discussed further below) with promising results on text classification tasks. 5.3 Pre-training—And:—Is bigger better? Are domains (or: varieties) still relevant? Large pre-trained models have become ubiquitous in NLP (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019). Fine-tuning a transformer-based model with a small amount of labeled data often reaches high performance across NLP tasks and has become a de-facto standard. It means starting from the pre-trained model weights and training a new task-specific layer on supervised data. A natural question which arises is how universal such large models are. Is bigger better? And are domains (or varieties) still relevant? We return to these questions after depicting pre-training strategies. We delineate: 1. Pre-training: pre-training alone (e.g., multilingual BERT; language-specfic BERTs"
2020.coling-main.603,D10-1069,0,0.0350586,"s on unlabeled instances, which are then treated as ‘pseudo’ gold labels. Pseudo-labeling applies semi-supervised methods (Abney, 2007; Zhu and Goldberg, 2009) such as bootstrapping methods like self-training, co-training and tri-training or methods such as temporal ensembling (Charniak, 1997; McClosky et al., 2006; Blum and Mitchell, 1998; Steedman et al., 2003; Zhou and Li, 2005; Søgaard and Rishøj, 2010; Saito et 6844 al., 2017; Laine and Aila, 2016) by using either the same model, a teacher model, or multiple bootstrap models which may include slower but more accurate hand-crafted models (Petrov et al., 2010) to guide pseudo-labeling. Most pseudo-labeling works date back to traditional non-neural learning methods. Bootstrapping methods for domain adaptation are well-studied in parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; Yu et al., 2015). They include models trained on other grammar formalisms to improve dependency parsing on Twitter (Foster et al., 2011). Recently, this line of classics has been revisited (Ruder and Plank, 2018; Rotman and Reichart, 2019; Lim et al., 2020). For example, classic methods such as tri-training constitute a strong baseline for domain shift in neural t"
2020.coling-main.603,P13-1147,1,0.800982,"izing for worst-case performance without the knowledge of the test distribution. To do so, it assumes a subpopulation shift, where the test population is a subpopulation mix of the training distribution. A model is then trained to do well over a wide range of potential test distributions. Some early work in dialogue (Bod, 1999) and parsing (Plank and Sima’an, 2008) adopted a similar idea of subdomains, however, with manually identified subpopulations. This bears some similarity to early work on leveraging general background knowledge (embeddings trained on general data) for domain adaptation (Plank and Moschitti, 2013; Nguyen and Grishman, 2015; Li et al., 2018a), and also relates to recent work on pre-training (Section 5.3). An alternative and complementary interesting line of research is to predict test set performance for new data varieties (Ravi et al., 2008; Van Asch and Daelemans, 2010; Elsahar and Gall´e, 2019; Xia et al., 2020). 4 Model-centric approaches Model-centric approaches redesign parts of the model: the feature space, the loss function or regularization and the structure of the model. We categorize them into feature-centric and loss-centric methods. 4.1 Feature-centric methods Two lines of"
2020.coling-main.603,plank-simaan-2008-subdomain,1,0.801714,"Missing"
2020.coling-main.603,P11-1157,1,0.904297,"Missing"
2020.coling-main.603,P14-2083,1,0.92045,"domain discrepancy information include maximum mean discrepancy (MMD) (Gretton et al., 2007) and its more efficient version called kernel mean matching (KMM) (Gretton et al., 2009). KMM reweights the training instances such that the means of the training and test points in reproducing a kernel Hilbert space are close to each other. Jiang and Zhai (2007) introduced instance weighting in NLP and proposed to learn weights by first training domain classifiers. The effectiveness of the method in neural setups remains to be seen. An early study reports non-significant improvements for POS tagging (Plank et al., 2014b). 5 Data-centric methods Recently, data-centric approaches are on a rise, due to rapid growth of data and the gain in popularity of pre-training methods. We summarize data-centric strands next, which differ whether they use pseudolabeling, select relevant data or use large unlabeled data or auxiliary tasks for model pre-training. 5.1 Pseudo-labeling The main idea of pseudo-labeling is to apply a trained classifier to predict labels on unlabeled instances, which are then treated as ‘pseudo’ gold labels. Pseudo-labeling applies semi-supervised methods (Abney, 2007; Zhu and Goldberg, 2009) such"
2020.coling-main.603,D14-1104,1,0.945068,"domain discrepancy information include maximum mean discrepancy (MMD) (Gretton et al., 2007) and its more efficient version called kernel mean matching (KMM) (Gretton et al., 2009). KMM reweights the training instances such that the means of the training and test points in reproducing a kernel Hilbert space are close to each other. Jiang and Zhai (2007) introduced instance weighting in NLP and proposed to learn weights by first training domain classifiers. The effectiveness of the method in neural setups remains to be seen. An early study reports non-significant improvements for POS tagging (Plank et al., 2014b). 5 Data-centric methods Recently, data-centric approaches are on a rise, due to rapid growth of data and the gain in popularity of pre-training methods. We summarize data-centric strands next, which differ whether they use pseudolabeling, select relevant data or use large unlabeled data or auxiliary tasks for model pre-training. 5.1 Pseudo-labeling The main idea of pseudo-labeling is to apply a trained classifier to predict labels on unlabeled instances, which are then treated as ‘pseudo’ gold labels. Pseudo-labeling applies semi-supervised methods (Abney, 2007; Zhu and Goldberg, 2009) such"
2020.coling-main.603,D08-1093,0,0.116496,"of potential test distributions. Some early work in dialogue (Bod, 1999) and parsing (Plank and Sima’an, 2008) adopted a similar idea of subdomains, however, with manually identified subpopulations. This bears some similarity to early work on leveraging general background knowledge (embeddings trained on general data) for domain adaptation (Plank and Moschitti, 2013; Nguyen and Grishman, 2015; Li et al., 2018a), and also relates to recent work on pre-training (Section 5.3). An alternative and complementary interesting line of research is to predict test set performance for new data varieties (Ravi et al., 2008; Van Asch and Daelemans, 2010; Elsahar and Gall´e, 2019; Xia et al., 2020). 4 Model-centric approaches Model-centric approaches redesign parts of the model: the feature space, the loss function or regularization and the structure of the model. We categorize them into feature-centric and loss-centric methods. 4.1 Feature-centric methods Two lines of work can be found within feature-centric methods: feature augmentation and feature generalization methods. The former use pivots (common shared features) to construct an aligned feature space. The latter use autoencoders to find latent representati"
2020.coling-main.603,P07-1078,0,0.113107,"d tri-training or methods such as temporal ensembling (Charniak, 1997; McClosky et al., 2006; Blum and Mitchell, 1998; Steedman et al., 2003; Zhou and Li, 2005; Søgaard and Rishøj, 2010; Saito et 6844 al., 2017; Laine and Aila, 2016) by using either the same model, a teacher model, or multiple bootstrap models which may include slower but more accurate hand-crafted models (Petrov et al., 2010) to guide pseudo-labeling. Most pseudo-labeling works date back to traditional non-neural learning methods. Bootstrapping methods for domain adaptation are well-studied in parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; Yu et al., 2015). They include models trained on other grammar formalisms to improve dependency parsing on Twitter (Foster et al., 2011). Recently, this line of classics has been revisited (Ruder and Plank, 2018; Rotman and Reichart, 2019; Lim et al., 2020). For example, classic methods such as tri-training constitute a strong baseline for domain shift in neural times (Ruder and Plank, 2018). Pseudo-labeling has recently been studied for parsing with contextualized word representations (Rotman and Reichart, 2019; Lim et al., 2020) and a recent work proposes adaptive ensembling (Desai et al.,"
2020.coling-main.603,W19-4307,1,0.834169,"ions to study divergences in annotations (Plank et al., 2014a). Back to the roots and how knowledge transfers Revisiting classics in neural times is beneficial, as shown for example in recent work which brings back SCL and pseudo-labeling methods (see Table 1), but much is left to see how these methods generalize. This can be linked to the question on what representations capture (Belinkov and Glass, 2019) and how knowledge transfers (Rethmeier et al., 2020). X scarcity Even unlabeled data can be scarce (X scarcity), particularly in highly-specialized language varieties (e.g., clinical data) (Rethmeier and Plank, 2019). This is often due to data sharing restrictions. In some cases, only a trained source model could be available instead of raw or labeled texts (Laparra et al., 2020). Together with the quest for more efficient learning methods, the general question of how to adapt in light of X scarcity or absence becomes important. 8 Conclusion In this survey, we review strands of unsupervised domain adaptation, summarized into model-centric, data-centric, and hybrid methods, including trends in pre-training. We also revisit the notion of domain and suggest to use the term variety instead, to better capture"
2020.coling-main.603,D19-6102,0,0.048013,"Missing"
2020.coling-main.603,Q19-1044,0,0.163413,"odel-centric: (Ziser and Reichart 2017; 2018a; 2018b; 2019) (Miller, 2019) (Glorot et al., 2011) (Chen et al., 2012) (Yang and Eisenstein, 2014) (Clinchant et al., 2016) (Ganin et al., 2016) (Li et al., 2017) (Kim et al., 2017) (Sato et al., 2017) (Wu et al., 2017) (Yasunaga et al., 2018) (Shen et al., 2018) (Li et al., 2018a) (Alam et al., 2018a) (Wang et al., 2019) (Shah et al., 2018) (Fu et al., 2017) (Rios et al., 2018) (Xu et al., 2019) (Shi et al., 2018) (Rocha and Lopes Cardoso, 2019)* (Ghosal et al., 2020) (Naik and Rose, 2020) Data-centric: (Ruder and Plank, 2018) (Lim et al., 2020) (Rotman and Reichart, 2019) (Han and Eisenstein, 2019) (Li et al., 2019) (Gururangan et al., 2020) Hybrid: (Saito et al., 2017) (Desai et al., 2019) (Jia et al., 2019) (Cui and Bollegala, 2019) (Peng and Dredze, 2017) (Guo et al., 2020) (Ben-David et al., 2020) Neural SCL Neural SCL (Joint AE-SCL) SDA MSDA MSDA MSDA DANN DANN+SCL MemNet DANN/DSN DANN DANN DANN DANN DANN DANN DANN DANN+Wasserstein DANN DANN DANN DSN (GSN) DANN, Shared encoders DANN (concept embeddings) DANN (context embeddings) 3 3 3 3 SSL, Multitask tri-training SSL Deep self-training AdaptaBERT Adaptive pre-training Adaptive pre-training (incl. multi-"
2020.coling-main.603,D17-1038,1,0.95615,"of domain adaptation in NLP We categorize research into model-centric, datacentric and hybrid approaches, as shown in Figure 1. Model-centric methods target approaches to augment the feature space, alter the loss function, the architecture or model parameters (Blitzer et al., 2006; Pan et al., 2010; Ganin et al., 2016). Data-centric methods focus on the data aspect and either involve pseudo-labeling (or bootstrapping) to bridge the domain gap (Abney, 2007; Zhu and Goldberg, 2009; Ruder and Plank, 2018; Cui and Bollegala, 2019), data selection (Axelrod et al., 2011; Plank and van Noord, 2011; Ruder and Plank, 2017) and pre-training methods (Han and Eisenstein, 2019; Guo et al., 2020). As some approaches take elements of both, we include a hybrid category.2 A comprehensive overview of UDA methods and the tasks each method is applied to is provided in Table 1. Other surveys Comprehensive reviews on DA exist, each with a different focus: visual applications (Csurka, 2017; Patel et al., 2015; Wilson and Cook, 2020), machine translation (MT) (Chu and Wang, 2018), pre-neural DA methods in NLP (Jiang, 2008; Margolis, 2011). Seminal surveys in machine learning on transfer learning include Pan and Yang (2009), W"
2020.coling-main.603,P18-1096,1,0.903716,"able solution. We believe these advances in UDA will help for out-of-distribution generalization. A categorization of domain adaptation in NLP We categorize research into model-centric, datacentric and hybrid approaches, as shown in Figure 1. Model-centric methods target approaches to augment the feature space, alter the loss function, the architecture or model parameters (Blitzer et al., 2006; Pan et al., 2010; Ganin et al., 2016). Data-centric methods focus on the data aspect and either involve pseudo-labeling (or bootstrapping) to bridge the domain gap (Abney, 2007; Zhu and Goldberg, 2009; Ruder and Plank, 2018; Cui and Bollegala, 2019), data selection (Axelrod et al., 2011; Plank and van Noord, 2011; Ruder and Plank, 2017) and pre-training methods (Han and Eisenstein, 2019; Guo et al., 2020). As some approaches take elements of both, we include a hybrid category.2 A comprehensive overview of UDA methods and the tasks each method is applied to is provided in Table 1. Other surveys Comprehensive reviews on DA exist, each with a different focus: visual applications (Csurka, 2017; Patel et al., 2015; Wilson and Cook, 2020), machine translation (MT) (Chu and Wang, 2018), pre-neural DA methods in NLP (Ji"
2020.coling-main.603,N19-5004,0,0.159659,"f labeled target domain data is available, along with some larger amount of labeled source domain data. The task is to adapt from the source to the specific target domain in light of limited target domain data. However, annotation is a substantial timerequiring and costly manual effort. While annotation directly mitigates the lack of labeled data, it does not easily scale to new application targets. In contrast, DA methods aim to shift the ability of models from the traditional interpolation of similar examples to models that extrapolate to examples outside the original training distribution (Ruder, 2019). Unsupervised domain adaptation (UDA) mitigates the domain shift issue by learning only from unlabeled target data, which is typically available for both source and target domain(s). UDA fits the classical real-world scenario better, in which labeled data in 1 Accompanying repository: https://github.com/bplank/awesome-neural-adaptation-in-NLP This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 6838 Proceedings of the 28th International Conference on Computational Linguistics, pages 6838–6855 Barce"
2020.coling-main.603,K17-3007,0,0.174755,"ns by using unlabeled data from both domains. The two approaches differ in the specifics of the method to construct the shared space. SCL uses auxiliary functions inspired by Ando and Zhang (2005), while SFA uses a graph-based spectral learning method. Creating domain-specific and domain-general features is the key idea of EasyAdapt (Daum´e III, 2007), a seminal 6841 Model-centric: (Ziser and Reichart 2017; 2018a; 2018b; 2019) (Miller, 2019) (Glorot et al., 2011) (Chen et al., 2012) (Yang and Eisenstein, 2014) (Clinchant et al., 2016) (Ganin et al., 2016) (Li et al., 2017) (Kim et al., 2017) (Sato et al., 2017) (Wu et al., 2017) (Yasunaga et al., 2018) (Shen et al., 2018) (Li et al., 2018a) (Alam et al., 2018a) (Wang et al., 2019) (Shah et al., 2018) (Fu et al., 2017) (Rios et al., 2018) (Xu et al., 2019) (Shi et al., 2018) (Rocha and Lopes Cardoso, 2019)* (Ghosal et al., 2020) (Naik and Rose, 2020) Data-centric: (Ruder and Plank, 2018) (Lim et al., 2020) (Rotman and Reichart, 2019) (Han and Eisenstein, 2019) (Li et al., 2019) (Gururangan et al., 2020) Hybrid: (Saito et al., 2017) (Desai et al., 2019) (Jia et al., 2019) (Cui and Bollegala, 2019) (Peng and Dredze, 2017) (Guo et al., 2020) (Ben-David"
2020.coling-main.603,D18-1131,0,0.26226,"auxiliary functions inspired by Ando and Zhang (2005), while SFA uses a graph-based spectral learning method. Creating domain-specific and domain-general features is the key idea of EasyAdapt (Daum´e III, 2007), a seminal 6841 Model-centric: (Ziser and Reichart 2017; 2018a; 2018b; 2019) (Miller, 2019) (Glorot et al., 2011) (Chen et al., 2012) (Yang and Eisenstein, 2014) (Clinchant et al., 2016) (Ganin et al., 2016) (Li et al., 2017) (Kim et al., 2017) (Sato et al., 2017) (Wu et al., 2017) (Yasunaga et al., 2018) (Shen et al., 2018) (Li et al., 2018a) (Alam et al., 2018a) (Wang et al., 2019) (Shah et al., 2018) (Fu et al., 2017) (Rios et al., 2018) (Xu et al., 2019) (Shi et al., 2018) (Rocha and Lopes Cardoso, 2019)* (Ghosal et al., 2020) (Naik and Rose, 2020) Data-centric: (Ruder and Plank, 2018) (Lim et al., 2020) (Rotman and Reichart, 2019) (Han and Eisenstein, 2019) (Li et al., 2019) (Gururangan et al., 2020) Hybrid: (Saito et al., 2017) (Desai et al., 2019) (Jia et al., 2019) (Cui and Bollegala, 2019) (Peng and Dredze, 2017) (Guo et al., 2020) (Ben-David et al., 2020) Neural SCL Neural SCL (Joint AE-SCL) SDA MSDA MSDA MSDA DANN DANN+SCL MemNet DANN/DSN DANN DANN DANN DANN DANN DANN DANN DANN+Wa"
2020.coling-main.603,2020.acl-main.468,0,0.0256573,"litics, environmental law, molecular biology) and socio-demographic aspects (e.g., gender), among other unknown factors, as well as stylistic or data sampling impacts (e.g., sentence length, annotator bias). In spirit of the variety space (Plank, 2016), we suggest to use the more general term variety, rather than domain, which pinpoints better to the underlying linguistic differences and their implications rather than the technical assumptions. Each corpus is inevitably biased towards a specialized language and some latent aspects. Understanding bias sources and effects, besides effects only (Shah et al., 2020), and documenting the known are the first important steps (Bender and Friedman, 2018), as is building broader, more varied corpora (Ide and Suderman, 2004). What we need more work on is to link the known to the unknown, and studying its impact. Doing so will ultimately help to not only overcome overfitting 4 As outlined in Plank (2011), there exists a three-way distinction for domain adaptation: supervised DA, unsupervised DA but also semi-supervised DA. The latter was coined around 2010 to distinguish purely unsupervised DA from cases where a small amount of labeled data is available in addit"
2020.coling-main.603,D18-1125,0,0.01312,"(DSNs) (Bousmalis et al., 2016) have been proposed. DSNs separate latent representations in i) separate private encoders (i.e., one for each domain) and ii) a shared encoder (in charge to reconstruct the input instance using these representations). This bears similarities to a traditional supervised method (Daum´e III, 2007). The main drawback of DSNs is that domain-specific representations are solely used in the decoder, leaving the classifier to be trained on the domain-invariant representations only. DSNs have seen a notable success in Computer Vision (CV) (Bousmalis et al., 2016). In NLP, Shi et al. (2018) propose the genre separation networks (GSNs) as a variant of the DSNs, introducing a novel reconstruction component that leverages both shared and private feature representations in the learning process. As noted also by Han and Eisenstein (2019), a downside of adversarial methods is that they require careful balancing between objectives (Kim et al., 2017; Alam et al., 2018a) to avoid instability during learning (Arjovsky et al., 2017). Reweighting This family of methods is an instance-level adaptation method. The core idea of instance weighting (also known as importance weighting) is to assi"
2020.coling-main.603,C10-1120,0,0.060389,"use pseudolabeling, select relevant data or use large unlabeled data or auxiliary tasks for model pre-training. 5.1 Pseudo-labeling The main idea of pseudo-labeling is to apply a trained classifier to predict labels on unlabeled instances, which are then treated as ‘pseudo’ gold labels. Pseudo-labeling applies semi-supervised methods (Abney, 2007; Zhu and Goldberg, 2009) such as bootstrapping methods like self-training, co-training and tri-training or methods such as temporal ensembling (Charniak, 1997; McClosky et al., 2006; Blum and Mitchell, 1998; Steedman et al., 2003; Zhou and Li, 2005; Søgaard and Rishøj, 2010; Saito et 6844 al., 2017; Laine and Aila, 2016) by using either the same model, a teacher model, or multiple bootstrap models which may include slower but more accurate hand-crafted models (Petrov et al., 2010) to guide pseudo-labeling. Most pseudo-labeling works date back to traditional non-neural learning methods. Bootstrapping methods for domain adaptation are well-studied in parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; Yu et al., 2015). They include models trained on other grammar formalisms to improve dependency parsing on Twitter (Foster et al., 2011). Recently, this li"
2020.coling-main.603,N03-1031,0,0.158882,"ic strands next, which differ whether they use pseudolabeling, select relevant data or use large unlabeled data or auxiliary tasks for model pre-training. 5.1 Pseudo-labeling The main idea of pseudo-labeling is to apply a trained classifier to predict labels on unlabeled instances, which are then treated as ‘pseudo’ gold labels. Pseudo-labeling applies semi-supervised methods (Abney, 2007; Zhu and Goldberg, 2009) such as bootstrapping methods like self-training, co-training and tri-training or methods such as temporal ensembling (Charniak, 1997; McClosky et al., 2006; Blum and Mitchell, 1998; Steedman et al., 2003; Zhou and Li, 2005; Søgaard and Rishøj, 2010; Saito et 6844 al., 2017; Laine and Aila, 2016) by using either the same model, a teacher model, or multiple bootstrap models which may include slower but more accurate hand-crafted models (Petrov et al., 2010) to guide pseudo-labeling. Most pseudo-labeling works date back to traditional non-neural learning methods. Bootstrapping methods for domain adaptation are well-studied in parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; Yu et al., 2015). They include models trained on other grammar formalisms to improve dependency parsing on Twi"
2020.coling-main.603,P16-1013,0,0.018082,"a like BookCorpus and Wikipedia in BERT (Devlin et al., 2019) or target-specific samples, like papers from Semantic Scholar in SciBERT (Beltagy et al., 2019), and PubMed abstracts and PMC full-text articles in BioBERT (Lee et al., 2020). What denotes relevant data is an open question. Today, it is either general background knowledge, domain-specific target data, or a combination thereof, possibly via auxiliary tasks or intermediate training stages. Most of these have been carefully selected manually, raising interesting connections to data selection (Section 5.2) and finding better curricula (Tsvetkov et al., 2016) to learn under domain shift (Ruder and Plank, 2017). While large pre-trained models have shown to work well, many questions and challenges remain. Recent work has shown that these models degrade on out-of-domain data, maximum likelihood training makes them too over-confident (Oren et al., 2019) and particularly calibration is important for out-ofdomain generalization (Hendrycks et al., 2020). An acknowledged issue with fine-tuning is the brittleness of the process (Phang et al., 2018; Dodge et al., 2020). Even with the same hyperparameters, distinct runs can lead to drastically different resu"
2020.coling-main.603,W19-4206,0,0.0205935,"Missing"
2020.coling-main.603,W10-2605,0,0.371551,"Missing"
2020.coling-main.603,D17-1147,0,0.0604433,"Missing"
2020.coling-main.603,D19-1254,0,0.109758,"ared space. SCL uses auxiliary functions inspired by Ando and Zhang (2005), while SFA uses a graph-based spectral learning method. Creating domain-specific and domain-general features is the key idea of EasyAdapt (Daum´e III, 2007), a seminal 6841 Model-centric: (Ziser and Reichart 2017; 2018a; 2018b; 2019) (Miller, 2019) (Glorot et al., 2011) (Chen et al., 2012) (Yang and Eisenstein, 2014) (Clinchant et al., 2016) (Ganin et al., 2016) (Li et al., 2017) (Kim et al., 2017) (Sato et al., 2017) (Wu et al., 2017) (Yasunaga et al., 2018) (Shen et al., 2018) (Li et al., 2018a) (Alam et al., 2018a) (Wang et al., 2019) (Shah et al., 2018) (Fu et al., 2017) (Rios et al., 2018) (Xu et al., 2019) (Shi et al., 2018) (Rocha and Lopes Cardoso, 2019)* (Ghosal et al., 2020) (Naik and Rose, 2020) Data-centric: (Ruder and Plank, 2018) (Lim et al., 2020) (Rotman and Reichart, 2019) (Han and Eisenstein, 2019) (Li et al., 2019) (Gururangan et al., 2020) Hybrid: (Saito et al., 2017) (Desai et al., 2019) (Jia et al., 2019) (Cui and Bollegala, 2019) (Peng and Dredze, 2017) (Guo et al., 2020) (Ben-David et al., 2020) Neural SCL Neural SCL (Joint AE-SCL) SDA MSDA MSDA MSDA DANN DANN+SCL MemNet DANN/DSN DANN DANN DANN DANN DA"
2020.coling-main.603,P09-1076,0,0.0285533,"rely unsupervised DA from cases where a small amount of labeled data is available in addition to the unlabeled target data. As this setup still assumes some labeled data and it has not received much attention, it is not discussed in the current survey. 6840 to overrepresented domains (e.g., the newswire bias (Plank, 2016)), but also work on robustness and ultimately out-of-distribution generalization, as described later on. Treating data as ‘just another input’ to machine learning is very problematic. For example, it is less known that the well-known Penn Treebank consists of multiple genres (Webber, 2009; Plank and van Noord, 2011), including reviews and some prose. It has almost universally been treated as prototypical news domain. Similarly, social media is typically considered only non-canonical data, but an analysis revealed the data to lie on a “continuum of similarity” (Baldwin et al., 2013). This has implications on NLP performance. As we have seen, there are a multitude of dimensions to consider in corpus composition and annotations, which are tied to the theoretical notion of a variety space. They challenge the true generalization capabilities of current models. What remains is to st"
2020.coling-main.603,D17-1187,0,0.148428,"d data from both domains. The two approaches differ in the specifics of the method to construct the shared space. SCL uses auxiliary functions inspired by Ando and Zhang (2005), while SFA uses a graph-based spectral learning method. Creating domain-specific and domain-general features is the key idea of EasyAdapt (Daum´e III, 2007), a seminal 6841 Model-centric: (Ziser and Reichart 2017; 2018a; 2018b; 2019) (Miller, 2019) (Glorot et al., 2011) (Chen et al., 2012) (Yang and Eisenstein, 2014) (Clinchant et al., 2016) (Ganin et al., 2016) (Li et al., 2017) (Kim et al., 2017) (Sato et al., 2017) (Wu et al., 2017) (Yasunaga et al., 2018) (Shen et al., 2018) (Li et al., 2018a) (Alam et al., 2018a) (Wang et al., 2019) (Shah et al., 2018) (Fu et al., 2017) (Rios et al., 2018) (Xu et al., 2019) (Shi et al., 2018) (Rocha and Lopes Cardoso, 2019)* (Ghosal et al., 2020) (Naik and Rose, 2020) Data-centric: (Ruder and Plank, 2018) (Lim et al., 2020) (Rotman and Reichart, 2019) (Han and Eisenstein, 2019) (Li et al., 2019) (Gururangan et al., 2020) Hybrid: (Saito et al., 2017) (Desai et al., 2019) (Jia et al., 2019) (Cui and Bollegala, 2019) (Peng and Dredze, 2017) (Guo et al., 2020) (Ben-David et al., 2020) Neur"
2020.coling-main.603,2020.acl-main.764,0,0.0381588,"d parsing (Plank and Sima’an, 2008) adopted a similar idea of subdomains, however, with manually identified subpopulations. This bears some similarity to early work on leveraging general background knowledge (embeddings trained on general data) for domain adaptation (Plank and Moschitti, 2013; Nguyen and Grishman, 2015; Li et al., 2018a), and also relates to recent work on pre-training (Section 5.3). An alternative and complementary interesting line of research is to predict test set performance for new data varieties (Ravi et al., 2008; Van Asch and Daelemans, 2010; Elsahar and Gall´e, 2019; Xia et al., 2020). 4 Model-centric approaches Model-centric approaches redesign parts of the model: the feature space, the loss function or regularization and the structure of the model. We categorize them into feature-centric and loss-centric methods. 4.1 Feature-centric methods Two lines of work can be found within feature-centric methods: feature augmentation and feature generalization methods. The former use pivots (common shared features) to construct an aligned feature space. The latter use autoencoders to find latent representations that transfer better across domains. Pivots-based DA Seminal pivot-base"
2020.coling-main.603,P14-2088,0,0.0262393,"Glorot et al. (2011), who introduced the stacked denoising autoencoder (SDA) for domain adaptation. Basically, a SDA automatically learns a robust and unified feature representation for all domains by stacking multiple layers, and artificially corrupts the inputs with a Gaussian noise that the decoder needs to reconstruct. However, SDAs showed issues in speed and scalability to high-dimensional data. To mitigate these limitations, a more efficient marginalized stacked denoising autoencoder (MSDA) that marginalizes the noise was proposed (Chen et al., 2012). MSDAs have been further extended by Yang and Eisenstein (2014) with marginalized structured dropout, and by Clinchant et al. (2016), which improved the regularization of MSDAs following the insights from the domain adversarial training of neural networks (Ganin and Lempitsky, 2015; Ganin et al., 2016) (described in Section 4.2). The main drawback of autoencoder approaches is that the induced representations do not make use of any linguistic information. 4.2 Loss-centric methods Loss-centric approaches can be divided into methods which employ domain adversaries, and instancelevel reweighting methods. We outline these two strands of work in the following."
2020.coling-main.603,N18-1089,0,0.231312,"omains. The two approaches differ in the specifics of the method to construct the shared space. SCL uses auxiliary functions inspired by Ando and Zhang (2005), while SFA uses a graph-based spectral learning method. Creating domain-specific and domain-general features is the key idea of EasyAdapt (Daum´e III, 2007), a seminal 6841 Model-centric: (Ziser and Reichart 2017; 2018a; 2018b; 2019) (Miller, 2019) (Glorot et al., 2011) (Chen et al., 2012) (Yang and Eisenstein, 2014) (Clinchant et al., 2016) (Ganin et al., 2016) (Li et al., 2017) (Kim et al., 2017) (Sato et al., 2017) (Wu et al., 2017) (Yasunaga et al., 2018) (Shen et al., 2018) (Li et al., 2018a) (Alam et al., 2018a) (Wang et al., 2019) (Shah et al., 2018) (Fu et al., 2017) (Rios et al., 2018) (Xu et al., 2019) (Shi et al., 2018) (Rocha and Lopes Cardoso, 2019)* (Ghosal et al., 2020) (Naik and Rose, 2020) Data-centric: (Ruder and Plank, 2018) (Lim et al., 2020) (Rotman and Reichart, 2019) (Han and Eisenstein, 2019) (Li et al., 2019) (Gururangan et al., 2020) Hybrid: (Saito et al., 2017) (Desai et al., 2019) (Jia et al., 2019) (Cui and Bollegala, 2019) (Peng and Dredze, 2017) (Guo et al., 2020) (Ben-David et al., 2020) Neural SCL Neural SCL (Joint"
2020.coling-main.603,W15-2201,0,0.0366946,"as temporal ensembling (Charniak, 1997; McClosky et al., 2006; Blum and Mitchell, 1998; Steedman et al., 2003; Zhou and Li, 2005; Søgaard and Rishøj, 2010; Saito et 6844 al., 2017; Laine and Aila, 2016) by using either the same model, a teacher model, or multiple bootstrap models which may include slower but more accurate hand-crafted models (Petrov et al., 2010) to guide pseudo-labeling. Most pseudo-labeling works date back to traditional non-neural learning methods. Bootstrapping methods for domain adaptation are well-studied in parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; Yu et al., 2015). They include models trained on other grammar formalisms to improve dependency parsing on Twitter (Foster et al., 2011). Recently, this line of classics has been revisited (Ruder and Plank, 2018; Rotman and Reichart, 2019; Lim et al., 2020). For example, classic methods such as tri-training constitute a strong baseline for domain shift in neural times (Ruder and Plank, 2018). Pseudo-labeling has recently been studied for parsing with contextualized word representations (Rotman and Reichart, 2019; Lim et al., 2020) and a recent work proposes adaptive ensembling (Desai et al., 2019) as extensio"
2020.coling-main.603,N19-1131,0,0.0227451,"isenstein, 2013; Baldwin et al., 2013). Models trained on canonical data failed in light of the challenges on, e.g., Twitter (Gimpel et al., 2011; Foster et al., 2011). The general quest to understand the implications of variations of language on model performance led to lines of work on how human factors impact data in a covert or overt way, e.g., on how latent socio-demographic factors impact NLP performance (Hovy, 2015; Nguyen et al., 2016), or how direct data collection strategies like crowdsourcing impact corpus composition (Geva et al., 2019) or frequency effects impact NLP performance (Zhang et al., 2019). However, what is a domain? Is, say, Twitter, its own domain? Or is it a set of subdomains? Similarly, do language samples of social groups (e.g., sociolects) form a domain or a set of subdomains? Variety space We believe it is time to reconsider the notion of domain, the use of the term itself, and raise even more awareness of the underlying variation in the data samples NLP works with. NLP is pervasively facing heterogeneity in data along many underlying (often unknown) dimensions. A theoretical notion put forward by Plank (2016) is the variety space. In the variety space a corpus is seen a"
2020.coling-main.603,K17-1040,0,0.168654,"pivot-based methods include: structural correspondence learning (SCL) (Blitzer et al., 2006) and spectral feature alignment (SFA) (Pan et al., 2010). They both aim at finding features which are common across domains by using unlabeled data from both domains. The two approaches differ in the specifics of the method to construct the shared space. SCL uses auxiliary functions inspired by Ando and Zhang (2005), while SFA uses a graph-based spectral learning method. Creating domain-specific and domain-general features is the key idea of EasyAdapt (Daum´e III, 2007), a seminal 6841 Model-centric: (Ziser and Reichart 2017; 2018a; 2018b; 2019) (Miller, 2019) (Glorot et al., 2011) (Chen et al., 2012) (Yang and Eisenstein, 2014) (Clinchant et al., 2016) (Ganin et al., 2016) (Li et al., 2017) (Kim et al., 2017) (Sato et al., 2017) (Wu et al., 2017) (Yasunaga et al., 2018) (Shen et al., 2018) (Li et al., 2018a) (Alam et al., 2018a) (Wang et al., 2019) (Shah et al., 2018) (Fu et al., 2017) (Rios et al., 2018) (Xu et al., 2019) (Shi et al., 2018) (Rocha and Lopes Cardoso, 2019)* (Ghosal et al., 2020) (Naik and Rose, 2020) Data-centric: (Ruder and Plank, 2018) (Lim et al., 2020) (Rotman and Reichart, 2019) (Han and Ei"
2020.coling-main.603,D18-1022,0,0.192926,"s; LI = language identification; TC = binary text classification (incl. machine reading, duplicate question detection, stance detection, intent classification, political data identification); NLI = natural language inference; POS = part-of-speech (incl. Chinese word segmentation); DEP = dependency parsing; NER = named entity recognition (incl. slot tagging, event trigger identification, named entity segmentation); RE = relation extraction. *with cross-lingual adaptation.  applicable to UDA but main focus is supervised DA. supervised DA method. A recent line of work (Ziser and Reichart, 2017; Ziser and Reichart, 2018a; Ziser and Reichart, 2018b; Ziser and Reichart, 2019) brings SCL back to neural networks. In particular, Ziser and Reichart (2017) propose to combine the strengths of pivot-based methods with autoencoder neural networks in an autoencoder structural correspondence learning (AE-SCL) model. Autoencoders are used to learn latent representations to map non-pivots to pivots, and these encodings are then used to augment the training data. The main drawback of this approach is that the output vector representations of the text are unique and not context-dependent. To solve this problem, a pivot-base"
2020.coling-main.603,N18-1112,0,0.340611,"s; LI = language identification; TC = binary text classification (incl. machine reading, duplicate question detection, stance detection, intent classification, political data identification); NLI = natural language inference; POS = part-of-speech (incl. Chinese word segmentation); DEP = dependency parsing; NER = named entity recognition (incl. slot tagging, event trigger identification, named entity segmentation); RE = relation extraction. *with cross-lingual adaptation.  applicable to UDA but main focus is supervised DA. supervised DA method. A recent line of work (Ziser and Reichart, 2017; Ziser and Reichart, 2018a; Ziser and Reichart, 2018b; Ziser and Reichart, 2019) brings SCL back to neural networks. In particular, Ziser and Reichart (2017) propose to combine the strengths of pivot-based methods with autoencoder neural networks in an autoencoder structural correspondence learning (AE-SCL) model. Autoencoders are used to learn latent representations to map non-pivots to pivots, and these encodings are then used to augment the training data. The main drawback of this approach is that the output vector representations of the text are unique and not context-dependent. To solve this problem, a pivot-base"
2020.coling-main.603,P19-1591,0,0.0721306,"sification (incl. machine reading, duplicate question detection, stance detection, intent classification, political data identification); NLI = natural language inference; POS = part-of-speech (incl. Chinese word segmentation); DEP = dependency parsing; NER = named entity recognition (incl. slot tagging, event trigger identification, named entity segmentation); RE = relation extraction. *with cross-lingual adaptation.  applicable to UDA but main focus is supervised DA. supervised DA method. A recent line of work (Ziser and Reichart, 2017; Ziser and Reichart, 2018a; Ziser and Reichart, 2018b; Ziser and Reichart, 2019) brings SCL back to neural networks. In particular, Ziser and Reichart (2017) propose to combine the strengths of pivot-based methods with autoencoder neural networks in an autoencoder structural correspondence learning (AE-SCL) model. Autoencoders are used to learn latent representations to map non-pivots to pivots, and these encodings are then used to augment the training data. The main drawback of this approach is that the output vector representations of the text are unique and not context-dependent. To solve this problem, a pivot-based language modeling (PBLM) method has been proposed (Zi"
2020.emnlp-main.431,W18-2501,0,0.0202919,"confirm our hypothesis that hdi, hr, hi (option 1) is the most viable representation; it leads to the highest F1 score, largely 3 In case τ = 0 ∨ τ = 1, we adopt the same strategy, since all or no labels would be potentially predicted, respectively. 4 http://bionlp-st.dbcls.jp/GE/2011/ eval-test/. No gold entities In biomedical event extraction, entities are typically given in advance. To evaluate B EE SL in a setup with predicted entities (Section 6.3), we firstly employ our model as singletask sequence labeler for BIO-tagged entity mentions using default settings and a standard CRF decoder (Gardner et al., 2018). Note that for comparison purposes in all other experiments we assume entity mentions are gold-tagged. Then, we evaluate B EE SL with raw texts and predicted entities as input, thus indirectly penalizing events that take over-predicted entities or that miss entities since they are under-predicted. 5 Results First, we evaluate the MTL and multi-label decoding strategies on the development set to determine the best setup (Sections 5.1, 5.2). Then, we compare B EE SL to the results obtained by the top performing systems on the official test set (Section 5.3). Finally, we gauge its speed (Section"
2020.emnlp-main.431,D18-1162,0,0.0426754,"Missing"
2020.emnlp-main.431,W11-1802,0,0.53947,"gure 1: Performance of biomedical event extraction on the BioNLP Genia 2011 test set over time. Introduction Biomedical event extraction provides invaluable means for assisting domain experts in the curation of knowledge bases and biomolecular pathways (Ananiadou et al., 2010). While the task has received significant attention in research over the last decade, it remains challenging. Progress has been rather stagnating (see Figure 1). Events are typically highly complex and nested structures, which require deep contextual knowledge to resolve. This is particularly the case for biomedical NLP (Kim et al., 2011), where biomolecular events can be nested (Miwa et al., 2014) and long-distance arguments are frequent (Li et al., 2019). Figure 2 shows an example with four events. Each event consists of an event mention (trigger) and one or more arguments. For instance, there is a +R EGULATION event triggered by the 1 The source code is available at https://github. com/cosbi-research/beesl. span “induced”, with a P ROTEIN entity (i.e., “IL12”) as C AUSE and a nested +R EGULATION event (i.e., “activation”) as T HEME. Many state-of-theart biomedical event extraction systems still work as a pipeline and extrac"
2020.emnlp-main.431,D19-1279,0,0.02122,"opose to use multi-task learning (MTL) which allows to learn interdependencies while cutting down the label space, paired with multi-label prediction. An overview of B EE SL is shown in Figure 3. We use BERT (Devlin et al., 2019) as encoder, pretrained on biomedical texts (Section 4). We mask entity spans for better generalization (Alt et al., 2019). The first WordPiece (Schuster and Nakajima, 2012) of each token xi is used for prediction, where the contextual hidden representation ei of the token xi is encoded with layer-wise attention over the BERT layers, similarly to (Peters et al., 2018; Kondratyuk and Straka, 2019). As decoders, we use standard softmax with a cross entropy loss unless otherwise specified, and introduce a multilabel decoder (Section 3.2) (Figure 3, upper right). We empirically evaluate both single-task and multi-task setups, including several MTL encoding alternatives, discussing their limitations and benefits. In the following, we first introduce the multi-task setups, and then multi-label decoding. 3.1 Multi-task strategies We denote the label spaces for each component of the labels as di ∈ D, ri ∈ R, and hi ∈ H. Further, 3. hd, hi, hri: up to L = |D |× |H |+ |R|; 4. hdi, hri, hhi: up"
2020.emnlp-main.431,N19-1145,0,0.241642,"Missing"
2020.emnlp-main.431,2020.acl-main.713,0,0.0823967,"r work framed biomedical event extraction as syntactic and semantic tree- or graph-parsing (McClosky et al., 2011; Rao et al., 2017). In particular, McClosky et al. (2011) do dependency parsing, followed by a second-stage parse reranker model for event extraction, and Rao et al. (2017) cast the problem as subgraph identification problem. Joint learning for biomedical event extraction was explored in early work (Riedel and McCallum, 2011; Venugopal et al., 2014; Vlachos and Craven, 2012). Contemporary to our work, a very recent study proposes oneIE, a joint learning model for event extraction (Lin et al., 2020). It proposes a single end-to-end model for event extraction using 4 stages, paired with a beam search, obtaining good results on ACE data. Processing multiple heads has previously been done for relation extraction using multi-head selection (Bekoulis et al., 2018a,b), and sequence labeling has been employed for joint entity and relation classification (Dai et al., 2019) with inter-token attention. We employ it at the token-level for multi-label sequence labeling. 8 Conclusion This paper proposes B EE SL, a new end-to-end biomedical event extraction system which is both efficient and accurate."
2020.emnlp-main.431,W16-6308,0,0.0141679,"ly. This is due to the fact a T RANSCRIPTION is a gene E XPRESSION. Regarding the identification of arguments, overpredictions are quite uncommon. If erroneous, the main error we found may benefit from syntactic information, which we aim to integrate in a multitask setup in future work. We found no misclassification of arguments in our document samples. Under-prediction of arguments are instead mostly due to under-predicted events. 7 Related Work Biomedical event extraction has a long-standing tradition (Riedel et al., 2011; Miwa et al., 2012; Vlachos and Craven, 2012; Venugopal et al., 2014; Majumder et al., 2016). Current work has explored neural methods and uses multiple classification stages. Namely, first identifying trigger mentions, and then evaluating all entity pairs (Li et al., 2019; Bj¨orne and Salakoski, 2018). They come with the shortcomings of traditional pipeline methods. Many studies use dependency parsers to obtain features or for guidance of Tree-LSTMs (Li et al., 2019; Bj¨orne and Salakoski, 2018). Recent work in syntactic parsing has shown that reducing parsing to sequence labeling is a viable alternative for both constituent and dependency parsing (Spoustov´a and Spousta, 2010; G´om"
2020.emnlp-main.431,P11-1163,0,0.125676,"nd Salakoski, 2018). They come with the shortcomings of traditional pipeline methods. Many studies use dependency parsers to obtain features or for guidance of Tree-LSTMs (Li et al., 2019; Bj¨orne and Salakoski, 2018). Recent work in syntactic parsing has shown that reducing parsing to sequence labeling is a viable alternative for both constituent and dependency parsing (Spoustov´a and Spousta, 2010; G´omezRodr´ıguez and Vilares, 2018; Strzyz et al., 2019), which we took as inspiration. Moreover, earlier work framed biomedical event extraction as syntactic and semantic tree- or graph-parsing (McClosky et al., 2011; Rao et al., 2017). In particular, McClosky et al. (2011) do dependency parsing, followed by a second-stage parse reranker model for event extraction, and Rao et al. (2017) cast the problem as subgraph identification problem. Joint learning for biomedical event extraction was explored in early work (Riedel and McCallum, 2011; Venugopal et al., 2014; Vlachos and Craven, 2012). Contemporary to our work, a very recent study proposes oneIE, a joint learning model for event extraction (Lin et al., 2020). It proposes a single end-to-end model for event extraction using 4 stages, paired with a beam"
2020.emnlp-main.431,C14-1214,0,0.0237919,"oNLP Genia 2011 test set over time. Introduction Biomedical event extraction provides invaluable means for assisting domain experts in the curation of knowledge bases and biomolecular pathways (Ananiadou et al., 2010). While the task has received significant attention in research over the last decade, it remains challenging. Progress has been rather stagnating (see Figure 1). Events are typically highly complex and nested structures, which require deep contextual knowledge to resolve. This is particularly the case for biomedical NLP (Kim et al., 2011), where biomolecular events can be nested (Miwa et al., 2014) and long-distance arguments are frequent (Li et al., 2019). Figure 2 shows an example with four events. Each event consists of an event mention (trigger) and one or more arguments. For instance, there is a +R EGULATION event triggered by the 1 The source code is available at https://github. com/cosbi-research/beesl. span “induced”, with a P ROTEIN entity (i.e., “IL12”) as C AUSE and a nested +R EGULATION event (i.e., “activation”) as T HEME. Many state-of-theart biomedical event extraction systems still work as a pipeline and extract event triggers and their arguments independently (Bj¨orne a"
2020.emnlp-main.431,W19-5034,0,0.0563642,"Missing"
2020.emnlp-main.431,N18-1202,0,0.01236,"EE SL, we instead propose to use multi-task learning (MTL) which allows to learn interdependencies while cutting down the label space, paired with multi-label prediction. An overview of B EE SL is shown in Figure 3. We use BERT (Devlin et al., 2019) as encoder, pretrained on biomedical texts (Section 4). We mask entity spans for better generalization (Alt et al., 2019). The first WordPiece (Schuster and Nakajima, 2012) of each token xi is used for prediction, where the contextual hidden representation ei of the token xi is encoded with layer-wise attention over the BERT layers, similarly to (Peters et al., 2018; Kondratyuk and Straka, 2019). As decoders, we use standard softmax with a cross entropy loss unless otherwise specified, and introduce a multilabel decoder (Section 3.2) (Figure 3, upper right). We empirically evaluate both single-task and multi-task setups, including several MTL encoding alternatives, discussing their limitations and benefits. In the following, we first introduce the multi-task setups, and then multi-label decoding. 3.1 Multi-task strategies We denote the label spaces for each component of the labels as di ∈ D, ri ∈ R, and hi ∈ H. Further, 3. hd, hi, hri: up to L = |D |× |H"
2020.emnlp-main.431,W11-1807,0,0.0179756,"ative for both constituent and dependency parsing (Spoustov´a and Spousta, 2010; G´omezRodr´ıguez and Vilares, 2018; Strzyz et al., 2019), which we took as inspiration. Moreover, earlier work framed biomedical event extraction as syntactic and semantic tree- or graph-parsing (McClosky et al., 2011; Rao et al., 2017). In particular, McClosky et al. (2011) do dependency parsing, followed by a second-stage parse reranker model for event extraction, and Rao et al. (2017) cast the problem as subgraph identification problem. Joint learning for biomedical event extraction was explored in early work (Riedel and McCallum, 2011; Venugopal et al., 2014; Vlachos and Craven, 2012). Contemporary to our work, a very recent study proposes oneIE, a joint learning model for event extraction (Lin et al., 2020). It proposes a single end-to-end model for event extraction using 4 stages, paired with a beam search, obtaining good results on ACE data. Processing multiple heads has previously been done for relation extraction using multi-head selection (Bekoulis et al., 2018a,b), and sequence labeling has been employed for joint entity and relation classification (Dai et al., 2019) with inter-token attention. We employ it at the t"
2020.emnlp-main.431,W11-1808,0,0.0297104,"iggers in the corpora are annotated as E XPRESSION and T RANSCRIP TION types interchangeably. This is due to the fact a T RANSCRIPTION is a gene E XPRESSION. Regarding the identification of arguments, overpredictions are quite uncommon. If erroneous, the main error we found may benefit from syntactic information, which we aim to integrate in a multitask setup in future work. We found no misclassification of arguments in our document samples. Under-prediction of arguments are instead mostly due to under-predicted events. 7 Related Work Biomedical event extraction has a long-standing tradition (Riedel et al., 2011; Miwa et al., 2012; Vlachos and Craven, 2012; Venugopal et al., 2014; Majumder et al., 2016). Current work has explored neural methods and uses multiple classification stages. Namely, first identifying trigger mentions, and then evaluating all entity pairs (Li et al., 2019; Bj¨orne and Salakoski, 2018). They come with the shortcomings of traditional pipeline methods. Many studies use dependency parsers to obtain features or for guidance of Tree-LSTMs (Li et al., 2019; Bj¨orne and Salakoski, 2018). Recent work in syntactic parsing has shown that reducing parsing to sequence labeling is a viabl"
2020.emnlp-main.431,N19-1077,0,0.0461345,"Missing"
2020.emnlp-main.431,D14-1090,0,0.0206199,"TION types interchangeably. This is due to the fact a T RANSCRIPTION is a gene E XPRESSION. Regarding the identification of arguments, overpredictions are quite uncommon. If erroneous, the main error we found may benefit from syntactic information, which we aim to integrate in a multitask setup in future work. We found no misclassification of arguments in our document samples. Under-prediction of arguments are instead mostly due to under-predicted events. 7 Related Work Biomedical event extraction has a long-standing tradition (Riedel et al., 2011; Miwa et al., 2012; Vlachos and Craven, 2012; Venugopal et al., 2014; Majumder et al., 2016). Current work has explored neural methods and uses multiple classification stages. Namely, first identifying trigger mentions, and then evaluating all entity pairs (Li et al., 2019; Bj¨orne and Salakoski, 2018). They come with the shortcomings of traditional pipeline methods. Many studies use dependency parsers to obtain features or for guidance of Tree-LSTMs (Li et al., 2019; Bj¨orne and Salakoski, 2018). Recent work in syntactic parsing has shown that reducing parsing to sequence labeling is a viable alternative for both constituent and dependency parsing (Spoustov´a"
2020.emnlp-main.431,N19-1341,0,0.0191779,"space size. h+R EGULATION, T HEME, +R EG−1 i, h+R EGULATION, C AUSE, +R EG+1 i Single-task A single-task (ST) setup is used as a baseline. It predicts a single label yi = hd, r, hi for each input token xi . The label space is up to L = |D |× |R |× |H|. hT HEME, hC AUSE, +R EG−1 i +R EG+1 i h+R EGULATIONi Multi-task The label yi for each token xi is decomposed into parts (hereafter, sub-labels), each treated as a prediction task. The decomposition of the label space allows each sub-label space to be framed as a different task with its own private decoder, mitigating the output space sparsity (Vilares et al., 2019). Depending on the decomposition of the label yi = hd, r, hi, we have four multi-task learning options (pairs of tasks, or each subpart as a task, respectively) with the following properties: Softmax lj ∈ D lj ∈ R × H Softmax decoder Multi-label decoder BERT encoder [ENT] induced [ENT] activation showed 1. hdi, hr, hi: up to L = |D |+ |R |× |H|; to 2. hd, ri, hhi: up to L = |D |× |R |+ |H|; Figure 3: B EE SL uses a multi-task multi-label model, using a BERT encoder with layer attention, and dedicated decoders for predicting the labels for each label sub-space, which are trivially merged. hd, r"
2020.emnlp-main.431,W17-2315,0,0.128337,"Missing"
2020.lrec-1.244,W18-2311,0,0.0548287,"Missing"
2020.lrec-1.244,P07-1033,0,0.400922,"Missing"
2020.lrec-1.244,W11-1802,0,0.0963623,"Missing"
2020.lrec-1.244,N19-1145,0,0.075973,"rom the edges. For instance, consider the example in Figure 1. Firstly, “phosphorylation” and “augments” are identified as Phosphorylation and +Regulation triggers, respectively. Then, arguments for those event triggers are determined (e.g., Phosphorylation is the Cause of +Regulation). Lastly, arguments are composed into self-contained event structures: two Phosphorylation events and two +Regulation events. Recent studies on EE have shown that supervised machine learning approaches and in particular neural methods provide state-of-the-art performance on the task (Bj¨orne and Salakoski, 2018; Li et al., 2019). These methods require 1 Unlike traditional relation extraction, event representations can capture the association of one or more participants in different semantic roles, where each association in turn can be argument of higher-level associations. 1 trigger detection SLP-76 2 and Vav Tyr phosphorylation augments IL-2 activity IL-2 activity IL-2 activity edge detection Theme Cause Theme Theme Site SLP-76 3 and Vav Tyr eventconstruction unmerging event Theme Theme SLP-76 and Vav Site phosphorylation augments Cause Theme Cause Theme Site Tyr phosphorylation augments Figure 1: The classification"
2020.lrec-1.244,W19-5034,0,0.140578,"propose to focus on the most widely used edge types across all corpora (Section 3.2.3). 3.2.1. Preprocessing of Event Structures Each document in a corpus is accompanied by two annotation files, one for entities and one for both triggers and event structures. Since an edge is a subset of an event and its endpoints could be both triggers and entities, edges are implicitly encoded in both annotation files. We thus used these files in order to divide event structures into a set of intra-sentence edge examples.4 We use the scispaCy model with custom postprocessing rules for sentence segmentation (Neumann et al., 2019). Similarly to Miwa et al. (2013), we also handle name variations on the labels that refer to the same edge type,5 mapping them to their canonical type (e.g., {T heme, T heme2, T heme3} 7→ T heme). Due to both the differences in the topic of texts – thus, in the provided edge annotations – and the goal of a cross-domain study, we retain all the semantic edge types which are annotated in multiple corpora. These edge types are Theme, Cause, Site, CSite,6 AtLoc, ToLoc, and FromLoc. For the grouping of these edges refer to Section 3.2.3 while for the formal definition of the edge types refer to th"
2020.lrec-1.244,P15-2060,0,0.0729863,"uage varieties would be desirable to enable cross-domain generalizability. Although domain adaptation has received increasing importance in other fields, little and scattered work has been done so far in biomedical EE. Vlachos and Craven (2012) showed that a simple supervised domain adaptation approach (Daum´e, 2007) is beneficial in handling the differences between abstracts and full-texts, i.e., what we hereafter refer to as textual scope, in GE11. However, their work assumed labeled data is available in the target domain, and that the textual scope is the only source of language variation. Nguyen and Grishman (2015) conducted experiments in the newswire domain, showing that CNNs without any external features are more robust than other statistical approaches for the trigger detection stage. Miwa and Ananiadou (2015) integrated weighting and covariate shift into their EE system showing how these methods could improve recall at the cost of precision, while Miwa et al. (2013) proposed a multi-corpus learning approach combining semantic annotations shared across corpora, heuristically filtering corpus-specific annotation instances. Although these works are the closest to our goal, data and performance evaluat"
2020.lrec-1.244,W11-1803,0,0.183618,"the best of our knowledge, we are the first to provide such insights. We thus believe our work could encourage research efforts in domain adaptation in the near future, as well as in-depth evaluation of other stages. 2. Related Work In recent years, a number of Shared Tasks have been organized in order to promote the development of techniques for biomedical natural language processing, providing annotated corpora and evaluation means (Huang and Lu, 2016). Of particular interest for biomedical EE are the GE11 corpus (Kim et al., 2011), the ID11 corpus (Pyysalo et al., 2011), the EPI11 corpus (Ohta et al., 2011), the PC13 corpus (Ohta et al., 2013), and the MLEE corpus (Pyysalo et al., 2012). Several techniques have been employed to tackle the problem, ranging from rule-based to machine learning based systems (Vanegas et al., 2015). Recently, neural methods have shown to provide state-of-the-art performance on the task, using either Convolutional Neural Networks (CNNs) (Bj¨orne and Salakoski, 2018) or Long Short-Term Memory (LSTM) networks (Li et al., 2019). However, due to the lack of explicit data for evaluating edge detection, most work only report end-to-end performance of EE systems. Further, bi"
2020.lrec-1.244,W13-2009,0,0.0319688,"e first to provide such insights. We thus believe our work could encourage research efforts in domain adaptation in the near future, as well as in-depth evaluation of other stages. 2. Related Work In recent years, a number of Shared Tasks have been organized in order to promote the development of techniques for biomedical natural language processing, providing annotated corpora and evaluation means (Huang and Lu, 2016). Of particular interest for biomedical EE are the GE11 corpus (Kim et al., 2011), the ID11 corpus (Pyysalo et al., 2011), the EPI11 corpus (Ohta et al., 2011), the PC13 corpus (Ohta et al., 2013), and the MLEE corpus (Pyysalo et al., 2012). Several techniques have been employed to tackle the problem, ranging from rule-based to machine learning based systems (Vanegas et al., 2015). Recently, neural methods have shown to provide state-of-the-art performance on the task, using either Convolutional Neural Networks (CNNs) (Bj¨orne and Salakoski, 2018) or Long Short-Term Memory (LSTM) networks (Li et al., 2019). However, due to the lack of explicit data for evaluating edge detection, most work only report end-to-end performance of EE systems. Further, biomedical corpora comprise many textua"
2020.lrec-1.244,W11-1804,0,0.0729586,"Missing"
2020.lrec-1.244,C14-1220,0,0.0619732,"Missing"
2020.semeval-1.238,D19-1565,0,0.0350191,"Missing"
2020.semeval-1.238,2020.semeval-1.186,0,0.0800053,"Missing"
2020.semeval-1.238,N19-1423,0,0.0356106,"que classification (TC) sub-task at SemEval 2020 task 11 “Detection of propaganda techniques in news articles”.1 TC is a multi class classification problem in which a system needs to identify the propaganda techniques of a given span of an article. For instance, when given the span “stupid and petty” the system should classify it as Loaded Language. Our system is an ensemble model based on stacked generalization (Wolpert, 1992) which enables the incorporation of both traditional engineered features (Nalini and Sheela, 2014) and the Transformer (Vaswani et al., 2017) based language model BERT (Devlin et al., 2019). 2 Related work In addition to formulating the original problem of fine-grained propaganda identification and creating the corpus needed to solve the task, Da San Martino et al. (2019) also designed a multi-granularity neural network. This model outperformed several strong BERT baseline models in the high granularity fragment-level classification by using information from low granularity classification (e.g. document-level) to drive higher-granularity classification (e.g. paragraph-level). As the TC sub-task of this competition does not require span detection, a multi-granularity approach is"
2020.semeval-1.238,D17-1317,0,0.0360604,"from BERT with simple span-based and article-based global features. We present an ablation study which shows that even though BERT representations are very powerful also for this task, BERT still benefits from being combined with carefully designed task-specific features. 1 Introduction The purpose of propaganda is to use communication to foster predetermined agendas, or to achieve a response that furthers a desired outcome (Jowett and O’Donnell, 2018). Prior research has focused on creating machine learning models that label whole news articles or even entire news outlets as propagandistic (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019). To increase the granularity of these coarse models, a new data set was developed in a study by Da San Martino et al (2019), which enabled models to jointly identify fragments of propaganda within a document, while also classifying their respective propaganda techniques (Da San Martino et al., 2019; Yu et al., 2019). This paper presents our solution (DiSaster, finishing at 11th place) to the technique classification (TC) sub-task at SemEval 2020 task 11 “Detection of propaganda techniques in news articles”.1 TC is a multi class classification problem in which a"
2020.semeval-1.238,D18-1310,0,0.0276599,"g), HCF (hand-crafted features), wrf (word resemble factor), aowc, (article one word counter), assc (article span sentence counter), swl (span word length), wcss (word count span sent). 0.621 (Table 2). This is also supported by our data analysis (Section 3, Table 1) which shows that the Avg one word counter are much higher for Repetition compared to the other labels. The fact that we obtained better quality predictions by augmenting BERT-predictions with additional information about the text shows that feature engineering is still a relevant discipline as other recent research also suggests (Wu et al., 2018; Zhang and Li, 2019). The augmented BERT approach worked well on both the training set and the development set, but our score dropped significantly when predicting on the test set (0.628 dev set micro F1 → 0.566 test set micro F1). As we do not have access to the test set labels, a detailed error analysis is difficult for now and left for future work. However, by comparing the F1 score from the official test set with the cross validation scores in Table 2, we do see a particularly large drop in F1 for the Repetition category (from 0.646 cross validation → 0.204 test). This drop in Repetition"
2020.wnut-1.44,N19-1423,0,0.159344,"from the training data. The introduction of neural networks has led to an increase in performance for many natural language processing tasks (Manning, 2015). However, previous work on classification showed that SVMs with character and/or word n-grams often still outperform neural networks (Zampieri et al., 2017; Medvedeva et al., 2017; C¸o¨ ltekin and Rama, 2018; Basile et al., 2018). Neural network approaches can elegantly exploit raw data, by pre-training word embeddings using a language modeling objective. Recently, more powerful contextual embeddings were introduced (Peters et al., 2018; Devlin et al., 2019), which base each word embedding on its context. These contextual embeddings are generally pre-trained on huge amounts of raw data, and then fine-tuned on the target task. This leads to the question: How do the three types of classification models viz. SVM, neural models with pre-trained embeddings and various contextual models compare and perform in this classification task? (RQ1) Neural networks as well as transformer-based models can directly exploit additional raw data by 331 Proceedings of the 2020 EMNLP Workshop W-NUT: The Sixth Workshop on Noisy User-generated Text, pages 331–336 c Onli"
2020.wnut-1.44,2020.acl-main.740,0,0.0390507,"Missing"
2020.wnut-1.44,2021.ccl-1.108,0,0.0875078,"Missing"
2020.wnut-1.44,W17-1219,1,0.831455,"source code is available on: https://github.com/ AGMoller/noisy_text/ 2 https://www.worldometers.info/ coronavirus/ shared task, informative tweets were defined as to contain information about COVID-19 cases such as statistics, locations or travel history. Figure 1 shows two examples from the training data. The introduction of neural networks has led to an increase in performance for many natural language processing tasks (Manning, 2015). However, previous work on classification showed that SVMs with character and/or word n-grams often still outperform neural networks (Zampieri et al., 2017; Medvedeva et al., 2017; C¸o¨ ltekin and Rama, 2018; Basile et al., 2018). Neural network approaches can elegantly exploit raw data, by pre-training word embeddings using a language modeling objective. Recently, more powerful contextual embeddings were introduced (Peters et al., 2018; Devlin et al., 2019), which base each word embedding on its context. These contextual embeddings are generally pre-trained on huge amounts of raw data, and then fine-tuned on the target task. This leads to the question: How do the three types of classification models viz. SVM, neural models with pre-trained embeddings and various conte"
2020.wnut-1.44,D14-1162,0,0.0895765,"Missing"
2020.wnut-1.44,N18-1202,0,0.0366105,"1 shows two examples from the training data. The introduction of neural networks has led to an increase in performance for many natural language processing tasks (Manning, 2015). However, previous work on classification showed that SVMs with character and/or word n-grams often still outperform neural networks (Zampieri et al., 2017; Medvedeva et al., 2017; C¸o¨ ltekin and Rama, 2018; Basile et al., 2018). Neural network approaches can elegantly exploit raw data, by pre-training word embeddings using a language modeling objective. Recently, more powerful contextual embeddings were introduced (Peters et al., 2018; Devlin et al., 2019), which base each word embedding on its context. These contextual embeddings are generally pre-trained on huge amounts of raw data, and then fine-tuned on the target task. This leads to the question: How do the three types of classification models viz. SVM, neural models with pre-trained embeddings and various contextual models compare and perform in this classification task? (RQ1) Neural networks as well as transformer-based models can directly exploit additional raw data by 331 Proceedings of the 2020 EMNLP Workshop W-NUT: The Sixth Workshop on Noisy User-generated Text"
2020.wnut-1.44,W17-1201,0,0.0606178,"Missing"
2021.adaptnlp-1.19,D19-6118,0,0.0116664,"wever, it remains unclear in which situations these dataset embeddings thrive best. Furthermore, two often overseen issues with dataset embeddings are that they are commonly learned from the gold data-source labels attached to each training and test instance and it is assumed that the test data is from a distribution which is seen during training. In many real world situations these assumptions are clearly violated. A common strategy when the test data is drawn from a different distribution as the training datasets (zero-shot), is to use a manually assigned proxy treebank (Smith et al., 2018; Barry et al., 2019; Meechan-Maddon and Nivre, 2019). Recent work showed that for unseen datasets in mono-lingual setups (Wagner et al., 2020), interpolated dataset embeddings can be used to improve performance for zero-shot settings. We use automatically predicted proxy data sources instead, and focus on mono-linugal as well as cross-lingual setups. In this paper, we provide an extensive evaluation of the usefulness of dataset embeddings in existing 2 More recently, (Conneau and Lample, 2019) showed that embedding the language can also be beneficial for training contextualized embeddings with masked language mo"
2021.adaptnlp-1.19,E17-2052,0,0.0192738,"i et al., 2017; Medvedeva et al., 2017; C¸o¨ ltekin and Rama, 2018; Basile et al., 2018). We performed a grid search with n ∈ [1−7] and all sequential combinations (1-2, 1-3, etc.) for n-grams. For this hyper-parameter tuning, we used ¨ un et al. (2019), and the eight datasets from Ust¨ found the most robust parameters to be 1-2 for words and 1-5 for characters. The obtained macro 184 3¨ Ust¨un et al. (2019) showed that performance gains from external embeddings are highly complementary to performance gains from dataset embeddings. 4 The full groups can be seen in Appendix C and D. 5 However, Bhat et al. (2017) and Ravishankar (2018) have shown the usefulness of word-level language labels for processing code-switched data. Filtering Morphological Tagging (F1) #src base concat gold pred All 104 92.04 91.43 92.75 91.85 91.10 91.02 92.55 91.41 58 72.92 74.07 75.53 74.52 59 94.14 93.94 95.84 94.13 93.66 93.83 95.73 93.84 45 89.30 88.14 88.69 88.88 87.75 87.33 88.38 88.22 10 80.48 79.84 82.74 80.29 48 71.35 72.87 74.03 73.32 Single-lang Multi-lang Lemmatization (Accuracy) base concat gold pred Dependency Parsing (LAS) #src base concat gold pred Table 1: Results per task: overall average, and monolingual"
2021.adaptnlp-1.19,R13-1046,0,0.0350566,"Missing"
2021.adaptnlp-1.19,L18-1293,0,0.0325632,"Missing"
2021.adaptnlp-1.19,P11-1068,0,0.0608535,"Missing"
2021.adaptnlp-1.19,W17-6314,0,0.0200542,"Missing"
2021.adaptnlp-1.19,P12-1066,0,0.0340474,". Especially data annotated for the same task from other sources can be beneficial to exploit. However, because of heterogeneity in language or domain this might lead to sub-optimal performance. In early work on combining training sources, data was selected at training time (Plank ∗ Equal contributions source code is available at: https://bitbucket. org/robvanderg/dataembs/src 1 Barbara Plank IT University of Copenhagen bapl@itu.dk and van Noord, 2011; Khan et al., 2013) for a given test set. A more nuanced way to exploit heterogeneous data is to encode properties of the language as features (Naseem et al., 2012). Recently, Ammar et al. (2016) showed that encoding the language of an instance as an embedding in a neural model is beneficial for multi-lingual learning.2 Follow-up work found that multiple datasets within the same language can also be combined by encoding their origin (Stymne et al., 2018; ¨ un et al., 2019), thereby implicitly learning useUst¨ ful commonalities, while still encoding datasetspecific knowledge. These dataset embeddings are employed in groups of datasets which usually range in size from 2 to 10 datasets. However, it remains unclear in which situations these dataset embedding"
2021.adaptnlp-1.19,2020.lrec-1.497,0,0.0573975,"Missing"
2021.adaptnlp-1.19,P11-1157,1,0.804958,"Missing"
2021.adaptnlp-1.19,K18-2011,0,0.0266402,"Missing"
2021.adaptnlp-1.19,P18-2098,0,0.0470847,"Missing"
2021.adaptnlp-1.19,W19-4206,1,0.893414,"Missing"
2021.adaptnlp-1.19,2020.acl-main.778,0,0.0148955,"s with dataset embeddings are that they are commonly learned from the gold data-source labels attached to each training and test instance and it is assumed that the test data is from a distribution which is seen during training. In many real world situations these assumptions are clearly violated. A common strategy when the test data is drawn from a different distribution as the training datasets (zero-shot), is to use a manually assigned proxy treebank (Smith et al., 2018; Barry et al., 2019; Meechan-Maddon and Nivre, 2019). Recent work showed that for unseen datasets in mono-lingual setups (Wagner et al., 2020), interpolated dataset embeddings can be used to improve performance for zero-shot settings. We use automatically predicted proxy data sources instead, and focus on mono-linugal as well as cross-lingual setups. In this paper, we provide an extensive evaluation of the usefulness of dataset embeddings in existing 2 More recently, (Conneau and Lample, 2019) showed that embedding the language can also be beneficial for training contextualized embeddings with masked language modeling. 183 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 183–194 April 20, 2021. ©2021 Associatio"
2021.adaptnlp-1.19,W17-1201,0,0.0595627,"Missing"
2021.adaptnlp-1.19,K18-2001,0,0.0299399,"Missing"
2021.bppf-1.3,N13-1062,0,0.0223759,"it fundamentally hides the true nature of the task we are trying to solve. However, there is plenty of evidence that gold labels are an idealization, and that unreconcilable disagreement is abundant. Figure 1 shows two examples from CV and NLP. This is particularly true for tasks involving highly subjective judgments, such as hate speech detection (Akhtar et al., 2019, 2020) or sentiment analysis (Kenyon-Dean et al., 2018). However, it is not a trivial issue even in more linguistic tasks, such as part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012; Jurgens, 2013), or coreference resolution (Poesio and Artstein, 2005; Recasens et al., 2011). Systematic disagreement also exists in image classification tasks, where labels may overlap (Rodrigues and Pereira, 2018; Peterson et al., 2019). Disagreement and task difficulty and subjectivity also challenge traditional agreement measures (Artstein and Poesio, 2008). High agreement is typically used as a proxy for data quality. However, it obscures possible sources of disagreement (Poesio and Artstein, 2005). We summarize some of the evidence on disagreement in Section 2. The need for metrics not based on the as"
2021.bppf-1.3,2021.naacl-main.204,1,0.779829,"s disagree. There is no consensus yet on this form of evaluation, but a few proposals have been used already. In fact, a way of performing soft evaluation exists which is a natural extension of current practice in NLP. This is to evaluate ambiguity-aware models by treating the probability distribution of labels they produce as a soft label, and comparing that to a full distribution of labels, instead of a ‘one-hot’ approach. This can be done using, for example, cross-entropy, although other options also exist. This approach was adopted in, inter alia, (Peterson et al., 2019; Uma et al., 2020; Fornaciari et al., 2021). Peterson et al. (2019) tested this approach on image classification tasks, generating the soft label by transforming the item annotation distribution using standard normalization. Uma et al. (2020) employed this form of soft metric evaluation for NLP , also comparing different ways to obtain a soft label from the raw data. They use soft metrics to compare the classifiers’ distribution to the human-derived label distributions, complementing traditional hard evaluation measures. Basile (2020) suggested a more extreme evaluation framework, where a model is required to produce different outputs"
2021.bppf-1.3,N18-1171,0,0.0210479,"unnecessary: while evaluation methods that include disagreement are not yet established, several methodologies already do exist. Removing the disagreement might lead to better evaluation scores, but it fundamentally hides the true nature of the task we are trying to solve. However, there is plenty of evidence that gold labels are an idealization, and that unreconcilable disagreement is abundant. Figure 1 shows two examples from CV and NLP. This is particularly true for tasks involving highly subjective judgments, such as hate speech detection (Akhtar et al., 2019, 2020) or sentiment analysis (Kenyon-Dean et al., 2018). However, it is not a trivial issue even in more linguistic tasks, such as part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012; Jurgens, 2013), or coreference resolution (Poesio and Artstein, 2005; Recasens et al., 2011). Systematic disagreement also exists in image classification tasks, where labels may overlap (Rodrigues and Pereira, 2018; Peterson et al., 2019). Disagreement and task difficulty and subjectivity also challenge traditional agreement measures (Artstein and Poesio, 2008). High agreement is typically used as a proxy for data quality."
2021.bppf-1.3,P11-2008,0,0.163137,"Missing"
2021.bppf-1.3,W04-1013,0,0.0362296,"nd, therefore, the disagreement levels. Such individual differences can be partially explained by cultural and socio-demographic norms and variables, such as age, gender, instruction level, or cultural background. However, none of them is sufficient to capture the uniqueness of each subject and their evaluations. 2.2 Disagreement in ‘Objective’ Tasks The NLP community has long been aware that it makes no sense to evaluate natural language generation applications against a hypothetical ‘gold’ output. These areas have developed specialized training and evaluation methods (Papineni et al., 2002; Lin, 2004). More surprisingly, disagreements in interpretation have been found to be frequent in annotation projects concerned with apparently more ‘objective’ aspects of language, such as coreference (Poesio and Artstein, 2005; Recasens et al., 2011), part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012) and semantic role labelling (Dumitrache et al., 2019), to name a few examples. Even if in these tasks individual instances can be found to be reasonably objective, these findings appear to reflect the existence of extensive and systematic disagreement on what"
2021.bppf-1.3,P02-1040,0,0.11206,"he annotation outcome and, therefore, the disagreement levels. Such individual differences can be partially explained by cultural and socio-demographic norms and variables, such as age, gender, instruction level, or cultural background. However, none of them is sufficient to capture the uniqueness of each subject and their evaluations. 2.2 Disagreement in ‘Objective’ Tasks The NLP community has long been aware that it makes no sense to evaluate natural language generation applications against a hypothetical ‘gold’ output. These areas have developed specialized training and evaluation methods (Papineni et al., 2002; Lin, 2004). More surprisingly, disagreements in interpretation have been found to be frequent in annotation projects concerned with apparently more ‘objective’ aspects of language, such as coreference (Poesio and Artstein, 2005; Recasens et al., 2011), part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012) and semantic role labelling (Dumitrache et al., 2019), to name a few examples. Even if in these tasks individual instances can be found to be reasonably objective, these findings appear to reflect the existence of extensive and systematic disagreem"
2021.bppf-1.3,S13-2025,0,0.0191364,"based on the assumption that a gold standard exists has long been accepted for end-to-end tasks, particularly those involving an aspect of natural language generation, such as conversational agents, machine translation, surface realisation, image captioning, or summarization. Metrics such as BLEU for machine translation/generation, ROUGE for summarization, or NDCG for ranking Web searches all support more than one gold standard reference. Shared tasks in this areas (particularly on paraphrasing), have also considered the role of disagreement in their evaluation metrics (Butnariu et al., 2009; Hendrickx et al., 2013). Variability in the annotation is a feature of 2 Disagreement in NLP In this section, we outline three possible sources of disagreement. Afterward, we describe how disagreement has been studied in objective and arguably more subjective tasks in NLP. 2.1 Sources of Disagreement Annotation implies an interaction between the human judge, the instance which has to be evaluated, and the moment/context in which the process takes place. For each instance, the annotation outcome 16 depends on these three elements, assuming the task is properly defined, designed, and carried out, e.g., in terms of qua"
2021.bppf-1.3,W13-2323,0,0.0253399,"arking: Past, Present and Future, pages 15–21 August 5–6, 2021. ©2021 Association for Computational Linguistics best possible approximation of the truth about a given phenomenon, or at least a reasonable one. This ground truth is usually obtained by developing an annotation scheme for the task aiming to achieve the highest possible agreement between human annotators (Artstein and Poesio, 2008). Disagreements between annotators are either reconciled by hand or aggregated (particularly in the case of crowdsourced annotations) to extract the most likely or agreed-upon choices (Hovy et al., 2013; Passonneau and Carpenter, 2013; Paun et al., 2018). This aggregated data is referred to as “gold standard” (see Ide and Pustejovsky (2017) for an in-depth analysis of annotation methodology). many such tasks (see, e.g., van der Lee et al. (2019) for agreement issues in generated text evaluation) even though many corpora still may come with single references due to data collection costs. High agreement is disfavored, and even bears risks of non-natural, highly homogenized system outputs for generation tasks (Amidei et al., 2018). The main argument of this position paper is that we should recognize that the same issues, if p"
2021.bppf-1.3,N13-1132,1,0.771702,"Workshop on Benchmarking: Past, Present and Future, pages 15–21 August 5–6, 2021. ©2021 Association for Computational Linguistics best possible approximation of the truth about a given phenomenon, or at least a reasonable one. This ground truth is usually obtained by developing an annotation scheme for the task aiming to achieve the highest possible agreement between human annotators (Artstein and Poesio, 2008). Disagreements between annotators are either reconciled by hand or aggregated (particularly in the case of crowdsourced annotations) to extract the most likely or agreed-upon choices (Hovy et al., 2013; Passonneau and Carpenter, 2013; Paun et al., 2018). This aggregated data is referred to as “gold standard” (see Ide and Pustejovsky (2017) for an in-depth analysis of annotation methodology). many such tasks (see, e.g., van der Lee et al. (2019) for agreement issues in generated text evaluation) even though many corpora still may come with single references due to data collection costs. High agreement is disfavored, and even bears risks of non-natural, highly homogenized system outputs for generation tasks (Amidei et al., 2018). The main argument of this position paper is that we should reco"
2021.bppf-1.3,Q18-1040,1,0.826112,", pages 15–21 August 5–6, 2021. ©2021 Association for Computational Linguistics best possible approximation of the truth about a given phenomenon, or at least a reasonable one. This ground truth is usually obtained by developing an annotation scheme for the task aiming to achieve the highest possible agreement between human annotators (Artstein and Poesio, 2008). Disagreements between annotators are either reconciled by hand or aggregated (particularly in the case of crowdsourced annotations) to extract the most likely or agreed-upon choices (Hovy et al., 2013; Passonneau and Carpenter, 2013; Paun et al., 2018). This aggregated data is referred to as “gold standard” (see Ide and Pustejovsky (2017) for an in-depth analysis of annotation methodology). many such tasks (see, e.g., van der Lee et al. (2019) for agreement issues in generated text evaluation) even though many corpora still may come with single references due to data collection costs. High agreement is disfavored, and even bears risks of non-natural, highly homogenized system outputs for generation tasks (Amidei et al., 2018). The main argument of this position paper is that we should recognize that the same issues, if perhaps in less extre"
2021.bppf-1.3,Q19-1043,0,0.0234722,"have been found to be frequent in annotation projects concerned with apparently more ‘objective’ aspects of language, such as coreference (Poesio and Artstein, 2005; Recasens et al., 2011), part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012) and semantic role labelling (Dumitrache et al., 2019), to name a few examples. Even if in these tasks individual instances can be found to be reasonably objective, these findings appear to reflect the existence of extensive and systematic disagreement on what can be concluded from a natural language statement (Pavlick and Kwiatkowski, 2019). Stimulus Characteristics. Instance characteristics have paramount importance for the annotation as well. Language meaning is often equivocal and carries ambiguities of several kinds: lexical, syntactical, semantic, and others. Humour, for example, often relies on lexical or syntactic ambiguity (Raskin, 1985; Poesio, 2020). Other genres using deliberate ambiguity as a rhetorical device include poetry (Su, 1994) or political discourse (Winkler, 2015). For some instances, more than one label is correct, and the relative annotation task would be better framed as multi-label multi-class, rather t"
2021.bppf-1.3,D15-1035,0,0.0223806,"though many corpora still may come with single references due to data collection costs. High agreement is disfavored, and even bears risks of non-natural, highly homogenized system outputs for generation tasks (Amidei et al., 2018). The main argument of this position paper is that we should recognize that the same issues, if perhaps in less extreme version, apply to the analysis tasks we discuss here. In recent years, proposals have been put forward to consider the disagreement as informative content that can be leveraged to improve task performance (Plank et al., 2014; Aroyo and Welty, 2015; Jamison and Gurevych, 2015). Uma et al. (2020) and Basile (2020) investigated the impact of disagreement-informed data on the quality of NLP evaluation, and found it to be beneficial and providing complementary information, as further discussed in Section 3. This led them to organize a first shared task on learning from disagreement and providing non-aggregated benchmarks for evaluation (Uma et al., 2021). In contrast with this trend, Bowman and Dahl (2021) recently proposed to study biases and artifacts in data to eliminate them. Beigman Klebanov and Beigman (2009) adopt a slightly softer stance, proposing to only eval"
2021.bppf-1.3,P14-2083,1,0.879095,"issues in generated text evaluation) even though many corpora still may come with single references due to data collection costs. High agreement is disfavored, and even bears risks of non-natural, highly homogenized system outputs for generation tasks (Amidei et al., 2018). The main argument of this position paper is that we should recognize that the same issues, if perhaps in less extreme version, apply to the analysis tasks we discuss here. In recent years, proposals have been put forward to consider the disagreement as informative content that can be leveraged to improve task performance (Plank et al., 2014; Aroyo and Welty, 2015; Jamison and Gurevych, 2015). Uma et al. (2020) and Basile (2020) investigated the impact of disagreement-informed data on the quality of NLP evaluation, and found it to be beneficial and providing complementary information, as further discussed in Section 3. This led them to organize a first shared task on learning from disagreement and providing non-aggregated benchmarks for evaluation (Uma et al., 2021). In contrast with this trend, Bowman and Dahl (2021) recently proposed to study biases and artifacts in data to eliminate them. Beigman Klebanov and Beigman (2009) ad"
2021.bppf-1.3,W05-0311,1,0.768942,"the task we are trying to solve. However, there is plenty of evidence that gold labels are an idealization, and that unreconcilable disagreement is abundant. Figure 1 shows two examples from CV and NLP. This is particularly true for tasks involving highly subjective judgments, such as hate speech detection (Akhtar et al., 2019, 2020) or sentiment analysis (Kenyon-Dean et al., 2018). However, it is not a trivial issue even in more linguistic tasks, such as part-of-speech tagging (Plank et al., 2014), word sense disambiguation (Passonneau et al., 2012; Jurgens, 2013), or coreference resolution (Poesio and Artstein, 2005; Recasens et al., 2011). Systematic disagreement also exists in image classification tasks, where labels may overlap (Rodrigues and Pereira, 2018; Peterson et al., 2019). Disagreement and task difficulty and subjectivity also challenge traditional agreement measures (Artstein and Poesio, 2008). High agreement is typically used as a proxy for data quality. However, it obscures possible sources of disagreement (Poesio and Artstein, 2005). We summarize some of the evidence on disagreement in Section 2. The need for metrics not based on the assumption that a gold standard exists has long been acc"
2021.bppf-1.3,P19-1572,0,0.0275709,"arning from disagreements in NLP and CV using datasets containing information about disagreements for interpreting language and classifying images. Five well-known datasets for very different NLP and CV tasks were identified, all characterized by a multiplicity of labels for each instance, by having a size sufficient to train state-of-the-art models, and by evincing different characteristics in terms of the crowd annotators and data collection procedure. These include: a dataset of Twitter posts annotated with POS tags collected by Gimpel et al. (2011), a datasets for humour identification by Simpson et al. (2019), and two CV datasets on object identification namely the LabelMe (Russell et al., 2008) and CIFAR -10 datasets (Peterson et al., 2019). Both hard evaluation metrics (F1) and soft evaluation metrics (cross-entropy, as discussed in Section 3) were used for evaluation (Uma et al., 2021). The results showed that in nearly all cases, models that account for noise and disagreement have the best (lowest) cross-entropy scores. These results are consistent with the findings of Uma et al. (2020) and Peterson et al. (2019). Evaluation in Light of Disagreement While the research mentioned in the previous"
2021.bppf-1.3,2021.semeval-1.41,1,0.865681,"discuss here. In recent years, proposals have been put forward to consider the disagreement as informative content that can be leveraged to improve task performance (Plank et al., 2014; Aroyo and Welty, 2015; Jamison and Gurevych, 2015). Uma et al. (2020) and Basile (2020) investigated the impact of disagreement-informed data on the quality of NLP evaluation, and found it to be beneficial and providing complementary information, as further discussed in Section 3. This led them to organize a first shared task on learning from disagreement and providing non-aggregated benchmarks for evaluation (Uma et al., 2021). In contrast with this trend, Bowman and Dahl (2021) recently proposed to study biases and artifacts in data to eliminate them. Beigman Klebanov and Beigman (2009) adopt a slightly softer stance, proposing to only evaluating on “easy” (as in, highly agreed upon) instances. Based on the evidence about the prevalence of disagreement in NLP judgments, we argue against this approach. First, it leads to information loss in the attempt to reducing noise in the data. Second, it is unnecessary: while evaluation methods that include disagreement are not yet established, several methodologies already d"
2021.bppf-1.3,W19-8643,0,0.0631318,"Missing"
2021.codi-main.1,Q18-1041,0,0.0306547,"Missing"
2021.codi-main.1,N19-1300,0,0.134119,"y annotate the question-answer pairs in house (by three of the authors of this paper, all highly proficient in English). Examples from our dataset are provided in Table 1. A data statement is provided in Appendix A. Introduction Humans are very good at interpreting indirect answers to polar questions. In conversations, even if direct answers are possible, humans often prefer indirect answers due to cooperativeness and to advance the dialogue (Stenström, 1984). For dialogue systems and Natural Language Processing (NLP) more generally, however, interpreting indirect answers remains a challenge (Clark et al., 2019). Recent seminal work introduced C IRCA, a new large-scale dataset containing pairs of polar questions and indirect answers in English (Louis et al., 2020). This allows for data-driven experiments in this question-answering domain. ♥ The authors contributed equally to this work. 1 Proceedings of the 2nd Workshop on Computational Approaches to Discourse, pages 1–11 November 10–11, 2021. ©2021 Association for Computational Linguistics Contributions In this paper, we a) introduce a new dataset, F RIENDS -QIA, with 5,930 polar question-indirect answer pairs in English;1 b) study the effectiveness"
2021.codi-main.1,W09-3920,0,0.105147,"Missing"
2021.codi-main.1,N19-1423,0,0.0198381,"lel. As shown in Figure 2, our implementation of the parallel convolutions consists of a convolutional 1d, a max pooling, a flatten and a dropout layer, after which they are concatenated and fed to the final output layer which uses softmax as activation function. The convolutional layer applies the ReLU activation function after which the maximum of each filter is selected. The dropout layer applies a dropout rate of 0.5 which is the same rate we use on the final layer when we apply regularization with a constraint of the L2-norm on the weights. The models either take English BERT embeddings (Devlin et al., 2019) as input or use GloVe embeddings (Pennington et al., 2014). The models are Crowd Layer Figure 3 illustrates the key idea of the DL F C approach we adopt on top of our CNN. Training the models with a crowd layer is closely based on the paper and code by Rodrigues and Pereira (2018). Following their implementation, our crowd layer is applied on top of the existing network. It takes as input the output of the dense layer and uses the annotator-specific labels, each modeled as a separate task, to propagate errors through the network and adjust the gradients. The layer is applied on the already tr"
2021.codi-main.1,P92-1009,0,0.422064,"earning. There is emerging interest in learning from disagreement, and we refer the reader to a recent survey on this topic (Uma et al., 2021). Related Work The task of understanding indirect answers in Natural Language Processing is relatively new and has not been attempted by many yet. To enable progress on the task, Louis et al. (2020) created and released the first large-scale English language corpus, C IRCA, consisting of 34,268 polar questions and their corresponding indirect answers. The difficulty of interpreting indirect answers is, however, widely studied in some early papers, (e.g. Green and Carberry, 1992, 1999). Following this work, de Marneffe et al. (2009) propose a logical inference model with probabilistic methods. They realize the influence of discourse conditions as well as the difference between the literal meaning of the answer and the interpretation by the two speakers. Hockey et al. (1997) further underline the complexity of interpreting indirect answers to polar questions. They explore the existing Edinburgh map task corpus (Thompson et al., 1993) which consists of two-person dialogues already coded for dialogue structure. Clark et al. (2019) recently also explore the understanding"
2021.codi-main.1,J99-3004,0,0.582169,"Missing"
2021.codi-main.1,H93-1005,0,0.514201,"Missing"
2021.codi-main.1,D14-1181,0,0.00332963,"t. We performed a grid search on the base CNN to find the optimal hyperparameters. For the CNN with BERT no further hyperparameter tuning was done. 7,499 38 14 0.02 Table 5: Vocabulary statistics (in number of tokens). 4 Experiments We experiment with different variants of a Convolutional Neural Network (CNN). In particular, we explore the use of BERT embeddings, a crowd layer and combining datasets to obtain higher performance on F RIENDS -QIA. Figure 3: Illustration of deep learning from crowds proposed by Rodrigues and Pereira (2018). Base CNN Our CNNs are implemented with inspiration from Kim (2014), who uses 1dimensional convolutions in parallel. As shown in Figure 2, our implementation of the parallel convolutions consists of a convolutional 1d, a max pooling, a flatten and a dropout layer, after which they are concatenated and fed to the final output layer which uses softmax as activation function. The convolutional layer applies the ReLU activation function after which the maximum of each filter is selected. The dropout layer applies a dropout rate of 0.5 which is the same rate we use on the final layer when we apply regularization with a constraint of the L2-norm on the weights. The"
2021.codi-main.1,2020.emnlp-main.601,0,0.235059,"esearch, we propose a new dataset for studying indirect answers. We provide both aggregated (ground truth/gold) annotations and the raw annotations. This allows us to study the effect of learning to integrate human disagreement into understanding indirect answers. Our dataset, called F RIENDS -QIA, was created by collecting question and answer pairs from transcripts of a popular TV series. The data collection differs in comparison to the recently introduced C IRCA corpus. In their study, a set of 10 dialogue prompts were defined, and both questions and answers were collected by crowdsourcing (Louis et al., 2020). Following this step, the annotation was crowdsourced again, resulting in a set of labels which was later conflated into a relaxed set of six classes. Instead, we opted to collect the data from transcripts and manually annotate the question-answer pairs in house (by three of the authors of this paper, all highly proficient in English). Examples from our dataset are provided in Table 1. A data statement is provided in Appendix A. Introduction Humans are very good at interpreting indirect answers to polar questions. In conversations, even if direct answers are possible, humans often prefer indi"
2021.codi-main.1,Q19-1043,0,0.0208745,"rs in English;1 b) study the effectiveness of neural classifiers based on Convolutional Neural Networks, both with traditional pre-trained word embeddings and contextualized BERT embeddings; c) provide results on crossdataset evaluation, for which we train a model on both C IRCA and F RIENDS -QIA; and d) show that modeling human disagreement via deep learning from crowds is beneficial for this task. 2 requires collecting labels from multiple annotators. A unanimous gold standard label cannot always be clearly achieved (e.g. Aroyo and Welty, 2015; Rodrigues et al., 2013; Palomaki et al., 2018; Pavlick and Kwiatkowski, 2019). The problem of learning from multiple annotators has become more important and several attempts have been made to deal with biases present in such data. Yan et al. (2014) look into different levels of expertise among annotators and how a model can learn from this, taking into consideration the biases present while labeling the dataset. By measuring the inter-annotator agreement and incorporating the annotator uncertainty in model training, Plank et al. (2014) show that modelling annotator disagreement can be useful even in cases of seemingly more objective annotation tasks, like part-of-spee"
2021.codi-main.1,D14-1162,0,0.085696,"rallel convolutions consists of a convolutional 1d, a max pooling, a flatten and a dropout layer, after which they are concatenated and fed to the final output layer which uses softmax as activation function. The convolutional layer applies the ReLU activation function after which the maximum of each filter is selected. The dropout layer applies a dropout rate of 0.5 which is the same rate we use on the final layer when we apply regularization with a constraint of the L2-norm on the weights. The models either take English BERT embeddings (Devlin et al., 2019) as input or use GloVe embeddings (Pennington et al., 2014). The models are Crowd Layer Figure 3 illustrates the key idea of the DL F C approach we adopt on top of our CNN. Training the models with a crowd layer is closely based on the paper and code by Rodrigues and Pereira (2018). Following their implementation, our crowd layer is applied on top of the existing network. It takes as input the output of the dense layer and uses the annotator-specific labels, each modeled as a separate task, to propagate errors through the network and adjust the gradients. The layer is applied on the already trained and saved base model with an annotator-specific weigh"
2021.codi-main.1,E14-1078,1,0.904222,"human disagreement. 1 Q: Are you back from Minsk? A: Well, just for a couple of days. L: Y ES, N O, Y ES , SUBJECT TO SOME CONDITIONS Table 1: Examples from the dataset with polar question (Q), indirect answer (A) and annotator labels (L). Understanding indirect answers is a pragmatic problem, and even though humans typically have little difficulty in interpreting indirect answers, they may not all agree on a possible interpretation. Work on learning from human disagreement has shown that incorporating disagreement from human annotation is not only noise, but can provide valuable information (Plank et al., 2014; Aroyo and Welty, 2015; Rodrigues and Pereira, 2018). Motivated by these two lines of research, we propose a new dataset for studying indirect answers. We provide both aggregated (ground truth/gold) annotations and the raw annotations. This allows us to study the effect of learning to integrate human disagreement into understanding indirect answers. Our dataset, called F RIENDS -QIA, was created by collecting question and answer pairs from transcripts of a popular TV series. The data collection differs in comparison to the recently introduced C IRCA corpus. In their study, a set of 10 dialogu"
2021.crac-1.7,D19-1588,0,0.0170539,"s MUC, B 3 , and CEAFe. The best result per column is boldfaced. Coreference models We provide benchmark scores for two strong neural models using three different transformer representations. Lee et al. (2017) (L EE 2017) presented the first neural endto-end coreference model in which the spans are learned in the same training pass as the pairwise clustering. The pairwise clustering may produce globally inconsistent clusters, and Lee et al. (2018) (L EE 2018) presented a higher-order upgrade that did not only consider pairs of spans but a matrix of all spans to counter global inconsistencies. Joshi et al. (2019) showed that the model from Lee et al. (2018) performed even better on English data when using transformer-based models instead of the word embeddings of the original implementations. We follow their approach and try representations from three different pre-trained models: Danish BERT (DA BERT)4 and two multilingual, cased models: multilingual BERT (M BERT) (Devlin et al., 2019) and the base model of XLM-Roberta (XLM-R) (Conneau et al., 2020). Instead of the TensorFlow implementations released by Joshi et al. (2019), we use the PyTorch-based implementation from AllenNLP 1.3.0.5 with PyTorch ve"
2021.crac-1.7,2020.acl-main.747,0,0.0234922,"(2018) (L EE 2018) presented a higher-order upgrade that did not only consider pairs of spans but a matrix of all spans to counter global inconsistencies. Joshi et al. (2019) showed that the model from Lee et al. (2018) performed even better on English data when using transformer-based models instead of the word embeddings of the original implementations. We follow their approach and try representations from three different pre-trained models: Danish BERT (DA BERT)4 and two multilingual, cased models: multilingual BERT (M BERT) (Devlin et al., 2019) and the base model of XLM-Roberta (XLM-R) (Conneau et al., 2020). Instead of the TensorFlow implementations released by Joshi et al. (2019), we use the PyTorch-based implementation from AllenNLP 1.3.0.5 with PyTorch version 1.7.1. A description of the tuning process is in Appendix A. After model selection, we retrain the models for a maximum of 1200 epochs with early stopping and a patience of 10. 5 M ODEL https://github.com/botxo/nordic_bert https://github.com/allenai/allennlp 65 S ENTENCE E NTITY QID KG CONTEXT L ABEL The same sentence could be heard when Elvis had left the scene after a Las Vegas show. Elvis Q303 birth name Elvis Aaron Presley given nam"
2021.crac-1.7,2020.coling-main.583,1,0.853948,"Missing"
2021.crac-1.7,D17-1018,0,0.0295074,"l, 7,173 tokens were annotated with a QID. 2,193 unique QIDs were used. Coreference results The coreference benchmark results are presented in Table 2. Models based on the two multi-lingual, cased transformer models perform a lot better than the uncased DA BERT. The best model is L EE 2018 trained with XLM-R. 4 F1 Table 2: Coreference results: M(ention) R(ecall), average P(recision), R(ecall) and F1 across MUC, B 3 , and CEAFe. The best result per column is boldfaced. Coreference models We provide benchmark scores for two strong neural models using three different transformer representations. Lee et al. (2017) (L EE 2017) presented the first neural endto-end coreference model in which the spans are learned in the same training pass as the pairwise clustering. The pairwise clustering may produce globally inconsistent clusters, and Lee et al. (2018) (L EE 2018) presented a higher-order upgrade that did not only consider pairs of spans but a matrix of all spans to counter global inconsistencies. Joshi et al. (2019) showed that the model from Lee et al. (2018) performed even better on English data when using transformer-based models instead of the word embeddings of the original implementations. We fol"
2021.crac-1.7,N18-2108,0,0.0437791,"Missing"
2021.crac-1.7,W19-3502,0,0.0274694,"Missing"
2021.crac-1.7,W00-1007,0,0.367464,"lations into coreference clusters and the training and evaluation of coreference models on this data. To the best of our knowledge, these are the first publicly available neural coreference models for Danish. We also present a new entity linking annotation on the dataset using WikiData identifiers, a named entity disambiguation (NED) dataset, and a larger automatically created NED dataset enabling wikily supervised NED models. The entity linking annotation is benchmarked using a state-of-the-art neural entity disambiguation model. 1 2 Related work Danish anaphors have received some attention: Navarretta (2000) shows that Danish deictics are used in more contexts than the English ones and Houser et al. (2006) specifically discuss the use of verb phrase pronominalization in Danish. Danish has gendered possessive pronouns, but nongendered reflexive pronouns. This has made it useful as an unambiguous testbed for gender bias in natural language inference models, machine translation models, and language models (González et al., 2020). But automatic coreference resolution for Danish has received no attention, and there was no established evaluation set for this task. Linked resources such as Wikipedia ena"
2021.crac-1.7,2020.lrec-1.497,0,0.0547533,"Missing"
2021.crac-1.7,P17-1178,0,0.0266149,"lly discuss the use of verb phrase pronominalization in Danish. Danish has gendered possessive pronouns, but nongendered reflexive pronouns. This has made it useful as an unambiguous testbed for gender bias in natural language inference models, machine translation models, and language models (González et al., 2020). But automatic coreference resolution for Danish has received no attention, and there was no established evaluation set for this task. Linked resources such as Wikipedia enable multi-lingual entity linking/NED models and datasets, and Danish is often among the evaluation languages (Pan et al., 2017; McNamee et al., 2011). DBpedia Spotlight2 is the most recent entity linking system that also supports Danish. But due to this being a different task from NED, we can not compare our model to DBpedia Spotlight. Introduction The Danish Dependency Treebank (DDT) (BuchKromann, 2003) is a beneficial resource for Danish NLP that contains several annotation layers. Most of the layers were annotated as part of the Copenhagen Dependency Treebank project, but a conversion of the dependency syntax annotation into Universal dependencies (Johannsen et al., 2015; Nivre et al., 2020) and the addition of na"
2021.crac-1.7,W19-6143,1,0.77503,"recent entity linking system that also supports Danish. But due to this being a different task from NED, we can not compare our model to DBpedia Spotlight. Introduction The Danish Dependency Treebank (DDT) (BuchKromann, 2003) is a beneficial resource for Danish NLP that contains several annotation layers. Most of the layers were annotated as part of the Copenhagen Dependency Treebank project, but a conversion of the dependency syntax annotation into Universal dependencies (Johannsen et al., 2015; Nivre et al., 2020) and the addition of named entities annotation layers (Hvingelby et al., 2020; Plank, 2019; Plank et al., 2020) are newer additions. The partial coreference annotation has received no attention for NLP purposes despite being very well documented. This paper describes converting the coreference relations into coreference clusters and a new entity linking annotation with unique Wikidata item identification codes (QIDs) (Vrandeˇci´c, 2012) on the same data. Entity linking is the task of detecting mentions and matching the mentions to a knowledge base. The two annotation layers—coreference and entity linking—complement each other as two types of 1 https://github.com/alexandrainst/ danl"
2021.eacl-demos.22,abeille-etal-2000-building,0,0.715948,"Missing"
2021.eacl-demos.22,Q16-1022,1,0.891073,"Missing"
2021.eacl-demos.22,W19-6902,0,0.0848922,"Missing"
2021.eacl-demos.22,2020.wac-1.7,0,0.0495287,"Missing"
2021.eacl-demos.22,P16-1070,0,0.0567015,"Missing"
2021.eacl-demos.22,N18-1090,0,0.0611861,"Missing"
2021.eacl-demos.22,P18-1131,0,0.0450046,"Missing"
2021.eacl-demos.22,2021.adaptnlp-1.6,1,0.806857,"Missing"
2021.eacl-demos.22,W19-7803,0,0.0247002,"Missing"
2021.eacl-demos.22,W18-6004,0,0.0229452,"Missing"
2021.eacl-demos.22,S17-2001,0,0.0162745,"ng data (i.e., with the same number of column idx columns), but --raw text can be specified to read a data file containing raw texts with one sentence per line. For models trained Hyperparameter Tuning In this section we describe the procedure how we determined robust default parameters for M AC H A MP; note that the goal is not to beat the state-of-the-art, but to reach competitive performance for multiple tasks simultaneously.7 For the tuning of hyperparameters, we used the GLUE classification datasets (Wang et al., 2018; Warstadt et al., 2019; Socher et al., 2013; Dolan and Brockett, 2005; Cer et al., 2017; Williams et al., 2018; Rajpurkar et al., 2018; Bentivogli et al., 2009; Levesque et al., 2012) and the English Web Treebank (EWT 2.6) (Silveira et al., 2014) with multilingual BERT8 (mBERT) as embeddings.9 For each of these setups, we averaged the scores over all datasets/tasks and perform a grid search. The best hyperparameters across all datasets are reported in Table 1 and are the defaults values for M AC H A MP. 7 Compared to M AC H A MP v0.1 (van der Goot et al., 2020) we removed parameters with negligible effects (word dropout, layer dropout, adaptive softmax, and layer attention). 8 h"
2021.eacl-demos.22,P15-2019,0,0.0637769,"Missing"
2021.eacl-demos.22,L18-1347,0,0.0515319,"Missing"
2021.eacl-demos.22,D19-1162,0,0.0214484,"Missing"
2021.eacl-demos.22,W17-0405,0,0.0652177,"Missing"
2021.eacl-demos.22,W17-1406,0,0.0440789,"Missing"
2021.eacl-demos.22,L16-1248,0,0.0318008,"Missing"
2021.eacl-demos.22,2020.lrec-1.637,0,0.0969613,"Missing"
2021.eacl-demos.22,L16-1378,0,0.0606972,"Missing"
2021.eacl-demos.22,N15-1016,0,0.0167693,"toire in natural language processing (NLP). It enables neural networks to learn tasks in parallel (Caruana, 1993) while leveraging the benefits of sharing parameters. The shift—or “tsunami” (Manning, 2015)—of deep learning in NLP has facilitated the wide-spread use of MTL since the seminal work by Collobert et al. (2011), which has led to a multi-task learning “wave” (Ruder and Plank, 2018) in NLP. It has since been applied to a wide range of NLP tasks, developing into a viable alternative to classical pipeline approaches. This includes early adoption in Recurrent Neural Network models, e.g. (Lazaridou et al., 2015; Chrupała et al., 2015; Plank et al., 2016; Søgaard and Goldberg, 2016; Hashimoto et al., 2017), to the use of large pre-trained language models with multi-task objectives (Radford et al., 2019; Devlin et al., 2019). MTL comes in many flavors, based on the type of sharing, the weighting of 1 The code is available at: https://github. com/machamp-nlp/machamp (v0.2), and an instructional video at https://www.youtube.com/watch? v=DauTEdMhUDI. losses, and the design and relations of tasks and layers. In general though, outperforming single-task settings remains a challenge (Mart´ınez Alonso and Pl"
2021.eacl-demos.22,W17-0408,0,0.0247751,"Missing"
2021.eacl-demos.22,N18-1088,0,0.0630392,"Missing"
2021.eacl-demos.22,2021.ccl-1.108,0,0.0659631,"Missing"
2021.eacl-demos.22,2020.tlt-1.11,0,0.0485635,"Missing"
2021.eacl-demos.22,E17-1005,1,0.870771,"et al., 2015; Chrupała et al., 2015; Plank et al., 2016; Søgaard and Goldberg, 2016; Hashimoto et al., 2017), to the use of large pre-trained language models with multi-task objectives (Radford et al., 2019; Devlin et al., 2019). MTL comes in many flavors, based on the type of sharing, the weighting of 1 The code is available at: https://github. com/machamp-nlp/machamp (v0.2), and an instructional video at https://www.youtube.com/watch? v=DauTEdMhUDI. losses, and the design and relations of tasks and layers. In general though, outperforming single-task settings remains a challenge (Mart´ınez Alonso and Plank, 2017; Clark et al., 2019). For an overview of MTL in NLP we refer to Ruder (2017). As a separate line of research, the idea of language model pre-training and contextual embeddings (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019) is to pre-train rich representation on large quantities of monolingual or multilingual text data. Taking these representations as a starting point has led to enormous improvements across a wide variety of NLP problems. Related to MTL, recent research effort focuses on fine-tuning contextualized embeddings on a variety of tasks with supervised objectives"
2021.eacl-demos.22,W16-3905,0,0.0429128,"Missing"
2021.eacl-demos.22,P13-2017,0,0.0829332,"Missing"
2021.eacl-demos.22,W19-5008,0,0.0328324,"Missing"
2021.eacl-demos.22,W09-3035,0,0.0707414,"Missing"
2021.eacl-demos.22,L16-1250,0,0.0494103,"Missing"
2021.eacl-demos.22,L18-1710,0,0.0411309,"Missing"
2021.eacl-demos.22,W18-6015,0,0.0279114,"Missing"
2021.eacl-demos.22,N18-1202,0,0.0432845,"2019). MTL comes in many flavors, based on the type of sharing, the weighting of 1 The code is available at: https://github. com/machamp-nlp/machamp (v0.2), and an instructional video at https://www.youtube.com/watch? v=DauTEdMhUDI. losses, and the design and relations of tasks and layers. In general though, outperforming single-task settings remains a challenge (Mart´ınez Alonso and Plank, 2017; Clark et al., 2019). For an overview of MTL in NLP we refer to Ruder (2017). As a separate line of research, the idea of language model pre-training and contextual embeddings (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019) is to pre-train rich representation on large quantities of monolingual or multilingual text data. Taking these representations as a starting point has led to enormous improvements across a wide variety of NLP problems. Related to MTL, recent research effort focuses on fine-tuning contextualized embeddings on a variety of tasks with supervised objectives (Kondratyuk and Straka, 2019; Sanh et al., 2019; Hu et al., 2020). We introduce M AC H A MP, a flexible toolkit for multi-task learning and fine-tuning of NLP problems. The main advantages of M AC H A MP are: • Ease of co"
2021.eacl-demos.22,W19-8016,0,0.0309174,"Missing"
2021.eacl-demos.22,P16-2067,1,0.840818,"enables neural networks to learn tasks in parallel (Caruana, 1993) while leveraging the benefits of sharing parameters. The shift—or “tsunami” (Manning, 2015)—of deep learning in NLP has facilitated the wide-spread use of MTL since the seminal work by Collobert et al. (2011), which has led to a multi-task learning “wave” (Ruder and Plank, 2018) in NLP. It has since been applied to a wide range of NLP tasks, developing into a viable alternative to classical pipeline approaches. This includes early adoption in Recurrent Neural Network models, e.g. (Lazaridou et al., 2015; Chrupała et al., 2015; Plank et al., 2016; Søgaard and Goldberg, 2016; Hashimoto et al., 2017), to the use of large pre-trained language models with multi-task objectives (Radford et al., 2019; Devlin et al., 2019). MTL comes in many flavors, based on the type of sharing, the weighting of 1 The code is available at: https://github. com/machamp-nlp/machamp (v0.2), and an instructional video at https://www.youtube.com/watch? v=DauTEdMhUDI. losses, and the design and relations of tasks and layers. In general though, outperforming single-task settings remains a challenge (Mart´ınez Alonso and Plank, 2017; Clark et al., 2019). For an over"
2021.eacl-demos.22,2020.acl-demos.15,0,0.0211475,"ecoder 2.2 Supported task types We here describe the tasks M AC H A MP supports. Contextualized Embeddings SEQ For traditional token-level sequence prediction tasks, like part-of-speech tagging. M AC H A MP uses greedy decoding with a softmax output layer on the output of the contextual embeddings. <CLS> Smell ya later ! Figure 1: Overview of M AC H A MP, when training jointly for sentiment analysis and POS tagging. A shared encoding representation and task-specific decoders are exploited to accomplish both tasks. level and flexible. It should be noted that contemporary to M AC H A MP, jiant (Pruksachatkun et al., 2020) was developed, and AllenNLP included multi-task learning as well since release 2.0. M AC H A MP distinguishes from the other toolkits by supporting simple configurations, and a variety of multi-task settings. 2 STRING 2 STRING An extension to SEQ , which learns a conversion for each input token to its label. Instead of predicting the labels directly, the model can now learn to predict the conversion. This strategy is commonly used for lemmatization (Chrupała, 2006; Kondratyuk and Straka, 2019), where it greatly reduces the label vocabulary. We use the transformation algorithm from UDPipeFutur"
2021.eacl-demos.22,W15-1821,0,0.0362216,"Missing"
2021.eacl-demos.22,W17-7623,0,0.109046,"Missing"
2021.eacl-demos.22,W19-7811,0,0.0433701,"Missing"
2021.eacl-demos.22,rognvaldsson-etal-2012-icelandic,0,0.0249297,"Missing"
2021.eacl-demos.22,D13-1170,0,0.00878005,"assumed to be in the same format as the training data (i.e., with the same number of column idx columns), but --raw text can be specified to read a data file containing raw texts with one sentence per line. For models trained Hyperparameter Tuning In this section we describe the procedure how we determined robust default parameters for M AC H A MP; note that the goal is not to beat the state-of-the-art, but to reach competitive performance for multiple tasks simultaneously.7 For the tuning of hyperparameters, we used the GLUE classification datasets (Wang et al., 2018; Warstadt et al., 2019; Socher et al., 2013; Dolan and Brockett, 2005; Cer et al., 2017; Williams et al., 2018; Rajpurkar et al., 2018; Bentivogli et al., 2009; Levesque et al., 2012) and the English Web Treebank (EWT 2.6) (Silveira et al., 2014) with multilingual BERT8 (mBERT) as embeddings.9 For each of these setups, we averaged the scores over all datasets/tasks and perform a grid search. The best hyperparameters across all datasets are reported in Table 1 and are the defaults values for M AC H A MP. 7 Compared to M AC H A MP v0.1 (van der Goot et al., 2020) we removed parameters with negligible effects (word dropout, layer dropout,"
2021.eacl-demos.22,P16-2038,0,0.0295686,"rks to learn tasks in parallel (Caruana, 1993) while leveraging the benefits of sharing parameters. The shift—or “tsunami” (Manning, 2015)—of deep learning in NLP has facilitated the wide-spread use of MTL since the seminal work by Collobert et al. (2011), which has led to a multi-task learning “wave” (Ruder and Plank, 2018) in NLP. It has since been applied to a wide range of NLP tasks, developing into a viable alternative to classical pipeline approaches. This includes early adoption in Recurrent Neural Network models, e.g. (Lazaridou et al., 2015; Chrupała et al., 2015; Plank et al., 2016; Søgaard and Goldberg, 2016; Hashimoto et al., 2017), to the use of large pre-trained language models with multi-task objectives (Radford et al., 2019; Devlin et al., 2019). MTL comes in many flavors, based on the type of sharing, the weighting of 1 The code is available at: https://github. com/machamp-nlp/machamp (v0.2), and an instructional video at https://www.youtube.com/watch? v=DauTEdMhUDI. losses, and the design and relations of tasks and layers. In general though, outperforming single-task settings remains a challenge (Mart´ınez Alonso and Plank, 2017; Clark et al., 2019). For an overview of MTL in NLP we refer"
2021.eacl-demos.22,P18-2098,0,0.0643982,"Missing"
2021.eacl-demos.22,W19-8008,0,0.0870308,"Missing"
2021.eacl-demos.22,vincze-etal-2010-hungarian,0,0.0146532,"Missing"
2021.eacl-demos.22,2020.acl-main.778,0,0.0149149,"task level. Supported metrics are ‘acc’, ‘las’, ‘micro-f1’, ‘macro-f1’, ‘span f1’, ’multi span f1’, ’bleu’ and ’perplexity’. Loss weight In multi-task settings, not all tasks might be equally important, or some tasks might just be harder to learn, and therefore should gain more weight during training. This can be tuned by setting the loss weight parameter on the task level (by default the value is 1.0 for all tasks). Dataset embedding Ammar et al. (2016) have shown that embedding which language an instance belongs to can be beneficial for multilingual models. Later work (Stymne et al., 2018; Wagner et al., 2020) has also shown that more fine-grained distinctions on the dataset level5 can be beneficial when training on multiple datasets within the same language (family). In previous work, this embedding is usually concatenated to the word embedding before the encoding. However, in contextualized embeddings, the word embeddings themselves are commonly used as encoder, hence we concatenate the dataset embeddings in between the encoder and the decoder. This parameter is set on the dataset 5 These are called treebank embeddings in their work. We will use the more general term “dataset embeddings”, which w"
2021.emnlp-main.393,2020.findings-emnlp.151,0,0.240051,"such as genre guide such as highly cross-lingual dependency parsing a reality (Kondratyuk and Straka, 2019). Adja- our selection of cross-lingual training data from a cently, it has also been recognized that they cap- significantly larger, diverse pool (Figure 1)? ture characteristics relevant for training data seWithin the heterogeneity of written and spoken lection (Aharoni and Goldberg, 2020) and can be (transcribed) data, genre broadly encompasses variefficiently fine-tuned for higher task-specific per- ation along the functional role of a text (Kessler formance (Gururangan et al., 2020; Dai et al., 2020; et al., 1997). A clear definition is complex if not imLauscher et al., 2020; Üstün et al., 2020). These possible and communities refer to genre, domain, considerations are especially important in compu- style or register in different ways (Kessler et al., tationally restricted environments and when data 1997; Lee, 2001; Webber, 2009; Plank, 2011). In from the target distribution are unavailable. this work, we take a pragmatic approach and use 4786 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4786–4802 c November 7–11, 2021. 2021 Association fo"
2021.emnlp-main.393,2020.lrec-1.632,0,0.0371958,"trongly related languages in mBERT’s repertoire: Hindi-English (QHE) → Hindi, English as well as Turkish-German (QTD) → Turkish, German. The six included genres cover the high-resource news ( ) and fiction ( ) as well as the medium resource wiki ( ) and the lower resource spoken ( ), grammar-examples ( ) and social ( ). 5.2 Data Selection Setup In order to train parsers for these largely test-only treebanks, we compare seven proxy training data selection strategies for each target (note that only the first strategy uses in-language training data). make use of proxy in-language data: SA-Vedic (Hellwig et al., 2020) for SA-UFAL, KPV-IKDP (Partanen et al., 2018) for KPV-Lattice and FOFarPaHC (Ingason et al., 2020) for FO-OFT. For the targets YUE-HK, CKT-HSE and MYV-JR no in-language training data are currently available. R AND selects a random sample of nrand sentences from the non-target-language UD. We do not restrict this selection to treebanks containing the target genre such that data from a more diverse pool of languages may be selected. To ensure an equivalent comparison, we set nrand to the mean of the number of instances selected by B OOT, LDA and GMM (see Appendix C for values of nrand ). S ENT"
2021.emnlp-main.393,P97-1005,0,0.777054,"Missing"
2021.emnlp-main.393,D19-1279,0,0.110399,"ng wider linguistic coverage. 1 Introduction Criteria for selecting training data within such Multilingual masked language models (MLMs) settings vary, and a practitioner may determine relevance by proxy of language relatedness or treebank trained on immense quantities of heterogeneous texts (Devlin et al., 2019; Brown et al., 2020; Con- content. This leads us to the question: If our goal is to develop a parser for a known domain in an neau et al., 2020) have recently made applications unseen language, can a signal such as genre guide such as highly cross-lingual dependency parsing a reality (Kondratyuk and Straka, 2019). Adja- our selection of cross-lingual training data from a cently, it has also been recognized that they cap- significantly larger, diverse pool (Figure 1)? ture characteristics relevant for training data seWithin the heterogeneity of written and spoken lection (Aharoni and Goldberg, 2020) and can be (transcribed) data, genre broadly encompasses variefficiently fine-tuned for higher task-specific per- ation along the functional role of a text (Kessler formance (Gururangan et al., 2020; Dai et al., 2020; et al., 1997). A clear definition is complex if not imLauscher et al., 2020; Üstün et al.,"
2021.emnlp-main.393,W17-6530,0,0.0598861,"Missing"
2021.findings-acl.158,benikova-etal-2014-nosta,0,0.253234,"ting nested NE corpora span only a handful of languages and text domains. This is in stark contrast to resources for flat NER, which are available for at least up to 282 languages (Pan et al., 2017) and multiple domains, including a very recent effort (Liu et al., 2021). Existing NNER resources for English cover newswire (e.g., ACE, WSJ) (Mitchell et al., 2005; Ringland et al., 2019) and biomedical data (e.g., GENIA) (Kim et al., 2003; Alex et al., 2007; Pyysalo et al., 2007). Beyond English, there exist free and publicly available nested NER datasets. These include the GermEval 2014 dataset (Benikova et al., 2014a), which is one of the largest existing German NER resources covering largely news articles (Benikova et al., 2014b). Recently, the GermEval annotation guidelines inspired the creation of a Danish corpus (Plank et al., 2020). They added a layer of nested NER on top of the existing Danish Universal Dependency treebank (Johannsen et al., 2015). Both German and Danish corpora derive their annotation guidelines from the NoStA-D annotation scheme (Benikova et al., 2014b), which we adopt for EWT-NNER (Section 3.1). To facilitate research, a fine-grained nested NER annotation on top of the Penn Tree"
2021.findings-acl.158,X98-1012,0,0.447145,"main contributions are: i) We introduce EWT-NNER, a corpus for nested NER over five web domains. ii) A report on cross-lingual and in-language baselines. Our results highlight the challenges of processing web texts, and the need for research on cross-lingual cross-domain NNER. 2 Related Work Nested NER Much research has been devoted to flat Named Entity Recognition, with a long tradition of shared tasks (Grishman and Sund1808 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1808–1815 August 1–6, 2021. ©2021 Association for Computational Linguistics heim, 1996; Grishman, 1998; Tjong Kim Sang and De Meulder, 2003; Baldwin et al., 2015). The problem of nested named entity recognition (NNER) has instead received less attention. This lack of breadth of research has been attributed to practical reasons (Finkel and Manning, 2009), including a lack of annotated corpora (Ringland et al., 2019). Existing nested NE corpora span only a handful of languages and text domains. This is in stark contrast to resources for flat NER, which are available for at least up to 282 languages (Pan et al., 2017) and multiple domains, including a very recent effort (Liu et al., 2021). Existi"
2021.findings-acl.158,C96-1079,0,0.894158,"Missing"
2021.findings-acl.158,2020.acl-main.740,0,0.0284133,"Missing"
2021.findings-acl.158,N18-1079,0,0.139127,"esolution and question answering. The task has received a substantial amount of attention. However, tools and existing benchmarks largely focus on flat, coarsegrained entities and single-domain evaluation. Flat, coarse-grained entities however eschew semantic distinctions which can be important in downstream applications (Ringland et al., 2019). Examples include embedded locations (‘New York Times’), entities formed via derivation (‘Italian cuisine’) and tokens which are in part named entities (‘the Chicago-based company’). Research interest on methods to handle nested entities is increasing (Katiyar and Cardie, 2018). However, there is a lack of datasets, particularly resources which cover multiple target domains. To facilitate research on cross-domain nested NER, we introduce a new layer on top of the English Web Treebank (EWT), manually annotated for NNER. The corpus spans five web domains, four major named entity types, enriched with suffixes marking derivations and partial NEs. Figure 1 shows the domain overlap in terms of word types. Besides providing in-language benchmark results, EWT-NNER enables research on crosslingual transfer from German and Danish. Contributions The main contributions are: i)"
2021.findings-acl.158,P19-1511,0,0.0200596,"Missing"
2021.findings-acl.158,N19-1308,0,0.0208491,"n on top of the Penn Treebank WSJ has been released recently (Ringland et al., 2019). In contrast to ours, the WSJ NNER corpus spans 114 entity types and 6 layers, and includes numericals and time expressions beyond named entities. We instead focus on NEs with a total of 12 classes and 2 layers. As outlined by Katiyar and Cardie (2018), nested named entities are attracting more research attention. Modeling solutions opt for diverse strategies, from hierarchical systems to graph-based methods and models based on linearization (Alex et al., 2007; Finkel and Manning, 2009; Sohrab and Miwa, 2018; Luan et al., 2019; Lin et al., 2019; Zheng et al., 2019; Strakov´a et al., 2019; Shibuya and Hovy, 2020). The current top-performing neural systems use typically either a linearization, a multi-task learning or a graph-based approach (Strakov´a et al., 2019; Plank et al., 2020; Yu et al., 2020). We evaluate two such methods. English Web Treebank The English Web Treebank (EN-EWT) (Bies et al., 2012; Petrov and McDonald, 2012; Silveira et al., 2014) is a dataset introduced as part of the first workshop on Syntactic Analysis of Non-Canonical Language (SANCL). The advantage of EWT is that it spans over 200k tokens"
2021.findings-acl.158,P17-1178,0,0.0170392,"1815 August 1–6, 2021. ©2021 Association for Computational Linguistics heim, 1996; Grishman, 1998; Tjong Kim Sang and De Meulder, 2003; Baldwin et al., 2015). The problem of nested named entity recognition (NNER) has instead received less attention. This lack of breadth of research has been attributed to practical reasons (Finkel and Manning, 2009), including a lack of annotated corpora (Ringland et al., 2019). Existing nested NE corpora span only a handful of languages and text domains. This is in stark contrast to resources for flat NER, which are available for at least up to 282 languages (Pan et al., 2017) and multiple domains, including a very recent effort (Liu et al., 2021). Existing NNER resources for English cover newswire (e.g., ACE, WSJ) (Mitchell et al., 2005; Ringland et al., 2019) and biomedical data (e.g., GENIA) (Kim et al., 2003; Alex et al., 2007; Pyysalo et al., 2007). Beyond English, there exist free and publicly available nested NER datasets. These include the GermEval 2014 dataset (Benikova et al., 2014a), which is one of the largest existing German NER resources covering largely news articles (Benikova et al., 2014b). Recently, the GermEval annotation guidelines inspired the"
2021.findings-acl.158,2020.coling-main.583,1,0.847787,"Missing"
2021.findings-acl.158,2020.tacl-1.39,0,0.0120084,"19). In contrast to ours, the WSJ NNER corpus spans 114 entity types and 6 layers, and includes numericals and time expressions beyond named entities. We instead focus on NEs with a total of 12 classes and 2 layers. As outlined by Katiyar and Cardie (2018), nested named entities are attracting more research attention. Modeling solutions opt for diverse strategies, from hierarchical systems to graph-based methods and models based on linearization (Alex et al., 2007; Finkel and Manning, 2009; Sohrab and Miwa, 2018; Luan et al., 2019; Lin et al., 2019; Zheng et al., 2019; Strakov´a et al., 2019; Shibuya and Hovy, 2020). The current top-performing neural systems use typically either a linearization, a multi-task learning or a graph-based approach (Strakov´a et al., 2019; Plank et al., 2020; Yu et al., 2020). We evaluate two such methods. English Web Treebank The English Web Treebank (EN-EWT) (Bies et al., 2012; Petrov and McDonald, 2012; Silveira et al., 2014) is a dataset introduced as part of the first workshop on Syntactic Analysis of Non-Canonical Language (SANCL). The advantage of EWT is that it spans over 200k tokens of texts from five web domains: Yahoo! answers, newsgroups, weblogs, local business re"
2021.findings-acl.158,silveira-etal-2014-gold,0,0.072245,"Missing"
2021.findings-acl.158,D18-1309,0,0.044511,"Missing"
2021.findings-acl.158,P19-1527,0,0.0262409,"Missing"
2021.findings-acl.158,W03-0419,0,0.409431,"Missing"
2021.findings-acl.158,P19-1280,0,0.0277103,"Missing"
2021.findings-acl.158,2020.acl-main.577,0,0.0517645,"ses and 2 layers. As outlined by Katiyar and Cardie (2018), nested named entities are attracting more research attention. Modeling solutions opt for diverse strategies, from hierarchical systems to graph-based methods and models based on linearization (Alex et al., 2007; Finkel and Manning, 2009; Sohrab and Miwa, 2018; Luan et al., 2019; Lin et al., 2019; Zheng et al., 2019; Strakov´a et al., 2019; Shibuya and Hovy, 2020). The current top-performing neural systems use typically either a linearization, a multi-task learning or a graph-based approach (Strakov´a et al., 2019; Plank et al., 2020; Yu et al., 2020). We evaluate two such methods. English Web Treebank The English Web Treebank (EN-EWT) (Bies et al., 2012; Petrov and McDonald, 2012; Silveira et al., 2014) is a dataset introduced as part of the first workshop on Syntactic Analysis of Non-Canonical Language (SANCL). The advantage of EWT is that it spans over 200k tokens of texts from five web domains: Yahoo! answers, newsgroups, weblogs, local business reviews from Google and Enron emails. Gold annotations are available for several NLP tasks. The corpus was originally annotated for part-of-speech tags and constituency structure in Penn Treeba"
2021.findings-acl.158,D19-1034,0,0.0119822,"been released recently (Ringland et al., 2019). In contrast to ours, the WSJ NNER corpus spans 114 entity types and 6 layers, and includes numericals and time expressions beyond named entities. We instead focus on NEs with a total of 12 classes and 2 layers. As outlined by Katiyar and Cardie (2018), nested named entities are attracting more research attention. Modeling solutions opt for diverse strategies, from hierarchical systems to graph-based methods and models based on linearization (Alex et al., 2007; Finkel and Manning, 2009; Sohrab and Miwa, 2018; Luan et al., 2019; Lin et al., 2019; Zheng et al., 2019; Strakov´a et al., 2019; Shibuya and Hovy, 2020). The current top-performing neural systems use typically either a linearization, a multi-task learning or a graph-based approach (Strakov´a et al., 2019; Plank et al., 2020; Yu et al., 2020). We evaluate two such methods. English Web Treebank The English Web Treebank (EN-EWT) (Bies et al., 2012; Petrov and McDonald, 2012; Silveira et al., 2014) is a dataset introduced as part of the first workshop on Syntactic Analysis of Non-Canonical Language (SANCL). The advantage of EWT is that it spans over 200k tokens of texts from five web domains: Yahoo"
2021.findings-acl.158,P19-1510,0,0.216636,"cognition (NER) is the task of finding and classifying named entities in text, such as locations, organizations, and person names. It is a key task in Natural Language Processing (NLP), and an important step for downstream applications like relation extraction, co-reference resolution and question answering. The task has received a substantial amount of attention. However, tools and existing benchmarks largely focus on flat, coarsegrained entities and single-domain evaluation. Flat, coarse-grained entities however eschew semantic distinctions which can be important in downstream applications (Ringland et al., 2019). Examples include embedded locations (‘New York Times’), entities formed via derivation (‘Italian cuisine’) and tokens which are in part named entities (‘the Chicago-based company’). Research interest on methods to handle nested entities is increasing (Katiyar and Cardie, 2018). However, there is a lack of datasets, particularly resources which cover multiple target domains. To facilitate research on cross-domain nested NER, we introduce a new layer on top of the English Web Treebank (EWT), manually annotated for NNER. The corpus spans five web domains, four major named entity types, enriched"
2021.findings-emnlp.36,S14-2098,0,0.0655893,"Missing"
2021.findings-emnlp.36,Q17-1010,0,0.0756182,"Missing"
2021.findings-emnlp.36,P19-1266,0,0.0354071,"Missing"
2021.findings-emnlp.36,2020.emnlp-main.638,0,0.389197,"lects informative inthese methods are usually limited in their notion stances avoiding only hard-to-learn and easy-toof informativeness, which is tied to post-training learn cases, which leads to better AL and comparamodel uncertainty and batch diversity. ble or better results than full dataset training. 395 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 395–406 November 7–11, 2021. ©2021 Association for Computational Linguistics 2 Related Work AL has seen many usage scenarios in the Natural Language Processing (NLP) field (Shen et al., 2018; Lowell et al., 2019; Ein-Dor et al., 2020). The perspective of AL is that if a model is allowed to select the data from which it will learn the most, it will achieve comparable (or better) performance with less training instances (Siddhant and Lipton, 2018), and at the same time addressing the costly labeling process with a human annotator. A popular scenario is pool-based active learning (Lewis and Gale, 1994; Settles, 2009, 2012), which assumes a small set of labeled data L and a large pool of unlabeled data U. Most AL algorithms start similarly: a model is fit to L to get access to Pθ (y |x), then apply a query strategy to get the"
2021.findings-emnlp.36,C02-1150,0,0.672235,"nstance i are defined as the statistics calculated over E epochs. These statistics are then used as the coordinates in the plot. The following statistics are calculated, confidence, variability, and correctness, following the notation of Swayamdipta et al. (2020): across epochs E. E 1 X ˆ 1(yˆi = yi∗ |xi ) φi = E (3) e=1 Last, correctness (Equation 3) is denoted as the fraction of times the model correctly labels instance xi across epochs E. Given the aforementioned training dynamics and the obtained statistics per instance, we plot the data maps for both AGNews (Zhang et al., 2015) and TREC (Li and Roth, 2002), using all training data (Figure 1). The data map is based on a Multi-layer E 1 X Perceptron (MLP). As shown by Swayamdipta µ ˆi = pθ(e) (yi∗ |xi ) (1) et al. (2020), data maps identify three distinct reE e=1 gions: easy-to-learn, ambiguous, and hard-to-learn. 1 Confidence (Equation 1) is the mean model prob- The easy-to-learn instances are consistently preability of the gold label (yi∗ ) across epochs. Where dicted correctly with high confidence, these inpθ(e) is the model’s probability with parameters stances can be found in the upper region of the plot. The ambiguous samples have high vari"
2021.findings-emnlp.36,D19-1003,0,0.027069,"approach optimally selects informative inthese methods are usually limited in their notion stances avoiding only hard-to-learn and easy-toof informativeness, which is tied to post-training learn cases, which leads to better AL and comparamodel uncertainty and batch diversity. ble or better results than full dataset training. 395 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 395–406 November 7–11, 2021. ©2021 Association for Computational Linguistics 2 Related Work AL has seen many usage scenarios in the Natural Language Processing (NLP) field (Shen et al., 2018; Lowell et al., 2019; Ein-Dor et al., 2020). The perspective of AL is that if a model is allowed to select the data from which it will learn the most, it will achieve comparable (or better) performance with less training instances (Siddhant and Lipton, 2018), and at the same time addressing the costly labeling process with a human annotator. A popular scenario is pool-based active learning (Lewis and Gale, 1994; Settles, 2009, 2012), which assumes a small set of labeled data L and a large pool of unlabeled data U. Most AL algorithms start similarly: a model is fit to L to get access to Pθ (y |x), then apply a que"
2021.findings-emnlp.36,2020.emnlp-main.746,0,0.0984053,"Missing"
2021.findings-emnlp.36,D18-1318,0,0.0142214,"comparamodel uncertainty and batch diversity. ble or better results than full dataset training. 395 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 395–406 November 7–11, 2021. ©2021 Association for Computational Linguistics 2 Related Work AL has seen many usage scenarios in the Natural Language Processing (NLP) field (Shen et al., 2018; Lowell et al., 2019; Ein-Dor et al., 2020). The perspective of AL is that if a model is allowed to select the data from which it will learn the most, it will achieve comparable (or better) performance with less training instances (Siddhant and Lipton, 2018), and at the same time addressing the costly labeling process with a human annotator. A popular scenario is pool-based active learning (Lewis and Gale, 1994; Settles, 2009, 2012), which assumes a small set of labeled data L and a large pool of unlabeled data U. Most AL algorithms start similarly: a model is fit to L to get access to Pθ (y |x), then apply a query strategy to get the best scored instance from U, label this instance and add it to L in an iterative process. Common Strategies A commonly used query strategy is uncertainty sampling (Lewis and Gale, 1994; Lewis and Catlett, 1994). In"
2021.naacl-main.197,D14-1179,0,0.0477248,"Missing"
2021.naacl-main.197,D19-5611,0,0.0206095,"er linguistic knowledge from English to the target language together with the target task, we implement a NMT decoder based on the shared encoder. We use a sequenceto-sequence model (Sutskever et al., 2014) with a recurrent neural network decoder, which suits the auto-regressive nature of the machine translation tasks (Cho et al., 2014), and an attention mechanism to avoid compressing the whole source sentence into a fixed-length vector (Bahdanau et al., 2015). We found that fine-tuning the shared encoder achieves good performance on our machine translation datasets (Conneau and Lample, 2019; Clinchant et al., 2019), alleviating the need for freezing its parameters during training in order to avoid catastrophic forgetting (Imamura and Sumita, 2019; Goodfellow et al., 2014). Similar to MLM, we use 100,000 sentences, and a weight of 0.01. Data For this auxiliary task, we use the same data as for NMT- TRANSFER, described in detail above. 3.5 Universal Dependencies (aux-ud) Using syntax in hierarchical multi-task learning has previously shown to be beneficial (Hashimoto et al., 2017; Godwin et al., 2016). We here use full Universal Dependency (UD) parsing, i.e., partof-speech (POS) tagging, lemmatization, mo"
2021.naacl-main.197,2020.acl-main.747,0,0.0993936,"Missing"
2021.naacl-main.197,D17-1206,0,0.0273981,"d that fine-tuning the shared encoder achieves good performance on our machine translation datasets (Conneau and Lample, 2019; Clinchant et al., 2019), alleviating the need for freezing its parameters during training in order to avoid catastrophic forgetting (Imamura and Sumita, 2019; Goodfellow et al., 2014). Similar to MLM, we use 100,000 sentences, and a weight of 0.01. Data For this auxiliary task, we use the same data as for NMT- TRANSFER, described in detail above. 3.5 Universal Dependencies (aux-ud) Using syntax in hierarchical multi-task learning has previously shown to be beneficial (Hashimoto et al., 2017; Godwin et al., 2016). We here use full Universal Dependency (UD) parsing, i.e., partof-speech (POS) tagging, lemmatization, morphological tagging and dependency parsing as joint 2483 mBERT lang2vec en — de-st — de 0.18 da 0.18 nl 0.19 it 0.22 sr 0.23 id 0.24 ar 0.30 zh 0.33 kk 0.37 tr 0.38 ja∗ 0.41 Avg. 97.6 0.0 97.3 0.0 97.5 48.5 50.9 53.0 44.5 47.6 33.0 34.5 34.6 33.3 29.1 73.9 60.8 75.9 71.4 73.7 80.4 63.7 82.2 76.9 73.3 75.0 51.0 78.0 71.9 61.8 67.4 41.3 63.8 58.5 56.8 71.1 54.2 69.5 62.9 61.1 45.8 48.2 48.1 38.7 42.6 72.9 27.9 69.4 70.3 64.9 48.5 0.2 51.3 38.2 45.2 55.7 52.0 58.4 50.2 5"
2021.naacl-main.197,2021.eacl-main.87,0,0.0771479,"Missing"
2021.naacl-main.197,W18-2501,0,0.0134155,"Auxiliary tasks are sorted by dataset availability (MLM  NMT  UD), where the first type can be used with any raw text, the second one needs parallel data – which is readily available for many languages as a byproduct of multilingual data sources – and the last one requires explicit human annotation. For South Tyrolean, a German dialect, no labeled target data of any sort is available; we use the German task data instead. We provide more details of data sources and sizes in Appendix B. 3.1 Baseline All our models are implemented in MaChAmp v0.2 (van der Goot et al., 2021), an AllenNLPbased (Gardner et al., 2018) multi-task learning toolkit. It uses contextual embeddings, and finetunes them during training. In the multi-task setup, the encoding is shared, and each task has its own decoder. For slot prediction, a greedy decoding with a softmax layer is used, for intents it uses a linear classification layer over the [CLS] token (see Figure 2).8 The data for each task is split in batches, and the batches are then shuffled. We use the default hyperparameters of MaChAmp for all experiments which were optimized on a wide variety of tasks (van der Goot et al., 2021).9 The following models are extensions of"
2021.naacl-main.197,N18-3017,0,0.0173969,"esentations. The first stream of research iliary tasks in two settings. Our results showed that focuses on generating training data in the target lan- masked language modeling led to the most stable guage with machine translation and mapping the performance improvements; however, when a lanslot labels through attention or an external word guage is not seen during pre-training, UD parsing aligner. The translation-based approach can be fur- led to an even larger performance increase. On ther improved by filtering the resulting training the intents, generating target language training data data (Gaspers et al., 2018; Do and Gaspers, 2019), using machine translation was outperforming all post-fixing the annotation by humans (Castellucci our proposed models, at a much higher computaet al., 2019), or by using a soft-alignment based on tional cost however. Our analysis further shows that attention, which alleviates error propagation and nmt-transfer struggles with span detection. Given outperforms annotation projection using external training time and availability trade-off, MLM multiword aligners (Xu et al., 2020). tasking is a viable approach for SLU. 2487 Acknowledgements International Workshop on Spoken"
2021.naacl-main.197,2020.acl-main.740,0,0.0555848,"Missing"
2021.naacl-main.197,H90-1021,0,0.730752,"Missing"
2021.naacl-main.197,D19-5603,0,0.018541,"ared encoder. We use a sequenceto-sequence model (Sutskever et al., 2014) with a recurrent neural network decoder, which suits the auto-regressive nature of the machine translation tasks (Cho et al., 2014), and an attention mechanism to avoid compressing the whole source sentence into a fixed-length vector (Bahdanau et al., 2015). We found that fine-tuning the shared encoder achieves good performance on our machine translation datasets (Conneau and Lample, 2019; Clinchant et al., 2019), alleviating the need for freezing its parameters during training in order to avoid catastrophic forgetting (Imamura and Sumita, 2019; Goodfellow et al., 2014). Similar to MLM, we use 100,000 sentences, and a weight of 0.01. Data For this auxiliary task, we use the same data as for NMT- TRANSFER, described in detail above. 3.5 Universal Dependencies (aux-ud) Using syntax in hierarchical multi-task learning has previously shown to be beneficial (Hashimoto et al., 2017; Godwin et al., 2016). We here use full Universal Dependency (UD) parsing, i.e., partof-speech (POS) tagging, lemmatization, morphological tagging and dependency parsing as joint 2483 mBERT lang2vec en — de-st — de 0.18 da 0.18 nl 0.19 it 0.22 sr 0.23 id 0.24 a"
2021.naacl-main.197,2020.emnlp-main.40,0,0.0240877,"ge, we manually picked a matching UD treebank from version 2.6 (Nivre et al., 2020) (details in the Appendix). Whenever available, we picked an in-language treebank, otherwise we choose a related language. We used size, annotation quality, and domain as criteria. 4 4.1 Results Experimental Setup as is standard for these tasks.11 All reported results (including analysis and test data) are the average over 5 runs with different random seeds. To choose the final model, we use the scores on the English development data. We are aware that this was recently shown to be sub-optimal in some settings (Keung et al., 2020), however there is no clear solution on how to circumvent this in a pure zero-shot cross-lingual setup (i.e. without assuming any target language target task annotation data). We use multilingual BERT (mBERT) as contextual encoder for our experiments. We are also interested in low-resource setups. As all of our languages are included in pre-training of mBERT (except the de-st dialect), we also study XLM15 (XLM MLM - TLM - XNLI 15-1024), which in pre-training covers only 5 of the 13 X SID languages, to simulate further a real low-resource setup. Table 3 reports the scores on 13 X SID languages,"
2021.naacl-main.197,L16-1147,0,0.0292956,"target language outputs; we map the label of each token to the highest scoring alignment target token. We convert the output to valid BIO tags: we use the label of the B for the whole span, and an I following an O is converted to a B. Data To ensure that our machine translation data is suitable for the target domain, we choose to use a combination of transcribed spoken parallel data. For languages included in the IWSLT 2016 Ted talks dataset (Cettolo et al., 2016), we use the train and development data included, and enlarge the training data with the training split from Opensubtitles10 2018 (Lison and Tiedemann, 2016), and Tatoeba (Tiedemann, 2012). For languages absent in IWSLT2016, we used the Opensubtitles data for training and Tatoeba as development set. For Kazakh, the Opensubtitles data only contains 2,000 sentences, so we concatenated out-ofdomain data from the WMT2019 data (Barrault et al., 2019), consisting of English-Kazakh crawled corpora. We adapt the BertBasic tokenizer (which splits punctuation, it does not perform subword tokenization) to match the Facebook and Snips dataset tokenization and use this to pre-tokenize the data. 10 http://www.opensubtitles.org/ 3.3 Masked Language Modeling (aux"
2021.naacl-main.197,E17-2002,0,0.0605313,"Missing"
2021.naacl-main.197,N19-1380,0,0.142156,"(Chen et al., 2019; Qin et al., 2020). Despite advances in neural modeling for slot and intent detection (§ 6), datasets for SLU remain limited, hampering progress toward providing SLU for many language varieties. Most availAdd reminder to swim at 11am tomorrow intent: add reminder Figure 1: English example from X SID annotated with intents (add reminder) and slots ( todo , datetime ). The full set of languages is shown in Table 2. able datasets either support only a specific domain (like air traffic systems) (Xu et al., 2020), or are broader but limited to English and a few other languages (Schuster et al., 2019; Coucke et al., 2018). We release X SID, a new benchmark intended for SLU evaluation in low-resource scenarios. X SID contains evaluation data for 13 languages from six language families, including a very low-resource dialect. It homogenizes annotation styles of two recent datasets (Schuster et al., 2019; Coucke et al., 2018) and provides the broadest public multilingual evaluation data for modern digital assistants. Most previous efforts to multilingual SLU typically focus on translation or multilingual embeddings transfer. In this work, we propose an orthogonal approach, and study non-Engli"
2021.naacl-main.197,P16-1162,0,0.00928267,"anslation with Attention (nmt-transfer) For comparison, we trained a NMT model to translate the NLU training data into the target language, and map the annotations using attention. As opposed to most previous work using this method (Xu et al., 2020; He et al., 2013; Schuster et al., 2019), we opt for an open-source implementation and provide the scripts to rerun the experiments. More specifically, we use the Fairseq toolkit (Ott et al., 2019) implementation of the Transformer-based model (Vaswani et al., 2017) with default hyperparameters. Sentences were encoded using bytepair encoding (BPE) (Sennrich et al., 2016), with a shared vocabulary of 32,000 tokens. At inference time, we set the beam size to 4, and extracted alignment scores to target tokens calculated from the attention weights matrix. These scores are used to align annotation labels to target language outputs; we map the label of each token to the highest scoring alignment target token. We convert the output to valid BIO tags: we use the label of the B for the whole span, and an I following an O is converted to a B. Data To ensure that our machine translation data is suitable for the target domain, we choose to use a combination of transcribe"
2021.naacl-main.197,E12-2021,0,0.0163083,"Missing"
2021.naacl-main.197,2021.eacl-demos.22,1,0.807471,"Missing"
2021.naacl-main.204,P14-2064,0,0.0628865,"Missing"
2021.naacl-main.204,D12-1091,0,0.0822475,"Missing"
2021.naacl-main.204,P13-1004,0,0.0147721,"; Han functions is not significant, we find that the inverse et al., 2018b,a) treat disagreement as a corruption version of KL gives the best results in all the experof a theoretical gold standard. Since the robustness imental conditions but one. This finding supports of machine learning models is affected by the data our idea of emphasizing the coders’ disagreement annotation quality, reducing noisy labels generally during training. We conjecture that predicting the improves the models’ performance. The closest to soft labels acts as a regularizer, reducing overfitour work are the studies of Cohn and Specia (2013) ting. That effect is especially likely for ambiguous and Rodrigues and Pereira (2018), who both use instances, where annotators’ label distributions difMTL. In contrast to our approach, though, each fer especially strongly from one-hot encoded gold of their tasks represents an annotator. We instead labels. propose to learn from both the gold labels and the Acknowledgements distribution over multiple annotators, which we treat as soft label distributions in a single auxil- DH and TF are members of the Data and Marketing iary task. Compared to treating each annotator as a Insights Unit at the B"
2021.naacl-main.204,N13-1132,1,0.807851,"ed labels. The 1 Introduction main impediment to the direct use of soft labels as targets, though, is the lack of universally accepted Usually, the labels used in NLP classification tasks are produced by sets of human annotators. As dis- performance metrics to evaluate the divergence between probability distributions. (Most metrics lack agreement between annotators is common, many an upper bound, making it difficult to assess predicmethods aggregate the different answers into a supposedly correct one (Dawid and Skene, 1979; tion quality). Usually, annotations are incorporated Carpenter, 2008; Hovy et al., 2013; Raykar et al., into the models without soft labels (Plank et al., 2010; Paun et al., 2018; Ruiz et al., 2019). How- 2014; Rodrigues and Pereira, 2018). Where soft labels are used, they are variously filtered according ever, the aggregated labels obtained in this way to their distance from the correct labels and then mask the world’s real complexity: instances can be intrinsically ambiguous (Poesio and Artstein, used to weight the training instances rather than as prediction targets. These models still predict only 2005; Zeman, 2010; Plank et al., 2014; Pavlick and true labels (Jamison and Gu"
2021.naacl-main.204,P14-2062,1,0.739372,"ging and morphological stemming. We use the respective data sets from Plank et al. (2014) and Jamison and Gurevych (2015) (where data sets are sufficiently large to train a neural model). In both cases, we use data sets where both one-hot (gold) and probabilistic (soft) labels (i.e., distributions over labels annotations) are available. The code for all models in this paper will be available on github.com/fornaciari. 3.1 POS tagging Data set For this task, we use the data set released by Gimpel et al. (2010) with the crowdThis measures the divergence from P to Q and sourced labels provided by Hovy et al. (2014). The encourages a narrow Q distribution because the same data set was used by Jamison and Gurevych model will try to allocate mass to Q in all the places (2015). Similarly, we use the CONLL Universal 2592 i POS tags (Petrov et al., 2012) and 5-fold crossvalidation. The soft labels come from the annotation of 177 annotators, with at least five annotations for each instance. Differently from Jamison and Gurevych (2015), however, we also test the model on a completely independent test set, released by Plank et al. (2014). This data set does not contain soft labels. However, they are not necessar"
2021.naacl-main.204,2021.naacl-main.49,1,0.714309,"ularization methods for neural networks. Among them, label smoothing (Pereyra et al., 2017) penalizes the cases of over-confident network predictions. Both label smoothing and soft labels reduce overfitting regulating the loss size. However, label smoothing relies on the gold labels’ distribution, not accounting for the instances’ inherent ambiguity, while soft labels selectively train the models to reduce the confidence when dealing with unclear cases, not affecting the prediction of clear cases. Disagreement also relates to the issue of annotator biases (Shah et al., 2020; Sap et al., 2019; Hovy and Yang, 2021), and our method can provide a possible way to address it. Several different lines of research use annotation disagreement. One line focuses on the aggregation of multiple annotations before model training. Seminal work includes the proposal by Dawid and Skene (1979), who proposed an ExpectationMaximization (EM) based aggregation model. This model has since influenced a large body of work on annotation aggregation, and modeling annotator competence (Carpenter et al., 2009; Hovy et al., 2013; Raykar et al., 2010; Paun et al., 2018; Ruiz et al., 2019). In our experiments on POS-tagging, we evalu"
2021.naacl-main.204,D15-1035,0,0.537521,"y et al., 2013; Raykar et al., into the models without soft labels (Plank et al., 2010; Paun et al., 2018; Ruiz et al., 2019). How- 2014; Rodrigues and Pereira, 2018). Where soft labels are used, they are variously filtered according ever, the aggregated labels obtained in this way to their distance from the correct labels and then mask the world’s real complexity: instances can be intrinsically ambiguous (Poesio and Artstein, used to weight the training instances rather than as prediction targets. These models still predict only 2005; Zeman, 2010; Plank et al., 2014; Pavlick and true labels (Jamison and Gurevych, 2015). Kwiatkowski, 2019), or so challenging to evaluate that considerable disagreement between different In contrast to previous approaches, we use Multiannotators is unavoidable. In those cases, it is rea- Task Learning (MTL) to predict a probability distrisonable to wonder whether the ambiguity is indeed bution over the soft labels as additional output. We harmful to the models or whether it carries valuable jointly model the main task of predicting standard information about the relative difficulty of each in- gold labels and the novel auxiliary task of predictstance (Aroyo and Welty, 2015). Se"
2021.naacl-main.204,Q18-1040,1,0.944915,"though, is the lack of universally accepted Usually, the labels used in NLP classification tasks are produced by sets of human annotators. As dis- performance metrics to evaluate the divergence between probability distributions. (Most metrics lack agreement between annotators is common, many an upper bound, making it difficult to assess predicmethods aggregate the different answers into a supposedly correct one (Dawid and Skene, 1979; tion quality). Usually, annotations are incorporated Carpenter, 2008; Hovy et al., 2013; Raykar et al., into the models without soft labels (Plank et al., 2010; Paun et al., 2018; Ruiz et al., 2019). How- 2014; Rodrigues and Pereira, 2018). Where soft labels are used, they are variously filtered according ever, the aggregated labels obtained in this way to their distance from the correct labels and then mask the world’s real complexity: instances can be intrinsically ambiguous (Poesio and Artstein, used to weight the training instances rather than as prediction targets. These models still predict only 2005; Zeman, 2010; Plank et al., 2014; Pavlick and true labels (Jamison and Gurevych, 2015). Kwiatkowski, 2019), or so challenging to evaluate that considerable disagree"
2021.naacl-main.204,Q19-1043,0,0.110921,"Missing"
2021.naacl-main.204,D14-1162,0,0.0838039,"Missing"
2021.naacl-main.204,petrov-etal-2012-universal,0,0.119656,"Missing"
2021.naacl-main.204,W14-1601,1,0.870017,"Missing"
2021.naacl-main.204,E14-1078,1,0.897989,"tations are incorporated Carpenter, 2008; Hovy et al., 2013; Raykar et al., into the models without soft labels (Plank et al., 2010; Paun et al., 2018; Ruiz et al., 2019). How- 2014; Rodrigues and Pereira, 2018). Where soft labels are used, they are variously filtered according ever, the aggregated labels obtained in this way to their distance from the correct labels and then mask the world’s real complexity: instances can be intrinsically ambiguous (Poesio and Artstein, used to weight the training instances rather than as prediction targets. These models still predict only 2005; Zeman, 2010; Plank et al., 2014; Pavlick and true labels (Jamison and Gurevych, 2015). Kwiatkowski, 2019), or so challenging to evaluate that considerable disagreement between different In contrast to previous approaches, we use Multiannotators is unavoidable. In those cases, it is rea- Task Learning (MTL) to predict a probability distrisonable to wonder whether the ambiguity is indeed bution over the soft labels as additional output. We harmful to the models or whether it carries valuable jointly model the main task of predicting standard information about the relative difficulty of each in- gold labels and the novel auxil"
2021.naacl-main.204,P16-2067,1,0.774899,"Q in all the places (2015). Similarly, we use the CONLL Universal 2592 i POS tags (Petrov et al., 2012) and 5-fold crossvalidation. The soft labels come from the annotation of 177 annotators, with at least five annotations for each instance. Differently from Jamison and Gurevych (2015), however, we also test the model on a completely independent test set, released by Plank et al. (2014). This data set does not contain soft labels. However, they are not necessary to test our models. Model We use a tagging model that takes two kinds of input representations, at the character and the word level (Plank et al., 2016). At the character level, we use character embeddings trained on the same data set; at the word level, we use Glove embeddings (Pennington et al., 2014). We feed the word representation into a ‘context bi-RNN’, selecting the hidden state of the RNN at the target word’s position in the sentence. The character representation is then fed into a ‘sequence bi-RNN’, whose output is its final state. The two outputs are concatenated and passed to an attention mechanism, as proposed by Vaswani et al. (2017). In the STL models, the attention mechanisms’ output is passed to a last attention mechanism and"
2021.naacl-main.204,W05-0311,1,0.842083,"Missing"
2021.naacl-main.204,P19-1163,0,0.0225307,"search area of regularization methods for neural networks. Among them, label smoothing (Pereyra et al., 2017) penalizes the cases of over-confident network predictions. Both label smoothing and soft labels reduce overfitting regulating the loss size. However, label smoothing relies on the gold labels’ distribution, not accounting for the instances’ inherent ambiguity, while soft labels selectively train the models to reduce the confidence when dealing with unclear cases, not affecting the prediction of clear cases. Disagreement also relates to the issue of annotator biases (Shah et al., 2020; Sap et al., 2019; Hovy and Yang, 2021), and our method can provide a possible way to address it. Several different lines of research use annotation disagreement. One line focuses on the aggregation of multiple annotations before model training. Seminal work includes the proposal by Dawid and Skene (1979), who proposed an ExpectationMaximization (EM) based aggregation model. This model has since influenced a large body of work on annotation aggregation, and modeling annotator competence (Carpenter et al., 2009; Hovy et al., 2013; Raykar et al., 2010; Paun et al., 2018; Ruiz et al., 2019). In our experiments on"
2021.naacl-main.204,2020.acl-main.468,1,0.71779,"y belongs to the research area of regularization methods for neural networks. Among them, label smoothing (Pereyra et al., 2017) penalizes the cases of over-confident network predictions. Both label smoothing and soft labels reduce overfitting regulating the loss size. However, label smoothing relies on the gold labels’ distribution, not accounting for the instances’ inherent ambiguity, while soft labels selectively train the models to reduce the confidence when dealing with unclear cases, not affecting the prediction of clear cases. Disagreement also relates to the issue of annotator biases (Shah et al., 2020; Sap et al., 2019; Hovy and Yang, 2021), and our method can provide a possible way to address it. Several different lines of research use annotation disagreement. One line focuses on the aggregation of multiple annotations before model training. Seminal work includes the proposal by Dawid and Skene (1979), who proposed an ExpectationMaximization (EM) based aggregation model. This model has since influenced a large body of work on annotation aggregation, and modeling annotator competence (Carpenter et al., 2009; Hovy et al., 2013; Raykar et al., 2010; Paun et al., 2018; Ruiz et al., 2019). In"
2021.nodalida-main.21,P19-1595,0,0.0121917,"owever, BERTOverflow is able to get a four point increase in F1 with I2B2 as auxiliary task in MaChAmp. For Bilty with BERTOverflow we see a slightly greater gain with both CoNLL and I2B2 as auxiliary tasks. When comparing the auxiliary data sources to each other, we note that the closer text domain (CoNLL news) is more beneficial than the closer label set (I2B2) from a more distant medical text source. This is consistent for the strongest models. In general, it can be challenging to train multitask networks that outperform or even match their single-task counterparts (Alonso and Plank, 2017; Clark et al., 2019). Ruder (2017) mentions training on a large number of tasks is known to help regularize multi-task models. A related benefit of MTL is the transfer of learned “knowledge” between closely related tasks. In our case, it has been beneficial to add auxiliary tasks to improve our performance on both development and test compared to a single task setting. In particular, it seemed to have helped with pertaining a high recall score. Performance on the test set Finally, we evaluate the best performing models on our held out test set. The best models are selected based on their performance on F1, precis"
2021.nodalida-main.21,C18-1139,0,0.016577,"ernoncourt et al. (2017) were the first to use Bi-LSTMs, which they used in combination with character-level embeddings. Similarly, Khin et al. (2018) performed de-identification by using a Bi-LSTM-CRF architecture with ELMo embeddings (Peters et al., 2018). Liu et al. (2017) used four individual methods (CRF-based, Bi-LSTM, Bi-LSTM with features, and rule-based methods) for de-identification, and used an ensemble learning method to combine all PHI instances predicted by the three methods. Trienes et al. (2020) opted for a Bi-LSTM-CRF as well, but applied it with contextual string embeddings (Akbik et al., 2018). Most recently, Johnson et al. (2020) fine-tuned BERTbase and BERTlarge (Devlin et al., 2019) for de-identification. Next to “vanilla” BERT, they experiment with fine-tuning different domain specific pre-trained language models, such as SciBERT (Beltagy et al., 2019) and BioBERT (Lee et al., 2020). They achieve state-of-the art performance in de-identification on the I2B2 dataset with the fine-tuned BERTlarge model. From a different perspective, the approach of Friedrich et al. (2019) is based on adversarial learning, which automatically pseudo-anonymizes EHRs. 2.2 De-identification in other"
2021.nodalida-main.21,E17-1005,1,0.841277,"compared to BERTbase . However, BERTOverflow is able to get a four point increase in F1 with I2B2 as auxiliary task in MaChAmp. For Bilty with BERTOverflow we see a slightly greater gain with both CoNLL and I2B2 as auxiliary tasks. When comparing the auxiliary data sources to each other, we note that the closer text domain (CoNLL news) is more beneficial than the closer label set (I2B2) from a more distant medical text source. This is consistent for the strongest models. In general, it can be challenging to train multitask networks that outperform or even match their single-task counterparts (Alonso and Plank, 2017; Clark et al., 2019). Ruder (2017) mentions training on a large number of tasks is known to help regularize multi-task models. A related benefit of MTL is the transfer of learned “knowledge” between closely related tasks. In our case, it has been beneficial to add auxiliary tasks to improve our performance on both development and test compared to a single task setting. In particular, it seemed to have helped with pertaining a high recall score. Performance on the test set Finally, we evaluate the best performing models on our held out test set. The best models are selected based on their perf"
2021.nodalida-main.21,W09-3302,0,0.0393016,"te our annotation guidelines, a sample of the data was annotated by three annotators, one with a background in Linguistics (A1) and two with a background in Computer Science (A2, A3). We used an open source text annotation tool named Doccano (Nakayama et al., 2018). There are around 1,500 overlapping sentences that we calculated agreement on. The annotations were compared using Cohen’s κ (Fleiss and Cohen, 1973) between pairs of annotators, and Fleiss’ κ (Fleiss, 1971), which generalises Cohen’s κ to more than two concurrent annotations. Table 2 shows three levels of κ calculations, we follow Balasuriya et al. (2009)’s approach of calculating agreement in NER. (1) Token is calculated on the token level, comparing the agreement of annotators on each token (including nonentities) in the annotated dataset. (2) Entity is calculated on the agreement between named entities alone, excluding agreement in cases where all annotators agreed that a token was not a namedentity. (3) Unlabeled refers to the agreement between annotators on the exact span match over the surface string, regardless of the type of named entity (i.e., we only check the position of tag without regarding the type of the named entity). Landis an"
2021.nodalida-main.21,N19-1423,0,0.0203566,"character-level embeddings. Similarly, Khin et al. (2018) performed de-identification by using a Bi-LSTM-CRF architecture with ELMo embeddings (Peters et al., 2018). Liu et al. (2017) used four individual methods (CRF-based, Bi-LSTM, Bi-LSTM with features, and rule-based methods) for de-identification, and used an ensemble learning method to combine all PHI instances predicted by the three methods. Trienes et al. (2020) opted for a Bi-LSTM-CRF as well, but applied it with contextual string embeddings (Akbik et al., 2018). Most recently, Johnson et al. (2020) fine-tuned BERTbase and BERTlarge (Devlin et al., 2019) for de-identification. Next to “vanilla” BERT, they experiment with fine-tuning different domain specific pre-trained language models, such as SciBERT (Beltagy et al., 2019) and BioBERT (Lee et al., 2020). They achieve state-of-the art performance in de-identification on the I2B2 dataset with the fine-tuned BERTlarge model. From a different perspective, the approach of Friedrich et al. (2019) is based on adversarial learning, which automatically pseudo-anonymizes EHRs. 2.2 De-identification in other Domains Data protection in general however is not only limited to the medical domain. Even tho"
2021.nodalida-main.21,R19-1030,0,0.0975575,"ge models, such as SciBERT (Beltagy et al., 2019) and BioBERT (Lee et al., 2020). They achieve state-of-the art performance in de-identification on the I2B2 dataset with the fine-tuned BERTlarge model. From a different perspective, the approach of Friedrich et al. (2019) is based on adversarial learning, which automatically pseudo-anonymizes EHRs. 2.2 De-identification in other Domains Data protection in general however is not only limited to the medical domain. Even though work outside the clinical domain is rare, personal and sensitive data is in abundance in all kinds of data. For example, Eder et al. (2019) pseudonymised German emails. Bevendorff et al. (2020) published a large preprocessed email corpus, where only the email addresses themselves where anonymized. Apart from emails, several works went into deidentification of SMS messages (Treurniet et al., 2012; Patel et al., 2013; Chen and Kan, 2013) in Dutch, French, English and Mandarin respectively. Both Treurniet et al. (2012); Chen and Kan (2013) conducted the same strategy and automatically anonymized all occurrences of dates, times, decimal amounts, and numbers with more than one digit (telephone numbers, bank accounts, et cetera), email"
2021.nodalida-main.21,P19-1584,0,0.0281742,"Missing"
2021.nodalida-main.21,2021.eacl-demos.22,1,0.807235,"Missing"
2021.nodalida-main.21,2005.mtsummit-papers.11,0,0.0330074,"Missing"
2021.nodalida-main.21,D14-1162,0,0.0828727,"Missing"
2021.nodalida-main.21,N18-1202,0,0.0498369,"r, earlier work showed the use of CRFs in an ensemble with rules (Stubbs et al., 2015). Other ML approaches include data augmentation by McMurry et al. (2013), where they added public medical texts to properly distinguish common medical words and phrases from PHI and trained decision trees on the augmented data. Neural methods Third, regarding neural methods, Dernoncourt et al. (2017) were the first to use Bi-LSTMs, which they used in combination with character-level embeddings. Similarly, Khin et al. (2018) performed de-identification by using a Bi-LSTM-CRF architecture with ELMo embeddings (Peters et al., 2018). Liu et al. (2017) used four individual methods (CRF-based, Bi-LSTM, Bi-LSTM with features, and rule-based methods) for de-identification, and used an ensemble learning method to combine all PHI instances predicted by the three methods. Trienes et al. (2020) opted for a Bi-LSTM-CRF as well, but applied it with contextual string embeddings (Akbik et al., 2018). Most recently, Johnson et al. (2020) fine-tuned BERTbase and BERTlarge (Devlin et al., 2019) for de-identification. Next to “vanilla” BERT, they experiment with fine-tuning different domain specific pre-trained language models, such as"
2021.nodalida-main.21,P16-2067,1,0.827785,"22 75.65 ± 1.41 80.26 ± 1.32 77.66 ± 0.82 66.24 ± 0.98 68.47 ± 1.03 69.41 ± 0.89 Table 4: Performance of multi-task learning on the development set across three runs. and character embeddings which are updated during model training. The BERTOverflow model is a transformer with the same architecture as BERTbase . It has been trained from scratch on a large corpus of text from the Q&A section of Stackoverflow, making it closer to our text domain than the “vanilla” BERT model. However, BERTOverflow is not trained on the job postings portion of Stackoverflow. 4.3 Auxiliary tasks Both the Bi-LSTM (Plank et al., 2016) and the MaChAmp (van der Goot et al., 2021) toolkit are capable of Multi Task Learning (MTL) (Caruana, 1997). We therefore, set up a number of experiments testing the impact of three different auxiliary tasks. The auxiliary tasks and their datasets are as follows: • I2B2/UTHealth (Stubbs and Uzuner, 2015) Medical de-identification; • CoNLL 2003 (Sang and De Meulder, 2003) News Named Entity Recognition; • The combination of the above. The data of the two tasks are similar to our dataset in two different ways. The I2B2 lies in a different text domain, namely medical notes, however, the label se"
2021.nodalida-main.21,W03-0419,0,0.485235,"Missing"
2021.nodalida-main.21,2020.acl-main.443,0,0.0734602,"-regressive models trained on massive text collections pose a potential risk for exposing private or sensitive information (Carlini et al., 2019, 2020), and de-identification can be one way to address this. In this paper, we analyze how effectively sequence labeling models are in identifying privacyrelated entities in job posts. To the best of our knowledge, we are the first study that investigates de-identification methods applied to job vacancies. In particular, we examine: How do Transformerbased models compare to LSTM-based models on this task (RQ1)? How does BERT compare to BERTOverflow (Tabassum et al., 2020) (RQ2)? To what extent can we use existing medical deidentification data and Named Entity Recognition (NER) data to improve de-identification performance (RQ3)? To answer these questions, we put forth a new corpus, J OB S TACK, annotated with around 22,000 sentences in English job postings from Stackoverflow for person names, contact details, locations, and information about the profession of the job post itself. Contributions We present J OB S TACK, the first job postings dataset with professional and personal entity annotations from Stackoverflow. Our experiments on entity de-identification"
2021.nodalida-main.21,treurniet-etal-2012-collection,0,0.0546779,"Missing"
2021.semeval-1.41,J09-4005,0,0.0549194,"s and Pereira, 2018; Peterson et al., 2019) Much recent research has explored the question of whether corpora of this type, besides being more 338 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 338–347 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics accurate characterizations of the linguistic reality of language interpretation and image categorization, are also better resources for training nlp and computer vision models, and if so, what is the best way for exploiting disagreements in modeling. Beigman Klebanov and Beigman (2009) used information about disagreements to exclude items on which judgements are unclear (‘hard’ items). In the CrowdTruth project (Aroyo and Welty, 2015; Dumitrache et al., 2019) information about disagreement is used to weigh the items used for training. Plank et al. (2014a) proposed to use the information about disagreement to supplement the gold label during training. Finally, methods were proposed for training directly from the data with disagreements, without first obtaining an aggregated label (Sheng et al., 2008; Rodrigues and Pereira, 2018; Peterson et al., 2019; Uma et al., 2020). Only"
2021.semeval-1.41,N19-1423,0,0.00621773,"t model.10 4 Participating systems Unfortunately, we observed a dramatic difference in the number of participants that signed up to the competition (over 100 groups), the number of groups that participated in the trial phase, and the number of groups that submitted a run for official evaluation.11 Only one group, uor, submitted in the evaluation phase (Osei-Brefo et al., 2021). However, they did submit models for each of the tasks, and did adopt a learning from disagreements approach. pos tagging. For pos tagging, uor developed a novel pos tagging model by fine-tuning the bert language model (Devlin et al., 2019). The (tweet, token) pairs were encoded in the form [cls] Tweeted text [sep] Token [sep] where the ‘[cls]’ token was added for classification and the ‘[sep]’ token separated the tweet from the token under consideration. To learn the class for the token, the learned classification token was passed through a single feed-forward neural network layer with softmax activation. The output of this layer represented the probabilities of the token belonging to each of the 12 classes. To extend this model for crowd learning, uor added an adaptation of the crowd layer from Rodrigues and Pereira (2018). Ra"
2021.semeval-1.41,N19-1224,1,0.89232,"Missing"
2021.semeval-1.41,P11-2008,0,0.593029,"Missing"
2021.semeval-1.41,C16-1177,0,0.0200632,"341 The pos tagging model. The pos tagger is a bilstm (Plank et al., 2016) with additional use of attention over the input word and character embeddings, as used in Uma et al. (2020). The pdis classification model. The model for this task was developed by comparing architectures from two models: a state-of-the-art coreference model and a state-of-the-art is classification model. We combined the mention representation component of Lee et al.’s (2018) coreference resolution system with the mention sorting and non-syntactic feature extraction components of the is classification model proposed by Hou (2016)9 to create a novel is classification model that outperforms Hou (2016) on the pdis corpus. The training parameters were set following Lee et al. (2018). The humour preference learning model. We use as base model for this task Gaussian process preference learning (gppl) with stochastic variational inference, as described and implemented by Simpson and Gurevych (2020). As an input vector to gppl, we first take the mean word embedding of a text, using 300-dimensional word2vec embeddings trained on the Google News corpus (Mikolov et al., 2013). Then, we compute the frequency of each unigram in th"
2021.semeval-1.41,N13-1111,0,0.0272005,"y. Hence, it is a strong baseline for accounting for disagreement among annotators. This same gppl approach set the previous state of the art on the humour dataset (Simpson et al., 2019). The LabelMe image classification model. For this task, we replicated the model from Rodrigues and Pereira (2018). The images were encoded using pretrained cnn layers of the vgg-16 deep neural network (Simonyan et al., 2013). This encoding is passed into a feed-forward neural network layer 9This model was developed for fine-grained information status classification on the isnotes corpus (Markert et al., 2012; Hou et al., 2013). with a relu activated hidden layer with 128 units. A 0.2 dropout is applied to this learned representation which is then passed through a final layer with softmax activation to produce the model’s predictions. The cifar-10 image classification model. The trained model provided for this task is the ResNet34A model (He et al., 2016), a deep residual framework which is one of the best performing systems for the cifar-10 image classification. We made available to participants the publicly available Pytorch implementation of this ResNet model.10 4 Participating systems Unfortunately, we observed"
2021.semeval-1.41,N13-1132,0,0.326957,"Missing"
2021.semeval-1.41,D15-1035,0,0.722999,"ch judgements are unclear (‘hard’ items). In the CrowdTruth project (Aroyo and Welty, 2015; Dumitrache et al., 2019) information about disagreement is used to weigh the items used for training. Plank et al. (2014a) proposed to use the information about disagreement to supplement the gold label during training. Finally, methods were proposed for training directly from the data with disagreements, without first obtaining an aggregated label (Sheng et al., 2008; Rodrigues and Pereira, 2018; Peterson et al., 2019; Uma et al., 2020). Only limited comparisons of these methods have been carried out (Jamison and Gurevych, 2015), and the sparse research landscape remains fragmented; in particular, methods applied in cv have not yet been tested in nlp, and vice versa. The objective of SemEval-2021 Task 12, Learning with Disagreements (Le-wi-Di), was to provide a unified testing framework for learning from disagreements in nlp and cv using datasets containing information about disagreements for interpreting language and classifying images. The expectation being that unifying research on disagreement from different fields may lead to novel insights and impact ai widely. 2 that the crowd learning adaptations of the base"
2021.semeval-1.41,N18-2108,0,0.0415615,"Missing"
2021.semeval-1.41,P12-1084,0,0.036338,"confidence accordingly. Hence, it is a strong baseline for accounting for disagreement among annotators. This same gppl approach set the previous state of the art on the humour dataset (Simpson et al., 2019). The LabelMe image classification model. For this task, we replicated the model from Rodrigues and Pereira (2018). The images were encoded using pretrained cnn layers of the vgg-16 deep neural network (Simonyan et al., 2013). This encoding is passed into a feed-forward neural network layer 9This model was developed for fine-grained information status classification on the isnotes corpus (Markert et al., 2012; Hou et al., 2013). with a relu activated hidden layer with 128 units. A 0.2 dropout is applied to this learned representation which is then passed through a final layer with softmax activation to produce the model’s predictions. The cifar-10 image classification model. The trained model provided for this task is the ResNet34A model (He et al., 2016), a deep residual framework which is one of the best performing systems for the cifar-10 image classification. We made available to participants the publicly available Pytorch implementation of this ResNet model.10 4 Participating systems Unfortun"
2021.semeval-1.41,W16-1706,1,0.843985,"coder disagreement can be genuine—i.e., resulting from debatable, difficult, or linguistic ambiguity—has long been known for anaphora and coreference (Poesio and Artstein, 2005; Versley, 2008; Recasens et al., 2011).1 But in recent years, we have also seen evidence that disagreements among subjects/coders are common with virtually every aspect of language interpretation, from apparently simple aspects such as partof-speech tagging (Plank et al., 2014b), to more 1See also the analysis of disagreements in OntoNotes and word senses in Pradhan et al. (2012), Passonneau et al. (2012), and Martínez Alonso et al. (2016). complex ones like semantic role assignment (Dumitrache et al., 2019), to subjective tasks such as sentiment analysis (Kenyon-Dean et al., 2018), and to the inferences that can be drawn from sentences (Pavlick and Kwiatkowski, 2019). In computer vision, as well, the assumption that gold labels may be specified for items has proven an idealization (Rodrigues and Pereira, 2018)—in fact, possibly even more than for nlp. In many widely used crowdsourced datasets for computer vision, different coders assign equally plausible labels to the same items. The problem of disagreement among coders, inclu"
2021.semeval-1.41,N15-1152,1,0.827054,"ry. 2.2 Evaluation metrics While recent research questions the assumption that a single ‘hard’ label (a gold label) exists for every employed, paid in line with the federal minimum wage. 5http://labelme.csail.mit.edu/Release3.0 6https://github.com/jcpeterson/cifar-10h 3https://github.com/dali-ambiguity 4us-based workers from Amazon Mechanical Turk were 340 item in a dataset, the models proposed for learning from multiple interpretations are still largely evaluated under this assumption, using ‘hard’ measures like accuracy or class-weighted F1 (Sheng et al., 2008; Plank et al., 2014a; Martínez Alonso et al., 2015; Sharmanska et al., 2016; Rodrigues and Pereira, 2018). For reference and comparison reasons, we also evaluate the models produced for this shared task using F1 . However, a way of evaluating models as to their ability to capture disagreement is needed, especially for datasets with substantial extent of disagreement. The simplest ‘soft’ metric of this type is to evaluate ambiguity-aware models by treating the probability distribution of labels they produce as a soft label, and comparing that to the full distribution produced by annotators, using, for example, cross-entropy. This approach was"
2021.semeval-1.41,2021.semeval-1.186,0,0.0350661,"is the ResNet34A model (He et al., 2016), a deep residual framework which is one of the best performing systems for the cifar-10 image classification. We made available to participants the publicly available Pytorch implementation of this ResNet model.10 4 Participating systems Unfortunately, we observed a dramatic difference in the number of participants that signed up to the competition (over 100 groups), the number of groups that participated in the trial phase, and the number of groups that submitted a run for official evaluation.11 Only one group, uor, submitted in the evaluation phase (Osei-Brefo et al., 2021). However, they did submit models for each of the tasks, and did adopt a learning from disagreements approach. pos tagging. For pos tagging, uor developed a novel pos tagging model by fine-tuning the bert language model (Devlin et al., 2019). The (tweet, token) pairs were encoded in the form [cls] Tweeted text [sep] Token [sep] where the ‘[cls]’ token was added for classification and the ‘[sep]’ token separated the tweet from the token under consideration. To learn the class for the token, the learned classification token was passed through a single feed-forward neural network layer with softm"
2021.semeval-1.41,Q14-1025,0,0.0225794,"ld aim to collect all distinct interpretations of an expression (Smyth et al., 1994; Poesio and Artstein, 2005; Aroyo and Welty, 2015; Sharmanska et al., 2016; Plank, 2016; Kenyon-Dean et al., 2018; Firman et al., 2018; Pavlick and Kwiatkowski, 2019). Poesio and Artstein (2005) and Recasens et al. (2012) suggest that the best way to create resources capturing disagreements is by preserving implicit ambiguity—i.e., having multiple annotators label the items, and then keeping all these annotations, not just an aggregated ‘gold standard’. A number of corpora with these characteristics now exist (Passonneau and Carpenter, 2014; Plank et al., 2014a; Dumitrache et al., 2019; Poesio et al., 2019; Rodrigues and Pereira, 2018; Peterson et al., 2019) Much recent research has explored the question of whether corpora of this type, besides being more 338 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 338–347 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics accurate characterizations of the linguistic reality of language interpretation and image categorization, are also better resources for training nlp and computer vision models, and i"
2021.semeval-1.41,Q19-1043,0,0.132281,"n recent years, we have also seen evidence that disagreements among subjects/coders are common with virtually every aspect of language interpretation, from apparently simple aspects such as partof-speech tagging (Plank et al., 2014b), to more 1See also the analysis of disagreements in OntoNotes and word senses in Pradhan et al. (2012), Passonneau et al. (2012), and Martínez Alonso et al. (2016). complex ones like semantic role assignment (Dumitrache et al., 2019), to subjective tasks such as sentiment analysis (Kenyon-Dean et al., 2018), and to the inferences that can be drawn from sentences (Pavlick and Kwiatkowski, 2019). In computer vision, as well, the assumption that gold labels may be specified for items has proven an idealization (Rodrigues and Pereira, 2018)—in fact, possibly even more than for nlp. In many widely used crowdsourced datasets for computer vision, different coders assign equally plausible labels to the same items. The problem of disagreement among coders, including experts, on the classification of noisy image data has arisen in many cv applications. This includes classification of astronomical images (Smyth et al., 1994), medical image classification (Raykar et al., 2010), and numerous ot"
2021.semeval-1.41,petrov-etal-2012-universal,0,0.151453,"Missing"
2021.semeval-1.41,E14-1078,1,0.856087,"st a convenient idealization; virtually every project devoted to large-scale annotation has found that genuine disagreements are widespread. In nlp, that annotator/coder disagreement can be genuine—i.e., resulting from debatable, difficult, or linguistic ambiguity—has long been known for anaphora and coreference (Poesio and Artstein, 2005; Versley, 2008; Recasens et al., 2011).1 But in recent years, we have also seen evidence that disagreements among subjects/coders are common with virtually every aspect of language interpretation, from apparently simple aspects such as partof-speech tagging (Plank et al., 2014b), to more 1See also the analysis of disagreements in OntoNotes and word senses in Pradhan et al. (2012), Passonneau et al. (2012), and Martínez Alonso et al. (2016). complex ones like semantic role assignment (Dumitrache et al., 2019), to subjective tasks such as sentiment analysis (Kenyon-Dean et al., 2018), and to the inferences that can be drawn from sentences (Pavlick and Kwiatkowski, 2019). In computer vision, as well, the assumption that gold labels may be specified for items has proven an idealization (Rodrigues and Pereira, 2018)—in fact, possibly even more than for nlp. In many wide"
2021.semeval-1.41,P14-2083,1,0.835772,"st a convenient idealization; virtually every project devoted to large-scale annotation has found that genuine disagreements are widespread. In nlp, that annotator/coder disagreement can be genuine—i.e., resulting from debatable, difficult, or linguistic ambiguity—has long been known for anaphora and coreference (Poesio and Artstein, 2005; Versley, 2008; Recasens et al., 2011).1 But in recent years, we have also seen evidence that disagreements among subjects/coders are common with virtually every aspect of language interpretation, from apparently simple aspects such as partof-speech tagging (Plank et al., 2014b), to more 1See also the analysis of disagreements in OntoNotes and word senses in Pradhan et al. (2012), Passonneau et al. (2012), and Martínez Alonso et al. (2016). complex ones like semantic role assignment (Dumitrache et al., 2019), to subjective tasks such as sentiment analysis (Kenyon-Dean et al., 2018), and to the inferences that can be drawn from sentences (Pavlick and Kwiatkowski, 2019). In computer vision, as well, the assumption that gold labels may be specified for items has proven an idealization (Rodrigues and Pereira, 2018)—in fact, possibly even more than for nlp. In many wide"
2021.semeval-1.41,P16-2067,1,0.810277,"we briefly discuss the baseline models for each task that we provided. In Section 5, we report the results using these base models and two crowd learning approaches: majority voting and the soft loss method (Peterson et al., 2019; Uma et al., 2020). 7Our competition can be found at https://competitions. codalab.org/competitions/25748. 8This proved unnecessary as the inherent difficulty of the shared task was enough of a deterrent. CodaLab was the designated site for hosting SemEval-2021 competitions.7 Le-wi-Di was run in two main phases: 341 The pos tagging model. The pos tagger is a bilstm (Plank et al., 2016) with additional use of attention over the input word and character embeddings, as used in Uma et al. (2020). The pdis classification model. The model for this task was developed by comparing architectures from two models: a state-of-the-art coreference model and a state-of-the-art is classification model. We combined the mention representation component of Lee et al.’s (2018) coreference resolution system with the mention sorting and non-syntactic feature extraction components of the is classification model proposed by Hou (2016)9 to create a novel is classification model that outperforms Hou"
2021.semeval-1.41,W05-0311,1,0.897441,"assumption that natural language (nl) expressions have a single and clearly identifiable interpretation in a given context, or that images have a preferred labels, still underlies most work in nlp and computer vision. However, there is now plenty of evidence that this assumption is just a convenient idealization; virtually every project devoted to large-scale annotation has found that genuine disagreements are widespread. In nlp, that annotator/coder disagreement can be genuine—i.e., resulting from debatable, difficult, or linguistic ambiguity—has long been known for anaphora and coreference (Poesio and Artstein, 2005; Versley, 2008; Recasens et al., 2011).1 But in recent years, we have also seen evidence that disagreements among subjects/coders are common with virtually every aspect of language interpretation, from apparently simple aspects such as partof-speech tagging (Plank et al., 2014b), to more 1See also the analysis of disagreements in OntoNotes and word senses in Pradhan et al. (2012), Passonneau et al. (2012), and Martínez Alonso et al. (2016). complex ones like semantic role assignment (Dumitrache et al., 2019), to subjective tasks such as sentiment analysis (Kenyon-Dean et al., 2018), and to th"
2021.semeval-1.41,N19-1176,1,0.889311,"Missing"
2021.semeval-1.41,W12-4501,0,0.044054,"genuine disagreements are widespread. In nlp, that annotator/coder disagreement can be genuine—i.e., resulting from debatable, difficult, or linguistic ambiguity—has long been known for anaphora and coreference (Poesio and Artstein, 2005; Versley, 2008; Recasens et al., 2011).1 But in recent years, we have also seen evidence that disagreements among subjects/coders are common with virtually every aspect of language interpretation, from apparently simple aspects such as partof-speech tagging (Plank et al., 2014b), to more 1See also the analysis of disagreements in OntoNotes and word senses in Pradhan et al. (2012), Passonneau et al. (2012), and Martínez Alonso et al. (2016). complex ones like semantic role assignment (Dumitrache et al., 2019), to subjective tasks such as sentiment analysis (Kenyon-Dean et al., 2018), and to the inferences that can be drawn from sentences (Pavlick and Kwiatkowski, 2019). In computer vision, as well, the assumption that gold labels may be specified for items has proven an idealization (Rodrigues and Pereira, 2018)—in fact, possibly even more than for nlp. In many widely used crowdsourced datasets for computer vision, different coders assign equally plausible labels to th"
2021.semeval-1.41,recasens-etal-2012-annotating,0,0.0365301,"tion (Raykar et al., 2010), and numerous others (Sharmanska et al., 2016; Rodrigues and Pereira, 2018; Firman et al., 2018). Many ai researchers have concluded that rather than attempting to eliminate disagreements from annotated corpora, we should preserve them—indeed, some researchers have argued that corpora should aim to collect all distinct interpretations of an expression (Smyth et al., 1994; Poesio and Artstein, 2005; Aroyo and Welty, 2015; Sharmanska et al., 2016; Plank, 2016; Kenyon-Dean et al., 2018; Firman et al., 2018; Pavlick and Kwiatkowski, 2019). Poesio and Artstein (2005) and Recasens et al. (2012) suggest that the best way to create resources capturing disagreements is by preserving implicit ambiguity—i.e., having multiple annotators label the items, and then keeping all these annotations, not just an aggregated ‘gold standard’. A number of corpora with these characteristics now exist (Passonneau and Carpenter, 2014; Plank et al., 2014a; Dumitrache et al., 2019; Poesio et al., 2019; Rodrigues and Pereira, 2018; Peterson et al., 2019) Much recent research has explored the question of whether corpora of this type, besides being more 338 Proceedings of the 15th International Workshop on S"
2021.semeval-1.41,P19-1572,1,0.473071,"d bigram frequencies with the mean word embedding vector to obtain the input vector representation for each short text. The gppl model is trained on pairwise labels from the training set to obtain a ranking function that can be used to score test instances or output pairwise label probabilities. As a Bayesian model, it takes into account sparsity and noise in the crowdsourced training labels, and moderates its confidence accordingly. Hence, it is a strong baseline for accounting for disagreement among annotators. This same gppl approach set the previous state of the art on the humour dataset (Simpson et al., 2019). The LabelMe image classification model. For this task, we replicated the model from Rodrigues and Pereira (2018). The images were encoded using pretrained cnn layers of the vgg-16 deep neural network (Simonyan et al., 2013). This encoding is passed into a feed-forward neural network layer 9This model was developed for fine-grained information status classification on the isnotes corpus (Markert et al., 2012; Hou et al., 2013). with a relu activated hidden layer with 128 units. A 0.2 dropout is applied to this learned representation which is then passed through a final layer with softmax acti"
2021.teachingnlp-1.9,D15-1166,0,0.0580566,"-grams) to their deep learning-based counterparts (dense representations, ‘soft’ ngrams in CNNS and RNN-based encoders). Consequently, the following topics are covered in the lecture: • Introduction to NLP and DL (deep learning), what makes language so difficult; traditional versus neural approaches As key textbook references, I would like to refer the reader to chapters 3-9 of Jurafsky and Martin’s 3rd edition (under development) (Jurafsky and Martin, 2020),6 and Yoav Goldberg’s NLP primer (Goldberg, 2015). Besides these textbooks, key papers include (Kim, 2014) for CNNs on texts, attention (Luong et al., 2015) and Elmo (Peters et al., 2018). • N-gram Language Models, 3 • Feedforward Neural Networks (FFNNs) Complementary notebooks of earlier material This lecture evolved from a series of lectures given earlier, amongst which a short course given in Malta in 2019, and a MSc-level course I taught at the University of Groningen (Language Technology Project). To complement the Keynote slides of the summer school lecture provided here, earlier Jupyter notebooks can be found at the website. These cover a subset of the material above. • What’s the input? Sparse traditional vs dense representations; Bag of"
2021.teachingnlp-1.9,N18-1202,0,0.0127108,"-based counterparts (dense representations, ‘soft’ ngrams in CNNS and RNN-based encoders). Consequently, the following topics are covered in the lecture: • Introduction to NLP and DL (deep learning), what makes language so difficult; traditional versus neural approaches As key textbook references, I would like to refer the reader to chapters 3-9 of Jurafsky and Martin’s 3rd edition (under development) (Jurafsky and Martin, 2020),6 and Yoav Goldberg’s NLP primer (Goldberg, 2015). Besides these textbooks, key papers include (Kim, 2014) for CNNs on texts, attention (Luong et al., 2015) and Elmo (Peters et al., 2018). • N-gram Language Models, 3 • Feedforward Neural Networks (FFNNs) Complementary notebooks of earlier material This lecture evolved from a series of lectures given earlier, amongst which a short course given in Malta in 2019, and a MSc-level course I taught at the University of Groningen (Language Technology Project). To complement the Keynote slides of the summer school lecture provided here, earlier Jupyter notebooks can be found at the website. These cover a subset of the material above. • What’s the input? Sparse traditional vs dense representations; Bag of words (BOW) vs continuous BOW ("
2021.wnut-1.2,D14-1181,0,0.0249609,"were selected for annotation. We release the full filtered dataset (Twitter ids of all 9,000 tweets) for future work on this collection.3 See the appendix for annotation guidelines and data quality. 2.3 3 Methodology This section outlines the models that will be used. 3.1 Naive Bayes We used Naive Bayes with word unigrams (Jurafsky and Martin, 2021) to compare our more complex model to a simple probabilistic baseline. 3.2 CNN We experimented with a Convolutional Neural Network and test both word embeddings derived by word2vec and contextualized BERT embeddings. The CNN is based on the work of Kim (2014). The embedding layer is initialized with pretrained word2vec embeddings of size 400 that are based on Danish Twitter data (available from another WNUT 2021 shared task).4 We also use pre-trained BERT embeddings as static input to the CNN model with a class weighted loss. Both Multilingual BERT (MBERT 5 ) and Danish BERT (DBERT 6 ) embeddings are used because we want to determine which is the best fit for this particular dataset. We extensively tuned our CNN with a hyperparameter search (outlined in 3.4), which resulted in a CNN with 4 different filter sizes 1,3,5, and 7 with Data partitions F"
2021.wnut-1.2,2020.wnut-1.57,0,0.0632014,"Missing"
2021.wnut-1.2,C12-1160,0,0.0652533,"Missing"
C14-1168,I13-1041,0,0.0122948,"Missing"
C14-1168,P11-1040,0,0.0233001,"Missing"
C14-1168,D07-1074,0,0.00844979,"tagging search queries, tag search queries and the associated snippets provided by the search engine, projecting tags from the snippets to the queries, guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more advanced matching of snippets and search queries, giving priority to n-gram matches with larger n. Search queries contain limited contexts, like tweets, but are generally much shorter and exhibit less spelling variation than tweets. In NER, it is common to use gazetteers, but also dictionaries as distant supervision (Kazama and Torisawa, 2007; Cucerzan, 2007). R¨ud et al. (2011) consider using search engines for distant supervision of NER of search queries. Their set-up is very similar to Ganchev et al. (2012), except they do not use click-through data. They use the search engine snippets to generate feature representations rather than projections. Want et al. (2013) also use distant supervision for NER, i.e., Wikipedia page view counts, 1790 applying their model to Twitter data, but their results are considerably below the state of the art. Also, their source of supervision is not linked to the individual tweets in the way mentioned websites are."
C14-1168,P11-1061,0,0.03304,", 2013; Derczynski et al., 2013). Strictly supervised approaches to analyzing Twitter has the weakness that labeled data quickly becomes unrepresentative of what people write on Twitter. This paper presents results using no in-domain labeled data that are significantly better than several offthe-shelf systems, as well as results leveraging a mixture of out-of-domain and in-domain labeled data to reach new highs across several data sets. Type-constrained POS tagging using tag dictionaries has been explored in weakly supervised settings (Li et al., 2012), as well as for cross-language learning (Das and Petrov, 2011; T¨ackstr¨om et al., 2013). Our type constraints in POS tagging come from tag dictionaries, but also from linked websites. The idea of using linked websites as distant supervision is similar in spirit to the idea presented in Ganchev et al. (2012) for search query tagging. Ganchev et al. (2012), considering the problem of POS tagging search queries, tag search queries and the associated snippets provided by the search engine, projecting tags from the snippets to the queries, guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more advanced matching"
C14-1168,W10-2608,0,0.0382342,"Missing"
C14-1168,R13-1026,0,0.0929452,"oriously hard to analyze (Foster et al., 2011; Eisenstein, 2013; Baldwin et al., 2013). The challenges include dealing with variations in spelling, specific conventions for commenting and retweeting, frequent use of abbreviations and emoticons, non-standard syntax, fragmented or mixed language, etc. Gimpel et al. (2011) showed that we can induce POS tagging models with high accuracy on in-sample Twitter data with relatively little annotation effort. Learning taggers for Twitter data from small amounts of labeled data has also been explored by others (Ritter et al., 2011; Owoputi et al., 2013; Derczynski et al., 2013). Hovy et al. (2014), on the other hand, showed that these models overfit their respective samples and suffer severe drops when evaluated on out-of-sample Twitter data, sometimes performing even worse than newswire models. This may be due to drift on Twitter (Eisenstein, 2013) or simply due to the heterogeneous nature of Twitter, which makes small samples biased. So while existing systems perform well on their own (in-sample) data sets, they over-fit the samples they were induced from, and suffer on other (out-of-sample) Twitter data sets. This bias can, at least in theory, be corrected by lea"
C14-1168,N13-1037,0,0.0133621,"Missing"
C14-1168,W10-0713,0,0.0396482,"the universal POS tag set by Petrov et al. (2012). The data sets also differ in a few annotation conventions, e.g., some annotate URLs as NOUN, some as X. Moreover, our newswire tagger baselines tend to get Twitter-specific symbols such as URLs, hashtags and user accounts wrong. Instead of making annotations more consistent across data sets, we follow Ritter et al. (2011) in using a few post-processing rules to deterministically assign Twitter-specific symbols to their correct tags. The major difference between the NER data sets is whether Twitter user accounts are annotated as PER. We follow Finin et al. (2010) in doing so. Unlabeled data We downloaded 200k tweet-website pairs from the Twitter search API over a period of one week in August 2013 by searching for tweets that contain the string http and downloading the content of the websites they linked to. We filter out duplicate tweets and restrict ourselves to websites that contain more than one sentence (after removing boilerplate text, scripts, HTML, etc).13 We also require website and tweet to have at least one matching word that is not a stopword (as defined by the NLTK stopword list).14 Finally we restrict ourselves to pairs where the website"
C14-1168,P05-1045,0,0.0126701,"(2011), in particular features for word tokens, a set of features that check for the presence of hyphens, digits, single quotes, upper/lowercase, 3 character prefix and suffix information. Moreover, we add Brown word cluster features that use 2i for i ∈ 1, ..., 4 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013), which is publicly available.5 We use a pool size of 1000 tweets. We experimented with other pool sizes {500,2000} showing similar performance. The number of iterations i is set on the development data. For NER on websites, we use the Stanford NER system (Finkel et al., 2005)6 with POS tags from the L APOS tagger (Tsuruoka et al., 2011).7 For POS we found it to be superior to use the current POS model for re-tagging websites; for NER it was slightly better to use the Stanford NER tagger and thus off-line NER tagging rather than retagging the websites in every iteration. 3.2 Data In our experiments, we consider two scenarios, sometimes referred to as unsupervised and semisupervised domain adaptation (DA), respectively (Daum´e et al., 2010; Plank, 2011). In unsupervised DA, we assume only (labeled) newswire data, in semi-supervised DA, we assume labeled data from bo"
C14-1168,I11-1100,0,0.0448938,"Missing"
C14-1168,fromreide-etal-2014-crowdsourcing,1,0.118987,"red-task 9 LDC2011T03. 10 http://www.clips.ua.ac.be/conll2003/ner/ 11 http://www.isi.edu/publications/licensed-sw/mace/ 5 1786 et al. (2011). For NER, we simply use the parameters from our POS tagging experiments and thus do not assume to have access to further development data. For both POS tagging and NER, we have three test sets. For POS tagging, the ones used in Foster et al. (2011) (F OSTER -T EST) and Ritter et al. (2011) (R ITTER -T EST),12 as well as the one presented in Hovy et al. (2014) (H OVY-T EST). For NER, we use the data set from Ritter et al. (2011) and the two data sets from Fromreide et al. (2014) as test sets. One is a manual correction of a held-out portion of F ININ -T RAIN, named F ININ -T EST; the other one is referred to as F ROMREIDE -T EST. Since the different POS corpora use different tag sets, we map all of them corpora onto the universal POS tag set by Petrov et al. (2012). The data sets also differ in a few annotation conventions, e.g., some annotate URLs as NOUN, some as X. Moreover, our newswire tagger baselines tend to get Twitter-specific symbols such as URLs, hashtags and user accounts wrong. Instead of making annotations more consistent across data sets, we follow Rit"
C14-1168,P12-2047,1,0.432581,"that are significantly better than several offthe-shelf systems, as well as results leveraging a mixture of out-of-domain and in-domain labeled data to reach new highs across several data sets. Type-constrained POS tagging using tag dictionaries has been explored in weakly supervised settings (Li et al., 2012), as well as for cross-language learning (Das and Petrov, 2011; T¨ackstr¨om et al., 2013). Our type constraints in POS tagging come from tag dictionaries, but also from linked websites. The idea of using linked websites as distant supervision is similar in spirit to the idea presented in Ganchev et al. (2012) for search query tagging. Ganchev et al. (2012), considering the problem of POS tagging search queries, tag search queries and the associated snippets provided by the search engine, projecting tags from the snippets to the queries, guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more advanced matching of snippets and search queries, giving priority to n-gram matches with larger n. Search queries contain limited contexts, like tweets, but are generally much shorter and exhibit less spelling variation than tweets. In NER, it is common to use gazet"
C14-1168,P11-2008,0,0.147708,"Missing"
C14-1168,N13-1132,1,0.602877,"n our experiments, we use the SANCL shared task8 splits of the OntoNotes 4.0 distribution of the WSJ newswire annotations as newswire training data for POS tagging.9 For NER, we use the CoNLL 2003 data sets of annotated newswire from the Reuters corpus.10 The in-domain training POS data comes from Gimpel et al. (2011), and the in-domain NER data comes from Finin et al. (2010) (F ININ -T RAIN). These data sets are added to the newswire sets when doing semi-supervised DA. Note that for NER, we thus do not rely on expertannotated Twitter data, but rely on crowdsourced annotations. We use MACE11 (Hovy et al., 2013) to resolve inter-annotator conflicts between turkers (50 iterations, 10 restarts, no confidence threshold). We believe relying on crowdsourced annotations makes our set-up more robust across different samples of Twitter data. Development and test data. We use several evaluation sets for both tasks to prevent overfitting to a specific sample. We use the (out-of-sample) development data sets from Ritter et al. (2011) and Foster 4 http://www.chokkan.org/software/crfsuite/ http://www.ark.cs.cmu.edu/TweetNLP/ 6 http://http://nlp.stanford.edu/software/CRF-NER.shtml 7 http://www.logos.ic.i.u-tokyo.a"
C14-1168,hovy-etal-2014-pos,1,0.895368,"Foster et al., 2011; Eisenstein, 2013; Baldwin et al., 2013). The challenges include dealing with variations in spelling, specific conventions for commenting and retweeting, frequent use of abbreviations and emoticons, non-standard syntax, fragmented or mixed language, etc. Gimpel et al. (2011) showed that we can induce POS tagging models with high accuracy on in-sample Twitter data with relatively little annotation effort. Learning taggers for Twitter data from small amounts of labeled data has also been explored by others (Ritter et al., 2011; Owoputi et al., 2013; Derczynski et al., 2013). Hovy et al. (2014), on the other hand, showed that these models overfit their respective samples and suffer severe drops when evaluated on out-of-sample Twitter data, sometimes performing even worse than newswire models. This may be due to drift on Twitter (Eisenstein, 2013) or simply due to the heterogeneous nature of Twitter, which makes small samples biased. So while existing systems perform well on their own (in-sample) data sets, they over-fit the samples they were induced from, and suffer on other (out-of-sample) Twitter data sets. This bias can, at least in theory, be corrected by learning from additiona"
C14-1168,P07-1034,0,0.0634483,"cross different samples of tweets than existing approaches. We consider both the scenario where a small sample of labeled Twitter data is available, and the scenario where only newswire data is available. Training on a mixture of out-of-domain (WSJ) and in-domain (Twitter) data as well as unlabeled data, we get the best reported results in the literature for both POS tagging and NER on Twitter. Our tagging models are publicly available at https://bitbucket.org/lowlands/ttagger-nsd 2 Tagging with not-so-distant supervision We assume that our labeled data is highly biased by domain differences (Jiang and Zhai, 2007), population drift (Hand, 2006), or by our sample size simply being too small. To correct this bias, we want to use unlabeled Twitter data. It is well-known that semi-supervised learning algorithms such as self-training sometimes effectively correct model biases (McClosky et al., 2006; Huang et al., 2009). This paper presents an augmented self-training algorithm that corrects model bias by exploiting unlabeled data and not-so-distant supervision. More specifically, the idea is to use hyperlinks to condition tagging decisions in tweets on a richer linguistic context than what is available in th"
C14-1168,P11-1016,0,0.0902879,"Missing"
C14-1168,D07-1073,0,0.0341914,"sidering the problem of POS tagging search queries, tag search queries and the associated snippets provided by the search engine, projecting tags from the snippets to the queries, guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more advanced matching of snippets and search queries, giving priority to n-gram matches with larger n. Search queries contain limited contexts, like tweets, but are generally much shorter and exhibit less spelling variation than tweets. In NER, it is common to use gazetteers, but also dictionaries as distant supervision (Kazama and Torisawa, 2007; Cucerzan, 2007). R¨ud et al. (2011) consider using search engines for distant supervision of NER of search queries. Their set-up is very similar to Ganchev et al. (2012), except they do not use click-through data. They use the search engine snippets to generate feature representations rather than projections. Want et al. (2013) also use distant supervision for NER, i.e., Wikipedia page view counts, 1790 applying their model to Twitter data, but their results are considerably below the state of the art. Also, their source of supervision is not linked to the individual tweets in the way mentio"
C14-1168,D12-1127,0,0.0865368,"Missing"
C14-1168,N06-1020,0,0.0511572,"l as unlabeled data, we get the best reported results in the literature for both POS tagging and NER on Twitter. Our tagging models are publicly available at https://bitbucket.org/lowlands/ttagger-nsd 2 Tagging with not-so-distant supervision We assume that our labeled data is highly biased by domain differences (Jiang and Zhai, 2007), population drift (Hand, 2006), or by our sample size simply being too small. To correct this bias, we want to use unlabeled Twitter data. It is well-known that semi-supervised learning algorithms such as self-training sometimes effectively correct model biases (McClosky et al., 2006; Huang et al., 2009). This paper presents an augmented self-training algorithm that corrects model bias by exploiting unlabeled data and not-so-distant supervision. More specifically, the idea is to use hyperlinks to condition tagging decisions in tweets on a richer linguistic context than what is available in the tweets. This semi-supervised approach gives state-of-the-art performance across available Twitter POS and NER data sets. The overall semi-supervised learning algorithm is presented in Figure 1. The aim is to correct model bias by predicting tag sequences on small pools of unlabeled"
C14-1168,P09-1113,0,0.139295,"his is the hypothesis we explore in this paper. We present a semi-supervised learning method that does not require additional labeled in-domain data to correct sample bias, but rather leverages pools of unlabeled Twitter data. However, since taggers trained on newswire perform poorly on Twitter data, we need additional guidance when utilizing the unlabeled data. This paper proposes distant supervision to help our models learn from unlabeled data. Distant supervision is a weakly supervised learning paradigm, where a knowledge resource is exploited to gather (possible noisy) training instances (Mintz et al., 2009). Our basic idea is to can use linguistic analysis of linked websites as a novel kind of distant supervision for learning how to analyze tweets. We explore standard sources of distant supervision, such as Wiktionary for POS tagging, but we also propose to use the linked websites of tweets with URLs as supervision. The intuition is that we can use websites to provide a richer linguistic context for our tagging decisions. We exploit the fact that tweets with URLs provide a one-to-one map between an unlabeled instance and the source of supervision, making this This work is licensed under a Creati"
C14-1168,N13-1039,0,0.158057,"Missing"
C14-1168,petrov-etal-2012-universal,1,0.678346,"gging and NER, we have three test sets. For POS tagging, the ones used in Foster et al. (2011) (F OSTER -T EST) and Ritter et al. (2011) (R ITTER -T EST),12 as well as the one presented in Hovy et al. (2014) (H OVY-T EST). For NER, we use the data set from Ritter et al. (2011) and the two data sets from Fromreide et al. (2014) as test sets. One is a manual correction of a held-out portion of F ININ -T RAIN, named F ININ -T EST; the other one is referred to as F ROMREIDE -T EST. Since the different POS corpora use different tag sets, we map all of them corpora onto the universal POS tag set by Petrov et al. (2012). The data sets also differ in a few annotation conventions, e.g., some annotate URLs as NOUN, some as X. Moreover, our newswire tagger baselines tend to get Twitter-specific symbols such as URLs, hashtags and user accounts wrong. Instead of making annotations more consistent across data sets, we follow Ritter et al. (2011) in using a few post-processing rules to deterministically assign Twitter-specific symbols to their correct tags. The major difference between the NER data sets is whether Twitter user accounts are annotated as PER. We follow Finin et al. (2010) in doing so. Unlabeled data W"
C14-1168,N10-1021,0,0.0862976,"Missing"
C14-1168,D11-1141,0,0.836453,"ze named entities. Tweets, however, are notoriously hard to analyze (Foster et al., 2011; Eisenstein, 2013; Baldwin et al., 2013). The challenges include dealing with variations in spelling, specific conventions for commenting and retweeting, frequent use of abbreviations and emoticons, non-standard syntax, fragmented or mixed language, etc. Gimpel et al. (2011) showed that we can induce POS tagging models with high accuracy on in-sample Twitter data with relatively little annotation effort. Learning taggers for Twitter data from small amounts of labeled data has also been explored by others (Ritter et al., 2011; Owoputi et al., 2013; Derczynski et al., 2013). Hovy et al. (2014), on the other hand, showed that these models overfit their respective samples and suffer severe drops when evaluated on out-of-sample Twitter data, sometimes performing even worse than newswire models. This may be due to drift on Twitter (Eisenstein, 2013) or simply due to the heterogeneous nature of Twitter, which makes small samples biased. So while existing systems perform well on their own (in-sample) data sets, they over-fit the samples they were induced from, and suffer on other (out-of-sample) Twitter data sets. This b"
C14-1168,P11-1097,0,0.0422016,"Missing"
C14-1168,Q13-1001,1,0.89785,"Missing"
C14-1168,W11-0328,0,0.0169234,"tures that check for the presence of hyphens, digits, single quotes, upper/lowercase, 3 character prefix and suffix information. Moreover, we add Brown word cluster features that use 2i for i ∈ 1, ..., 4 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013), which is publicly available.5 We use a pool size of 1000 tweets. We experimented with other pool sizes {500,2000} showing similar performance. The number of iterations i is set on the development data. For NER on websites, we use the Stanford NER system (Finkel et al., 2005)6 with POS tags from the L APOS tagger (Tsuruoka et al., 2011).7 For POS we found it to be superior to use the current POS model for re-tagging websites; for NER it was slightly better to use the Stanford NER tagger and thus off-line NER tagging rather than retagging the websites in every iteration. 3.2 Data In our experiments, we consider two scenarios, sometimes referred to as unsupervised and semisupervised domain adaptation (DA), respectively (Daum´e et al., 2010; Plank, 2011). In unsupervised DA, we assume only (labeled) newswire data, in semi-supervised DA, we assume labeled data from both domains, besides unlabeled target data, but the amount of l"
C14-1168,W03-0419,0,\N,Missing
C14-1168,D10-1002,0,\N,Missing
C14-3005,W06-1615,0,0.0791074,"tion or instance weighting (Wang et al., 2013). In language technology, the bias correction problem is harder. In the case of elections, you have a single output variable and various demographic observed variables. All values taken by discrete variables at test time can be assumed to have been observed, and all values observed at training time can be assumed to be seen at test time. In language technology, we typically have several features only seen in training data and several features only seen in test data. The latter observation has led to interest in bridging unseen words to known ones (Blitzer et al., 2006; Turian et al., 2010), while the former has led to the development of learning algorithms that prevent feature swamping (Sutton et al., 2006), i.e., that very predictive features prevents weights associated with less predictive, correlated features from being updated. Note, however, that post-stratification (Smith, 1988) may prevent feature swamping, and that predictive approaches to bias correction (Royall, 1988) may solve both problems. Instance weighting (Shimodaira, 2000), which is a generalization of post-stratificiation, has received some interest in language technology (Jiang and Zhai,"
C14-3005,E03-1068,0,0.0275044,"taggers for English Twitter as our running example. Label Bias In most annotation projects, there is an initial stage, where the project managers compare annotators’ performance, compute agreement scores, select reliable annotators, adjudicate, and elaborate on annotation guidelines, if necessary. Such procedures are considered necessary to correct for the individual biases of the annotators (label bias). However, this is typically only for the first batches of data, and it is well-known that even some of the most widely used annotated corpora (such as the Penn Treebank) contain many errors (Dickinson and Meurers, 2003) in the form of inconsistent annotations of the same n-grams. Obviously, using non-expert annotators, e.g., through crowd-sourcing platforms, increase the label bias considerably. One way to reduce this bias involves collecting several annotations for each datapoint and averaging over them, which is often feasible because of the low cost of non-expert annotation. This is called majority voting and is analogous to using ensembles of models to obtain more robust systems. In the tutorial we discuss alternatives to averaging over annotators, incl., using EM to estimate annotator confidence (Hovy e"
C14-3005,I11-1100,0,0.0552098,"Missing"
C14-3005,N13-1132,1,0.821754,"2003) in the form of inconsistent annotations of the same n-grams. Obviously, using non-expert annotators, e.g., through crowd-sourcing platforms, increase the label bias considerably. One way to reduce this bias involves collecting several annotations for each datapoint and averaging over them, which is often feasible because of the low cost of non-expert annotation. This is called majority voting and is analogous to using ensembles of models to obtain more robust systems. In the tutorial we discuss alternatives to averaging over annotators, incl., using EM to estimate annotator confidence (Hovy et al., 2013), and joint learning of annotator competence and model parameters (Raykar and Yu, 2012). Bias in Ground Truth In annotation projects, we use inter-annotator agreement measures and annotation guidelines to ensure consistent annotations. However, annotation guidelines often make linguistically debatable and even somewhat arbitrary decisions, and inter-annotator agreement is often less than perfect. Some annotators, for example, may annotate socialin social media as a noun, others may annotate it as an adjective. In this part of the tutorial, we discuss how to correct for the bias introduced by a"
C14-3005,P07-1034,0,0.0177436,"er et al., 2006; Turian et al., 2010), while the former has led to the development of learning algorithms that prevent feature swamping (Sutton et al., 2006), i.e., that very predictive features prevents weights associated with less predictive, correlated features from being updated. Note, however, that post-stratification (Smith, 1988) may prevent feature swamping, and that predictive approaches to bias correction (Royall, 1988) may solve both problems. Instance weighting (Shimodaira, 2000), which is a generalization of post-stratificiation, has received some interest in language technology (Jiang and Zhai, 2007; Foster et al., 2011), but most work on domain adaptation in language technology has focused on predictive This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 11 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts, pages 11–13, Dublin, Ireland, August 23-29 2014. approaches, i.e., semi-supervised learning (Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; McClosky et al., 2"
C14-3005,N10-1004,0,0.0227936,"ang and Zhai, 2007; Foster et al., 2011), but most work on domain adaptation in language technology has focused on predictive This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 11 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts, pages 11–13, Dublin, Ireland, August 23-29 2014. approaches, i.e., semi-supervised learning (Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; McClosky et al., 2010; Chen et al., 2011). Selection bias introduces a bias in P (X). Note that, in theory, this should not hurt discriminative algorithms trying to estimate P (Y |X), without estimating P (X), but in practice it still does. The inductive bias of our algorithms and the size our samples make our models sensitive to selection bias (Zadrozny, 2004). Predictive approaches try to correct this bias by adding more (pseudo-labeled) data to the training sample, while post-stratification and instance weighting reweigh the data to make P (X) similar to the distribution observed in the population. As mentioned"
C14-3005,C14-1168,1,0.824408,"make P (X) similar to the distribution observed in the population. As mentioned, this will never solve the problem with unseen features, since you cannot up-weigh a null feature. Semi-supervised learning can correct modest selection bias, but if the domain gap is too wide, our initial predictions in the target domain will be poor, and semi-supervised learning is likely to increase bias rather than decrease it. However, recent work has shown that semi-supervised learning can be combined with distant supervision and correct bias in cases where semi-supervised learning algorithms typically fail (Plank et al., 2014). In the tutorial we illustrate these different approaches to selection bias correction, with discriminative learning of POS taggers for English Twitter as our running example. Label Bias In most annotation projects, there is an initial stage, where the project managers compare annotators’ performance, compute agreement scores, select reliable annotators, adjudicate, and elaborate on annotation guidelines, if necessary. Such procedures are considered necessary to correct for the individual biases of the annotators (label bias). However, this is typically only for the first batches of data, and"
C14-3005,P07-1078,0,0.0272566,"has received some interest in language technology (Jiang and Zhai, 2007; Foster et al., 2011), but most work on domain adaptation in language technology has focused on predictive This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 11 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts, pages 11–13, Dublin, Ireland, August 23-29 2014. approaches, i.e., semi-supervised learning (Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; McClosky et al., 2010; Chen et al., 2011). Selection bias introduces a bias in P (X). Note that, in theory, this should not hurt discriminative algorithms trying to estimate P (Y |X), without estimating P (X), but in practice it still does. The inductive bias of our algorithms and the size our samples make our models sensitive to selection bias (Zadrozny, 2004). Predictive approaches try to correct this bias by adding more (pseudo-labeled) data to the training sample, while post-stratification and instance weighting reweigh the data to make P (X) similar to the distri"
C14-3005,D07-1111,0,0.0319065,"language technology (Jiang and Zhai, 2007; Foster et al., 2011), but most work on domain adaptation in language technology has focused on predictive This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 11 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Tutorial Abstracts, pages 11–13, Dublin, Ireland, August 23-29 2014. approaches, i.e., semi-supervised learning (Reichart and Rappoport, 2007; Sagae and Tsujii, 2007; McClosky et al., 2010; Chen et al., 2011). Selection bias introduces a bias in P (X). Note that, in theory, this should not hurt discriminative algorithms trying to estimate P (Y |X), without estimating P (X), but in practice it still does. The inductive bias of our algorithms and the size our samples make our models sensitive to selection bias (Zadrozny, 2004). Predictive approaches try to correct this bias by adding more (pseudo-labeled) data to the training sample, while post-stratification and instance weighting reweigh the data to make P (X) similar to the distribution observed in the p"
C14-3005,N06-1012,0,0.0210595,"ave a single output variable and various demographic observed variables. All values taken by discrete variables at test time can be assumed to have been observed, and all values observed at training time can be assumed to be seen at test time. In language technology, we typically have several features only seen in training data and several features only seen in test data. The latter observation has led to interest in bridging unseen words to known ones (Blitzer et al., 2006; Turian et al., 2010), while the former has led to the development of learning algorithms that prevent feature swamping (Sutton et al., 2006), i.e., that very predictive features prevents weights associated with less predictive, correlated features from being updated. Note, however, that post-stratification (Smith, 1988) may prevent feature swamping, and that predictive approaches to bias correction (Royall, 1988) may solve both problems. Instance weighting (Shimodaira, 2000), which is a generalization of post-stratificiation, has received some interest in language technology (Jiang and Zhai, 2007; Foster et al., 2011), but most work on domain adaptation in language technology has focused on predictive This work is licenced under a"
C14-3005,P10-1040,0,0.0127431,"ting (Wang et al., 2013). In language technology, the bias correction problem is harder. In the case of elections, you have a single output variable and various demographic observed variables. All values taken by discrete variables at test time can be assumed to have been observed, and all values observed at training time can be assumed to be seen at test time. In language technology, we typically have several features only seen in training data and several features only seen in test data. The latter observation has led to interest in bridging unseen words to known ones (Blitzer et al., 2006; Turian et al., 2010), while the former has led to the development of learning algorithms that prevent feature swamping (Sutton et al., 2006), i.e., that very predictive features prevents weights associated with less predictive, correlated features from being updated. Note, however, that post-stratification (Smith, 1988) may prevent feature swamping, and that predictive approaches to bias correction (Royall, 1988) may solve both problems. Instance weighting (Shimodaira, 2000), which is a generalization of post-stratificiation, has received some interest in language technology (Jiang and Zhai, 2007; Foster et al.,"
C16-1059,P12-2073,0,0.0798809,"ation studies. In fact, most prior work that uses keystroke logs focuses on experimental research. For example, Hanoulle et al. (2015) study whether a bilingual glossary reduces the working time of professional translators. They consider pause 616 durations before terms extracted from keystroke logs and find that a bilingual glossary in the translation process of documentaries reduces the translators’ workload. Other translation research has combined eye-tracking data with keystroke logs to study the translation process (Carl et al., 2016). An analysis of users’ typing behavior was studied by Baba and Suzuki (2012). They collect keystroke logs of online users describing images to measure spelling difficulty. They analyzed corrected and uncorrected spelling mistakes in Japanese and English and found that spelling errors related to phonetic problems remain mostly unnoticed. Goodkind and Rosenberg (2015) is the only study prior to us that use keystroke loggings in NLP. In particular, they investigate the relationship between pre-word pauses and multi-word expressions and found within MWE pauses vary depending on cognitive task. We take a novel approach and learn keystroke patterns and use them to inform sh"
C16-1059,D15-1041,0,0.0303477,"tterns help to inform NLP (Barrett and Søgaard, 2015; Klerke et al., 2016). We believe this is also the case for biometric keystroke logging data. 3 Tagging with bi-LSTMs We draw on the recent success of bi-directional recurrent neural network (bi-RNNs) (Graves and Schmidhuber, 2005), in particular Long Short-Term Memory (LSTM) models (Hochreiter and Schmidhuber, 1997). They read the input sequences twice, in both directions. Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015). For further details, see Goldberg (2015) and Cho (2015). 3.1 Bidirectional Long-Short Term Memory Models Our model is a a hierarchical bi-LSTM as illustrated in Figure 5. It takes as input word embeddings w ~ concatenated with character embeddings obtained from the last two states (forward, backward) of running a lower-level bi-LSTM on the characters. Adding character representations as additional information has been shown to be effective for a number of tasks, including parsing and tagging (Ballesteros et al., 2015; Gillick et al., 2015; P"
C16-1059,W15-2401,0,0.442803,"ween successive keystrokes and their duration reflect the unique typing behavior of a person. Keystroke logs, the recordings of a user’s typing dynamics, are studied mostly in cognitive writing and translation process research to gain insights into the cognitive load involved in the writing process. However, until now this source has not yet been explored to inform NLP models. Very recent work has shown that cognitive processing data carries valuable signal for NLP. For instance, eye tracking data can inform sentence compression (Klerke et al., 2016) and gaze is predictive for part-of-speech (Barrett and Søgaard, 2015; Barrett et al., 2016). Keystroke logs have the distinct advantage over other cognitive modalities like eye tracking or brain scanning, that they are readily available and can be harvested easily, because they do not rely on any special equipment beyond a keyboard. Moreover, they are non-intrusive, inexpensive, and have the potential to offer continuous adaptation to specific users. Imagine integrating keystroke logging into (online) text processing tools. We hypothesize that keystroke logs carry syntactic signal. Writing time between words can be seen as proxy of the planning process involve"
C16-1059,J81-4005,0,0.758026,"Missing"
C16-1059,P15-1033,0,0.0217688,"shown that gaze patterns help to inform NLP (Barrett and Søgaard, 2015; Klerke et al., 2016). We believe this is also the case for biometric keystroke logging data. 3 Tagging with bi-LSTMs We draw on the recent success of bi-directional recurrent neural network (bi-RNNs) (Graves and Schmidhuber, 2005), in particular Long Short-Term Memory (LSTM) models (Hochreiter and Schmidhuber, 1997). They read the input sequences twice, in both directions. Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015). For further details, see Goldberg (2015) and Cho (2015). 3.1 Bidirectional Long-Short Term Memory Models Our model is a a hierarchical bi-LSTM as illustrated in Figure 5. It takes as input word embeddings w ~ concatenated with character embeddings obtained from the last two states (forward, backward) of running a lower-level bi-LSTM on the characters. Adding character representations as additional information has been shown to be effective for a number of tasks, including parsing and tagging (Ballesteros et al., 201"
C16-1059,I11-1100,0,0.0719863,"Missing"
C16-1059,W15-0914,0,0.115833,"Missing"
C16-1059,Q16-1023,0,0.0322954,"(Barrett and Søgaard, 2015; Klerke et al., 2016). We believe this is also the case for biometric keystroke logging data. 3 Tagging with bi-LSTMs We draw on the recent success of bi-directional recurrent neural network (bi-RNNs) (Graves and Schmidhuber, 2005), in particular Long Short-Term Memory (LSTM) models (Hochreiter and Schmidhuber, 1997). They read the input sequences twice, in both directions. Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015). For further details, see Goldberg (2015) and Cho (2015). 3.1 Bidirectional Long-Short Term Memory Models Our model is a a hierarchical bi-LSTM as illustrated in Figure 5. It takes as input word embeddings w ~ concatenated with character embeddings obtained from the last two states (forward, backward) of running a lower-level bi-LSTM on the characters. Adding character representations as additional information has been shown to be effective for a number of tasks, including parsing and tagging (Ballesteros et al., 2015; Gillick et al., 2015; Plank et al., 2016). In more deta"
C16-1059,N16-1179,0,0.235037,"a user’s typing pattern. When a person types, the latencies between successive keystrokes and their duration reflect the unique typing behavior of a person. Keystroke logs, the recordings of a user’s typing dynamics, are studied mostly in cognitive writing and translation process research to gain insights into the cognitive load involved in the writing process. However, until now this source has not yet been explored to inform NLP models. Very recent work has shown that cognitive processing data carries valuable signal for NLP. For instance, eye tracking data can inform sentence compression (Klerke et al., 2016) and gaze is predictive for part-of-speech (Barrett and Søgaard, 2015; Barrett et al., 2016). Keystroke logs have the distinct advantage over other cognitive modalities like eye tracking or brain scanning, that they are readily available and can be harvested easily, because they do not rely on any special equipment beyond a keyboard. Moreover, they are non-intrusive, inexpensive, and have the potential to offer continuous adaptation to specific users. Imagine integrating keystroke logging into (online) text processing tools. We hypothesize that keystroke logs carry syntactic signal. Writing ti"
C16-1059,D12-1127,0,0.0952565,"Missing"
C16-1059,D15-1176,0,0.0238621,"iosyncratic (Kanan et al., 2015). Nevertheless it has been shown that gaze patterns help to inform NLP (Barrett and Søgaard, 2015; Klerke et al., 2016). We believe this is also the case for biometric keystroke logging data. 3 Tagging with bi-LSTMs We draw on the recent success of bi-directional recurrent neural network (bi-RNNs) (Graves and Schmidhuber, 2005), in particular Long Short-Term Memory (LSTM) models (Hochreiter and Schmidhuber, 1997). They read the input sequences twice, in both directions. Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015). For further details, see Goldberg (2015) and Cho (2015). 3.1 Bidirectional Long-Short Term Memory Models Our model is a a hierarchical bi-LSTM as illustrated in Figure 5. It takes as input word embeddings w ~ concatenated with character embeddings obtained from the last two states (forward, backward) of running a lower-level bi-LSTM on the characters. Adding character representations as additional information has been shown to be effective for a number of tas"
C16-1059,D15-1168,0,0.0324997,"ke et al., 2016). We believe this is also the case for biometric keystroke logging data. 3 Tagging with bi-LSTMs We draw on the recent success of bi-directional recurrent neural network (bi-RNNs) (Graves and Schmidhuber, 2005), in particular Long Short-Term Memory (LSTM) models (Hochreiter and Schmidhuber, 1997). They read the input sequences twice, in both directions. Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015). For further details, see Goldberg (2015) and Cho (2015). 3.1 Bidirectional Long-Short Term Memory Models Our model is a a hierarchical bi-LSTM as illustrated in Figure 5. It takes as input word embeddings w ~ concatenated with character embeddings obtained from the last two states (forward, backward) of running a lower-level bi-LSTM on the characters. Adding character representations as additional information has been shown to be effective for a number of tasks, including parsing and tagging (Ballesteros et al., 2015; Gillick et al., 2015; Plank et al., 2016). In more detail, our model is a"
C16-1059,E14-1078,1,0.850243,"further unlabeled data is considered. Datasets An overview of the syntactic datasets considered in this paper is given in Table 3. For chunking, we use the original CoNLL data (Tjong Kim Sang and Buchholz, 2000) from WSJ (WSJ sections 15-18 as training data and section 20 as test data, containing 8936 and 2012 sentences, respectively).5 For testing we take out-of-domain data whenever available, to test the adaptability of the method to noisy out-of-domain data. For chunking we use Twitter data from Ritter (2011) (all, 2364 tweets) and Foster et al. (2011) (250 sentences), converted to chunks (Plank et al., 2014). The CCG supertagging data also comes from WSJ (39604 training and 2407 test sentences). We unfortunately do not have access to out-of-domain test data, hence use the CCG tagging test set. sentences C O NLL 2000 F OSTER R ITTER CCG T RAIN D EV T EST 8936 – – – 269 – 2012 250 2364 39604 1913 2407 Table 3: Statistics on the data sets The keystroke logging data stems from students taking an actual test on spreadsheet modeling in a university course (Stewart et al., 2011; Monaco et al., 2013). The advantage of this dataset is that it contains free-text input.6 We used data from 38 users,7 which p"
C16-1059,P16-2067,1,0.855476,"5; Kiperwasser and Goldberg, 2016; Liu et al., 2015). For further details, see Goldberg (2015) and Cho (2015). 3.1 Bidirectional Long-Short Term Memory Models Our model is a a hierarchical bi-LSTM as illustrated in Figure 5. It takes as input word embeddings w ~ concatenated with character embeddings obtained from the last two states (forward, backward) of running a lower-level bi-LSTM on the characters. Adding character representations as additional information has been shown to be effective for a number of tasks, including parsing and tagging (Ballesteros et al., 2015; Gillick et al., 2015; Plank et al., 2016). In more detail, our model is a context bi-LSTM taking as input word embeddings w. ~ Character embeddings ~c are incorporated via a hierarchical bi-LSTM using a sequence bi-LSTM at the lower level (Ballesteros et al., 2015; Plank et al., 2016). The character representation is concatenated with the (learned) word embeddings w ~ to form the input to the context bi-LSTM at the upper layers. For the hidden layers, we use stacked LSTMs with h=3 layers. The 3-layer bi-LSTM and lower-level character bi-LSTM represents the shared structure between tasks. From the topmost (h=3) layer labels for the di"
C16-1059,P16-2038,0,0.116217,"ns were obtained by looking up the possible tag of a token in English wiktionary (Li et al., 2012). 613 task (chunking or CCG tagging) is represented by the solid arrow, the auxiliary task (keystroke logs) is indicated by the dashed arrow. During training, we randomly sample a task and instance, and backpropagate the loss of the current instance through the shared deep network. In this way, we learn a joint model from distinct sources. Note that we also experimented with predicting the pause durations at lower levels (h=1), motivated by having lower-level tasks at lower layers in the network (Søgaard and Goldberg, 2016), however, we found the setup with both tasks at the outer layer more robust. Predicting all tasks at the outermost layer is the most commonly used form of multi-task learning in neural networks (Caruana, 1998; Collobert et al., 2011). 4 Experiments We implement our model in CNN/pycnn.4 For all experiments, we use the same hyperparameters, set on a held-out portion of the CoNLL 2000 data, i.e., SGD with cross-entropy loss, no mini-batches, 30 epochs, default learning rate (0.1), 64 dimensions for word embeddings, 100 for character embeddings, random initialization for all embeddings, 100 hidde"
C16-1059,W14-1601,1,0.731056,"taking an actual test on spreadsheet modeling in a university course (Stewart et al., 2011; Monaco et al., 2013). The advantage of this dataset is that it contains free-text input.6 We used data from 38 users,7 which produced on average 250 sentences. The data totals to 7699 sentences. To evaluate our models we use standard evaluation measures computed with conlleval.pl with default parameters, i.e., we report F1 on chunks and accuracy on CCG tags. Statistical significance is computed using the approximate randomization test (Noreen, 1989) using i = 1000 iterations and p-values are reported (Søgaard et al., 2014). 4.1 Results Baseline model Both or baseline models are comparable to prior work, while being simpler. The results are summarized in Table 4. Our chunking baseline achieves an F 1 of 93.21 on CoNLL, compared to the F 1 of 93.88 of Suzuki and Isozaki (2008), who use a CRF and gold POS tags. We do not use any POS information. A similar bi-LSTM achieves 93.64 (Huang et al., 2015), however, additionally uses 4 https://github.com/clab/cnn http://www.cnts.ua.ac.be/conll2000/chunking/ 6 In contrast to http://www.casmacat.eu/ data that logs revisions from MT post-editing. 7 Disregarding users due to"
C16-1059,P08-1076,0,0.035141,"The data totals to 7699 sentences. To evaluate our models we use standard evaluation measures computed with conlleval.pl with default parameters, i.e., we report F1 on chunks and accuracy on CCG tags. Statistical significance is computed using the approximate randomization test (Noreen, 1989) using i = 1000 iterations and p-values are reported (Søgaard et al., 2014). 4.1 Results Baseline model Both or baseline models are comparable to prior work, while being simpler. The results are summarized in Table 4. Our chunking baseline achieves an F 1 of 93.21 on CoNLL, compared to the F 1 of 93.88 of Suzuki and Isozaki (2008), who use a CRF and gold POS tags. We do not use any POS information. A similar bi-LSTM achieves 93.64 (Huang et al., 2015), however, additionally uses 4 https://github.com/clab/cnn http://www.cnts.ua.ac.be/conll2000/chunking/ 6 In contrast to http://www.casmacat.eu/ data that logs revisions from MT post-editing. 7 Disregarding users due to issues with logging (Stewart et al., 2011). 5 614 Chunking F1 Our model Suzuki and Isozaki (2008) 93.21 93.88 CCG tagging Our model Xu et al. (2015) Accuracy 92.41 93.00 Table 4: Baseline model, comparison to existing systems POS embeddings. Our baseline CC"
C16-1059,W00-0726,0,0.0775622,"eters, set on a held-out portion of the CoNLL 2000 data, i.e., SGD with cross-entropy loss, no mini-batches, 30 epochs, default learning rate (0.1), 64 dimensions for word embeddings, 100 for character embeddings, random initialization for all embeddings, 100 hidden states, h = 3 stacked layers, Gaussian noise with σ=0.2. As training is stochastic, we use a fixed seed throughout (chosen and fixed upfront). No further unlabeled data is considered. Datasets An overview of the syntactic datasets considered in this paper is given in Table 3. For chunking, we use the original CoNLL data (Tjong Kim Sang and Buchholz, 2000) from WSJ (WSJ sections 15-18 as training data and section 20 as test data, containing 8936 and 2012 sentences, respectively).5 For testing we take out-of-domain data whenever available, to test the adaptability of the method to noisy out-of-domain data. For chunking we use Twitter data from Ritter (2011) (all, 2364 tweets) and Foster et al. (2011) (250 sentences), converted to chunks (Plank et al., 2014). The CCG supertagging data also comes from WSJ (39604 training and 2407 test sentences). We unfortunately do not have access to out-of-domain test data, hence use the CCG tagging test set. se"
C16-1059,N16-1027,0,0.0203884,"http://www.cnts.ua.ac.be/conll2000/chunking/ 6 In contrast to http://www.casmacat.eu/ data that logs revisions from MT post-editing. 7 Disregarding users due to issues with logging (Stewart et al., 2011). 5 614 Chunking F1 Our model Suzuki and Isozaki (2008) 93.21 93.88 CCG tagging Our model Xu et al. (2015) Accuracy 92.41 93.00 Table 4: Baseline model, comparison to existing systems POS embeddings. Our baseline CCG supertagging model achieves 92.41, compared to the more complex model by Xu et al. (2015) achieving an accuracy of 93.00. Very recently even higher accuracies were reported, e.g. (Vaswani et al., 2016), however, in this exploratory paper we are interested in examining whether we find signal in keystroke data, and are not interested in beating the latest state-of-the-art. F OSTER . DEV F OSTER . TEST R ITTER CCG Baseline +PAUSE 73.93 74.63† 73.61 74.32† 66.65 66.91† 92.41 92.62† p-values &lt;0.01 &lt;0.01 &lt;0.01 &lt;0.048 Table 5: Chunking results (F1, +Pause is average over 38 participants) and CCG accuracy (using all pause data at once). Results marked with † are significantly better than the corresponding baseline using a randomization test with i = 1000 iterations; p-values provided in row below."
C16-1059,D14-1030,0,0.0275568,"nd and Rosenberg (2015) is the only study prior to us that use keystroke loggings in NLP. In particular, they investigate the relationship between pre-word pauses and multi-word expressions and found within MWE pauses vary depending on cognitive task. We take a novel approach and learn keystroke patterns and use them to inform shallow syntactic parsing. A recent related line of work explores eye tracking data to inform sentence compression (Klerke et al., 2016) and induce part-of-speech (Barrett and Søgaard, 2015). Similarly, there are recent studies that predict fMRI activation from reading (Wehbe et al., 2014) or use fMRI data for POS induction (Bingel et al., 2016). The distinct advantage of keystroke dynamics is that it is easy to get, non-expensive and non-intrusive. 7 Conclusions Keystroke dynamics contain useful information for shallow syntactic parsing. Our model, a bi-LSTM, integrates keystroke data as auxiliary task, and outperforms models trained on the linguistic signal alone. We obtain promising results for two syntactic tasks, chunking and CCG supertagging. This warrants many directions for future research, e.g., using information from the non-linear writing process, which we here disre"
C16-1059,P15-2041,0,0.0297476,"n Table 4. Our chunking baseline achieves an F 1 of 93.21 on CoNLL, compared to the F 1 of 93.88 of Suzuki and Isozaki (2008), who use a CRF and gold POS tags. We do not use any POS information. A similar bi-LSTM achieves 93.64 (Huang et al., 2015), however, additionally uses 4 https://github.com/clab/cnn http://www.cnts.ua.ac.be/conll2000/chunking/ 6 In contrast to http://www.casmacat.eu/ data that logs revisions from MT post-editing. 7 Disregarding users due to issues with logging (Stewart et al., 2011). 5 614 Chunking F1 Our model Suzuki and Isozaki (2008) 93.21 93.88 CCG tagging Our model Xu et al. (2015) Accuracy 92.41 93.00 Table 4: Baseline model, comparison to existing systems POS embeddings. Our baseline CCG supertagging model achieves 92.41, compared to the more complex model by Xu et al. (2015) achieving an accuracy of 93.00. Very recently even higher accuracies were reported, e.g. (Vaswani et al., 2016), however, in this exploratory paper we are interested in examining whether we find signal in keystroke data, and are not interested in beating the latest state-of-the-art. F OSTER . DEV F OSTER . TEST R ITTER CCG Baseline +PAUSE 73.93 74.63† 73.61 74.32† 66.65 66.91† 92.41 92.62† p-valu"
C16-1179,W13-3520,0,0.0189266,"Missing"
C16-1179,D15-1263,0,0.225908,"f discourse structure exist. For instance, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) analyzes texts as constituency trees covering entire documents. This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse as tree structures and documents are not fully covered. In this study we focus on the RST-DT, but among other things, we consider the question of whether the information in PDTB can be used to improve RST discourse parsers. Discourse parsing is known to be a hard task (Stede, 2011). It involves several complex and interacting factors, touching upon all layers of lingu"
C16-1179,W01-1605,0,0.0602111,". [The gain on the sale couldn’t be estimated] [until the “tax treatment has been determined.”] b. [On Friday, Datuk Daim added spice to an otherwise unremarkable address on Malaysia’s proposed budget for 1990] [by ordering the Kuala Lumpur Stock Exchange “to take appropriate action immediately” to cut its links with the Stock Exchange of Singapore.] Different theories of discourse structure exist. For instance, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) analyzes texts as constituency trees covering entire documents. This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse"
C16-1179,J81-4005,0,0.703759,"Missing"
C16-1179,P12-1007,0,0.031503,"building. Joty et al. (2012) (TSP) built a two-stage parsing system, training separate sequential models (CRF) for the intra and the inter-sentential levels. These models jointly learn the relation and the structure, and a CKY-like algorithm is used to find the optimal tree. Feng and Hirst (2014) noticed the inefficiency of TSP and proposed a greedy approach inspired by 1910 HILDA but using CRF as local models for the inter- and intra-sententials levels, allowing to take into account sequential dependencies. Last studies also focused on the issue of building a good representation of the data. Feng and Hirst (2012) introduced linguistic features, mostly syntactic and contextual ones. Ji and Eisenstein (2014b) (DPLP) proposed to learn jointly the representation and the task, more precisely a projection matrix that maps the bag-of-words representation of the discourse units into a new vector space. This idea is promising, but a drawback could be the limited amount of data available in the RST-DT, an issue even more crucial for other languages. Discourse parsing has proven useful for many applications (Taboada and Mann, 2006), ranging from summarization (Daum´e III and Marcu, 2009; Thione et al., 2004; Spo"
C16-1179,P14-1048,0,0.329829,"spice to an otherwise unremarkable address on Malaysia’s proposed budget for 1990] [by ordering the Kuala Lumpur Stock Exchange “to take appropriate action immediately” to cut its links with the Stock Exchange of Singapore.] Different theories of discourse structure exist. For instance, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) analyzes texts as constituency trees covering entire documents. This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse as tree structures and documents are not fully covered. In this study we focus on the RST-DT, but among other things, we consi"
C16-1179,N04-1024,0,0.0346199,"hetorical Structure Theory (RST) (Mann and Thompson, 1988) analyzes texts as constituency trees covering entire documents. This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse as tree structures and documents are not fully covered. In this study we focus on the RST-DT, but among other things, we consider the question of whether the information in PDTB can be used to improve RST discourse parsers. Discourse parsing is known to be a hard task (Stede, 2011). It involves several complex and interacting factors, touching upon all layers of linguistic analysis, from syntax, semantics up to p"
C16-1179,N06-2015,0,0.0358279,"NS-E LABORATION ( SN-ATTRIBUTION (1) ( NS-E LABORATION (2)(3) ) (4) ) ) (5) ) b. ( NS-E LABORATION ( SN-ATTRIBUTION (1) ( NS-E LABORATION (2)(3) ) (4) ) (5) ) 3 Auxiliary tasks We consider two types of auxiliary tasks: first, tasks derived from the RST-DT (multi-view), that is dependency encoding of the trees and additional auxiliary tasks derived from the main one; second, we consider tasks derived from additional data, namely, the Penn Discourse Treebank (Prasad et al., 2008), Timebank (Pustejovsky et al., 2003; Pustejovsky et al., 2005), Factbank (Saur´ı and Pustejovsky, 2009), Ontonotes (Hovy et al., 2006) and the Santa Barbara corpus of spoken American English (Du Bois, 2000). All the auxiliary tasks are, as the main one, document-level sequence prediction tasks. In Table 1 we report the number of documents and single labels for each task. We hypothesize that such auxiliary information is useful to address data sparsity for RST discourse parsing. 4 Using the implementation available in NLTK. 1905 NN-T EXTUAL O RGANIZATION NN-L IST NN-S AME U NIT NN-L IST 1 6 NS-E LABORATION 2 3 7 NS-E LABORATION 4 =⇒ 5 EDU 1 EDU 2 EDU 3 EDU 4 EDU 5 EDU 6 EDU 7 root -1 NN-L IST -2 NN-S AME U NIT -1 NS-E LABORAT"
C16-1179,P14-1002,0,0.749887,"unremarkable address on Malaysia’s proposed budget for 1990] [by ordering the Kuala Lumpur Stock Exchange “to take appropriate action immediately” to cut its links with the Stock Exchange of Singapore.] Different theories of discourse structure exist. For instance, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) analyzes texts as constituency trees covering entire documents. This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse as tree structures and documents are not fully covered. In this study we focus on the RST-DT, but among other things, we consider the question of wheth"
C16-1179,D12-1083,0,0.147778,", Datuk Daim added spice to an otherwise unremarkable address on Malaysia’s proposed budget for 1990] [by ordering the Kuala Lumpur Stock Exchange “to take appropriate action immediately” to cut its links with the Stock Exchange of Singapore.] Different theories of discourse structure exist. For instance, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) analyzes texts as constituency trees covering entire documents. This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse as tree structures and documents are not fully covered. In this study we focus on the RST-DT, but among"
C16-1179,N16-1179,1,0.383709,"e first to propose using bi-LSTMs for tree structure prediction problems. Zhou and Xu (2015), for example, use bi-LSTMs to produce semantic role labelling structures. Zhang et al. (2015) did the same for relation extraction. None of them considered multi-task learning architectures, however. Multi-task learning in deep networks was first introduced by Caruana (1993), who did multi-task learning by doing parameter sharing across several deep networks, letting them share hidden layers. The same technique was used by Collobert et al. (2011) for various NLP tasks, and for sentence compression in (Klerke et al., 2016). Hierarchical multi-task bi-LSTMs have been previously used for part-of-speech tagging (Plank et al., 2016). 8 Conclusion and future work We presented the first experiments exploiting different views of the data and related tasks to improve textlevel discourse parsing. We presented a hierarchical bi-LSTM model allowing to leverage information from various sequence prediction tasks (multi-task learning) that achieves a new state-of-the-art performance on unlabeled text-level discourse parsing, and competitive performance in predicting nuclearity and discourse relations. For relation prediction"
C16-1179,P13-1047,0,0.0338202,"t al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010), sentiment analysis (Bhatia et al., 2015) or essay scoring (Burstein et al., 2003; Higgins et al., 2004). However, the range of applications and the improvement allowed are for now limited by the low performance of the existing discourse parsers. We are not aware of other studies trying to combine various encodings of the RST-DT trees or to leverage relevant information through multi-task learning to improve discourse parsing. To the best of our knowledge, multi-task learning has only been used for discourse relation classification (Lan et al., 2013) on the Penn Discourse Treebank to combine implicit and explicit data. We are not the first to propose using bi-LSTMs for tree structure prediction problems. Zhou and Xu (2015), for example, use bi-LSTMs to produce semantic role labelling structures. Zhang et al. (2015) did the same for relation extraction. None of them considered multi-task learning architectures, however. Multi-task learning in deep networks was first introduced by Caruana (1993), who did multi-task learning by doing parameter sharing across several deep networks, letting them share hidden layers. The same technique was used"
C16-1179,C04-1048,0,0.0973187,"Missing"
C16-1179,P14-1003,0,0.0415062,"-L IST 1 6 NS-E LABORATION 2 3 7 NS-E LABORATION 4 =⇒ 5 EDU 1 EDU 2 EDU 3 EDU 4 EDU 5 EDU 6 EDU 7 root -1 NN-L IST -2 NN-S AME U NIT -1 NS-E LABORATION -1 NS-E LABORATION -5 NN-T EXTUAL O RGANIZATION -1 NN-L IST Figure 3: From RST-DT discourse trees to dependency sequence labels. The numbers indicate the position of the head of the EDU, e.g. EDU 2 and EDU 3 have the root EDU 1 as head. 3.1 Building other views of the RST-DT trees Binary dependencies We first use a representation of the RST-DT trees as binary dependencies (RSTDep). We roughly do the same transformation as (Muller et al., 2012; Li et al., 2014) but contrary to the latter, we choose as root the nucleus of the root node of the tree rather than the first EDU of the document. More precisely, we associate each node with its saliency set as defined in (Marcu, 1997): The nucleus is the salient EDU of a relation, and the nuclei can go up in the tree with possibly several nuclei in the saliency set of a node. Like Li et al. (2014), we replace all multi-nuclear relations (NN) by mono-nuclear ones choosing the left DU as the nucleus (NS). We thus have only one nucleus in each saliency set. Figure 3 illustrates the conversion of an RST tree int"
C16-1179,D09-1036,0,0.016877,"Missing"
C16-1179,W10-4327,0,0.142625,"Different theories of discourse structure exist. For instance, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) analyzes texts as constituency trees covering entire documents. This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse as tree structures and documents are not fully covered. In this study we focus on the RST-DT, but among other things, we consider the question of whether the information in PDTB can be used to improve RST discourse parsers. Discourse parsing is known to be a hard task (Stede, 2011). It involves several complex and interacting factors, touching upo"
C16-1179,J00-3005,0,0.563392,"uning We used a development set of 25 documents randomly chosen among the training set. We optimized the number of passes p over the data (p ∈ [10, 60]), the value of the Gaussian noise (σ ∈ {0.0, 0.2}), the number of hidden dimensions (d ∈ {200, 400}), the number of stacked layers (h ∈ {1, 2, 3, 4, 5}), and the auxiliary tasks to be included and combined. In the end, we report results using 2 feed-forward layers with 128 dimensions, a Gaussian noise with sigma of 0.2, 200 hidden dimensions, 20 passes over the data, 2 layers and Polyglot embeddings (Al-Rfou et al., 2013)8 . Metrics Following (Marcu, 2000b) and most subsequent work, output trees are evaluated against gold trees in terms of how similar they bracket the EDUs (Span), how often they agree about nuclei when predicting a true bracket (Nuclearity), and in terms of the relation label, i.e., the overlap between the shared brackets between predicted and gold trees (Relation).9 These scores are analogous to labeled and unlabeled syntactic parser evaluation metrics. The exact definitions of the three metrics are: • Span: This metric is the unlabeled F1 over gold and predicted trees, and identical to the PARSEVAL metric in syntactic parsin"
C16-1179,J93-2004,0,0.0630102,"Missing"
C16-1179,C12-1115,0,0.0322083,"IST NN-S AME U NIT NN-L IST 1 6 NS-E LABORATION 2 3 7 NS-E LABORATION 4 =⇒ 5 EDU 1 EDU 2 EDU 3 EDU 4 EDU 5 EDU 6 EDU 7 root -1 NN-L IST -2 NN-S AME U NIT -1 NS-E LABORATION -1 NS-E LABORATION -5 NN-T EXTUAL O RGANIZATION -1 NN-L IST Figure 3: From RST-DT discourse trees to dependency sequence labels. The numbers indicate the position of the head of the EDU, e.g. EDU 2 and EDU 3 have the root EDU 1 as head. 3.1 Building other views of the RST-DT trees Binary dependencies We first use a representation of the RST-DT trees as binary dependencies (RSTDep). We roughly do the same transformation as (Muller et al., 2012; Li et al., 2014) but contrary to the latter, we choose as root the nucleus of the root node of the tree rather than the first EDU of the document. More precisely, we associate each node with its saliency set as defined in (Marcu, 1997): The nucleus is the salient EDU of a relation, and the nuclei can go up in the tree with possibly several nuclei in the saliency set of a node. Like Li et al. (2014), we replace all multi-nuclear relations (NN) by mono-nuclear ones choosing the left DU as the nucleus (NS). We thus have only one nucleus in each saliency set. Figure 3 illustrates the conversion"
C16-1179,P09-1077,0,0.0189492,"Missing"
C16-1179,P16-2067,1,0.830709,"e bi-LSTMs to produce semantic role labelling structures. Zhang et al. (2015) did the same for relation extraction. None of them considered multi-task learning architectures, however. Multi-task learning in deep networks was first introduced by Caruana (1993), who did multi-task learning by doing parameter sharing across several deep networks, letting them share hidden layers. The same technique was used by Collobert et al. (2011) for various NLP tasks, and for sentence compression in (Klerke et al., 2016). Hierarchical multi-task bi-LSTMs have been previously used for part-of-speech tagging (Plank et al., 2016). 8 Conclusion and future work We presented the first experiments exploiting different views of the data and related tasks to improve textlevel discourse parsing. We presented a hierarchical bi-LSTM model allowing to leverage information from various sequence prediction tasks (multi-task learning) that achieves a new state-of-the-art performance on unlabeled text-level discourse parsing, and competitive performance in predicting nuclearity and discourse relations. For relation prediction, future work includes adding additional information at the sentence level, such as syntactic information us"
C16-1179,prasad-etal-2008-penn,0,0.0947831,"This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse as tree structures and documents are not fully covered. In this study we focus on the RST-DT, but among other things, we consider the question of whether the information in PDTB can be used to improve RST discourse parsers. Discourse parsing is known to be a hard task (Stede, 2011). It involves several complex and interacting factors, touching upon all layers of linguistic analysis, from syntax, semantics up to pragmatics. Consequently, also annotation is complex and time consuming, and hence available annotated corpora are sparse"
C16-1179,E14-1068,0,0.0350995,"Missing"
C16-1179,H05-1033,0,0.0635365,"ock Exchange of Singapore.] Different theories of discourse structure exist. For instance, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) analyzes texts as constituency trees covering entire documents. This theory has led to the RST Discourse Treebank (RST-DT) (Carlson et al., 2001) for English and the development of textlevel discourse parsers (Hernault et al., 2010; Joty et al., 2012; Feng and Hirst, 2014; Ji and Eisenstein, 2014b). Such parsers have proven to be useful for several downstream applications (Taboada and Mann, 2006; Daum´e III and Marcu, 2009; Thione et al., 2004; Sporleder and Lapata, 2005; Louis et al., 2010; Bhatia et al., 2015; Burstein et al., 2003; Higgins et al., 2004). Another corpus has been annotated for discourse phenomena in English, the Penn Discourse Treebank (Prasad et al., 2008) (PDTB). In contrast to RST-DT, PDTB does not encode discourse as tree structures and documents are not fully covered. In this study we focus on the RST-DT, but among other things, we consider the question of whether the information in PDTB can be used to improve RST discourse parsers. Discourse parsing is known to be a hard task (Stede, 2011). It involves several complex and interacting f"
C16-1179,W04-1009,0,0.0875429,"Missing"
C16-1179,W12-1623,0,0.028001,"Missing"
C16-1179,Y15-1009,0,0.00549823,"ance of the existing discourse parsers. We are not aware of other studies trying to combine various encodings of the RST-DT trees or to leverage relevant information through multi-task learning to improve discourse parsing. To the best of our knowledge, multi-task learning has only been used for discourse relation classification (Lan et al., 2013) on the Penn Discourse Treebank to combine implicit and explicit data. We are not the first to propose using bi-LSTMs for tree structure prediction problems. Zhou and Xu (2015), for example, use bi-LSTMs to produce semantic role labelling structures. Zhang et al. (2015) did the same for relation extraction. None of them considered multi-task learning architectures, however. Multi-task learning in deep networks was first introduced by Caruana (1993), who did multi-task learning by doing parameter sharing across several deep networks, letting them share hidden layers. The same technique was used by Collobert et al. (2011) for various NLP tasks, and for sentence compression in (Klerke et al., 2016). Hierarchical multi-task bi-LSTMs have been previously used for part-of-speech tagging (Plank et al., 2016). 8 Conclusion and future work We presented the first expe"
C16-1179,P15-1109,0,0.127731,"s of two stacked layers. For multi-task learning, each task is associated with a specific output layer, whereas the inner layers – the stacked LSTMs – are shared across the tasks. At training time, we randomly sample data points from target or auxiliary tasks and do forward predictions. In the backward pass, we modify the weights of the shared layers and the task-specific outer layer. Except for the outer layer, the target task model is thus regularized by the induction of auxiliary models. Bi-LSTMs have already been used for syntactic chunking (Huang et al., 2015) and semantic role labeling (Zhou and Xu, 2015), as well as other tasks. Our model differs from most of these models in being a hierarchical model, composing word embeddings into sentence embeddings that are the inputs of a bigger bi-LSTM model. This means our model can also be initialized by pre-trained word embeddings. We implemented our recurrent network in CNN/pycnn,7 fixing the random seed. We use standard SGD for learning our model parameters. 5 Experiments Data The RST-DT contains 385 Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993), with 347 documents for training and 38 for testing in the split used in pre"
C16-1179,Q15-1024,0,\N,Missing
C16-1179,P02-1057,0,\N,Missing
C16-1333,W13-3520,0,0.0122335,"e code is available at https://github.com/bjerva/semantic-tagging. We represent each sentence using both a character-based representation (Sc ) and a word-based representation (Sw ). The character-based representation is a 3-dimensional matrix Sc ∈ Rs×w×dc , where s is the zero-padded sentence length, w is the zero-padded word length, and dc is the dimensionality of the character embeddings. The word-based representation is a 2-dimensional matrix Sw ∈ Rs×dw , where s is the zero-padded sentence length and dw is the dimensionality of the word embeddings. We use the English Polyglot embeddings (Al-Rfou et al., 2013) in order to initialise the word embedding layer, but also experiment with randomly initialised word embeddings. Word embeddings are passed directly into a two-layer bi-GRU (Chung et al., 2014). We also experimented using a bi-LSTM. However, we found GRUs to yield comparatively better validation data performance on semtags. We also observe better validation data performance when running two consecutive forward and backward passes before concatenating the GRU layers, rather than concatenating after each forward/backward pass as is commonplace in NLP literature. 3535 We use CNNs for character-le"
C16-1333,P14-1133,0,0.0284775,"Missing"
C16-1333,W16-4816,1,0.900908,"n’ information path in the network facilitates optimisation (He et al., 2016). ResNets have recently shown state-of-the-art performance for image classification tasks (He et al., 2015; He et al., 2016), and have This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 3531 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3531–3541, Osaka, Japan, December 11-17 2016. ¨ also seen some recent use in NLP (Ostling, 2016; Conneau et al., 2016; Bjerva, 2016; Wu et al., 2016). However, no previous work has attempted to apply ResNets to NLP tagging tasks. To answer our second question, we carry out an extrinsic evaluation exercise. We investigate the effect of using semantic tags as an auxiliary loss for POS tagging. Since POS tags are useful for many NLP tasks, it follows that semantic tags must be useful if they can improve POS tagging. 2 2.1 Semantic Tagging Background We refer to semantic tagging, or sem-tagging, as the task of assigning semantic class categories to the smallest meaningful units in a sentence. In the context of this paper thes"
C16-1333,W08-2222,1,0.211215,"bank (PTB) Part-of-Speech tagset (Marcus et al., 1993) does not make the necessary semantic distinctions, in addition to containing redundant information for semantic processing. Let us consider a couple of examples. There are significant differences in meaning between the determiners every (universal quantification), no (negation), and some (existential quantification), but they all receive the DT (determiner) POS label in PTB. Since determiners form a closed class, one could enumerate all word forms for each class. Indeed some recent implementations of semantic parsing follow this strategy (Bos, 2008; Butler, 2010). This might work for a single language, but it falls short when considering a multilingual setting. Furthermore, determiners like any can have several interpretations and need to be disambiguated in context. Semantic tagging does not only apply to determiners, but reaches all parts of speech. Other examples where semantic classes disambiguate are reflexive versus emphasising pronouns (both POS-tagged as PRP, personal pronoun); the comma, that could be a conjunction, disjunction, or apposition; intersective vs. subsective and privative adjectives (all POS-tagged as JJ, adjective"
C16-1333,A00-1031,0,0.0508761,"ing independent variables in our experiments: 1. character and word representations (w, ~ ~c); 2. residual bypass for character representations (~cbp ); 3. convolutional representations (Basic CNN and ResNets); 4. auxiliary loss (using coarse semtags on ST and fine semtags on UD). 3536 We compare our results to four baselines: 1. the most frequent baseline per word (MFC), where we assign the most frequent tag for a word in the training data to that word in the test data, and unseen words get the global majority tag; 2. the trigram statistic based TNT tagger offers a slightly tougher baseline (Brants, 2000); 3. the B I - LSTM baseline, running the off-the-shelf state-of-the-art POS tagger for the UD dataset (Plank et al., 2016) (using default parameters with pre-trained Polyglot embeddings); 4. we use a baseline consisting of running our own system with only a B I - GRU using word representations (w), ~ with pre-trained Polyglot embeddings. 4.1 Experiments on semantic tagging We evaluate our system on two semantic tagging (ST) datasets: our silver semtag dataset and our gold semtag dataset. For the +AUX condition we use coarse semtags as an auxiliary loss. Results from these experiments are show"
C16-1333,D15-1085,0,0.0325507,"in a space with dimensionality dc as an image of dimensionality n × dc . This view gives us additional freedom in terms of sizes of convolutional patches used, which offers more computational flexibility than using only, e.g., 4 × dc convolutions. This view is applied to all CNN variations explored in this work. A neural network is trained with respect to some loss function, such as the cross-entropy between the predicted tag probability distribution and the gold probability distribution. Recent work has shown that the addition of an auxiliary loss function can be beneficial to several tasks. Cheng et al. (2015) use a language modelling task as an auxiliary loss, as they attempt to predict the next token while performing named entity recognition. Plank et al. (2016) use the log frequency of the current token as an auxiliary loss function, and find this to improve POS tagging accuracy. Since our semantic tagging task is based on predicting fine semtags, which can be mapped to coarse semtags, we add the prediction of these coarse semtags as an auxiliary loss for the sem-tagging experiments. Similarly, we also experiment with POS tagging, where we use the fine semtags as an auxiliary information. 3.4.1"
C16-1333,P16-1160,0,0.0190157,"ork is the first to apply ResNets to NLP sequence tagging tasks. We further contribute to the literature on ResNets by introducing a residual bypass function. The intuition is to combine both deep and shallow processing, which opens a path of easy signal propagation between lower and higher layers in the network. 3.3 Modelling character information and residual bypass Using sub-token representations instead of, or in combination with, word-level representations has recently obtained a lot of attention due to their effectiveness (Sutskever et al., 2011; Chrupała, 2013; 3534 Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015). The use of sub-token representations can be approached in several ways. Plank et al. (2016) and Yang et al. (2016) use a hierarchical bidirectional RNN, first passing over characters in order to create word-level representations. Gillick et al. (2015) similarly apply an LSTM-based model using byte-level information directly. Dos Santos and Zadrozny (2014) construct character-based word-level representations by running a convolutional network over the character representations of each word. All of these approaches have in common that the character-based representation i"
C16-1333,D13-1146,1,0.363033,"matically obtained with the C&C tools (Curran et al., 2007) and then manually corrected), as well as a set of manually crafted rules to output semantic tags. Some tags related to specific phenomena were hand-corrected in a second stage. Our second dataset is smaller but equipped with gold standard semantic tags and used for testing (PMB, the Parallel Meaning Bank). It comprises a selection of 400 sentences of the English part of a parallel corpus. It has no overlap with the GMB corpus. For this dataset, we used the Elephant tokeniser, which performs word, multi-word and sentence segmentation (Evang et al., 2013). We then used the 3532 ANA PRO DEF HAS REF EMP ACT GRE ITJ HES QUE ATT QUA UOM IST REL RLI SST PRI INT SCO LOG ALT EXC NIL DIS IMP AND BUT pronoun definite possessive reflexive emphasizing greeting interjection hesitation interrogative quantity measurement intersective relation rel. inv. scope subsective privative intensifier score alternative exclusive empty disjunct./exist. implication conjunct./univ. contrast COM EQA MOR LES TOP BOT ORD DEM PRX MED DST DIS SUB COO APP MOD NOT NEC POS ENT CON ROL NAM GPE PER LOC ORG ART NAT HAP URL equative comparative pos. comparative neg. pos. superlative"
C16-1333,J93-2004,0,0.0565979,". Since POS tags are useful for many NLP tasks, it follows that semantic tags must be useful if they can improve POS tagging. 2 2.1 Semantic Tagging Background We refer to semantic tagging, or sem-tagging, as the task of assigning semantic class categories to the smallest meaningful units in a sentence. In the context of this paper these units can be morphemes, words, punctuation, or multi-word expressions. The linguistic information traditionally obtained for deep processing is insufficient for fine-grained lexical semantic analysis. The widely used Penn Treebank (PTB) Part-of-Speech tagset (Marcus et al., 1993) does not make the necessary semantic distinctions, in addition to containing redundant information for semantic processing. Let us consider a couple of examples. There are significant differences in meaning between the determiners every (universal quantification), no (negation), and some (existential quantification), but they all receive the DT (determiner) POS label in PTB. Since determiners form a closed class, one could enumerate all word forms for each class. Indeed some recent implementations of semantic parsing follow this strategy (Bos, 2008; Butler, 2010). This might work for a single"
C16-1333,W16-2003,0,0.135016,"eeper networks, since keeping a ‘clean’ information path in the network facilitates optimisation (He et al., 2016). ResNets have recently shown state-of-the-art performance for image classification tasks (He et al., 2015; He et al., 2016), and have This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 3531 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3531–3541, Osaka, Japan, December 11-17 2016. ¨ also seen some recent use in NLP (Ostling, 2016; Conneau et al., 2016; Bjerva, 2016; Wu et al., 2016). However, no previous work has attempted to apply ResNets to NLP tagging tasks. To answer our second question, we carry out an extrinsic evaluation exercise. We investigate the effect of using semantic tags as an auxiliary loss for POS tagging. Since POS tags are useful for many NLP tasks, it follows that semantic tags must be useful if they can improve POS tagging. 2 2.1 Semantic Tagging Background We refer to semantic tagging, or sem-tagging, as the task of assigning semantic class categories to the smallest meaningful units in a sentenc"
C16-1333,P16-2067,1,0.925084,"cope with longer input sequences than vanilla RNNs. GRUs are similar to the more commonly-used Long Short-Term Memory networks (LSTMs), both in purpose and implementation (Chung et al., 2014). A bi-directional GRU is a GRU which makes both forward and backward passes over sequences, and can therefore use both preceding and succeeding contexts to predict a tag (Graves and Schmidhuber, 2005; Goldberg, 2015). Bi-directional GRUs and LSTMs have been shown to yield high performance on several NLP tasks, such as POS tagging, named entity tagging, and chunking (Wang et al., 2015; Yang et al., 2016; Plank et al., 2016). We build on previous approaches by combining bi-GRUs with character representations from a basic CNN and ResNets. 3533 Figure 1: Model architecture. Left: Architecture with basic CNN char representations (~c), Middle: basic CNN with char and word representations and bypass (~cbp ∧ w), ~ Right: ResNet with auxiliary loss and residual bypass (+AUXbp ). 3.2 Deep Residual Networks Deep Residual Networks (ResNets) are built up by stacking residual units. A residual unit can be expressed as: yl = h(xl ) + F(xl , Wl ), (3) xl+1 = f (yl ), where xl and xl+1 are the input and output of the l-th layer"
C16-1333,N04-1030,0,0.0174103,"Missing"
C16-1333,P07-2009,1,\N,Missing
C16-1333,L16-1262,0,\N,Missing
D14-1104,D10-1044,0,0.0285571,", but nevertheless different distribution. The problem of automatically adjusting the model induced from source to a different target is referred to as domain adaptation. Some researchers have studied domain adaptation scenarios, where small samples of labeled data have been assumed to be available for the target domains. This is usually an unrealistic assumption, since even for major languages, small samples are only available from a limited number of domains, and in this work we focus on unsupervised domain adaptation, assuming only unlabeled target data is available. Jiang and Zhai (2007), Foster et al. (2010; Plank and Moschitti (2013) and Søgaard and Haulrich (2011) have previously tried to use importance weighting to correct sample bias in NLP. Importance weighting means assigning a weight to each training instance, reflecting its importance for modeling the target distribution. Importance weighting is a generalization over poststratification (Smith, 1991) and importance sampling (Smith et al., 1997) and can be used to correct bias in the labeled data. Out of the four papers mentioned, only Søgaard and Haulrich (2011) and Plank and Moschitti (2013) considered an unsupervised domain adaptation s"
D14-1104,P07-1034,0,0.525385,"sampled from a related, but nevertheless different distribution. The problem of automatically adjusting the model induced from source to a different target is referred to as domain adaptation. Some researchers have studied domain adaptation scenarios, where small samples of labeled data have been assumed to be available for the target domains. This is usually an unrealistic assumption, since even for major languages, small samples are only available from a limited number of domains, and in this work we focus on unsupervised domain adaptation, assuming only unlabeled target data is available. Jiang and Zhai (2007), Foster et al. (2010; Plank and Moschitti (2013) and Søgaard and Haulrich (2011) have previously tried to use importance weighting to correct sample bias in NLP. Importance weighting means assigning a weight to each training instance, reflecting its importance for modeling the target distribution. Importance weighting is a generalization over poststratification (Smith, 1991) and importance sampling (Smith et al., 1997) and can be used to correct bias in the labeled data. Out of the four papers mentioned, only Søgaard and Haulrich (2011) and Plank and Moschitti (2013) considered an unsupervise"
D14-1104,C08-1071,0,0.0756699,"Missing"
D14-1104,P13-1147,1,0.933854,"fferent distribution. The problem of automatically adjusting the model induced from source to a different target is referred to as domain adaptation. Some researchers have studied domain adaptation scenarios, where small samples of labeled data have been assumed to be available for the target domains. This is usually an unrealistic assumption, since even for major languages, small samples are only available from a limited number of domains, and in this work we focus on unsupervised domain adaptation, assuming only unlabeled target data is available. Jiang and Zhai (2007), Foster et al. (2010; Plank and Moschitti (2013) and Søgaard and Haulrich (2011) have previously tried to use importance weighting to correct sample bias in NLP. Importance weighting means assigning a weight to each training instance, reflecting its importance for modeling the target distribution. Importance weighting is a generalization over poststratification (Smith, 1991) and importance sampling (Smith et al., 1997) and can be used to correct bias in the labeled data. Out of the four papers mentioned, only Søgaard and Haulrich (2011) and Plank and Moschitti (2013) considered an unsupervised domain adaptation scenario, obtaining mixed res"
D14-1104,E14-1078,1,0.836428,"sion and transition probabilities across the various domains. 99 Cortes et al. (2010) show that importance weighting potentially leads to over-fitting, but propose to use quantiles to obtain more robust weight functions. The idea is to rank all weights and obtain q quantiles. If a data point x is weighted by w, and w lies in the ith quantile of the ranking (i ≤ q), x is weighted by the average weight of data points in the ith quantile. The weighted structured perceptron (§3) used in the experiments below was recently used for a different problem, namely for correcting for bias in annotations (Plank et al., 2014). wsj answers reviews emails weblogs newsgroups 98 ● 97 ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 93 94 95 96 ● Related work 92 2 0 Most prior work on importance weighting use a domain classifier, i.e., train a classifier to discriminate between source and target instances (Søgaard and Haulrich, 2011; Plank and Moschitti, 2013) (y ∈ {s, t}). For instance, Søgaard and Haulrich (2011) train a n-gram text classifier and Plank and Moschitti (2013) a tree-kernel based classifier on relation extraction instances. In these studies, Pˆ (t|x) is used as an approximation of Pt (x) Ps (x) , following Zadrozn"
D14-1104,Q14-1002,0,0.0267413,"Missing"
D14-1104,W11-2906,1,0.915587,"em of automatically adjusting the model induced from source to a different target is referred to as domain adaptation. Some researchers have studied domain adaptation scenarios, where small samples of labeled data have been assumed to be available for the target domains. This is usually an unrealistic assumption, since even for major languages, small samples are only available from a limited number of domains, and in this work we focus on unsupervised domain adaptation, assuming only unlabeled target data is available. Jiang and Zhai (2007), Foster et al. (2010; Plank and Moschitti (2013) and Søgaard and Haulrich (2011) have previously tried to use importance weighting to correct sample bias in NLP. Importance weighting means assigning a weight to each training instance, reflecting its importance for modeling the target distribution. Importance weighting is a generalization over poststratification (Smith, 1991) and importance sampling (Smith et al., 1997) and can be used to correct bias in the labeled data. Out of the four papers mentioned, only Søgaard and Haulrich (2011) and Plank and Moschitti (2013) considered an unsupervised domain adaptation scenario, obtaining mixed results. These two papers assume co"
D14-1104,Q13-1001,0,0.0257099,"Missing"
D14-1104,W02-1001,0,\N,Missing
D14-1104,petrov-etal-2012-universal,0,\N,Missing
D14-1104,W06-1615,0,\N,Missing
D17-1038,D11-1033,0,0.218578,"e J and the value of J is returned. Gaussian Processes (GP) are a popular choice for p(f ) due to their descriptive power (Rasmussen, 2006). We use GP with Monte Carlo acquistion and Expected Improvement (EI) Data selection model In order to select training data for adaptation for a task T , existing approaches rank the available n training examples X = {x1 , x2 , · · · , xn } of k source domains D = {D1 , D2 , · · · , Dk } according to a domain similarity measure S and choose the top m samples for training their algorithm. While this has been shown to work empirically (Moore and Lewis, 2010; Axelrod et al., 2011; Plank and van Noord, 2011; Van Asch and Daelemans, 2010; Remus, 2012), using a pre-existing metric leaves us unable to adapt to the characteristics of our task T and target domain DT and foregoes additional knowledge that may be gleaned from the interaction of different metrics. For this reason, we propose to learn the following linear domain similarity measure S as a linear combina373 (Moˇckus, 1974) as acquisition function as this combination has been shown to outperform comparable approaches (Snoek et al., 2012).1 3.2 • Bhattacharyya (Bhattacharya, P √ distance 1943): ln( i Pi Qi ) • Cosi"
D17-1038,P07-1034,0,0.116529,"We show the importance of complementing similarity with diversity, and that learned measures are—to some degree—transferable across models, domains, and even tasks. 1 Introduction Natural Language Processing (NLP) models suffer considerably when applied in the wild. The distribution of the test data is typically very different from the data used during training, causing a model’s performance to deteriorate substantially. Domain adaptation is a prominent approach to transfer learning that can help to bridge this gap; many approaches were suggested so far (Blitzer et al., 2007; Daumé III, 2007; Jiang and Zhai, 2007; Ma et al., 2014; Schnabel and Schütze, 2014). However, most work focused on one-toone scenarios. Only recently research considered using multiple sources. Such studies are rare and typically rely on specific model transfer approaches (Mansour, 2009; Wu and Huang, 2016). Inspired by work on curriculum learning (Bengio et al., 2009; Tsvetkov et al., 2016), we instead propose—to the best of our knowledge—the first model-agnostic data selection approach to trans2 Background: Transfer learning Transfer learning generally involves the concepts of a domain and a task (Pan and Yang, 2010). A domain"
D17-1038,Q16-1023,0,0.0539593,"most similar domain baseline. We train an LDA model (Blei et al., 2003) with 50 topics and 10 iterations for topic distribution-based representations and use GloVe embeddings (Pennington et al., 2014) trained on 42B tokens of Common Crawl data6 for word embedding-based representations. For sentiment analysis, we conduct 10 runs of each feature set for every domain and report mean and variance. For POS tagging and parsing, we observe that variance is low and perform one run while retaining random seeds for reproducibility. Parsing For parsing, we evaluate the state-ofthe-art Bi-LSTM parser by Kiperwasser and Goldberg (2016) with default hyperparameters.4 We use the same domains as used for POS tagging, i.e., the dependency parsing data with gold POS as made available in the SANCL 2012 shared task.5 2 All code is available at https://github.com/ sebastianruder/learn-to-select-data. 3 https://github.com/bplank/bilstm-aux 4 https://github.com/elikip/bist-parser 5 We leave investigating the effect of the adapted taggers on parsing for future work. 6 https://nlp.stanford.edu/projects/ glove/ 375 Base Learned measures Feature set Random Jensen-Shannon divergence – examples Jensen-Shannon divergence – domain Similarity"
D17-1038,P07-1056,0,0.743989,"s, partof-speech tagging, and parsing. We show the importance of complementing similarity with diversity, and that learned measures are—to some degree—transferable across models, domains, and even tasks. 1 Introduction Natural Language Processing (NLP) models suffer considerably when applied in the wild. The distribution of the test data is typically very different from the data used during training, causing a model’s performance to deteriorate substantially. Domain adaptation is a prominent approach to transfer learning that can help to bridge this gap; many approaches were suggested so far (Blitzer et al., 2007; Daumé III, 2007; Jiang and Zhai, 2007; Ma et al., 2014; Schnabel and Schütze, 2014). However, most work focused on one-toone scenarios. Only recently research considered using multiple sources. Such studies are rare and typically rely on specific model transfer approaches (Mansour, 2009; Wu and Huang, 2016). Inspired by work on curriculum learning (Bengio et al., 2009; Tsvetkov et al., 2016), we instead propose—to the best of our knowledge—the first model-agnostic data selection approach to trans2 Background: Transfer learning Transfer learning generally involves the concepts of a domain and"
D17-1038,W06-1615,0,0.497077,"1 We also experimented with FABOLAS (Klein et al., 2017), but found its ability to adjust the training set size during optimization to be inconclusive for our relatively small training sets. 374 4.1 Tasks, datasets, and models We evaluate our approach on three tasks: sentiment analysis, part-of speech (POS) tagging, and dependency parsing. We use the n examples with the highest score as determined by the learned data selection measure for training our models.2 We show statistics for all datasets in Table 1. Sentiment Analysis For sentiment analysis, we evaluate on the Amazon reviews dataset (Blitzer et al., 2006). We use tf-idf-weighted unigram and bigram features and a linear SVM classifier (Blitzer et al., 2007). We set the vocabulary size to 10,000 and the number of training examples n = 1600 to conform with existing approaches (Bollegala et al., 2011) and stratify the training set. # labeled # unlabeled Sentiment Experiments Book DVD Electronics Kitchen 2000 2000 2000 2000 4465 3586 5681 5945 POS/Parsing 4 T Domain Answers Emails Newsgroups Reviews Weblogs WSJ 3489 4900 2391 3813 2031 2976 27274 1194173 1000000 1965350 524834 30060 Table 1: Number of labeled and unlabeled sentences for each domain"
D17-1038,P14-1014,0,0.0365427,"of complementing similarity with diversity, and that learned measures are—to some degree—transferable across models, domains, and even tasks. 1 Introduction Natural Language Processing (NLP) models suffer considerably when applied in the wild. The distribution of the test data is typically very different from the data used during training, causing a model’s performance to deteriorate substantially. Domain adaptation is a prominent approach to transfer learning that can help to bridge this gap; many approaches were suggested so far (Blitzer et al., 2007; Daumé III, 2007; Jiang and Zhai, 2007; Ma et al., 2014; Schnabel and Schütze, 2014). However, most work focused on one-toone scenarios. Only recently research considered using multiple sources. Such studies are rare and typically rely on specific model transfer approaches (Mansour, 2009; Wu and Huang, 2016). Inspired by work on curriculum learning (Bengio et al., 2009; Tsvetkov et al., 2016), we instead propose—to the best of our knowledge—the first model-agnostic data selection approach to trans2 Background: Transfer learning Transfer learning generally involves the concepts of a domain and a task (Pan and Yang, 2010). A domain D consists of a f"
D17-1038,P11-1014,0,0.0383682,"pproach on three tasks: sentiment analysis, part-of speech (POS) tagging, and dependency parsing. We use the n examples with the highest score as determined by the learned data selection measure for training our models.2 We show statistics for all datasets in Table 1. Sentiment Analysis For sentiment analysis, we evaluate on the Amazon reviews dataset (Blitzer et al., 2006). We use tf-idf-weighted unigram and bigram features and a linear SVM classifier (Blitzer et al., 2007). We set the vocabulary size to 10,000 and the number of training examples n = 1600 to conform with existing approaches (Bollegala et al., 2011) and stratify the training set. # labeled # unlabeled Sentiment Experiments Book DVD Electronics Kitchen 2000 2000 2000 2000 4465 3586 5681 5945 POS/Parsing 4 T Domain Answers Emails Newsgroups Reviews Weblogs WSJ 3489 4900 2391 3813 2031 2976 27274 1194173 1000000 1965350 524834 30060 Table 1: Number of labeled and unlabeled sentences for each domain in the Amazon Reviews dataset (Blitzer et al., 2006) (above) and the SANCL 2012 dataset (Petrov and McDonald, 2012) for POS tagging and parsing (below). POS tagging For POS tagging and parsing, we evaluate on the coarse-grained POS data (12 unive"
D17-1038,P13-2119,0,0.201258,"nce these are very different tasks. Between related tasks, the combination of similarity and diversity features achieves the most robust transfer and outperforms the baselines in both cases. This suggests that even in the absence of target task data, we only require data of a related task to learn a successful data selection measure. 6 Related work Most prior work on data selection for transfer learning focuses on phrase-based machine translation. Typically language models are leveraged via perplexity or cross-entropy scoring to select target data (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Mirkin and Besacier, 2014). A recent study investigates data selection for neural machine translation (van der Wees et al., 2017). Perplexity was also used to select training data for dependency parsing (Søgaard, 2011), but has been found to be less suitable for tasks such as sentiment analysis (Ruder et al., 2017). In general, there are fewer studies on data selection for other tasks, e.g., constituent parsing (McClosky et al., 2010), dependency parsing (Plank and van Noord, 2011; Søgaard, 2011) and sentiment analysis (Remus, 2012). Work on predicting task accuracy is related, but can be se"
D17-1038,2014.amta-researchers.23,0,0.238541,"s sentiment in more similar ways, while for POS tagging having more varied training instances is intuitively more beneficial. In fact, when inspecting the domain distribution of our approach, we find that the best SA model chooses more instances from the closest domain, while for POS tagging instances are more balanced across domains. This suggests that the Web treebank domains are less clear-cut. In fact, training a model on all sources, which is considerably more and varied data (in this setup, 14-17.5k training instances) is beneficial. This is in line with findings in machine translation (Mirkin and Besacier, 2014), which show that similarity-based selection works best if domains are very different. Results are thus less pronounced for POS tagging, and we leave experimenting with larger n for future work. To gain some insight into the optimization procedure, Figure 1 shows the development accuracy for the Structured Perceptron for an example domain. The top-right and bottom graphs show the hypothesis space exploration of Bayesian Optimization for different single feature sets, while the Feature set ↓ MS → Term similarity Diversity Term similarity+diversity Answers B Pproxy 93.43 93.67 92.58 93.19 93.46"
D17-1038,Q14-1002,0,0.0176484,"similarity with diversity, and that learned measures are—to some degree—transferable across models, domains, and even tasks. 1 Introduction Natural Language Processing (NLP) models suffer considerably when applied in the wild. The distribution of the test data is typically very different from the data used during training, causing a model’s performance to deteriorate substantially. Domain adaptation is a prominent approach to transfer learning that can help to bridge this gap; many approaches were suggested so far (Blitzer et al., 2007; Daumé III, 2007; Jiang and Zhai, 2007; Ma et al., 2014; Schnabel and Schütze, 2014). However, most work focused on one-toone scenarios. Only recently research considered using multiple sources. Such studies are rare and typically rely on specific model transfer approaches (Mansour, 2009; Wu and Huang, 2016). Inspired by work on curriculum learning (Bengio et al., 2009; Tsvetkov et al., 2016), we instead propose—to the best of our knowledge—the first model-agnostic data selection approach to trans2 Background: Transfer learning Transfer learning generally involves the concepts of a domain and a task (Pan and Yang, 2010). A domain D consists of a feature space X and a marginal"
D17-1038,D14-1162,0,0.120323,"set. We optimize each similarity measure using Bayesian Optimization with 300 iterations according to the objective measure J of each task (accuracy for sentiment analysis and POS tagging; LAS for parsing) with respect to the validation set of the corresponding target domain. Unlabeled data is used in addition to calculate the representation of the target domain and to calculate the source domain representation for the most similar domain baseline. We train an LDA model (Blei et al., 2003) with 50 topics and 10 iterations for topic distribution-based representations and use GloVe embeddings (Pennington et al., 2014) trained on 42B tokens of Common Crawl data6 for word embedding-based representations. For sentiment analysis, we conduct 10 runs of each feature set for every domain and report mean and variance. For POS tagging and parsing, we observe that variance is low and perform one run while retaining random seeds for reproducibility. Parsing For parsing, we evaluate the state-ofthe-art Bi-LSTM parser by Kiperwasser and Goldberg (2016) with default hyperparameters.4 We use the same domains as used for POS tagging, i.e., the dependency parsing data with gold POS as made available in the SANCL 2012 share"
D17-1038,D10-1069,0,0.0376712,"target domain used for learning metric S. B: Book. D: DVD. E: Electronics. K: Kitchen. Sim: term distributionbased similarity. Div: diversity. Best per feature set: bold. In-domain results: gray. SDAMS (Wu and Huang, 2016) listed as comparison. Transfer across models In addition, we are interested how well the metric learned for one target domain transfers to other settings. We first investigate its ability to transfer to another model. In practice, a metric can be learned using a model that is cheap to evaluate and serves as proxy for a state-of-the-art model, in a way similar to uptraining (Petrov et al., 2010). For this, we employ the data selection features learned using the Structured Perceptron model for POS tagging and use them to select data for the Bi-LSTM tagger. The results in Table 4 indicate that cross-model transfer is indeed possible, with most transferred feature sets achieving similar results or even outperforming features learned with the Bi-LSTM. In particular, transferred diversity significantly outperforms its in-model equivalent. This is encouraging, as it allows to learn a data selection metric using less complex models. 5.2 DS B D E K B D E K B D E K - with the highest performa"
D17-1038,P11-2120,0,0.0712149,"absence of target task data, we only require data of a related task to learn a successful data selection measure. 6 Related work Most prior work on data selection for transfer learning focuses on phrase-based machine translation. Typically language models are leveraged via perplexity or cross-entropy scoring to select target data (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Mirkin and Besacier, 2014). A recent study investigates data selection for neural machine translation (van der Wees et al., 2017). Perplexity was also used to select training data for dependency parsing (Søgaard, 2011), but has been found to be less suitable for tasks such as sentiment analysis (Ruder et al., 2017). In general, there are fewer studies on data selection for other tasks, e.g., constituent parsing (McClosky et al., 2010), dependency parsing (Plank and van Noord, 2011; Søgaard, 2011) and sentiment analysis (Remus, 2012). Work on predicting task accuracy is related, but can be seen as complementary (Ravi et al., 2008; Van Asch and Daelemans, 2010). Many domain similarity metrics have been proposed. Blitzer et al. (2007) show that proxy A distance can be used to measure the adaptability between t"
D17-1038,P11-1157,1,0.848495,"Missing"
D17-1038,P16-1013,0,0.303901,"training, causing a model’s performance to deteriorate substantially. Domain adaptation is a prominent approach to transfer learning that can help to bridge this gap; many approaches were suggested so far (Blitzer et al., 2007; Daumé III, 2007; Jiang and Zhai, 2007; Ma et al., 2014; Schnabel and Schütze, 2014). However, most work focused on one-toone scenarios. Only recently research considered using multiple sources. Such studies are rare and typically rely on specific model transfer approaches (Mansour, 2009; Wu and Huang, 2016). Inspired by work on curriculum learning (Bengio et al., 2009; Tsvetkov et al., 2016), we instead propose—to the best of our knowledge—the first model-agnostic data selection approach to trans2 Background: Transfer learning Transfer learning generally involves the concepts of a domain and a task (Pan and Yang, 2010). A domain D consists of a feature space X and a marginal probability distribution P (X) over X , 372 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 372–382 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics tion of feature values: where X = {x1 , · · · , xn } ∈ X . For document"
D17-1038,P16-2067,1,0.889797,"00-5000 labeled sentences and more than 100,000 unlabeled sentences. In the case of WSJ, we use its dev and test data as labeled samples and treat the remaining sections as unlabeled. We set n = 2000 for POS tagging and parsing to retain enough examples for the most-similar-domain baseline. To evaluate the impact of model choice, we compare two models: a Structured Perceptron (inhouse implementation with commonly used features pertaining to tags, words, case, prefixes, as well as prefixes and suffixes) trained for 5 iterations with a learning rate of 0.2; and a state-of-theart Bi-LSTM tagger (Plank et al., 2016) with word and character embeddings as input. We perform early stopping on the validation set with patience of 2 and use otherwise default hyperparameters3 as provided by the authors. 4.2 Training details In practice, as feature values occupy different ranges, we have found it helpful to apply znormalisation similar to Tsvetkov et al. (2016). We moreover constrain the weights w to [−1, 1]. For each dataset, we treat each domain as target domain and all other domains as source domains. Similar to Bousmalis et al. (2016), we chose to use a small number (100) target domain examples as validation"
D17-1038,W10-2605,0,0.472993,"Missing"
D17-1038,D17-1147,0,0.141376,"Missing"
D17-1038,D08-1093,0,0.151303,", 2014). A recent study investigates data selection for neural machine translation (van der Wees et al., 2017). Perplexity was also used to select training data for dependency parsing (Søgaard, 2011), but has been found to be less suitable for tasks such as sentiment analysis (Ruder et al., 2017). In general, there are fewer studies on data selection for other tasks, e.g., constituent parsing (McClosky et al., 2010), dependency parsing (Plank and van Noord, 2011; Søgaard, 2011) and sentiment analysis (Remus, 2012). Work on predicting task accuracy is related, but can be seen as complementary (Ravi et al., 2008; Van Asch and Daelemans, 2010). Many domain similarity metrics have been proposed. Blitzer et al. (2007) show that proxy A distance can be used to measure the adaptability between two domains in order to determine examples for annotation. Van Asch and Daelemans (2010) find that Rényi divergence outperforms other metrics in predicting POS tagging accuracy, while Plank and van Noord (2011) observe that topic distribution-based representations with Jensen-Shannon divergence perform best for data selection for parsing. Remus (2012) apply JensenShannon divergence to select training examples for se"
D17-1038,P16-1029,0,0.512777,"istribution of the test data is typically very different from the data used during training, causing a model’s performance to deteriorate substantially. Domain adaptation is a prominent approach to transfer learning that can help to bridge this gap; many approaches were suggested so far (Blitzer et al., 2007; Daumé III, 2007; Jiang and Zhai, 2007; Ma et al., 2014; Schnabel and Schütze, 2014). However, most work focused on one-toone scenarios. Only recently research considered using multiple sources. Such studies are rare and typically rely on specific model transfer approaches (Mansour, 2009; Wu and Huang, 2016). Inspired by work on curriculum learning (Bengio et al., 2009; Tsvetkov et al., 2016), we instead propose—to the best of our knowledge—the first model-agnostic data selection approach to trans2 Background: Transfer learning Transfer learning generally involves the concepts of a domain and a task (Pan and Yang, 2010). A domain D consists of a feature space X and a marginal probability distribution P (X) over X , 372 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 372–382 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computationa"
D17-1038,P10-2041,0,\N,Missing
D17-1038,N10-1004,0,\N,Missing
D18-1061,Q16-1022,1,0.912057,"Missing"
D18-1061,P13-1057,0,0.0191641,"pensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models: – aligned parallel corpora to project POS annotations to target languages (Yarowsky et al., 2001; Agi´c et al., 2015; Fang and Cohn, 2016), – noisy tag dictionaries for type-level approximation of full supervision (Li et al., 2012), – combination of projection and type constraints (Das and Petrov, 2011; T¨ackstr¨om et al., 2013), – rapid annotation of seed training data (Garrette and Baldridge, 2013; Garrette et al., 2013). However, only one or two compatible sources of distant supervision are typically employed. In reality severely under-resourced languages may require a more pragmatic “take what you can get” viewpoint. Our results suggest that combining supervision sources is the way to go about creating viable low-resource taggers. We propose a method to strike a balance between model simplicity and the capacity to easily integrate heterogeneous learning signals. Our system is a uniform neural model for POS tagging that learns from disparate sources of distant supervision (D S D S). We use it to combine: i)"
D18-1061,E17-2040,1,0.905539,"Missing"
D18-1061,W18-3401,1,0.886224,"Missing"
D18-1061,A00-1031,0,0.742426,"Missing"
D18-1061,Q16-1023,0,0.0929291,"Missing"
D18-1061,W06-2920,0,0.0763302,"Missing"
D18-1061,L16-1498,0,0.0355781,"n l-dimensional space. We represent ~esrc as concatenation of all embedded m properties of length l, and a zero vector otherwise. Tuning on the dev set, we found the second embedding approach to perform best, and simple concatenation outperformed mean vector representations. We evaluate two dictionary sources, motivated by ease of accessibility to many languages: W IK TIONARY , a word type dictionary that maps tokens to one of the 12 Universal POS tags (Li et al., 2012; Petrov et al., 2012); and U NI M ORPH, a morphological dictionary that provides inflectional paradigms across 350 languages (Kirov et al., 2016). For Wiktionary, we use the freely available dictionaries from Li et al. (2012) and Agi´c et al. (2017). The size of the dictionaries ranges from a few thousands (e.g., Hindi and Bulgarian) to 2M (Finnish UniMorph). Sizes are provided in Table 1, first columns. UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively). Word embeddings. Embeddings are available for many languages. Pre-initialization of w ~ offers consistent and considerable performance improvements in our distant supervision setup (Section 4). We use off-the-shelf Polyglot embeddings (AlRfou"
D18-1061,P17-1177,0,0.0384161,"the languages from Li et al. (2012) and all the remaining languages in Table 1. Cohn, 2016; Kann et al., 2018). Our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed. Most prior work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are unnecessary for deep learning methods. They rely on end-to-end training without resorting to additional linguistic resources. Our study shows that this is not the case. Only few prior studies investigate such sources, e.g., for MT (Sennrich and Haddow, 2016; Chen et al., 2017; Li et al., 2017; Passban et al., 2018) and Sagot and Mart´ınez Alonso (2017) for POS tagging use lexicons, but only as n-hot features and without examining the cross-lingual aspect. 7 Conclusions We show that our approach of distant supervision from disparate sources (D S D S) is simple yet surprisingly effective for low-resource POS tagging. Only 5k instances of projected data paired with off-the-shelf embeddings and lexical information integrated into a neural tagger are sufficient to reach a new state of the art, and both data selection and embeddings are essential components to boost neu"
D18-1061,P17-1064,0,0.0349681,"Li et al. (2012) and all the remaining languages in Table 1. Cohn, 2016; Kann et al., 2018). Our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed. Most prior work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are unnecessary for deep learning methods. They rely on end-to-end training without resorting to additional linguistic resources. Our study shows that this is not the case. Only few prior studies investigate such sources, e.g., for MT (Sennrich and Haddow, 2016; Chen et al., 2017; Li et al., 2017; Passban et al., 2018) and Sagot and Mart´ınez Alonso (2017) for POS tagging use lexicons, but only as n-hot features and without examining the cross-lingual aspect. 7 Conclusions We show that our approach of distant supervision from disparate sources (D S D S) is simple yet surprisingly effective for low-resource POS tagging. Only 5k instances of projected data paired with off-the-shelf embeddings and lexical information integrated into a neural tagger are sufficient to reach a new state of the art, and both data selection and embeddings are essential components to boost neural tagging perfo"
D18-1061,P11-1061,0,0.336529,"e languages lack manually annotated data to learn even the most basic models such as part-of-speech (POS) taggers. To compensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models: – aligned parallel corpora to project POS annotations to target languages (Yarowsky et al., 2001; Agi´c et al., 2015; Fang and Cohn, 2016), – noisy tag dictionaries for type-level approximation of full supervision (Li et al., 2012), – combination of projection and type constraints (Das and Petrov, 2011; T¨ackstr¨om et al., 2013), – rapid annotation of seed training data (Garrette and Baldridge, 2013; Garrette et al., 2013). However, only one or two compatible sources of distant supervision are typically employed. In reality severely under-resourced languages may require a more pragmatic “take what you can get” viewpoint. Our results suggest that combining supervision sources is the way to go about creating viable low-resource taggers. We propose a method to strike a balance between model simplicity and the capacity to easily integrate heterogeneous learning signals. Our system is a uniform"
D18-1061,D12-1127,0,0.229408,"Missing"
D18-1061,H01-1035,0,0.93712,"urprisingly effective, resulting in a new state of the art without access to any gold annotated data. Figure 1: Illustration of D S D S (Distant Supervision from Disparate Sources). 1 Introduction Low-resource languages lack manually annotated data to learn even the most basic models such as part-of-speech (POS) taggers. To compensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models: – aligned parallel corpora to project POS annotations to target languages (Yarowsky et al., 2001; Agi´c et al., 2015; Fang and Cohn, 2016), – noisy tag dictionaries for type-level approximation of full supervision (Li et al., 2012), – combination of projection and type constraints (Das and Petrov, 2011; T¨ackstr¨om et al., 2013), – rapid annotation of seed training data (Garrette and Baldridge, 2013; Garrette et al., 2013). However, only one or two compatible sources of distant supervision are typically employed. In reality severely under-resourced languages may require a more pragmatic “take what you can get” viewpoint. Our results suggest that combining supervision sources is the way t"
D18-1061,N18-1006,0,0.0796088,"Missing"
D18-1061,petrov-etal-2012-universal,0,0.086565,", with m the number of lexicon properties; b) by embedding the lexical features, i.e., ~esrc is a lexicon src embedded into an l-dimensional space. We represent ~esrc as concatenation of all embedded m properties of length l, and a zero vector otherwise. Tuning on the dev set, we found the second embedding approach to perform best, and simple concatenation outperformed mean vector representations. We evaluate two dictionary sources, motivated by ease of accessibility to many languages: W IK TIONARY , a word type dictionary that maps tokens to one of the 12 Universal POS tags (Li et al., 2012; Petrov et al., 2012); and U NI M ORPH, a morphological dictionary that provides inflectional paradigms across 350 languages (Kirov et al., 2016). For Wiktionary, we use the freely available dictionaries from Li et al. (2012) and Agi´c et al. (2017). The size of the dictionaries ranges from a few thousands (e.g., Hindi and Bulgarian) to 2M (Finnish UniMorph). Sizes are provided in Table 1, first columns. UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively). Word embeddings. Embeddings are available for many languages. Pre-initialization of w ~ offers consistent and consider"
D18-1061,P16-2067,1,0.92707,"Missing"
D18-1061,W17-6304,0,0.0822315,"Missing"
D18-1061,W16-2209,0,0.0294493,"M iterations, separate for the languages from Li et al. (2012) and all the remaining languages in Table 1. Cohn, 2016; Kann et al., 2018). Our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed. Most prior work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are unnecessary for deep learning methods. They rely on end-to-end training without resorting to additional linguistic resources. Our study shows that this is not the case. Only few prior studies investigate such sources, e.g., for MT (Sennrich and Haddow, 2016; Chen et al., 2017; Li et al., 2017; Passban et al., 2018) and Sagot and Mart´ınez Alonso (2017) for POS tagging use lexicons, but only as n-hot features and without examining the cross-lingual aspect. 7 Conclusions We show that our approach of distant supervision from disparate sources (D S D S) is simple yet surprisingly effective for low-resource POS tagging. Only 5k instances of projected data paired with off-the-shelf embeddings and lexical information integrated into a neural tagger are sufficient to reach a new state of the art, and both data selection and embeddings are essential comp"
D18-1061,Q13-1001,0,0.536998,"Missing"
D18-1061,C18-1327,0,0.0725938,"Missing"
D18-1061,W13-3520,0,\N,Missing
D18-1061,N13-1014,0,\N,Missing
D18-1061,P15-2044,1,\N,Missing
D18-1061,K16-1018,0,\N,Missing
D19-6408,K18-1030,0,0.273727,"beneficial than proposed local views which retain individual participant information. While gaze data is informative for supervised POS tagging, which complements previous findings on unsupervised POS induction, almost no improvement is obtained for binary phrase chunking, except for a single specific setup. Hence, caution is warranted when using gaze data as signal for NLP, as no single view is robust over tasks, modeling choice and gaze corpus. 1 Figure 1: Gaze (binned) captured during reading. in multi-task learning (Klerke et al., 2016; Hollenstein et al., 2019), gaze as word embeddings (Barrett et al., 2018b), gaze as type dictionaries (Barrett et al., 2016; Hollenstein and Zhang, 2019) and as attention (Barrett et al., 2018a). We follow this line of work and require no gaze data at test time. Choosing a gaze representation means choosing what to consider as signal and what to consider as noise. Aggregation is a way to implement this choice; where the kind of aggregation typically depends on the modeling framework. In this work we investigate how different levels of aggregation and the kind of variability preserved in representations of gaze duration from early and late processing states interac"
D19-6408,P16-2094,0,0.143354,"individual participant information. While gaze data is informative for supervised POS tagging, which complements previous findings on unsupervised POS induction, almost no improvement is obtained for binary phrase chunking, except for a single specific setup. Hence, caution is warranted when using gaze data as signal for NLP, as no single view is robust over tasks, modeling choice and gaze corpus. 1 Figure 1: Gaze (binned) captured during reading. in multi-task learning (Klerke et al., 2016; Hollenstein et al., 2019), gaze as word embeddings (Barrett et al., 2018b), gaze as type dictionaries (Barrett et al., 2016; Hollenstein and Zhang, 2019) and as attention (Barrett et al., 2018a). We follow this line of work and require no gaze data at test time. Choosing a gaze representation means choosing what to consider as signal and what to consider as noise. Aggregation is a way to implement this choice; where the kind of aggregation typically depends on the modeling framework. In this work we investigate how different levels of aggregation and the kind of variability preserved in representations of gaze duration from early and late processing states interact with two low-level syntactic sequence labeling ta"
D19-6408,N18-1184,0,0.653449,"beneficial than proposed local views which retain individual participant information. While gaze data is informative for supervised POS tagging, which complements previous findings on unsupervised POS induction, almost no improvement is obtained for binary phrase chunking, except for a single specific setup. Hence, caution is warranted when using gaze data as signal for NLP, as no single view is robust over tasks, modeling choice and gaze corpus. 1 Figure 1: Gaze (binned) captured during reading. in multi-task learning (Klerke et al., 2016; Hollenstein et al., 2019), gaze as word embeddings (Barrett et al., 2018b), gaze as type dictionaries (Barrett et al., 2016; Hollenstein and Zhang, 2019) and as attention (Barrett et al., 2018a). We follow this line of work and require no gaze data at test time. Choosing a gaze representation means choosing what to consider as signal and what to consider as noise. Aggregation is a way to implement this choice; where the kind of aggregation typically depends on the modeling framework. In this work we investigate how different levels of aggregation and the kind of variability preserved in representations of gaze duration from early and late processing states interac"
D19-6408,K15-1038,0,0.354505,"e; where the kind of aggregation typically depends on the modeling framework. In this work we investigate how different levels of aggregation and the kind of variability preserved in representations of gaze duration from early and late processing states interact with two low-level syntactic sequence labeling tasks. Specifically, we address the following questions: Introduction Digital traces of human cognitive processing can provide valuable signal for Natural Language Processing (Klerke et al., 2016; Plank, 2016a,b). One emerging source of information studied within NLP is eye-tracking data (Barrett and Søgaard, 2015a; Klerke et al., 2016; Mishra et al., 2017a; Jaffe et al., 2018; Barrett et al., 2018b; Hollenstein et al., 2019). While ubiquitous gaze recording remains unavailable, NLP research has focused on exploring the value of including gaze information from large, mostly disjointly labeled gaze datasets in recurrent neural network models. This models the assumption that no new gaze data will be available at test time. The proposed approaches under this paradigm include gaze as auxiliary task RQ1 Is a local view of individual gaze trace beneficial for syntactic sequence labeling in comparison to an a"
D19-6408,W15-2401,0,0.306255,"e; where the kind of aggregation typically depends on the modeling framework. In this work we investigate how different levels of aggregation and the kind of variability preserved in representations of gaze duration from early and late processing states interact with two low-level syntactic sequence labeling tasks. Specifically, we address the following questions: Introduction Digital traces of human cognitive processing can provide valuable signal for Natural Language Processing (Klerke et al., 2016; Plank, 2016a,b). One emerging source of information studied within NLP is eye-tracking data (Barrett and Søgaard, 2015a; Klerke et al., 2016; Mishra et al., 2017a; Jaffe et al., 2018; Barrett et al., 2018b; Hollenstein et al., 2019). While ubiquitous gaze recording remains unavailable, NLP research has focused on exploring the value of including gaze information from large, mostly disjointly labeled gaze datasets in recurrent neural network models. This models the assumption that no new gaze data will be available at test time. The proposed approaches under this paradigm include gaze as auxiliary task RQ1 Is a local view of individual gaze trace beneficial for syntactic sequence labeling in comparison to an a"
D19-6408,W18-0503,1,0.843606,"sequence of words or sentences. The most commonly applied feature extraction approach is based on counting durations of fixations, visits and re-visits per word as pioneered in the psycholinguistic tradition and most commonly aggregating to the global mean across multiple readers (see orange line in Figure 1). An alternative paradigm to psycholinguisticsbased feature extraction is to instead represent raw recorded scanpaths over entire word sequences as 2D or 3D matrices and images (von der Malsburg 1 E.g. total reading time subsumes first pass reading time entirely. 52 Augereau et al., 2016; Bingel et al., 2018). Noticeably, the opposite approach of using maximally aggregated type-level representations which average all readings across all occurrences and all participants, has also been shown to contribute to improvements (Barrett et al., 2016; Bingel et al., 2018; Hollenstein et al., 2019). The effect of these two different views (global vs local) on the same task hence remained unexplored and is a gap we seek to fill in this paper. We focus on the use of gaze for syntax-oriented NLP tasks, because human readers’ eye movements reflect necessary language processing work, including syntax parsing, to"
D19-6408,W16-1904,0,0.147684,"data for NLP (Barrett et al., 2016). Our results show that these type-level aggregates aid also supervised POS tagging, supporting further this type-level view. We here proposed a type-level view with novel global aggregation metrics and leveraging dictionary-based embeddings (Plank and Agi´c, 2018). Recent related work on gaze in NLP rely to a greater extent on the strong emotional and affective gaze responses associated with the semantic content of a text. These works include the classification of sentiment (Mishra et al., 2017b; Hollenstein et al., 2019), coreferences (Jaffe et al., 2018; Cheri et al., 2016), named entities (Hollenstein and Zhang, 2019), sarcasm (Mishra et al., 2016) and multi-word detection (Yaneva et al., 2017; Rohanian et al., 2017). 7 8 Token vs type-level Integrating the gaze data as type-level dictionary is the most beneficial and aids Part-of-Speech tagging, more than multi-task learning does. In particular, for the dictionarybased approach, we observe improvements in 9 out of 12 cases, yielding to up to +.23 absolute accuracy improvement. This shows that gaze data aids POS tagging also in our high-resource supervised POS tagging setup, which complements earlier findings r"
D19-6408,D13-1075,0,0.239527,"and one level of aggregation, we focus on multiple levels of aggregation and two NLP tasks. The latter study represents a group of studies where individual readers’ records are available at training time (i.e., multiple copies of the data with annotations obtained from different reading behaviour) rather than learning from the aggregate of multiple readers. This approach which involves a minimal level of aggregation is frequently applied where individual readers’ cognition is of primary interest, such as categorizing individual language skill level or behaviour (Mart´ınezG´omez et al., 2012; Matthies and Søgaard, 2013; Background and Motivation Eye movements during reading consist of fixations which are short stationary glances on individual words. These are interrupted by saccades, which are the ballistic movements between fixations. The gaze loosely traces the sequence of words in a text and gaze research in reading has at its basis the understanding that deviations from a monotone eye movement progression tend to occur when the reader’s cognitive processing is being challenged by the text. The raw gaze signal is a time series of (x, y)coordinates mapped to word positons on the screen and clustered into"
D19-6408,P17-1035,0,0.183977,"s on the modeling framework. In this work we investigate how different levels of aggregation and the kind of variability preserved in representations of gaze duration from early and late processing states interact with two low-level syntactic sequence labeling tasks. Specifically, we address the following questions: Introduction Digital traces of human cognitive processing can provide valuable signal for Natural Language Processing (Klerke et al., 2016; Plank, 2016a,b). One emerging source of information studied within NLP is eye-tracking data (Barrett and Søgaard, 2015a; Klerke et al., 2016; Mishra et al., 2017a; Jaffe et al., 2018; Barrett et al., 2018b; Hollenstein et al., 2019). While ubiquitous gaze recording remains unavailable, NLP research has focused on exploring the value of including gaze information from large, mostly disjointly labeled gaze datasets in recurrent neural network models. This models the assumption that no new gaze data will be available at test time. The proposed approaches under this paradigm include gaze as auxiliary task RQ1 Is a local view of individual gaze trace beneficial for syntactic sequence labeling in comparison to an aggregate global view, where information is"
D19-6408,N19-1001,0,0.438358,"information. While gaze data is informative for supervised POS tagging, which complements previous findings on unsupervised POS induction, almost no improvement is obtained for binary phrase chunking, except for a single specific setup. Hence, caution is warranted when using gaze data as signal for NLP, as no single view is robust over tasks, modeling choice and gaze corpus. 1 Figure 1: Gaze (binned) captured during reading. in multi-task learning (Klerke et al., 2016; Hollenstein et al., 2019), gaze as word embeddings (Barrett et al., 2018b), gaze as type dictionaries (Barrett et al., 2016; Hollenstein and Zhang, 2019) and as attention (Barrett et al., 2018a). We follow this line of work and require no gaze data at test time. Choosing a gaze representation means choosing what to consider as signal and what to consider as noise. Aggregation is a way to implement this choice; where the kind of aggregation typically depends on the modeling framework. In this work we investigate how different levels of aggregation and the kind of variability preserved in representations of gaze duration from early and late processing states interact with two low-level syntactic sequence labeling tasks. Specifically, we address"
D19-6408,C16-1059,1,0.832349,"onsider as signal and what to consider as noise. Aggregation is a way to implement this choice; where the kind of aggregation typically depends on the modeling framework. In this work we investigate how different levels of aggregation and the kind of variability preserved in representations of gaze duration from early and late processing states interact with two low-level syntactic sequence labeling tasks. Specifically, we address the following questions: Introduction Digital traces of human cognitive processing can provide valuable signal for Natural Language Processing (Klerke et al., 2016; Plank, 2016a,b). One emerging source of information studied within NLP is eye-tracking data (Barrett and Søgaard, 2015a; Klerke et al., 2016; Mishra et al., 2017a; Jaffe et al., 2018; Barrett et al., 2018b; Hollenstein et al., 2019). While ubiquitous gaze recording remains unavailable, NLP research has focused on exploring the value of including gaze information from large, mostly disjointly labeled gaze datasets in recurrent neural network models. This models the assumption that no new gaze data will be available at test time. The proposed approaches under this paradigm include gaze as auxiliary task RQ"
D19-6408,D18-1061,1,0.873332,"Missing"
D19-6408,W18-0101,0,0.0803985,"ework. In this work we investigate how different levels of aggregation and the kind of variability preserved in representations of gaze duration from early and late processing states interact with two low-level syntactic sequence labeling tasks. Specifically, we address the following questions: Introduction Digital traces of human cognitive processing can provide valuable signal for Natural Language Processing (Klerke et al., 2016; Plank, 2016a,b). One emerging source of information studied within NLP is eye-tracking data (Barrett and Søgaard, 2015a; Klerke et al., 2016; Mishra et al., 2017a; Jaffe et al., 2018; Barrett et al., 2018b; Hollenstein et al., 2019). While ubiquitous gaze recording remains unavailable, NLP research has focused on exploring the value of including gaze information from large, mostly disjointly labeled gaze datasets in recurrent neural network models. This models the assumption that no new gaze data will be available at test time. The proposed approaches under this paradigm include gaze as auxiliary task RQ1 Is a local view of individual gaze trace beneficial for syntactic sequence labeling in comparison to an aggregate global view, where information is traced via i) the cen"
D19-6408,P16-2067,1,0.83726,"rd embedding inputs fixed, which are set to 64 (size of the pre-trained Polyglot embeddings). We tune LSTM dimensions, character representations and hidden dimensions on the dev data. Early stopping was important to avoid overfitting of the auxiliary task.7 For binary chunking, the hyperparameters are: character and hidden dimensions 64, hidden layer size 100, cascaded model for MTL. For POS tagging, the parameters are: character input and hidIn all our experiments, we use a bidirectional long short-term memory network (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997; Plank et al., 2016) with a word encoding model which consists of a hierarchical model that combines pre-trained word embeddings with subword-character representations obtained from a recurrent character-based bi-LSTM. For chunking and the MTL setup, we use the cascaded model proposed by (Klerke et al., 2016): it predicts the chunks at the outermost stacked-biLSTM layer of a 3-layer stacked network; and it predicts the gaze label at the first bi-LSTM layer. Note that our model differs from theirs in that we add a subword bi-LSTM at the character level, which has shown to be effective for POS tagging. Moreover, fo"
D19-6408,N16-1179,1,0.268762,"res that capture the central tendency or variability of gaze data is more beneficial than proposed local views which retain individual participant information. While gaze data is informative for supervised POS tagging, which complements previous findings on unsupervised POS induction, almost no improvement is obtained for binary phrase chunking, except for a single specific setup. Hence, caution is warranted when using gaze data as signal for NLP, as no single view is robust over tasks, modeling choice and gaze corpus. 1 Figure 1: Gaze (binned) captured during reading. in multi-task learning (Klerke et al., 2016; Hollenstein et al., 2019), gaze as word embeddings (Barrett et al., 2018b), gaze as type dictionaries (Barrett et al., 2016; Hollenstein and Zhang, 2019) and as attention (Barrett et al., 2018a). We follow this line of work and require no gaze data at test time. Choosing a gaze representation means choosing what to consider as signal and what to consider as noise. Aggregation is a way to implement this choice; where the kind of aggregation typically depends on the modeling framework. In this work we investigate how different levels of aggregation and the kind of variability preserved in repr"
D19-6408,rohanian-etal-2017-using,0,0.0693149,"type-level view. We here proposed a type-level view with novel global aggregation metrics and leveraging dictionary-based embeddings (Plank and Agi´c, 2018). Recent related work on gaze in NLP rely to a greater extent on the strong emotional and affective gaze responses associated with the semantic content of a text. These works include the classification of sentiment (Mishra et al., 2017b; Hollenstein et al., 2019), coreferences (Jaffe et al., 2018; Cheri et al., 2016), named entities (Hollenstein and Zhang, 2019), sarcasm (Mishra et al., 2016) and multi-word detection (Yaneva et al., 2017; Rohanian et al., 2017). 7 8 Token vs type-level Integrating the gaze data as type-level dictionary is the most beneficial and aids Part-of-Speech tagging, more than multi-task learning does. In particular, for the dictionarybased approach, we observe improvements in 9 out of 12 cases, yielding to up to +.23 absolute accuracy improvement. This shows that gaze data aids POS tagging also in our high-resource supervised POS tagging setup, which complements earlier findings restricted to unsupervised POS induction with naturally lower baselines (Barrett and Søgaard, 2015a; Barrett et al., 2016). MTL leads to a few but n"
D19-6408,W00-0726,0,0.296139,"Missing"
D19-6408,W12-4904,0,0.0671303,"Missing"
E09-3005,J97-4005,0,0.0613588,"Noord and Malouf, 2005; van Noord, 2006) is a robust computational analyzer for Dutch that implements the conceptual two-stage parsing approach. The system consists of approximately 800 grammar rules in the tradition of HPSG, and a large hand-crafted lexicon, that together with a left-corner parser constitutes the generation component. For parse selection, Alpino employs a discriminative approach based on Maximum Entropy (MaxEnt). The output of the parser is dependency structure based on the guidelines of CGN (Oostdijk, 2000). The Maximum Entropy model (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997) is a conditional model that assigns a probability to every possible parse ω for a given sentence s. The model consists of a set of m feature functions fj (ω) that describe properties of parses, together with their associated weights θj . The denominator is a normalization term where Y (s) is the set of parses with yield s: P exp( m j=1 θj fj (ω)) Pm (1) pθ (ω|s; θ) = P y∈Y (s) exp( j=1 θj fj (y))) The parameters (weights) θj can be estimated efficiently by maximizing the regularized conditional likelihood of a training corpus (Johnson et al., 1999; van Noord and Malouf, 2005): Pm 2 j=1 θj ˆ ("
E09-3005,P99-1069,0,0.233588,"t of 8 teams. However, based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive.1 Thus, the effectiveness of SCL is rather unexplored for parsing. So far, most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea, 2001; Roark and Bacchiani, 2003; McClosky et al., 2006; Shimizu and Nakagawa, 2007), i.e. systems employing (constituent or dependency based) treebank grammars (Charniak, 1996). Parse selection constitutes an important part of many parsing systems (Johnson et al., 1999; Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006). Yet, the adaptation of parse selection models to novel domains is a far less studied area. This may be motivated by the fact that potential gains for this task are inherently bounded by the underlying grammar. The few studies on adapting disambiguation models (Hara et al., 2005; Plank and van Noord, 2008) have focused exclusively on the supervised scenario. Therefore, the direction we explore in this study is semi-supervised domain adaptation for parse disambiguation. We examine the effectiveness of Structural Corresponde"
E09-3005,W02-2018,0,0.0219227,"the Dutch part of Wikipedia as data collection, described in the following. 5 Experiments and Results 5.1 CA − base × 100 oracle − base Experimental design The base (source domain) disambiguation model is trained on the Alpino Treebank (van Noord, 2006) (newspaper text), which consists of approximately 7,000 sentences and 145,000 tokens. For parameter estimation of the disambiguation model, in all reported experiments we use the TADM2 toolkit (toolkit for advanced discriminative training), with a Gaussian prior (σ 2 =1000) and the (default) limited memory variable metric estimation technique (Malouf, 2002). For training the binary pivot predictors, we use the MegaM3 Optimization Package with the socalled ”bernoulli implicit” input format. To compute the SVD, we use SVDLIBC.4 The output of the parser is dependency structure. A standard evaluation metric is to measure the amount of generated dependencies that are identical to the stored dependencies (correct labeled dependencies), expressed as f-score. An alternative measure is concept accuracy (CA), which is similar to f-score, but allows possible discrepancy between the number of returned dependencies (van Noord, 2006; Plank and van Noord, 5.2"
E09-3005,W06-2911,0,0.136471,"data. The set of features used in Alpino is further described in van Noord and Malouf (2005). Matrix and SVD Following Blitzer et al. (2006) (which follow Ando and Zhang (2005)), we only use positive entries in the pivot predictors weight vectors to compute the SVD. Thus, when constructing the matrix W , we disregard all negative entries in W and compute the SVD (W = U DV T ) on the resulting non-negative sparse matrix. This sparse representation saves both time and space. 4.2 Further practical issues of SCL In practice, there are more free parameters and model choices (Ando and Zhang, 2005; Ando, 2006; Blitzer et al., 2006; Blitzer, 2008) besides the ones discussed above. Feature normalization and feature scaling. Blitzer et al. (2006) found it necessary to normalize and scale the new features obtained by the projection θ, in order to “allow them to receive more weight from a regularized discriminative learner”. For each of the features, they centered them by subtracting out the mean and normalized them to unit variance (i.e. x − mean/sd). They then rescaled the features by a factor α found on heldout data: αθx. Restricted Regularization. When training the supervised model on the augmented"
E09-3005,N06-1020,0,0.587106,"(Roark and Bacchiani, 2003; Hara et al., 2005; Daum´e III and Marcu, 2006; Daum´e III, 2007; Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007). We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daum´e III, 2007): supervised and semi-supervised. In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daum´e III, 2007), besides the labeled source data, we have access to a comparably small, but labeled amount of target data. In contrast, semi-supervised domain adaptation (Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007) is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain data. Semi-supervised adaptation is a much more realistic situation, while at the same time also considerably more difficult. Studies on the supervised task have shown that straightforward baselines (e.g. models based on source only, target only, or the union of the data) achieve a relatively high performance level and are “surprisingly difficult to beat” (Daum´e III, 2007). Thus, one conclusion from that line of work is that as soon as there is a reasona"
E09-3005,J96-1002,0,0.00596947,"3 Background: Alpino parser Alpino (van Noord and Malouf, 2005; van Noord, 2006) is a robust computational analyzer for Dutch that implements the conceptual two-stage parsing approach. The system consists of approximately 800 grammar rules in the tradition of HPSG, and a large hand-crafted lexicon, that together with a left-corner parser constitutes the generation component. For parse selection, Alpino employs a discriminative approach based on Maximum Entropy (MaxEnt). The output of the parser is dependency structure based on the guidelines of CGN (Oostdijk, 2000). The Maximum Entropy model (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997) is a conditional model that assigns a probability to every possible parse ω for a given sentence s. The model consists of a set of m feature functions fj (ω) that describe properties of parses, together with their associated weights θj . The denominator is a normalization term where Y (s) is the set of parses with yield s: P exp( m j=1 θj fj (ω)) Pm (1) pθ (ω|s; θ) = P y∈Y (s) exp( j=1 θj fj (y))) The parameters (weights) θj can be estimated efficiently by maximizing the regularized conditional likelihood of a training corpus (Johnson et al., 1999; van Noord a"
E09-3005,oostdijk-2000-spoken,0,0.0420567,"05; van Noord, 2006). For our empirical eval3 Background: Alpino parser Alpino (van Noord and Malouf, 2005; van Noord, 2006) is a robust computational analyzer for Dutch that implements the conceptual two-stage parsing approach. The system consists of approximately 800 grammar rules in the tradition of HPSG, and a large hand-crafted lexicon, that together with a left-corner parser constitutes the generation component. For parse selection, Alpino employs a discriminative approach based on Maximum Entropy (MaxEnt). The output of the parser is dependency structure based on the guidelines of CGN (Oostdijk, 2000). The Maximum Entropy model (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997) is a conditional model that assigns a probability to every possible parse ω for a given sentence s. The model consists of a set of m feature functions fj (ω) that describe properties of parses, together with their associated weights θj . The denominator is a normalization term where Y (s) is the set of parses with yield s: P exp( m j=1 θj fj (ω)) Pm (1) pθ (ω|s; θ) = P y∈Y (s) exp( j=1 θj fj (y))) The parameters (weights) θj can be estimated efficiently by maximizing the regularized conditional likelihood of a tr"
E09-3005,W06-1615,0,0.0721047,"ttention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daum´e III and Marcu, 2006; Daum´e III, 2007; Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007). We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daum´e III, 2007): supervised and semi-supervised. In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daum´e III, 2007), besides the labeled source data, we have access to a comparably small, but labeled amount of target data. In contrast, semi-supervised domain adaptation (Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007) is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain data. Semi-supervised adaptation is a much more realistic situation, while at the same time also considerably more difficult. Studies on the supervised task have shown that straightforward baselines (e.g. models based on source only, target only, or the union of the data) achieve a relatively high performance level and are “surprisingly difficult to beat” (Daum´e III, 2007). Thus, one conclusion from that line of work is that as soo"
E09-3005,W08-1302,1,0.726699,"Missing"
E09-3005,P07-1056,0,0.758822,"ult to beat” (Daum´e III, 2007). Thus, one conclusion from that line of work is that as soon as there is a reasonable (often even small) amount of labeled target data, it is often more fruitful to either just use that, or to apply simple adaptation techniques (Daum´e III, 2007; Plank and van Noord, 2008). The paper presents an application of Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for domain adaptation of a stochastic attribute-value grammar (SAVG). So far, SCL has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007). An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007), however, without any clear conclusions. We report on our exploration of applying SCL to adapt a syntactic disambiguation model and show promising initial results on Wikipedia domains. 1 Introduction Many current, effective natural language processing systems are based on supervised Machine Learning techniques. The parameters of such systems are estimated to best reflect the characteristics of the training data, at the cost of portability: a system will be successf"
E09-3005,N03-1027,0,0.0250241,"performance substantially over a state of the art baseline”. In the same shared task, an attempt was made to apply SCL to domain adaptation for data-driven dependency parsing (Shimizu and Nakagawa, 2007). The system just ended up at rank 7 out of 8 teams. However, based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive.1 Thus, the effectiveness of SCL is rather unexplored for parsing. So far, most previous work on domain adaptation for parsing has focused on data-driven systems (Gildea, 2001; Roark and Bacchiani, 2003; McClosky et al., 2006; Shimizu and Nakagawa, 2007), i.e. systems employing (constituent or dependency based) treebank grammars (Charniak, 1996). Parse selection constitutes an important part of many parsing systems (Johnson et al., 1999; Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006). Yet, the adaptation of parse selection models to novel domains is a far less studied area. This may be motivated by the fact that potential gains for this task are inherently bounded by the underlying grammar. The few studies on adapting disambiguation models (Hara et al., 2005; Plank and"
E09-3005,D07-1129,0,0.467588,"introduce the parsing system. Section 4 reviews Structural Correspondence Learning and shows our application of SCL to parse selection, including all our design choices. In Section 5 we present the datasets, introduce the process of constructing target domain data from Wikipedia, and discuss interesting initial empirical results of this ongoing study. are obtained. Similarly, Structural Correspondence Learning (Blitzer et al., 2006; Blitzer et al., 2007; Blitzer, 2008) has proven to be successful for the two tasks examined, PoS tagging and Sentiment Classification. In contrast, Dredze et al. (2007) report on “frustrating” results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. ”no team was able to improve target domain performance substantially over a state of the art baseline”. In the same shared task, an attempt was made to apply SCL to domain adaptation for data-driven dependency parsing (Shimizu and Nakagawa, 2007). The system just ended up at rank 7 out of 8 teams. However, based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive.1 Thus, the effectivene"
E09-3005,P07-1033,0,0.0460767,"Missing"
E09-3005,2006.jeptalnrecital-invite.2,0,0.0560716,"Missing"
E09-3005,W01-0521,0,0.24649,"d show promising initial results on Wikipedia domains. 1 Introduction Many current, effective natural language processing systems are based on supervised Machine Learning techniques. The parameters of such systems are estimated to best reflect the characteristics of the training data, at the cost of portability: a system will be successful only as long as the training material resembles the input that the model gets. Therefore, whenever we have access to a large amount of labeled data from some “source” (out-of-domain), but we would like a model that performs well on some new “target” domain (Gildea, 2001; Daum´e III, 2007), we face the problem of domain adaptation. The need for domain adaptation arises in many NLP tasks: Part-of-Speech tagging, Sentiment Analysis, Semantic Role Labeling or Statistical Parsing, to name but a few. For example, the performance of a statistical parsing system drops in an appalling way when a model trained on the Wall Street Journal is applied to the more varied Brown corpus (Gildea, 2001). 2 Motivation and Prior Work While several authors have looked at the supervised adaptation case, there are less (and especially less successful) studies on semi-supervised doma"
E09-3005,W07-2201,0,0.0309809,"Missing"
E09-3005,I05-1018,0,0.212779,"uctural Correspondence Learning for Parse Disambiguation Barbara Plank Alfa-informatica University of Groningen, The Netherlands b.plank@rug.nl Abstract The problem itself has started to get attention only recently (Roark and Bacchiani, 2003; Hara et al., 2005; Daum´e III and Marcu, 2006; Daum´e III, 2007; Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007). We distinguish two main approaches to domain adaptation that have been addressed in the literature (Daum´e III, 2007): supervised and semi-supervised. In supervised domain adaptation (Gildea, 2001; Roark and Bacchiani, 2003; Hara et al., 2005; Daum´e III, 2007), besides the labeled source data, we have access to a comparably small, but labeled amount of target data. In contrast, semi-supervised domain adaptation (Blitzer et al., 2006; McClosky et al., 2006; Dredze et al., 2007) is the scenario in which, in addition to the labeled source data, we only have unlabeled and no labeled target domain data. Semi-supervised adaptation is a much more realistic situation, while at the same time also considerably more difficult. Studies on the supervised task have shown that straightforward baselines (e.g. models based on source only, target"
E14-1078,J09-4005,0,0.0425861,"han others, and which is thus not reflected in learned predictors. We incorporate the annotator uncertainty on certain labels by measuring annotator agreement and use it in the modified loss function of a structured perceptron. We show that this approach works well independent of regularization, both on in-sample and out-of-sample data. Moreover, when evaluating the models trained with our loss function on downstream tasks, we observe improvements on two different tasks. Our results suggest that we need to pay more attention to annotator confidence when training predictors. In a similar vein, Klebanov and Beigman (2009) divide the instance space into easy and hard cases, i.e. easy cases are reliably annotated, whereas items that are hard show confusion and disagreement. Hard cases are assumed to be annotated by individual annotator’s coin-flips, and thus cannot be assumed to be uniformly distributed (Klebanov and Beigman, 2009). They show that learning with annotator noise can have deteriorating effect at test time, and thus propose to remove hard cases, both at test time (Klebanov and Beigman, 2009) and training time (Beigman and Klebanov, 2009). Acknowledgements We would like to thank the anonymous reviewe"
E14-1078,W02-1001,0,0.0419919,"and A2 another, and vice versa. We experiment with both agreement scores (F 1 and confusion matrix probabilities) to augment the loss function in our learner. The next section describes this modification in detail. 4 γ(yj , yi )) = 1 − P ({A1 (X), A2 (X)} = {yj , yi }) In both loss functions, a lower gamma value means that the tags are more likely to be confused by a pair of annotators. In this case, the update is smaller. In contrast, the learner incurs greater loss when easy tags are confused. It is straight-forward to extend these costsensitive loss functions to the structured perceptron (Collins, 2002). In Figure 4, we provide the pseudocode for the cost-sensitive structured online learning algorithm. We refer to the cost-sensitive structured learners as F 1- and CM-weighted below. Inter-annotator agreement loss We briefly introduce the cost-sensitive perceptron classifier. Consider the weighted perceptron loss on our ith example hxi , yi i (with learning rate α = 1), Lw (hxi , yi i): γ(sign(w · xi ), yi ) max(0, −yi w · xi ) 5 Experiments In our main experiments, we use structured perceptron (Collins, 2002) with random corruptions In a non-cost-sensitive classifier, the weight function γ(y"
E14-1078,P11-1061,0,0.00446396,"o disagree whether will is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP"
E14-1078,R13-1026,0,0.0146309,"Missing"
E14-1078,N13-1070,1,0.0361792,"2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP tasks that rely on syntactic features (Elming et al., 2013). Variation across annotated corpora in linguistic theory also poses challenges to intrinsic evaluation (Schwartz et al., 2011; Tsarfaty et al., 2012), as well as Our approach Instead of glossing over those annotation disagreements, we consider what happens if we embrace the uncertainty exhibited by human annotators 742 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742–751, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics R ITTER -T RAIN and G IMPEL -T RAIN and evaluate them on the remain"
E14-1078,J93-2004,0,0.0539354,"ussion on October 10th re the aftermath of #seanref . . . While linguists will agree that in is a preposition, and panel discussion a compound noun, they are likely to disagree whether will is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections o"
E14-1078,W10-0713,0,0.111643,"Missing"
E14-1078,I11-1100,0,0.0198995,"Missing"
E14-1078,N13-1039,0,0.0226252,"Missing"
E14-1078,fromreide-etal-2014-crowdsourcing,1,0.835032,", we do not have access to carefully annotated Twitter data for training, but rely on the crowdsourced annotations described in Finin et al. (2010). We use the concatenation of the CoNLL 2003 training split of annotated data from the Reuters corpus and the Finin data for training, as in this case training on the union resulted in a model that is substantially better than training on any of the individual data sets. For evaluation, we have three Twitter data set. We use the recently published data set from the MSM 2013 challenge (29k tokens)6 , the data set of Ritter et al. (2011) used also by Fromheide et al. (2014) (46k tokens), as well as an in-house annotated data set (20k tokens) (Fromheide et al., 2014). F1: BL CM R ITTER 78.20 78.30 MSM 82.25 82.00 7 Related work Cost-sensitive learning takes costs, such as misclassification cost, into consideration. That is, each instance that is not classified correctly during the learning process may contribute differently to the overall error. Geibel and Wysotzki (2003) introduce instance-dependent cost values for the perceptron algorithm and apply it to a set of binary classification problems. We focus here on structured problems and propose cost-sensitive lea"
E14-1078,petrov-etal-2012-universal,0,0.0301893,"l is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP tasks that rely on sy"
E14-1078,J08-3001,0,0.0420123,"red and the predicted tag is in the same class as the gold tag, a loss σ occurred, otherwise it counts as full cost. In contrast to our approach, they let the learner focus on the more difficult cases by occurring a bigger loss when the predicted POS tag I N -H OUSE 82.58 82.77 Table 3: Downstream results for named entity recognition (F1 scores). Table 3 shows the result of using our POS models in downstream NER evaluation. Here we observe mixed results. The cost-sensitive model is 5 http://www.ark.cs.cmu.edu/TweetNLP/ http://oak.dcs.shef.ac.uk/msm2013/ie_ challenge/ 6 748 dom but systematic (Reidsma and Carletta, 2008). However, rather than training on subsets of data or training separate models – which all implicitly assume that there is a large amount of training data available – we propose to integrate inter-annotator biases directly into the loss function. Regarding measurements for agreements, several scores have been suggested in the literature. Apart from the simple agreement measure, which records how often annotators choose the same value for an item, there are several statistics that qualify this measure by adjusting for other factors, such as Cohen’s κ (Cohen and others, 1960), the G-index score"
E14-1078,P11-2008,0,0.080131,"Missing"
E14-1078,W08-1203,0,0.405608,"Missing"
E14-1078,D11-1141,0,0.0112468,"s (Owoputi et al., 2013).5 For NER, we do not have access to carefully annotated Twitter data for training, but rely on the crowdsourced annotations described in Finin et al. (2010). We use the concatenation of the CoNLL 2003 training split of annotated data from the Reuters corpus and the Finin data for training, as in this case training on the union resulted in a model that is substantially better than training on any of the individual data sets. For evaluation, we have three Twitter data set. We use the recently published data set from the MSM 2013 challenge (29k tokens)6 , the data set of Ritter et al. (2011) used also by Fromheide et al. (2014) (46k tokens), as well as an in-house annotated data set (20k tokens) (Fromheide et al., 2014). F1: BL CM R ITTER 78.20 78.30 MSM 82.25 82.00 7 Related work Cost-sensitive learning takes costs, such as misclassification cost, into consideration. That is, each instance that is not classified correctly during the learning process may contribute differently to the overall error. Geibel and Wysotzki (2003) introduce instance-dependent cost values for the perceptron algorithm and apply it to a set of binary classification problems. We focus here on structured pr"
E14-1078,P11-1067,0,0.0210056,"entences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP tasks that rely on syntactic features (Elming et al., 2013). Variation across annotated corpora in linguistic theory also poses challenges to intrinsic evaluation (Schwartz et al., 2011; Tsarfaty et al., 2012), as well as Our approach Instead of glossing over those annotation disagreements, we consider what happens if we embrace the uncertainty exhibited by human annotators 742 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742–751, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics R ITTER -T RAIN and G IMPEL -T RAIN and evaluate them on the remaining data, the dev and test set provided by Foster et al. (2011) as well as an inhouse annotated data set of 3k tokens (see bel"
E14-1078,N03-1028,0,0.0984998,"Missing"
E14-1078,P13-2113,1,0.933032,"we incorporate the uncertainty exhibited by annotators in the training of our model. We measure inter-annotator agreement on small samples of data, then incorporate this in the loss function of a structured learner to reflect the confidence we can put in the annotations. This provides us with cost-sensitive online learning algorithms for inducing models from annotated data that take inter-annotator agreement into consideration. Specifically, we use online structured perceptron with drop-out, which has previously been applied to POS tagging and is known to be robust across samples and domains (Søgaard, 2013a). We incorporate the inter-annotator agreement in the loss function either as inter-annotator F 1-scores or as the confusion probability between annotators (see Section 3 below for a more detailed description). We use a small amounts of doublyannotated Twitter data to estimate F 1-scores and confusion probabilities, and incorporate them during training via a modified loss function. Specifically, we use POS annotations made by two annotators on a set of 500 newly sampled tweets to estimate our agreement scores, and train models on existing Twitter data sets (described below). We evaluate the"
E14-1078,N13-1077,1,0.937341,"we incorporate the uncertainty exhibited by annotators in the training of our model. We measure inter-annotator agreement on small samples of data, then incorporate this in the loss function of a structured learner to reflect the confidence we can put in the annotations. This provides us with cost-sensitive online learning algorithms for inducing models from annotated data that take inter-annotator agreement into consideration. Specifically, we use online structured perceptron with drop-out, which has previously been applied to POS tagging and is known to be robust across samples and domains (Søgaard, 2013a). We incorporate the inter-annotator agreement in the loss function either as inter-annotator F 1-scores or as the confusion probability between annotators (see Section 3 below for a more detailed description). We use a small amounts of doublyannotated Twitter data to estimate F 1-scores and confusion probabilities, and incorporate them during training via a modified loss function. Specifically, we use POS annotations made by two annotators on a set of 500 newly sampled tweets to estimate our agreement scores, and train models on existing Twitter data sets (described below). We evaluate the"
E14-1078,N13-1013,0,0.0467928,"Missing"
E14-1078,P12-1108,0,0.0190218,"ssification problems. We focus here on structured problems and propose cost-sensitive learning for POS tagging using the structured perceptron algorithm. In a similar spirit, Higashiyama et al. (2013) applied cost-sensitive learning to the structured perceptron for an entity recognition task in the medical domain. They consider the distance between the predicted and true label sequence smoothed by a parameter that they estimate on a development set. This means that the entire sequence is scored at once, while we update on a per-label basis. The work most related to ours is the recent study of Song et al. (2012). They suggest that some errors made by a POS tagger are more serious than others, especially for downstream tasks. They devise a hierarchy of POS tags for the Penn treebank tag set (e.g. the class NOUN contains NN, NNS, NNP, NNPS and CD) and use that in an SVM learner. They modify the Hinge loss that can take on three values: 0, σ, 1. If an error occurred and the predicted tag is in the same class as the gold tag, a loss σ occurred, otherwise it counts as full cost. In contrast to our approach, they let the learner focus on the more difficult cases by occurring a bigger loss when the predicte"
E14-1078,E12-1006,0,0.022199,"in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effects on downstream performance in NLP tasks that rely on syntactic features (Elming et al., 2013). Variation across annotated corpora in linguistic theory also poses challenges to intrinsic evaluation (Schwartz et al., 2011; Tsarfaty et al., 2012), as well as Our approach Instead of glossing over those annotation disagreements, we consider what happens if we embrace the uncertainty exhibited by human annotators 742 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 742–751, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics R ITTER -T RAIN and G IMPEL -T RAIN and evaluate them on the remaining data, the dev and test set provided by Foster et al. (2011) as well as an inhouse annotated data set of 3k tokens (see below). when learning predi"
E14-1078,I08-3008,0,0.00695885,"on a compound noun, they are likely to disagree whether will is heading the main verb take or vice versa. Even at a more basic level of analysis, it is not completely clear how to assign POS tags to each word in this sentence: is part a particle or a noun; is 10th a numeral or a noun? Some linguistic controversies may be resolved by changing the vocabulary of linguistic theory, e.g., by leaving out numerals or introducing ad hoc parts of speech, e.g. for English to (Marcus et al., 1993) or words ending in -ing (Manning, 2011). However, standardized label sets have practical advantages in NLP (Zeman and Resnik, 2008; Zeman, 2010; Das and Petrov, 2011; Petrov et al., 2012; McDonald et al., 2013). For these and other reasons, our annotators (even when they are trained linguists) often disagree on how to analyze sentences. The strategy in most previous work in NLP has been to monitor and later resolve disagreements, so that the final labels are assumed to be reliable when used as input to machine learning models. Introduction POS-annotated corpora and treebanks are collections of sentences analyzed by linguists according to some linguistic theory. The specific choice of linguistic theory has dramatic effect"
E14-1078,P09-1032,0,\N,Missing
E14-1078,hovy-etal-2014-pos,1,\N,Missing
E14-1078,P13-2017,0,\N,Missing
E17-1005,P98-1013,0,0.0583186,"), the size of the label inventory counting Blabels and I-labels as different (|Y |), and the proportion of out-of-span labels, which we refer to as O labels. The table also provides some of the information-theoretical measures we describe in Section 2.4. Note that D EP R ELS and POS are the only datasets without any O labels, while F RAMES and S EM T RAITS are the two tasks with O labels but no B/I-span notation, as tokens are annotated individually. Main tasks We use the following main tasks, aimed to represent a variety of semantic sequence labeling tasks. F RAMES: We use the FrameNet 1.5 (Baker et al., 1998) annotated corpus for a joint frame detection and frame identification tasks where a word can receive a predicate label like Arson or Personal success. We use the data splits from (Das et al., 2014; Hermann et al., 2014). While frame identification is normally treated as single classification, we keep the sequence-prediction paradigm so all main tasks rely on the same architecture. S UPERSENSES: We use the supersense version of SemCor (Miller et al., 1993) from (Ciaramita and Altun, 2006), with coarse-grained semantic labels like noun.person or verb.change. NER: The CONLL2003 shared-task data"
E17-1005,D15-1041,0,0.00538846,"rk is trained jointly such that the hidden representation captures the important information from the sequence for the prediction task. A bi-directional recurrent neural network (Graves and Schmidhuber, 2005) is an extension of an RNN that reads the input sequence twice, from left to right and right to left, and the encodings are concatenated. An LSTM (Long Short-Term Memory) is an extension of an RNN with more stable gradients (Hochreiter and Schmidhuber, 1997). Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015; Plank et al., 2016). For further details, cf. Goldberg (2015) and Cho (2015). We use an off-the-shelf bidirectional LSTM model (Plank et al., 2016).2 The model is illustrated in Figure 1. It is a context bi-LSTM taking as input word embeddings w. ~ Character embeddings ~c are incorporated via a hierarchical biLSTM using a sequence bi-LSTM at the lower level (Ballesteros et al., 2015; Plank et al., 2016). The character representation is concatenated with the (learned) word embeddings w ~ to form the input to the context bi-LSTM at the upper la"
E17-1005,C16-1333,1,0.672525,"e experiment with different data sources to control for label inventory size and corpus source for the auxiliary task. Introduction The recent success of recurrent neural networks (RNNs) for sequence prediction has raised a great deal of interest, which has lead researchers to propose competing architectures for several language-processing tasks. These architectures often rely on multitask learning (Caruana, 1997). Multitask learning (MTL) has been applied with success to a variety of sequence-prediction tasks including chunking and tagging (Collobert et al., 2011; Søgaard and Goldberg, 2016; Bjerva et al., 2016; Plank, 2016), name error detection (Cheng et al., 2015) and machine translation (Luong et al., 2016). However, little is known about MTL for tasks which are more semantic in nature, i.e., tasks that aim at labeling some aspect of the meaning of words (Cruse, 1986), instead their morphosyntactic behavior. In fact, results on semantic tasks are either mixed (Collobert et al., 2011) or, due to the file drawer bias (Rosenthal, 1979), simply not reported. There is no prior study—to From our empirical study we observe the MTL architecture’s sensitivity to label distribution properties, and its pre"
E17-1005,C16-1179,1,0.0804739,"or N ER. BL (w + c) nition (Cheng et al., 2015), tagging and chunking (Collobert et al., 2011; Plank et al., 2016), entity and relation extraction (Gupta et al., 2016), machine translation (Luong et al., 2016) and machine translation quality estimation including modeling annotator bias (Cohn and Specia, 2013; Shah and Specia, 2016). Most earlier work had in common that it assumed jointly labeled data (same corpus annotated with multiple labels). In contrast, in this paper we evaluate multitask training from distinct sources to address data paucity, like done recently (Kshirsagar et al., 2015; Braud et al., 2016; Plank, 2016). Sutton et al. (2007) demonstrate improvements for POS tagging by training a joint CRF model for both POS tagging and noun-phrase chunking. However, it is not clear under what conditions multi-task learning works. In fact, Collobert et al. (2011) train a joint feedforward neural network for POS, chunks and NER, and observe only improvements in chunking (similar to our findings, cf. Section 4.2), however, did not investigate data properties of these tasks. To the best of our knowledge, this is the first extensive evaluation of the effect of data properties and main-auxiliary task"
E17-1005,W06-1670,0,0.0684416,"g main tasks, aimed to represent a variety of semantic sequence labeling tasks. F RAMES: We use the FrameNet 1.5 (Baker et al., 1998) annotated corpus for a joint frame detection and frame identification tasks where a word can receive a predicate label like Arson or Personal success. We use the data splits from (Das et al., 2014; Hermann et al., 2014). While frame identification is normally treated as single classification, we keep the sequence-prediction paradigm so all main tasks rely on the same architecture. S UPERSENSES: We use the supersense version of SemCor (Miller et al., 1993) from (Ciaramita and Altun, 2006), with coarse-grained semantic labels like noun.person or verb.change. NER: The CONLL2003 shared-task data for named entity recognition for labels Person, Loc, etc. (Tjong Kim Sang and De Meulder, 2003). S EM T RAITS: We have used the EurWordNet list of ontological types for senses (Vossen et al., 1998) to convert the S UPERSENSES into coarser semantic traits like Animate or UnboundedEvent.1 MPQA: The Multi-Perspective Question Answering (MPQA) corpus (Deng and Wiebe, 2015), which contains sentiment information among others. We use the annotation corresponding to the 2.4 Information-theoretic"
E17-1005,P13-1004,0,0.0137168,"ngs are informative for N ER, because they approximate the well-known capitalization features in traditional models. Character features are not informative for tasks that are more dependent on word identity (like F RAMES), but are indeed useful for tasks where parts of the word can be informative, such as P OS or N ER. BL (w + c) nition (Cheng et al., 2015), tagging and chunking (Collobert et al., 2011; Plank et al., 2016), entity and relation extraction (Gupta et al., 2016), machine translation (Luong et al., 2016) and machine translation quality estimation including modeling annotator bias (Cohn and Specia, 2013; Shah and Specia, 2016). Most earlier work had in common that it assumed jointly labeled data (same corpus annotated with multiple labels). In contrast, in this paper we evaluate multitask training from distinct sources to address data paucity, like done recently (Kshirsagar et al., 2015; Braud et al., 2016; Plank, 2016). Sutton et al. (2007) demonstrate improvements for POS tagging by training a joint CRF model for both POS tagging and noun-phrase chunking. However, it is not clear under what conditions multi-task learning works. In fact, Collobert et al. (2011) train a joint feedforward neu"
E17-1005,P16-1101,0,0.0120729,"ine). All reported systems degrade around 0.50 points with regards to the baseline, except S UPERSENSES which improves slightly form 96.27 to 96.44. The high precision obtained for the also very difficult F RAMES tasks suggests that this architecture, while not suitable for frame disambiguation, can be used for frame-target identification. Disregarding F REQ B IN, the only low-level tasks that seems to aid prediction is POS. An interesting observation from the BIO task analysis is that while the standard bi-LSTM model used here does not have a Viterbi-style decoding like more complex systems (Ma and Hovy, 2016; Lample et al., 2016), we have found very few invalid BIO sequences. For N ER, there are only ten I-labels after an O-label, out of the 27K predicted by the bi-LSTM. For S UPERSENSES there are 59, out of 1,5K predicted I-labels. The amount of invalid predicted sequences is lower than expected, indicating that an additional decoding layer plays a smaller role in prediction quality than label distribution and corpus size, e.g. N ER is a large dataset with few labels, and the system has little difficulty in learning label precedences. For larger label sets or smaller data sizes, Results This sec"
E17-1005,J93-2004,0,0.0912746,"Missing"
E17-1005,N15-1146,0,0.00520123,"sks rely on the same architecture. S UPERSENSES: We use the supersense version of SemCor (Miller et al., 1993) from (Ciaramita and Altun, 2006), with coarse-grained semantic labels like noun.person or verb.change. NER: The CONLL2003 shared-task data for named entity recognition for labels Person, Loc, etc. (Tjong Kim Sang and De Meulder, 2003). S EM T RAITS: We have used the EurWordNet list of ontological types for senses (Vossen et al., 1998) to convert the S UPERSENSES into coarser semantic traits like Animate or UnboundedEvent.1 MPQA: The Multi-Perspective Question Answering (MPQA) corpus (Deng and Wiebe, 2015), which contains sentiment information among others. We use the annotation corresponding to the 2.4 Information-theoretic measures In order to quantify the properties of the different label distributions, we calculate three informationtheoretical quantities based on two metrics, kurtosis and entropy. Entropy is the best-known informationtheoretical metric. It indicates the amount of uncertainty in a distribution. We calculate two variants of entropy, one taking all labels in consideration H(Yf ull ), and another one H(Y−O ) where we discard the O label and only measure the entropy for the name"
E17-1005,H93-1061,0,0.117633,"n tasks We use the following main tasks, aimed to represent a variety of semantic sequence labeling tasks. F RAMES: We use the FrameNet 1.5 (Baker et al., 1998) annotated corpus for a joint frame detection and frame identification tasks where a word can receive a predicate label like Arson or Personal success. We use the data splits from (Das et al., 2014; Hermann et al., 2014). While frame identification is normally treated as single classification, we keep the sequence-prediction paradigm so all main tasks rely on the same architecture. S UPERSENSES: We use the supersense version of SemCor (Miller et al., 1993) from (Ciaramita and Altun, 2006), with coarse-grained semantic labels like noun.person or verb.change. NER: The CONLL2003 shared-task data for named entity recognition for labels Person, Loc, etc. (Tjong Kim Sang and De Meulder, 2003). S EM T RAITS: We have used the EurWordNet list of ontological types for senses (Vossen et al., 1998) to convert the S UPERSENSES into coarser semantic traits like Animate or UnboundedEvent.1 MPQA: The Multi-Perspective Question Answering (MPQA) corpus (Deng and Wiebe, 2015), which contains sentiment information among others. We use the annotation corresponding"
E17-1005,P15-1033,0,0.0135916,"s. The entire network is trained jointly such that the hidden representation captures the important information from the sequence for the prediction task. A bi-directional recurrent neural network (Graves and Schmidhuber, 2005) is an extension of an RNN that reads the input sequence twice, from left to right and right to left, and the encodings are concatenated. An LSTM (Long Short-Term Memory) is an extension of an RNN with more stable gradients (Hochreiter and Schmidhuber, 1997). Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015; Plank et al., 2016). For further details, cf. Goldberg (2015) and Cho (2015). We use an off-the-shelf bidirectional LSTM model (Plank et al., 2016).2 The model is illustrated in Figure 1. It is a context bi-LSTM taking as input word embeddings w. ~ Character embeddings ~c are incorporated via a hierarchical biLSTM using a sequence bi-LSTM at the lower level (Ballesteros et al., 2015; Plank et al., 2016). The character representation is concatenated with the (learned) word embeddings w ~ to form the input to the conte"
E17-1005,P16-2067,1,0.929504,", we take the index of the k-quantilized cumulative frequency for a word w. We use this parametric version of F REQ B IN with the median number of labels produced by the previous variants to examine the importance of the label distribution being skewed. For k=5, this variant maximizes the entropy of a F REQ B IN five-label distribution. Note that this method still places all hapaxes and outof-vocabulary words of the test data in the same frequency bin. F REQ B IN variants Recently, a simple auxiliary task has been proposed with success for POS tagging: predicting the log frequency of a token (Plank et al., 2016). The intuition behind this model is that the auxiliary loss, predicting word frequency, helps differentiate rare and common words, thus providing better predictions for frequency-sensitive labels. They refer to this auxiliary task as F REQ B IN, however, focus on POS only. Plank et al. (2016) used the discretized log frequency of the current word to build the F REQ B IN auxiliary task to aid POS Even though we could have used a reference corpus to have the same F REQ B IN for all the data, we prefer to use the main-task corpus for F RE Q B IN . Using an external corpus would otherwise lead to"
E17-1005,C16-1059,1,0.936533,"ferent data sources to control for label inventory size and corpus source for the auxiliary task. Introduction The recent success of recurrent neural networks (RNNs) for sequence prediction has raised a great deal of interest, which has lead researchers to propose competing architectures for several language-processing tasks. These architectures often rely on multitask learning (Caruana, 1997). Multitask learning (MTL) has been applied with success to a variety of sequence-prediction tasks including chunking and tagging (Collobert et al., 2011; Søgaard and Goldberg, 2016; Bjerva et al., 2016; Plank, 2016), name error detection (Cheng et al., 2015) and machine translation (Luong et al., 2016). However, little is known about MTL for tasks which are more semantic in nature, i.e., tasks that aim at labeling some aspect of the meaning of words (Cruse, 1986), instead their morphosyntactic behavior. In fact, results on semantic tasks are either mixed (Collobert et al., 2011) or, due to the file drawer bias (Rosenthal, 1979), simply not reported. There is no prior study—to From our empirical study we observe the MTL architecture’s sensitivity to label distribution properties, and its preference for co"
E17-1005,C16-1239,0,0.00762229,"about 2.5 points, namely M PQA and F RAMES. For the other two tasks we observe drops up to a maximum of 8-points for N ER. Character embeddings are informative for N ER, because they approximate the well-known capitalization features in traditional models. Character features are not informative for tasks that are more dependent on word identity (like F RAMES), but are indeed useful for tasks where parts of the word can be informative, such as P OS or N ER. BL (w + c) nition (Cheng et al., 2015), tagging and chunking (Collobert et al., 2011; Plank et al., 2016), entity and relation extraction (Gupta et al., 2016), machine translation (Luong et al., 2016) and machine translation quality estimation including modeling annotator bias (Cohn and Specia, 2013; Shah and Specia, 2016). Most earlier work had in common that it assumed jointly labeled data (same corpus annotated with multiple labels). In contrast, in this paper we evaluate multitask training from distinct sources to address data paucity, like done recently (Kshirsagar et al., 2015; Braud et al., 2016; Plank, 2016). Sutton et al. (2007) demonstrate improvements for POS tagging by training a joint CRF model for both POS tagging and noun-phrase chun"
E17-1005,P14-1136,0,0.00482107,"measures we describe in Section 2.4. Note that D EP R ELS and POS are the only datasets without any O labels, while F RAMES and S EM T RAITS are the two tasks with O labels but no B/I-span notation, as tokens are annotated individually. Main tasks We use the following main tasks, aimed to represent a variety of semantic sequence labeling tasks. F RAMES: We use the FrameNet 1.5 (Baker et al., 1998) annotated corpus for a joint frame detection and frame identification tasks where a word can receive a predicate label like Arson or Personal success. We use the data splits from (Das et al., 2014; Hermann et al., 2014). While frame identification is normally treated as single classification, we keep the sequence-prediction paradigm so all main tasks rely on the same architecture. S UPERSENSES: We use the supersense version of SemCor (Miller et al., 1993) from (Ciaramita and Altun, 2006), with coarse-grained semantic labels like noun.person or verb.change. NER: The CONLL2003 shared-task data for named entity recognition for labels Person, Loc, etc. (Tjong Kim Sang and De Meulder, 2003). S EM T RAITS: We have used the EurWordNet list of ontological types for senses (Vossen et al., 1998) to convert the S UPERS"
E17-1005,N16-1069,0,0.0210238,"N ER, because they approximate the well-known capitalization features in traditional models. Character features are not informative for tasks that are more dependent on word identity (like F RAMES), but are indeed useful for tasks where parts of the word can be informative, such as P OS or N ER. BL (w + c) nition (Cheng et al., 2015), tagging and chunking (Collobert et al., 2011; Plank et al., 2016), entity and relation extraction (Gupta et al., 2016), machine translation (Luong et al., 2016) and machine translation quality estimation including modeling annotator bias (Cohn and Specia, 2013; Shah and Specia, 2016). Most earlier work had in common that it assumed jointly labeled data (same corpus annotated with multiple labels). In contrast, in this paper we evaluate multitask training from distinct sources to address data paucity, like done recently (Kshirsagar et al., 2015; Braud et al., 2016; Plank, 2016). Sutton et al. (2007) demonstrate improvements for POS tagging by training a joint CRF model for both POS tagging and noun-phrase chunking. However, it is not clear under what conditions multi-task learning works. In fact, Collobert et al. (2011) train a joint feedforward neural network for POS, chu"
E17-1005,P16-2038,0,0.625312,"ibution, and iii) for P OS we experiment with different data sources to control for label inventory size and corpus source for the auxiliary task. Introduction The recent success of recurrent neural networks (RNNs) for sequence prediction has raised a great deal of interest, which has lead researchers to propose competing architectures for several language-processing tasks. These architectures often rely on multitask learning (Caruana, 1997). Multitask learning (MTL) has been applied with success to a variety of sequence-prediction tasks including chunking and tagging (Collobert et al., 2011; Søgaard and Goldberg, 2016; Bjerva et al., 2016; Plank, 2016), name error detection (Cheng et al., 2015) and machine translation (Luong et al., 2016). However, little is known about MTL for tasks which are more semantic in nature, i.e., tasks that aim at labeling some aspect of the meaning of words (Cruse, 1986), instead their morphosyntactic behavior. In fact, results on semantic tasks are either mixed (Collobert et al., 2011) or, due to the file drawer bias (Rosenthal, 1979), simply not reported. There is no prior study—to From our empirical study we observe the MTL architecture’s sensitivity to label distribution pr"
E17-1005,Q16-1023,0,0.00584593,"that the hidden representation captures the important information from the sequence for the prediction task. A bi-directional recurrent neural network (Graves and Schmidhuber, 2005) is an extension of an RNN that reads the input sequence twice, from left to right and right to left, and the encodings are concatenated. An LSTM (Long Short-Term Memory) is an extension of an RNN with more stable gradients (Hochreiter and Schmidhuber, 1997). Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015; Plank et al., 2016). For further details, cf. Goldberg (2015) and Cho (2015). We use an off-the-shelf bidirectional LSTM model (Plank et al., 2016).2 The model is illustrated in Figure 1. It is a context bi-LSTM taking as input word embeddings w. ~ Character embeddings ~c are incorporated via a hierarchical biLSTM using a sequence bi-LSTM at the lower level (Ballesteros et al., 2015; Plank et al., 2016). The character representation is concatenated with the (learned) word embeddings w ~ to form the input to the context bi-LSTM at the upper layers. For hyperparameter setting"
E17-1005,P15-2036,0,0.0197467,"nformative, such as P OS or N ER. BL (w + c) nition (Cheng et al., 2015), tagging and chunking (Collobert et al., 2011; Plank et al., 2016), entity and relation extraction (Gupta et al., 2016), machine translation (Luong et al., 2016) and machine translation quality estimation including modeling annotator bias (Cohn and Specia, 2013; Shah and Specia, 2016). Most earlier work had in common that it assumed jointly labeled data (same corpus annotated with multiple labels). In contrast, in this paper we evaluate multitask training from distinct sources to address data paucity, like done recently (Kshirsagar et al., 2015; Braud et al., 2016; Plank, 2016). Sutton et al. (2007) demonstrate improvements for POS tagging by training a joint CRF model for both POS tagging and noun-phrase chunking. However, it is not clear under what conditions multi-task learning works. In fact, Collobert et al. (2011) train a joint feedforward neural network for POS, chunks and NER, and observe only improvements in chunking (similar to our findings, cf. Section 4.2), however, did not investigate data properties of these tasks. To the best of our knowledge, this is the first extensive evaluation of the effect of data properties and"
E17-1005,N16-1030,0,0.00432161,"systems degrade around 0.50 points with regards to the baseline, except S UPERSENSES which improves slightly form 96.27 to 96.44. The high precision obtained for the also very difficult F RAMES tasks suggests that this architecture, while not suitable for frame disambiguation, can be used for frame-target identification. Disregarding F REQ B IN, the only low-level tasks that seems to aid prediction is POS. An interesting observation from the BIO task analysis is that while the standard bi-LSTM model used here does not have a Viterbi-style decoding like more complex systems (Ma and Hovy, 2016; Lample et al., 2016), we have found very few invalid BIO sequences. For N ER, there are only ten I-labels after an O-label, out of the 27K predicted by the bi-LSTM. For S UPERSENSES there are 59, out of 1,5K predicted I-labels. The amount of invalid predicted sequences is lower than expected, indicating that an additional decoding layer plays a smaller role in prediction quality than label distribution and corpus size, e.g. N ER is a large dataset with few labels, and the system has little difficulty in learning label precedences. For larger label sets or smaller data sizes, Results This section describes the res"
E17-1005,D15-1168,0,0.00719371,"captures the important information from the sequence for the prediction task. A bi-directional recurrent neural network (Graves and Schmidhuber, 2005) is an extension of an RNN that reads the input sequence twice, from left to right and right to left, and the encodings are concatenated. An LSTM (Long Short-Term Memory) is an extension of an RNN with more stable gradients (Hochreiter and Schmidhuber, 1997). Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015; Plank et al., 2016). For further details, cf. Goldberg (2015) and Cho (2015). We use an off-the-shelf bidirectional LSTM model (Plank et al., 2016).2 The model is illustrated in Figure 1. It is a context bi-LSTM taking as input word embeddings w. ~ Character embeddings ~c are incorporated via a hierarchical biLSTM using a sequence bi-LSTM at the lower level (Ballesteros et al., 2015; Plank et al., 2016). The character representation is concatenated with the (learned) word embeddings w ~ to form the input to the context bi-LSTM at the upper layers. For hyperparameter settings, see Section 3.1"
E17-1022,W15-5301,1,0.894616,"Missing"
E17-1022,P15-2044,1,0.884906,"Missing"
E17-1022,W09-0106,0,0.0296839,"t such a formalism lends itself more naturally to a simple and linguistically sound rulebased approach to cross-lingual parsing. In this paper we present such an approach. Our system is a dependency parser that requires no training, and relies solely on explicit part-ofspeech (POS) constraints that UD imposes. In particular, UD prescribes that trees are single-rooted, and that function words like adpositions, auxiliaries, and determiners are always dependents of content words, while other formalisms might treat them as heads (De Marneffe et al., 2014). We ascribe our work to the viewpoints of Bender (2009) about the incorporation of linguistic knowledge in language-independent systems. We propose UDP, the first training-free parser for Universal Dependencies (UD). Our algorithm is based on PageRank and a small set of head attachment rules. It features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD, which can be used as a baseline for such systems. The parser has very few parameters"
E17-1022,A00-1031,0,0.0296335,"ord forms. If there is more than one treebank per language, we use the treebank that has the 5.2 The resulting trees always pass the validation script in github.com/UniversalDependencies/tools. They also had a special connection to some extremists They - also had - • • • • a special connection - some extremists • • - to • • • • • - - Evaluation setup Our system relies solely on POS tags. To estimate the quality degradation of our system under non-gold POS scenarios, we evaluate UDP on two alternative scenarios. The first is predicted POS (UDPP ), where we tag the respective test set with TnT (Brants, 2000) trained on each language’s training set. The second is a naive typeconstrained two-POS tag scenario (UDPN ), and approximates a lower bound. We give each word either CONTENT or FUNCTION tag, depending on the word’s frequency. The 100 most frequent words of the input test section receive the FUNC TION tag. 4 −→ Baseline • • • • • - Table 4: Matrix representation of the directed graph for the words in the sentence. 234 Finally, we compare our parser UDP to a supervised cross-lingual system (MSD). It is a multisource delexicalized transfer parser, referred to as multi-dir in the original paper b"
E17-1022,P11-1061,0,0.0305356,"ing, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et al., 2011). These works mainly use supervised learning and domain adaptation techniques for the target language. The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies,"
E17-1022,de-marneffe-etal-2014-universal,0,0.0875731,"Missing"
E17-1022,W12-1909,0,0.0167797,"head attachment rules. It features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD, which can be used as a baseline for such systems. The parser has very few parameters and is distinctly robust to domain change across languages. 1 Introduction Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012). However, many data-driven approaches struggle with learning relations that match the conventions of the test data, e.g., Klein and Manning reported the tendency of their DMV parser to make determiners the heads of German nouns, which would not be an error if the test data used a DP analysis (Abney, 1987). Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences. The Universal Dependencies (UD) project (Nivre et al., 2015; Nivre et al., 2016) offers a dependency formalism that aims at providing a consistent represent"
E17-1022,P10-2036,0,0.0744508,"roach that takes a fresh angle on both aspects. Specifically, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploiting the UD constraints. These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is now a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et al., 2011). These works mainly use supervised learning and domain adaptation techniques for the target language. The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer ann"
E17-1022,P16-2091,1,0.781898,"Missing"
E17-1022,P06-1063,0,0.117258,"Missing"
E17-1022,P04-1061,0,0.494345,"eRank and a small set of head attachment rules. It features two-step decoding to guarantee that function words are attached as leaf nodes. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual parsing for UD, which can be used as a baseline for such systems. The parser has very few parameters and is distinctly robust to domain change across languages. 1 Introduction Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012). However, many data-driven approaches struggle with learning relations that match the conventions of the test data, e.g., Klein and Manning reported the tendency of their DMV parser to make determiners the heads of German nouns, which would not be an error if the test data used a DP analysis (Abney, 1987). Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences. The Universal Dependencies (UD) project (Nivre et al., 2015; Nivre et al., 2016) offers a dependency formalism that aims at providing"
E17-1022,P14-1126,0,0.0366931,"et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann et al. (2014; 2016) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora for annotation projection (Ma and Xia, 2014; Rasooli and Collins, 2015). The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al. (2011) independently extended the approach by using multiple sources, requiring uniform POS and dependency representations (McDonald et al., 2013). Both model transfer and annotation projection rely on a large number of presumptions to derive their compet"
E17-1022,P13-2109,0,0.0770734,"Missing"
E17-1022,D11-1006,0,0.500127,"y few parameters and is distinctly robust to domain change across languages. 1 Introduction Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012). However, many data-driven approaches struggle with learning relations that match the conventions of the test data, e.g., Klein and Manning reported the tendency of their DMV parser to make determiners the heads of German nouns, which would not be an error if the test data used a DP analysis (Abney, 1987). Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences. The Universal Dependencies (UD) project (Nivre et al., 2015; Nivre et al., 2016) offers a dependency formalism that aims at providing a consistent representation across languages, while enforcing a few hard constraints. The arrival of such treebanks, expanded and improved on a regular basis, provides a new milestone for crosslingual dependency parsing research (McDonald et al., 2013). Contributions We introduce, to the best of our knowledge, the first unsupervised rule-based dependency parser for Universal Dependencies"
E17-1022,W10-2105,1,0.869888,"Missing"
E17-1022,D15-1039,0,0.0523361,"ereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann et al. (2014; 2016) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora for annotation projection (Ma and Xia, 2014; Rasooli and Collins, 2015). The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al. (2011) independently extended the approach by using multiple sources, requiring uniform POS and dependency representations (McDonald et al., 2013). Both model transfer and annotation projection rely on a large number of presumptions to derive their competitive parsing models. By and"
E17-1022,N10-1116,0,0.0314682,"hey typically do not exploit constraints placed on linguistic structures through a formalism, and they do so by design. With the emergence of UD as the practical standard for multilingual POS and syntactic dependency annotation, we argue for an approach that takes a fresh angle on both aspects. Specifically, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploiting the UD constraints. These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is now a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et"
E17-1022,D10-1120,0,0.0928531,"on both aspects. Specifically, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploiting the UD constraints. These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is now a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et al., 2011). These works mainly use supervised learning and domain adaptation techniques for the target language. The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource"
E17-1022,W10-2902,0,0.029288,"hey typically do not exploit constraints placed on linguistic structures through a formalism, and they do so by design. With the emergence of UD as the practical standard for multilingual POS and syntactic dependency annotation, we argue for an approach that takes a fresh angle on both aspects. Specifically, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploiting the UD constraints. These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is now a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of these parsers falls far behind the approaches involving any sort of supervision. Our work builds on the line of research on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et"
E17-1022,W12-1910,1,0.850332,"ed on the fly at runtime. We refer henceforth to our UD parser as UDP. 231 3.1 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: PageRank setup Our system uses the PageRank (PR) algorithm (Page et al., 1999) to estimate the relevance of the content words of a sentence. PR uses a random walk to estimate which nodes in the graph are more likely to be visited often, and thus, it gives higher rank to nodes with more incoming edges, as well as to nodes connected to those. Using PR to score word relevance requires an effective graphbuilding strategy. We have experimented with the strategies by Søgaard (2012b), such as words being connected to adjacent words, but our system fares best strictly using the dependency rules in Table 1 to build the graph. UD trees are often very flat, and a highly connected graph yields a PR distribution that is closer to uniform, thereby removing some of the difference of word relevance. We build a multigraph of all words in the sentence covered by the head-dependent rules in Table 1, giving each word an incoming edge for each eligible dependent, i.e., ADV depends on ADJ and VERB . This strategy does not always yield connected graphs, and we use a teleport probabilit"
E17-1022,C14-1175,0,0.0824867,"Missing"
E17-1022,H01-1035,0,0.0694204,"on ruleaided unsupervised dependency parsing by Gillenwater et al. (2010) and Naseem et al. (2010), and also relates to Søgaard’s (2012a; 2012b) work. Our parser, however, features two key differences: Related work Cross-lingual learning Recent years have seen exciting developments in cross-lingual linguistic structure prediction based on transfer or projection of POS and dependencies (Das and Petrov, 2011; McDonald et al., 2011). These works mainly use supervised learning and domain adaptation techniques for the target language. The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann et al. (2014; 2016) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora for annotation projection (Ma and Xia, 20"
E17-1022,I08-3008,0,0.275845,"s. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann et al. (2014; 2016) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora for annotation projection (Ma and Xia, 2014; Rasooli and Collins, 2015). The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al. (2011) independently extended the approach by using multiple sources, requiring uniform POS and dependency representations (McDonald et al., 2013). Both model transfer and annotation projection rely on a large number of presumptions to derive their competitive parsing models. By and large, these presumptions are unrealistic and exclusive to a group of very closely related, resource-rich IndoEuropean lan"
E17-1022,Q16-1022,1,\N,Missing
E17-2040,P15-2044,1,0.928871,"Missing"
E17-2040,Q16-1022,1,0.909051,"Missing"
E17-2040,petrov-etal-2012-universal,0,0.0285098,"We express the quality of predicted rankings using precision (P@1) and Kendall’s τb statistic (Knight, 1966). Data. We train and test our taggers on data from UD version 1.2 (Nivre et al., 2015). We intersect this collection with the dictionaries we make available for this experiment: 9 of the Wiktionaries come from Li et al. (2012), and we collect 16 new on top of that. Thus, we experiment with a total of 25 languages from the UD. We refer to the 9 languages of Li et al. (2012) as development languages. To make the Wiktionaries and the UD data compatible, we map all POS tags to the tagset by Petrov et al. (2012). We estimate the frequencies for the +freq variants of the soft metrics by using the multilingual Bible corpus by Christodouloupoulos and Steedman (2014) and the Watchtower corpus (Agi´c et al., 2016) combined. We translate the English Wiktionary from Li et al. (2012) by using bilingual dictionaries from Wiktionary to obtain Dtrans for 20 languages.3 precision(D, G) = ∑i=1 ∣D∣ ∣{Di ∩Gi }∣ ∣{t∈Di }∣ recall(D, G) = ∑i=1 ∣D∣ ∣{Di ∩Gi }∣ ∣{t∈Gi }∣ Namely, for each word wi covered by both D and G, we check how many tags Di and Gi intersect, and then use the intersection to estimate dictionary prec"
E17-2040,P13-2109,0,0.0384735,"Missing"
E17-2040,K15-1033,1,0.885938,"Missing"
E17-2040,P13-2017,0,0.0419349,"Missing"
E17-2040,P16-2067,1,0.880248,"Missing"
E17-2040,A00-1031,0,0.765935,"Missing"
E17-2040,N13-1014,0,0.131467,"Missing"
E17-2040,D13-1032,0,0.0602914,"Missing"
E17-2040,P15-2111,0,0.0278218,"Missing"
E17-2040,D12-1127,0,\N,Missing
hovy-etal-2014-pos,zeman-2008-reusable,0,\N,Missing
hovy-etal-2014-pos,J08-3001,0,\N,Missing
hovy-etal-2014-pos,I11-1100,0,\N,Missing
hovy-etal-2014-pos,N13-1037,0,\N,Missing
hovy-etal-2014-pos,R13-1026,0,\N,Missing
hovy-etal-2014-pos,petrov-etal-2012-universal,0,\N,Missing
hovy-etal-2014-pos,D11-1141,0,\N,Missing
hovy-etal-2014-pos,P11-2008,0,\N,Missing
hovy-etal-2014-pos,N13-1039,0,\N,Missing
I17-4024,P16-2067,1,0.844873,"e-shelf tokenizers. We use tinysegmenter2 for Japanese and the NLTK TweetTokenizer for all other languages. The Japanese segmenter was crucial to get sufficient coverage from the word embeddings later. No additional preprocessing is performed. 3.4 Model and Features Additionally, we experimented with adding Part-Of-Speech (POS) tags to our model. However, to keep in line with our goal to build a single system for all languages we trained a single multilingual POS tagger by exploiting the projected multilingual embeddings. In particular, we trained a state-of-the-art bidirectional LSTM tagger (Plank et al., 2016)4 that uses both word and character representations on the concatenation of language-specific data provided from the Universal Dependencies data (version 1.2 for En, Fr and Es and version 2.0 data for Japanese, as the latter was not available in free-form in the earlier version). The word embeddings module of the tagger is initialized with the multilingual embeddings. We investigated POS n-grams (1 to 3 grams) as additional features. Multilingual Embeddings Word embeddings for single languages are readily available, for example the Polyglot3 or Facebook embeddings (Bojanowski et al., 2016), wh"
I17-4024,W13-3520,0,0.0838574,"Missing"
I17-4024,W17-1201,0,0.0486258,"Missing"
I17-4024,W17-5043,1,0.859241,"Missing"
I17-4024,I17-4004,0,0.140879,"aries) and is readily applicable to off-the-shelf embeddings. In brief, the approach aims at learning a transformation in which word vector spaces are orthogonal (by applying SVD) and it leverages so-called “pseudodictionaries”. That is, the method first finds the common word types in two embedding spaces, and uses those as pivots to learn to align the two spaces (cf. further details in Smith et al. (2017)). 3 T RAIN D EV T EST EN ES FR JP 3066 501 501 1632 302 300 1951 401 401 1527 251 301 Table 1: Overview of the dataset (instances). 3.1 Task Description The customer feedback analysis task (Liu et al., 2017) is a short text classification task. Given a customer feedback message, the goal is to detect the type of customer feedback. For each message, the organizers provided one or more labels. To give a more concrete idea of the data, the following are examples of the English dataset: • “Still calls keep dropping with the new update” (bug) • “Room was grubby, mold on windows frames.” (complaint) • “The new update is amazing.” (comment) • “Needs more control s and tricks..” (request) • “Enjoy the sunshine!!” (meaningless) 3.2 Data The data stems from a joint ADAPT-Microsoft project. An overview of t"
I17-4024,W17-5007,0,0.0181842,"AFNLP Our key motivation is to provide a simple, general system as opposed to the usual ad-hoc setups one can expect in a multilingual shared task. So we rely on character n-grams, word embeddings, and a traditional classifier, motivated as follows. First, character n-grams and traditional machine learning algorithms have proven successful for a variety of classification tasks, e.g., native language identification and language detection. In recent shared tasks simple traditional models outperformed deep neural approaches like CNNs or RNNs, e.g., (Medvedeva et al., 2017; Zampieri et al., 2017; Malmasi et al., 2017; Kulmizev et al., 2017). This motivated our choice of using a traditional model with character n-gram features. Second, we build upon the recent success of multilingual embeddings. These are embedding spaces in which word types of different languages are embedded into the same high-dimensional space. Early approaches focus mainly on bilingual approaches, while recent research aims at mapping several languages into a single space. The body of literature is huge, but an excellent recent overview is given in Ruder (2017). We chose a very simple and recently proposed method that does not rely on"
I17-4024,W17-1219,1,0.787591,", Taiwan, November 27 – December 1, 2017. 2017 AFNLP Our key motivation is to provide a simple, general system as opposed to the usual ad-hoc setups one can expect in a multilingual shared task. So we rely on character n-grams, word embeddings, and a traditional classifier, motivated as follows. First, character n-grams and traditional machine learning algorithms have proven successful for a variety of classification tasks, e.g., native language identification and language detection. In recent shared tasks simple traditional models outperformed deep neural approaches like CNNs or RNNs, e.g., (Medvedeva et al., 2017; Zampieri et al., 2017; Malmasi et al., 2017; Kulmizev et al., 2017). This motivated our choice of using a traditional model with character n-gram features. Second, we build upon the recent success of multilingual embeddings. These are embedding spaces in which word types of different languages are embedded into the same high-dimensional space. Early approaches focus mainly on bilingual approaches, while recent research aims at mapping several languages into a single space. The body of literature is huge, but an excellent recent overview is given in Ruder (2017). We chose a very simple and re"
J17-4007,W13-2212,0,0.0736247,"Missing"
J17-4007,P13-1166,0,0.125253,"is not clear what is learned; and (b) when algorithms are reimplemented for replicability purposes. Regarding (a), we think that differences in used parameter settings are not actually a bad thing; we learn from this that we overfit on the previous task, or that we need to adapt our systems to another data set or domain. Regarding (b), this is a real problem because starting from scratch to reimplement existing systems is unnecessarily time-consuming. In addition, it would always be desirable to be able to directly reproduce the same results of the same model for the same task (Pedersen 2008; Fokkens et al. 2013). Withdrawal from Competition. Participants may withdraw from a shared task if their ranking in the competition can negatively affect their reputation and/or future funding. For example, Parra Escart´ın et al. (2017) suggest that companies might prefer to withdraw from the competition if they are not highly ranked, to avoid blemishing their reputation. This is something that we could not quantify in our survey, as in case of withdrawal there would be no evidence of participation in reports. There are two aspects, though, that we can quantify. The first aspect is the number of teams that do not"
J17-4007,W13-3601,0,0.0214099,"o build on others’ work to try out new variations of a method, without having to reimplement things from scratch. To ensure this, it is desirable that everything needed to reproduce experimental results is publicly and freely available, including code, data, pre-trained models, and so on. Interestingly, at the CoNLL-2013 shared task a similar step was taken, but only in terms of pre-condition: “While all teams in the shared task use the NUCLE corpus, they are also allowed to use additional external resources (both corpora and tools) so long as they are publicly available and not proprietary” (Ng et al. 2013). We would like to take this a step further, by enforcing the sharing of whatever resource teams might choose to use, so as to favor the injection of new resources in the field. Applying this principle to shared tasks in practice, we propose making the primary competition a “public track,” where participants can use any code, data, and pre-trained models they want, as long as others can then freely obtain them. In other words: All resources used to participate in the shared task should be subsequently shared with the community. Although this does not ensure equal access to resources for the cu"
J17-4007,W17-1608,0,0.103254,"Missing"
J17-4007,J08-3010,0,\N,Missing
J17-4007,W14-1701,0,\N,Missing
K15-1033,E06-1040,0,0.0477065,"ng et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. Using automatic measures such as labeled and u"
K15-1033,P02-1040,0,0.100561,"appropriate downstream tasks (Elming et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. Using automa"
K15-1033,W06-2920,0,0.0844944,"0 sentences for each of the 5 languages) annotated with human judgments for the preferred automatically parsed dependency tree, enabling further research in this direction. 2 UCP = LCP = We evaluate seven dependency parsing metrics, described in this section. Given a labeled gold tree G = hV, EG , lG (·)i and a labeled predicted tree P = hV, EP , lP (·)i, let E ⊂ V × V be the set of directed edges from dependents to heads, and let l : V × V → L be the edge labeling function, with L the set of dependency labels. The three most commonly used metrics are those from the CoNLL 2006–7 shared tasks (Buchholz and Marsi, 2006): unlabeled attachment score (UAS), label accuracy (LA), both introduced by Eisner (1996), and labeled attachment score (LAS), the pivotal dependency parsing metric introduced by Nivre et al. (2004). LAS = 3 Experiment In our analysis, we compare the metrics with human judgments. We examine how well the automatic metrics correlate with each other, as well as with human judgments, and whether interannotator agreement exceeds annotator-metric agreement. |{e |e ∈ EG ∩ EP }| |V | |{e |lG (e) = lP (e), e ∈ EG ∩ EP }| |V | LA = |{v |Vverb , cG (v) = cP (v) ∧ lG (v, ·) = lP (v, ·)}| |Vverb | For the"
K15-1033,W07-0718,0,0.188994,"e human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. Using automatic measures such as labeled and unlabeled attachment scores is"
K15-1033,P11-1067,0,0.0697005,"Missing"
K15-1033,C96-1058,0,0.12707,"ly parsed dependency tree, enabling further research in this direction. 2 UCP = LCP = We evaluate seven dependency parsing metrics, described in this section. Given a labeled gold tree G = hV, EG , lG (·)i and a labeled predicted tree P = hV, EP , lP (·)i, let E ⊂ V × V be the set of directed edges from dependents to heads, and let l : V × V → L be the edge labeling function, with L the set of dependency labels. The three most commonly used metrics are those from the CoNLL 2006–7 shared tasks (Buchholz and Marsi, 2006): unlabeled attachment score (UAS), label accuracy (LA), both introduced by Eisner (1996), and labeled attachment score (LAS), the pivotal dependency parsing metric introduced by Nivre et al. (2004). LAS = 3 Experiment In our analysis, we compare the metrics with human judgments. We examine how well the automatic metrics correlate with each other, as well as with human judgments, and whether interannotator agreement exceeds annotator-metric agreement. |{e |e ∈ EG ∩ EP }| |V | |{e |lG (e) = lP (e), e ∈ EG ∩ EP }| |V | LA = |{v |Vverb , cG (v) = cP (v) ∧ lG (v, ·) = lP (v, ·)}| |Vverb | For the final figure of seven different parsing metrics, on top of the previous five, in our expe"
K15-1033,C12-1147,0,0.117595,"r NLP tasks. Also, inter-annotator agreement is sometimes higher than the agreement between judgments and metrics, indicating that the standard metrics fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech. 1 Introduction In dependency parser evaluation, the standard accuracy metrics—labeled and unlabeled attachment scores—are defined simply as averages over correct attachment decisions. Several authors have pointed out problems with these metrics; they are both sensitive to annotation guidelines (Schwartz et al., 2012; Tsarfaty et al., 2011), and they fail to say anything about how parsers fare on rare, but important linguistic constructions (Nivre et al., 2010). Both criticisms rely on the intuition that some parsing errors are more important than others, and that our metrics should somehow reflect that. There are sentences that are hard to annotate because they are ambiguous, or because they contain phenomena peripheral to linguistic theory, such as punctuation, clitics, or fragments. Manning (2011) discusses similar issues for part-ofspeech tagging. 315 Proceedings of the 19th Conference on Computationa"
K15-1033,N13-1070,1,0.846825,"ges: Croatian, Danish, English, German, and Spanish. For the human judgments, we asked professional linguists with dependency annotation experience to judge which of two parsers produced the better parse. Our stance here is that, insofar experts are able to annotate dependency trees, they are also able to determine the quality of a predicted syntactic structure, which we can in turn use to evaluate parser evaluation metrics. Even though downstream evaluation is critical in assessing the usefulness of parses, it also presents non-trivial challenges in choosing the appropriate downstream tasks (Elming et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter,"
K15-1033,D11-1036,0,0.220614,"r-annotator agreement is sometimes higher than the agreement between judgments and metrics, indicating that the standard metrics fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech. 1 Introduction In dependency parser evaluation, the standard accuracy metrics—labeled and unlabeled attachment scores—are defined simply as averages over correct attachment decisions. Several authors have pointed out problems with these metrics; they are both sensitive to annotation guidelines (Schwartz et al., 2012; Tsarfaty et al., 2011), and they fail to say anything about how parsers fare on rare, but important linguistic constructions (Nivre et al., 2010). Both criticisms rely on the intuition that some parsing errors are more important than others, and that our metrics should somehow reflect that. There are sentences that are hard to annotate because they are ambiguous, or because they contain phenomena peripheral to linguistic theory, such as punctuation, clitics, or fragments. Manning (2011) discusses similar issues for part-ofspeech tagging. 315 Proceedings of the 19th Conference on Computational Language Learning, pag"
K15-1033,N13-1132,0,0.022444,"78 .437 .250* .469 .404 .230* .195* .297 .331 .232 .318 .323 .171 .223 .466 .453 .310 .501 .331 .120* .190* .540 .397 .467 .446 .405* .120* .143* .457 .425 .324* .448 .361* .126* .195* Table 4: Correlations between human judgments and metrics (micro avg). * means significantly different from LAS ρ using Fisher’s z-transform. Bold: highest correlation per language. correlated, e.g., LAS and LA, and UAS and NED, but some exhibit very low correlation coefficients. Next we study correlations with human judgments (Table 4). In order to aggregate over the annotations, we use an item-response model (Hovy et al., 2013). The correlations are relatively weak compared to similar findings for other NLP tasks. For instance, ROUGE-1 (Lin, 2004) correlates strongly with perceived summary quality, with a coefficient of 0.99. The same holds for BLEU and human judgments of machine translation quality (Papineni et al., 2002). We find that, overall, LAS is the metric that correlates best with human judgments. It is closely followed by UAS, which does not differ significantly from LAS, albeit the correlations for UAS are slightly lower on average. NED is in turn highly correlated with UAS. The correlations for the predi"
K15-1033,E12-1006,0,0.0277816,"Missing"
K15-1033,W04-1013,0,0.191111,"tasks (Elming et al., 2013), we see human judgments as an important supplement to extrinsic evaluation. To the best of our knowledge, no prior study has analyzed the correlation between dependency parsing metrics and human judgments. For a range of other NLP tasks, metrics have been evaluated by how well they correlate with human judgments. For instance, the standard automatic metrics for certain tasks—such as BLEU in machine translation, or ROUGE-N and NIST in summarization or natural language generation—were evaluated, reaching correlation coefficients well above .80 (Papineni et al., 2002; Lin, 2004; Belz and Reiter, 2006; Callison-Burch et al., 2007). We find that correlations between evaluation metrics and human judgments are weaker for dependency parsing than other NLP tasks—our correlation coefficients are typically between .35 and .55—and that inter-annotator agreement is sometimes higher than human-metric agreement. Moreover, our analysis (§5) reveals that humans have a preference for attachment over labeling decisions, and that attachments closer to the root are more important. Our findings suggest that the currently employed metrics are not fully adequate. Using automatic measure"
K15-1033,P05-1012,0,0.0845214,". |{v |v ∈ V, lG (v, ·) = lP (v, ·)}| |V | Data In our experiments we use data from five languages: The English (en), German (de) and Spanish (es) treebanks from the Universal Dependencies (UD v1.0) project (Nivre et al., 2015), the Copenhagen Dependency Treebank (da) (BuchKromann, 2003), and the Croatian Dependency Treebank (hr) (Agi´c and Merkler, 2013). We keep the original POS tags for all datasets (17 tags in case of UD, 13 tags for Croatian, and 23 for Danish). Data characteristics are in Table 1. For the parsing systems, we follow McDonald and Nivre (2007) and use the second order MST (McDonald et al., 2005), as well as Malt parser with pseudo-projectivization (Nivre and Nilsson, 2005) and default parameters. For each language, we train the parsers on the canonical training section. We randomly select 200 sentences from the test sections, where our two deWe include two further metrics—namely, labeled (LCP) and unlabeled (UCP) complete predications—to give account for the relevance of correct predicate prediction for parsing quality. LCP is inspired by the complete predicates metric from the SemEval 2015 shared task on semantic parsing (Oepen et al., 2015).2 LCP is triggered by a verb (i.e., set o"
K15-1033,P05-1013,0,0.0676584,"a from five languages: The English (en), German (de) and Spanish (es) treebanks from the Universal Dependencies (UD v1.0) project (Nivre et al., 2015), the Copenhagen Dependency Treebank (da) (BuchKromann, 2003), and the Croatian Dependency Treebank (hr) (Agi´c and Merkler, 2013). We keep the original POS tags for all datasets (17 tags in case of UD, 13 tags for Croatian, and 23 for Danish). Data characteristics are in Table 1. For the parsing systems, we follow McDonald and Nivre (2007) and use the second order MST (McDonald et al., 2005), as well as Malt parser with pseudo-projectivization (Nivre and Nilsson, 2005) and default parameters. For each language, we train the parsers on the canonical training section. We randomly select 200 sentences from the test sections, where our two deWe include two further metrics—namely, labeled (LCP) and unlabeled (UCP) complete predications—to give account for the relevance of correct predicate prediction for parsing quality. LCP is inspired by the complete predicates metric from the SemEval 2015 shared task on semantic parsing (Oepen et al., 2015).2 LCP is triggered by a verb (i.e., set of nodes Vverb ) and checks whether all its core arguments match, i.e., all outg"
K15-1033,W04-2407,0,0.0333814,"dependency parsing metrics, described in this section. Given a labeled gold tree G = hV, EG , lG (·)i and a labeled predicted tree P = hV, EP , lP (·)i, let E ⊂ V × V be the set of directed edges from dependents to heads, and let l : V × V → L be the edge labeling function, with L the set of dependency labels. The three most commonly used metrics are those from the CoNLL 2006–7 shared tasks (Buchholz and Marsi, 2006): unlabeled attachment score (UAS), label accuracy (LA), both introduced by Eisner (1996), and labeled attachment score (LAS), the pivotal dependency parsing metric introduced by Nivre et al. (2004). LAS = 3 Experiment In our analysis, we compare the metrics with human judgments. We examine how well the automatic metrics correlate with each other, as well as with human judgments, and whether interannotator agreement exceeds annotator-metric agreement. |{e |e ∈ EG ∩ EP }| |V | |{e |lG (e) = lP (e), e ∈ EG ∩ EP }| |V | LA = |{v |Vverb , cG (v) = cP (v) ∧ lG (v, ·) = lP (v, ·)}| |Vverb | For the final figure of seven different parsing metrics, on top of the previous five, in our experiments we also include the neutral edge direction metric (NED) (Schwartz et al., 2011), and tree edit distan"
K15-1033,C10-1094,0,0.118629,"cs fail to capture certain aspects of parse quality, such as the relevance of root attachment or the relative importance of the different parts of speech. 1 Introduction In dependency parser evaluation, the standard accuracy metrics—labeled and unlabeled attachment scores—are defined simply as averages over correct attachment decisions. Several authors have pointed out problems with these metrics; they are both sensitive to annotation guidelines (Schwartz et al., 2012; Tsarfaty et al., 2011), and they fail to say anything about how parsers fare on rare, but important linguistic constructions (Nivre et al., 2010). Both criticisms rely on the intuition that some parsing errors are more important than others, and that our metrics should somehow reflect that. There are sentences that are hard to annotate because they are ambiguous, or because they contain phenomena peripheral to linguistic theory, such as punctuation, clitics, or fragments. Manning (2011) discusses similar issues for part-ofspeech tagging. 315 Proceedings of the 19th Conference on Computational Language Learning, pages 315–320, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics Contributions We present i)"
K15-1033,S15-2153,0,0.0389892,"e (2007) and use the second order MST (McDonald et al., 2005), as well as Malt parser with pseudo-projectivization (Nivre and Nilsson, 2005) and default parameters. For each language, we train the parsers on the canonical training section. We randomly select 200 sentences from the test sections, where our two deWe include two further metrics—namely, labeled (LCP) and unlabeled (UCP) complete predications—to give account for the relevance of correct predicate prediction for parsing quality. LCP is inspired by the complete predicates metric from the SemEval 2015 shared task on semantic parsing (Oepen et al., 2015).2 LCP is triggered by a verb (i.e., set of nodes Vverb ) and checks whether all its core arguments match, i.e., all outgoing dependency edges except for punctuation. Since LCP is a very strict metric, we also evaluate UCP, its unlabeled variant. Given a function cX (v) that retrieves the set of child nodes of a node v from a tree X, we first define UCP as follows, and then incorporate the label matching for LCP: 1 The dataset is publicly available at https:// bitbucket.org/lowlands/release 2 http://alt.qcri.org/semeval2015/ 3 316 http://www.tsarfaty.com/unipar/ L ANG PARSER LAS UAS LA NED TED"
K15-1033,S14-2008,0,\N,Missing
L16-1258,I13-1041,0,0.0608582,"Missing"
L16-1258,D11-1120,0,0.0508224,"one, while this is much less the case for the other two dimensions. However, for languages for which we have fewer than 500 authors, namely Italian and German, the model usually does not outperform the majority baseline. Discussion and related work Because different dataset types and sizes, collection methods, evaluation metrics, and preprocessing methods make direct comparisons impossible, we conclude from our gender identification results that they are comparable to or better than the best published results on gender identification from Twitter for the different languages in our corpus. See Burger et al. (2011) for another comparative multilingual study on gender identification from twitter data, but using an approach that is difficult to compare to ours (learning all languages with one classifier). Predicting Myers-Briggs type indicators from linguistic input has been studied in the seminal paper of Luyckx and Daelemans (2008). They created a corpus for Dutch, consisting of 145 student essays about a documentary on artificial life. Recently, the CSI (CLiPS Stylometry Investigation) corpus was introduced, which includes Dutch reviews as well as essays and annotations for both Big Five and MBTI annot"
L16-1258,D13-1114,0,0.0879019,"Missing"
L16-1258,P11-1137,0,0.0173024,"Missing"
L16-1258,N13-1037,0,0.0238928,"Missing"
L16-1258,P12-3005,0,0.0437849,"Missing"
L16-1258,W14-1303,0,0.0339602,"Missing"
L16-1258,luyckx-daelemans-2008-personae,1,0.803938,"ted random baseline, which should be regarded as the main point of comparison. For four languages (Dutch, French, Portuguese, Spanish) our model even outperforms the higher majority baseline consistently for two dimensions, namely I NTROVERT–E XTRAVERT and T HINKING – F EELING. The other two dimensions are more difficult to predict and our model does not reach majority baseline (with only one exception, S–N for Dutch). This has been observed earlier on English by Plank and Hovy (2015). They found the exact same dimensions where no improvement was achieved, and a similar trend was described by Luyckx and Daelemans (2008) for the last dimension (J–P). This suggests that I NTROVERT–E XTRAVERT as well as T HINKING –F EELING are predictable from linguistic input alone, while this is much less the case for the other two dimensions. However, for languages for which we have fewer than 500 authors, namely Italian and German, the model usually does not outperform the majority baseline. Discussion and related work Because different dataset types and sizes, collection methods, evaluation metrics, and preprocessing methods make direct comparisons impossible, we conclude from our gender identification results that they ar"
L16-1258,D15-1130,0,0.0823792,"Missing"
L16-1258,W11-1515,0,0.105661,"Missing"
L16-1258,W15-2913,1,0.777475,"hich are written in highly canonical language. Such controlled settings inhibit the expression of individual traits much more than spontaneous language. As such data is hard to obtain, only limited amounts were available. With the availability of social media text, recent efforts shifted toward using such data (Schwartz et al., 2013a; Schwartz et al., 2013b; Park et al., 2015; Kosinski et al., 2015). For example, Kosinski et al. (2015) collected a large amount of social media data with Big Five (Kosinski et al., 2015) annotations through a tailored Facebook app. Another approach, suggested by Plank and Hovy (2015), is to use the large amounts of textual data voluntarily produced on social media (i.e., Twitter) together with self-assessed Myers-Briggs Type Indicators (Briggs Myers and Myers, 2010), abbreviated MBTI, to collect large amounts of labeled data. Myers-Briggs classifies users along four dimensions (I NTROVERT–E XTRAVERT, I N TUITIVE –S ENSING , T HINKING –F EELING , J UDGING – P ERCEIVING), amounting to 16 different types, e.g., INTJ, ESFP, etc. As such, Myers-Briggs personality types have the distinct advantage of being readily available in large quantities on social media, in particular Twi"
L16-1258,W15-1203,0,0.226107,"al and McKeown, 2011; Nguyen et al., 2011; Eisenstein et al., 2011; Volkova et al., 2013; Alowibdi et al., 2013; Ciot et al., 2013; Volkova et al., 2015). Apart from demographic features, such as age or gender, there is also a growing interest in predicting psychological properties such as personality, attested by a growing literature and recent shared tasks on this topic (Celli et al., 2013; Celli et al., 2014; Rangel et al., 2015). Predicting personality types is not only of interest for psychology, but also for commercial purposes to even health care. Recent work by Preot¸iuc-Pietro et al. (2015) investigated the link between personality types, social media behavior, and psychological disorders, such as depression and post-traumatic stress disorder. They found that certain personality traits are predictive of mental illness. However, computational personality recognition is hampered by the availability of limited amounts of labeled data (Nowson and Gill, 2014). Many early existing data sets contain written essays of a certain topic, which are written in highly canonical language. Such controlled settings inhibit the expression of individual traits much more than spontaneous language."
L16-1258,P11-1077,0,0.193224,"Missing"
L16-1258,verhoeven-daelemans-2014-clips,1,0.206658,"ther comparative multilingual study on gender identification from twitter data, but using an approach that is difficult to compare to ours (learning all languages with one classifier). Predicting Myers-Briggs type indicators from linguistic input has been studied in the seminal paper of Luyckx and Daelemans (2008). They created a corpus for Dutch, consisting of 145 student essays about a documentary on artificial life. Recently, the CSI (CLiPS Stylometry Investigation) corpus was introduced, which includes Dutch reviews as well as essays and annotations for both Big Five and MBTI annotations (Verhoeven and Daelemans, 2014). In contrast, we here focus on social media data, in particular Twitter, and self-assessed (and self-reported) MBTI personality types. In many prior studies, participants were asked to participate in a personality test and produce essay(s). Collecting personality data from social media has been done before (Schwartz et al., 2013a; Schwartz et al., 2013b; Park et al., 2015). For instance, the myPersonality dataset (Kosinski et al., 2015) contains personality types and messages from 75,000 users collected through a Facebook app. Earlier work using social media data is mostly smaller scale, e.g."
L16-1258,D13-1187,0,0.0568628,"Missing"
N15-1135,W06-1615,0,0.437228,"Missing"
N15-1135,J92-4003,0,0.658692,"obin Williams playing an adult Peter and Dustin Hoffman as the dastardly Captain Hook. Rooofiii Oooooo, didn’t think ppl<3the movie as much as me, this movie will always b the peter pan story2me #robin #williams #hook I loved that movie... Uhm... You know, Hook. With Robin Williams, uh. peter pan williams movie Table 1: Examples from source (top row) and target domains (bottom rows) spelling variations with the standard form (youuuuuuuu → you), which reduces the vocabulary size. For languages where no such normalization dictionary is available, we use word clusterings based on Brown clusters (Brown et al., 1992) to generalize tags from unambiguous words to previously unseen words in the same class. C LUSTER 01011110 01011110 01011110 01011110 01011110 01011110 01011110 T OKEN offish alreadyyy finali aleady previously already recently TAG ∈ D ADJ ??? ??? ??? ADV ADV ADV P ROJ . TAG — ADV ADV ADV — — — Figure 1: Example of a Brown cluster with unambiguous tokens, as well as projected tags for new tokens (tokens marked “—” are unchanged in D0 ). In particular, to extend the dictionary D to D0 using clusters, we first run clustering on the unlabeled data T , using Brown clustering.2 We then assign to eac"
N15-1135,P07-1033,0,0.668034,"Missing"
N15-1135,I11-1100,0,0.0495232,"Missing"
N15-1135,P11-2008,0,0.158516,"Missing"
N15-1135,P11-1038,0,0.0629429,"generalize across spelling variations and synonyms. Additionally, we evaluate our approach on Dutch, Portuguese and Spanish Twitter and present tow novel data sets for the latter two languages. 2 Data 2.1 Wiktionary In our experiments, we use the (unigram) tag dictionaries from Wiktionary, as collected by Li et al. (2012).1 The size and quality of our tag dictionaries crucially influence how much unambiguous data we can extract, and for some languages, the number of dictionary entries is small. We can resort to normalization dictionaries to extend Wiktionary’s coverage. We do so for English (Han and Baldwin, 2011). It replaces some 1 https://code.google.com/p/ wikily-supervised-pos-tagger/ 1256 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1256–1261, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics N EWSWIRE T WITTER S POKEN Q UERIES Spielberg took the helm of this big budget live action project with Robin Williams playing an adult Peter and Dustin Hoffman as the dastardly Captain Hook. Rooofiii Oooooo, didn’t think ppl<3the movie as much as me, this movie will always b the peter pan story2me #robin #will"
N15-1135,hovy-etal-2014-pos,1,0.86932,"guese, we use the training section of the Bosque treebank.7 For Spanish, we use the training section of the Cast3LB treebank.8 In order to map between Wiktionary and the treebanks, we need a common coarse tag set. We thus map all data to the universal tag set (Petrov et al., 2012). Dev and test sets Our approach is basically parameter free. However, we did experiment with different ways of extending Wiktionary and hence used an average over three English Twitter dev sections as development set (Ritter et al., 2011; Gimpel et al., 2011; Foster et al., 2011), all mapped and normalized following Hovy et al. (2014). For evaluation, we use three domains: tweets, spoken data and queries. For Twitter, we performed experiments in four languages: English, Portuguese, Spanish and Dutch. The Spanish and Portuguese tweets were annotated in-house, which will be made available.9 For the other languages, we use preexisting datasets for English (Hovy et al., 2014) and Dutch (Avontuur et al., 2012). Table 2 lists the complete statistics for the different language data sets. For the other two domains, we use the manually labeled data from Switchboard section 4 as spoken data test set. For queries, we use manually lab"
N15-1135,I05-1017,0,0.0340948,"from the set of licensed ones. In the second case, we assume the unknown word to be a NOUN, since unknown words mostly tend to be proper names. When added to newswire, this data results in worse models, presumably by introducing too much noise. However, for low-resource languages or domains with longer sentences and no available newswire data, this might be a viable alternative. 6 Related Work Our approach is similar to mining high-precision items. However, previous approaches on this in NLP have mainly focused on well-defined classification tasks, such as PP attachment (Pantel and Lin, 2000; Kawahara and Kurohashi, 2005), or discourse connective disambiguation (Marcu and Echihabi, 2002). In contrast, we mine for sequences of unambiguous tokens in a structured prediction task. While we use the same dictionaries as in Li et al. (2012) and T¨ackstr¨om et al. (2013), our approach differs in several respects. First, we use Wiktionary to mine for training data, rather than as type constraints, and second, we use Brown clusters to extend Wiktionary. We did experiment with different ways of doing this, including using various forms of word embeddings, leading to models similar to the baseline models in Socher et al."
N15-1135,2005.mtsummit-papers.11,0,0.0607748,"r example, is only about 0.012 (or 1 in 84), and the distribution of tags in the Twitter data set is heavily skewed towards nouns, while several other labels are under-represented. Twitter We collect the unlabeled data from the Twitter streaming API.3 We collected 57m tweets for English, 8.2m for Spanish, 4.1m for Portuguese, and 0.5m for Dutch. We do not perform sentence splitting on tweets, but take them as unit sequences. Spoken language We use the Switchboard corpus of transcribed telephone conversations (Godfrey et al., 1992), sections 2 and 3, as well as the English section of EuroParl (Koehn, 2005) and CHILDES (MacWhinney, 1997). We removed all meta-data and inline annotations (gestures, sounds, etc.), as well as dialogue markers. The final joint corpus contains transcriptions of 570k spoken sentences. Search queries For search queries, we use a combination of queries from Yahoo4 and AOL. We only use the search terms and ignore any additional information, such as user ID, time, and linked URLs. The resulting data set contains 10m queries. 3 2 https://github.com/percyliang/ brown-cluster 4 1257 Unlabeled data https://github.com/saffsd/langid.py http://webscope.sandbox.yahoo.com/ 2.3 Labe"
N15-1135,D12-1127,0,0.0753564,"Missing"
N15-1135,P02-1047,0,0.0803701,"n word to be a NOUN, since unknown words mostly tend to be proper names. When added to newswire, this data results in worse models, presumably by introducing too much noise. However, for low-resource languages or domains with longer sentences and no available newswire data, this might be a viable alternative. 6 Related Work Our approach is similar to mining high-precision items. However, previous approaches on this in NLP have mainly focused on well-defined classification tasks, such as PP attachment (Pantel and Lin, 2000; Kawahara and Kurohashi, 2005), or discourse connective disambiguation (Marcu and Echihabi, 2002). In contrast, we mine for sequences of unambiguous tokens in a structured prediction task. While we use the same dictionaries as in Li et al. (2012) and T¨ackstr¨om et al. (2013), our approach differs in several respects. First, we use Wiktionary to mine for training data, rather than as type constraints, and second, we use Brown clusters to extend Wiktionary. We did experiment with different ways of doing this, including using various forms of word embeddings, leading to models similar to the baseline models in Socher et al. (2013), but the approach based on Brown clusters led to the best re"
N15-1135,P09-1113,0,0.130157,"Missing"
N15-1135,N13-1039,0,0.203014,"Missing"
N15-1135,P00-1014,0,0.0692605,"omly at training time from the set of licensed ones. In the second case, we assume the unknown word to be a NOUN, since unknown words mostly tend to be proper names. When added to newswire, this data results in worse models, presumably by introducing too much noise. However, for low-resource languages or domains with longer sentences and no available newswire data, this might be a viable alternative. 6 Related Work Our approach is similar to mining high-precision items. However, previous approaches on this in NLP have mainly focused on well-defined classification tasks, such as PP attachment (Pantel and Lin, 2000; Kawahara and Kurohashi, 2005), or discourse connective disambiguation (Marcu and Echihabi, 2002). In contrast, we mine for sequences of unambiguous tokens in a structured prediction task. While we use the same dictionaries as in Li et al. (2012) and T¨ackstr¨om et al. (2013), our approach differs in several respects. First, we use Wiktionary to mine for training data, rather than as type constraints, and second, we use Brown clusters to extend Wiktionary. We did experiment with different ways of doing this, including using various forms of word embeddings, leading to models similar to the ba"
N15-1135,petrov-etal-2012-universal,0,0.0266651,"ndbox.yahoo.com/ 2.3 Labeled data We train our models on newswire, as well as mined unambiguous instances. For English, we use the OntoNotes release of the WSJ section of the Penn Treebank as training data for Twitter, spoken data, and queries.5 For Dutch, we use the training section of the Alpino treebank from the CoNLL task.6 For Portuguese, we use the training section of the Bosque treebank.7 For Spanish, we use the training section of the Cast3LB treebank.8 In order to map between Wiktionary and the treebanks, we need a common coarse tag set. We thus map all data to the universal tag set (Petrov et al., 2012). Dev and test sets Our approach is basically parameter free. However, we did experiment with different ways of extending Wiktionary and hence used an average over three English Twitter dev sections as development set (Ritter et al., 2011; Gimpel et al., 2011; Foster et al., 2011), all mapped and normalized following Hovy et al. (2014). For evaluation, we use three domains: tweets, spoken data and queries. For Twitter, we performed experiments in four languages: English, Portuguese, Spanish and Dutch. The Spanish and Portuguese tweets were annotated in-house, which will be made available.9 For"
N15-1135,C14-1168,1,0.853249,"Missing"
N15-1135,D11-1141,0,0.0257816,"ries.5 For Dutch, we use the training section of the Alpino treebank from the CoNLL task.6 For Portuguese, we use the training section of the Bosque treebank.7 For Spanish, we use the training section of the Cast3LB treebank.8 In order to map between Wiktionary and the treebanks, we need a common coarse tag set. We thus map all data to the universal tag set (Petrov et al., 2012). Dev and test sets Our approach is basically parameter free. However, we did experiment with different ways of extending Wiktionary and hence used an average over three English Twitter dev sections as development set (Ritter et al., 2011; Gimpel et al., 2011; Foster et al., 2011), all mapped and normalized following Hovy et al. (2014). For evaluation, we use three domains: tweets, spoken data and queries. For Twitter, we performed experiments in four languages: English, Portuguese, Spanish and Dutch. The Spanish and Portuguese tweets were annotated in-house, which will be made available.9 For the other languages, we use preexisting datasets for English (Hovy et al., 2014) and Dutch (Avontuur et al., 2012). Table 2 lists the complete statistics for the different language data sets. For the other two domains, we use the manuall"
N15-1135,Q13-1001,0,0.0669642,"Missing"
N15-1135,P99-1023,0,0.0664404,"floresta_English.html 8 http://www.iula.upf.edu/recurs01_tbk_ uk.htm 9 http://lowlands.ku.dk/results 10 https://code.google.com/p/crfpp/ 6 1258 fault parameters. As baselines we consider a) a CRF model trained only on newswire; b) available off-the-shelf systems (T OOLS); and c) a weakly supervised model (L I 10). For English, the off-theshelf tagger is the Stanford tagger (Toutanova et al., 2003), for the other languages we use TreeTagger (Schmid, 1994) with pre-trained models. The weakly supervised model trained is on the unannotated data. It is a second-order HMM model (Mari et al., 1997; Thede and Harper, 1999) (SOHMM) using logistic regression to estimate the emission probabilities. This method allows us to use feature vectors rather than just word identity, as in standard HMMs. In addition, we constrain the inference space of the tagger using type-level tag constraints derived from Wiktionary. This model, called L I 10 in Table 3, was originally proposed by Li et al. (2012). We extend the model by adding continuous word representations, induced from the unlabeled data using the skip-gram algorithm (Mikolov et al., 2013), to the feature representations. Our logistic regression model thus works over"
N15-1135,N03-1033,0,0.0285312,"10). 3 Experiments 3.1 Model We use a CRF10 model (Lafferty et al., 2001) with the same features as Owoputi et al. (2013) and de5 LDC2011T03. http://www.let.rug.nl/˜vannoord/trees/ 7 http://www.linguateca.pt/floresta/info_ floresta_English.html 8 http://www.iula.upf.edu/recurs01_tbk_ uk.htm 9 http://lowlands.ku.dk/results 10 https://code.google.com/p/crfpp/ 6 1258 fault parameters. As baselines we consider a) a CRF model trained only on newswire; b) available off-the-shelf systems (T OOLS); and c) a weakly supervised model (L I 10). For English, the off-theshelf tagger is the Stanford tagger (Toutanova et al., 2003), for the other languages we use TreeTagger (Schmid, 1994) with pre-trained models. The weakly supervised model trained is on the unannotated data. It is a second-order HMM model (Mari et al., 1997; Thede and Harper, 1999) (SOHMM) using logistic regression to estimate the emission probabilities. This method allows us to use feature vectors rather than just word identity, as in standard HMMs. In addition, we constrain the inference space of the tagger using type-level tag constraints derived from Wiktionary. This model, called L I 10 in Table 3, was originally proposed by Li et al. (2012). We e"
N15-1152,N13-1070,1,0.847311,"Related Work Plank et al. (2014a) propose IAA-weighted costsensitive learning for POS tagging. We extend their line of work to dependency parsing. A single sentence can have more than one plausible dependency annotation. Some researchers have 1360 proposed evaluation metrics that do not penalize disagreements (Schwartz et al., 2011; Tsarfaty et al., 2011), while others have argued that we should instead ensure the consistency of treebanks (Dickinson, 2010; Manning, 2011; McDonald et al., 2013). Others have claimed that because of these ambiguities, only downstream evaluations are meaningful (Elming et al., 2013). Syntactic annotation disagreement has typically been studied in the context of treebank development. Haverinen et al. (2012), for example, analyze annotator disagreement for Finnish dependency syntax, and compare it against parser performance. Skjærholt (2014) use doubly-annotated data to evaluate various agreement metrics. Our paper differs from both lines of research in that we leverage disagreements from doubly-annotated data to obtain more robust models. While we agree that evaluation metrics should probably reflect disagreements, we show that our learning algorithms can indeed benefit f"
N15-1152,C12-1059,0,0.0219341,"l1 i and hh2 , l2 i count as disagreement, iff hj < i < h k . d) H EAD P OS: disagreement on head POS. That is, hh1 , l1 i and hh2 , l2 i count as disagreement, iff POS(h1 )6=POS(h2 ). e) H EAD P OS D, i.e., H EAD P OS, plus direction. That is, hh1 , l1 i and hh2 , l2 i count as disagreement, iff POS(h1 )6=POS(h2 ) or hj < i < hk . train 13.7k/209k 3.6k/70k 4.2k/74k 3.9k/73k 3.1k/79k 9.1k/123k Cost-sensitive updates We use the cost-sensitive perceptron classifier, following Plank et al. (2014a), but extend it to transition-based dependency parsing, where the predicted values are transitions (Goldberg and Nivre, 2012). Given a gold yi and predicted label yˆi (POS tags or transitions), the loss is weighted by γ(ˆ yi , yi ): Lw (ˆ yi , yi ) = γ(ˆ yi , yi ) max(0, −yi w · xi ) Whenever a transition has been wrongly predicted, we retrieve the predicted edge and compare it to the gold dependency to calculate γ. γ(yi , yj ) is then the inverse of the confusion probability estimated from our sample of doubly-annotated data. For example, using the factorization L ABEL, if the parser predicts wi to be S UBJECT and the gold annotation is O B JECT , the confusion probability is the number of times one annotator said"
N15-1152,P05-1013,0,0.133665,"Missing"
N15-1152,E14-1078,1,0.731472,"-annotated data, and involves two steps: i) how to factorize attachment or labeling disagreements; and ii) how to inform the parser of them during learning (§3). 2 Introduction Typically, NLP annotation projects employ guidelines to maximize inter-annotator agreement. Possible inconsistencies are resolved by adjudication, and models are induced assuming there is one single ground truth. However, there exist linguistically hard cases where there is no clear answer (Zeman, 2010; Manning, 2011), and incorporating such disagreements into the training of a model has proven helpful for POS tagging (Plank et al., 2014a; Plank et al., 2014b). Inter-annotator agreement (IAA) is straightforward to calculate for POS, but not for dependency trees. There is no well-established standard for computing agreement on trees (Skjærholt, 2014). Factorizations Assume a sample of sentences annotated by annotators A1 and A2 . With such a sample we can estimate probabilities of the two annotators’ disagreeing on the annotation of a word or span, relative to some dependency tree factorization. We factorize disagreement on dependency tree annotations relative to four properties of the annotated dependency edges: the POS of th"
N15-1152,P14-2083,1,0.854013,"-annotated data, and involves two steps: i) how to factorize attachment or labeling disagreements; and ii) how to inform the parser of them during learning (§3). 2 Introduction Typically, NLP annotation projects employ guidelines to maximize inter-annotator agreement. Possible inconsistencies are resolved by adjudication, and models are induced assuming there is one single ground truth. However, there exist linguistically hard cases where there is no clear answer (Zeman, 2010; Manning, 2011), and incorporating such disagreements into the training of a model has proven helpful for POS tagging (Plank et al., 2014a; Plank et al., 2014b). Inter-annotator agreement (IAA) is straightforward to calculate for POS, but not for dependency trees. There is no well-established standard for computing agreement on trees (Skjærholt, 2014). Factorizations Assume a sample of sentences annotated by annotators A1 and A2 . With such a sample we can estimate probabilities of the two annotators’ disagreeing on the annotation of a word or span, relative to some dependency tree factorization. We factorize disagreement on dependency tree annotations relative to four properties of the annotated dependency edges: the POS of th"
N15-1152,P11-1067,0,0.259652,"Missing"
N15-1152,P14-1088,1,0.930456,"guidelines to maximize inter-annotator agreement. Possible inconsistencies are resolved by adjudication, and models are induced assuming there is one single ground truth. However, there exist linguistically hard cases where there is no clear answer (Zeman, 2010; Manning, 2011), and incorporating such disagreements into the training of a model has proven helpful for POS tagging (Plank et al., 2014a; Plank et al., 2014b). Inter-annotator agreement (IAA) is straightforward to calculate for POS, but not for dependency trees. There is no well-established standard for computing agreement on trees (Skjærholt, 2014). Factorizations Assume a sample of sentences annotated by annotators A1 and A2 . With such a sample we can estimate probabilities of the two annotators’ disagreeing on the annotation of a word or span, relative to some dependency tree factorization. We factorize disagreement on dependency tree annotations relative to four properties of the annotated dependency edges: the POS of the dependent, the POS of the head, the label of the edge and the direction (left or right) of the head with regards to the dependent. This section describes the different factorizations. We present five factorizations"
N15-1152,solberg-etal-2014-norwegian,1,0.900114,"Missing"
N15-1152,D11-1036,0,0.0431991,"Missing"
N15-1152,P10-1075,0,\N,Missing
N15-1152,arias-etal-2014-boosting,0,\N,Missing
N19-1265,N16-1014,0,0.0429711,"e state of the dialogue has some advantages (e.g., ease of interfacing with knowledge bases), but it has also some key disadvantages: the variables to be tracked have to be defined in advance and the system needs to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in computer vision have proposed tasks t"
N19-1265,D16-1127,0,0.0416633,"e state of the dialogue has some advantages (e.g., ease of interfacing with knowledge bases), but it has also some key disadvantages: the variables to be tracked have to be defined in advance and the system needs to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in computer vision have proposed tasks t"
N19-1265,W18-5045,0,0.0315275,"otated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in computer vision have proposed tasks that combine visual processing with dialogue interaction. Pertinent datasets created by Das et al. (2017a) and de Vries et al. (2017) include VisDial and GuessWhat?!, respectively, where two participants ask and answer questions about an image."
N19-1265,C18-1104,1,0.831206,"Missing"
N19-1265,N15-1020,0,0.0310712,"symbolic representations to characterise the state of the dialogue has some advantages (e.g., ease of interfacing with knowledge bases), but it has also some key disadvantages: the variables to be tracked have to be defined in advance and the system needs to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in"
N19-1265,P17-1062,0,0.0143494,"e and the system needs to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in computer vision have proposed tasks that combine visual processing with dialogue interaction. Pertinent datasets created by Das et al. (2017a) and de Vries et al. (2017) include VisDial and GuessWhat?!, respectively, where two partic"
N19-1265,W13-4065,0,0.0202117,"sier to train than RL. • A first in-depth study to compare cooperative learning to a state-of-the-art RL system. Our study shows that the linguistic skills of the models differ dramatically, despite approaching comparable task success levels. This underlines the importance of linguistic analysis to complement solely numeric evaluation. 2 Related Work Task-oriented dialogue systems The conventional architecture of task-oriented dialogue systems includes a pipeline of components, and the task of tracking the dialogue state is typically modelled as a partially-observable Markov decision process (Williams et al., 2013; Young et al., 2013; Kim et al., 2014) that operates on a symbolic dialogue state consisting of predefined variables. The use of symbolic representations to characterise the state of the dialogue has some advantages (e.g., ease of interfacing with knowledge bases), but it has also some key disadvantages: the variables to be tracked have to be defined in advance and the system needs to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neur"
N19-1265,L16-1019,1,0.80446,"Missing"
N19-1265,W16-3601,0,0.0150046,"to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in computer vision have proposed tasks that combine visual processing with dialogue interaction. Pertinent datasets created by Das et al. (2017a) and de Vries et al. (2017) include VisDial and GuessWhat?!, respectively, where two participants ask and answer que"
P11-1157,W06-1615,0,0.163561,"tivation Previous research on domain adaptation has focused on the task of adapting a system trained on one domain, say newspaper text, to a particular new domain, say biomedical data. Usually, some amount of (labeled or unlabeled) data from the new domain was given – which has been determined by a human. However, with the growth of the web, more and more data is becoming available, where each document “is potentially its own domain” (McClosky et al., 2010). It is not straightforward to determine Most previous work on domain adaptation, for instance Hara et al. (2005), McClosky et al. (2006), Blitzer et al. (2006), Daum´e III (2007), sidestepped this problem of automatic domain selection and adaptation. For parsing, to our knowledge only one recent study has started to examine this issue (McClosky et al., 2010) – we will discuss their approach in Section 2. Rather, an implicit assumption of all of these studies is that domains are given, i.e. that they are represented by the respective corpora. Thus, a corpus has been considered a homogeneous unit. As more data is becoming available, it is unlikely that domains will be ‘given’. Moreover, a given corpus might not always be as homogeneous as originally t"
P11-1157,W06-2920,0,0.0504599,"eywords seem to come from a controlled vocabulary. There are 76 distinct topic markers. The three most frequent keywords are: TENDER OFFERS, MERGERS, ACQUISITIONS (TNM), EARNINGS (ERN), STOCK MARKET, OFFERINGS (STK). This reflects the fact that a lot of articles come from the financial domain. But the corpus also contains articles from more distant domains, like MARKETING, ADVERTISING (MKT), COMPUTERS AND INFORMATION TECHNOLOGY (CPR), HEALTH CARE PROVIDERS, MEDICINE, DENTISTRY (HEA), PETROLEUM (PET). 4 a system that can be trained on a variety of languages given training data in CoNLL format (Buchholz and Marsi, 2006). Additionally, the parser implements both projective and non-projective parsing algorithms. The projective algorithm is used for the experiments on English, while the non-projective variant is used for Dutch. We train the parser using default settings. MST takes PoS-tagged data as input; we use gold-standard tags in the experiments. We estimate topic models using Latent Dirichlet Allocation (Blei et al., 2003) implemented in the MALLET4 toolkit. Like Lippincott et al. (2010), we set the number of topics to 100, and otherwise use standard settings (no further optimization). We experimented wit"
P11-1157,P07-1033,0,0.652472,"Missing"
P11-1157,W01-0521,0,0.0719896,"ch model or data is most beneficial for an arbitrary piece of new text. Moreover, if we had such a measure, a related question is whether it can tell us something more about what is actually meant by “domain”. So far, it was mostly arbitrarily used to refer to some kind of coherent unit (related to topic, style or genre), e.g.: newspaper text, biomedical abstracts, questions, fiction. It is well known that parsing accuracy suffers when a model is applied to out-of-domain data. It is also known that the most beneficial data to parse a given domain is data that matches the domain (Sekine, 1997; Gildea, 2001). Hence, an important task is to select appropriate domains. However, most previous work on domain adaptation relied on the implicit assumption that domains are somehow given. As more and more data becomes available, automatic ways to select data that is beneficial for a new (unknown) target domain are becoming attractive. This paper evaluates various ways to automatically acquire related training data for a given test set. The results show that an unsupervised technique based on topic models is effective – it outperforms random data selection on both languages examined, English and Dutch. Mor"
P11-1157,I05-1018,0,0.0298232,"available for English. 1 Introduction and Motivation Previous research on domain adaptation has focused on the task of adapting a system trained on one domain, say newspaper text, to a particular new domain, say biomedical data. Usually, some amount of (labeled or unlabeled) data from the new domain was given – which has been determined by a human. However, with the growth of the web, more and more data is becoming available, where each document “is potentially its own domain” (McClosky et al., 2010). It is not straightforward to determine Most previous work on domain adaptation, for instance Hara et al. (2005), McClosky et al. (2006), Blitzer et al. (2006), Daum´e III (2007), sidestepped this problem of automatic domain selection and adaptation. For parsing, to our knowledge only one recent study has started to examine this issue (McClosky et al., 2010) – we will discuss their approach in Section 2. Rather, an implicit assumption of all of these studies is that domains are given, i.e. that they are represented by the respective corpora. Thus, a corpus has been considered a homogeneous unit. As more data is becoming available, it is unlikely that domains will be ‘given’. Moreover, a given corpus mig"
P11-1157,J04-3001,0,0.0144241,"Missing"
P11-1157,P07-1034,0,0.0844254,"Missing"
P11-1157,W07-2416,0,0.025415,"arser on the training section (02-21). The result on the standard test set (section 23) is identical to previously reported results (excluding punctuation tokens: LAS 87.50, Unlabeled Attachment Score (UAS) 90.75; with punctuation tokens: LAS 87.07, UAS 89.95). The latter has been reported in (Surdeanu and Manning, 2010). English - Genia (G) & Brown (B) For the Domain Adaptation experiments, we added 1,552 articles from the GENIA10 treebank (biomedical abstracts from Medline) and 190 files from the Brown corpus to the pool of data. We converted the data to CoNLL format with the LTH converter (Johansson and Nugues, 2007). The size of the test files is, respectively: Genia 1,360 sentences with an average number of 26.20 words per sentence; the Brown test set is the same as used in the CoNLL 2008 shared task and contains 426 sentences with a mean of 16.80 words. 8 Using the LTH converter: http://nlp.cs.lth.se/ software/treebank_converter/ 9 This was a non-trivial task, as we actually noticed that some sentences have been omitted from the CoNLL 2008 shared task. 10 We use the GENIA distribution in Penn Treebank format available at http://bllip.cs.brown.edu/download/ genia1.0-division-rel1.tar.gz 1570 5 Experimen"
P11-1157,C10-1078,0,0.0387069,"Missing"
P11-1157,N06-1020,0,0.061119,"h. 1 Introduction and Motivation Previous research on domain adaptation has focused on the task of adapting a system trained on one domain, say newspaper text, to a particular new domain, say biomedical data. Usually, some amount of (labeled or unlabeled) data from the new domain was given – which has been determined by a human. However, with the growth of the web, more and more data is becoming available, where each document “is potentially its own domain” (McClosky et al., 2010). It is not straightforward to determine Most previous work on domain adaptation, for instance Hara et al. (2005), McClosky et al. (2006), Blitzer et al. (2006), Daum´e III (2007), sidestepped this problem of automatic domain selection and adaptation. For parsing, to our knowledge only one recent study has started to examine this issue (McClosky et al., 2010) – we will discuss their approach in Section 2. Rather, an implicit assumption of all of these studies is that domains are given, i.e. that they are represented by the respective corpora. Thus, a corpus has been considered a homogeneous unit. As more data is becoming available, it is unlikely that domains will be ‘given’. Moreover, a given corpus might not always be as homo"
P11-1157,N10-1004,0,0.686263,"nglish and Dutch. Moreover, the technique works better than manually assigned labels gathered from meta-data that is available for English. 1 Introduction and Motivation Previous research on domain adaptation has focused on the task of adapting a system trained on one domain, say newspaper text, to a particular new domain, say biomedical data. Usually, some amount of (labeled or unlabeled) data from the new domain was given – which has been determined by a human. However, with the growth of the web, more and more data is becoming available, where each document “is potentially its own domain” (McClosky et al., 2010). It is not straightforward to determine Most previous work on domain adaptation, for instance Hara et al. (2005), McClosky et al. (2006), Blitzer et al. (2006), Daum´e III (2007), sidestepped this problem of automatic domain selection and adaptation. For parsing, to our knowledge only one recent study has started to examine this issue (McClosky et al., 2010) – we will discuss their approach in Section 2. Rather, an implicit assumption of all of these studies is that domains are given, i.e. that they are represented by the respective corpora. Thus, a corpus has been considered a homogeneous un"
P11-1157,H05-1066,0,0.232443,"Missing"
P11-1157,P10-2041,0,0.0284131,"mance loss. Their goal is different, but related: rather than finding related data for a new domain, they want to estimate the loss in accuracy of a PoS tagger when applied to a new domain. We will briefly discuss results obtained with the Renyi divergence in Section 5.1. Lippincott et al. (2010) examine subdomain variation in biomedicine corpora and propose awareness of NLP tools to such variation. However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis, 2010). A subset of the available data is automatically selected as training data for a Language Model based on a scoring mechanism that compares crossentropy scores. Their approach considerably outperformed random selection and two previous proposed approaches both based on perplexity scoring.1 3 Measures of Domain Similarity 3.1 Measuring Similarity Automatically Feature Representations A similarity function may be defined over any set of events that are con1 We tested data selection by perplexity scoring, but found the Language Models too small to be useful in our setting. sidered to be relevant"
P11-1157,D10-1069,0,0.0797885,"Missing"
P11-1157,plank-simaan-2008-subdomain,1,0.91806,"Missing"
P11-1157,W10-2105,1,0.835843,"Missing"
P11-1157,D08-1093,0,0.0157287,", it can be easily applied to other tasks or languages for which annotated (or automatically annotated) data is available. 2 Related Work The work most related to ours is McClosky et al. (2010). They try to find the best combination of source models to parse data from a new domain, which is related to Plank and Sima’an (2008). In the latter, unlabeled data was used to create several parsers by weighting trees in the WSJ according to their similarity to the subdomain. McClosky et al. (2010) coined the term multiple source domain adaptation. Inspired by work on parsing accuracy 1567 prediction (Ravi et al., 2008), they train a linear regression model to predict the best (linear interpolation) of source domain models. Similar to us, McClosky et al. (2010) regard a target domain as mixture of source domains, but they focus on phrasestructure parsing. Furthermore, our approach differs from theirs in two respects: we do not treat source corpora as one entity and try to mix models, but rather consider articles as base units and try to find subsets of related articles (the most similar articles); moreover, instead of creating a supervised model (in their case to predict parsing accuracy), our approach is ‘s"
P11-1157,A97-1015,0,0.157385,"determine which model or data is most beneficial for an arbitrary piece of new text. Moreover, if we had such a measure, a related question is whether it can tell us something more about what is actually meant by “domain”. So far, it was mostly arbitrarily used to refer to some kind of coherent unit (related to topic, style or genre), e.g.: newspaper text, biomedical abstracts, questions, fiction. It is well known that parsing accuracy suffers when a model is applied to out-of-domain data. It is also known that the most beneficial data to parse a given domain is data that matches the domain (Sekine, 1997; Gildea, 2001). Hence, an important task is to select appropriate domains. However, most previous work on domain adaptation relied on the implicit assumption that domains are somehow given. As more and more data becomes available, automatic ways to select data that is beneficial for a new (unknown) target domain are becoming attractive. This paper evaluates various ways to automatically acquire related training data for a given test set. The results show that an unsupervised technique based on topic models is effective – it outperforms random data selection on both languages examined, English"
P11-1157,W10-2605,0,0.309959,"Missing"
P11-1157,2006.jeptalnrecital-invite.2,1,0.787305,"Missing"
P11-1157,P09-1076,0,0.139875,"III (2007), sidestepped this problem of automatic domain selection and adaptation. For parsing, to our knowledge only one recent study has started to examine this issue (McClosky et al., 2010) – we will discuss their approach in Section 2. Rather, an implicit assumption of all of these studies is that domains are given, i.e. that they are represented by the respective corpora. Thus, a corpus has been considered a homogeneous unit. As more data is becoming available, it is unlikely that domains will be ‘given’. Moreover, a given corpus might not always be as homogeneous as originally thought (Webber, 2009; Lippincott et al., 2010). For instance, recent work has shown that the well-known Penn Treebank (PT) Wall Street Journal (WSJ) actually contains a variety of genres, including letters, wit and short verse (Webber, 2009). In this study we take a different approach. Rather than viewing a given corpus as a monolithic entity, 1566 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1566–1576, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics we break it down to the article-level and disregard corpora boundaries. Given"
P11-1157,P10-1077,0,0.0155103,"surface characteristics (unlabeled text). This has the advantage that we do not need to rely on additional supervised tools; moreover, it is interesting to know how far we can get with this level of information only. We examine the following feature representations: relative frequencies of words, relative frequencies of character tetragrams, and topic models. Our motivation was as follows. Relative frequencies of words are a simple and effective representation used e.g. in text classification (Manning and Sch¨utze, 1999), while character n-grams have proven successful in genre classification (Wu et al., 2010). Topic models (Blei et al., 2003; Steyvers and Griffiths, 2007) can be considered an advanced model over word distributions: every article is represented by a topic distribution, which in turn is a distribution over words. Similarity between documents can be measured by comparing topic distributions. Similarity Functions There are many possible similarity (or distance) functions. They fall broadly into two categories: probabilistically-motivated and geometrically-motivated functions. The similarity functions examined in this study will be described in the following. The Kullback-Leibler (KL)"
P11-1157,C00-2137,0,0.0404575,"PoS tagging using two different taggers: MXPOST, the MaxEnt tagger of Ratnaparkhi5 and Citar,6 a trigram HMM tagger. In all experiments, parsing performance is measured as Labeled Attachment Score (LAS), the percentage of tokens with correct dependency edge and label. To compute LAS, we use the CoNLL 2007 evaluation script7 with punctuation tokens excluded from scoring (as was the default setting in CoNLL 2006). PoS tagging accuracy is measured as the percentage of correctly labeled words out of all words. Statistical significance is determined by Approximate Randomization Test (Noreen, 1989; Yeh, 2000) with 10,000 iterations. Experimental Setup 4.2 4.1 Tools & Evaluation The parsing system used in this study is the MST parser (McDonald et al., 2005), a state-of-the-art data-driven graph-based dependency parser. It is English - WSJ For English, we use the portion of the Penn Treebank Wall Street Journal (WSJ) that has been made available in the CoNLL 2008 shared 4 3 It is not known what IN stands for, as also stated in Mark Liberman’s notes in the readme of the ACL/DCI corpus. However, a reviewer suggested that IN might stand for “index terms” which seems plausible. 1569 Data 5 http://mallet"
P11-1157,N10-1091,0,\N,Missing
P11-2034,J97-4005,0,0.403553,"y of attribute-value grammars is their reversibility. Attribute-value grammars are usually coupled with separate statistical components for parse selection and fluency ranking. We propose reversible stochastic attribute-value grammars, in which a single statistical model is employed both for parse selection and fluency ranking. 1 Introduction Reversible grammars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for"
P11-2034,P09-2025,0,0.0818586,"treebank. The resulting dependency structures are fed into the Alpino chart generator to construct derivations for each dependency structure. The derivations for which the corresponding sentences are closest to the original sentence in the treebank are marked correct. Due to a limit on generation time, some longer sentences and corresponding dependency structures were excluded from the data. As a result, the average sentence length was 15.7 tokens, with a maximum of 26 tokens. To compare a realization to the correct sentence, we use the General Text Matcher (GTM) method (Melamed et al., 2003; Cahill, 2009). 3.4 Training the models Models are trained by taking an informative sample of Ω(c) for each c in the training data (Osborne, 2000). This sample consists of at most 100 randomly selected derivations. Frequency-based feature selection is applied (Ratnaparkhi, 1999). A feature f partitions Ω(c), if there are derivations d and d0 in Ω(c) such that f (c, d) 6= f (c, d0 ). A feature is used if it partitions the informative sample of Ω(c) for at least two c. Table 1 lists the resulting characteristics of the training data for each model. We estimate the parameters of the conditional 197 Inputs 3688"
P11-2034,P04-1014,0,0.0375364,"oduction Reversible grammars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation. To Gertjan van Noord University of Groningen g.j.m.van.noord@rug.nl some"
P11-2034,W07-1203,0,0.0121463,"mars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation. To Gertjan van Noord University of Groningen g.j.m.van.noord@rug.nl some extent this is"
P11-2034,A00-2021,0,0.25022,"e Kok and van Noord, 2010). In the experiments, the cdbl part of the Alpino Treebank (van der Beek et al., 2002) is used as training data (7,154 sentences). The WR-P-P-H part (2,267 sentences) of the LASSY corpus (van Noord et al., 2010), which consists of text from the Trouw 2001 newspaper, is used for testing. 3.1 Features The features that we use in the experiment are the same features which are available in the Alpino parser and generator. In the following section, these features are described in some detail. Word adjacency. Two word adjacency features are used as auxiliary distributions (Johnson and Riezler, 2000). The first feature is the probability of the sentence according to a word trigram model. The second feature is the probability of the sentence according to a tag trigram model that uses the partof-speech tags assigned by the Alpino system. In both models, linear interpolation smoothing for unknown trigrams, and Laplacian smoothing for unknown words and tags is applied. The trigram models have been trained on the Twente Nieuws Corpus corpus (approximately 110 million words), excluding the Trouw 2001 corpus. In conventional parsing tasks, the value of the word trigram model is the same for all"
P11-2034,P99-1069,0,0.219742,"se selection and fluency ranking. We propose reversible stochastic attribute-value grammars, in which a single statistical model is employed both for parse selection and fluency ranking. 1 Introduction Reversible grammars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied b"
P11-2034,T75-1004,0,0.795418,"Missing"
P11-2034,N03-2021,0,0.082798,"ency structure in the treebank. The resulting dependency structures are fed into the Alpino chart generator to construct derivations for each dependency structure. The derivations for which the corresponding sentences are closest to the original sentence in the treebank are marked correct. Due to a limit on generation time, some longer sentences and corresponding dependency structures were excluded from the data. As a result, the average sentence length was 15.7 tokens, with a maximum of 26 tokens. To compare a realization to the correct sentence, we use the General Text Matcher (GTM) method (Melamed et al., 2003; Cahill, 2009). 3.4 Training the models Models are trained by taking an informative sample of Ω(c) for each c in the training data (Osborne, 2000). This sample consists of at most 100 randomly selected derivations. Frequency-based feature selection is applied (Ratnaparkhi, 1999). A feature f partitions Ω(c), if there are derivations d and d0 in Ω(c) such that f (c, d) 6= f (c, d0 ). A feature is used if it partitions the informative sample of Ω(c) for at least two c. Table 1 lists the resulting characteristics of the training data for each model. We estimate the parameters of the conditional"
P11-2034,P05-1011,0,0.0220256,"fluency ranking. 1 Introduction Reversible grammars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation. To Gertjan van Noord University of Groningen g.j."
P11-2034,W05-1510,0,0.0216691,"if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation. To Gertjan van Noord University of Groningen g.j.m.van.noord@rug.nl some extent this is reasonable, because some features are only relevant in a certain direction. For instance, features that represent aspects of the surface word order are important for generation, but irrelevant for parsing. Similarly, features which describe aspects of the logical form are important for parsing, but irrelevant for ge"
P11-2034,C00-1085,0,0.686985,"ivation constructed by rule 233’. In addition, there are features describing more complex syntactic patterns such as: fronting of subjects and other noun phrases, orderings in the middle field, long-distance dependencies, and parallelism of conjuncts in coordination. 3.2 Parse disambiguation Earlier we assumed that a treebank is a set of correct derivations. In practice, however, a treebank only contains an abstraction of such derivations (in our case sentences with corresponding dependency structures), thus abstracting away from syntactic details needed in a parse disambiguation model. As in Osborne (2000), the derivations for the parse disambiguation model are created by parsing the training corpus. In the current setting, up to at most 3000 derivations are created for every sentence. These derivations are then compared to the gold standard dependency structure to judge the quality of the parses. For a given sentence, the parses with the highest concept accuracy (van Noord, 2006) are considered correct, the rest is treated as incorrect. Generation Parse Reversible Fluency ranking For fluency ranking we also need access to full derivations. To ensure that the system is able to generate from the"
P11-2034,P02-1035,0,0.0482951,"cal model is employed both for parse selection and fluency ranking. 1 Introduction Reversible grammars were introduced as early as 1975 by Martin Kay (1975). In the eighties, the popularity of attribute-value grammars (AVG) was in part motivated by their inherent reversible nature. Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). Training a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generatio"
P11-2034,2006.jeptalnrecital-invite.2,1,0.880111,"Missing"
P11-2034,W07-2201,1,0.915836,"Missing"
P11-2034,W06-1661,0,0.055508,"ining a SAVG is feasible if a stochastic model is assumed which is conditioned on the input sentences (Johnson et al., 1999). Various parsers based on this approach now exist for various languages (Toutanova et al., 2002; Riezler et al., 2002; van Noord and Malouf, 2005; Miyao and Tsujii, 2005; Clark and Curran, 2004; Forst, 2007). SAVG can be applied for generation to select the most fluent realization from the set of possible realizations (Velldal et al., 2004). In this case, the stochastic model is conditioned on the input logical forms. Such generators exist for various languages as well (Velldal and Oepen, 2006; Nakanishi and Miyao, 2005; Cahill et al., 2007; de Kok and van Noord, 2010). If an AVG is applied both to parsing and generation, two distinct stochastic components are required, one for parsing, and one for generation. To Gertjan van Noord University of Groningen g.j.m.van.noord@rug.nl some extent this is reasonable, because some features are only relevant in a certain direction. For instance, features that represent aspects of the surface word order are important for generation, but irrelevant for parsing. Similarly, features which describe aspects of the logical form are important for par"
P11-2034,W98-1426,0,\N,Missing
P11-2034,J88-1004,0,\N,Missing
P11-2034,W02-2103,0,\N,Missing
P11-2034,W00-1505,0,\N,Missing
P11-2034,W05-1612,0,\N,Missing
P11-2034,J90-3001,0,\N,Missing
P11-2034,W96-0411,0,\N,Missing
P11-2034,A00-2023,0,\N,Missing
P11-2034,C94-1039,1,\N,Missing
P11-2034,W03-1013,0,\N,Missing
P11-2034,W08-2222,0,\N,Missing
P11-2034,J08-1003,0,\N,Missing
P11-2034,W07-2303,0,\N,Missing
P11-2034,J96-1002,0,\N,Missing
P11-2034,P84-1018,0,\N,Missing
P11-2034,P95-1034,0,\N,Missing
P11-2034,P02-1036,0,\N,Missing
P11-2034,C88-2128,0,\N,Missing
P11-2034,P90-1026,0,\N,Missing
P11-2034,P04-1057,1,\N,Missing
P11-2034,P99-1018,0,\N,Missing
P11-2034,A00-1031,0,\N,Missing
P11-2034,W09-0608,0,\N,Missing
P11-2034,P06-1042,0,\N,Missing
P11-2034,W11-2708,1,\N,Missing
P11-2034,2009.eamt-1.21,0,\N,Missing
P11-2034,P85-1015,0,\N,Missing
P11-2034,P83-1021,0,\N,Missing
P11-2034,I05-1015,0,\N,Missing
P11-2034,W05-0908,0,\N,Missing
P11-2034,P96-1027,0,\N,Missing
P11-2034,W04-3223,0,\N,Missing
P11-2034,2005.mtsummit-papers.15,0,\N,Missing
P11-2034,W03-1020,0,\N,Missing
P11-2034,W09-2609,1,\N,Missing
P11-2034,W02-2018,0,\N,Missing
P11-2034,W10-4216,1,\N,Missing
P11-2034,P11-1111,0,\N,Missing
P13-1147,P07-1073,0,0.0241181,"following text snippet: Google CEO Larry Page holds a press announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. Approaches that can cope with domain changes are essential. This is the problem of domain adaptation (DA) or transfer learning (TL). Technically, domain adaptation addresses the problem of learning when the assumption of independent and identic"
P13-1147,C10-1018,0,0.286619,"Larry Page holds a press announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. Approaches that can cope with domain changes are essential. This is the problem of domain adaptation (DA) or transfer learning (TL). Technically, domain adaptation addresses the problem of learning when the assumption of independent and identically distributed (i.i.d.) samples is viola"
P13-1147,P04-1054,0,0.34107,"combination of syntax and lexical generalization is very promising for domain adaptation. 1 Introduction Relation extraction is the task of extracting semantic relationships between entities in text, e.g. to detect an employment relationship between the person Larry Page and the company Google in the following text snippet: Google CEO Larry Page holds a press announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismatch between the training data"
P13-1147,P07-1033,0,0.237181,"Missing"
P13-1147,D10-1057,0,0.0358331,"Missing"
P13-1147,P07-1034,0,0.901861,"r question classification (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Croce et al., 2011). These kernels have not yet been studied for either domain adaptation or RE. Brown clusters were studied previously for feature-based approaches to RE (Sun et al., 2011; Chan and Roth, 2010), but they were not yet evaluated in kernels. Thus, we present a novel application of semantic syntactic tree kernels and Brown clusters for domain adaptation of tree-kernel based relation extraction. Regarding domain adaptation, several methods have been proposed, ranging from instance weighting (Jiang and Zhai, 2007) to approaches that change the feature representation (Daum´e III, 2007) or try to exploit pivot features to find a generalized shared representation between domains (Blitzer et al., 2006). The easy-adapt approach presented in Daum´e III (2007) assumes the supervised adaptation setting and is thus not applicable here. Structural correspondence learning (Blitzer et al., 2006) exploits unlabeled data from both source and target domain to find correspondences among features from different domains. These correspondences are then integrated as new features in the labeled data of the source domain."
P13-1147,P09-1114,0,0.0565774,"f independent and identically distributed (i.i.d.) samples is violated. Domain adaptation has been studied extensively during the last couple of years for various NLP tasks, e.g. two shared tasks have been organized on domain adaptation for dependency parsing (Nivre et al., 2007; Petrov and McDonald, 2012). Results were mixed, thus it is still a very active research area. However, to the best of our knowledge, there is almost no work on adapting relation extraction (RE) systems to new domains.1 There are some prior studies on the related tasks of multi-task transfer learning (Xu et al., 2008; Jiang, 2009) and distant supervision (Mintz et al., 2009), which are clearly related but different: the former is the problem of how to transfer knowledge from old to new relation types, while distant supervision tries to learn new relations from unlabeled text by exploiting weak-supervision in the form of a knowledge resource (e.g. Freebase). We assume the same relation types but a shift in the underlying 1 Besides an unpublished manuscript of a student project, but it is not clear what data was used. http://tinyurl.com/ bn2hdwk 1498 Proceedings of the 51st Annual Meeting of the Association for Computati"
P13-1147,P09-1113,0,0.128507,"ted (i.i.d.) samples is violated. Domain adaptation has been studied extensively during the last couple of years for various NLP tasks, e.g. two shared tasks have been organized on domain adaptation for dependency parsing (Nivre et al., 2007; Petrov and McDonald, 2012). Results were mixed, thus it is still a very active research area. However, to the best of our knowledge, there is almost no work on adapting relation extraction (RE) systems to new domains.1 There are some prior studies on the related tasks of multi-task transfer learning (Xu et al., 2008; Jiang, 2009) and distant supervision (Mintz et al., 2009), which are clearly related but different: the former is the problem of how to transfer knowledge from old to new relation types, while distant supervision tries to learn new relations from unlabeled text by exploiting weak-supervision in the form of a knowledge resource (e.g. Freebase). We assume the same relation types but a shift in the underlying 1 Besides an unpublished manuscript of a student project, but it is not clear what data was used. http://tinyurl.com/ bn2hdwk 1498 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1498–1507, c Sofia, B"
P13-1147,W11-2906,0,0.033603,"ion that is minimized during the training process. Let l(x, y, θ) be some loss function. Then, as shown in Jiang and Zhai (2007), the loss function can be weighted i) by βi l(x, y, θ), such that βi = PPst (x (xi ) , where Ps and Pt are the source and target distributions, respectively. Huang et al. (2007) present an application of instance weighting to support vector machines by minimizing the following re-weighted P function: minθ,ξ 21 ||θ||2 + C m i=1 βi ξi . Finding a good weight function is non-trivial (Jiang and Zhai, 2007) and several approximations have been evaluated in the past, e.g. Søgaard and Haulrich (2011) use a bigram-based text classifier to discriminate between domains. We will use a binary classifier trained on RE instance representations. 4 Computational Structures for RE A common way to represent a constituency-based relation instance is the PET (path-enclosed-tree), the smallest subtree including the two target entities (Zhang et al., 2006). This is basically the former structure PAF2 (predicate argument feature) defined in Moschitti (2004) for the extraction of predicate argument relations. The syntactic rep2 It is the smallest subtree enclosing the predicate and one of its argument nod"
P13-1147,C12-2114,0,0.0226788,"nald, 2012). Participants were asked to build a single system that can robustly parse all domains (reviews, weblogs, answers, emails, newsgroups), rather than to build several domain-specific systems. We consider this as a shift in what was considered domain adaptation in the past (adapt from source to a specific target) and what can be considered a somewhat different recent view of DA, that became widespread since 2011/2012. The latter assumes that the target domain(s) is/are not really known in advance. In this setup, the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012), i.e. one wants to build a system that can robustly handle any kind of data. We propose to combine (i) term generalization approaches and (ii) structured kernels to improve the performance of a relation extractor on new domains. Previous studies have shown that lexical and syntactic features are both very important (Zhang et al., 2006). We combine structural features with lexical information generalized by clusters or similarity. Given the complexity of feature engineering, we exploit kernel methods (ShaweTaylor and Cristianini, 2004). We encode word clusters or similarity in tree kernels, wh"
P13-1147,P11-1053,0,0.633811,"ess announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. Approaches that can cope with domain changes are essential. This is the problem of domain adaptation (DA) or transfer learning (TL). Technically, domain adaptation addresses the problem of learning when the assumption of independent and identically distributed (i.i.d.) samples is violated. Domain adaptat"
P13-1147,I08-2119,0,0.231292,"P 70.7 71.3 72.6 Table 3: Comparison to previous work on the 7 relations of ACE 2004. K: kernel-based; F: featurebased; yes/no: models argument order explicitly. 0.0 nw_bn Type K,yes K,yes K,yes F,yes F,no Type K,yes K,yes K,yes http://disi.unitn.it/ikernels/ Alignment to Prior Work Although most prior studies performed 5-fold cross-validation on ACE 2004, it is often not clear whether the partitioning has been done on the instance or on the document level. Moreover, it is often not stated whether argument order is modeled explicitly, making it difficult to compare system performance. Citing Wang (2008), “We feel that there is a sense of increasing confusion down this line of research”. To ease comparison for future research we use the same 5-fold split on the document level as Sun et al. (2011)13 and make our system publicly available (see Section 5). Table 3 shows that our system (bottom) aligns well with the state of the art. Our best system (composite kernel with polynomial expansion) reaches an F1 of 70.1, which aligns well to the 70.4 of Sun et al. (2011) that use the same datasplit. This is slightly behind that of Zhang (2006); the reason might be threefold: i) different data partitio"
P13-1147,xu-etal-2008-adaptation,0,0.0186049,"the assumption of independent and identically distributed (i.i.d.) samples is violated. Domain adaptation has been studied extensively during the last couple of years for various NLP tasks, e.g. two shared tasks have been organized on domain adaptation for dependency parsing (Nivre et al., 2007; Petrov and McDonald, 2012). Results were mixed, thus it is still a very active research area. However, to the best of our knowledge, there is almost no work on adapting relation extraction (RE) systems to new domains.1 There are some prior studies on the related tasks of multi-task transfer learning (Xu et al., 2008; Jiang, 2009) and distant supervision (Mintz et al., 2009), which are clearly related but different: the former is the problem of how to transfer knowledge from old to new relation types, while distant supervision tries to learn new relations from unlabeled text by exploiting weak-supervision in the form of a knowledge resource (e.g. Freebase). We assume the same relation types but a shift in the underlying 1 Besides an unpublished manuscript of a student project, but it is not clear what data was used. http://tinyurl.com/ bn2hdwk 1498 Proceedings of the 51st Annual Meeting of the Association"
P13-1147,P04-1043,1,0.615409,"ξi . Finding a good weight function is non-trivial (Jiang and Zhai, 2007) and several approximations have been evaluated in the past, e.g. Søgaard and Haulrich (2011) use a bigram-based text classifier to discriminate between domains. We will use a binary classifier trained on RE instance representations. 4 Computational Structures for RE A common way to represent a constituency-based relation instance is the PET (path-enclosed-tree), the smallest subtree including the two target entities (Zhang et al., 2006). This is basically the former structure PAF2 (predicate argument feature) defined in Moschitti (2004) for the extraction of predicate argument relations. The syntactic rep2 It is the smallest subtree enclosing the predicate and one of its argument node. 1500 resentation used by Zhang et al. (2006) (we will refer to it as PET Zhang) is the PET with enriched entity information: e.g. E1-NAM-PER, including entity type (PER, GPE, LOC, ORG) and mention type (NAM, NOM, PRO, PRE: name, nominal, pronominal or premodifier). An alternative kernel that does not use syntactic information is the Bag-of-Words (BOW) kernel, where a single root node is added above the terminals. Note that in this BOW kernel w"
P13-1147,W02-1010,0,0.0290122,"shows that a suitable combination of syntax and lexical generalization is very promising for domain adaptation. 1 Introduction Relation extraction is the task of extracting semantic relationships between entities in text, e.g. to detect an employment relationship between the person Larry Page and the company Google in the following text snippet: Google CEO Larry Page holds a press announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismat"
P13-1147,I05-1034,0,0.00665656,"between the person Larry Page and the company Google in the following text snippet: Google CEO Larry Page holds a press announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. Approaches that can cope with domain changes are essential. This is the problem of domain adaptation (DA) or transfer learning (TL). Technically, domain adaptation addresses the problem of l"
P13-1147,D09-1143,1,0.851526,"snippet: Google CEO Larry Page holds a press announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. Approaches that can cope with domain changes are essential. This is the problem of domain adaptation (DA) or transfer learning (TL). Technically, domain adaptation addresses the problem of learning when the assumption of independent and identically distributed (i.i"
P13-1147,P06-1104,0,0.808043,"ompany Google in the following text snippet: Google CEO Larry Page holds a press announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. Approaches that can cope with domain changes are essential. This is the problem of domain adaptation (DA) or transfer learning (TL). Technically, domain adaptation addresses the problem of learning when the assumption of independ"
P13-1147,P05-1053,0,0.496205,"arry Page and the company Google in the following text snippet: Google CEO Larry Page holds a press announcement at its headquarters in New York on May 21, 2012. Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al., 2002; Culotta and Sorensen, 2004; ∗ The first author was affiliated with the Department of Computer Science and Information Engineering of the University of Trento (Povo, Italy) during the design of the models, experiments and writing of the paper. Zhang et al., 2005; Zhou et al., 2005; Zhang et al., 2006; Bunescu, 2007; Nguyen et al., 2009; Chan and Roth, 2010; Sun et al., 2011). However, the clear drawback of supervised methods is the need of training data, which can slow down the delivery of commercial applications in new domains: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. Approaches that can cope with domain changes are essential. This is the problem of domain adaptation (DA) or transfer learning (TL). Technically, domain adaptation addresses the problem of learning when the as"
P13-1147,J92-4003,0,\N,Missing
P13-1147,W06-1615,0,\N,Missing
P13-1147,D07-1096,0,\N,Missing
P14-1118,J08-4004,0,0.0253937,"Missing"
P14-1118,I13-1041,0,0.0122599,"ouTube contain rapidly changing information generated by millions of users that can dramatically affect the reputation of a person or an organization. This raises the importance of automatic extraction of sentiments and opinions expressed in social media. YouTube is a unique environment, just like Twitter, but probably even richer: multi-modal, with a social graph, and discussions between people sharing an interest. Hence, doing sentiment research in such an environment is highly relevant for the community. While the linguistic conventions used on Twitter and YouTube indeed show similarities (Baldwin et al., 2013), focusing on YouTube allows to exploit context information, possibly also multi-modal information, not available in isolated tweets, thus rendering it a valuable resource for the future research. Nevertheless, there is almost no work showing effective OM on YouTube comments. To the best of our knowledge, the only exception is given by The comment contains a product name xoom and some negative expressions, thus, a bag-of-words model would derive a negative polarity for this product. In contrast, the opinion towards the product is neutral as the negative sentiment is expressed towards the video"
P14-1118,P07-1056,0,0.0386524,"des (T¨ackstr¨om and McDonald, 2011) and syntactic parse trees with vectorized nodes (Socher et al., 2011), recently, a comprehensive study by Wang and Manning (2012) showed that a simple model using bigrams and SVMs performs on par with more complex models. In contrast, we show that adding structural features from syntactic trees is particularly useful for the cross-domain setting. They help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn"
P14-1118,P11-2008,0,0.0867098,"Missing"
P14-1118,P13-1147,1,0.835714,"by Wang and Manning (2012) showed that a simple model using bigrams and SVMs performs on par with more complex models. In contrast, we show that adding structural features from syntactic trees is particularly useful for the cross-domain setting. They help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn and Moschitti, 2013) and Semantic Textual Similarity (Severyn et al., 2013). Moreover, we introduce additional tags, e.g., video concepts, polarit"
P14-1118,D11-1141,0,0.0139627,"cally targets the sentiment and comment type classification tasks. In particular, our shallow tree structure is a two-level syntactic hierarchy built from word lemmas (leaves) and part-of-speech tags that are further grouped into chunks (Fig. 1). As full syntactic parsers such as constituency or dependency tree parsers would significantly degrade in performance on noisy texts, e.g., Twitter or YouTube comments, we opted for shallow structures, which rely on simpler and more robust components: a part-of-speech tagger and a chunker. Moreover, such taggers have been recently updated with models (Ritter et al., 2011; Gimpel et al., 2011) trained specifically to process noisy texts showing significant reductions in the error rate on usergenerated texts, e.g., Twitter. Hence, we use the CMU Twitter pos-tagger (Gimpel et al., 2011; Owoputi et al., 2013) to obtain the part-of-speech tags. Our second component – chunker – is taken from (Ritter et al., 2011), which also comes with a model trained on Twitter data3 and shown to perform better on noisy data such as user comments. To address the specifics of OM tasks on YouTube comments, we enrich syntactic trees with semantic tags to encode: (i) central concepts"
P14-1118,D13-1044,1,0.825377,"l., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn and Moschitti, 2013) and Semantic Textual Similarity (Severyn et al., 2013). Moreover, we introduce additional tags, e.g., video concepts, polarity and negation words, to achieve better generalization across different domains where the word distribution and vocabulary changes. 3 Representations and models Our approach to OM on YouTube relies on the design of classifiers to predict comment type and opinion polarity. Such classifiers are traditionally based on bag-of-words and more advanced features. In the next sections, we define a baseline feature vector model and a novel structural model based on kernel methods"
P14-1118,P13-2125,1,0.826898,"own to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn and Moschitti, 2013) and Semantic Textual Similarity (Severyn et al., 2013). Moreover, we introduce additional tags, e.g., video concepts, polarity and negation words, to achieve better generalization across different domains where the word distribution and vocabulary changes. 3 Representations and models Our approach to OM on YouTube relies on the design of classifiers to predict comment type and opinion polarity. Such classifiers are traditionally based on bag-of-words and more advanced features. In the next sections, we define a baseline feature vector model and a novel structural model based on kernel methods. 3.1 Feature Set We enrich the traditional bag-of-word"
P14-1118,D11-1014,0,0.0282501,"atings, rather than predicting the sentiment expressed in a YouTube comment or its information content. Exploiting the information from user ratings is a feature that we have not exploited thus far, but we believe that it is a valuable feature to use in future work. Most of the previous work on supervised sentiment analysis use feature vectors to encode documents. While a few successful attempts have been made to use more involved linguistic analysis for opinion mining, such as dependency trees with latent nodes (T¨ackstr¨om and McDonald, 2011) and syntactic parse trees with vectorized nodes (Socher et al., 2011), recently, a comprehensive study by Wang and Manning (2012) showed that a simple model using bigrams and SVMs performs on par with more complex models. In contrast, we show that adding structural features from syntactic trees is particularly useful for the cross-domain setting. They help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust sys"
P14-1118,C12-2114,0,0.0125563,"ently, a comprehensive study by Wang and Manning (2012) showed that a simple model using bigrams and SVMs performs on par with more complex models. In contrast, we show that adding structural features from syntactic trees is particularly useful for the cross-domain setting. They help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013). This is in line with recent advances in parsing the web (Petrov and McDonald, 2012), where participants where asked to build a single system able to cope with different yet re1253 lated domains. Our approach relies on robust syntactic structures to automatically generate patterns that adapt better. These representations have been inspired by the semantic models developed for Question Answering (Moschitti, 2008; Severyn and Moschitti, 2012; Severyn and Moschitti, 2013) and Semantic Textual Similarity (Severyn et al., 2013). Moreover, we introduce additional tags, e"
P14-1118,E06-1015,1,0.894293,"sible subtrees, thus producing generalized (back-off) features, e.g., [S [negative-VP [negative-V [destroy]] [PRODUCT-NP]]]] or [S [negative-VP [PRODUCT-NP]]]]. nary classifier is trained for each of the classes and the predicted class is obtained by taking a class from the classifier with a maximum prediction score. Our back-end binary classifier is SVMlight-TK4 , which encodes structural kernels in the SVM-light (Joachims, 2002) solver. We define a novel and efficient tree kernel function, namely, Shallow syntactic Tree Kernel (SHTK), which is as expressive as the Partial Tree Kernel (PTK) (Moschitti, 2006a) to handle feature engineering over the structural representations of the STRUCT model. A polynomial kernel of degree 3 is applied to feature vectors (FVEC). Combining structural and vector models. A typical kernel machine, e.g., SVM, classifies a test input x using Pthe following prediction funcx, x i ), where αi are x) = tion: h(x i αi yi K(x the model parameters estimated from the training data, yi are target variables, xi are support vectors, and K(·, ·) is a kernel function. The latter computes the similarity between two comments. The STRUCT model treats each comment as a tuT , v i comp"
P14-1118,P11-2100,0,0.0628624,"Missing"
P14-1118,uryupina-etal-2014-sentube,1,0.829353,"ogle.com/youtube/v3/ annotated 208 videos with around 35k comments (128 videos TABLETS and 80 for AUTO). To evaluate the quality of the produced labels, we asked 5 annotators to label a sample set of one hundred comments and measured the agreement. The resulting annotator agreement α value (Krippendorf, 2004; Artstein and Poesio, 2008) scores are 60.6 (AUTO), 72.1 (TABLETS) for the sentiment task and 64.1 (AUTO), 79.3 (TABLETS) for the type classification task. For the rest of the comments, we assigned the entire annotation task to a single coder. Further details on the corpus can be found in Uryupina et al. (2014). 5 Experiments This section reports: (i) experiments on individual subtasks of opinion and type classification; (ii) the full task of predicting type and sentiment; (iii) study on the adaptability of our system by learning on one domain and testing on the other; (iv) learning curves that provide an indication on the required amount and type of data and the scalability to other domains. 5.1 Task description Sentiment classification. We treat each comment as expressing positive, negative or neutral sentiment. Hence, the task is a threeway classification. Type classification. One of the challeng"
P14-1118,N13-1039,0,0.0154655,"(Fig. 1). As full syntactic parsers such as constituency or dependency tree parsers would significantly degrade in performance on noisy texts, e.g., Twitter or YouTube comments, we opted for shallow structures, which rely on simpler and more robust components: a part-of-speech tagger and a chunker. Moreover, such taggers have been recently updated with models (Ritter et al., 2011; Gimpel et al., 2011) trained specifically to process noisy texts showing significant reductions in the error rate on usergenerated texts, e.g., Twitter. Hence, we use the CMU Twitter pos-tagger (Gimpel et al., 2011; Owoputi et al., 2013) to obtain the part-of-speech tags. Our second component – chunker – is taken from (Ritter et al., 2011), which also comes with a model trained on Twitter data3 and shown to perform better on noisy data such as user comments. To address the specifics of OM tasks on YouTube comments, we enrich syntactic trees with semantic tags to encode: (i) central concepts of the video, (ii) sentiment-bearing words expressing positive or negative sentiment and (iii) negation words. To automatically identify concept words of the video we use context words (tokens detected as nouns by the part-of-speech tagger"
P14-1118,pak-paroubek-2010-twitter,0,0.00951516,"si.unitn. it/iKernels/projects/sentube/ corpus of blogs (Kessler et al., 2010), etc. The aforementioned corpora are, however, only partially suitable for developing models on social media, since the informal text poses additional challenges for Information Extraction and Natural Language Processing. Similar to Twitter, most YouTube comments are very short, the language is informal with numerous accidental and deliberate errors and grammatical inconsistencies, which makes previous corpora less suitable to train models for OM on YouTube. A recent study focuses on sentiment analysis for Twitter (Pak and Paroubek, 2010), however, their corpus was compiled automatically by searching for emoticons expressing positive and negative sentiment only. Siersdorfer et al. (2010) focus on exploiting user ratings (counts of ‘thumbs up/down’ as flagged by other users) of YouTube video comments to train classifiers to predict the community acceptance of new comments. Hence, their goal is different: predicting comment ratings, rather than predicting the sentiment expressed in a YouTube comment or its information content. Exploiting the information from user ratings is a feature that we have not exploited thus far, but we b"
P14-1118,P12-2018,0,0.0351227,"a YouTube comment or its information content. Exploiting the information from user ratings is a feature that we have not exploited thus far, but we believe that it is a valuable feature to use in future work. Most of the previous work on supervised sentiment analysis use feature vectors to encode documents. While a few successful attempts have been made to use more involved linguistic analysis for opinion mining, such as dependency trees with latent nodes (T¨ackstr¨om and McDonald, 2011) and syntactic parse trees with vectorized nodes (Socher et al., 2011), recently, a comprehensive study by Wang and Manning (2012) showed that a simple model using bigrams and SVMs performs on par with more complex models. In contrast, we show that adding structural features from syntactic trees is particularly useful for the cross-domain setting. They help to build a system that is more robust across domains. Therefore, rather than trying to build a specialized system for every new target domain, as it has been done in most prior work on domain adaptation (Blitzer et al., 2007; Daum´e, 2007), the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012; Plank and Moschitti, 2013)"
P14-1118,H05-1044,0,0.00590306,"Feature Set We enrich the traditional bag-of-word representation with features from a sentiment lexicon and features quantifying the negation present in the comment. Our model (FVEC) encodes each document using the following feature groups: - word n-grams: we compute unigrams and bigrams over lower-cased word lemmas where binary values are used to indicate the presence/absence of a given item. - lexicon: a sentiment lexicon is a collection of words associated with a positive or negative sentiment. We use two manually constructed sentiment lexicons that are freely available: the MPQA Lexicon (Wilson et al., 2005) and the lexicon of Hu and Liu (2004). For each of the lexicons, we use the number of words found in the comment that have positive and negative sentiment as a feature. - negation: the count of negation words, e.g., {don’t, never, not, etc.}, found in a comment.2 Our structural representation (defined next) enables a more involved treatment of negation. - video concept: cosine similarity between a comment and the title/description of the video. Most of the videos come with a title and a short description, which can be used to encode the topicality of 2 The list of negation words is adopted htt"
P14-1118,P07-1033,0,\N,Missing
P14-2062,W10-0701,0,0.0321628,", we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks. 1 Introduction Training good predictive NLP models typically requires annotated data, but getting professional annotators to build useful data sets is often timeconsuming and expensive. Snow et al. (2008) showed, however, that crowdsourced annotations can produce similar results to annotations made by experts. Crowdsourcing services such as Amazon’s Mechanical Turk has since been successfully used for various annotation tasks in NLP (Jha et al., 2010; Callison-Burch and Dredze, 2010). However, most applications of crowdsourcing in NLP have been concerned with classification problems, such as document classification and constructing lexica (Callison-Burch and Dredze, 2010). A large part of NLP problems, however, are structured prediction tasks. Typically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex interactions between the annotations. Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder. 1 One of the reviewers alerted us to an unpublished masters the"
P14-2062,R13-1026,0,0.0281052,"Missing"
P14-2062,N13-1037,0,0.0131419,", and the data contains mostly non-entities, so the complexity is manageable. The question of whether a more linguistically involved structured task like part-of-speech (POS) tagging can be crowdsourced has remained largely unaddressed.1 In this paper, we investigate how well lay annotators can produce POS labels for Twitter data. In our setup, we present annotators with one word at a time, with a minimal surrounding context (two words to each side). Our choice of annotating Twitter data is not coincidental: with the shortlived nature of Twitter messages, models quickly lose predictive power (Eisenstein, 2013), and retraining models on new samples of more representative data becomes necessary. Expensive professional annotation may be prohibitive for keeping NLP models up-to-date with linguistic and topical changes on Twitter. We use a minimum of instructions and require few qualifications. Obviously, lay annotation is generally less reliable than professional annotation. It is therefore common to aggregate over multiple annotations for the same item to get more robust annotations. In this paper we compare two aggregation schemes, namely majority voting (MV) and MACE (Hovy et al., 2013). We also sho"
P14-2062,W10-0713,0,0.224342,"Missing"
P14-2062,I11-1100,0,0.0604865,"Missing"
P14-2062,P11-2008,0,0.210133,"Missing"
P14-2062,D08-1027,0,0.435201,"Missing"
P14-2062,W10-0702,0,0.140457,"s experts. Further, we show that the models learned from crowdsourced annotations fare as well as the models learned from expert annotations in downstream tasks. 1 Introduction Training good predictive NLP models typically requires annotated data, but getting professional annotators to build useful data sets is often timeconsuming and expensive. Snow et al. (2008) showed, however, that crowdsourced annotations can produce similar results to annotations made by experts. Crowdsourcing services such as Amazon’s Mechanical Turk has since been successfully used for various annotation tasks in NLP (Jha et al., 2010; Callison-Burch and Dredze, 2010). However, most applications of crowdsourcing in NLP have been concerned with classification problems, such as document classification and constructing lexica (Callison-Burch and Dredze, 2010). A large part of NLP problems, however, are structured prediction tasks. Typically, sequence labeling tasks employ a larger set of labels than classification problems, as well as complex interactions between the annotations. Disagreement among annotators is therefore potentially higher, and the task of annotating structured data thus harder. 1 One of the reviewers alerte"
P14-2062,Q13-1001,0,0.0363069,"Missing"
P14-2062,D07-1031,0,0.113169,"Missing"
P14-2062,D12-1127,0,0.0605467,"Missing"
P14-2062,J94-2001,0,0.583625,"Missing"
P14-2062,N13-1039,0,0.0526092,"Missing"
P14-2062,petrov-etal-2012-universal,0,0.0651366,"and annotation effort, we can produce structured annotations of near-expert quality. We also show that these annotations lead to better POS tagging models than previous models learned from crowdsourced lexicons (Li et al., 2012). Finally, we show that models learned from these annotations are competitive with models learned from expert annotations on various downstream tasks. 2 In order to use the annotations to train models that can be applied across various data sets, i.e., making out-of-sample evaluation possible (see Section 5), we follow Hovy et al. (2014) in using the universal tag set (Petrov et al., 2012) with 12 labels. Our Approach We crowdsource the training section of the data from Gimpel et al. (2011)2 with POS tags. We use Crowdflower,3 to collect five annotations for each word, and then find the most likely label for each word among the possible annotations. See Figure 1 for an example. If the correct label is not among the annotations, we are unable to recover the correct answer. This was the case for 1497 instances in our data (cf. the token “:” in the example). We thus report on oracle score, i.e., the best label sequence that could possibly be found, which is correct except for the"
P14-2062,P09-1057,0,0.0433099,"Missing"
P14-2062,D11-1141,0,0.0374043,"Missing"
P14-2062,N03-1028,0,0.197607,"Missing"
P14-2062,hovy-etal-2014-pos,1,\N,Missing
P14-2062,N13-1132,1,\N,Missing
P14-2083,W07-1508,0,0.074711,"Missing"
P14-2083,P11-2008,0,0.0296121,"Missing"
P14-2083,N13-1132,1,0.642283,"Missing"
P14-2083,P14-2062,1,0.89815,"Missing"
P14-2083,jurgens-2014-analysis,0,0.0843971,"Missing"
P14-2083,J93-2004,0,0.0463277,"Missing"
P14-2083,petrov-etal-2012-universal,0,0.0307925,"Missing"
P14-2083,E14-1078,1,0.860375,"Missing"
P14-2083,C12-1149,0,0.0661935,"e hard cases. Moreover, we compare professional annotation to that of lay people. We instructed annotators to use the 12 universal POS tags of Petrov et al. (2012). We did so in order to make comparison between existing data sets possible. Moreover, this allows us to focus on really hard cases, as any debatable case in the coarse-grained tag set is necessarily also part of the finer-grained tag set.2 For each domain, we collected exactly 500 doubly-annotated sentences/tweets. Besides these English data sets, we also obtained doubly-annotated POS data from the French Social Media Bank project (Seddah et al., 2012).3 All data sets, except the French one, are publicly available at http://lowlands.ku. dk/. We present disagreements as Hinton diagrams in Figure 1a–c. Note that the spoken language data does not include punctuation. The correlations between the disagreements are highly significant, with Spearman coefficients ranging from 0.644 Our analyses show that a) experts disagree on the known hard cases when freely annotating text, and b) that these disagreements are the same across text types. More surprisingly, though, we also find that, as discussed next, c) roughly the same disagreements are also ob"
P14-2083,E03-1068,0,0.0104121,"probably because they rely more on orthographic cues than on distributional evidence. The disagreements are still strongly correlated with the ones observed with expert annotators, but at a slightly lower coefficient (with a Spearman’s ρ of 0.493 and Kendall’s τ of 0.366 for WSJ). Figure 3: Disagreement on French social media of the variation. In this section, we investigate what happens if we weed out obvious errors by detecting annotation inconsistencies across a corpus. The disagreements that remain are the truly hard cases. We use a modified version of the a priori algorithm introduced in Dickinson and Meurers (2003) to identify annotation inconsistencies. It works by collecting “variation n-grams”, i.e. the longest sequence of words (n-gram) in a corpus that has been observed with a token being tagged differently in another occurence of the same n-gram in the same corpus. The algorithm starts off by looking for unigrams and expands them until no longer n-grams are found. For each variation n-gram that we found in WSJ-00, i.e, a word in various contexts and the possible tags associated with it, we present annotators with the cross product of contexts and tags. Essentially, we ask for a binary decision: Is"
P15-1062,H05-1091,0,0.148681,"ompare the tree kernel-based and the feature-based method for RE in a compatible way, on the same resources and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeti"
P15-1062,P07-1073,0,0.0249017,"ompatible way, on the same resources and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the"
P15-1062,C10-1018,0,0.0199233,"t the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics chitti (2"
P15-1062,P07-1033,0,0.543746,"Missing"
P15-1062,W10-2604,0,0.0185843,"14), and Nguyen and Grishman (2015) employ word embeddings in the framework of convolutional neural networks for relation classification and extraction, respectively. Sterckx et al. (2014) utilize word embeddings to reduce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work examines word embeddings for tree kernels as well as domain adaptation as we do. Regarding DA, in the unsupervised DA setting, Huang and Yates (2010) attempt to learn multidimensional feature representations while Blitzer et al. (2006) introduce structural correspondence learning. Daum´e (2007) proposes an easy adaptation framework (EA) while Xiao and Guo (2013) present a log-bilinear language adaptation technique in the supervised DA setting. Unfortunately, all of this work assumes some prior (in the form of either labeled or unlabeled data) on the target domains for the sequential labeling tasks, in contrast to our single-system unsupervised DA setting for relation extraction. An alternative method that is also popular to DA is instance"
P15-1062,N07-1015,0,0.246901,"rror analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, c Beijing, China, July 26-31, 2015. 2015 Association for Computational"
P15-1062,D10-1115,0,0.0295108,"der the following methods to obtain the semantic representation Vi from the word embeddings of the context words of Ri (assuming d is the dimensionality of the word embeddings): HEAD: Vi = the concatenation of the word embeddings of the two entity mention heads of Ri . This representation is inherited from Nguyen and Grishman (2014) that only examine embeddings at the word level separately for the feature-based method without considering the compositionality embeddings of relation mentions. The dimensionality of HEAD is 2d. According to the principle of compositionality (Werning et al., 2006; Baroni and Zamparelli, 2010; Paperno et al., 2014), the meaning of a complex expression is determined by the meanings of its components and the rules to combine them. We study the following two compositionality embeddings for relation mentions that can be generated from the embeddings of the context words: PHRASE: Vi = the mean of the embeddings of the words contained in the PET tree Ti of Ri . Although this composition is simple, it is in fact competitive to the more complicated methods based on recursive neural networks (Socher et al., 2012b; Blacoe and Lapata, 2012; Sterckx et al., 2014) on representing phrase semant"
P15-1062,P07-1034,0,0.625569,"rror analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, c Beijing, China, July 26-31, 2015. 2015 Association for Computational"
P15-1062,P04-3022,0,0.0514675,"system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, c Beij"
P15-1062,D12-1050,0,0.0269934,"ciple of compositionality (Werning et al., 2006; Baroni and Zamparelli, 2010; Paperno et al., 2014), the meaning of a complex expression is determined by the meanings of its components and the rules to combine them. We study the following two compositionality embeddings for relation mentions that can be generated from the embeddings of the context words: PHRASE: Vi = the mean of the embeddings of the words contained in the PET tree Ti of Ri . Although this composition is simple, it is in fact competitive to the more complicated methods based on recursive neural networks (Socher et al., 2012b; Blacoe and Lapata, 2012; Sterckx et al., 2014) on representing phrase semantics. TREE: This is motivated by the training of recursive neural networks (Socher et al., 2012a) for semantic compositionality and attempts to aggregate the context words embeddings syntactically. In particular, we compute an embedding for every node in the PET tree in a bottom-up manner. The embeddings of the leaves are the embeddings of the words associated with them while the embeddings of the internal nodes are the means of the embeddings of their children nodes. We use the embeddings of the root of the PET tree to represent the relation"
P15-1062,W06-1615,0,0.38594,"ional neural networks for relation classification and extraction, respectively. Sterckx et al. (2014) utilize word embeddings to reduce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work examines word embeddings for tree kernels as well as domain adaptation as we do. Regarding DA, in the unsupervised DA setting, Huang and Yates (2010) attempt to learn multidimensional feature representations while Blitzer et al. (2006) introduce structural correspondence learning. Daum´e (2007) proposes an easy adaptation framework (EA) while Xiao and Guo (2013) present a log-bilinear language adaptation technique in the supervised DA setting. Unfortunately, all of this work assumes some prior (in the form of either labeled or unlabeled data) on the target domains for the sequential labeling tasks, in contrast to our single-system unsupervised DA setting for relation extraction. An alternative method that is also popular to DA is instance weighting (Jiang and Zhai, 2007b). However, as shown by Plank and Moschitti (2013), in"
P15-1062,P07-1056,0,0.36694,"Missing"
P15-1062,P14-2012,1,0.114933,"stem trained on some source domain to perform well on new target domains. We here focus on the unsupervised domain adaptation (i.e., no labeled target data) and singlesystem DA (Petrov and McDonald, 2012; Plank and Moschitti, 2013), i.e., building a single system that is able to cope with different, yet related target domains. While DA has been investigated extensively in the last decade for various natural language processing (NLP) tasks, the examination of DA for RE is only very recent. To the best of our knowledge, there have been only three studies on DA for RE (Plank and Moschitti, 2013; Nguyen and Grishman, 2014; Nguyen et al., 2014). Of these, Nguyen et al. (2014) follow the supervised DA paradigm and assume some labeled data in the target domains. In contrast, Plank and Moschitti (2013) and Nguyen and Grishman (2014) work on the unsupervised DA. In our view, unsupervised DA is more challenging, but more realistic and practical for RE as we usually do not know which target domains we need to work on in advance, thus cannot expect to possess labeled data of the target domains. Our current work therefore focuses on the single-system unsupervised DA. Besides, note that this setting tries to construct a"
P15-1062,W15-1506,1,0.63793,"ngton Post is reporting she shot several Iraqi soldiers before she was captured and she was shot herself , too.”. However, as the syntactical structure of X1 is more similar to X2’s, and is remarkably different from X3 as well as the other closest phrases (ranked from 2nd to 8th), the new kernel function Knew would still prefer X2 due to its trade-off between syntax and semantics. 6 Related work Word embeddings are only applied to RE recently. Socher et al. (2012b) use word embeddings as input for matrix-vector recursive neural networks in relation classification while Zeng et al. (2014), and Nguyen and Grishman (2015) employ word embeddings in the framework of convolutional neural networks for relation classification and extraction, respectively. Sterckx et al. (2014) utilize word embeddings to reduce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work examines word embeddings for tree kernels as well as domain adaptation as we do. Regarding DA, in the unsupervised DA setting, Huang and Yates (2010) attempt to l"
P15-1062,D09-1143,0,0.0167878,"es and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Nat"
P15-1062,P14-1076,0,0.0259328,"Missing"
P15-1062,N15-1155,0,0.124471,"Missing"
P15-1062,C14-1220,0,0.048275,"the sentence “The Washington Post is reporting she shot several Iraqi soldiers before she was captured and she was shot herself , too.”. However, as the syntactical structure of X1 is more similar to X2’s, and is remarkably different from X3 as well as the other closest phrases (ranked from 2nd to 8th), the new kernel function Knew would still prefer X2 due to its trade-off between syntax and semantics. 6 Related work Word embeddings are only applied to RE recently. Socher et al. (2012b) use word embeddings as input for matrix-vector recursive neural networks in relation classification while Zeng et al. (2014), and Nguyen and Grishman (2015) employ word embeddings in the framework of convolutional neural networks for relation classification and extraction, respectively. Sterckx et al. (2014) utilize word embeddings to reduce noise of training data in distant supervision. Kuksa et al. (2010) present a string kernel for bio-relation extraction with word embeddings, and Yu et al. (2014; 2015) study the factor-based compositional embedding models. However, none of this work examines word embeddings for tree kernels as well as domain adaptation as we do. Regarding DA, in the unsupervised DA setting, Hua"
P15-1062,P06-1104,0,0.394474,"method for RE in a compatible way, on the same resources and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Lingu"
P15-1062,P13-1147,1,0.0781232,", New York University, New York, NY 10003, USA § Center for Language Technology, University of Copenhagen, Denmark thien@cs.nyu.edu,bplank@cst.dk,grishman@cs.nyu.edu Abstract of the traditional RE techniques degrades significantly in such a domain mismatch case (Plank and Moschitti, 2013). To alleviate this performance loss, we need to resort to domain adaptation (DA) techniques to adapt a system trained on some source domain to perform well on new target domains. We here focus on the unsupervised domain adaptation (i.e., no labeled target data) and singlesystem DA (Petrov and McDonald, 2012; Plank and Moschitti, 2013), i.e., building a single system that is able to cope with different, yet related target domains. While DA has been investigated extensively in the last decade for various natural language processing (NLP) tasks, the examination of DA for RE is only very recent. To the best of our knowledge, there have been only three studies on DA for RE (Plank and Moschitti, 2013; Nguyen and Grishman, 2014; Nguyen et al., 2014). Of these, Nguyen et al. (2014) follow the supervised DA paradigm and assume some labeled data in the target domains. In contrast, Plank and Moschitti (2013) and Nguyen and Grishman ("
P15-1062,P05-1052,1,0.231135,"Missing"
P15-1062,C08-1088,0,0.0525648,"on the same resources and settings, to gain insights into which kind of system is more robust to domain changes. Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International J"
P15-1062,P05-1053,0,0.690766,". Our results and error analysis shows that the tree kernel-based method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, c Beijing, China, July 26-31, 2015. 2015 Associa"
P15-1062,D12-1110,0,0.0862588,"According to the principle of compositionality (Werning et al., 2006; Baroni and Zamparelli, 2010; Paperno et al., 2014), the meaning of a complex expression is determined by the meanings of its components and the rules to combine them. We study the following two compositionality embeddings for relation mentions that can be generated from the embeddings of the context words: PHRASE: Vi = the mean of the embeddings of the words contained in the PET tree Ti of Ri . Although this composition is simple, it is in fact competitive to the more complicated methods based on recursive neural networks (Socher et al., 2012b; Blacoe and Lapata, 2012; Sterckx et al., 2014) on representing phrase semantics. TREE: This is motivated by the training of recursive neural networks (Socher et al., 2012a) for semantic compositionality and attempts to aggregate the context words embeddings syntactically. In particular, we compute an embedding for every node in the PET tree in a bottom-up manner. The embeddings of the leaves are the embeddings of the words associated with them while the embeddings of the internal nodes are the means of the embeddings of their children nodes. We use the embeddings of the root of the PET tree"
P15-1062,P11-1053,1,0.953305,"ed method outperforms the feature-based approach. 1 Introduction Relation Extraction (RE) is an important aspect of information extraction that aims to discover the semantic relationships between two entity mentions appearing in the same sentence. Previous research on RE has followed either the kernelbased approach (Zelenko et al., 2003; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Zhang et al., 2006; Bunescu, 2007; Qian et al., 2008; Nguyen et al., 2009) or the feature-based approach (Kambhatla, 2004; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007a; Chan and Roth, 2010; Sun et al., 2011). Usually, in such supervised machine learning systems, it is assumed that the training data and the data to which the RE system is applied to are sampled independently and identically from the same distribution. This assumption is often violated in reality and exemplified in the fact that the performance 635 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 635–644, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics chitti (2013) consider the d"
P15-1062,P10-1040,0,0.610192,"e of research. Therefore, this is actually the first attempt to compare the two methods (tree kernel-based and feature-based) on the same settings. To ease the comparison for future work and circumvent the Zigglebottom pitfall (Pedersen, 2008), the entire setup and package is available.1 cation of word clusters and word embeddings for DA of RE on the feature-based method. Although word clusters (Brown et al., 1992) have been employed by both studies to improve the performance of relation extractors across domains, the application of word embeddings (Bengio et al., 2003; Mnih and Hinton, 2008; Turian et al., 2010) for DA of RE is only examined in the feature-based method and never explored in the tree kernelbased method so far, giving rise to the first question we want to address in this paper: (i) Can word embeddings help the tree kernelbased methods on DA for RE and more importantly, in which way can we do it effectively? This question is important as word embeddings are real valued vectors, while the tree kernel-based methods rely on the symbolic matches or mismatches of concrete labels in the parse trees to compute the kernels. It is unclear at the first glance how to encode word embeddings into th"
P15-1062,I08-2119,0,0.0156253,"ing between relation classes and their inverses) but Nguyen and Grishman (2014) disregard this relation direction. Finally, we note that although both studies evaluate their systems on the ACE 2005 dataset, they actually have different dataset partitions. In order to overcome this limitation, we conduct an evaluation in which the two methods are directed to use the same resources and settings, and are thus compared in a compatible manner to achieve an insight on their effectiveness for DA of RE. In fact, the problem of incompatible comparison is unfortunately very common in the RE literature (Wang, 2008; Plank and Moschitti, 2013) and we believe there is a need to tackle this increasing confusion in this line of research. Therefore, this is actually the first attempt to compare the two methods (tree kernel-based and feature-based) on the same settings. To ease the comparison for future work and circumvent the Zigglebottom pitfall (Pedersen, 2008), the entire setup and package is available.1 cation of word clusters and word embeddings for DA of RE on the feature-based method. Although word clusters (Brown et al., 1992) have been employed by both studies to improve the performance of relation"
P15-1062,J92-4003,0,\N,Missing
P15-1062,P14-1009,0,\N,Missing
P15-1062,D13-1170,0,\N,Missing
P15-1062,J08-3010,0,\N,Missing
P15-1165,P14-2131,0,0.0530581,"a mean vector to out-of-vocabulary words. System For our system, we simply augment the delexicalized POS tagger with the I NVERTED distributional representation of the current word. The best parameter setting on Spanish development data was σ = 0.01, δ = 160. 3.3 http://code.google.com/p/uni-dep-tb/ https://code.google.com/p/ wikily-supervised-pos-tagger/ 8 For our embeddings baselines, we augment the feature space by adding embedding vectors for head h and dependent d. We experimented with different versions of combining embedding vectors, from firing separate h and d per-dimension features (Bansal et al., 2014) to combining their information. We found that combining the embeddings of h and d is effective and consistently use the absolute difference between the embedding vectors, since that worked better than addition and multiplication on development data. Delexicalized transfer (D ELEX) uses three (3) iterations over the data in both the single-source and the multi-source set-up, a parameter set on the Spanish development data. The remaining parameters were obtained by averaging over performance with different embeddings on the Spanish development data, obtaining: σ = 0.005, δ = 20, i = 3, and abso"
P15-1165,P14-1023,0,0.0109756,"proaches focus on different kinds of similarity, some more syntactic, some more semantic. The representations are typically either clusters of distributionally similar words, e.g., Brown et al. (1992), or vector representations. In this paper, we focus on vector representations. In vector-based approaches, similar representations are vectors close in some multi-dimensional space. 2.1 Count-based and prediction-based representations There are, briefly put, two approaches to inducing vector-based distributional word representations from large corpora: count-based and predictionbased approaches (Baroni et al., 2014). Countbased approaches represent words by their cooccurrences. Dimensionality reduction is typically performed on a raw or weighted co-occurrence matrix using methods such as singular value decomposition (SVD), a method for maximizing the variance in a dataset in few dimensions. In our inverted indexing, we use raw co-occurrence data. Prediction-based methods use discriminative learning techniques to learn how to predict words from their context, or vice versa. They rely on a neural network architecture, and once the network converges, they use word representations from a middle layer as thei"
P15-1165,C10-1011,1,0.695318,"ing. For compatibility with Xiao and Guo (2014), we also present results on CoNLL 2006 and 2007 treebanks for languages for which we had baseline and system word representations (de, es, sv). Our parameter settings for these experiments were the same as those tuned on the Spanish development data from the Google Universal Treebanks v. 1.0. Baselines The most obvious baseline in our experiments is delexicalized transfer (D ELEX) (McDonald et al., 2011; Søgaard, 2011). This baseline system simply learns models without lexical features. We use a modified version of the first-order Mate 7 parser (Bohnet, 2010) that also takes continuousvalued embeddings as input an disregards features that include lexical items. 3.4 Word alignment Data We use the manually word-aligned EnglishSpanish Europarl data from Graca et al. (2008). The dataset contains 100 sentences. The annotators annotated whether word alignments were certain or possible, and we present results with all word alignments and with only the certain ones. See Graca et al. (2008) for details. Baselines For word alignment, we simply align every aligned word in the gold data, for which we have a word embedding, to its (Euclidean) nearest neighbor"
P15-1165,J92-4003,0,0.127536,"al and sparse models. Also, simple bagof-words models fail to capture the relatedness of words. In many tasks, synonymous words should be treated alike, but their bag-of-words representations are as different as those of dog and therefore. Distributional word representations are supposed to capture distributional similarities between words. Intuitively, we want similar words to have similar representations. Known approaches focus on different kinds of similarity, some more syntactic, some more semantic. The representations are typically either clusters of distributionally similar words, e.g., Brown et al. (1992), or vector representations. In this paper, we focus on vector representations. In vector-based approaches, similar representations are vectors close in some multi-dimensional space. 2.1 Count-based and prediction-based representations There are, briefly put, two approaches to inducing vector-based distributional word representations from large corpora: count-based and predictionbased approaches (Baroni et al., 2014). Countbased approaches represent words by their cooccurrences. Dimensionality reduction is typically performed on a raw or weighted co-occurrence matrix using methods such as sing"
P15-1165,W02-1001,0,0.0575931,"s well as tag dictionaries (Li et al., 2012) needed for the POS tagging experiments. Baselines One baseline method is a typeconstrained structured perceptron with only ortographic features, which are expected to transfer across languages. The type constraints come from Wiktionary, a crowd-sourced tag dictionary.8 Type constraints from Wiktionary were first used by Li et al. (2012), but note that their set-up is unsupervised learning. T¨ackstr¨om et al. (2013) also used type constraints in a supervised set-up. Our learning algorithm is the structured perceptron algorithm originally proposed by Collins (2002). In our POS tagging experiments, we always do 10 passes over the data. We also present two other baselines, where we augment the feature representation with different embeddings for the target word, K LEMENTIEV and C HANDAR. With all the embeddings in POS tagging, we assign a mean vector to out-of-vocabulary words. System For our system, we simply augment the delexicalized POS tagger with the I NVERTED distributional representation of the current word. The best parameter setting on Spanish development data was σ = 0.01, δ = 160. 3.3 http://code.google.com/p/uni-dep-tb/ https://code.google.com"
P15-1165,N15-1157,1,0.757356,"peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, but even better results were obtained with delexicalized transfer in McDonald et al. (2011) by simply using several source languages. This paper introduces a simple method for obtaining truly inter-lingual word representations in order to train models with lexical features on several source languages at the same time. Briefly put,"
P15-1165,graca-etal-2008-building,0,0.0195628,"tings for these experiments were the same as those tuned on the Spanish development data from the Google Universal Treebanks v. 1.0. Baselines The most obvious baseline in our experiments is delexicalized transfer (D ELEX) (McDonald et al., 2011; Søgaard, 2011). This baseline system simply learns models without lexical features. We use a modified version of the first-order Mate 7 parser (Bohnet, 2010) that also takes continuousvalued embeddings as input an disregards features that include lexical items. 3.4 Word alignment Data We use the manually word-aligned EnglishSpanish Europarl data from Graca et al. (2008). The dataset contains 100 sentences. The annotators annotated whether word alignments were certain or possible, and we present results with all word alignments and with only the certain ones. See Graca et al. (2008) for details. Baselines For word alignment, we simply align every aligned word in the gold data, for which we have a word embedding, to its (Euclidean) nearest neighbor in the target sentence. We evaluate this strategy by its precision (P@1). System We compare I NVERTED with K LEMEN TIEV and C HANDAR . To ensure a fair comparison, we use the subset of words covered by all three emb"
P15-1165,C12-1089,0,0.772355,"th rely on three level architectures with input, output and a middle layer for intermediate target word representations. The major difference is that skip-gram uses the target word as input and the context as output, whereas the CBOW model does it the other way around. Learning goes by back-propagation, and random target words are used as negative examples. Levy and Goldberg (2014) show that prediction-based representations obtained with the skip-gram model can be related to count-based ones obtained with PMI. They argue that which is best, varies across tasks. 2.1.2 Bilingual representations Klementiev et al. (2012) learn distinct embedding models for the source and target languages, but while learning to minimize the sum of the two models’ losses, they jointly learn a regularizing interaction matrix, enforcing word pairs aligned in parallel text to have similar representations. Note that Klementiev et al. (2012) rely on word-aligned parallel text, and thereby on a large-coverage soft mapping of source words to target words. Other approaches rely on small coverage dictionaries with hard 1:1 mappings between words. Klementiev et al. (2012) do not use skip-gram or CBOW, but the language model presented in"
P15-1165,P14-2050,0,0.00763126,"ed Table 1: Three nearest neighbors in the English training data of six words occurring in the Spanish test data, in the embeddings used in our experiments. Only 2/6 words were in the German data. skip-gram model and CBOW. The two models both rely on three level architectures with input, output and a middle layer for intermediate target word representations. The major difference is that skip-gram uses the target word as input and the context as output, whereas the CBOW model does it the other way around. Learning goes by back-propagation, and random target words are used as negative examples. Levy and Goldberg (2014) show that prediction-based representations obtained with the skip-gram model can be related to count-based ones obtained with PMI. They argue that which is best, varies across tasks. 2.1.2 Bilingual representations Klementiev et al. (2012) learn distinct embedding models for the source and target languages, but while learning to minimize the sum of the two models’ losses, they jointly learn a regularizing interaction matrix, enforcing word pairs aligned in parallel text to have similar representations. Note that Klementiev et al. (2012) rely on word-aligned parallel text, and thereby on a lar"
P15-1165,D12-1127,0,0.0369956,"Missing"
P15-1165,D11-1006,0,0.435697,"the availability of English resources and the availability of parallel data for (and translations between) English and most other languages. In cross-lingual syntactic parsing, for example, two approaches to cross-lingual learning have been explored, namely annotation projection and delexicalized transfer. Annotation projection (Hwa et al., 2005) uses word-alignments in human translations to project predicted sourceside analyses to the target language, producing a noisy syntactically annotated resource for the target language. On the other hand, delexicalized transfer (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao"
P15-1165,D09-1139,0,0.0521645,"Missing"
P15-1165,P10-1114,0,0.0276803,"on and word alignment, we fix the number of dimensions to 40. For both our baselines and systems, we also tune a scaling factor σ ∈ {1.0, 0.1, 0.01, 0.001} for POS tagging and dependency parsing, using the scaling method from Turian et al. (2010), also used in Gouws and Søgaard (2015). We do not scale our embeddings for document classification or word alignment. 3 Experiments The data set characteristics are found in Table 2.3. 3.1 Document classification Data Our first document classification task is topic classification on the cross-lingual multi-domain sentiment analysis dataset A MAZON in Prettenhofer and Stein (2010).4 We represent each document by the average of the representations of those words that we find both in the documents and in our embeddings. Rather than classifying reviews by sentiment, we classify by topic, trying to discriminate between book reviews, music reviews and DVD reviews, as a three-way classification problem, training on English and testing on German. Unlike in the other tasks below, we always 4 use unscaled word representations, since these are our only features. All word representations have 40 dimensions. The other document classification task is a fourway classification proble"
P15-1165,P11-1061,0,0.039505,"the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, but even better results were obtained with delexicalized transfer in McDonald et al. (2011) by simply using several source lan"
P15-1165,P11-2120,1,0.534532,"lish resources and the availability of parallel data for (and translations between) English and most other languages. In cross-lingual syntactic parsing, for example, two approaches to cross-lingual learning have been explored, namely annotation projection and delexicalized transfer. Annotation projection (Hwa et al., 2005) uses word-alignments in human translations to project predicted sourceside analyses to the target language, producing a noisy syntactically annotated resource for the target language. On the other hand, delexicalized transfer (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014)."
P15-1165,I05-1075,0,0.0207395,"reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, but even better results were obtained with delexicalized transfer in McDonald et al. (2011) by simply u"
P15-1165,Q13-1001,0,0.0450202,"Missing"
P15-1165,P10-1040,0,0.0800173,"NLL 07 – D EPENDENCY PARSING en es de sv 18.6 – – – 447k – – – en es – – – – – 206 357 389 – 5.7k 5.7k 5.7k – 0.841 0.616 n/a E UROPARL – W ORD A LIGNMENT 100 100 – – 0.370 0.533 Table 2: Characteristics of the data sets. Embeddings coverage (token-level) for K LEMENTIEV, C HAN DAR and I NVERTED on the test sets. We use the common vocabulary on W ORD A LIGNMENT . sification and word alignment, we fix the number of dimensions to 40. For both our baselines and systems, we also tune a scaling factor σ ∈ {1.0, 0.1, 0.01, 0.001} for POS tagging and dependency parsing, using the scaling method from Turian et al. (2010), also used in Gouws and Søgaard (2015). We do not scale our embeddings for document classification or word alignment. 3 Experiments The data set characteristics are found in Table 2.3. 3.1 Document classification Data Our first document classification task is topic classification on the cross-lingual multi-domain sentiment analysis dataset A MAZON in Prettenhofer and Stein (2010).4 We represent each document by the average of the representations of those words that we find both in the documents and in our embeddings. Rather than classifying reviews by sentiment, we classify by topic, trying t"
P15-1165,W14-1613,0,0.53136,"2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackstr¨om et al., 2013; Xiao and Guo, 2014). In cross-lingual POS tagging, mostly annotation projection has been explored (Fossum and Abney, 2005; Das and Petrov, 2011), since all features in POS tagging models are typically lexical. However, using bilingual word representations was recently explored as an alternative to projectionbased approaches (Gouws and Søgaard, 2015). The major drawback of using bi-lexical representations is that it limits us to using a single source language. T¨ackstr¨om et al. (2013) obtained significant improvements using bilingual word clusters over a single source delexicalized transfer model, for example, b"
P15-1165,I08-3008,0,0.017072,"asons for this; namely, the availability of English resources and the availability of parallel data for (and translations between) English and most other languages. In cross-lingual syntactic parsing, for example, two approaches to cross-lingual learning have been explored, namely annotation projection and delexicalized transfer. Annotation projection (Hwa et al., 2005) uses word-alignments in human translations to project predicted sourceside analyses to the target language, producing a noisy syntactically annotated resource for the target language. On the other hand, delexicalized transfer (Zeman and Resnik, 2008; McDonald et al., 2011; Søgaard, 2011) simply removes lexical features from mono-lingual parsing models, but assumes reliable POS tagging for the target language. Delexicalized transfer works particularly well when resources from several source languages are used for training; learning from multiple other languages prevents over-fitting to the peculiarities of the source language. Some authors have also combined annotation projection and delexicalized transfer, e.g., McDonald et al. (2011). Others have tried to augment delexicalized transfer models with bilingual word representations (T¨ackst"
P16-2067,Q16-1023,1,0.732171,"ry loss, being predictive of word frequency, helps to differentiate the representations of rare and common words. We indeed observe performance gains on rare and out-of-vocabulary words. These performance gains transfer into general improvements for morphologically rich languages. Introduction Recently, bidirectional long short-term memory networks (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) have been used for language modelling (Ling et al., 2015), POS tagging (Ling et al., 2015; Wang et al., 2015), transition-based dependency parsing (Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016), fine-grained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015). LSTMs are recurrent neural networks (RNNs) in which layers are designed to prevent vanishing gradients. Bidirectional LSTMs make a backward and forward pass through the sequence Contributions In this paper, we a) evaluate the effectiveness of different representations in biLSTMs, b) compare these models across a large set of languages and under varying conditions (data size, label noise) and c) propose a novel biLSTM model with auxiliary loss (L OGFREQ)"
P16-2067,D15-1041,0,0.302413,"model is that the auxiliary loss, being predictive of word frequency, helps to differentiate the representations of rare and common words. We indeed observe performance gains on rare and out-of-vocabulary words. These performance gains transfer into general improvements for morphologically rich languages. Introduction Recently, bidirectional long short-term memory networks (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) have been used for language modelling (Ling et al., 2015), POS tagging (Ling et al., 2015; Wang et al., 2015), transition-based dependency parsing (Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016), fine-grained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015). LSTMs are recurrent neural networks (RNNs) in which layers are designed to prevent vanishing gradients. Bidirectional LSTMs make a backward and forward pass through the sequence Contributions In this paper, we a) evaluate the effectiveness of different representations in biLSTMs, b) compare these models across a large set of languages and under varying conditions (data size, label noise) and c) propose a novel biLSTM mod"
P16-2067,D15-1025,0,0.0859971,"Missing"
P16-2067,A00-1031,0,0.175325,"ank that has the canonical language name (e.g., Finnish instead of Finnish-FTB). We consider all languages that have at least 60k tokens and are distributed with word forms, resulting in 22 languages. We also report accuracies on WSJ (45 POS) using the standard splits (Collins, 2002; Manning, 2011). The overview of languages is provided in Table 1. 3.2 FINE Semitic Slavic Slavic Germanic Germanic Germanic Romance Table 1: Grouping of languages. Taggers We want to compare POS taggers under varying conditions. We hence use three different types of taggers: our implementation of a bi-LSTM; T NT (Brants, 2000)—a second order HMM with suffix trie handling for OOVs. We use T NT as it was among the best performing taggers evaluated in Horsmann et al. (2015).3 We complement the NN-based and HMM-based tagger with a CRF tagger, using a freely available implementation (Plank et al., 2014) based on crfsuite. 3.1 COARSE non-IE Indoeuropean Indoeuropean Indoeuropean Indoeuropean Indoeuropean Indoeuropean Language isolate Indoeuropean non-IE Indoeuropean Results Our results are given in Table 2. First of all, notice that T N T performs remarkably well across the 22 languages, closely followed by CRF. The biLS"
P16-2067,D15-1085,0,0.0532663,"close to taggers using carefully designed feature templates. Ling et al. (2015) extend this line and compare a novel bi-LSTM model, learning word representations through character embeddings. They evaluate their model on a language modeling and POS tagging setup, and show that bi-LSTMs outperform the CNN approach of Santos and Zadrozny (2014). Similarly, Labeau et al. (2015) evaluate character embeddings for German. Bi-LSTMs for POS tagging are also reported in Wang et al. (2015), however, they only explore word embeddings, orthographic information and evaluate on WSJ only. A related study is Cheng et al. (2015) who propose a multi-task RNN for named entity recognition by jointly predicting the next token and current token’s name label. Our model is simpler, it uses a very coarse set of labels rather then integrating an entire language modeling task which is computationally more expensive. An interesting recent study is Gillick et al. (2016), they build a single byte-to-span model for multiple languages based on a sequence-to-sequence RNN (Sutskever et al., 2014) achieving impressive results. We would like to extend this work in their direction. Figure 3: Amount of training data (number of sentences)"
P16-2067,D15-1176,0,0.324201,"s the POS tagging loss function with an auxiliary loss function that accounts for rare words. The model obtains state-of-the-art performance across 22 languages, and works especially well for morphologically complex languages. Our analysis suggests that biLSTMs are less sensitive to training data size and label corruptions (at small noise levels) than previously assumed. 1 We consider using bi-LSTMs for POS tagging. Previous work on using deep learning-based methods for POS tagging has focused either on a single language (Collobert et al., 2011; Wang et al., 2015) or a small set of languages (Ling et al., 2015; Santos and Zadrozny, 2014). Instead we evaluate our models across 22 languages. In addition, we compare performance with representations at different levels of granularity (words, characters, and bytes). These levels of representation were previously introduced in different efforts (Chrupała, 2013; Zhang et al., 2015; Ling et al., 2015; Santos and Zadrozny, 2014; Gillick et al., 2016; Kim et al., 2015), but a comparative evaluation was missing. Moreover, deep networks are often said to require large volumes of training data. We investigate to what extent bi-LSTMs are more sensitive to the am"
P16-2067,D15-1168,0,0.0430138,"e the representations of rare and common words. We indeed observe performance gains on rare and out-of-vocabulary words. These performance gains transfer into general improvements for morphologically rich languages. Introduction Recently, bidirectional long short-term memory networks (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) have been used for language modelling (Ling et al., 2015), POS tagging (Ling et al., 2015; Wang et al., 2015), transition-based dependency parsing (Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016), fine-grained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015). LSTMs are recurrent neural networks (RNNs) in which layers are designed to prevent vanishing gradients. Bidirectional LSTMs make a backward and forward pass through the sequence Contributions In this paper, we a) evaluate the effectiveness of different representations in biLSTMs, b) compare these models across a large set of languages and under varying conditions (data size, label noise) and c) propose a novel biLSTM model with auxiliary loss (L OGFREQ). 412 Proceedings of the 54th Annual Meeting of the"
P16-2067,C14-1168,1,0.903295,"ollins, 2002; Manning, 2011). The overview of languages is provided in Table 1. 3.2 FINE Semitic Slavic Slavic Germanic Germanic Germanic Romance Table 1: Grouping of languages. Taggers We want to compare POS taggers under varying conditions. We hence use three different types of taggers: our implementation of a bi-LSTM; T NT (Brants, 2000)—a second order HMM with suffix trie handling for OOVs. We use T NT as it was among the best performing taggers evaluated in Horsmann et al. (2015).3 We complement the NN-based and HMM-based tagger with a CRF tagger, using a freely available implementation (Plank et al., 2014) based on crfsuite. 3.1 COARSE non-IE Indoeuropean Indoeuropean Indoeuropean Indoeuropean Indoeuropean Indoeuropean Language isolate Indoeuropean non-IE Indoeuropean Results Our results are given in Table 2. First of all, notice that T N T performs remarkably well across the 22 languages, closely followed by CRF. The biLSTM tagger (w) ~ without lower-level bi-LSTM for subtokens falls short, outperforms the traditional taggers only on 3 languages. The bi-LSTM Rare words In order to evaluate the effect of modeling sub-token information, we examine accuracy rates at different frequency rates. Fig"
P16-2067,P15-1109,0,0.0284485,"are and out-of-vocabulary words. These performance gains transfer into general improvements for morphologically rich languages. Introduction Recently, bidirectional long short-term memory networks (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) have been used for language modelling (Ling et al., 2015), POS tagging (Ling et al., 2015; Wang et al., 2015), transition-based dependency parsing (Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016), fine-grained sentiment analysis (Liu et al., 2015), syntactic chunking (Huang et al., 2015), and semantic role labeling (Zhou and Xu, 2015). LSTMs are recurrent neural networks (RNNs) in which layers are designed to prevent vanishing gradients. Bidirectional LSTMs make a backward and forward pass through the sequence Contributions In this paper, we a) evaluate the effectiveness of different representations in biLSTMs, b) compare these models across a large set of languages and under varying conditions (data size, label noise) and c) propose a novel biLSTM model with auxiliary loss (L OGFREQ). 412 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 412–418, c Berlin, Germany, August 7-12,"
P16-2067,W13-3520,0,\N,Missing
P16-2067,N16-1155,0,\N,Missing
P16-4027,pianta-etal-2008-textpro,0,0.0882451,"Missing"
P16-4027,agerri-etal-2014-ixa,0,0.0559234,"Missing"
P16-4027,P13-1147,1,0.764657,"ct opinion expressions, it uses a standard sequence labeler for subjective expression markup similar to the approach by (Breck et al., 2007). The system has been developed on the MPQA corpus that contains news articles. It internally uses the syntactic/semantic LTH dependency parser of (Johansson and Nugues, 2008). The opinion mining tool thus requires CoNLL-2008formatted data as input, as output by the parser, and as such needs pre-tokenized and tagged input. Relation Extraction. The relation extractor (RE) is a tree-kernel based system developed at the University of Trento (Moschitti, 2006; Plank and Moschitti, 2013). Tree kernel-based methods have been shown to outperform feature-based RE approach (Nguyen et al., 2015). The system takes as input the entity mentions detected by the EMD module (which provides information on the entity types, i.e. PERSON, LOCATION, ORGANIZATION or ENTITY). The first version of the relation extractor was trained on the ACE 2004 data. It provides the following binary relations as output: Physical, Personal/Social, Employment/Membership, PER/ORG Affiliation and GPE Affiliation. An extended version of the Relation Extractor includes an additional model trained on the CoNLL 2004"
P16-4027,W14-1605,1,0.760477,"l users that need high-performance standard tools at a zero engineering cost. The local version, on the contrary, requires some installation and configuration effort, but in return it offers a great flexibility in implementing and integrating user-specific modules. Since the beginning of the LiMoSINe project, the platform has been used for providing robust preprocessing for a variety of high-level tasks. Thus, we have recently shown how structural representations, extracted with our pipeline, improve multilingual opinion mining on YouTube (Severyn et al., 2015) or crossword puzzle resolution (Barlacchi et al., 2014). The pipeline has been adopted by other parties, most importantly by the joint QCRI and MIT project IYAS (Interactive sYstem for Answer Selection). IYAS focuses on Question Answering, showing that representations, based on linguistic preprocessing, significantly outperform more shallow methods (Tymoshenko and Moschitti, 2015; Tymoshenko et al., 2014). As part of the LiMoSINe project, we have created the LiMoSINe Common Corpus: a large collection of documents downloaded from different web resources 4 http://www.let.rug.nl/vannoord/alp/ Alpino/ 161 in any of the four addressed languages. These"
P16-4027,W04-2401,0,0.0741955,"kernel-based methods have been shown to outperform feature-based RE approach (Nguyen et al., 2015). The system takes as input the entity mentions detected by the EMD module (which provides information on the entity types, i.e. PERSON, LOCATION, ORGANIZATION or ENTITY). The first version of the relation extractor was trained on the ACE 2004 data. It provides the following binary relations as output: Physical, Personal/Social, Employment/Membership, PER/ORG Affiliation and GPE Affiliation. An extended version of the Relation Extractor includes an additional model trained on the CoNLL 2004 data (Roth and Yih, 2004) following the setup of Giuliano et al. (2007). The model uses a composite kernel consisting of a constituency-based path-enclosed tree kernel and a linear feature vector encoding local and global contexts (Giuliano et al., 2007). The CoNLL 2004 model contains the following relations: LiveIn, LocatedIn, WorkFor, OrgBasedIn, Kill. Both models exhibit state-of-the art performance. For the ACE 2004 data, experiments are reported in (Plank and Moschitti, 2013). For the CoNLL 2004 data, our model achieves results comparable to or advancing the state-of-the-art (Giuliano et al., 2007; Ghosh and Mure"
P16-4027,S10-1021,1,0.842758,"ns used by the English RE models. Coreference Resolution. A coreference model for BART has been trained on the Italian portion of the SemEval-2010 Task 1 dataset (Uryupina and Moschitti, 2014). Apart from retraining the model, we have incorporated some language-specific features to account, 3 160 http://www.di.unito.it/˜tutreeb/ for example, for abbreviation and aliasing patterns in Italian. The Italian version of BART, therefore, is a high-performance language-specific system. It has shown reliable performance at the recent shared tasks for Italian, in particular, at the SemEval-2010 Task 1 (Broscheit et al., 2010) and at the EvalIta 2009 (Biggio et al., 2009). Both our English and Italian coreference modules are based on BART. Their configurations (parameter settings and features) have been optimized separately to enhance the performance level on a specific language. Since BART is a highly modular toolkit itself and its language-specific functionality can be controlled via a Language Plugin, no extra BART installation is required to run the Italian coreference resolver. 3.3 Spanish We have tested two publicly available toolkits supporting language processing in Spanish: OpenNLP and IXA (Agerri et al.,"
P16-4027,E14-1070,1,0.83486,"robust preprocessing for a variety of high-level tasks. Thus, we have recently shown how structural representations, extracted with our pipeline, improve multilingual opinion mining on YouTube (Severyn et al., 2015) or crossword puzzle resolution (Barlacchi et al., 2014). The pipeline has been adopted by other parties, most importantly by the joint QCRI and MIT project IYAS (Interactive sYstem for Answer Selection). IYAS focuses on Question Answering, showing that representations, based on linguistic preprocessing, significantly outperform more shallow methods (Tymoshenko and Moschitti, 2015; Tymoshenko et al., 2014). As part of the LiMoSINe project, we have created the LiMoSINe Common Corpus: a large collection of documents downloaded from different web resources 4 http://www.let.rug.nl/vannoord/alp/ Alpino/ 161 in any of the four addressed languages. These data were annotated automatically. We illustrate the processing capabilities of our pipeline on the Spanish part of the corpus (EsLCC). To this end, we developed a UIMA Collection Processing Engine (CPE). Once the EsLCC was downloaded it was first tidied up with Apache Tika. The pipeline was then applied to clean text. It was capable of processing the"
P16-4027,I13-1012,1,0.859539,"), covering a wide variety of mentions, has been developed at the University of Trento as a part of BART (see below). A more recent version has been proposed for the CoNLL-2011/2012 Shared Tasks (Uryupina et al., 2011; Uryupina et al., 2012). It is a rule-based system that combines the outputs of a parser and an NE-tagger to extract mention boundaries (both full and minimal nominal spans) and assign mention types (name, nominal or pronoun) and semantic classes (inferred from WordNet for common nouns, from NER labels for proper nouns). We are currently planning to integrate learning-based EMD (Uryupina and Moschitti, 2013) to cover additional languages, in particular, Arabic. Opinion Mining. The opinion expression annotator is a system developed at the University of Trento by Johansson and Moschitti (2011). It extracts fine-grained opinion expressions together with their polarity. To extract opinion expressions, it uses a standard sequence labeler for subjective expression markup similar to the approach by (Breck et al., 2007). The system has been developed on the MPQA corpus that contains news articles. It internally uses the syntactic/semantic LTH dependency parser of (Johansson and Nugues, 2008). The opinion"
P16-4027,W11-1908,1,0.905436,"Missing"
P16-4027,C12-2039,0,0.0196184,"and Yih, 2004) following the setup of Giuliano et al. (2007). The model uses a composite kernel consisting of a constituency-based path-enclosed tree kernel and a linear feature vector encoding local and global contexts (Giuliano et al., 2007). The CoNLL 2004 model contains the following relations: LiveIn, LocatedIn, WorkFor, OrgBasedIn, Kill. Both models exhibit state-of-the art performance. For the ACE 2004 data, experiments are reported in (Plank and Moschitti, 2013). For the CoNLL 2004 data, our model achieves results comparable to or advancing the state-of-the-art (Giuliano et al., 2007; Ghosh and Muresan, 2012). Coreference Resolution. Our coreference resolution Analysis Engine is a wrapper around BART—a toolkit for Coreference Resolution developed at the University of Trento (Versley et al., 2008; Uryupina et al., 2012). It is a modular anaphora resolution system that supports state-of-the-art statistical approaches to the task and enables efficient feature engineering. BART implements several models of anaphora resolution (mentionpair and entity-mention; best-first vs. ranking), has interfaces to different machine learners (MaxEnt, SVM, decision trees) and provides a large set of linguistically mo"
P16-4027,P11-2018,1,0.836762,"ed Tasks (Uryupina et al., 2011; Uryupina et al., 2012). It is a rule-based system that combines the outputs of a parser and an NE-tagger to extract mention boundaries (both full and minimal nominal spans) and assign mention types (name, nominal or pronoun) and semantic classes (inferred from WordNet for common nouns, from NER labels for proper nouns). We are currently planning to integrate learning-based EMD (Uryupina and Moschitti, 2013) to cover additional languages, in particular, Arabic. Opinion Mining. The opinion expression annotator is a system developed at the University of Trento by Johansson and Moschitti (2011). It extracts fine-grained opinion expressions together with their polarity. To extract opinion expressions, it uses a standard sequence labeler for subjective expression markup similar to the approach by (Breck et al., 2007). The system has been developed on the MPQA corpus that contains news articles. It internally uses the syntactic/semantic LTH dependency parser of (Johansson and Nugues, 2008). The opinion mining tool thus requires CoNLL-2008formatted data as input, as output by the parser, and as such needs pre-tokenized and tagged input. Relation Extraction. The relation extractor (RE) i"
P16-4027,D08-1008,0,0.0171416,"based EMD (Uryupina and Moschitti, 2013) to cover additional languages, in particular, Arabic. Opinion Mining. The opinion expression annotator is a system developed at the University of Trento by Johansson and Moschitti (2011). It extracts fine-grained opinion expressions together with their polarity. To extract opinion expressions, it uses a standard sequence labeler for subjective expression markup similar to the approach by (Breck et al., 2007). The system has been developed on the MPQA corpus that contains news articles. It internally uses the syntactic/semantic LTH dependency parser of (Johansson and Nugues, 2008). The opinion mining tool thus requires CoNLL-2008formatted data as input, as output by the parser, and as such needs pre-tokenized and tagged input. Relation Extraction. The relation extractor (RE) is a tree-kernel based system developed at the University of Trento (Moschitti, 2006; Plank and Moschitti, 2013). Tree kernel-based methods have been shown to outperform feature-based RE approach (Nguyen et al., 2015). The system takes as input the entity mentions detected by the EMD module (which provides information on the entity types, i.e. PERSON, LOCATION, ORGANIZATION or ENTITY). The first ve"
P16-4027,W12-4515,1,0.938727,"ver and relation extractor require information on mentions—textual units that correspond to real-world objects. Even though some studies focus on specific subtypes of mentions (for example, on pronominal coreference or on relations between named entities), we believe that a reliable pipeline should provide information on all the possible mentions. An entity mention detector (EMD), covering a wide variety of mentions, has been developed at the University of Trento as a part of BART (see below). A more recent version has been proposed for the CoNLL-2011/2012 Shared Tasks (Uryupina et al., 2011; Uryupina et al., 2012). It is a rule-based system that combines the outputs of a parser and an NE-tagger to extract mention boundaries (both full and minimal nominal spans) and assign mention types (name, nominal or pronoun) and semantic classes (inferred from WordNet for common nouns, from NER labels for proper nouns). We are currently planning to integrate learning-based EMD (Uryupina and Moschitti, 2013) to cover additional languages, in particular, Arabic. Opinion Mining. The opinion expression annotator is a system developed at the University of Trento by Johansson and Moschitti (2011). It extracts fine-graine"
P16-4027,P08-4003,1,0.782988,"local and global contexts (Giuliano et al., 2007). The CoNLL 2004 model contains the following relations: LiveIn, LocatedIn, WorkFor, OrgBasedIn, Kill. Both models exhibit state-of-the art performance. For the ACE 2004 data, experiments are reported in (Plank and Moschitti, 2013). For the CoNLL 2004 data, our model achieves results comparable to or advancing the state-of-the-art (Giuliano et al., 2007; Ghosh and Muresan, 2012). Coreference Resolution. Our coreference resolution Analysis Engine is a wrapper around BART—a toolkit for Coreference Resolution developed at the University of Trento (Versley et al., 2008; Uryupina et al., 2012). It is a modular anaphora resolution system that supports state-of-the-art statistical approaches to the task and enables efficient feature engineering. BART implements several models of anaphora resolution (mentionpair and entity-mention; best-first vs. ranking), has interfaces to different machine learners (MaxEnt, SVM, decision trees) and provides a large set of linguistically motivated features, along with the possibility to design new ones. Entity Linking. The Entity Linking Analysis Engine (“Semanticizer”) makes use of the Entity Linking Web Service developed by"
P16-4027,P14-5010,0,\N,Missing
P16-4027,P15-1062,1,\N,Missing
P16-4027,padro-stanilovsky-2012-freeling,0,\N,Missing
P18-1096,P07-1056,0,0.582094,"ritraining with disagreement is only slightly better than self-training, showing that the disagreement component might not be useful when there is a strong domain shift. Tri-training achieves the best average results on two target domains and clearly outperforms the state of the art on average. MT-Tri finally outperforms the state of the art on 3/4 domains, and even slightly traditional tritraining, resulting in the overall best method. This improvement is mainly due to the B-&gt;E and D-&gt;E scenarios, on which tri-training struggles. These domain pairs are among those with the highest Adistance (Blitzer et al., 2007), which highlights that tri-training has difficulty dealing with a strong shift in domain. Our method is able to mitigate this deficiency by training one of the three output layers only on pseudo-labeled target domain examples. In addition, MT-Tri is more efficient as it adds a smaller number of pseudo-labeled examples than tri-training at every epoch. For sentiment analysis, tri-training adds around 1800-1950/2000 unlabeled examples at every epoch, while MT-Tri only adds around 100-300 in early epochs. This shows that the orthogonality constraint is useful for inducing diversity. In addition,"
P18-1096,W06-1615,0,0.897186,"eural networks (DNNs) excel at learning from labeled data and have achieved state of the art in a wide array of supervised NLP tasks such as dependency parsing (Dozat and Manning, 2017), named entity recognition (Lample et al., 2016), and semantic role labeling (He et al., 2017). In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words (Blitzer et al., 2006; Wu and Huang, 2016) or distributional features (Schnabel and Schütze, 2014; Yin et al., 2015) which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary datasets (Kim et al., 2017) or on a single benchmark (Zhou et al., 2016), which carries the risk of overfitting to the task. In addition, most models only compare against weak baselines and, strikingly, almost none considers evaluating against approaches from the extensive semi-supervised learning (SSL) literature (Chapelle et al., 2006). In this work, we make the argument that such"
P18-1096,D15-1085,0,0.0373485,"Missing"
P18-1096,P13-1004,0,0.027757,"the line between “explicit” and “implicit” ensembling (Huang et al., 2017), like dropout (Srivastava et al., 2014) or temporal ensembling (Saito et al., 2017), is more fuzzy. As we noted earlier our multi-task learning setup can be seen as a form of self-ensembling. Multi-task learning in NLP Neural networks are particularly well-suited for MTL allowing for parameter sharing (Caruana, 1993). Recent NLP conferences witnessed a “tsunami” of deep learning papers (Manning, 2015), followed by what we call a multi-task learning “wave”: MTL has been successfully applied to a wide range of NLP tasks (Cohn and Specia, 2013; Cheng et al., 2015; Luong et al., 2015; Plank et al., 2016; Fang and Cohn, 2016; Søgaard and Goldberg, 2016; Ruder et al., 2017; Augenstein et al., 2018). Related to it is the pioneering work on adversarial learning (DANN) (Ganin et al., 2016). For sentiment analysis we found tri-training and our MT-Tri model to outperform DANN. Our MT-Tri model lends itself well to shared-private models such as those proposed recently (Liu et al., 2017; Kim et al., 2017), which extend upon (Ganin et al., 2016) by having separate source and target-specific encoders. 5 Conclusions We re-evaluate a range of tr"
P18-1096,W17-3203,0,0.0455565,"Missing"
P18-1096,K16-1018,0,0.0189065,"opout (Srivastava et al., 2014) or temporal ensembling (Saito et al., 2017), is more fuzzy. As we noted earlier our multi-task learning setup can be seen as a form of self-ensembling. Multi-task learning in NLP Neural networks are particularly well-suited for MTL allowing for parameter sharing (Caruana, 1993). Recent NLP conferences witnessed a “tsunami” of deep learning papers (Manning, 2015), followed by what we call a multi-task learning “wave”: MTL has been successfully applied to a wide range of NLP tasks (Cohn and Specia, 2013; Cheng et al., 2015; Luong et al., 2015; Plank et al., 2016; Fang and Cohn, 2016; Søgaard and Goldberg, 2016; Ruder et al., 2017; Augenstein et al., 2018). Related to it is the pioneering work on adversarial learning (DANN) (Ganin et al., 2016). For sentiment analysis we found tri-training and our MT-Tri model to outperform DANN. Our MT-Tri model lends itself well to shared-private models such as those proposed recently (Liu et al., 2017; Kim et al., 2017), which extend upon (Ganin et al., 2016) by having separate source and target-specific encoders. 5 Conclusions We re-evaluate a range of traditional generalpurpose bootstrapping algorithms in the context of neural networ"
P18-1096,P17-2093,0,0.0213515,"redictions. In contrast, if the parameters in all output softmax layers were the same, the method would degenerate to self-training. To guarantee diversity, we introduce an orthogonality constraint (Bousmalis et al., 2016) as an additional loss term, which we define as follows: &gt; Lorth = kWm Wm2 k2F 1 (1) where |· k2F is the squared Frobenius norm and Wm1 and Wm2 are the softmax output parameters 2 Note: we use the term multi-task learning here albeit all tasks are of the same kind, similar to work on multi-lingual modeling treating each language (but same label space) as separate task e.g., (Fang and Cohn, 2017). It is interesting to point out that our model is further doing implicit multi-view learning by way of the orthogonality constraint. 1046 of the two source and pseudo-labeled output layers m1 and m2 , respectively. The orthogonality constraint encourages the models not to rely on the same features for prediction. As enforcing pairwise orthogonality between three matrices is not possible, we only enforce orthogonality between the softmax output layers of m1 and m2 ,3 while m3 is gradually trained to be more target-specific. We parameterize Lorth by γ=0.01 following (Liu et al., 2017). We do no"
P18-1096,W17-4404,1,0.881914,"Missing"
P18-1096,P17-1044,0,0.0265255,"a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state of the art. We conclude that classic approaches constitute an important and strong baseline. 1 Introduction Deep neural networks (DNNs) excel at learning from labeled data and have achieved state of the art in a wide array of supervised NLP tasks such as dependency parsing (Dozat and Manning, 2017), named entity recognition (Lample et al., 2016), and semantic role labeling (He et al., 2017). In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words (Blitzer et al., 2006; Wu and Huang, 2016) or distributional features (Schnabel and Schütze, 2014; Yin et al., 2015) which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary datasets (Kim et al., 2017) or on a single be"
P18-1096,P18-1031,1,0.801555,"udo-labeled target domain examples. In addition, MT-Tri is more efficient as it adds a smaller number of pseudo-labeled examples than tri-training at every epoch. For sentiment analysis, tri-training adds around 1800-1950/2000 unlabeled examples at every epoch, while MT-Tri only adds around 100-300 in early epochs. This shows that the orthogonality constraint is useful for inducing diversity. In addition, adding fewer examples poses a smaller risk of swamping the learned representations with useless signals and is more akin to fine-tuning, the standard method for supervised domain adaptation (Howard and Ruder, 2018). We observe an asymmetry in the results between some of the domain pairs, e.g. B-&gt;D and D-&gt;B. We hypothesize that the asymmetry may be due to properties of the data and that the domains are relatively far apart e.g., in terms of A-distance. In fact, asymmetry in these domains is already reflected 1049 Model Src (+glove) Self Tri Tri-D Asym MT-Tri FLORS Target domains Newsgroups Reviews ep Answers Emails Weblogs Avg WSJ (5) (4) (7) (3) (4) 87.63 ±.37 87.64 ±.18 88.42 ±.16 88.50 ±.04 87.81 ±.19 87.92 ±.18 86.49 ±.35 86.58 ±.30 87.46 ±.20 87.63 ±.15 86.97 ±.17 87.20 ±.23 88.60 ±.22 88.42 ±.24 87"
P18-1096,P07-1034,0,0.128543,"method works well on unknown word-tag combinations. UWT tokens are very difficult to predict correctly using an unsupervised approach; the less lexicalized and more context-driven approach taken by FLORS is clearly superior for these cases, resulting in higher UWT accuracies for 4/5 domains. 4 Related work Learning under Domain Shift There is a large body of work on domain adaptation. Studies on unsupervised domain adaptation include early work on bootstrapping (Steedman et al., 2003; McClosky et al., 2006a), shared feature representations (Blitzer et al., 2006, 2007) and instance weighting (Jiang and Zhai, 2007). Recent ap1051 proaches include adversarial learning (Ganin et al., 2016) and fine-tuning (Sennrich et al., 2016). There is almost no work on bootstrapping approaches for recent neural NLP, in particular under domain shift. Tri-training is less studied, and only recently re-emerged in the vision community (Saito et al., 2017), albeit is not compared to classic tri-training. Neural network ensembling Related work on self-ensembling approaches includes snapshot ensembling (Huang et al., 2017) or temporal ensembling (Laine and Aila, 2017). In general, the line between “explicit” and “implicit” e"
P18-1096,P17-1119,0,0.197404,"tic role labeling (He et al., 2017). In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words (Blitzer et al., 2006; Wu and Huang, 2016) or distributional features (Schnabel and Schütze, 2014; Yin et al., 2015) which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary datasets (Kim et al., 2017) or on a single benchmark (Zhou et al., 2016), which carries the risk of overfitting to the task. In addition, most models only compare against weak baselines and, strikingly, almost none considers evaluating against approaches from the extensive semi-supervised learning (SSL) literature (Chapelle et al., 2006). In this work, we make the argument that such algorithms make strong baselines for any task in line with recent efforts highlighting the usefulness of classic approaches (Melis et al., 2017; Denkowski and Neubig, 2017). We re-evaluate bootstrapping algorithms in the context of DNNs. The"
P18-1096,Q16-1023,0,0.0145505,"g some languagespecific morphological features). We want to gauge to what extent we can adopt a nowadays fairly standard (but more lexicalized) general neural tagger. Our POS tagging model is a state-of-the-art Bi-LSTM tagger (Plank et al., 2016) with word and 100-dim character embeddings. Word embeddings are initialized with the 100-dim Glove embeddings (Pennington et al., 2014). The BiLSTM has one hidden layer with 100 dimensions. The base POS model is trained on WSJ with early stopping on the WSJ development set, using patience 2, Gaussian noise with σ = 0.2 and word dropout with p = 0.25 (Kiperwasser and Goldberg, 2016). Regarding data, the source domain is the Ontonotes 4.0 release of the Penn treebank Wall Street Journal (WSJ) annotated for 48 fine-grained POS tags. This amounts to 30,060 labeled sentences. We use 100,000 WSJ sentences from 1988 as unlabeled data, following Schnabel and Schütze (2014).5 As target data, we use the five SANCL domains (answers, emails, newsgroups, reviews, weblogs). We restrict the amount of unlabeled data for each SANCL domain to the first 100k sentences, and do not do any pre-processing. We consider the development set of A NSWERS as our only target dev set to set hyperpara"
P18-1096,N16-1030,0,0.0574276,"are negative: while our novel method establishes a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state of the art. We conclude that classic approaches constitute an important and strong baseline. 1 Introduction Deep neural networks (DNNs) excel at learning from labeled data and have achieved state of the art in a wide array of supervised NLP tasks such as dependency parsing (Dozat and Manning, 2017), named entity recognition (Lample et al., 2016), and semantic role labeling (He et al., 2017). In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words (Blitzer et al., 2006; Wu and Huang, 2016) or distributional features (Schnabel and Schütze, 2014; Yin et al., 2015) which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary"
P18-1096,P17-1001,0,0.136431,"e.g., (Fang and Cohn, 2017). It is interesting to point out that our model is further doing implicit multi-view learning by way of the orthogonality constraint. 1046 of the two source and pseudo-labeled output layers m1 and m2 , respectively. The orthogonality constraint encourages the models not to rely on the same features for prediction. As enforcing pairwise orthogonality between three matrices is not possible, we only enforce orthogonality between the softmax output layers of m1 and m2 ,3 while m3 is gradually trained to be more target-specific. We parameterize Lorth by γ=0.01 following (Liu et al., 2017). We do not further tune γ. More formally, let us illustrate the model by taking the sequence prediction task (Figure 1) as illustration. Given an utterance with labels y1 , .., yn , our Multi-task Tri-training loss consists of three task-specific (m1 , m2 , m3 ) tagging loss functions (where ~h is the uppermost Bi-LSTM encoding): XX L(θ) = − log Pmi (y|~h) + γLorth (2) Algorithm 3 Multi-task Tri-training 1: 2: 3: 4: 5: 6: 7: m ← train_model(L) repeat for i ∈ {1..3} do Li ← ∅ for x ∈ U do if pj (x) = pk (x)(j, k 6= i) then Li ← Li ∪ {(x, pj (x))} if i = 3 then mi = train_model(Li ) elsemi ← tr"
P18-1096,N06-1020,0,0.877312,"gorithms compared to state-of-the-art approaches on two benchmark datasets. d) We shed light on the task and data characteristics that yield the best performance for each model. 2 Neural bootstrapping methods We first introduce three classic bootstrapping methods, self-training, tri-training, and tri-training with disagreement and detail how they can be used with neural networks. For in-depth details we refer the reader to (Abney, 2007; Chapelle et al., 2006; Zhu and Goldberg, 2009). We introduce our novel multitask tri-training method in §2.3. 2.1 Self-training Self-training (Yarowsky, 1995; McClosky et al., 2006b) is one of the earliest and simplest bootstrapping approaches. In essence, it leverages the model’s own predictions on unlabeled data to obtain additional information that can be used during training. Typically the most confident predictions are taken at face value, as detailed next. Self-training trains a model m on a labeled training set L and an unlabeled data set U . At each iteration, the model provides predictions m(x) in the form of a probability distribution over classes for all unlabeled examples x in U . If the probability assigned to the most likely class is higher than a predeter"
P18-1096,P06-1043,0,0.82215,"gorithms compared to state-of-the-art approaches on two benchmark datasets. d) We shed light on the task and data characteristics that yield the best performance for each model. 2 Neural bootstrapping methods We first introduce three classic bootstrapping methods, self-training, tri-training, and tri-training with disagreement and detail how they can be used with neural networks. For in-depth details we refer the reader to (Abney, 2007; Chapelle et al., 2006; Zhu and Goldberg, 2009). We introduce our novel multitask tri-training method in §2.3. 2.1 Self-training Self-training (Yarowsky, 1995; McClosky et al., 2006b) is one of the earliest and simplest bootstrapping approaches. In essence, it leverages the model’s own predictions on unlabeled data to obtain additional information that can be used during training. Typically the most confident predictions are taken at face value, as detailed next. Self-training trains a model m on a labeled training set L and an unlabeled data set U . At each iteration, the model provides predictions m(x) in the form of a probability distribution over classes for all unlabeled examples x in U . If the probability assigned to the most likely class is higher than a predeter"
P18-1096,D14-1162,0,0.0860106,"Missing"
P18-1096,P16-2067,1,0.93714,"ov and McDonald, 2012) and compare to the top results in both low and high-data conditions (Schnabel and Schütze, 2014; Yin et al., 2015). Both are strong baselines, as the FLORS tagger has been developed for this challenging dataset and it is based on contextual distributional features (excluding the word’s identity), and hand-crafted suffix and shape features (including some languagespecific morphological features). We want to gauge to what extent we can adopt a nowadays fairly standard (but more lexicalized) general neural tagger. Our POS tagging model is a state-of-the-art Bi-LSTM tagger (Plank et al., 2016) with word and 100-dim character embeddings. Word embeddings are initialized with the 100-dim Glove embeddings (Pennington et al., 2014). The BiLSTM has one hidden layer with 100 dimensions. The base POS model is trained on WSJ with early stopping on the WSJ development set, using patience 2, Gaussian noise with σ = 0.2 and word dropout with p = 0.25 (Kiperwasser and Goldberg, 2016). Regarding data, the source domain is the Ontonotes 4.0 release of the Penn treebank Wall Street Journal (WSJ) annotated for 48 fine-grained POS tags. This amounts to 30,060 labeled sentences. We use 100,000 WSJ se"
P18-1096,P07-1078,0,0.152605,"the highest confidence after every epoch and add them to the labeled data. This is one of the many variants for self-training, called throttling (Abney, 2007). We empirically confirm that this outperforms the classic selection in our experiments. Online learning In contrast to many classic algorithms, DNNs are trained online by default. We compare training setups and find that training until convergence on labeled data and then training until convergence using self-training performs best. Classic self-training has shown mixed success. In parsing it proved successful only with small datasets (Reichart and Rappoport, 2007) or when a generative component is used together with a reranker in high-data conditions (McClosky et al., 2006b; Suzuki and Isozaki, 2008). Some success was achieved with careful task-specific data selection (Petrov and McDonald, 2012), while others report limited success on a variety of NLP tasks (Plank, 2011; Van Asch and Daelemans, 2016; van der Goot et al., 2017). Its main downside is that the model is not able to correct its own mistakes and errors are amplified, an effect that is increased under domain shift. 2.2 Tri-training Tri-training (Zhou and Li, 2005) is a classic method that red"
P18-1096,D17-1035,0,0.0192184,"utoencoder (VFAE) (Louizos et al., 2015) model and domain-adversarial neural networks (DANN) (Ganin et al., 2016). 3.3 Baselines Besides comparing to the top results published on both datasets, we include the following baselines: a) b) c) d) e) the task model trained on the source domain; self-training (Self); tri-training (Tri); tri-training with disagreement (Tri-D); and asymmetric tri-training (Saito et al., 2017). Our proposed model is multi-task tri-training (MTTri). We implement our models in DyNet (Neubig et al., 2017). Reporting single evaluation scores might result in biased results (Reimers and Gurevych, 2017). Throughout the paper, we report mean accuracy and standard deviation over five runs for POS tagging and over ten runs for 5 Note that our unlabeled data might slightly differ from theirs. We took the first 100k sentences from the 1988 WSJ dataset from the BLLIP 1987-89 WSJ Corpus Release 1. 1048 Figure 2: Average results for unsupervised domain adaptation on the Amazon dataset. Domains: B (Book), D (DVD), E (Electronics), K (Kitchen). Results for VFAE, DANN, and Asym are from Saito et al. (2017). sentiment analysis. Significance is computed using bootstrap test. The code for all experiments"
P18-1096,Q14-1002,0,0.134351,"ieved state of the art in a wide array of supervised NLP tasks such as dependency parsing (Dozat and Manning, 2017), named entity recognition (Lample et al., 2016), and semantic role labeling (He et al., 2017). In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words (Blitzer et al., 2006; Wu and Huang, 2016) or distributional features (Schnabel and Schütze, 2014; Yin et al., 2015) which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary datasets (Kim et al., 2017) or on a single benchmark (Zhou et al., 2016), which carries the risk of overfitting to the task. In addition, most models only compare against weak baselines and, strikingly, almost none considers evaluating against approaches from the extensive semi-supervised learning (SSL) literature (Chapelle et al., 2006). In this work, we make the argument that such algorithms make strong baselines for any task in line with recent efforts hi"
P18-1096,P16-1009,0,0.0353828,"unsupervised approach; the less lexicalized and more context-driven approach taken by FLORS is clearly superior for these cases, resulting in higher UWT accuracies for 4/5 domains. 4 Related work Learning under Domain Shift There is a large body of work on domain adaptation. Studies on unsupervised domain adaptation include early work on bootstrapping (Steedman et al., 2003; McClosky et al., 2006a), shared feature representations (Blitzer et al., 2006, 2007) and instance weighting (Jiang and Zhai, 2007). Recent ap1051 proaches include adversarial learning (Ganin et al., 2016) and fine-tuning (Sennrich et al., 2016). There is almost no work on bootstrapping approaches for recent neural NLP, in particular under domain shift. Tri-training is less studied, and only recently re-emerged in the vision community (Saito et al., 2017), albeit is not compared to classic tri-training. Neural network ensembling Related work on self-ensembling approaches includes snapshot ensembling (Huang et al., 2017) or temporal ensembling (Laine and Aila, 2017). In general, the line between “explicit” and “implicit” ensembling (Huang et al., 2017), like dropout (Srivastava et al., 2014) or temporal ensembling (Saito et al., 2017)"
P18-1096,P10-2038,0,0.80088,"Neubig, 2017). We re-evaluate bootstrapping algorithms in the context of DNNs. These are general-purpose semi-supervised algorithms that treat the model as a black box and can thus be used easily—with a few additions—with the current generation of NLP models. Many of these methods, though, were originally developed with in-domain performance in mind, so their effectiveness in a domain adaptation setting remains unexplored. In particular, we re-evaluate three traditional bootstrapping methods, self-training (Yarowsky, 1995), tri-training (Zhou and Li, 2005), and tritraining with disagreement (Søgaard, 2010) for neural network-based approaches on two NLP tasks with different characteristics, namely, a sequence prediction and a classification task (POS tagging and sentiment analysis). We evaluate the methods across multiple domains on two wellestablished benchmarks, without taking any further task-specific measures, and compare to the best results published in the literature. We make the somewhat surprising observation that classic tri-training outperforms task-agnostic state-of-the-art semi-supervised learning (Laine and Aila, 2017) and recent neural adaptation approaches (Ganin et al., 2016; Sai"
P18-1096,P16-2038,0,0.0366655,"al., 2014) or temporal ensembling (Saito et al., 2017), is more fuzzy. As we noted earlier our multi-task learning setup can be seen as a form of self-ensembling. Multi-task learning in NLP Neural networks are particularly well-suited for MTL allowing for parameter sharing (Caruana, 1993). Recent NLP conferences witnessed a “tsunami” of deep learning papers (Manning, 2015), followed by what we call a multi-task learning “wave”: MTL has been successfully applied to a wide range of NLP tasks (Cohn and Specia, 2013; Cheng et al., 2015; Luong et al., 2015; Plank et al., 2016; Fang and Cohn, 2016; Søgaard and Goldberg, 2016; Ruder et al., 2017; Augenstein et al., 2018). Related to it is the pioneering work on adversarial learning (DANN) (Ganin et al., 2016). For sentiment analysis we found tri-training and our MT-Tri model to outperform DANN. Our MT-Tri model lends itself well to shared-private models such as those proposed recently (Liu et al., 2017; Kim et al., 2017), which extend upon (Ganin et al., 2016) by having separate source and target-specific encoders. 5 Conclusions We re-evaluate a range of traditional generalpurpose bootstrapping algorithms in the context of neural network approaches to semi-supervi"
P18-1096,N03-1031,0,0.400547,"Missing"
P18-1096,P08-1076,0,0.036247,"ng (Abney, 2007). We empirically confirm that this outperforms the classic selection in our experiments. Online learning In contrast to many classic algorithms, DNNs are trained online by default. We compare training setups and find that training until convergence on labeled data and then training until convergence using self-training performs best. Classic self-training has shown mixed success. In parsing it proved successful only with small datasets (Reichart and Rappoport, 2007) or when a generative component is used together with a reranker in high-data conditions (McClosky et al., 2006b; Suzuki and Isozaki, 2008). Some success was achieved with careful task-specific data selection (Petrov and McDonald, 2012), while others report limited success on a variety of NLP tasks (Plank, 2011; Van Asch and Daelemans, 2016; van der Goot et al., 2017). Its main downside is that the model is not able to correct its own mistakes and errors are amplified, an effect that is increased under domain shift. 2.2 Tri-training Tri-training (Zhou and Li, 2005) is a classic method that reduces the bias of predictions on unlabeled data by utilizing the agreement of three independently trained models. Tri-training (cf. Algorith"
P18-1096,P16-1029,0,0.0743195,"excel at learning from labeled data and have achieved state of the art in a wide array of supervised NLP tasks such as dependency parsing (Dozat and Manning, 2017), named entity recognition (Lample et al., 2016), and semantic role labeling (He et al., 2017). In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words (Blitzer et al., 2006; Wu and Huang, 2016) or distributional features (Schnabel and Schütze, 2014; Yin et al., 2015) which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary datasets (Kim et al., 2017) or on a single benchmark (Zhou et al., 2016), which carries the risk of overfitting to the task. In addition, most models only compare against weak baselines and, strikingly, almost none considers evaluating against approaches from the extensive semi-supervised learning (SSL) literature (Chapelle et al., 2006). In this work, we make the argument that such algorithms make stron"
P18-1096,P95-1026,0,0.95539,"s highlighting the usefulness of classic approaches (Melis et al., 2017; Denkowski and Neubig, 2017). We re-evaluate bootstrapping algorithms in the context of DNNs. These are general-purpose semi-supervised algorithms that treat the model as a black box and can thus be used easily—with a few additions—with the current generation of NLP models. Many of these methods, though, were originally developed with in-domain performance in mind, so their effectiveness in a domain adaptation setting remains unexplored. In particular, we re-evaluate three traditional bootstrapping methods, self-training (Yarowsky, 1995), tri-training (Zhou and Li, 2005), and tritraining with disagreement (Søgaard, 2010) for neural network-based approaches on two NLP tasks with different characteristics, namely, a sequence prediction and a classification task (POS tagging and sentiment analysis). We evaluate the methods across multiple domains on two wellestablished benchmarks, without taking any further task-specific measures, and compare to the best results published in the literature. We make the somewhat surprising observation that classic tri-training outperforms task-agnostic state-of-the-art semi-supervised learning (L"
P18-1096,D15-1155,0,0.26012,"wide array of supervised NLP tasks such as dependency parsing (Dozat and Manning, 2017), named entity recognition (Lample et al., 2016), and semantic role labeling (He et al., 2017). In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words (Blitzer et al., 2006; Wu and Huang, 2016) or distributional features (Schnabel and Schütze, 2014; Yin et al., 2015) which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary datasets (Kim et al., 2017) or on a single benchmark (Zhou et al., 2016), which carries the risk of overfitting to the task. In addition, most models only compare against weak baselines and, strikingly, almost none considers evaluating against approaches from the extensive semi-supervised learning (SSL) literature (Chapelle et al., 2006). In this work, we make the argument that such algorithms make strong baselines for any task in line with recent efforts highlighting the usef"
P18-1096,P16-1031,0,0.0194994,"rast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics such as sentiment words (Blitzer et al., 2006; Wu and Huang, 2016) or distributional features (Schnabel and Schütze, 2014; Yin et al., 2015) which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary datasets (Kim et al., 2017) or on a single benchmark (Zhou et al., 2016), which carries the risk of overfitting to the task. In addition, most models only compare against weak baselines and, strikingly, almost none considers evaluating against approaches from the extensive semi-supervised learning (SSL) literature (Chapelle et al., 2006). In this work, we make the argument that such algorithms make strong baselines for any task in line with recent efforts highlighting the usefulness of classic approaches (Melis et al., 2017; Denkowski and Neubig, 2017). We re-evaluate bootstrapping algorithms in the context of DNNs. These are general-purpose semi-supervised algori"
P18-2061,D11-1120,0,0.0396552,"ive: bleaching text, i.e., transforming lexical strings into more abstract features. This study provides evidence that such features allow for better transfer across languages. Moreover, we present a first study on the ability of humans to perform cross-lingual gender prediction. We find that human predictive power proves similar to that of our bleached models, and both perform better than lexical models. 1 Introduction Author profiling is the task of discovering latent user attributes disclosed through text, such as gender, age, personality, income, location and occupation (Rao et al., 2010; Burger et al., 2011; Feng et al., 2012; Jurgens, 2013; Bamman et al., 2014; Plank and Hovy, 2015; Flekova et al., 2016). It is of interest to several applications including personalized machine translation, forensics, and marketing (Mirkin et al., 2015; Rangel et al., 2015). Early approaches to gender prediction (Koppel et al., 2002; Schler et al., 2006, e.g.) are inspired by pioneering work on authorship attribution (Mosteller and Wallace, 1964). Such stylometric models typically rely on carefully handselected sets of content-independent features to capture style beyond topic. Recently, open vocabulary approach"
P18-2061,W17-4407,0,0.0934559,"rkin et al., 2015; Rangel et al., 2015). Early approaches to gender prediction (Koppel et al., 2002; Schler et al., 2006, e.g.) are inspired by pioneering work on authorship attribution (Mosteller and Wallace, 1964). Such stylometric models typically rely on carefully handselected sets of content-independent features to capture style beyond topic. Recently, open vocabulary approaches (Schwartz et al., 2013), where the entire linguistic production of an author is used, yielded substantial performance gains in online user-attribute prediction (Nguyen et al., 2014; Preo¸tiuc-Pietro et al., 2015; Emmery et al., 2017). Indeed, the best performing gender prediction models exploit chiefly lexical information (Rangel et al., 2017; Basile et al., 2017). Relying heavily on the lexicon though has its limitations, as it results in models with limited portability. Moreover, performance might be overly optimistic due to topic bias (Sarawgi et al., 2011). Recent work on cross-lingual author profiling has proposed the use of solely language-independent features (Ljubeši´c et al., 2017), e.g., specific textual elements (percentage of emojis, URLs, etc) and users’ meta-data/network (number of followers, etc), but this"
P18-2061,C14-1184,0,0.0612336,"Missing"
P18-2061,D12-1139,0,0.0579617,"Missing"
P18-2061,P16-1080,0,0.100081,"Missing"
P18-2061,N07-1051,0,0.0192212,"ence of additional user-specific metadata. Our approach can be seen as taking advantage of elements from a data-driven open-vocabulary approach, while trying to capture gender-specific style in text beyond topic. To represent utterances in a more language agnostic way, we propose to simply transform the text into alternative textual representations, which deviate from the lexical form to allow for abstraction. We propose the following transformations, exemplified in Table 1. They are mostly motivated by intuition and inspired by prior work, like the use of shape features from NER and parsing (Petrov and Klein, 2007; Schnabel and Schütze, 2014; Plank et al., 2016; Limsopatham and Collier, 2016): • AllAbs A combination (concatenation) of all previously described features. 3 Experiments In order to test whether abstract features are effective and transfer across languages, we set up experiments for gender prediction comparing lexicalized and bleached models for both in- and cross-language experiments. We compare them to a model using multilingual embeddings (Ruder, 2017). Finally, we elicit human judgments both within language and across language. The latter is to check whether a person with no prior knowl"
P18-2061,W16-3920,0,0.0222583,"aking advantage of elements from a data-driven open-vocabulary approach, while trying to capture gender-specific style in text beyond topic. To represent utterances in a more language agnostic way, we propose to simply transform the text into alternative textual representations, which deviate from the lexical form to allow for abstraction. We propose the following transformations, exemplified in Table 1. They are mostly motivated by intuition and inspired by prior work, like the use of shape features from NER and parsing (Petrov and Klein, 2007; Schnabel and Schütze, 2014; Plank et al., 2016; Limsopatham and Collier, 2016): • AllAbs A combination (concatenation) of all previously described features. 3 Experiments In order to test whether abstract features are effective and transfer across languages, we set up experiments for gender prediction comparing lexicalized and bleached models for both in- and cross-language experiments. We compare them to a model using multilingual embeddings (Ruder, 2017). Finally, we elicit human judgments both within language and across language. The latter is to check whether a person with no prior knowledge of (the lexicon of) a given language can predict the gender of a user, and"
P18-2061,W17-2901,1,0.855355,"Missing"
P18-2061,D15-1130,0,0.0306333,"ans to perform cross-lingual gender prediction. We find that human predictive power proves similar to that of our bleached models, and both perform better than lexical models. 1 Introduction Author profiling is the task of discovering latent user attributes disclosed through text, such as gender, age, personality, income, location and occupation (Rao et al., 2010; Burger et al., 2011; Feng et al., 2012; Jurgens, 2013; Bamman et al., 2014; Plank and Hovy, 2015; Flekova et al., 2016). It is of interest to several applications including personalized machine translation, forensics, and marketing (Mirkin et al., 2015; Rangel et al., 2015). Early approaches to gender prediction (Koppel et al., 2002; Schler et al., 2006, e.g.) are inspired by pioneering work on authorship attribution (Mosteller and Wallace, 1964). Such stylometric models typically rely on carefully handselected sets of content-independent features to capture style beyond topic. Recently, open vocabulary approaches (Schwartz et al., 2013), where the entire linguistic production of an author is used, yielded substantial performance gains in online user-attribute prediction (Nguyen et al., 2014; Preo¸tiuc-Pietro et al., 2015; Emmery et al., 20"
P18-2061,I17-4024,1,0.880203,"Missing"
P18-2061,W15-2913,1,0.891191,"eatures. This study provides evidence that such features allow for better transfer across languages. Moreover, we present a first study on the ability of humans to perform cross-lingual gender prediction. We find that human predictive power proves similar to that of our bleached models, and both perform better than lexical models. 1 Introduction Author profiling is the task of discovering latent user attributes disclosed through text, such as gender, age, personality, income, location and occupation (Rao et al., 2010; Burger et al., 2011; Feng et al., 2012; Jurgens, 2013; Bamman et al., 2014; Plank and Hovy, 2015; Flekova et al., 2016). It is of interest to several applications including personalized machine translation, forensics, and marketing (Mirkin et al., 2015; Rangel et al., 2015). Early approaches to gender prediction (Koppel et al., 2002; Schler et al., 2006, e.g.) are inspired by pioneering work on authorship attribution (Mosteller and Wallace, 1964). Such stylometric models typically rely on carefully handselected sets of content-independent features to capture style beyond topic. Recently, open vocabulary approaches (Schwartz et al., 2013), where the entire linguistic production of an auth"
P18-2061,P16-2067,1,0.838314,"ach can be seen as taking advantage of elements from a data-driven open-vocabulary approach, while trying to capture gender-specific style in text beyond topic. To represent utterances in a more language agnostic way, we propose to simply transform the text into alternative textual representations, which deviate from the lexical form to allow for abstraction. We propose the following transformations, exemplified in Table 1. They are mostly motivated by intuition and inspired by prior work, like the use of shape features from NER and parsing (Petrov and Klein, 2007; Schnabel and Schütze, 2014; Plank et al., 2016; Limsopatham and Collier, 2016): • AllAbs A combination (concatenation) of all previously described features. 3 Experiments In order to test whether abstract features are effective and transfer across languages, we set up experiments for gender prediction comparing lexicalized and bleached models for both in- and cross-language experiments. We compare them to a model using multilingual embeddings (Ruder, 2017). Finally, we elicit human judgments both within language and across language. The latter is to check whether a person with no prior knowledge of (the lexicon of) a given language can pr"
P18-2061,P15-1169,0,0.0610355,"Missing"
P18-2061,W11-0310,0,0.018254,"yle beyond topic. Recently, open vocabulary approaches (Schwartz et al., 2013), where the entire linguistic production of an author is used, yielded substantial performance gains in online user-attribute prediction (Nguyen et al., 2014; Preo¸tiuc-Pietro et al., 2015; Emmery et al., 2017). Indeed, the best performing gender prediction models exploit chiefly lexical information (Rangel et al., 2017; Basile et al., 2017). Relying heavily on the lexicon though has its limitations, as it results in models with limited portability. Moreover, performance might be overly optimistic due to topic bias (Sarawgi et al., 2011). Recent work on cross-lingual author profiling has proposed the use of solely language-independent features (Ljubeši´c et al., 2017), e.g., specific textual elements (percentage of emojis, URLs, etc) and users’ meta-data/network (number of followers, etc), but this information is not always available. We propose a novel approach where the actual text is still used, but bleached out and transformed into more abstract, and potentially better transferable features. One could view this as a method in between the open vocabulary strategy and the stylometric approach. It has the advantage of fading"
P18-2061,Q14-1002,0,0.020286,"specific metadata. Our approach can be seen as taking advantage of elements from a data-driven open-vocabulary approach, while trying to capture gender-specific style in text beyond topic. To represent utterances in a more language agnostic way, we propose to simply transform the text into alternative textual representations, which deviate from the lexical form to allow for abstraction. We propose the following transformations, exemplified in Table 1. They are mostly motivated by intuition and inspired by prior work, like the use of shape features from NER and parsing (Petrov and Klein, 2007; Schnabel and Schütze, 2014; Plank et al., 2016; Limsopatham and Collier, 2016): • AllAbs A combination (concatenation) of all previously described features. 3 Experiments In order to test whether abstract features are effective and transfer across languages, we set up experiments for gender prediction comparing lexicalized and bleached models for both in- and cross-language experiments. We compare them to a model using multilingual embeddings (Ruder, 2017). Finally, we elicit human judgments both within language and across language. The latter is to check whether a person with no prior knowledge of (the lexicon of) a g"
P18-2061,L16-1258,1,0.881658,"rediction comparing lexicalized and bleached models for both in- and cross-language experiments. We compare them to a model using multilingual embeddings (Ruder, 2017). Finally, we elicit human judgments both within language and across language. The latter is to check whether a person with no prior knowledge of (the lexicon of) a given language can predict the gender of a user, and how that compares to an in-language setup and the machine. If humans can predict gender cross-lingually, they are likely to rely on aspects beyond lexical information. Data We obtain data from the T WI S TY corpus (Verhoeven et al., 2016), a multi-lingual collection of Twitter users, for the languages with 500+ users, namely Dutch, French, Portuguese, and Spanish. We complement them with English, using data from a predecessor of T WI S TY (Plank and Hovy, 2015). All datasets contain manually annotated gender information. To simplify interpretation for the cross-language experiments, we balance gender in all datasets by downsampling to the minority class. The datasets’ final sizes are given in Table 2. We use 200 tweets per user, as done by previous work (Verhoeven et al., 2016). We leave the data untokenized to exclude any lan"
P19-1350,P15-2123,0,0.0320022,"those learned by the model trained on Wh-q independently: Y/N questions result in a big hard-to-distinguish “blob”, and are confused with Wh-q about size, as visible in Fig. 2 and the confusion matrix analysis (in the SM). In contrast, Rehearsal remembers how to distinguish among all kinds of Wh-q and between Wh-q and Y/N-q. The error analysis confirms that the model hardly makes any mistakes related to task confusion. However, despite the higher performance than EWC, Rehearsal is still not able to discern well between different kinds of Y/N-q. 5 Related Work Early work on life-long learning (Chen et al., 2015; Mitchell et al., 2015) is related to ours, but typically concerns a single task (e.g., relation extraction). Lee (2017) aims to transfer conversational skills from a synthetic domain to a customer-specific application in dialogue agents, while Yogatama et al. (2019) show that current models for different NLP tasks are not able to properly reuse previously learned knowledge. In general, continual learning has been mostly studied in computer vision. To the best of our knowledge, little has been done on catastrophic forgetting in VQA. A study on forgetting in the context of VQA and closest to o"
plank-2010-improved,W01-0521,0,\N,Missing
plank-2010-improved,D08-1093,0,\N,Missing
plank-2010-improved,W06-2920,0,\N,Missing
plank-2010-improved,P08-1108,0,\N,Missing
plank-2010-improved,2006.jeptalnrecital-invite.2,0,\N,Missing
plank-2010-improved,H05-1066,0,\N,Missing
plank-2010-improved,W06-1615,0,\N,Missing
plank-2010-improved,P07-1056,0,\N,Missing
plank-2010-improved,oostdijk-2000-spoken,0,\N,Missing
plank-2010-improved,P07-1033,0,\N,Missing
plank-2010-improved,N06-1020,0,\N,Missing
plank-simaan-2008-subdomain,J93-2004,0,\N,Missing
plank-simaan-2008-subdomain,P97-1003,0,\N,Missing
plank-simaan-2008-subdomain,W01-0521,0,\N,Missing
plank-simaan-2008-subdomain,A97-1015,0,\N,Missing
plank-simaan-2008-subdomain,J03-4003,0,\N,Missing
plank-simaan-2008-subdomain,P07-1034,0,\N,Missing
plank-simaan-2008-subdomain,P05-1022,0,\N,Missing
plank-simaan-2008-subdomain,P06-1043,0,\N,Missing
Q16-1022,A00-1031,0,0.555773,"observe a major advantage in using reverse-mode alignment for POS projection (4-5 accuracy points absolute).10 In addition, we use the IBM1 aligner efmaral11 by ¨ Ostling (2015). The intuition behind using IBM1 is that IBM2 introduces a bias toward more closely related languages, and we confirm this intuition through our experiments. We modify both aligners so that they output the alignment probability for each aligned token pair. Tagging and parsing The source-sides of the two multi-parallel corpora, EBC and WTC, are POStagged by taggers trained on the respective source languages, using TnT (Brants, 2000). We parse the corpora using TurboParser (Martins et al., 2013). The parser is used in simple arc-factored mode with pruning.12 We alter it to output per-sentence arc 7 http://universaldependencies.org/ format.html 8 https://github.com/coastalcph/ ud-conversion-tools. 9 Parameters used: utf, bisent, cautious, realign. 10 Parameters used: d, o, v, r. 11 Also reverse mode, with default settings, see https:// github.com/robertostling/efmaral. 12 Parameters used: basic. weight matrices.13 4 Experiments Outline For each sentence in a target language corpus, we retrieve the aligned sentences in the"
Q16-1022,P11-1061,0,0.159967,"ning data. In our dependency graph projection, we normalize the weights per sentence. For future development, we note that corpus-level normalization might achieve the same balancing effect while still preserving possibly important language-specific signals regarding structural disambiguations. EBC and WTC constitute a (hopefully small) subset of the publicly available multilingual parallel corpora. The outdated EBC texts can be replaced by newer ones, and the EBC itself replaced or aug310 Related work POS tagging While projection annotation of POS labels goes back to Yarowsky’s seminal work, Das and Petrov (2011) recently renewed interest in this problem. Das and Petrov (2011) go beyond our approach to POS annotation by combining annotation projection and unsupervised learning techniques, but they restrict themselves to Indo-European languages and a coarser tagset. Li et al. (2012) introduce an approach that leverages potentially noisy, but sizeable POS tag dictionaries in the form of Wiktionaries for 9 resource-rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c e"
Q16-1022,P13-1057,0,0.0330031,"newer ones, and the EBC itself replaced or aug310 Related work POS tagging While projection annotation of POS labels goes back to Yarowsky’s seminal work, Das and Petrov (2011) recently renewed interest in this problem. Das and Petrov (2011) go beyond our approach to POS annotation by combining annotation projection and unsupervised learning techniques, but they restrict themselves to Indo-European languages and a coarser tagset. Li et al. (2012) introduce an approach that leverages potentially noisy, but sizeable POS tag dictionaries in the form of Wiktionaries for 9 resource-rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c et al. (2015) were the first to introduce the idea of learning models for more than a dozen truly low-resource languages in one go, and our contribution can be seen as a non-trivial extension of theirs. Parsing With the exception of Zeman and Resnik (2008), initial work on cross-lingual dependency parsing focused on annotation projection (Hwa et al., 2005; Spreyer et al., 2010). McDonald et al. (2011) and Søgaard (2011) simultaneously took up"
Q16-1022,P16-2091,1,0.868444,"Missing"
Q16-1022,D12-1127,0,0.0917574,"Missing"
Q16-1022,C14-1075,0,0.0462189,"dea of delexicalized transfer after Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency parsing since Hwa et al. (2005). Notable approaches include the 18 http://www.ohchr.org/EN/UDHR/Pages/ SearchByLang.aspx 19 http://opus.lingfil.uu.se/ soft projection of reliable dependencies by Li et al. (2014), and the work of Ma and Xia (2014), who make use of the source-side distributions through a training objective function. Tiedemann and Agi´c (2016) provide a more detailed overview of model transfer and annotation projection, while introducing a competitive machine translation-based approach to synthesizing dependency treebanks. In their work, we note the IBM4 word alignments favor more closely related languages, and that building machine translation systems requires parallel data in quantities that far surpass EBC and WTC combined. The best results reported to date were presented by Rasooli"
Q16-1022,P14-1126,0,0.169202,"r Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency parsing since Hwa et al. (2005). Notable approaches include the 18 http://www.ohchr.org/EN/UDHR/Pages/ SearchByLang.aspx 19 http://opus.lingfil.uu.se/ soft projection of reliable dependencies by Li et al. (2014), and the work of Ma and Xia (2014), who make use of the source-side distributions through a training objective function. Tiedemann and Agi´c (2016) provide a more detailed overview of model transfer and annotation projection, while introducing a competitive machine translation-based approach to synthesizing dependency treebanks. In their work, we note the IBM4 word alignments favor more closely related languages, and that building machine translation systems requires parallel data in quantities that far surpass EBC and WTC combined. The best results reported to date were presented by Rasooli and Collins (2015). They use the in"
Q16-1022,P13-2109,0,0.0643836,"Missing"
Q16-1022,P05-1012,0,0.0672856,"Missing"
Q16-1022,D11-1006,0,0.675369,"es in the arc projection, but we use unit votes in POS voting. The opposite yields the best IBM2 scores: binarizing the alignment scores in dependency projection, while weight-voting the POS tags. We also evaluated a number of different normalization techniques in projection, only to arrive at standardization and softmax as by far the best choices. Baselines and upper bounds We compare our systems to three competitive baselines, as well as three informed upper bounds or oracles. First, we list our baselines. D ELEX -MS: This is the multi-source direct delexicalized parser transfer baseline of McDonald et al. (2011).15 DCA-P ROJ: This is the direct correspondence assumption (DCA)-based approach to projection, i.e., the de facto standard for projecting dependencies. First introduced by Hwa et al. (2005), it was recently elucidated by Tiedemann (2014), whose implementation we follow here. In contrast to our approach, 15 Referred to as multi-dir in the original paper. DCA projects trees on a source-target sentence pair basis, relying on heuristics and spurious nodes or edges to maintain the tree structure. In the setup, we basically plug DCA into our projection-voting pipeline instead of our own method. R E"
Q16-1022,P13-2017,0,0.0731737,"Missing"
Q16-1022,P15-2034,0,0.0166339,"there are more candidates, we select one through POS ranking.8 Alignment We sentence- and word-align all language pairs in both our multi-parallel corpora. We use hunalign (Varga et al., 2005) to perform conservative sentence alignment.9 The selected sentence pairs then enter word alignment. Here, we use two different aligners. The first one is IBM2 fastalign by Dyer et al. (2013), where we adopt the setup of Agi´c et al. (2015) who observe a major advantage in using reverse-mode alignment for POS projection (4-5 accuracy points absolute).10 In addition, we use the IBM1 aligner efmaral11 by ¨ Ostling (2015). The intuition behind using IBM1 is that IBM2 introduces a bias toward more closely related languages, and we confirm this intuition through our experiments. We modify both aligners so that they output the alignment probability for each aligned token pair. Tagging and parsing The source-sides of the two multi-parallel corpora, EBC and WTC, are POStagged by taggers trained on the respective source languages, using TnT (Brants, 2000). We parse the corpora using TurboParser (Martins et al., 2013). The parser is used in simple arc-factored mode with pruning.12 We alter it to output per-sentence a"
Q16-1022,petrov-etal-2012-universal,0,0.0990105,"S tagging Below, we present results with POS taggers based on annotation projection with both IBM1 and IBM2; cf. Table 3. We train TnT with default settings on the projected annotations. Note that we use the resulting POS taggers in our dependency parsing experiments in order not to have our parsers assume the existence of POS-annotated corpora. For a more extensive assessment, we refer to the work by Agi´c et al. (2015) who report baseline and upper bounds. In contrast to their work, we consider two different alignment models and use the UD POS tagset (17 tags), in contrast to the 12 tags of Petrov et al. (2012). This makes our POS tagging problem slightly more challenging, but our parsing models potentially benefit from the extended tagset.14 Dependency parsing We use arc-factored TurboParser for all parsing models, applying the same setup as in preprocessing. There are three sets of models: our systems, baselines, and upper bounds. 13 Our fork of TurboParser is available from https:// github.com/andersjo/TurboParser. 14 For example, the AUX vs. VERB distinction from UD POS does not exist the tagset of Petrov et al. (2012), and neither does NOUN vs. PROPN (proper noun). 307 Our systems are trained o"
Q16-1022,D15-1039,0,0.358424,"(73m), Hausa (50m), and Kurdish (30m). Cross-lingual transfer learning—or simply cross-lingual learning—refers to work on using annotated resources in other (source) languages to induce models for such low-resource (target) languages. Even simple cross-lingual learning techniques outperform unsupervised grammar induction by a large margin. Most work in cross-lingual learning, however, makes assumptions about the availability of linguistic resources that do not hold for the majority of low-resource languages. The best cross-lingual dependency parsing results reported to date were presented by Rasooli and Collins (2015). They use the intersection of languages covered in the Google dependency treebanks project and those contained in the Europarl corpus. Consequently, they only consider closely related Indo-European languages for which high-quality tokenization can be obtained with simple heuristics. In other words, we argue that recent approaches to cross-lingual POS tagging and dependency parsing are biased toward Indo-European languages, in particular the Germanic and Romance families. The bias is not hard to explain: treebanks, as well as large volumes of parallel data, are readily available for many Germa"
Q16-1022,P15-2040,0,0.143668,"Missing"
Q16-1022,N06-2033,0,0.0734653,"entation we follow here. In contrast to our approach, 15 Referred to as multi-dir in the original paper. DCA projects trees on a source-target sentence pair basis, relying on heuristics and spurious nodes or edges to maintain the tree structure. In the setup, we basically plug DCA into our projection-voting pipeline instead of our own method. R EPARSE: For this baseline, we parse a target sentence using multiple single-source delexicalized parsers. Then, we collect the output trees in a graph, unit-voting the individual edge weights, and finally using DMST to compute the best dependency tree (Sagae and Lavie, 2006). Now, we explain the three upper bounds: D ELEX -SB: This result is using the best singlesource delexicalized system for a given target language following McDonald et al. (2013). We parse a target with multiple single-source delexicalized parsers, and select the best-performing one. S ELF -T RAIN: For this result we parse the targetlanguage EBC and WTC data, train parsers on the output predictions, and evaluate the resulting parsers on the evaluation data. Note this result is available only for the source languages. Also, note that while we refer to this as self-training, we do not concatenat"
Q16-1022,spreyer-etal-2010-training,0,0.0207293,"naries in the form of Wiktionaries for 9 resource-rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c et al. (2015) were the first to introduce the idea of learning models for more than a dozen truly low-resource languages in one go, and our contribution can be seen as a non-trivial extension of theirs. Parsing With the exception of Zeman and Resnik (2008), initial work on cross-lingual dependency parsing focused on annotation projection (Hwa et al., 2005; Spreyer et al., 2010). McDonald et al. (2011) and Søgaard (2011) simultaneously took up the idea of delexicalized transfer after Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency parsing since Hwa et al. (2005). Notable approaches include the 18 http://www.ohchr.org/EN/UDHR/Pages/ SearchByLang.aspx 19"
Q16-1022,P11-2120,1,0.889189,"rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c et al. (2015) were the first to introduce the idea of learning models for more than a dozen truly low-resource languages in one go, and our contribution can be seen as a non-trivial extension of theirs. Parsing With the exception of Zeman and Resnik (2008), initial work on cross-lingual dependency parsing focused on annotation projection (Hwa et al., 2005; Spreyer et al., 2010). McDonald et al. (2011) and Søgaard (2011) simultaneously took up the idea of delexicalized transfer after Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency parsing since Hwa et al. (2005). Notable approaches include the 18 http://www.ohchr.org/EN/UDHR/Pages/ SearchByLang.aspx 19 http://opus.lingfil.uu.se/ soft projection"
Q16-1022,W14-1614,1,0.84878,"Missing"
Q16-1022,tiedemann-2012-parallel,0,0.0365742,"Missing"
Q16-1022,C14-1175,0,0.211283,"ncy such that (ut , vt ) becomes a dependency edge in the target sentence, making the a dependent of word. Obviously, dependency annotation projection is more challenging than projecting POS, as there is a structural constraint: the projected edges must form a dependency tree on the target side. Hwa et al. (2005) were the first to consider this problem, applying heuristics to ensure well-formed trees on the target side. The heuristics were not perfect, as they have been shown to result in excessive non-projectivity and the introduction of spurious relations and tokens (Tiedemann et al., 2014; Tiedemann, 2014). These design choices all lead to di1 https://bitbucket.org/lowlands/release Figure 1: An outline of dependency annotation projection, voting, and decoding in our method, using two sources i (German) and j (Croatian) and a target t (English). Part 1 represents the multi-parallel corpus preprocessing, while parts 2 and 3 relate to our projection method. The graphs are represented as adjacency matrices with column indices encoding dependency heads. We highlight how the weight of target edge (ut = was, vt = beginning) is computed from the two contributing sources. minished parsing quality. We in"
Q16-1022,H01-1035,0,0.430375,"ts weight matrices from multiple sources, rather than dependency trees or individual dependencies from a single source. (iii) We show that our approach performs significantly better than commonly used heuristics for annotation projection, as well as than delexicalized transfer baselines. Moreover, in comparison to these systems, our approach performs particularly well on truly low-resource non-Indo-European languages. 302 All code and data are made freely available for general use.1 2 Weighted annotation projection Motivation Our approach is based on the general idea of annotation projection (Yarowsky et al., 2001) using parallel sentences. The goal is to augment an unannotated target sentence with syntactic annotations projected from one or more source sentences through word alignments. The principle is illustrated in Figure 1, where the source languages are German and Croatian, and the target is English. The simplest case is projecting POS labels, which are observed in the source sentences but unknown in the target language. In order to induce the grammatical category of the target word beginning, we project POS from the aligned words Anfang and poˇcetku, both of which are correctly annotated as N OUN"
Q16-1022,I08-3008,0,0.081872,"ges and a coarser tagset. Li et al. (2012) introduce an approach that leverages potentially noisy, but sizeable POS tag dictionaries in the form of Wiktionaries for 9 resource-rich languages. Garrette et al. (2013) also consider the problem of learning POS taggers for truly low-resource languages, but suggest crowdsourcing such POS tag dictionaries. Finally, Agi´c et al. (2015) were the first to introduce the idea of learning models for more than a dozen truly low-resource languages in one go, and our contribution can be seen as a non-trivial extension of theirs. Parsing With the exception of Zeman and Resnik (2008), initial work on cross-lingual dependency parsing focused on annotation projection (Hwa et al., 2005; Spreyer et al., 2010). McDonald et al. (2011) and Søgaard (2011) simultaneously took up the idea of delexicalized transfer after Zeman and Resnik (2008), but more importantly, they also introduced the idea of multi-source cross-lingual transfer in the context of dependency parsing. McDonald et al. (2011) were the first to combine annotation projection and multi-source transfer, the approach taken in this paper. Annotation projection has been explored in the context of cross-lingual dependency"
Q16-1022,P15-2044,1,\N,Missing
Q16-1022,N13-1073,0,\N,Missing
S14-1001,P05-3014,0,0.0381676,"se (MFS) of a word has been proven to be a strong baseline. Following this, our MFS baseline simply predicts the supersense of the most frequent WordNet sense for a tuple of a word and a part of speech. We use the part of speech predicted by the L APOS tagger (Tsuruoka et al., 2011). Any word not in WordNet is labeled as noun.person, which is the most frequent sense overall in the training data. After tagging, we run a script to correct the BI tag prefixes, as described above for the annotation ask. We also compare to the performance of existing SST systems. In particular we use SenseLearner (Mihalcea and Csomai, 2005) as a baseline, which produces estimates of the WordNet sense for each word. For these predictions, we retrieve the corresponding supersense. Finally, we use a publicly available reimplementation of Ciaramita and Altun (2006) by Michael Heilman, which reaches comparable performance on goldtagged S EM C OR.3 3.3 4 Results The results are presented in Table 2. We distinguish between three settings with various degrees of supervision: weakly supervised, which uses no domain annotated information, but solely relies on embeddings trained on unlabeled Twitter data; unsupervised domain adaptation (DA"
S14-1001,C12-1028,0,0.0175268,"rds. 8 Yuret and Yatbaz (2010) present a weakly unsupervised approach to this problem, still evaluating on S ENS E VAL -2 and S ENS E VAL -3. They focus only on nouns, relying on gold part-of-speech, but also experiment with a coarse-grained mapping, using only three high level classes. For Twitter, we are aware of little previous work on word sense disambiguation. Gella et al. (2014) present lexical sample word sense disambiguation annotation of 20 target nouns on Twitter, but no experimental results with this data. There has also been related work on disambiguation to Wikipedia for Twitter (Cassidy et al., 2012). In sum, existing work on supersense tagging and coarse-grained word sense disambiguation for English has to the best of our knowledge all focused on newswire and literature. Moreover, they all rely on gold standard POS information, making previous performance estimates rather optimistic. still predicted the correct verb.cognition as supersense. 6 Related Work There has been relatively little previous work on supersense tagging, and to the best of our knowledge, all of it has been limited to English newswire and literature (S EM C OR and S ENS E VAL). The task of supersense tagging was first"
S14-1001,W06-1670,0,0.762458,"aptation (here, from newswire to Twitter). In We present two Twitter datasets annotated with coarse-grained word senses (supersenses), as well as a series of experiments with three learning scenarios for supersense tagging: weakly supervised learning, as well as unsupervised and supervised domain adaptation. We show that (a) off-the-shelf tools perform poorly on Twitter, (b) models augmented with embeddings learned from Twitter data perform much better, and (c) errors can be reduced using type-constrained inference with distant supervision from WordNet. 1 Introduction Supersense tagging (SST, Ciaramita and Altun, 2006) is the task of assigning high-level ontological classes to open-class words (here, nouns and verbs). It is thus a coarse-grained word sense disambiguation task. The labels are based on the lexicographer file names for Princeton WordNet (Fellbaum, 1998). They include 15 senses for verbs and 26 for nouns (see Table 1). While WordNet also provides catch-all supersenses for adjectives and adverbs, these are grammatically, not semantically motivated, and do not provide any higherlevel abstraction (recently, however, Tsvetkov et al. (2014) proposed a semantic taxonomy for adjectives). They will not"
S14-1001,H94-1046,0,0.25003,"simply express the opinions of the author on some subject matter. Supersense tagging is relevant for Twitter, because it can aid e.g. QA and open RE. If someone posts a message saying that some LaTeX module now supports “drawing trees”, it is important to know whether the post is about drawing natural objects such as oaks or pines, or about drawing tree-shaped data representations. This paper is, to the best of our knowledge, the first work to address the problem of SST for Twitter. While there exist corpora of newswire and literary texts that are annotated with supersenses, e.g., S EM C OR (Miller et al., 1994), no data is available for microblogs or related domains. This paper introduces two new data sets. Furthermore, most, if not all, of previous work on SST has relied on gold standard part-of-speech (POS) tags as input. However, in a domain such as Twitter, which has proven to be challenging for POS tagging (Foster et al., 2011; Ritter et al., 2011), results obtained under the assumption of available perfect POS information are almost meaningless for any real-life application. In this paper, we instead use predicted POS tags and investigate experimental settings in which one or more of the follo"
S14-1001,W02-1001,0,0.0447847,"ocial v.stative v.weather Table 1: The 41 noun and verb supersenses in WordNet Finally, we annotated data sets for Twitter, making supervised domain adaptation (SU) experiments possible. For supervised domain adaptation, we use the annotated training data sets from both the newswire and the Twitter domain, as well as WordNet. 2 More or less supervised models This sections covers the varying degree of supervision of our systems as well as the usage of type constraints as distant supervision. For both unsupervised domain adaptation and supervised domain adaptation, we use structured perceptron (Collins, 2002), i.e., a discriminative HMM model, and search-based structured prediction (S EARN) (Daume et al., 2009). We augment both the EM-trained HMM2, discriminative HMMs and S EARN with type constraints and continuous word representations. We also experimented with conditional random fields (Lafferty et al., 2001), but obtained worse or similar results than with the other models. 2.1 Distant supervision Distant supervision in these experiments was implemented by only allowing a system to predict a certain supersense for a given word if that supersense had either been observed in the training data, or"
S14-1001,R13-1026,0,0.0254562,"aramita and Altun (2006).1 S EARN performed slightly better than structured perceptron, so we use it as our inhouse baseline in the experiments below. In this section, we briefly explain the two approaches. 3 Experiments We experiment with weakly supervised learning, unsupervised domain adaptation, as well as supervised domain adaptation, i.e., where our models are induced from hand-annotated newswire and Twitter data. Note that in all our experiments, 1 https://github.com/coastalcph/ rungsted 2 3 http://hunch.net/˜vw/ use for training, development and evaluation, using the splits proposed in Derczynski et al. (2013), and (b) supersense annotations for a sample of 200 tweets, which we use for additional, out-of-sample evaluation. We call these data sets R ITTER{T RAIN ,D EV,E VAL} and I N -H OUSE -E VAL, respectively. The I N -H OUSE -E VAL dataset was downloaded in 2013 and is a sample of tweets that contain links to external homepages but are otherwise unbiased. It was previously used (with partof-speech annotation) in (Plank et al., 2014). Both data sets are made publicly available with this paper. Supersenses are annotated with in spans defined by the BIO (Begin-Inside-Other) notation. To obtain the T"
S14-1001,E14-1078,1,0.832545,"l our experiments, 1 https://github.com/coastalcph/ rungsted 2 3 http://hunch.net/˜vw/ use for training, development and evaluation, using the splits proposed in Derczynski et al. (2013), and (b) supersense annotations for a sample of 200 tweets, which we use for additional, out-of-sample evaluation. We call these data sets R ITTER{T RAIN ,D EV,E VAL} and I N -H OUSE -E VAL, respectively. The I N -H OUSE -E VAL dataset was downloaded in 2013 and is a sample of tweets that contain links to external homepages but are otherwise unbiased. It was previously used (with partof-speech annotation) in (Plank et al., 2014). Both data sets are made publicly available with this paper. Supersenses are annotated with in spans defined by the BIO (Begin-Inside-Other) notation. To obtain the Twitter data sets, we carried out an annotation task. We first pre-annotated all data sets with WordNet’s most frequent senses. If the word was not in WordNet and a noun, we assigned it the sense n.person. All other words were labeled O. Chains of nouns were altered to give every element the sense of the head noun, and the BI tags adjusted, i.e.: we use predicted POS tags as input to the system, in order to produce a realistic est"
S14-1001,I11-1100,0,0.0237767,"Missing"
S14-1001,D11-1141,0,0.1048,"ng tree-shaped data representations. This paper is, to the best of our knowledge, the first work to address the problem of SST for Twitter. While there exist corpora of newswire and literary texts that are annotated with supersenses, e.g., S EM C OR (Miller et al., 1994), no data is available for microblogs or related domains. This paper introduces two new data sets. Furthermore, most, if not all, of previous work on SST has relied on gold standard part-of-speech (POS) tags as input. However, in a domain such as Twitter, which has proven to be challenging for POS tagging (Foster et al., 2011; Ritter et al., 2011), results obtained under the assumption of available perfect POS information are almost meaningless for any real-life application. In this paper, we instead use predicted POS tags and investigate experimental settings in which one or more of the following resources are available to us: • a large corpus of unlabeled Twitter data; • Princeton WordNet (Fellbaum, 1998); • S EM C OR (Miller et al., 1994); and • a small corpus of Twitter data annotated with supersenses. We approach SST of Twitter using various degrees of supervision for both learning and domain adaptation (here, from newswire to Twi"
S14-1001,E14-4042,0,0.0248191,"tiv e .pers on noun .grou p noun .artif noun a ct .com muni catio n noun .even t noun .loca tion noun .time noun Figure 2: Inter-annotator confusion matrix on T WITTER -E VAL. 0.4 0.3 0.2 0.1 0 Figure 3: Sense distribution of OOV words. 8 Yuret and Yatbaz (2010) present a weakly unsupervised approach to this problem, still evaluating on S ENS E VAL -2 and S ENS E VAL -3. They focus only on nouns, relying on gold part-of-speech, but also experiment with a coarse-grained mapping, using only three high level classes. For Twitter, we are aware of little previous work on word sense disambiguation. Gella et al. (2014) present lexical sample word sense disambiguation annotation of 20 target nouns on Twitter, but no experimental results with this data. There has also been related work on disambiguation to Wikipedia for Twitter (Cassidy et al., 2012). In sum, existing work on supersense tagging and coarse-grained word sense disambiguation for English has to the best of our knowledge all focused on newswire and literature. Moreover, they all rely on gold standard POS information, making previous performance estimates rather optimistic. still predicted the correct verb.cognition as supersense. 6 Related Work Th"
S14-1001,P12-2050,0,0.379949,"rovide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. They evaluate their model on all-words data from S ENSE E VAL -2 and S ENSE E VAL -3. They use a classification approach rather than structured prediction. 7 Conclusion In this paper, we present two Twitter data sets wit"
S14-1001,P05-1005,0,0.0587046,"tion sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) and, in addition, discuss how to retrieve fine-grained senses from those predictions. They evaluate their model on all-words data from S ENSE E VAL -2 and S ENSE E VAL -3. They use a classification approach rather than structured prediction. 7 Conclusion In this paper, we present two Twitter data sets with manually annotated supersenses, as well as a series of experiments with these data sets. The data is publicly available for download. In this article we have provided, t"
S14-1001,P99-1023,0,0.187848,"Missing"
S14-1001,S10-1049,0,0.0509089,"nd Paaß, 2008; Paaß and Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNe"
S14-1001,D12-1127,0,0.0563414,"Missing"
S14-1001,W11-0328,0,0.0190122,"er k on development data for using the k-most frequent senses inWordNet as type constraints. Our supervised models are trained on S EM C OR +R ITTER -T RAIN or simply R ITTER -T RAIN, depending on what gave us the best performance on the held-out data. Baselines For most word sense disambiguation studies, predicting the most frequent sense (MFS) of a word has been proven to be a strong baseline. Following this, our MFS baseline simply predicts the supersense of the most frequent WordNet sense for a tuple of a word and a part of speech. We use the part of speech predicted by the L APOS tagger (Tsuruoka et al., 2011). Any word not in WordNet is labeled as noun.person, which is the most frequent sense overall in the training data. After tagging, we run a script to correct the BI tag prefixes, as described above for the annotation ask. We also compare to the performance of existing SST systems. In particular we use SenseLearner (Mihalcea and Csomai, 2005) as a baseline, which produces estimates of the WordNet sense for each word. For these predictions, we retrieve the corresponding supersense. Finally, we use a publicly available reimplementation of Ciaramita and Altun (2006) by Michael Heilman, which reach"
S14-1001,W13-0906,0,0.0402627,"d Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers from Princeton WordNet senses (supersenses) a"
S14-1001,tsvetkov-etal-2014-augmenting-english,1,0.22216,"n from WordNet. 1 Introduction Supersense tagging (SST, Ciaramita and Altun, 2006) is the task of assigning high-level ontological classes to open-class words (here, nouns and verbs). It is thus a coarse-grained word sense disambiguation task. The labels are based on the lexicographer file names for Princeton WordNet (Fellbaum, 1998). They include 15 senses for verbs and 26 for nouns (see Table 1). While WordNet also provides catch-all supersenses for adjectives and adverbs, these are grammatically, not semantically motivated, and do not provide any higherlevel abstraction (recently, however, Tsvetkov et al. (2014) proposed a semantic taxonomy for adjectives). They will not be considered in this paper. Coarse-grained categories such as supersenses are useful for downstream tasks such as questionanswering (QA) and open relation extraction (RE). SST is different from NER in that it has a larger set of labels and in the absence of strong orthographic cues (capitalization, quotation marks, etc.). Moreover, supersenses can be applied to any of the lexical parts of speech and not only proper names. Also, while high-coverage gazetteers can be found for named entity recognition, the lexical resources available"
S14-1001,S07-1051,0,0.166033,"and Paaß (Reichartz and Paaß, 2008; Paaß and Reichartz, 2009) extended this work, using a CRF model as well as LDA topic features. They report an F1 score of 80.2, again relying on gold standard POS features. Our implementation follows their setup and feature model, but we rely on predicted POS features, not gold standard features. Supersenses provide information similar to higher-level distributional clusters, but more interpretable, and have thus been used as highlevel features in various tasks, such as preposition sense disambiguation, noun compound interpretation, and metaphor detection (Ye and Baldwin, 2007; Tratz and Hovy, 2010; Tsvetkov et al., 2013). Princeton WordNet only provides a fully developed taxonomy of supersenses for verbs and nouns, but Tsvetkov et al. (2014) have recently proposed an extension of the taxonomy to cover adjectives. Outside of English, supersenses have been annotated for Arabic Wikipedia articles by Schneider et al. (2012). In addition, a few researchers have tried to solve coarse-grained word sense disambiguation problems that are very similar to supersense tagging. Kohomban and Lee (2005) and Kohomban and Lee (2007) also propose to use lexicographer file identifers"
S14-1001,J10-1004,0,0.0158174,"ystems (DA) S EARN (Baseline) S EARN S EARN HMM HMM + + + + + - Supervised domain adaptation systems (SU) S EARN (Baseline) S EARN S EARN HMM HMM + + + + + + + + + + Table 2: Weighted F1 average over 41 supersenses. 7 noun .act noun .food noun .attri bute noun .relat ion verb. cogn ition verb. creat ion verb. emot ion verb. moti on verb. perce ption verb. stativ e .pers on noun .grou p noun .artif noun a ct .com muni catio n noun .even t noun .loca tion noun .time noun Figure 2: Inter-annotator confusion matrix on T WITTER -E VAL. 0.4 0.3 0.2 0.1 0 Figure 3: Sense distribution of OOV words. 8 Yuret and Yatbaz (2010) present a weakly unsupervised approach to this problem, still evaluating on S ENS E VAL -2 and S ENS E VAL -3. They focus only on nouns, relying on gold part-of-speech, but also experiment with a coarse-grained mapping, using only three high level classes. For Twitter, we are aware of little previous work on word sense disambiguation. Gella et al. (2014) present lexical sample word sense disambiguation annotation of 20 target nouns on Twitter, but no experimental results with this data. There has also been related work on disambiguation to Wikipedia for Twitter (Cassidy et al., 2012). In sum,"
S14-2034,W09-1206,0,0.114151,"Missing"
S14-2034,C10-1011,0,0.164674,"Missing"
S14-2034,J93-2004,0,0.0469115,"Missing"
S14-2034,D07-1111,0,0.0867606,"Missing"
S14-2034,C08-1095,0,0.112956,"Missing"
S15-2118,N13-1037,0,0.0128538,"ata, label: -1.24; GSA prediction: +5. does not result always in the exact opposite sentiment and therefore it is not as simple as just inverting the scores from a general SA system. Only few studies have attempted SA on figurative language so far (Reyes and Rosso, 2012; Reyes et al., 2013). The prediction of a fine-grained sentiment score (between -5 and 5) for a tweet poses a series of challenges. First of all, accurate language technology on tweets is hard due to sample bias, i.e., collections of tweets are inherently biased towards the particular time (or way, cf. §2) they were collected (Eisenstein, 2013; Hovy et al., 2014). Secondly, the notion of figurativeness (or its complementary notion of literality) does not have a strong definition, let alone do irony, sarcasm, or satire. As pointed out by Reyes and Rosso (2012), “there is not a clear distinction about the boundaries among these terms”. Yet alone attaching a fine-grained score is far from straightforward. In fact, the gold standard consists of the average score assigned by humans through crowdsourcing reflecting an uncertainty in ground truth. 2 Data Analysis The goal of the initial data exploration was to investigate the amount of no"
S15-2118,W14-2602,1,0.832592,"−5.0 −2.5 0.0 2.5 5.0 gold Figure 1: Label Plots for RR predictions. distributions of the gold scores and GSA predictions for the trial data. It shows that the gold distribution is skewed with regards to the number of negative instances to positives, while the GSA predicts more positive sentiment. Support 135 22 29 8 3 92 Table 2: Tweet Label Type and Expression. The Effect of a General Sentiment System The data for this task is very different from data that most lexicon-based or general sentiment-analysis models fare best on. In fact, running a general sentiment classifier (GSA) described in Elming et al. (2014) on the trial data showed that its predictions are actually slightly anti-correlated with the gold standard scores for the Tweets in this task (cosine similarity score of -0.08 and MSE of 18.62). We exploited these anti-correlated results as features for our stacking systems (cf. § 3.2). Figure 2 shows the 700 Figure 2: Distribution of Gold Scores and GSA Predictions for Trial Data. 3 System Description We approach the task (Ghosh et al., 2015) as a regression task (cf. §4.4), combining several systems using stacking (§ 3.2), and relying on features without POS, lemma or explicit use of lexico"
S15-2118,S15-2080,0,0.0227948,"rom data that most lexicon-based or general sentiment-analysis models fare best on. In fact, running a general sentiment classifier (GSA) described in Elming et al. (2014) on the trial data showed that its predictions are actually slightly anti-correlated with the gold standard scores for the Tweets in this task (cosine similarity score of -0.08 and MSE of 18.62). We exploited these anti-correlated results as features for our stacking systems (cf. § 3.2). Figure 2 shows the 700 Figure 2: Distribution of Gold Scores and GSA Predictions for Trial Data. 3 System Description We approach the task (Ghosh et al., 2015) as a regression task (cf. §4.4), combining several systems using stacking (§ 3.2), and relying on features without POS, lemma or explicit use of lexicons, cf. § 3.3. 3.1 Single Systems 3.3 Ridge Regression (RR) A standard supervised ridge regression model with default parameters.3 PCA GMM Ridge Regression (GMM) A ridge regression model trained on the output of unsupervised induced features, i.e., a Gaussian Mixture Models (GMM) trained on PCA of word n-grams. PCA was used to reduce the dimensionality to 100, and GMM under the assumption that the data was sampled from different distributions o"
S15-2118,hovy-etal-2014-pos,1,0.896031,"GSA prediction: +5. does not result always in the exact opposite sentiment and therefore it is not as simple as just inverting the scores from a general SA system. Only few studies have attempted SA on figurative language so far (Reyes and Rosso, 2012; Reyes et al., 2013). The prediction of a fine-grained sentiment score (between -5 and 5) for a tweet poses a series of challenges. First of all, accurate language technology on tweets is hard due to sample bias, i.e., collections of tweets are inherently biased towards the particular time (or way, cf. §2) they were collected (Eisenstein, 2013; Hovy et al., 2014). Secondly, the notion of figurativeness (or its complementary notion of literality) does not have a strong definition, let alone do irony, sarcasm, or satire. As pointed out by Reyes and Rosso (2012), “there is not a clear distinction about the boundaries among these terms”. Yet alone attaching a fine-grained score is far from straightforward. In fact, the gold standard consists of the average score assigned by humans through crowdsourcing reflecting an uncertainty in ground truth. 2 Data Analysis The goal of the initial data exploration was to investigate the amount of non-figurativeness in"
S15-2118,W12-0607,0,0.115298,"Missing"
uryupina-etal-2014-sentube,J08-4004,0,\N,Missing
uryupina-etal-2014-sentube,P07-1056,0,\N,Missing
uryupina-etal-2014-sentube,pak-paroubek-2010-twitter,0,\N,Missing
W08-1302,J97-4005,0,0.271387,"e for the input the model gets. However, as soon as the model is applied to another domain, or text genre (Lease et al., 2006), accuracy degrades considerably. For example, the performance of a parser trained on the Wall Street Journal (newspaper text) significantly drops when evaluated on the more varied Brown (fiction/nonfiction) corpus (Gildea, 2001). A simple solution to improve performance on a new domain is to construct a parser specifically 2 Background: MaxEnt Models Maximum Entropy (MaxEnt) models are widely used in Natural Language Processing (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997). In this framework, a disambiguation model is specic 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 9 Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 9–16 Manchester, August 2008 fied by a set of feature functions describing properties of the data, together with their associated weights. The weights are learned during the training procedure so that their estimated value determines the contribution of each feature"
W08-1302,W98-1512,0,0.0139634,"= Ep˜[fj ], where j ∈ 1, ..., m. In MaxEnt estimation, the default model q0 is often only implicit (Velldal and Oepen, 2005) and not stated in the model equation, since the model is assumed to be uniform (e.g. the constant func1 tion Ω(s) for sentence s, where Ω(s) is the set of parse trees associated with s). Thus, we seek the model with minimum KL divergence from the uniform distribution, which means we search model p with maximum entropy (uncertainty) subject to given constraints (Abney, 1997). In alternative, if q0 is not uniform then p is called a minimum divergence model (according to (Berger and Printz, 1998)). In the statistical parsing literature, the default model q0 that can be used to incorporate prior knowledge is also referred to as base model (Berger and Printz, 1998), default or reference distribution (Hara et al., 2005; Johnson et al., 1999; Velldal and Oepen, 2005). The solution to the estimation problem of finding distribution p, that satisfies the expectedvalue constraints and minimally diverges from q0 , has been shown to take a specific parametric form (Berger and Printz, 1998): pθ (ω, s) = 1 q0 exp Zθ Pm j=1 θj fj (ω) Since the sum in equation 2 ranges over all possible parse trees"
W08-1302,J96-1002,0,0.00899713,"ebank it was trained on is representative for the input the model gets. However, as soon as the model is applied to another domain, or text genre (Lease et al., 2006), accuracy degrades considerably. For example, the performance of a parser trained on the Wall Street Journal (newspaper text) significantly drops when evaluated on the more varied Brown (fiction/nonfiction) corpus (Gildea, 2001). A simple solution to improve performance on a new domain is to construct a parser specifically 2 Background: MaxEnt Models Maximum Entropy (MaxEnt) models are widely used in Natural Language Processing (Berger et al., 1996; Ratnaparkhi, 1997; Abney, 1997). In this framework, a disambiguation model is specic 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 9 Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 9–16 Manchester, August 2008 fied by a set of feature functions describing properties of the data, together with their associated weights. The weights are learned during the training procedure so that their estimated value determines"
W08-1302,W07-2201,1,0.827647,"Missing"
W08-1302,A00-2021,0,0.583501,"ndlabeling a considerable amount of training data which is clearly very expensive and leads to an unsatisfactory solution. In alternative, techniques for domain adaptation, also known as parser adaptation (McClosky et al., 2006) or genre portability (Lease et al., 2006), try to leverage either a small amount of already existing annotated data (Hara et al., 2005) or unlabeled data (McClosky et al., 2006) of one domain to parse data from a different domain. In this study we examine an approach that assumes a limited amount of already annotated in-domain data. We explore auxiliary distributions (Johnson and Riezler, 2000) for domain adaptation, originally suggested for the incorporation of lexical selectional preferences into a parsing system. We gauge the effect of exploiting a more general, out-ofdomain model for parser adaptation to overcome the limited amount of in-domain training data. The approach is examined on two application domains, question answering and spoken data. For the empirical trials, we use Alpino (van Noord and Malouf, 2005; van Noord, 2006), a robust computational analyzer for Dutch. Alpino employs a discriminative approach to parse selection that bases its decision on a Maximum Entropy ("
W08-1302,2005.mtsummit-papers.15,0,0.0609118,"ecreasing weight. Once a model is trained, it can be applied to parse selection that chooses the parse with the highest sum of feature weights. During the training procedure, the weights vector is estimated to best fit the training data. In more detail, given m features with their corresponding empirical expectation Ep˜[fj ] and a default model q0 , we seek a model p that has minimum Kullback-Leibler (KL) divergence from the default model q0 , subject to the expected-value constraints: Ep [fj ] = Ep˜[fj ], where j ∈ 1, ..., m. In MaxEnt estimation, the default model q0 is often only implicit (Velldal and Oepen, 2005) and not stated in the model equation, since the model is assumed to be uniform (e.g. the constant func1 tion Ω(s) for sentence s, where Ω(s) is the set of parse trees associated with s). Thus, we seek the model with minimum KL divergence from the uniform distribution, which means we search model p with maximum entropy (uncertainty) subject to given constraints (Abney, 1997). In alternative, if q0 is not uniform then p is called a minimum divergence model (according to (Berger and Printz, 1998)). In the statistical parsing literature, the default model q0 that can be used to incorporate prior"
W08-1302,P99-1069,0,0.0800964,"ce s, where Ω(s) is the set of parse trees associated with s). Thus, we seek the model with minimum KL divergence from the uniform distribution, which means we search model p with maximum entropy (uncertainty) subject to given constraints (Abney, 1997). In alternative, if q0 is not uniform then p is called a minimum divergence model (according to (Berger and Printz, 1998)). In the statistical parsing literature, the default model q0 that can be used to incorporate prior knowledge is also referred to as base model (Berger and Printz, 1998), default or reference distribution (Hara et al., 2005; Johnson et al., 1999; Velldal and Oepen, 2005). The solution to the estimation problem of finding distribution p, that satisfies the expectedvalue constraints and minimally diverges from q0 , has been shown to take a specific parametric form (Berger and Printz, 1998): pθ (ω, s) = 1 q0 exp Zθ Pm j=1 θj fj (ω) Since the sum in equation 2 ranges over all possible parse trees ω ′ ∈ Ω admitted by the grammar, calculating the normalization constant renders the estimation process expensive or even intractable (Johnson et al., 1999). To tackle this problem, Johnson et al. (1999) redefine the estimation procedure by consi"
W08-1302,N06-1020,0,0.0625602,"uation Model Gertjan van Noord University of Groningen The Netherlands G.J.M.van.Noord@rug.nl Barbara Plank University of Groningen The Netherlands B.Plank@rug.nl Abstract for that domain. However, this amounts to handlabeling a considerable amount of training data which is clearly very expensive and leads to an unsatisfactory solution. In alternative, techniques for domain adaptation, also known as parser adaptation (McClosky et al., 2006) or genre portability (Lease et al., 2006), try to leverage either a small amount of already existing annotated data (Hara et al., 2005) or unlabeled data (McClosky et al., 2006) of one domain to parse data from a different domain. In this study we examine an approach that assumes a limited amount of already annotated in-domain data. We explore auxiliary distributions (Johnson and Riezler, 2000) for domain adaptation, originally suggested for the incorporation of lexical selectional preferences into a parsing system. We gauge the effect of exploiting a more general, out-ofdomain model for parser adaptation to overcome the limited amount of in-domain training data. The approach is examined on two application domains, question answering and spoken data. For the empirica"
W08-1302,oostdijk-2000-spoken,0,0.439741,"imation step and simply assign the two parameters equal values (equal weights), the method reduces to POU T (ω|s) × PIN (ω|s), i.e. just multiplying the respective model probabilities. 4 Experiments and Results 4.1 Experimental design The general model is trained on the Alpino Treebank (van Noord, 2006) (newspaper text; approximately 7,000 sentences). For the domain-specific corpora, in the first set of experiments (section 4.3) we consider the Alpino CLEF Treebank (questions; approximately 1,800 sentences). In the second part (section 4.4) we evaluate the approach on the Spoken Dutch corpus (Oostdijk, 2000) (CGN, ’Corpus Gesproken Nederlands’; spoken data; size varies, ranging from 17 to 1,193 sentences). The CGN corpus contains a variety of components/subdomains to account for the various dimensions of language use (Oostdijk, 2000). 4.2 Evaluation metric The output of the parser is evaluated by comparing the generated dependency structure for a corpus sentence to the gold standard dependency structure in a treebank. For this comparison, we represent the dependency structure (a directed acyclic graph) as a set of named dependency relations. To compare such sets of dependency relations, we count"
W08-1302,C00-1085,0,0.0234198,"ting the normalization constant renders the estimation process expensive or even intractable (Johnson et al., 1999). To tackle this problem, Johnson et al. (1999) redefine the estimation procedure by considering the conditional rather than the joint probability. Pm 1 q0 exp j=1 θj fj (ω) (3) Zθ with Zθ as in equation 2, but instead, summing over ω ′ ∈ Ω(s), where Ω(s) is the set of parse trees associated with sentence s. Thus, the probability of a parse tree is estimated by summing only over the possible parses of a specific sentence. Still, calculating Ω(s) is computationally very expensive (Osborne, 2000), because the number of parses is in the worst case exponential with respect to sentence length. Therefore, Osborne (2000) proposes a solution based on informative samples. He shows that is suffices to train on an informative subset of available training data to accurately estimate the model parameters. Alpino implements the Osborne-style approach to Maximum Entropy parsing. The standard version of the Alpino parser is trained on the Alpino newspaper Treebank (van Noord, 2006). Pθ (ω|s) = 3 Exploring auxiliary distributions for domain adaptation 3.1 Auxiliary distributions Auxiliary distributi"
W08-1302,2006.jeptalnrecital-invite.2,1,0.926399,"Missing"
W08-1302,I05-1018,0,\N,Missing
W08-1302,W01-0521,0,\N,Missing
W09-2205,W06-1615,0,0.679834,"ounded by the underlying grammar. The few studies on adapting parse disambiguation models, like Hara et al. (2005), have focused exclusively on supervised domain adaptation, i.e. one has access to a comparably small, but labeled amount of target data. In contrast, in semisupervised domain adaptation one has only unlabeled target data. It is a more realistic situation, but at the same time also considerably more difficult. In this paper we evaluate two semi-supervised approaches to domain adaptation of a discriminative parse selection model. We examine Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for this task, and compare it to several variants of Self-training (Abney, 2007; McClosky et al., 2006). For empirical evaluation (section 4) we use the Alpino parsing system for Dutch (van Noord So far, Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007). An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007). However, the system just ended up at rank 7 out of 8 teams. Based on annotation differences in the datasets (Dredze"
W09-2205,P07-1056,0,0.477512,"alistic situation, but at the same time also considerably more difficult. In this paper we evaluate two semi-supervised approaches to domain adaptation of a discriminative parse selection model. We examine Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for this task, and compare it to several variants of Self-training (Abney, 2007; McClosky et al., 2006). For empirical evaluation (section 4) we use the Alpino parsing system for Dutch (van Noord So far, Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007). An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007). However, the system just ended up at rank 7 out of 8 teams. Based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive. A recent attempt (Plank, 2009) shows promising results on applying SCL to parse disambiguation. In this paper, we extend that line of work and compare SCL to bootstrapping approaches such as self-training. Studies on self-training have focused mainly"
W09-2205,I05-1018,0,0.145711,"s b.plank@rug.nl Abstract and Malouf, 2005). As target domain, we exploit Wikipedia as primary test and training collection. This paper evaluates two semi-supervised techniques for the adaptation of a parse selection model to Wikipedia domains. The techniques examined are Structural Correspondence Learning (SCL) (Blitzer et al., 2006) and Self-training (Abney, 2007; McClosky et al., 2006). A preliminary evaluation favors the use of SCL over the simpler self-training techniques. 2 Previous Work 1 Introduction and Motivation Parse selection constitutes an important part of many parsing systems (Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006). Yet, there is little to no work focusing on the adaptation of parse selection models to novel domains. This is most probably due to the fact that potential gains for this task are inherently bounded by the underlying grammar. The few studies on adapting parse disambiguation models, like Hara et al. (2005), have focused exclusively on supervised domain adaptation, i.e. one has access to a comparably small, but labeled amount of target data. In contrast, in semisupervised domain adaptation one has only unlabeled target data. It is a more real"
W09-2205,P08-2026,0,0.0173626,"disambiguation. In this paper, we extend that line of work and compare SCL to bootstrapping approaches such as self-training. Studies on self-training have focused mainly on generative, constituent based parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007). Steedman et al. (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results. In contrast, McClosky et al. (2006) focus on large seeds and exploit a reranking-parser. Improvements are obtained (McClosky et al., 2006; McClosky and Charniak, 2008), showing that a reranker is necessary for successful self-training in such a high-resource scenario. While they self-trained a generative model, we examine self-training and SCL for semi-supervised adaptation of a discriminative parse selection system. 37 Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 37–42, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics 3 Semi-supervised Domain Adaptation 3.1 Structural Correspondence Learning Structural Correspondence Learning (Blitzer et al., 2006) exploits unlabeled"
W09-2205,N06-1020,0,0.472418,"target domain, we exploit Wikipedia as primary test and training collection. This paper evaluates two semi-supervised techniques for the adaptation of a parse selection model to Wikipedia domains. The techniques examined are Structural Correspondence Learning (SCL) (Blitzer et al., 2006) and Self-training (Abney, 2007; McClosky et al., 2006). A preliminary evaluation favors the use of SCL over the simpler self-training techniques. 2 Previous Work 1 Introduction and Motivation Parse selection constitutes an important part of many parsing systems (Hara et al., 2005; van Noord and Malouf, 2005; McClosky et al., 2006). Yet, there is little to no work focusing on the adaptation of parse selection models to novel domains. This is most probably due to the fact that potential gains for this task are inherently bounded by the underlying grammar. The few studies on adapting parse disambiguation models, like Hara et al. (2005), have focused exclusively on supervised domain adaptation, i.e. one has access to a comparably small, but labeled amount of target data. In contrast, in semisupervised domain adaptation one has only unlabeled target data. It is a more realistic situation, but at the same time also considera"
W09-2205,C00-1085,0,0.0356104,"Scoring methods We examine three simple scoringPfunctions for instance selection: i) Entropy (− y∈Y (s) p(ω|s, θ) log p(ω|s, θ)). ii) Number of parses (|Y (s)|); and iii) Sentence Length (|s|). 4 Experiments and Results Experimental Design The system used in this study is Alpino, a two-stage dependency parser for Dutch (van Noord and Malouf, 2005). The first stage consists of a HPSG-like grammar that constitutes the parse generation component. The second stage is a Maximum Entropy (MaxEnt) parse selection model. To train the MaxEnt model, parameters are estimated based on informative samples (Osborne, 2000). A parse is added to the training data with a score indicating its “goodness” (van Noord and Malouf, 2005). The score is obtained by comparing it with the gold standard (if available; otherwise the score is approximated through parse probability). The source domain is the Alpino Treebank (van Noord and Malouf, 2005) (newspaper text; approx. 7,000 sentences; 145k tokens). We use Wikipedia both as testset and as unlabeled target data source. We assume that in order to parse data from a very specific domain, say about the artist Prince, then data related to that domain, like information about th"
W09-2205,E09-3005,1,0.862102,"evaluation (section 4) we use the Alpino parsing system for Dutch (van Noord So far, Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007). An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007). However, the system just ended up at rank 7 out of 8 teams. Based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive. A recent attempt (Plank, 2009) shows promising results on applying SCL to parse disambiguation. In this paper, we extend that line of work and compare SCL to bootstrapping approaches such as self-training. Studies on self-training have focused mainly on generative, constituent based parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007). Steedman et al. (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results. In contrast, McClosky et al. (2006) focus on large seeds and exploit a reranking-parser."
W09-2205,P07-1078,0,0.0419667,"ency parsing (Shimizu and Nakagawa, 2007). However, the system just ended up at rank 7 out of 8 teams. Based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive. A recent attempt (Plank, 2009) shows promising results on applying SCL to parse disambiguation. In this paper, we extend that line of work and compare SCL to bootstrapping approaches such as self-training. Studies on self-training have focused mainly on generative, constituent based parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007). Steedman et al. (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results. In contrast, McClosky et al. (2006) focus on large seeds and exploit a reranking-parser. Improvements are obtained (McClosky et al., 2006; McClosky and Charniak, 2008), showing that a reranker is necessary for successful self-training in such a high-resource scenario. While they self-trained a generative model, we examine self-training and SCL for semi-supervised adaptation of a discriminative parse selection system. 37 Pro"
W09-2205,D07-1129,0,0.0218228,"oaches to domain adaptation of a discriminative parse selection model. We examine Structural Correspondence Learning (SCL) (Blitzer et al., 2006) for this task, and compare it to several variants of Self-training (Abney, 2007; McClosky et al., 2006). For empirical evaluation (section 4) we use the Alpino parsing system for Dutch (van Noord So far, Structural Correspondence Learning has been applied successfully to PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007). An attempt was made in the CoNLL 2007 shared task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007). However, the system just ended up at rank 7 out of 8 teams. Based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive. A recent attempt (Plank, 2009) shows promising results on applying SCL to parse disambiguation. In this paper, we extend that line of work and compare SCL to bootstrapping approaches such as self-training. Studies on self-training have focused mainly on generative, constituent based parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007). Steedman e"
W09-2205,E03-1008,0,0.438509,"red task to apply SCL to non-projective dependency parsing (Shimizu and Nakagawa, 2007). However, the system just ended up at rank 7 out of 8 teams. Based on annotation differences in the datasets (Dredze et al., 2007) and a bug in their system (Shimizu and Nakagawa, 2007), their results are inconclusive. A recent attempt (Plank, 2009) shows promising results on applying SCL to parse disambiguation. In this paper, we extend that line of work and compare SCL to bootstrapping approaches such as self-training. Studies on self-training have focused mainly on generative, constituent based parsing (Steedman et al., 2003; McClosky et al., 2006; Reichart and Rappoport, 2007). Steedman et al. (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results. In contrast, McClosky et al. (2006) focus on large seeds and exploit a reranking-parser. Improvements are obtained (McClosky et al., 2006; McClosky and Charniak, 2008), showing that a reranker is necessary for successful self-training in such a high-resource scenario. While they self-trained a generative model, we examine self-training and SCL for semi-supervised adaptat"
W10-2105,oostdijk-2000-spoken,0,0.048333,"pus) as target data. All datasets are described next. CoNLL2006 This is the testfile for Dutch that was used in the CoNLL 2006 shared task on multilingual dependency parsing. The file consists of 386 sentences from an institutional brochure (about youth healthcare). We use this file to check our data-driven models against state-of-the-art. Source: Cdb The cdb (Alpino Treebank) consists of 140,000 words (7,136 sentences) from the Eindhoven corpus (newspaper text). It is a collection of text fragments from 6 Dutch newspapers. The collection has been annotated according to the guidelines of CGN (Oostdijk, 2000) and stored in XML format. It is the standard treebank used to train the disambiguation component of the Alpino parser. Note that cdb is a subset of the training corpus used in the CoNLL 2006 shared task (Buchholz and Marsi, 2006). The CoNLL training data additionally contained a mix of nonnewspaper text,1 which we exclude here on purpose to keep a clean baseline. Alpino to CoNLL format In order to train the MST and Malt parser and evaluate it on the various Wikipedia and DPC articles, we needed to convert the Alpino Treebank format into the tabular CoNLL format. To this end, we adapted the tr"
W10-2105,W06-1615,0,0.0535997,"esources in the new domain) is a much more realistic situation but is clearly also considerably more difficult. Current studies on semisupervised approaches show very mixed results. Dredze et al. (2007) report on “frustrating” results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. “no team was able to improve target domain performance substantially over a state-of-the-art baseline”. On the other hand, there have been positive results as well. For instance, McClosky et al. (2006) improved a statistical parser by self-training. Structural Correspondence Learning (Blitzer et al., 2006) was effective for PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), while only modest gains were obtained for structured output tasks like parsing. In the past decade several parsing systems for natural language have emerged, which use different methods and formalisms. For instance, systems that employ a handcrafted grammar and a statistical disambiguation component versus purely statistical data-driven systems. What they have in common is the lack of portability to new domains: their performance might decrease substantially as the distance between test and trai"
W10-2105,W08-1302,1,0.863604,"Missing"
W10-2105,P07-1056,0,0.0308274,"re difficult. Current studies on semisupervised approaches show very mixed results. Dredze et al. (2007) report on “frustrating” results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. “no team was able to improve target domain performance substantially over a state-of-the-art baseline”. On the other hand, there have been positive results as well. For instance, McClosky et al. (2006) improved a statistical parser by self-training. Structural Correspondence Learning (Blitzer et al., 2006) was effective for PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), while only modest gains were obtained for structured output tasks like parsing. In the past decade several parsing systems for natural language have emerged, which use different methods and formalisms. For instance, systems that employ a handcrafted grammar and a statistical disambiguation component versus purely statistical data-driven systems. What they have in common is the lack of portability to new domains: their performance might decrease substantially as the distance between test and training domain increases. Yet, to which degree do they suffer from this problem, i.e. which kind of p"
W10-2105,W06-2920,0,0.400972,"ure with a separate second stage classifier to label the dependency edges. (3) MALT Parser (Nivre et al., 2007) is a datadriven transition-based dependency parser. Malt parser uses SVMs to learn a classifier that predicts the next parsing action. Instances represent parser configurations and the label to predict determines the next parser action. Both data-driven parsers (MST and Malt) are thus not specific for the Dutch Language, however, they can be trained on a variety of languages given that the training corpus complies with the columnbased format introduced in the 2006 CoNLL shared task (Buchholz and Marsi, 2006). Additionally, both parsers implement projective and non-projective parsing algorithms, where the latter will be used in our experiments on the relatively free word order language Dutch. Despite that, we train the data-driven parsers using their default settings (e.g. first order features for MST, SVM with polynomial kernel for Malt). 4 Wikipedia LOC (location) KUN (arts) POL (politics) SPO (sports) HIS (history) BUS (business) NOB (nobility) COM (comics) MUS (music) HOL (holidays) Total Example articles Belgium, Antwerp (city) Tervuren school Belgium elections 2003 Kim Clijsters History of B"
W10-2105,P07-1079,0,0.0138845,"is given here in terms of f-score of named dependencies. sents parses oracle arbitrary model 536 45011 95.74 76.56 89.39 (2) MST Parser (McDonald et al., 2005) is a Most previous work has focused on a single parsing system in isolation (Gildea, 2001; Hara et al., 2005; McClosky et al., 2006). However, there is an observable trend towards combining different parsing systems to exploit complementary strengths. For instance, Nivre and McDonald (2008) combine two data-driven systems to improve dependency accuracy. Similarly, two studies successfully combined grammar-based and datadriven systems: Sagae et al. (2007) incorporate data-driven dependencies as soft-constraint in a HPSG-based system for parsing the Wallstreet Journal. In the same spirit (but the other direction), Zhang and Wang (2009) use a deepgrammar based backbone to improve data-driven parsing accuracy. They incorporate features from the grammar-based backbone into the data-driven system to achieve better generalization across domains. This is the work most closest to ours. However, which kind of system (hand-crafted versus purely statistical) is more affected by the domain, and thus more sensitive to domain shifts? To the best of our know"
W10-2105,P07-1033,0,0.0468113,"Missing"
W10-2105,W09-2609,1,0.87949,"Missing"
W10-2105,P04-1057,1,0.85536,"Missing"
W10-2105,2006.jeptalnrecital-invite.2,1,0.910589,"Missing"
W10-2105,W01-0521,0,0.694075,"system is more affected by domain shifts? Intuitively, grammar-driven systems should be less affected by domain changes. To investigate this hypothesis, an empirical investigation on Dutch is carried out. The performance variation of a grammar-driven versus two data-driven systems across domains is evaluated, and a simple measure to quantify domain sensitivity proposed. This will give an estimate of which parsing system is more affected by domain shifts, and thus more in need for adaptation techniques. 1 For parsing, most previous work on domain adaptation has focused on data-driven systems (Gildea, 2001; McClosky et al., 2006; Dredze et al., 2007), i.e. systems employing (constituent or dependency based) treebank grammars. Only few studies examined the adaptation of grammar-based systems (Hara et al., 2005; Plank and van Noord, 2008), i.e. systems employing a hand-crafted grammar with a statistical disambiguation component. This may be motivated by the fact that potential gains for this task are inherently bound by the grammar. Yet, domain adaptation poses a challenge for both kinds of parsing systems. But to what extent do these different kinds of systems suffer from the problem? We test th"
W10-2105,W07-2201,1,0.900086,"Missing"
W10-2105,I05-1018,0,0.484071,"ut. The performance variation of a grammar-driven versus two data-driven systems across domains is evaluated, and a simple measure to quantify domain sensitivity proposed. This will give an estimate of which parsing system is more affected by domain shifts, and thus more in need for adaptation techniques. 1 For parsing, most previous work on domain adaptation has focused on data-driven systems (Gildea, 2001; McClosky et al., 2006; Dredze et al., 2007), i.e. systems employing (constituent or dependency based) treebank grammars. Only few studies examined the adaptation of grammar-based systems (Hara et al., 2005; Plank and van Noord, 2008), i.e. systems employing a hand-crafted grammar with a statistical disambiguation component. This may be motivated by the fact that potential gains for this task are inherently bound by the grammar. Yet, domain adaptation poses a challenge for both kinds of parsing systems. But to what extent do these different kinds of systems suffer from the problem? We test the hypothesis that grammar-driven systems are less affected by domain changes. We empirically investigate this in a case-study on Dutch. Introduction Most modern Natural Language Processing (NLP) systems are"
W10-2105,N06-1020,0,0.548125,"isingly difficult to beat” (Daum´e III, 2007). In contrast, semi-supervised adaptation (i.e. no annotated resources in the new domain) is a much more realistic situation but is clearly also considerably more difficult. Current studies on semisupervised approaches show very mixed results. Dredze et al. (2007) report on “frustrating” results on the CoNLL 2007 semi-supervised adaptation task for dependency parsing, i.e. “no team was able to improve target domain performance substantially over a state-of-the-art baseline”. On the other hand, there have been positive results as well. For instance, McClosky et al. (2006) improved a statistical parser by self-training. Structural Correspondence Learning (Blitzer et al., 2006) was effective for PoS tagging and Sentiment Analysis (Blitzer et al., 2006; Blitzer et al., 2007), while only modest gains were obtained for structured output tasks like parsing. In the past decade several parsing systems for natural language have emerged, which use different methods and formalisms. For instance, systems that employ a handcrafted grammar and a statistical disambiguation component versus purely statistical data-driven systems. What they have in common is the lack of portab"
W10-2105,H05-1066,0,0.137483,"Missing"
W10-2105,C00-2137,0,0.0752196,"r instance, because the syntactic annotation of Alpino allows words to be dependent on more than a single head (’secondary edges’) (van Noord, 2006). However, such edges are ignored in the CoNLL format; just a single head per token is allowed. Furthermore, there is another simplification. As the Dutch tagger used in the CoNLL 2006 shared task did not have the concept of multiwords, the organizers chose to treat them as a single token (Buchholz and Marsi, 2006). We here follow the CoNLL 2006 task setup. To determine whether results are significant, we us the Approximate Randomization Test (see Yeh (2000)) with 1000 random shuffles. 5 µtarget p = LASpi , sdtarget = p N s i= 1 PN i=1 (LASpi − µtarget )2 p N −1 However, standard deviation is highly influenced by outliers. Furthermore, this measure does not take the source domain performance (baseline) into consideration nor the size of the target domain itself. We thus propose to measure the domain sensitivity of a system, i.e. its average domain variation (adv), as weighted average difference from the baseline (source) mean, where weights represents the size of the various domains: PN adv = i i i=1 w ∗ ∆p , PN i i=1 w with size(wi ) ∆ip = LASpi"
W10-2105,P09-1043,0,0.546416,"ork has focused on a single parsing system in isolation (Gildea, 2001; Hara et al., 2005; McClosky et al., 2006). However, there is an observable trend towards combining different parsing systems to exploit complementary strengths. For instance, Nivre and McDonald (2008) combine two data-driven systems to improve dependency accuracy. Similarly, two studies successfully combined grammar-based and datadriven systems: Sagae et al. (2007) incorporate data-driven dependencies as soft-constraint in a HPSG-based system for parsing the Wallstreet Journal. In the same spirit (but the other direction), Zhang and Wang (2009) use a deepgrammar based backbone to improve data-driven parsing accuracy. They incorporate features from the grammar-based backbone into the data-driven system to achieve better generalization across domains. This is the work most closest to ours. However, which kind of system (hand-crafted versus purely statistical) is more affected by the domain, and thus more sensitive to domain shifts? To the best of our knowledge, no study has yet addressed this issue. We thus assess the performance variation of three dependency parsing systems for Dutch across domains, and propose a simple measure to qu"
W10-2105,P08-1108,0,0.0257329,"ount of parses can be constructed for some sentences. Furthermore, the maximum entropy disambiguation component does a good job in selecting good parses from those. Accuracy is given here in terms of f-score of named dependencies. sents parses oracle arbitrary model 536 45011 95.74 76.56 89.39 (2) MST Parser (McDonald et al., 2005) is a Most previous work has focused on a single parsing system in isolation (Gildea, 2001; Hara et al., 2005; McClosky et al., 2006). However, there is an observable trend towards combining different parsing systems to exploit complementary strengths. For instance, Nivre and McDonald (2008) combine two data-driven systems to improve dependency accuracy. Similarly, two studies successfully combined grammar-based and datadriven systems: Sagae et al. (2007) incorporate data-driven dependencies as soft-constraint in a HPSG-based system for parsing the Wallstreet Journal. In the same spirit (but the other direction), Zhang and Wang (2009) use a deepgrammar based backbone to improve data-driven parsing accuracy. They incorporate features from the grammar-based backbone into the data-driven system to achieve better generalization across domains. This is the work most closest to ours. H"
W10-2105,E09-1093,1,\N,Missing
W14-1601,N13-1070,1,0.862135,"in the absence of bias and with perfect metrics – gives us the level of confidence we expect as a research community, i.e., P P V = 0.95. Significance results would thus be more reliable and reduce type 1 error. 5 newswire corpora. This is also standard practice in the machine learning community (Demsar, 2006). Poor metrics. For tasks such as POS tagging and dependency parsing, our metrics are suboptimal (Manning, 2011; Schwartz et al., 2011; Tsarfaty et al., 2012). System A and System B may perform equally well as measured by some metric, but contribute very differently to downstream tasks. Elming et al. (2013) show how parsers trained on different annotation schemes lead to very different downstream results. This suggests that being wrong with respect to a gold standard, e.g., choosing NP analysis over a “correct” DP analysis, may in some cases lead to better downstream performance. See the discussion in Manning (2011) for POS tagging. One simple approach to this problem is to report results across available metrics. If System A improves over System B wrt. most metrics, we obtain significance against the odds. POS taggers and dependency parsers should also be evaluated by their impact on downstream"
W14-1601,W03-0425,0,0.0235909,"Missing"
W14-1601,W05-0909,0,0.0206194,"01 <0.001 0.2020 0.3965 0.0020 0.2480 0.4497 0.4497 UA (b) 0.0430 0.2566 <0.001 0.0143 <0.001 0.0210 0.0543 0.0024 0.0024 0.0924 SA (b) 0.3788 0.4515 <0.001 <0.001 0.1622 0.1238 0.0585 0.2435 0.2435 0.1111 SA(w) 0.9270 0.9941 <0.001 <0.001 0.0324 0.6602 0.0562 0.9390 0.9390 0.7853 et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric"
W14-1601,I11-1100,0,0.0289698,"Missing"
W14-1601,D12-1091,0,0.0568125,"ata sets, such tests seem to be the right choice (Demsar, 2006; Søgaard, 2013). The draw-back of rank-based tests is their relatively weak statistical power. When we reduce scores to ranks, we throw away information, and rank-based tests are therefore relatively conservative, potentially leading to high type 2 error rate ( , i.e., the number of false negatives over trials). An alternative, however, are randomization-based tests such as the bootstrap test (Efron and Tibshirani, 1993) and approximate randomization (Noreen, 1989), which are the de facto standards in NLP. In this paper, we follow Berg-Kirkpatrick et al. (2012) in focusing on the bootstrap test. The bootstrap test is non-parametric and stronger than rank-based testing, i.e., introduces fewer type 2 errors. For small samples, however, it does so at the expense of a 1 In many fields, including NLP, it has become good practice to report actual p-values, but we still need to understand how significance levels relate to the probability that research findings are false, to interpret such values. The fact that we propose a new cut-off level for the ideal case with perfect metrics and no bias does not mean that we do not recommend reporting actual p-values."
W14-1601,W06-1615,0,0.0543626,"Missing"
W14-1601,P07-1034,0,0.0536678,"Missing"
W14-1601,C00-1011,0,0.168794,"Missing"
W14-1601,W04-1013,0,0.0270518,"0.3569 <0.001 <0.001 0.2020 0.3965 0.0020 0.2480 0.4497 0.4497 UA (b) 0.0430 0.2566 <0.001 0.0143 <0.001 0.0210 0.0543 0.0024 0.0024 0.0924 SA (b) 0.3788 0.4515 <0.001 <0.001 0.1622 0.1238 0.0585 0.2435 0.2435 0.1111 SA(w) 0.9270 0.9941 <0.001 <0.001 0.0324 0.6602 0.0562 0.9390 0.9390 0.7853 et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers can cherry-"
W14-1601,W06-2920,0,0.0650816,"Missing"
W14-1601,C08-1015,0,0.0300507,"Missing"
W14-1601,W03-0423,0,0.0336765,"odels from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly available runs of the two best systems from the CoNLL 2003 shared task, namely F LORIAN (Florian et al., 2003) and C HIEU -N G (Chieu and Ng, 2003).3 • • • • • • • Table 1: Evaluation data. 3 Experiments Throughout the rest of the paper, we use four running examples: a synthetic toy example and three standard experimental NLP tasks, namely POS tagging, dependency parsing and NER. The toy example is supposed to illustrate the logic behind our reasoning and is not specific to NLP. It shows how likely we are to obtain a low p-value for the difference in means when sampling from exactly the same (Gaussian) distributions. For the NLP setups (2-4), we use off-the-shelf models or available runs, as described next. 3.2 Standard comparisons POS t"
W14-1601,P11-2031,0,0.00842907,"0020 0.2480 0.4497 0.4497 UA (b) 0.0430 0.2566 <0.001 0.0143 <0.001 0.0210 0.0543 0.0024 0.0024 0.0924 SA (b) 0.3788 0.4515 <0.001 <0.001 0.1622 0.1238 0.0585 0.2435 0.2435 0.1111 SA(w) 0.9270 0.9941 <0.001 <0.001 0.0324 0.6602 0.0562 0.9390 0.9390 0.7853 et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric when reporting resu"
W14-1601,P02-1040,0,0.113383,"Twitter TA (b) 0.3445 0.3569 <0.001 <0.001 0.2020 0.3965 0.0020 0.2480 0.4497 0.4497 UA (b) 0.0430 0.2566 <0.001 0.0143 <0.001 0.0210 0.0543 0.0024 0.0024 0.0924 SA (b) 0.3788 0.4515 <0.001 <0.001 0.1622 0.1238 0.0585 0.2435 0.2435 0.1111 SA(w) 0.9270 0.9941 <0.001 <0.001 0.0324 0.6602 0.0562 0.9390 0.9390 0.7853 et al., 2012) have been proposed in addition to unlabeled and labeled attachment scores, as well as exact matches. Perhaps more famously, in machine translation and summarization it is common practice to use multiple metrics, and there exists a considerable literature on that topic (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Clark et al., 2011; Rankel et al., 2011). Even in POS tagging, some report tagging accuracies, tagging accuracies over unseen words, macro-averages over sentence-level accuracies, or number of exact matches. Table 2: POS tagging p-values across tagging accuracy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers"
W14-1601,P07-1033,0,0.0204031,"Missing"
W14-1601,P11-1157,1,0.595908,"Missing"
W14-1601,Y09-1013,0,0.0190285,"acy (TA), accuracy for unseen words (UA) and sentence-level accuracy (SA) with bootstrap (b) and Wilcoxon (w) (p < 0.05 gray-shaded). Answers Emails Newsgroups Reviews Weblogs WSJ LAS 0.020 0.083 0.049 <0.001 <0.001 <0.001 The existence of several metrics is not in itself a problem, but if researchers can cherry-pick their favorite metric when reporting results, this increases the a priori chance of establishing significance. In POS tagging, most papers report significant improvements over tagging accuracy, but some report significant improvements over tagging accuracy of unknown words, e.g., Denis and Sagot (2009) and Umansky-Pesin et al. (2010). This corresponds to the situation in psychology where researchers cherry-pick between several dependent variables (Simmons et al., 2011), which also increases the chance of finding a significant correlation. UAS <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 Table 3: Parsing p-values (M ALT-L IN VS . S TANFORD -RNN) across LAS and UAS (p < 0.05 gray-shaded). than S TANFORD, but in one case it is the other way around. If we do a Wilcoxon test over the results on the 10 data sets, following the methodology in Demsar (2006) and Søgaard (2013), the difference, which is"
W14-1601,D11-1043,0,0.0493232,"Missing"
W14-1601,W05-0908,0,0.132601,"Missing"
W14-1601,D09-1085,0,0.0396616,"Missing"
W14-1601,P11-1067,0,0.0160959,"Figure 7: PPV for different ↵ (horizontal line is PPV for p = 0.05, vertical line is ↵ for PPV=0.95). could propose a p-value cut-off at p < 0.0025. This is the cut-off that – in the absence of bias and with perfect metrics – gives us the level of confidence we expect as a research community, i.e., P P V = 0.95. Significance results would thus be more reliable and reduce type 1 error. 5 newswire corpora. This is also standard practice in the machine learning community (Demsar, 2006). Poor metrics. For tasks such as POS tagging and dependency parsing, our metrics are suboptimal (Manning, 2011; Schwartz et al., 2011; Tsarfaty et al., 2012). System A and System B may perform equally well as measured by some metric, but contribute very differently to downstream tasks. Elming et al. (2013) show how parsers trained on different annotation schemes lead to very different downstream results. This suggests that being wrong with respect to a gold standard, e.g., choosing NP analysis over a “correct” DP analysis, may in some cases lead to better downstream performance. See the discussion in Manning (2011) for POS tagging. One simple approach to this problem is to report results across available metrics. If System"
W14-1601,P13-1045,0,0.00363571,"ASKS Dep. NER al., 2011; Tjong Kim Sang and De Meulder, 2003, LDC99T42; LDC2012T13). POS tagging. We compare the performance of two state-of-the-art newswire taggers across 10 evaluation data sets (see Table 1), namely the L A POS tagger (Tsuruoka et al., 2011) and the S TAN FORD tagger (Toutanova et al., 2003), both trained on WSJ00–18. We use the publicly available pretrained models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly available runs of the two best systems from the CoNLL 2003 shared task, namely F LORIAN (Florian et al., 2003) and C HIEU -N G (Chieu and Ng, 2003).3 • • • • • • • Table 1: Evaluation data. 3 Experiments Throughout the rest of the paper, we use four running examples: a synthetic toy example and three standard experimental NLP tasks, namely POS tagging, dependency"
W14-1601,N13-1068,1,0.72957,"nalysis from previous experiments, and research findings often consist in system comparisons showing that System A is better than System B. Effect size, i.e., one system’s improvements over another, can be seen as a random variable. If the random variable follows a known distribution, e.g., a normal distribution, we can use parametric tests to estimate whether System A is better than System B. If it follows a normal distribution, we can use Student’s t-test, for example. Effect sizes in NLP are generally not normally distributed or follow any of the other wellstudied distributions (Yeh, 2000; Søgaard, 2013). The standard significance testing methods in NLP are therefore rank- or randomization-based nonparametric tests (Yeh, 2000; Riezler and Maxwell, 1 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 1–10, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics Covariates. Sometimes we may bin our results by variables that are actually predictive of the outcome (covariates) (Simmons et al., 2011). In some subfields of NLP, such as machine translation or (unsupervised) syntactic parsing, for example, it is common to report resul"
W14-1601,N03-1033,0,0.00466178,"ndency parsing. D OMAIN #W ORDS C O NLL 2007 Bio Chem POS 4k 5k • • S WITCHBOARD 4 Spoken 162k • E NGLISH W EB T REEBANK Answers Emails Newsgrs Reviews Weblogs WSJ 29k 28k 21k 28k 20k 40k • • • • • • F OSTER Twitter 3k • C O NLL 2003 News 50k Figure 1: Accuracies of L APOS VS . S TANFORD across 10 data sets. TASKS Dep. NER al., 2011; Tjong Kim Sang and De Meulder, 2003, LDC99T42; LDC2012T13). POS tagging. We compare the performance of two state-of-the-art newswire taggers across 10 evaluation data sets (see Table 1), namely the L A POS tagger (Tsuruoka et al., 2011) and the S TAN FORD tagger (Toutanova et al., 2003), both trained on WSJ00–18. We use the publicly available pretrained models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly available runs of the two best systems from the CoNLL 2003"
W14-1601,E12-1006,0,0.0571022,"or a choice between tagging accuracy and sentence-level accuracy, we see a significant improvement in 4/10 cases, i.e., for 4/10 data sets the effect is significance wrt. at least one metric. If we allow for a free choice between all three metrics (TA, UA, and SA), we observe significance in 9/10 cases. This way the existence of multiple metrics almost guarantees significant differences. Note that there are only two data sets (Answers and Spoken), where all metric differences appear significant. Dependency parsing. While there are multiple metrics in dependency parsing (Schwartz et al., 2011; Tsarfaty et al., 2012), we focus on the two standard metrics: labeled (LAS) and unlabeled attachment score (UAS) (Buchholz and Marsi, 2006). If we just consider the results in Table 3, i.e., only the comparison of M ALT-L IN VS . S TANFORD -RNN, we observe significant improvements in all cases, if we allow for a free choice between metrics. Bod (2000) provides a good example of a parsing paper evaluating models using different metrics on different test sets. Chen et al. (2008), similarly, only report UAS. NER. While macro-f1 is fairly standard in NER, we do have several available multiple metrics, including the unl"
W14-1601,W11-0328,0,0.0154577,"across 3 runs for POS and NER and 10 runs for dependency parsing. D OMAIN #W ORDS C O NLL 2007 Bio Chem POS 4k 5k • • S WITCHBOARD 4 Spoken 162k • E NGLISH W EB T REEBANK Answers Emails Newsgrs Reviews Weblogs WSJ 29k 28k 21k 28k 20k 40k • • • • • • F OSTER Twitter 3k • C O NLL 2003 News 50k Figure 1: Accuracies of L APOS VS . S TANFORD across 10 data sets. TASKS Dep. NER al., 2011; Tjong Kim Sang and De Meulder, 2003, LDC99T42; LDC2012T13). POS tagging. We compare the performance of two state-of-the-art newswire taggers across 10 evaluation data sets (see Table 1), namely the L A POS tagger (Tsuruoka et al., 2011) and the S TAN FORD tagger (Toutanova et al., 2003), both trained on WSJ00–18. We use the publicly available pretrained models from the associated websites.2 Dependency parsing. Here we compare the pre-trained linear SVM MaltParser model for English (Nivre et al., 2007b) to the compositional vector grammar model for the Stanford parser (Socher et al., 2013). For this task, we use the subset of the POS data sets that comes with Stanfordstyle syntactic dependencies (cf. Table 1), excluding the Twitter data set which we found too small to produce reliable results. NER. We use the publicly availab"
W14-1601,C10-2146,0,0.0272174,"Missing"
W14-1601,C00-2137,0,0.652215,"as error analysis from previous experiments, and research findings often consist in system comparisons showing that System A is better than System B. Effect size, i.e., one system’s improvements over another, can be seen as a random variable. If the random variable follows a known distribution, e.g., a normal distribution, we can use parametric tests to estimate whether System A is better than System B. If it follows a normal distribution, we can use Student’s t-test, for example. Effect sizes in NLP are generally not normally distributed or follow any of the other wellstudied distributions (Yeh, 2000; Søgaard, 2013). The standard significance testing methods in NLP are therefore rank- or randomization-based nonparametric tests (Yeh, 2000; Riezler and Maxwell, 1 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 1–10, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics Covariates. Sometimes we may bin our results by variables that are actually predictive of the outcome (covariates) (Simmons et al., 2011). In some subfields of NLP, such as machine translation or (unsupervised) syntactic parsing, for example, it is common"
W14-1601,E14-4014,0,\N,Missing
W14-1601,W03-0419,0,\N,Missing
W14-1601,D07-1096,0,\N,Missing
W14-2602,W10-2608,0,0.165983,"Missing"
W14-2602,S13-2053,0,0.0263725,"Missing"
W14-2602,C12-2114,0,0.0752636,"d in the corrupted training stage. This results in each data point appearing in different, corrupted versions, as visualized in Figure 1. The copying process retains more of the information in the training data, since it is unlikely that the same feature is deleted in each copy. In our experiments, we used k=5. Larger values of k resulted in longer training times without improving performance. 2. over-using certain labels (since the label distribution on the target domain might differ). One approach that has been proven to reduce overfitting is data corruption, also known as dropout training (Søgaard and Johannsen, 2012; Søgaard, 2013), which is a way of regularizing the model by randomly leaving out features. Intuitively, this approach can be viewed as coercing the learning algorithm to rely on more general, but less consistent features. Rather than learning to mainly trust the features that are highly predictive for the given training data, the algorithm is encouraged to use the less predictive features, since the highly predictive features might be deleted by the corruption. Most prior work on dropout regularization (Søgaard and Johannsen, 2012; Wang and Manning, 2012; Søgaard, 2013) has used online corru"
W14-2602,P13-2113,0,0.0304851,"tage. This results in each data point appearing in different, corrupted versions, as visualized in Figure 1. The copying process retains more of the information in the training data, since it is unlikely that the same feature is deleted in each copy. In our experiments, we used k=5. Larger values of k resulted in longer training times without improving performance. 2. over-using certain labels (since the label distribution on the target domain might differ). One approach that has been proven to reduce overfitting is data corruption, also known as dropout training (Søgaard and Johannsen, 2012; Søgaard, 2013), which is a way of regularizing the model by randomly leaving out features. Intuitively, this approach can be viewed as coercing the learning algorithm to rely on more general, but less consistent features. Rather than learning to mainly trust the features that are highly predictive for the given training data, the algorithm is encouraged to use the less predictive features, since the highly predictive features might be deleted by the corruption. Most prior work on dropout regularization (Søgaard and Johannsen, 2012; Wang and Manning, 2012; Søgaard, 2013) has used online corruptions, i.e., th"
W14-2602,H05-1044,0,0.0650229,"Missing"
W14-2602,J92-4003,0,\N,Missing
W14-2602,P07-1056,0,\N,Missing
W14-2602,S13-2052,0,\N,Missing
W15-1617,ide-etal-2008-masc,0,0.0739475,"Missing"
W15-1617,D14-1108,0,0.0276301,"istribution of the Wall Street Journal dependency treebank (Bies et al., 2012; Petrov and McDonald, 2012). 2. Answers: The Yahoo! Answers test section from the English Web Treebank (Bies et al., 2012; Petrov and McDonald, 2012). 3. Spoken: The Switchboard corpus section of the MASC corpus (Ide et al., 2008). 4. Fiction: The literature subset of the test section of the Brown test set from CoNLL 2008 (Surdeanu et al., 2008), which encompasses the fiction, mystery, science-fiction, romance and humor categories of the Brown corpus. 5. Twitter: The test section of the Tweebank dependency treebank (Kong et al., 2014). WSJ is the perceived-of-as-canonical dataset. Answers and Twitter are datasets of social media texts from two different social media. We include Switchboard as an example of spoken language (transcriptions of telephone conversations), and Fiction to incorporate carefully edited (i.e., not user-generated) text that is lexically and syntactically different to newswire. From each corpus, we randomly selected 50 sentences and doubly-annotated them. D OMAIN A0 A1 MV WSJ Twitter Answers Spoken Fiction 99 88 92 100 96 76 72 79 86 76 72 56 63 81 78 Table 2: Frequency counts for arguments in the anno"
W15-1617,P05-1012,0,0.0577417,"ce (average per-edge confidence). between sentence length and sentence-wise agreement for all 250 annotated sentences, however, found the correlation to be low (0.1364). Consequently, it seems unlikely that sentence length had a major effect on our annotations. We may speculate that annotation disagreements can be due to rare linguistic phenomena and linguistic outliers. In Table 4 we show the correlation per domain between sentence-wise agreement and dependency parsing confidence. We have obtained this confidence from the edge-wise confidence scores provided by an instance of the MST parser (McDonald et al., 2005) trained on WSJ. The parsing confidence for a sentence is obtained from the average of the edges that have received a label (A0, MV, A1) by the annotators, averaged between the two annotators. The correlation for newswire is high, but not the highest, because despite high parsing confidence, annotation agreement is rather low. On the other end, the lowest correlation between parser confidence and agreement is for Answers, which has the highest inter-annotator agreement. These results, in our view, indicate that what makes annotating social media text hard (at times) is not what makes annotatin"
W15-1617,W08-2121,0,0.122021,"Missing"
W15-1831,P07-1007,0,0.0315542,"2008; Marcheggiani and Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan and Ng (2007) and Xiao and Guo (2013). 7 Conclusions The systems that use the M AX selection strategy have a strong bias for the longest possible sentence, resulting from the low probability values obtained when calculating the prediction confidence of very long sequences. With few exceptions (e.g. an 11-word sentence on the 5th iteration for AM1 ), the systems exhaust the maximum-length sentences, and proceed to choose the longest available, and so forth. We do not take our individual annotator’s bias into consideration, but we believe that such bias plays a minor role in the differences of performance be"
W15-1831,D14-1097,0,0.0143127,"e take a small seed of data points, train a sequential labeling, and iterate over an unlabeled pool of data, selecting the data points our labeler is least confident about. In the AL literature, the selected data points are often those close to a decision boundary or those most likely to decrease overall uncertainty. This obviously leads to biased sampling, which can sometimes be avoided using different techniques, e.g., by exploiting cluster structure in the data. Generally, active learning for sequential labeling has received less attention than for classification (Settles and Craven, 2008; Marcheggiani and Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan and Ng (2007) and Xiao and Guo (2"
W15-1831,D14-1104,1,0.838297,"ss attention than for classification (Settles and Craven, 2008; Marcheggiani and Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan and Ng (2007) and Xiao and Guo (2013). 7 Conclusions The systems that use the M AX selection strategy have a strong bias for the longest possible sentence, resulting from the low probability values obtained when calculating the prediction confidence of very long sequences. With few exceptions (e.g. an 11-word sentence on the 5th iteration for AM1 ), the systems exhaust the maximum-length sentences, and proceed to choose the longest available, and so forth. We do not take our individual annotator’s bias into consideration, but we believe that such bia"
W15-1831,W10-0104,0,0.0158409,"or those most likely to decrease overall uncertainty. This obviously leads to biased sampling, which can sometimes be avoided using different techniques, e.g., by exploiting cluster structure in the data. Generally, active learning for sequential labeling has received less attention than for classification (Settles and Craven, 2008; Marcheggiani and Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan and Ng (2007) and Xiao and Guo (2013). 7 Conclusions The systems that use the M AX selection strategy have a strong bias for the longest possible sentence, resulting from the low probability values obtained when calculating the prediction confidence of very long sequences. With few"
W15-1831,D08-1112,0,0.0559162,"here are very standard. We take a small seed of data points, train a sequential labeling, and iterate over an unlabeled pool of data, selecting the data points our labeler is least confident about. In the AL literature, the selected data points are often those close to a decision boundary or those most likely to decrease overall uncertainty. This obviously leads to biased sampling, which can sometimes be avoided using different techniques, e.g., by exploiting cluster structure in the data. Generally, active learning for sequential labeling has received less attention than for classification (Settles and Craven, 2008; Marcheggiani and Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan"
W15-1831,W13-3501,0,0.0135807,"Artieres, 2014). Our experiments were simple, and several things can be done to improve results, i.e., by reducing sampling bias. In particular, several techniques have been introduced for improving out-of-domain performance using active learning. Rai et al. (2010) perform target-domain AL with a seed of source-domain data. Among other things, they propose to use the source and target unlabeled data to train a classifier to learn what target domain data points are similar to the source domain, in a way similar to Plank et al. (2014). For more work along these lines, see Chan and Ng (2007) and Xiao and Guo (2013). 7 Conclusions The systems that use the M AX selection strategy have a strong bias for the longest possible sentence, resulting from the low probability values obtained when calculating the prediction confidence of very long sequences. With few exceptions (e.g. an 11-word sentence on the 5th iteration for AM1 ), the systems exhaust the maximum-length sentences, and proceed to choose the longest available, and so forth. We do not take our individual annotator’s bias into consideration, but we believe that such bias plays a minor role in the differences of performance between M AX and S AMPLE."
W15-2913,K15-1011,1,0.686596,"ur experiments show that social media data can provide sufficient linguistic evidence to reliably predict two of four personality dimensions. 1 Introduction Individual author attributes play an important role in customer modeling, as well as in business intelligence. In either task, Natural Language Processing (NLP) is increasingly used to analyze and classify extra-linguistic features based on textual input. Extra-linguistic and linguistic features are assumed to be sufficiently correlated to be predictive of each other, which in practice allows for mutual inference (Pennebaker et al., 2003; Johannsen et al., 2015). A whole body of work in NLP is concerned with attribute prediction from linguistic features (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Eisenstein et al., 2011; Volkova et al., 2013; Alowibdi et al., 2013; Ciot et al., 2013; 92 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 92–98, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. I NTUITIVE –S ENSING , T HINKING –F EELING , J UDGING –P ERCEIVING), e.g., INTJ, ENTJ, etc., amounting to 16 different types. MBTIs hav"
W15-2913,luyckx-daelemans-2008-personae,0,0.130146,"hough. See Henrich et al. (2010). 3 4 94 Using the sklearn toolkit. Tokenizer from: http://wwbp.org/ 5 Results 0.80 Table 3 shows the prediction accuracy for a majority-class baseline and our models on the full data set (10-fold cross-validation). While the model clearly improves on the I–E and F–T distinctions, we see no improvements over the baseline for S–N, and even a slight drop for P–J. This indicates that for the latter two dimensions, we either do not have the right features, or there is not linguistic evidence for them, given that they are more related to perception. The results from Luyckx and Daelemans (2008) on Dutch essays also suggest that P–J is difficult to learn. Given the heavy gender-skew of our data, we run additional experiments in which we control for gender. The gender-controlled dataset contains 1070 authors. The results in Table 4 show the same tendency as in the previous setup. Majority System 0.70 0.65 0.60 0.55 0.50 0.55 64.1 72.5 77.5 77.4 58.4 61.2 58.8 55.4 0.50 64.9 72.1 79.6 79.5 51.8 54.0 59.4 58.2 2000 classifier I-E baseline I-E classifier T-F baseline T-F 0.60 0 500 1000 1500 2000 Figure 2: Learning curves and majority baselines for I–E and T–F on whole data set (top) and"
W15-2913,W15-1202,0,0.0409612,"300 Copenhagen S dirk.hovy@hum.ku.dk bplank@cst.dk Abstract Volkova et al., 2015). Apart from demographic features, such as age or gender, there is also a growing interest in personality types. Predicting personality is not only of interest for commercial applications and psychology, but also for health care. Recent work by Preot¸iuc-Pietro et al. (2015) investigated the link between personality types, social media behavior, and psychological disorders, such as depression and posttraumatic stress disorder. They found that certain personality traits are predictive of mental illness. Similarly, Mitchell et al. (2015) show that linguistic traits are predictive of schizophrenia. However, as pointed out by Nowson and Gill (2014), computational personality recognition is limited by the availability of labeled data, which is expensive to annotate and often hard to obtain. Given the wide array of possible personality types, limited data size is a problem, since lowprobability types and combinations will not occur in statistically significant numbers. In addition, many existing data sets are comprised of written essays, which usually contain highly canonical language, often of a specific topic. Such controlled s"
W15-2913,W11-1515,0,0.414681,"Missing"
W15-2913,D13-1114,0,0.0375757,"as in business intelligence. In either task, Natural Language Processing (NLP) is increasingly used to analyze and classify extra-linguistic features based on textual input. Extra-linguistic and linguistic features are assumed to be sufficiently correlated to be predictive of each other, which in practice allows for mutual inference (Pennebaker et al., 2003; Johannsen et al., 2015). A whole body of work in NLP is concerned with attribute prediction from linguistic features (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Eisenstein et al., 2011; Volkova et al., 2013; Alowibdi et al., 2013; Ciot et al., 2013; 92 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 92–98, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. I NTUITIVE –S ENSING , T HINKING –F EELING , J UDGING –P ERCEIVING), e.g., INTJ, ENTJ, etc., amounting to 16 different types. MBTIs have the distinct advantage of being readily available in large quantities on social media. We are aware of the ongoing discussion in the psychological literature about the limited expressiveness of MBTI, and a preference for Big Five"
W15-2913,P11-1137,0,0.0233605,"author attributes play an important role in customer modeling, as well as in business intelligence. In either task, Natural Language Processing (NLP) is increasingly used to analyze and classify extra-linguistic features based on textual input. Extra-linguistic and linguistic features are assumed to be sufficiently correlated to be predictive of each other, which in practice allows for mutual inference (Pennebaker et al., 2003; Johannsen et al., 2015). A whole body of work in NLP is concerned with attribute prediction from linguistic features (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Eisenstein et al., 2011; Volkova et al., 2013; Alowibdi et al., 2013; Ciot et al., 2013; 92 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 92–98, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. I NTUITIVE –S ENSING , T HINKING –F EELING , J UDGING –P ERCEIVING), e.g., INTJ, ENTJ, etc., amounting to 16 different types. MBTIs have the distinct advantage of being readily available in large quantities on social media. We are aware of the ongoing discussion in the psychological literature about th"
W15-2913,N13-1037,0,0.0136457,"ata sets are comprised of written essays, which usually contain highly canonical language, often of a specific topic. Such controlled settings inhibit the expression of individual traits much more than spontaneous language. In this work, we take a data-driven approach to personality identification, to avoid both the limitation of small data samples and a limited vocabulary. We use the large amounts of personalized data voluntarily produced on social media (e.g., Twitter) to collect sufficient amounts of data. Twitter is highly non-canonical, and famous for an almost unlimited vocabulary size (Eisenstein, 2013; Fromreide et al., 2014). In order to enable data-driven personality research, we combine this data source with self-assessed Myers-Briggs Type Indicators (Briggs Myers and Myers, 2010), denoted MBTIs. Myers-Briggs uses four binary dimensions to classify users (I NTROVERT–E XTROVERT, Psychology research suggests that certain personality traits correlate with linguistic behavior. This correlation can be effectively modeled with statistical natural language processing techniques. Prediction accuracy generally improves with larger data samples, which also allows for more lexical features. Most e"
W15-2913,fromreide-etal-2014-crowdsourcing,1,0.690975,"ised of written essays, which usually contain highly canonical language, often of a specific topic. Such controlled settings inhibit the expression of individual traits much more than spontaneous language. In this work, we take a data-driven approach to personality identification, to avoid both the limitation of small data samples and a limited vocabulary. We use the large amounts of personalized data voluntarily produced on social media (e.g., Twitter) to collect sufficient amounts of data. Twitter is highly non-canonical, and famous for an almost unlimited vocabulary size (Eisenstein, 2013; Fromreide et al., 2014). In order to enable data-driven personality research, we combine this data source with self-assessed Myers-Briggs Type Indicators (Briggs Myers and Myers, 2010), denoted MBTIs. Myers-Briggs uses four binary dimensions to classify users (I NTROVERT–E XTROVERT, Psychology research suggests that certain personality traits correlate with linguistic behavior. This correlation can be effectively modeled with statistical natural language processing techniques. Prediction accuracy generally improves with larger data samples, which also allows for more lexical features. Most existing work on personali"
W15-2913,W15-1203,0,0.0203999,"Missing"
W15-2913,P11-1077,0,0.240969,"personality dimensions. 1 Introduction Individual author attributes play an important role in customer modeling, as well as in business intelligence. In either task, Natural Language Processing (NLP) is increasingly used to analyze and classify extra-linguistic features based on textual input. Extra-linguistic and linguistic features are assumed to be sufficiently correlated to be predictive of each other, which in practice allows for mutual inference (Pennebaker et al., 2003; Johannsen et al., 2015). A whole body of work in NLP is concerned with attribute prediction from linguistic features (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Eisenstein et al., 2011; Volkova et al., 2013; Alowibdi et al., 2013; Ciot et al., 2013; 92 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 92–98, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. I NTUITIVE –S ENSING , T HINKING –F EELING , J UDGING –P ERCEIVING), e.g., INTJ, ENTJ, etc., amounting to 16 different types. MBTIs have the distinct advantage of being readily available in large quantities on social media. We are aware of the ongoing discu"
W15-2913,verhoeven-daelemans-2014-clips,0,0.0225779,"Missing"
W15-2913,D13-1187,0,0.0494234,"important role in customer modeling, as well as in business intelligence. In either task, Natural Language Processing (NLP) is increasingly used to analyze and classify extra-linguistic features based on textual input. Extra-linguistic and linguistic features are assumed to be sufficiently correlated to be predictive of each other, which in practice allows for mutual inference (Pennebaker et al., 2003; Johannsen et al., 2015). A whole body of work in NLP is concerned with attribute prediction from linguistic features (Rosenthal and McKeown, 2011; Nguyen et al., 2011; Eisenstein et al., 2011; Volkova et al., 2013; Alowibdi et al., 2013; Ciot et al., 2013; 92 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 92–98, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. I NTUITIVE –S ENSING , T HINKING –F EELING , J UDGING –P ERCEIVING), e.g., INTJ, ENTJ, etc., amounting to 16 different types. MBTIs have the distinct advantage of being readily available in large quantities on social media. We are aware of the ongoing discussion in the psychological literature about the limited expressivene"
W16-1706,P13-1004,0,0.0701839,"Missing"
W16-1706,S14-1001,1,0.886972,"Missing"
W16-1706,N13-1062,0,0.0585746,"Missing"
W16-1706,W15-2711,1,0.817393,"Missing"
W16-1706,W15-1806,1,0.891015,"Missing"
W16-1706,N15-1152,1,0.559133,"Missing"
W16-1706,H94-1046,0,0.475658,"Missing"
W16-1706,Q14-1025,0,0.136872,"Missing"
W16-1706,passonneau-etal-2010-word,0,0.0561585,"Missing"
W16-1706,L16-1136,1,0.889351,"Missing"
W16-1706,E14-1078,1,0.909187,"Cognition, University of Groningen, The Netherlands ♠ Univ. Paris Diderot, Sorbonne Paris Cit – Alpage, INRIA, France hector.martinez-alonso@inria.fr,anders@johannsen.com,b.plank@rug.nl Abstract tion in agreement, providing smaller losses when a classifier training decision makes a misclassification that matches with human disagreement. For example, the loss for predicting a particle instead of an adverb is smaller than the loss for predicting a noun instead of an adverb, because the particle/adverb confusion is fairly common among annotators (Sec. 3). In this article, we apply the method of Plank et al. (2014a) to a semantic sequence-prediction task, namely supersense tagging (SST). SST is considered a more difficult task than POS tagging, because the semantic classes are more dependent on world knowledge, and the number of supersenses is higher than the number of POS labels. We experiment with different methods to calculate the label-wise agreement (Sec. 3.1), and apply these methods to datasets in two languages, namely English and Danish (Sec. 3.2). Moreover, we also perform cross-linguistic experiments to assess how much of the annotation variation in one language can be applied to another. Lin"
W16-1706,P14-2083,1,0.885319,"Cognition, University of Groningen, The Netherlands ♠ Univ. Paris Diderot, Sorbonne Paris Cit – Alpage, INRIA, France hector.martinez-alonso@inria.fr,anders@johannsen.com,b.plank@rug.nl Abstract tion in agreement, providing smaller losses when a classifier training decision makes a misclassification that matches with human disagreement. For example, the loss for predicting a particle instead of an adverb is smaller than the loss for predicting a noun instead of an adverb, because the particle/adverb confusion is fairly common among annotators (Sec. 3). In this article, we apply the method of Plank et al. (2014a) to a semantic sequence-prediction task, namely supersense tagging (SST). SST is considered a more difficult task than POS tagging, because the semantic classes are more dependent on world knowledge, and the number of supersenses is higher than the number of POS labels. We experiment with different methods to calculate the label-wise agreement (Sec. 3.1), and apply these methods to datasets in two languages, namely English and Danish (Sec. 3.2). Moreover, we also perform cross-linguistic experiments to assess how much of the annotation variation in one language can be applied to another. Lin"
W16-1706,W96-0213,0,0.590461,"Missing"
W16-1706,W08-1203,0,0.483304,"Missing"
W16-1706,D11-1141,0,0.0139462,"Missing"
W16-1706,N01-1010,0,0.170134,"Missing"
W16-1706,S15-1007,0,\N,Missing
W16-1706,K15-1033,1,\N,Missing
W17-1219,P12-3005,0,0.00882173,"tions showed very high results (Cavnar et al., 1994; Dunning, 1994), while the more recent models achieve nearperfect accuracies. Distinguishing closely-related languages, however, still remains a challenge. The Discriminating between similar languages (DSL) shared task (Zampieri et al., 2017) is aimed at solving this problem. For this year’s task our team (mm lct) built a model that discriminates between 14 languages or language varieties across 6 language groups (which had two or three languages or language varieties in them).1 The most popular of the more recent systems, such as langid.py (Lui and Baldwin, 2012) and CLD/CLD22 produce very good results based on Even though a number of researches in dialect identification have been conducted, (Tiedemann and Ljubeˇsi´c, 2012; Lui and Cook, 2013; Maier and G´omez-Rodriguez, 2014; Ljubeˇsi´c and Kranjcic, 2015, among many others), they mostly deal with particular language groups or language variations. We saw as our goal to create a language identifier that is able to produce comparable results for languages within all provided groups with the same set of features for every language group, so that it can be expanded outside those languages provided by the"
W17-1219,U13-1003,0,0.0375357,"remains a challenge. The Discriminating between similar languages (DSL) shared task (Zampieri et al., 2017) is aimed at solving this problem. For this year’s task our team (mm lct) built a model that discriminates between 14 languages or language varieties across 6 language groups (which had two or three languages or language varieties in them).1 The most popular of the more recent systems, such as langid.py (Lui and Baldwin, 2012) and CLD/CLD22 produce very good results based on Even though a number of researches in dialect identification have been conducted, (Tiedemann and Ljubeˇsi´c, 2012; Lui and Cook, 2013; Maier and G´omez-Rodriguez, 2014; Ljubeˇsi´c and Kranjcic, 2015, among many others), they mostly deal with particular language groups or language variations. We saw as our goal to create a language identifier that is able to produce comparable results for languages within all provided groups with the same set of features for every language group, so that it can be expanded outside those languages provided by the DSL shared task without any changes other than to the training corpus – as to make the system as language-independent and universal as possible. Most of the language identifiers that"
W17-1219,W16-4802,0,0.697704,"and Selamat, 2011; Zampieri and Gebre, 2012) and combinations of character and word n-grams (Milne et al., 2012; 1 The term language shall henceforth be used for both ‘language’ and ‘language variety’. 2 https://github.com/CLD2Owners/cld2 156 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 156–163, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Vogel and Tresner-Kirsch, 2012; Goldszmidt et al., 2013), also including top systems from previous DSL shared tasks (Goutte and L´eger, 2015; Malmasi and Dras, 2015; C¸o¨ ltekin and Rama, 2016). The overviews of the previous DSL shared tasks (Zampieri et al., 2014; Zampieri et al., 2015; Goutte et al., 2016) showed that SVMs always produce some of the top results in this task, especially when tested on same-domain datasets (C¸o¨ ltekin and Rama, 2016). Thus, we chose to put our efforts into improving upon SVM approaches, but still decided to experiment with an neural network to see if we could get comparable results, while using fewer features and reducing the chance of overfitting. The popularity of using NNs for NLP tasks is growing. A few neural language identifiers already exist"
W17-1219,W14-4204,0,0.0243981,"Missing"
W17-1219,W16-4831,0,0.0613836,"6). Thus, we chose to put our efforts into improving upon SVM approaches, but still decided to experiment with an neural network to see if we could get comparable results, while using fewer features and reducing the chance of overfitting. The popularity of using NNs for NLP tasks is growing. A few neural language identifiers already exist as well (Tian and Suontausta, 2003; Takc¸i and Ekinci, 2012; Sim˜oes et al., 2014, among others), however on average traditional systems still seem to outperform them. The results of the DSL 2016 shared task also show the same tendency overall (Bjerva, 2016; Cianflone and Kosseim, 2016; C¸o¨ ltekin and Rama, 2016; Malmasi et al., 2016). new addition is the Farsi language group, with the two variations Persian (fa-IR) and Dari (fa-AF). Thus, this year’s version contains 14 languages belonging to 6 groups: • BCS: containing Bosnian, Croatian and Serbian; • Spanish: containing Argentine, Peninsular and Peruvian varieties; • Farsi: containing Afghan Farsi (or Dari) and Iranian Farsi (or Persian); • French: containing Canadian and Hexagonal varieties; • Indonesian and Malay; and • Portuguese: containing Brazilian and European varieties. In this section, we first describe the dat"
W17-1219,W15-5407,0,0.136232,"dels (Carter et al., 2011; Ng and Selamat, 2011; Zampieri and Gebre, 2012) and combinations of character and word n-grams (Milne et al., 2012; 1 The term language shall henceforth be used for both ‘language’ and ‘language variety’. 2 https://github.com/CLD2Owners/cld2 156 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 156–163, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Vogel and Tresner-Kirsch, 2012; Goldszmidt et al., 2013), also including top systems from previous DSL shared tasks (Goutte and L´eger, 2015; Malmasi and Dras, 2015; C¸o¨ ltekin and Rama, 2016). The overviews of the previous DSL shared tasks (Zampieri et al., 2014; Zampieri et al., 2015; Goutte et al., 2016) showed that SVMs always produce some of the top results in this task, especially when tested on same-domain datasets (C¸o¨ ltekin and Rama, 2016). Thus, we chose to put our efforts into improving upon SVM approaches, but still decided to experiment with an neural network to see if we could get comparable results, while using fewer features and reducing the chance of overfitting. The popularity of using NNs for NLP tasks is growing. A few neural langu"
W17-1219,W15-5403,0,0.0155691,"ata, as supported by the results – during-development performance was almost the same as the performance on the test set. All instances come from short newspaper texts. However whereas last year’s version of the DSLCC contained Mexican Spanish, this year’s version has Peruvian Spanish (es-PE). Another As our first, most promising run we have developed and submitted a two-layer classifier, which first predicts for all instances which language group it belongs to, and then classifies the specific languages within the guessed language groups. This method has been used by DSL participants before (Franco-Salvador et al., 2015; Nisioi et al., 2016), and has shown to have a positive impact on the performance. Adopting this method, we have built a combination of SVMs with linear kernels. The first SVM is for deciding on the language group to which the language belongs. As features it uses character-based uni- to 6-grams (including whitespace and punctuation characters) weighted 3 Methodology and Data 157 Language Croatian Bosnian Serbian Code Training Instances Tokens Dev. Instances Tokens hr bs sr 18,000 18,000 18,000 658,492 555,680 606,403 2,000 2,000 2,000 72,731 61,574 66,494 Argentine Spanish Peninsular Spanish"
W17-1219,W16-4801,0,0.304098,"Missing"
W17-1219,W16-4822,0,0.0471897,"onstraints we did not fully explore many directions here, like feature space, hyperparameters or alternative models, but overall NN seemed less promising for this task. L = (1 − λ)L1 + λL2 , where λ = 0.1 We also experimented with NNs, in particular, an NN with a multi-task objective. The idea was to take advantage of language group information to guide learning. This represents a complimentary approach to run 1. Our preliminary experiments confirmed earlier findings that NN-based approaches are outperformed by more simple linear models for language identification (C¸o¨ ltekin and Rama, 2016; Gamallo et al., 2016). We compared recurrent NNs to simpler models based on continuous bag of word (CBOW) representations (Mikolov et al., 2013), which are similar to feedforward NNs and simply take the mean vector of the input embeddings as input representation. CBOW was not only quicker to train, it also outperformed their RNN/LSTM counterparts, thus resulting in our final submission. 4 Results Based on absolute scores, our first system (SVM with grouping) performed second best in the DSL shared task (Zampieri et al., 2017) with an accuracy of 0.9254. Both our other systems also performed substantially higher th"
W17-1219,W15-5413,0,0.150728,"Missing"
W17-1219,W16-4830,0,0.0154439,"ts – during-development performance was almost the same as the performance on the test set. All instances come from short newspaper texts. However whereas last year’s version of the DSLCC contained Mexican Spanish, this year’s version has Peruvian Spanish (es-PE). Another As our first, most promising run we have developed and submitted a two-layer classifier, which first predicts for all instances which language group it belongs to, and then classifies the specific languages within the guessed language groups. This method has been used by DSL participants before (Franco-Salvador et al., 2015; Nisioi et al., 2016), and has shown to have a positive impact on the performance. Adopting this method, we have built a combination of SVMs with linear kernels. The first SVM is for deciding on the language group to which the language belongs. As features it uses character-based uni- to 6-grams (including whitespace and punctuation characters) weighted 3 Methodology and Data 157 Language Croatian Bosnian Serbian Code Training Instances Tokens Dev. Instances Tokens hr bs sr 18,000 18,000 18,000 658,492 555,680 606,403 2,000 2,000 2,000 72,731 61,574 66,494 Argentine Spanish Peninsular Spanish Peruvian Spanish es-A"
W17-1219,L16-1284,0,0.144729,"Missing"
W17-1219,E17-1087,0,0.10945,"Missing"
W17-1219,C12-1160,0,0.270537,"Missing"
W17-1219,W14-5307,0,0.102411,"Missing"
W17-1219,W15-5401,0,0.203181,"Missing"
W17-1219,W17-1201,0,0.0844823,"Missing"
W17-4404,N13-1037,0,0.176312,"stead, its normalized form is analyzed correctly, as shown in Figure 1. While being a promising direction, we see at least two issues with the assessment of normalization as a successful step in POS tagging noncanonical text. Firstly, normalization experiments are usually carried out assuming that the tokens to be normalized are already detected (gold error detection). Thus little is known on how normalization impacts tagging accuracy in a realworld scenario (not assuming gold error detection). Secondly, normalization is one way to go about processing non-canonical data, but not the only one (Eisenstein, 2013; Plank, 2016). Indeed, alternative approaches leverage the abundance of unlabeled data kept in its raw form. For instance, Introduction Non-canonical data poses a series of challenges to Natural Language Processing, as reflected in large performance drops documented in a variety of tasks, e.g., on POS tagging (Gimpel et al., 2011; Hovy et al., 2014), parsing (McClosky, 2010; Foster et al., 2011) and named entity recognition (Ritter et al., 2011). In this paper we focus on POS tagging and on a particular source of non-canonical language, namely Twitter data. One obvious way to tackle the probl"
W17-4404,I11-1100,0,0.0249902,"Missing"
W17-4404,P11-2008,0,0.146694,"Missing"
W17-4404,W15-4319,0,0.125726,"Missing"
W17-4404,P11-1038,0,0.775345,"latter approach yields a tagging model that is competitive with a Twitter state-of-the-art tagger. 1 NN pix NNS pictures NN comming VBG coming NNS tomoroe NN tomorrow Figure 1: Example tweet from the test data, raw and normalized form, tagged with Stanford NLP. different times, typically a new specifically dedicated tool needs to be created. The alternative route is to take a general purpose state-of-the-art POS tagger and adapt it to successfully tag non-canonical data. In the case of Twitter, one way to go about this is lexical normalization. It is the task of detecting “ill-formed” words (Han and Baldwin, 2011) and replacing them with their canonical counterpart. To illustrate why this might help, consider the following tweet: “new pix comming tomoroe”. An off-the-shelf system such as the Stanford NLP suite1 makes several mistakes on the raw input, e.g., the verb ‘comming’ as well as the plural noun ‘pix’ are tagged as singular noun. Instead, its normalized form is analyzed correctly, as shown in Figure 1. While being a promising direction, we see at least two issues with the assessment of normalization as a successful step in POS tagging noncanonical text. Firstly, normalization experiments are usu"
W17-4404,hovy-etal-2014-pos,1,0.851806,"Missing"
W17-4404,L16-1012,0,0.0197501,"orming POS tagging. Finally, normalization for POS tagging is certainly not limited to non-canonical data stemming from social media. Indeed, another stream of related work is focused on historical data, usually originating from the 15th till the 18th century. The motivation behind this is that in order to apply current language processing tools, the texts need to be normalized first, as spelling has changed through time. Experiments on POS tagging historical data that was previously normalized have been investigated for English (Yang and Eisenstein, 2016), German (Bollmann, 2013), and Dutch (Hupkes and Bod, 2016; Tjong Kim Sang, 2016). In this latter work, different methods of ‘translating’ historical Dutch texts to modern Dutch are explored, and a vocabulary lookup-based approach appears to work best.8 In this paper we focused on normalization and POS tagging for Twitter data only. gers obtain the highest performance. The ARK tagger has difficulties with prepositions (P), which are mistagged as numerals ($). These are almost all cases of ‘2’ and ‘4’, which represent Twitter slang for ‘to’ and ‘for’, respectively. Our system performs a lot better on these, due to the normalization model as already ob"
W17-4404,P16-2067,1,0.840559,"e-of-art performance on the erroneous tokens (using gold error detection) on the LexNorm dataset (Han and Baldwin, 2011) as well as state-of-art on another corpus which is usually benchmarked without assuming gold error detection (Baldwin et al., 2015). We refer the reader to the paper (van der Goot and van Noord, 2017) for further details. To obtain a more detailed view of the effect of normalization on POS tagging, we investigate four experimental setups: We use B ILTY, an off-the-shelf bi-directional Long Short-Term Memory (bi-LSTM) tagger which utilizes both word and character embeddings (Plank et al., 2016). The tagger is trained on 1,576 training tweets (Section 2.1). We tune the parameters of the POS tagger on the development set to derive the following hyperparameter setup, which we use throughout the rest of the experiments: 10 epochs, 1 bi-LSTM layer, 100 input dimensions for words, 256 for characters, σ=0.2, constant embeddings initializer, Adam trainer, and updating embeddings during backpropagation.4 3 • normalizing only unknown words; To Normalize • considering all words: the model decides whether a word should be normalized or not; First we evaluate the impact of normalization on the P"
W17-4404,P14-3012,0,0.0196513,"ed as training in a semi-supervised setting (Section 4). For all experiments we use existing datasets as well as newly created resources, cf. Section 2.1. The POS model used is described in Section 2.2. 2 Some tags are rare, like M and Y. In fact, M occurs only once in T EST L; Y never occurs in D EV and only once in T EST L and three times in T EST O. Therefore our confusion matrices (over D EV and T EST O, respectively) have different number of labels on the axes. 3 https://en.wikipedia.org/wiki/Most_ common_words_in_English 32 2.2 Model We train the normalization model on 2,577 tweets from Li and Liu (2014). Our model (van der Goot and van Noord, 2017) achieves state-of-art performance on the erroneous tokens (using gold error detection) on the LexNorm dataset (Han and Baldwin, 2011) as well as state-of-art on another corpus which is usually benchmarked without assuming gold error detection (Baldwin et al., 2015). We refer the reader to the paper (van der Goot and van Noord, 2017) for further details. To obtain a more detailed view of the effect of normalization on POS tagging, we investigate four experimental setups: We use B ILTY, an off-the-shelf bi-directional Long Short-Term Memory (bi-LSTM"
W17-4404,D11-1141,0,0.471888,"alworld scenario (not assuming gold error detection). Secondly, normalization is one way to go about processing non-canonical data, but not the only one (Eisenstein, 2013; Plank, 2016). Indeed, alternative approaches leverage the abundance of unlabeled data kept in its raw form. For instance, Introduction Non-canonical data poses a series of challenges to Natural Language Processing, as reflected in large performance drops documented in a variety of tasks, e.g., on POS tagging (Gimpel et al., 2011; Hovy et al., 2014), parsing (McClosky, 2010; Foster et al., 2011) and named entity recognition (Ritter et al., 2011). In this paper we focus on POS tagging and on a particular source of non-canonical language, namely Twitter data. One obvious way to tackle the problem of processing non-canonical data is to build taggers that are specifically tailored to such text. A prime example is the ARK POS tagger, designed especially to process English Twitter data (Gimpel et al., 2011; Owoputi et al., 2013), on which it achieves state-of-the-art results. One drawback of this approach is that non-canonical data is not all of the same kind, so that for non-canonical non-Twitter data or even collections of Twitter sample"
W17-4404,N15-1142,0,0.0124374,"tion is to leave the text as is, and exploit very large amounts of raw data via semi-supervised learning. The rationale behind this is the following: provided the size of the data is sufficient, a model can be trained to naturally learn the POS tags of noisy data. 4.1 Effect of Word Embeddings An easy and effective use of word embeddings in neural network approaches is to use them to initialize the word lookup parameters. We train a skip-gram word embeddings model using word2vec (Mikolov et al., 2013) on 760M tweets (as described in Section 3.1). We also experiment with structured skip-grams (Ling et al., 2015), an adaptation of word2vec which takes word order into account. It has been shown to be beneficial for syntactically oriented tasks, like POS tagging. Therefore we want to evaluate structured skip-grams as well. The normalization model uses word embeddings with a window size of 1; we compare this Effect of Self-training WINDOW SIZE SKIPG . STRUCT. SKIPG . 1 5 88.14 (±.30) 88.51 (±.24) 87.56 (±.08) 88.11 (±.49) Table 2: Accuracy on raw D EV: various pretrained skip-gram embeddings for initialization. 34 training data where only tweets that do not contain named entities are selected. Hence, we"
W17-4404,N16-1157,0,0.0576384,"ting systems incorporated any normalization strategy before performing POS tagging. Finally, normalization for POS tagging is certainly not limited to non-canonical data stemming from social media. Indeed, another stream of related work is focused on historical data, usually originating from the 15th till the 18th century. The motivation behind this is that in order to apply current language processing tools, the texts need to be normalized first, as spelling has changed through time. Experiments on POS tagging historical data that was previously normalized have been investigated for English (Yang and Eisenstein, 2016), German (Bollmann, 2013), and Dutch (Hupkes and Bod, 2016; Tjong Kim Sang, 2016). In this latter work, different methods of ‘translating’ historical Dutch texts to modern Dutch are explored, and a vocabulary lookup-based approach appears to work best.8 In this paper we focused on normalization and POS tagging for Twitter data only. gers obtain the highest performance. The ARK tagger has difficulties with prepositions (P), which are mistagged as numerals ($). These are almost all cases of ‘2’ and ‘4’, which represent Twitter slang for ‘to’ and ‘for’, respectively. Our system performs a lot bet"
W17-4404,W17-1410,0,0.0368357,"Missing"
W17-4404,N10-1004,0,0.01616,"ttle is known on how normalization impacts tagging accuracy in a realworld scenario (not assuming gold error detection). Secondly, normalization is one way to go about processing non-canonical data, but not the only one (Eisenstein, 2013; Plank, 2016). Indeed, alternative approaches leverage the abundance of unlabeled data kept in its raw form. For instance, Introduction Non-canonical data poses a series of challenges to Natural Language Processing, as reflected in large performance drops documented in a variety of tasks, e.g., on POS tagging (Gimpel et al., 2011; Hovy et al., 2014), parsing (McClosky, 2010; Foster et al., 2011) and named entity recognition (Ritter et al., 2011). In this paper we focus on POS tagging and on a particular source of non-canonical language, namely Twitter data. One obvious way to tackle the problem of processing non-canonical data is to build taggers that are specifically tailored to such text. A prime example is the ARK POS tagger, designed especially to process English Twitter data (Gimpel et al., 2011; Owoputi et al., 2013), on which it achieves state-of-the-art results. One drawback of this approach is that non-canonical data is not all of the same kind, so that"
W17-4404,N13-1039,0,0.0770857,"Missing"
W17-5025,C14-1185,0,0.108067,"g.su.se Barbara Plank CLCG University of Groningen b.plank@rug.nl Abstract although the focus in the past has been on using written text, speech transcripts and audio features have also been included in recent editions, for instance in the 2016 Computational Paralinguistics Challenge (Schuller et al., 2016). Although these aspects are combined in the NLI Shared Task 2017, with both written and spoken responses available, we only utilise written responses in this work. For a further overview of NLI, we refer the reader to Malmasi (2016). Previous approaches to NLI have used syntactic features (Bykh and Meurers, 2014), string kernels (Ionescu et al., 2014), and variations of ensemble models (Malmasi and Dras, 2017; Tetreault et al., 2013). No systems used neural networks in the 2013 shared task (Tetreault et al., 2013), hence ours is one of the first works using a neural approach for this task, along with concurrent submissions in this shared task (Malmasi et al., 2017). We present the RUG-SU team’s submission at the Native Language Identification Shared Task 2017. We combine several approaches into an ensemble, based on spelling error features, a simple neural network using word representations, a deep re"
W17-5025,D14-1142,0,0.121905,"Groningen b.plank@rug.nl Abstract although the focus in the past has been on using written text, speech transcripts and audio features have also been included in recent editions, for instance in the 2016 Computational Paralinguistics Challenge (Schuller et al., 2016). Although these aspects are combined in the NLI Shared Task 2017, with both written and spoken responses available, we only utilise written responses in this work. For a further overview of NLI, we refer the reader to Malmasi (2016). Previous approaches to NLI have used syntactic features (Bykh and Meurers, 2014), string kernels (Ionescu et al., 2014), and variations of ensemble models (Malmasi and Dras, 2017; Tetreault et al., 2013). No systems used neural networks in the 2013 shared task (Tetreault et al., 2013), hence ours is one of the first works using a neural approach for this task, along with concurrent submissions in this shared task (Malmasi et al., 2017). We present the RUG-SU team’s submission at the Native Language Identification Shared Task 2017. We combine several approaches into an ensemble, based on spelling error features, a simple neural network using word representations, a deep residual network using word and character"
W17-5025,W13-1706,0,0.12628,"written text, speech transcripts and audio features have also been included in recent editions, for instance in the 2016 Computational Paralinguistics Challenge (Schuller et al., 2016). Although these aspects are combined in the NLI Shared Task 2017, with both written and spoken responses available, we only utilise written responses in this work. For a further overview of NLI, we refer the reader to Malmasi (2016). Previous approaches to NLI have used syntactic features (Bykh and Meurers, 2014), string kernels (Ionescu et al., 2014), and variations of ensemble models (Malmasi and Dras, 2017; Tetreault et al., 2013). No systems used neural networks in the 2013 shared task (Tetreault et al., 2013), hence ours is one of the first works using a neural approach for this task, along with concurrent submissions in this shared task (Malmasi et al., 2017). We present the RUG-SU team’s submission at the Native Language Identification Shared Task 2017. We combine several approaches into an ensemble, based on spelling error features, a simple neural network using word representations, a deep residual network using word and character features, and a system based on a recurrent neural network. Our best system is an e"
W17-5025,W17-1201,0,0.0820421,"Missing"
W17-5025,W17-5007,0,0.123361,"ed Task 2017, with both written and spoken responses available, we only utilise written responses in this work. For a further overview of NLI, we refer the reader to Malmasi (2016). Previous approaches to NLI have used syntactic features (Bykh and Meurers, 2014), string kernels (Ionescu et al., 2014), and variations of ensemble models (Malmasi and Dras, 2017; Tetreault et al., 2013). No systems used neural networks in the 2013 shared task (Tetreault et al., 2013), hence ours is one of the first works using a neural approach for this task, along with concurrent submissions in this shared task (Malmasi et al., 2017). We present the RUG-SU team’s submission at the Native Language Identification Shared Task 2017. We combine several approaches into an ensemble, based on spelling error features, a simple neural network using word representations, a deep residual network using word and character features, and a system based on a recurrent neural network. Our best system is an ensemble of neural networks, reaching an F1 score of 0.8323. Although our system is not the highest ranking one, we do outperform the baseline by far. 1 Introduction Native Language Identification (NLI) is the task of identifying the nat"
W17-5025,P14-5010,0,0.0211796,"ockholm University (team RUG-SU) submission to NLI Shared Task 2017 (Malmasi et al., 2017). Neural networks constitute one of the most popular methods in natural language processing these days (Manning, 2015), but appear not to have been previously used for NLI. Our goal in this paper is therefore twofold. On the one hand, we wish to investigate how well a neural system can perform the task. On the other hand, we wish to investigate the effect of using features based on spelling errors. 2 3 External data 3.1 PoS-tagged sentences We indirectly use the training data for the Stanford PoS tagger (Manning et al., 2014), and for initialising word embeddings we use GloVe embeddings from 840 billion tokens of web data.1 3.2 Spelling features We investigate learner misspellings, which is mainly motivated by two assumptions. For one, spelling errors are quite prevalent in learners’ written production (Kochmar, 2011). Additionally, spelling errors have been shown to be influenced by phonological L1 transfer (Grigonyt˙e and Hammarberg, 2014). We use the Aspell spell checker to detect misspelled words.2 Related Work NLI is an increasingly popular task, which has been the subject of several shared tasks in recent ye"
W17-5025,W17-1219,1,0.831843,"subset of the development set (70/30 split). For the selection of systems to include in the ensemble, we use the combination of systems resulting in the highest 237 6 References Discussion In isolation, the ResNet system yields a relatively high F1 score of 80.16. This indicates that, although simpler methods yield better results for this task, deep neural networks are also applicable. However, further experimentation is needed before such a system can outperform the more traditional feature-based systems. This is in line with previous findings for the related task of language identification (Medvedeva et al., 2017; Zampieri et al., 2017). Combining all of our systems without external data yields an F1 score of 83.23, which places our system in the third best performing group of the NLI Shared Task 2017 (Malmasi et al., 2017). When adding external data, the best performing systems are those including the spelling system predictions and/or the LSTM predictions. However, the highest F1 score obtained (81.91) is lower than our best score without external resources. This can attributed to overfitting of the ensemble on the development data. It is nonetheless interesting that adding spelling features does bo"
W17-5025,W16-2003,1,0.791824,"ly dropout (p = 0.5) on the final hidden layer. Deep Residual Networks Deep residual networks, or resnets, are a class of convolutional neural networks, which consist of several convolutional blocks with skip connections in between (He et al., 2015, 2016). Such skip connections facilitate error propagation to earlier layers in the network, which allows for building deeper networks. Although their primary application is image recognition and related tasks, recent work has found deep residual networks to be useful for a range of NLP tasks. Examples of this in¨ clude morphological re-inflection (Ostling, 2016), semantic tagging (Bjerva et al., 2016), and other text classification tasks (Conneau et al., 2016). We apply resnets with four residual blocks. Each residual block contains two successive onedimensional convolutions, with a kernel size and stride of 2. Each such block is followed by an average pooling layer and dropout (p = 0.5, Srivastava et al. (2014)). The resnets are applied to several input representations: word unigrams, and character 4- to 6-grams. These input representations are first embedded into a 64-dimensional space, and trained together with the task. We do not use any pre-trai"
W17-5025,D14-1162,0,0.0806817,"sing a batch size of 50. No pre-trained embeddings were used in this model. We additionally experiment with a simple multiplayer perceptron (MLP). In contrast to CBOW it uses n-hot features (of the size of the vocabulary), PoS-tagged sentences In order to easier capture general syntactic patterns, we use a sentence-level bidirectional LSTM over tokens and their corresponding part of speech tags from the Stanford CoreNLP toolkit (Manning et al., 2014). PoS tags are represented by 64-dimensional embeddings, initialised randomly; word tokens by 300-dimensional embeddings, initialised with GloVe (Pennington et al., 2014) embeddings trained on 840 billion words of English web data from the Common Crawl project.3 3 https://nlp.stanford.edu/projects/ glove/ 4 236 http://scikit-learn.org/ Table 1: Official results for the essay task, with and without external resources (ext. res.). Setting System F1 (macro) Accuracy Baselines Random Baseline Official Baseline 0.0909 0.7100 0.0909 0.7100 01 – Resnet (w1 +c5 ) 02 – Resnet (w1 +c5 ) 03 – Ensemble (Resnet (w1 +c5 ), Resnet (c4 )) 04 – Ensemble (Resnet (w1 +c5 ), Resnet (c6 ), Resnet (c4 ), Resnet (c3 )) 05 – Ensemble (Resnet (w1 +c5 ), Resnet (c6 ), Resnet (c4 ), CBO"
W17-5043,W17-5007,0,0.156163,"n explored (Tetreault et al., 2013). Ensemble systems, which combine the predictions of several classifiers and output the most likely class label via voting or probability-averaging, have further been shown to provide a boost in accuracy compared to the single-classifier approach. Such systems, however, are not light-weight. In training several classifiers simultaneously, quick training speeds are typically sacrificed in favor of a (usually marginal) performance gain. This paper is thus concerned with exploring each of these classification methods as they pertain to the NLI Shared Task 2017 (Malmasi et al., 2017). In this paper, we explore the performance of a linear SVM trained on languageindependent character features for the NLI Shared Task 2017. Our basic system (G RONINGEN) achieves the best performance (87.56 F1-score) on the evaluation set using only 1-9 character n-grams as features. We compare this against several ensemble and meta-classifiers in order to examine how the linear system fares when combined with other, especially non-linear classifiers. Special emphasis is placed on the topic bias that exists by virtue of the assessment essay prompt distribution. 1 Introduction Native Language I"
W17-5043,C16-1333,1,0.848832,"dropout, 20 epochs, trained with the adam optimization algorithm (Kingma and Ba, 2014) for 20 iterations with a batch size of 50. 3.3.2 Deep Residual Networks Deep residual networks (resnets) are a class of convolutional neural networks (CNNs), which consist of several convolutional blocks with skip connections in between (He et al., 2016). Such skip connections facilitate error propagation to earlier layers in the network, which allows for building deeper networks. Resnets have been shown to be useful for NLP tasks, such as text classification (Conneau et al., 2016), and sequence labelling (Bjerva et al., 2016). We applied resnets with four residual blocks in our en385 semble experiments, each containing two successive one-dimensional convolutions. Each such block is followed by an average pooling layer and dropout (p = 0.5, Srivastava et al. (2014)). The resnets were applied to several input representations: word unigrams, and character 4-6-grams. The outputs of each resnet are concatenated before passing through two fully connected layers. We trained the resnet over 50 epochs with adam, using the model with the lowest validation loss. In addition to dropout, we used weight decay for regularization"
W17-5043,C12-1025,0,0.0743422,"ect their L2 writing. At a large scale, this could be extended to enhance existing teaching pedagogies and tailor them towards students of a particular L1. NLI is another natural fit for forensic linguistics, where it can be used to detect the native language (and potentially the nationality) of an anonymous writer. NLI is typically framed as a multi-class classification problem, wherein a classifier is trained on more than two native languages simultaneously. As with many text-classification tasks, Support Vector Machines (SVM) have consistently produced the best results for the task, e.g., (Brooke and Hirst, 2012). However, other classifiers, such as Random Forests and Logistic Regression, have also been explored (Tetreault et al., 2013). Ensemble systems, which combine the predictions of several classifiers and output the most likely class label via voting or probability-averaging, have further been shown to provide a boost in accuracy compared to the single-classifier approach. Such systems, however, are not light-weight. In training several classifiers simultaneously, quick training speeds are typically sacrificed in favor of a (usually marginal) performance gain. This paper is thus concerned with e"
W17-5043,P16-2067,1,0.895832,"Missing"
W17-5043,W13-1706,0,0.128826,"students of a particular L1. NLI is another natural fit for forensic linguistics, where it can be used to detect the native language (and potentially the nationality) of an anonymous writer. NLI is typically framed as a multi-class classification problem, wherein a classifier is trained on more than two native languages simultaneously. As with many text-classification tasks, Support Vector Machines (SVM) have consistently produced the best results for the task, e.g., (Brooke and Hirst, 2012). However, other classifiers, such as Random Forests and Logistic Regression, have also been explored (Tetreault et al., 2013). Ensemble systems, which combine the predictions of several classifiers and output the most likely class label via voting or probability-averaging, have further been shown to provide a boost in accuracy compared to the single-classifier approach. Such systems, however, are not light-weight. In training several classifiers simultaneously, quick training speeds are typically sacrificed in favor of a (usually marginal) performance gain. This paper is thus concerned with exploring each of these classification methods as they pertain to the NLI Shared Task 2017 (Malmasi et al., 2017). In this pape"
W17-5043,D14-1142,0,0.431151,"Missing"
W17-5043,W13-1714,0,0.414695,"Missing"
W18-0523,W02-0109,0,0.217306,"or each user 1. User, session and client: Non-linguistic data, but also potential sources of error. • country country codes from which this user has done exercises 2. Task format: Whether a given data point belongs to the listen, reverse tap or reverse translate task format. Each task has a different error prior (Table 1). • client - the student’s device platform (one of: android, ios, or web) • session - the session type (one of: lesson, practice, or test; explanation below) 3. Word properties (base): Basic word properties, i.e., the word form and its stem. We use the NLTK Snowball stemmers (Loper and Bird, 2002) for the three languages at hand. We add the word’s log frequency calculated from Universal Dependencies (UD) 2.1 (Nivre et al., 2016). There were three tracks for learners of English, Spanish, and French. In particular, en-es consists of English learners (who already speak Spanish), es-en are Spanish learners (who already speak English), and fr-en are French learners (who already speak English). We participated in all three. An overview of the data for the three tracks, including number of users, tokens and average error rate is given in Table 1. The distribution of four attributes of the tex"
W18-0523,W18-0506,0,0.0297354,"the data before detailing each group of features and proceeding to describe the model and results. We present our submission to the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (SLAM). We focus on evaluating a range of features for the task, including user-derived measures, while examining how far we can get with a simple linear classifier. Our analysis reveals that errors differ per exercise format, which motivates our final and best-performing system: a task-wise (per exercise-format) model. 1 Introduction The shared task on Second Language Acquisition Modeling (SLAM) (Settles et al., 2018) consisted of an error prediction task, i.e., determining whether a language learner (user) made a tokenlevel mistake.1 Exploring if and how errors can be predicted can provide insights into the learning process and help pinpoint specific constructs that challenge learners of different languages. The design of each exercise and the time spent on a particular task and language course, which can be expected to influence the performance, are included in the data. The learning context and the learners’ background language skills, which would also influence performance, are not known or controlled"
W18-1113,P11-1137,0,0.0358478,"ven more so, of author traits. Introduction Language is a social phenomenon (Nguyen et al., 2015). Whenever we speak or write we transmit a good deal of additional non-verbal information that is related to identity and social factors of an author. Early work in authorship analysis has typically been concerned with finding the author of a text, i.e., authorship attribution (Mosteller and Wallace, 1964; Stamatatos, 2009). In recent years, there has been a surge of interest towards the social dimension of language. Studies are interested in linking social factors with linguistic features, e.g., (Eisenstein et al., 2011; Bamman et al., 2014), studying data biases (Hovy and Søgaard, 2015) or building actual attribute prediction models from linguistic features (i.e., author profiling). Modeling author traits can further help to improve prediction of related attributes (Liu et al., 2016; Benton et al., 2017), help debiasing models (Hovy, 2015; Zhang et al., 2018) or can be used for a wide range of applications like customer supContributions a) We study the effect of keystrokes to identify authorship in two corpora of varying size. b) We investigate the predictive power of typist data for age and gender predicti"
W18-1113,W17-4407,0,0.0114581,"ts from Keystroke Dynamics Barbara Plank IT University of Copenhagen Department of Computer Science Rued Langgaards Vej 7, 2300 Copenhagen S, Denmark bapl@itu.dk Abstract port, healthcare and personalized machine translation (Mirkin et al., 2015; Rabinovich et al., 2017). Factors studied so far include gender, age, personality or income, to name but a few (Mairesse and Walker, 2006; Luyckx and Daelemans, 2008; Rao et al., 2010; Rosenthal and McKeown, 2011; Nguyen et al., 2011; Volkova et al., 2013; Flekova et al., 2016b; Verhoeven et al., 2016; van Dalen et al., 2017; Ljubeˇsi´c et al., 2017; Emmery et al., 2017; van der Goot et al., 2018). Written text transmits a good deal of nonverbal information related to the author’s identity and social factors, such as age, gender and personality. However, it is less known to what extent behavioral biometric traces transmit such information. We use typist data to study the predictiveness of authorship, and present first experiments on predicting both age and gender from keystroke dynamics. Our results show that the model based on keystroke features leads to significantly higher accuracies for authorship than the text-based system, while being two orders of mag"
W18-1113,P12-2073,0,0.0235875,"sions—are considered traces of the recursive nature of the writing process. Bursts are defined as consecutive chunks of text produced and defined by a 2000ms time of inactivity (Wengelin, 2006). In fact, most prior work that uses keystroke logs focuses on experimental research. For example, Hanoulle et al. (2015) study whether a bilingual glossary reduces the working time of professional translators. They consider pause durations before terms extracted from keystroke logs and find that a bilingual glossary reduces the translators’ workload. An analysis of users’ typing behavior was studied by Baba and Suzuki (2012) to measure the impact of spelling mistakes. Goodkind and Rosenberg (2015) investigate pre-word pauses and their relation to multi-word expressions. They found that within MWE pauses vary depending on the cognitive task. Banerjee et al. (2014) were the first to use keystroke patterns for deception detection. Keystrokes were successfully used for author verification in computer security research (Stewart et al., 2011; Monaco et al., 2013; Locklear et al., 2014), as they are known to be idiosyncratic (Leggett and Williams, 1988). Our results show that keystroke biometrics are far superior over s"
W18-1113,P16-1080,0,0.0406711,"Missing"
W18-1113,P16-2051,0,0.0461328,"Missing"
W18-1113,D14-1155,0,0.136136,"from the English alphabet (cf. Figure 2 for an illustration); and ii) extended features: key hold times over groups of keys (like digits, punctuation etc) and transition (inter-key duration) features between successive keystrokes, e.g., between letters and non-letters, or individual letters and Figure 1: Keystroke logs illustrated: p are pauses between keystrokes. Figure adapted from (Goodkind and Rosenberg, 2015). Only very recently this source has been explored as information in natural language processing, for example, to aid shallow syntactic parsing (Plank, 2016) or deception detection (Banerjee et al., 2014) (Section 5). Keystroke logs have been used in computer security for user verification, however, combining keystroke biometrics with traditional stylometry metrics has not been proven successful (Stewart et al., 2011). The authors focused on a single task and dataset only. In contrast, in this paper we examine to what extent keystroke dynamics are informative for authorship attribution and author profiling. 3 Experiments Given a dataset with keystroke logs, we run two sets of experiments: a) authorship attribution, i.e., to determine who wrote a given piece of text; and b) authorship profiling"
W18-1113,W15-0914,0,0.0214355,"process. Bursts are defined as consecutive chunks of text produced and defined by a 2000ms time of inactivity (Wengelin, 2006). In fact, most prior work that uses keystroke logs focuses on experimental research. For example, Hanoulle et al. (2015) study whether a bilingual glossary reduces the working time of professional translators. They consider pause durations before terms extracted from keystroke logs and find that a bilingual glossary reduces the translators’ workload. An analysis of users’ typing behavior was studied by Baba and Suzuki (2012) to measure the impact of spelling mistakes. Goodkind and Rosenberg (2015) investigate pre-word pauses and their relation to multi-word expressions. They found that within MWE pauses vary depending on the cognitive task. Banerjee et al. (2014) were the first to use keystroke patterns for deception detection. Keystrokes were successfully used for author verification in computer security research (Stewart et al., 2011; Monaco et al., 2013; Locklear et al., 2014), as they are known to be idiosyncratic (Leggett and Williams, 1988). Our results show that keystroke biometrics are far superior over stylometry-based features in authorship attribution, and are predictive of"
W18-1113,P18-2061,1,0.881819,"Missing"
W18-1113,P15-1073,0,0.0201864,"text, i.e., authorship attribution (Mosteller and Wallace, 1964; Stamatatos, 2009). In recent years, there has been a surge of interest towards the social dimension of language. Studies are interested in linking social factors with linguistic features, e.g., (Eisenstein et al., 2011; Bamman et al., 2014), studying data biases (Hovy and Søgaard, 2015) or building actual attribute prediction models from linguistic features (i.e., author profiling). Modeling author traits can further help to improve prediction of related attributes (Liu et al., 2016; Benton et al., 2017), help debiasing models (Hovy, 2015; Zhang et al., 2018) or can be used for a wide range of applications like customer supContributions a) We study the effect of keystrokes to identify authorship in two corpora of varying size. b) We investigate the predictive power of typist data for age and gender prediction. c) We compare behavioral measures to traditional stylometric features. 98 Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 98–104 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics 2 Keystroke dynamics K"
W18-1113,P15-2079,0,0.0206714,"omenon (Nguyen et al., 2015). Whenever we speak or write we transmit a good deal of additional non-verbal information that is related to identity and social factors of an author. Early work in authorship analysis has typically been concerned with finding the author of a text, i.e., authorship attribution (Mosteller and Wallace, 1964; Stamatatos, 2009). In recent years, there has been a surge of interest towards the social dimension of language. Studies are interested in linking social factors with linguistic features, e.g., (Eisenstein et al., 2011; Bamman et al., 2014), studying data biases (Hovy and Søgaard, 2015) or building actual attribute prediction models from linguistic features (i.e., author profiling). Modeling author traits can further help to improve prediction of related attributes (Liu et al., 2016; Benton et al., 2017), help debiasing models (Hovy, 2015; Zhang et al., 2018) or can be used for a wide range of applications like customer supContributions a) We study the effect of keystrokes to identify authorship in two corpora of varying size. b) We investigate the predictive power of typist data for age and gender prediction. c) We compare behavioral measures to traditional stylometric feat"
W18-1113,P16-2096,0,0.0166808,"ond and larger dataset (n=121 authors). To the best of our knowledge, prior work on predicting demographics from typing behavior is typically limited to a single variable (Tsimperidis et al., 2015), except (Brizan et al., 2015), whose data is not available. Our study differs from theirs by studying age, and the focus on complementing textual with behavioral data. Disclaimer While modeling user demographics can be seen as one step towards addressing biases in NLP it is important to be aware of potential negative side effects, both from the modeling side through potential exclusion or dual use (Hovy and Spruit, 2016), as well as the data side, when dealing with privacy sensitive data (cognitive behavioral data) or labels (e.g., mental health). 6 Conclusions We have shown that behavioral biometrics contain highly predictive information for both authorship and author profiling. For authorship attribution, behavioral keystroke metrics significantly outperform traditional text-based features (words and character unigrams), while using a feature set which is orders of magnitude smaller (218 vs sev101 eral thousands of features). In addition, we show that keystroke dynamics are also predictive for author traits"
W18-1113,W16-4303,0,0.0240283,"lysis has typically been concerned with finding the author of a text, i.e., authorship attribution (Mosteller and Wallace, 1964; Stamatatos, 2009). In recent years, there has been a surge of interest towards the social dimension of language. Studies are interested in linking social factors with linguistic features, e.g., (Eisenstein et al., 2011; Bamman et al., 2014), studying data biases (Hovy and Søgaard, 2015) or building actual attribute prediction models from linguistic features (i.e., author profiling). Modeling author traits can further help to improve prediction of related attributes (Liu et al., 2016; Benton et al., 2017), help debiasing models (Hovy, 2015; Zhang et al., 2018) or can be used for a wide range of applications like customer supContributions a) We study the effect of keystrokes to identify authorship in two corpora of varying size. b) We investigate the predictive power of typist data for age and gender prediction. c) We compare behavioral measures to traditional stylometric features. 98 Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 98–104 c New Orleans, Louisiana, June 6, 2018. 2018 Associa"
W18-1113,W17-2901,0,0.0359601,"Missing"
W18-1113,C16-1059,1,0.849367,"ease time) features of the 26 letters from the English alphabet (cf. Figure 2 for an illustration); and ii) extended features: key hold times over groups of keys (like digits, punctuation etc) and transition (inter-key duration) features between successive keystrokes, e.g., between letters and non-letters, or individual letters and Figure 1: Keystroke logs illustrated: p are pauses between keystrokes. Figure adapted from (Goodkind and Rosenberg, 2015). Only very recently this source has been explored as information in natural language processing, for example, to aid shallow syntactic parsing (Plank, 2016) or deception detection (Banerjee et al., 2014) (Section 5). Keystroke logs have been used in computer security for user verification, however, combining keystroke biometrics with traditional stylometry metrics has not been proven successful (Stewart et al., 2011). The authors focused on a single task and dataset only. In contrast, in this paper we examine to what extent keystroke dynamics are informative for authorship attribution and author profiling. 3 Experiments Given a dataset with keystroke logs, we run two sets of experiments: a) authorship attribution, i.e., to determine who wrote a g"
W18-1113,luyckx-daelemans-2008-personae,0,0.0362763,"Missing"
W18-1113,N06-2022,0,0.0689406,"Missing"
W18-1113,W15-2913,1,0.902828,"Missing"
W18-1113,E17-1101,0,0.0522428,"Missing"
W18-1113,L16-1258,1,0.89479,"Missing"
W18-1113,D13-1187,0,0.0752281,"Missing"
W18-1113,P11-1077,0,0.0527075,"Missing"
W18-1113,D13-1193,0,0.0274308,"1 60.58 73.25 67.12 – 80k 52 218 80k 413 Table 2: Gender and age prediction results, F1-score (age: above/below 30). 5 Related Work Authorship attribution has a long tradition dating back to early works in the 19th century. The most influential work on authorship attribution goes back to Mosteller and Wallace (1964). For a long time approaches to authorship attribution focused on distributions of function words, high-frequency words that are presumably not consciously manipulated by the author (Nerbonne, 2007; Pennebaker, 2011). Recent work also includes authorship studies on microblog texts (Schwartz et al., 2013). An recent survey is Stamatatos (2009). We here study another source of information that is presumably not consciously manipulated, keystroke dynamics. A major scientific interest in keystroke dynamics arose in writing research, where it has developed into a promising non-intrusive method for studying cognitive processes involved in writing (Sullivan et al., 2006; Nottbusch et al., 2007; Wengelin, 2006; Van Waes et al., 2009; Baaijen et al., 2012). In these studies time measurements—pauses, bursts and revisions—are considered traces of the recursive nature of the writing process. Bursts are d"
W18-3401,D15-1041,0,0.028899,"with a character-based sequence-to-sequence model. Furthermore, we experiment with different choices of external resources and corresponding auxiliary tasks and show that autoencoding can be as efficient as an auxiliary task for low-resource POS tagging as lemmatization. Finally, we evaluate our models on 34 typologically diverse languages. 2 Figure 1: Our multi-task architecture, consisting of a shared character LSTM (down), as well as a sequence labeling (up) and a sequence-tosequence (right) part. subword-level LSTM is bi-directional and operates on the character level (Ling et al., 2015; Ballesteros et al., 2015). Its input is the character sequence of each input word, represented by the embedding sequence c1 , c2 , . . . , cm . The final character-based representation of each word is the concatenation of the two last LSTM hidden states: POS Tagging with Subword-level Supervision Hierarchical POS tagging LSTMs that receive both word-level and subword-level input, such as Plank et al. (2016), are known to perform well on unseen words. This is due to their ability to associate subword-level patterns with POS tags. However, hierarchical LSTMs are also very expressive, and thus prone to overfitting. We be"
W18-3401,E17-2026,1,0.947527,".6776(.00) .6226( - ) .6141( - ) .6188( - ) .6529( - ) Table 1: Averaged accuracies and standard deviations over 5 training runs on UD 2.0 test sets, with 478 tokens of POS-annotated data and varying amounts of data for the auxiliary task (low, medium and high). Best result for each language in bold. Autoencoding and lemmatization are on par across the board, and with 100 training sentences (low), random autoencoding is also competitive. 6.2 Why does Random String Autoencoding Help? languages look similar). They show exactly the patterns found to be predictive of multi-task learning gains by Bingel and Søgaard (2017), who offer the explanation that when the auxiliary loss does not plateau before the target task, it can help the model out of local minima during training. In the low setting, i.e., when using only 100 auxiliary task examples, autoencoding, especially of random strings, works better than or equally well as lemmatization for highly agglutinative languages such as Basque, Finnish, Hungarian, and Turkish. Further, while random string autoencoding is in general less efficient than autoencoding or lemmatization, it performs on par with these auxiliary tasks in the set-up with least auxiliary task"
W18-3401,W17-0225,1,0.850592,"or results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevant for low-reso"
W18-3401,C16-1333,1,0.850199,"owever, they report poor results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevan"
W18-3401,D11-1005,0,0.0784096,"Missing"
W18-3401,K17-2001,0,0.389323,"- ) .6141( - ) .6188( - ) .6529( - ) Table 1: Averaged accuracies and standard deviations over 5 training runs on UD 2.0 test sets, with 478 tokens of POS-annotated data and varying amounts of data for the auxiliary task (low, medium and high). Best result for each language in bold. Autoencoding and lemmatization are on par across the board, and with 100 training sentences (low), random autoencoding is also competitive. 6.2 Why does Random String Autoencoding Help? languages look similar). They show exactly the patterns found to be predictive of multi-task learning gains by Bingel and Søgaard (2017), who offer the explanation that when the auxiliary loss does not plateau before the target task, it can help the model out of local minima during training. In the low setting, i.e., when using only 100 auxiliary task examples, autoencoding, especially of random strings, works better than or equally well as lemmatization for highly agglutinative languages such as Basque, Finnish, Hungarian, and Turkish. Further, while random string autoencoding is in general less efficient than autoencoding or lemmatization, it performs on par with these auxiliary tasks in the set-up with least auxiliary task"
W18-3401,P15-2044,1,0.928316,"Missing"
W18-3401,N16-1077,0,0.0302103,"Lemmatization is a task from the area of inflectional morphology. In particular, it is a special case of morphological inflection. Its goal is to map a given inflected word form to its lemma, e.g., sue˜no 7→ so˜nar. (6) Word autoencoding. For the word autoencoding task, we use the inflected forms from the SIGMORPHON 2017 shared task dataset for each respective setting. Due to identical forms for different slot in the morphological paradigm of some lemmas, we might have duplicate examples in those datasets. Sequence-to-sequence models have shown strong performances on morphological inflection (Aharoni et al., 2016; Kann and Sch¨utze, 2016; Makarov et al., 2017). Therefore, when morphological dictionaries are available, we can easily combine a neural model for lemmatization with a POS tagger, using our architecture. Our intuition for this auxiliary task is that it should be possible to include morphological information into our character-based word representations. Formally, the task can be described as follows. Let AL be a discrete alphabet for language L and let TL be a set of morphological tags for L. The morphological paradigm π of a lemma w in L is a set of pairs n o π(w) = fk [w], tk (7) Random s"
W18-3401,P15-1166,0,0.0831235,"Missing"
W18-3401,P07-1094,0,0.133713,"Missing"
W18-3401,P17-2054,1,0.900977,"Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevant for low-resource settings and, thus, want to mention some important work here. Cross-lingual approaches have been used for a large variety of tasks, e.g., automatic speech recognition (Huang et al., 2013), entity recognition (Wang and Manning, 2014), language modeling (Ts"
W18-3401,N18-1172,1,0.839691,"nt training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we consider it highly relevant for low-resource settings and, thus, want to me"
W18-3401,P16-2067,1,0.917337,"ting of a shared character LSTM (down), as well as a sequence labeling (up) and a sequence-tosequence (right) part. subword-level LSTM is bi-directional and operates on the character level (Ling et al., 2015; Ballesteros et al., 2015). Its input is the character sequence of each input word, represented by the embedding sequence c1 , c2 , . . . , cm . The final character-based representation of each word is the concatenation of the two last LSTM hidden states: POS Tagging with Subword-level Supervision Hierarchical POS tagging LSTMs that receive both word-level and subword-level input, such as Plank et al. (2016), are known to perform well on unseen words. This is due to their ability to associate subword-level patterns with POS tags. However, hierarchical LSTMs are also very expressive, and thus prone to overfitting. We believe that using subword-level auxiliary tasks to regularize the character-level encoding in hierarchical LSTMs is a flexible and efficient way to get the best of both worlds: such a model is still able to make predictions about unknown words, but the subword-level auxiliary task should prevent it from overfitting. 2.1 vc,i = conc(LSTMc,f (c1:m ), (1) LSTMc,b (cm:1 )) Second, a cont"
W18-3401,P17-1194,0,0.0202508,". Furthermore, their model is also a multi-task model, being trained jointly on predicting the POS and the log-frequency of a word. Their architecture obtained state-of-the-art results for POS tagging in several languages. Hence, in the low-resource setting considered here, we build upon the architecture developed by Plank et al. (2016), and extend it to a multi-task architecture involving sequenceto-sequence learning. Note though that in contrast to our setup, their tasks are both sequence-labeling tasks and using the same input for both tasks. The same holds true for the multi-task model by Rei (2017), which is used to investigate how an additional language modeling objective could improve performance for sequence labeling without any need for additional training data. He reported Minimum Distance low medium high 0.031 0.027 0.074 0.032 0.031 0.092 0.033 0.032 0.104 0.018 Table 4: Minimum character embedding distances, averaged over all languages. In small sample regimes, pushing individual characters further apart is a potential advantage, since character collisions can be hurtful at inference time. We note how this is analogous to feature swamping of covariate features, as described in S"
W18-3401,P17-1182,1,0.881428,"Missing"
W18-3401,P16-2090,1,0.82551,"Missing"
W18-3401,P11-2120,1,0.816951,"Missing"
W18-3401,P16-2038,1,0.707545,"o obtain better performance on a sequence classification task. However, they report poor results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lin"
W18-3401,Q16-1023,0,0.0435006,"individual languages. Here, we notice that autoencoders often outperform lemmatization for agglutinative languages. An explanation for this might be that agglutinative morphology is harder to learn, and the chance of overfitting on a small sample is therefore higher. Hyperparameters For all networks, we use 300-dimensional character embeddings, 64-dimensional word embeddings and 100-dimensional LSTM hidden states. Encoder and decoder LSTMs have 1 hidden layer each. For training, we use ADAM (Kingma and Ba, 2014), as well as word dropout and character dropout, each with a coefficient of 0.25 (Kiperwasser and Goldberg, 2016). Gaussian noise is added to the concatenation of the last states of the character LSTMs for POS tagging. All models are trained using early stopping, with a minimum number of 75 (single-task and low), 30 (medium) or 20 (high) epochs and a maximum number of 300 epochs, which is never reached. We stop training if we obtain no improvement for 10 consecutive epochs. The best model on the development set is used for testing. 5 Results The test results for all languages and settings are presented in Table 1. Our first observation is that using 100 words of auxiliary task data seems to be sufficient"
W18-3401,N06-1012,0,0.0581584,"), which is used to investigate how an additional language modeling objective could improve performance for sequence labeling without any need for additional training data. He reported Minimum Distance low medium high 0.031 0.027 0.074 0.032 0.031 0.092 0.033 0.032 0.104 0.018 Table 4: Minimum character embedding distances, averaged over all languages. In small sample regimes, pushing individual characters further apart is a potential advantage, since character collisions can be hurtful at inference time. We note how this is analogous to feature swamping of covariate features, as described in Sutton et al. (2006). Sutton et al. (2006) use a group lasso regularizer to prevent feature swamping. In the same way, we could also detect distributionally similar characters and use a group lasso regularizer to prevent covariate characters to swamp each other. However, this effect can potentially also hurt performance if done in an uninformed way. We intuit that this makes it also impossible for the model to learn useful similarities between characters (random string autoencod7 4.5 4.5 POS, main task POS+AE-Random, main task POS+AE-Random, aux. task POS+Lemmatization, main task POS+Lemmatization, aux. task POS+"
W18-3401,Q13-1001,0,0.0766196,"Missing"
W18-3401,D12-1127,0,0.148688,"Missing"
W18-3401,N16-1161,0,0.0651137,"Missing"
W18-3401,E17-1005,1,0.841116,"ce classification task. However, they report poor results for joint training. We obtain different results: even simple sequence-to-sequence tasks can indeed be beneficial for the sequence labeling task of low-resource POS tagging. This might be due to differences in the architectures or tasks. Multi-task learning in NLP. Neural networks make multi-task learning via (hard) parameter sharing particularly easy; thus, different task combinations have been investigated exhaustively. For sequence labeling, many combinations of tasks have been explored, e.g. by Søgaard and Goldberg (2016); Mart´ınez Alonso and Plank (2017); Bjerva et al. (2016); Bjerva (2017a,b); Augenstein and Søgaard (2018). An analysis of task combinations is performed by Bingel and Søgaard (2017). Ruder et al. (2017) present a more flexible architecture, which learns what to share between the main and auxiliary tasks. Augenstein et al. (2017) combine multi-task learning with semi-supervised learning for strongly related tasks with different output spaces. However, work on combining sequence labeling main tasks and sequence-to-sequence auxiliary Cross-lingual learning. Even though we do not employ cross-lingual learning in this work, we cons"
W18-3401,Q14-1005,0,0.0607788,"Missing"
W18-3401,D13-1032,0,0.0818875,"Missing"
W18-3401,P12-1066,0,0.0687448,"Missing"
W18-3401,H01-1035,0,0.151424,"Missing"
W18-3401,N16-1004,0,0.0425229,"Missing"
W18-3928,W17-1301,0,0.0185378,"marks, as features in the second layer. This is in line with van der Lee and van den Bosch (2017), who, next to word n-grams, use global statistics and POS-tag n-grams in the classification of Dutch vs. Flemish subtitles, achieving an F1 score of 0.92. For this task we depart from our earlier system but explore alternatives, in particular one based on using syntactic information. Even though POS-tagging is a challenging method to use for language identification due to its language-dependent nature, it has often been explored for distinguishing between language varieties (Martinc et al., 2017; Adouane and Dobnik, 2017, among others). To add to previous experiments that exploited POS-tag n-grams and thus retaining the linear structure of the text, we explore the possibilities of using hierarchical subtrees to allow for non-contiguous groups of POS tags as well as to exploit the syntactic relations between them. 3 Methodology and Data In this section we describe the DFS data as it was released to participants, the features we used in our approaches, and our three system submissions. 245 F LEMISH Sami, DiMera, Amai, Sooz, interesseerd, Kenshee, Moz, vanop, AUDIO, Megatron, Shishio, enant, ACHTERGRONDMUZIEK, C"
W18-3928,W17-1214,0,0.270399,"Section 5. 2 Related Work Although this year is already VarDial’s fifth anniversary (Zampieri et al., 2018; Zampieri et al., 2017; Malmasi et al., 2016; Zampieri et al., 2015; Zampieri et al., 2014), it is the first time that the DFS shared task was organized. Previous iterations of VarDial saw shared tasks concerning Arabic dialect identification, German dialect identification, cross-lingual dependency parsing, and discrimination between similar languages (DSL). Last year our team participated in the DSL shared task (Medvedeva et al., 2017), where it ranked second with an F1 score of 0.925. Bestgen (2017) won by 0.002 points. Both systems used a two-layer classification, comparable to Goutte et al. (2014): the first layer identified the language group, the second layer trained multiple classifiers to identify language varieties within the groups. However, while we only used word and character n-grams in both classification layers, the winning team, Bestgen (2017), also used POS-tags n-grams (the POS tags were obtained using language-group specific POS taggers) and some global statistics, such as proportion of capital letters and punctuation marks, as features in the second layer. This is in li"
W18-3928,W14-5316,0,0.0541788,"Missing"
W18-3928,W16-4801,0,0.183733,"Missing"
W18-3928,W17-1219,1,0.940472,"tem submissions. Section 4 follows with the results, which are discussed in Section 5. 2 Related Work Although this year is already VarDial’s fifth anniversary (Zampieri et al., 2018; Zampieri et al., 2017; Malmasi et al., 2016; Zampieri et al., 2015; Zampieri et al., 2014), it is the first time that the DFS shared task was organized. Previous iterations of VarDial saw shared tasks concerning Arabic dialect identification, German dialect identification, cross-lingual dependency parsing, and discrimination between similar languages (DSL). Last year our team participated in the DSL shared task (Medvedeva et al., 2017), where it ranked second with an F1 score of 0.925. Bestgen (2017) won by 0.002 points. Both systems used a two-layer classification, comparable to Goutte et al. (2014): the first layer identified the language group, the second layer trained multiple classifiers to identify language varieties within the groups. However, while we only used word and character n-grams in both classification layers, the winning team, Bestgen (2017), also used POS-tags n-grams (the POS tags were obtained using language-group specific POS taggers) and some global statistics, such as proportion of capital letters and"
W18-3928,W17-5028,0,0.0301827,"Missing"
W18-3928,K17-3009,0,0.0286736,"Missing"
W18-3928,K17-3001,0,0.243624,"macro) of 0.62, which ranked us 4th in the shared task. 1 Introduction The Dutch language is regulated by the Dutch Language Union. The varieties of Dutch spoken in the Netherlands and spoken in Belgium are both subject to this regulation. Despite this, there are still differences to be found between Netherlandic Dutch and Flemish Dutch, most clearly in phonology and pronunciation, but also in terms of word use and word order. Nevertheless, there is little to no work on automatic classification to distinguish between the two varieties. A first attempt was made by van der Lee and van den Bosch (2017), in light of whose work this year’s iteration of the annual Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial) took on Discriminating between Dutch and Flemish in Subtitles (DFS) as one of their evaluation campaigns (Zampieri et al., 2018).1 For the DFS 2018 shared task, our team (mmb lct), built a model that discriminates between the two varieties. The model achieved the fourth place out of seven in the ranking of the results – statistical significance of the differences between the F1 scores of the submissions was taken into account, such that the third place was shared"
W18-3928,C12-1160,0,0.113626,"Missing"
W18-3928,W17-1224,0,0.184957,"Missing"
W18-3928,W14-5307,0,0.382876,"Missing"
W18-3928,W15-5401,0,0.350321,"Missing"
W18-3928,W17-1201,0,0.174239,"Missing"
W18-3928,W18-3901,0,0.0577764,"Missing"
W19-4307,P18-1198,0,0.0603707,"Missing"
W19-4307,N18-2034,0,0.0773527,"k semantics. Finally, we provide a surprisingly simple recipe to obtain specialized embeddings that better fit end-tasks. 1 Barbara Plank Department of Computer Science IT University of Copenhagen Rued Langgaards Vej 7 2300 Copenhagen S, Denmark bplank@itu.dk Introduction Word embeddings are ubiquitous in Natural Language Processing. They provide a low-effort, high pay-off way to improve the performance of a specific supervised end-task by transferring knowledge. However, recent works indicate that universally best embeddings are not yet possible (Bollegala and Bao, 2018; Kiela et al., 2018a; Dingwall and Potts, 2018), and that they instead need to be tuned to fit specific end-tasks using inductive bias – i.e., semantic supervision for the unsupervised embedding learning process (Conneau et al., 2018; Perone et al., 2018). This way, embeddings can be tuned to fit a specific single-task (ST) or multi-task (MT: set of tasks) semantic (Xiong et al., 2018). Fine-tuning requires labeled data, which is often either too small, not available or of low quality and creating or extending labeled data is costly and slow. Word embeddings are typically induced from huge unlabeled corpora with billions of tokens, but for"
W19-4307,N15-1184,0,0.0784227,"Missing"
W19-4307,D17-1206,0,0.0335017,"includes using embeddings for supervised tasks. However, transfer also works vice versa, in a supervised-to-unsupervised setup to (learn to) specialize embeddings to better fit a specific supervised signal (Ruder and Plank, 2017; Ye et al., 2018). This includes injecting generally relevant semantics via retrofitting or auxiliary multi-task supervision (Faruqui et al., 2015; Kiela et al., 2018b). Supervised-to-supervised methods provide knowledge transfer between supervised tasks which is exploited successively (Kirkpatrick et al., 2017), jointly (Kiela et al., 2018b) and in joint-succession (Hashimoto et al., 2017). Unsupervised-to-unsupervised transfer is less studied. Dingwall and Potts (2018) proposed a GloVe model-modification that retrofits publicly available GloVe embeddings to produce specialized domain embeddings, while Bollegala and We see that, for practical application, this allows M O RT Y to boost supervised MT performance even without using a supervised development split or proxy task(s), while also eliminating multi-epoch tuning. Both Figure 1 and Table 1 show similar overall (MT) improvements per corpus size, which suggests that 1-epoch training is sufficient and that M O RT Y is especia"
W19-4307,D18-1176,0,0.0905985,"pus sizes and end-task semantics. Finally, we provide a surprisingly simple recipe to obtain specialized embeddings that better fit end-tasks. 1 Barbara Plank Department of Computer Science IT University of Copenhagen Rued Langgaards Vej 7 2300 Copenhagen S, Denmark bplank@itu.dk Introduction Word embeddings are ubiquitous in Natural Language Processing. They provide a low-effort, high pay-off way to improve the performance of a specific supervised end-task by transferring knowledge. However, recent works indicate that universally best embeddings are not yet possible (Bollegala and Bao, 2018; Kiela et al., 2018a; Dingwall and Potts, 2018), and that they instead need to be tuned to fit specific end-tasks using inductive bias – i.e., semantic supervision for the unsupervised embedding learning process (Conneau et al., 2018; Perone et al., 2018). This way, embeddings can be tuned to fit a specific single-task (ST) or multi-task (MT: set of tasks) semantic (Xiong et al., 2018). Fine-tuning requires labeled data, which is often either too small, not available or of low quality and creating or extending labeled data is costly and slow. Word embeddings are typically induced from huge unlabeled corpora with"
W19-4307,C18-1140,0,0.0205701,"rd embedding methods, corpus sizes and end-task semantics. Finally, we provide a surprisingly simple recipe to obtain specialized embeddings that better fit end-tasks. 1 Barbara Plank Department of Computer Science IT University of Copenhagen Rued Langgaards Vej 7 2300 Copenhagen S, Denmark bplank@itu.dk Introduction Word embeddings are ubiquitous in Natural Language Processing. They provide a low-effort, high pay-off way to improve the performance of a specific supervised end-task by transferring knowledge. However, recent works indicate that universally best embeddings are not yet possible (Bollegala and Bao, 2018; Kiela et al., 2018a; Dingwall and Potts, 2018), and that they instead need to be tuned to fit specific end-tasks using inductive bias – i.e., semantic supervision for the unsupervised embedding learning process (Conneau et al., 2018; Perone et al., 2018). This way, embeddings can be tuned to fit a specific single-task (ST) or multi-task (MT: set of tasks) semantic (Xiong et al., 2018). Fine-tuning requires labeled data, which is often either too small, not available or of low quality and creating or extending labeled data is costly and slow. Word embeddings are typically induced from huge un"
W19-4307,W14-1618,0,0.107253,"Missing"
W19-4307,Q15-1016,0,0.0685311,"Missing"
W19-4307,D14-1162,0,0.0956945,"zed representations of Eorg using a reconstruction loss (mean square error, cf. below). (3) Pick: performance-optimal representation for the end-task(s) via a task’s development split(s) or proxy tasks, depending on the end-goal, i.e., specialization or generalization. (4) Gain: use optimal M O RT Y (Epost ) to push relative performance on end task(s). Embeddings and Corpus Size: We evaluate embeddings trained on small, medium (millions of tokens) and large (billions of tokens) corpus sizes. In particular, we train 100-dimensional embeddings with Fasttext (Bojanowski et al., 2016)4 and GloVe (Pennington et al., 2014)5 on the 2M and 103M WikiText created by Merity et al. (2016). We complement them with off-the-shelf webscale Fasttext and GloVe embeddings (trained on 600B and 840B tokens, respectively). This results in the following vocabulary sizes for Fasttext and GloVe embeddings, respectively: on 2M 25,249 and 33,237 word types. For 103M we get 197,256 and 267,633 vocabulary words. Public, off-the-shelf – common-crawl trained – Fasttext and GloVe embeddings have very large vocabularies of 1,999,995 and 2,196,008 words. To account for variation in results, we train both embedding methods five times each6"
W19-4307,N18-1202,0,0.0220475,"not available or of low quality and creating or extending labeled data is costly and slow. Word embeddings are typically induced from huge unlabeled corpora with billions of tokens, but for limited-resource domains 1 https://github.com/kudkudak/ word-embeddings-benchmarks 49 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 49–54 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics very little resources,2 producing a low carbon footprint, especially regarding recent, compute intensive, scale-up approaches like ELMo or BERT (Peters et al., 2018; Devlin et al., 2018) which have high hardware and training time requirements and a large carbon footprint as recently demonstrated by Strubell et al. (2019). As a result, we demonstrate a simple, unsupervised scale-down method, that allows further pretraining exploitation, while requiring minimum extra effort, time and compute resources. As in standard methodology, optimal post-processed embeddings can be selected according to multiple proxy-tasks for overall improvement or using a single end-task’s development split—e.g., on a fast baseline model for further time reduction. 2 Which autoenco"
W19-4307,D17-1038,1,0.846702,"ed M O RT Ys, and then test them on the 18 task (MT) setup. Figure 1 shows consistent MT/Σ score improvements for each of the 3 M O RT Y-over-Fasttext runs (red, yellow, green) on 2M, 103M, and 600B vs. base Fasttext (blue 100). Related Work There is a large body of work on information transfer between supervised and unsupervised tasks. First and foremost unsupervisedto-supervised transfer includes using embeddings for supervised tasks. However, transfer also works vice versa, in a supervised-to-unsupervised setup to (learn to) specialize embeddings to better fit a specific supervised signal (Ruder and Plank, 2017; Ye et al., 2018). This includes injecting generally relevant semantics via retrofitting or auxiliary multi-task supervision (Faruqui et al., 2015; Kiela et al., 2018b). Supervised-to-supervised methods provide knowledge transfer between supervised tasks which is exploited successively (Kirkpatrick et al., 2017), jointly (Kiela et al., 2018b) and in joint-succession (Hashimoto et al., 2017). Unsupervised-to-unsupervised transfer is less studied. Dingwall and Potts (2018) proposed a GloVe model-modification that retrofits publicly available GloVe embeddings to produce specialized domain embedd"
W19-4307,P19-1355,0,0.0146277,"ra with billions of tokens, but for limited-resource domains 1 https://github.com/kudkudak/ word-embeddings-benchmarks 49 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 49–54 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics very little resources,2 producing a low carbon footprint, especially regarding recent, compute intensive, scale-up approaches like ELMo or BERT (Peters et al., 2018; Devlin et al., 2018) which have high hardware and training time requirements and a large carbon footprint as recently demonstrated by Strubell et al. (2019). As a result, we demonstrate a simple, unsupervised scale-down method, that allows further pretraining exploitation, while requiring minimum extra effort, time and compute resources. As in standard methodology, optimal post-processed embeddings can be selected according to multiple proxy-tasks for overall improvement or using a single end-task’s development split—e.g., on a fast baseline model for further time reduction. 2 Which autoencoder variant? For step (2), we found the following autoencoder recipe to work best: A linear autoencoder with one hidden layer, trained via bRMSE (batch-wise r"
W19-4307,C18-1085,0,0.0208014,"test them on the 18 task (MT) setup. Figure 1 shows consistent MT/Σ score improvements for each of the 3 M O RT Y-over-Fasttext runs (red, yellow, green) on 2M, 103M, and 600B vs. base Fasttext (blue 100). Related Work There is a large body of work on information transfer between supervised and unsupervised tasks. First and foremost unsupervisedto-supervised transfer includes using embeddings for supervised tasks. However, transfer also works vice versa, in a supervised-to-unsupervised setup to (learn to) specialize embeddings to better fit a specific supervised signal (Ruder and Plank, 2017; Ye et al., 2018). This includes injecting generally relevant semantics via retrofitting or auxiliary multi-task supervision (Faruqui et al., 2015; Kiela et al., 2018b). Supervised-to-supervised methods provide knowledge transfer between supervised tasks which is exploited successively (Kirkpatrick et al., 2017), jointly (Kiela et al., 2018b) and in joint-succession (Hashimoto et al., 2017). Unsupervised-to-unsupervised transfer is less studied. Dingwall and Potts (2018) proposed a GloVe model-modification that retrofits publicly available GloVe embeddings to produce specialized domain embeddings, while Bolleg"
W19-6103,E17-2040,1,0.899087,"Missing"
W19-6103,P17-1080,0,0.0243907,"ior work in this direction can be found on machine translation (Sennrich and Haddow, 2016; Chen et al., 2017; Li et al., 2017; Passban et al., 2018), work on named entity recognition (Wu et al., 2018) and PoS tagging (Sagot and Mart´ınez Alonso, 2017) who use lexicons, but as n-hot features and without examining the crosslingual aspect. Somewhat complementary to evaluating the utility of linguistic resources empirically is the increasing body of work that uses linguistic insights to try to understand what properties neural-based representations capture (K´ad´ar et al., 2017; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Hupkes et al., 2018). Shi et al. (2016) and Adi et al. (2017) introduced the idea of probing tasks (or ‘diagnostic classifiers’), see Belinkov and Glass for a recent survey (Belinkov and Glass, 2019). Adi et al. (2017) evaluate several kinds of sentence encoders and propose a range of probing tasks around isolated aspects of sentence structure at the surface level (sentence length, word content and word order). This work has been greatly expanded by including both syntactic and semantic probing tasks, careful sampling of probing task training data, and extending the fra"
W19-6103,Q19-1004,0,0.0277942,"agot and Mart´ınez Alonso, 2017) who use lexicons, but as n-hot features and without examining the crosslingual aspect. Somewhat complementary to evaluating the utility of linguistic resources empirically is the increasing body of work that uses linguistic insights to try to understand what properties neural-based representations capture (K´ad´ar et al., 2017; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Hupkes et al., 2018). Shi et al. (2016) and Adi et al. (2017) introduced the idea of probing tasks (or ‘diagnostic classifiers’), see Belinkov and Glass for a recent survey (Belinkov and Glass, 2019). Adi et al. (2017) evaluate several kinds of sentence encoders and propose a range of probing tasks around isolated aspects of sentence structure at the surface level (sentence length, word content and word order). This work has been greatly expanded by including both syntactic and semantic probing tasks, careful sampling of probing task training data, and extending the framework to make it encoder agnostic (Conneau et al., 2018). A general observation here is that task-specific knowledge is needed in order to design relevant diagnostic tasks, which is not always straightforward. For example,"
W19-6103,P18-1246,0,0.0326046,"Missing"
W19-6103,P17-1177,0,0.0243674,"loser to each other in the distributional space. The majority of recent work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are obsolete for deep learning methods. They rely on end-to-end training without resorting to additional linguistic resources. Our study contributes to the increasing literature to show the utility of linguistic resources for deep learning models by providing a deep analysis of a recently proposed model (Plank and Agi´c, 2018). Most prior work in this direction can be found on machine translation (Sennrich and Haddow, 2016; Chen et al., 2017; Li et al., 2017; Passban et al., 2018), work on named entity recognition (Wu et al., 2018) and PoS tagging (Sagot and Mart´ınez Alonso, 2017) who use lexicons, but as n-hot features and without examining the crosslingual aspect. Somewhat complementary to evaluating the utility of linguistic resources empirically is the increasing body of work that uses linguistic insights to try to understand what properties neural-based representations capture (K´ad´ar et al., 2017; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Hupkes et al., 2018). Shi et al. (2016) and Adi et al. (2017) i"
W19-6103,P18-1198,0,0.0393206,"al information, respectively. We employ two binary probing tasks: predicting which words are long, i.e., contain more than 7 characters3 , and predicting which words are in the lexicon. The word length task is included as a task which can be learned independently of whether lexicon information is available to the neural model. Storing length-related information might help the model distinguish suffix patterns of relevance to PoS-tagging. Following Shi et al. (2016) and Gulordava et al (2018), we use a logistic regression classifier setup and a constant input dimensionality of 64 across tasks (Conneau et al., 2018). The classifiers are trained using 10-fold cross-validation for each of three trained runs of each neural model and averaged. We include a majority baseline and report macro F1-scores, as we are dealing with imbalanced classes. The training vocabulary of both probing tasks is restricted to the neural tagger training vocabulary, that is, all word types in the projected training data, as these are the representations which have been subject to updates during training of the neural model. Using the projected data has the advantage that the vocabulary is similar across languages as the data comes"
W19-6103,N15-1184,0,0.0412143,"Missing"
W19-6103,N18-1108,0,0.0190013,"rmed classifiers were trained on character-based word representations from the neural taggers with and without access to lexical information, respectively. We employ two binary probing tasks: predicting which words are long, i.e., contain more than 7 characters3 , and predicting which words are in the lexicon. The word length task is included as a task which can be learned independently of whether lexicon information is available to the neural model. Storing length-related information might help the model distinguish suffix patterns of relevance to PoS-tagging. Following Shi et al. (2016) and Gulordava et al (2018), we use a logistic regression classifier setup and a constant input dimensionality of 64 across tasks (Conneau et al., 2018). The classifiers are trained using 10-fold cross-validation for each of three trained runs of each neural model and averaged. We include a majority baseline and report macro F1-scores, as we are dealing with imbalanced classes. The training vocabulary of both probing tasks is restricted to the neural tagger training vocabulary, that is, all word types in the projected training data, as these are the representations which have been subject to updates during training of t"
W19-6103,L16-1498,0,0.0174157,"ddings offer further linguistically plausible fallback for the remaining OOVs through modeling intraword relations. Through these approaches, multilingual PoS tagging has seen tangible gains from neural methods in the recent years. 2.1 Lexical resources We use linguistic resources that are user-generated and available for many languages. The first is W IKTIONARY, a word type dictionary that maps words to one of the 12 Universal PoS tags (Li et al., 2012; Petrov et al., 2012). The second resource is U NI M ORPH, a morphological dictionary that provides inflectional paradigms for 350 languages (Kirov et al., 2016). For Wiktionary, we use the freely available dictionaries from Li et al. (2012). UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively).1 The sizes of the dictionaries vary considerably, from a few thousand entries (e.g., for Hindi and Bulgarian) to 2M entries (Finnish UniMorph). We study the impact of smaller dictionary sizes in Section 4.1. The tagger we analyze in this paper is an extension of the base tagger, called distant supervision from disparate sources (D S D S) tagger (Plank and Agi´c, 2018). It is trained on projected data and further differs"
W19-6103,P17-1064,0,0.021305,"in the distributional space. The majority of recent work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are obsolete for deep learning methods. They rely on end-to-end training without resorting to additional linguistic resources. Our study contributes to the increasing literature to show the utility of linguistic resources for deep learning models by providing a deep analysis of a recently proposed model (Plank and Agi´c, 2018). Most prior work in this direction can be found on machine translation (Sennrich and Haddow, 2016; Chen et al., 2017; Li et al., 2017; Passban et al., 2018), work on named entity recognition (Wu et al., 2018) and PoS tagging (Sagot and Mart´ınez Alonso, 2017) who use lexicons, but as n-hot features and without examining the crosslingual aspect. Somewhat complementary to evaluating the utility of linguistic resources empirically is the increasing body of work that uses linguistic insights to try to understand what properties neural-based representations capture (K´ad´ar et al., 2017; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Hupkes et al., 2018). Shi et al. (2016) and Adi et al. (2017) introduced the ide"
W19-6103,D12-1127,0,0.0778219,"Missing"
W19-6103,E17-5001,0,0.0406156,"Missing"
W19-6103,N18-1006,0,0.0583435,"Missing"
W19-6103,petrov-etal-2012-universal,0,0.03258,"gs are typically built from massive unlabeled datasets and thus OOVs are less likely to be encountered at test time, while ii) character embeddings offer further linguistically plausible fallback for the remaining OOVs through modeling intraword relations. Through these approaches, multilingual PoS tagging has seen tangible gains from neural methods in the recent years. 2.1 Lexical resources We use linguistic resources that are user-generated and available for many languages. The first is W IKTIONARY, a word type dictionary that maps words to one of the 12 Universal PoS tags (Li et al., 2012; Petrov et al., 2012). The second resource is U NI M ORPH, a morphological dictionary that provides inflectional paradigms for 350 languages (Kirov et al., 2016). For Wiktionary, we use the freely available dictionaries from Li et al. (2012). UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively).1 The sizes of the dictionaries vary considerably, from a few thousand entries (e.g., for Hindi and Bulgarian) to 2M entries (Finnish UniMorph). We study the impact of smaller dictionary sizes in Section 4.1. The tagger we analyze in this paper is an extension of the base tagger, cal"
W19-6103,D18-1061,1,0.807715,"Missing"
W19-6103,P16-2067,1,0.844025,"we investigate the reliance on dictionary size and properties; c) we analyze model-internal representations via a probing task to investigate to what extent model-internal representations capture morphosyntactic information. Our experiments confirm the synergetic effect between a neural tagger and symbolic linguistic knowledge. Moreover, our analysis shows that the composition of the dictionary plays a more important role than its coverage. 2 Methodology Our base tagger is a bidirectional long short-term memory network (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997; Plank et al., 2016) with a rich word encoding model which consists of a character-based biLSTM representation cw ~ paired with pre-trained word embeddings w. ~ Sub-word and especially character-level modeling is currently pervasive in top-performing neural sequence taggers, owing to its capacity to effectively capture morphological features that are useful in labeling out-ofvocabulary (OOV) items. Sub-word information is often coupled with standard word embeddings to mitigate OOV issues. Specifically, i) word embeddings are typically built from massive unlabeled datasets and thus OOVs are less likely to be encou"
W19-6103,W17-6304,0,0.0459679,"Missing"
W19-6103,W16-2209,0,0.0249023,"urce are encouraged to be closer to each other in the distributional space. The majority of recent work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are obsolete for deep learning methods. They rely on end-to-end training without resorting to additional linguistic resources. Our study contributes to the increasing literature to show the utility of linguistic resources for deep learning models by providing a deep analysis of a recently proposed model (Plank and Agi´c, 2018). Most prior work in this direction can be found on machine translation (Sennrich and Haddow, 2016; Chen et al., 2017; Li et al., 2017; Passban et al., 2018), work on named entity recognition (Wu et al., 2018) and PoS tagging (Sagot and Mart´ınez Alonso, 2017) who use lexicons, but as n-hot features and without examining the crosslingual aspect. Somewhat complementary to evaluating the utility of linguistic resources empirically is the increasing body of work that uses linguistic insights to try to understand what properties neural-based representations capture (K´ad´ar et al., 2017; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Hupkes et al., 2018). Shi et al. (2016) and"
W19-6103,D16-1159,0,0.179337,"he base- and DsDs-informed classifiers were trained on character-based word representations from the neural taggers with and without access to lexical information, respectively. We employ two binary probing tasks: predicting which words are long, i.e., contain more than 7 characters3 , and predicting which words are in the lexicon. The word length task is included as a task which can be learned independently of whether lexicon information is available to the neural model. Storing length-related information might help the model distinguish suffix patterns of relevance to PoS-tagging. Following Shi et al. (2016) and Gulordava et al (2018), we use a logistic regression classifier setup and a constant input dimensionality of 64 across tasks (Conneau et al., 2018). The classifiers are trained using 10-fold cross-validation for each of three trained runs of each neural model and averaged. We include a majority baseline and report macro F1-scores, as we are dealing with imbalanced classes. The training vocabulary of both probing tasks is restricted to the neural tagger training vocabulary, that is, all word types in the projected training data, as these are the representations which have been subject to u"
W19-6103,Q13-1001,0,0.0721283,"Missing"
W19-6103,P18-2117,0,0.0290151,"pecific knowledge is needed in order to design relevant diagnostic tasks, which is not always straightforward. For example, Gulordava (2018) investigate whether RNNs trained using a language model objective capture hierarchical syntactic information. They create nonsensical construction so that the RNN cannot rely on lexical or semantic clues, showing that RNNs still capture syntactic properties in sentence embeddings across the four tested languages while obfuscating lexical information. There is also more theoretical work on investigating the capabilities of recurrent neural networks, e.g., Weiss et al. (2018) show that specific types of RNNs (LSTMs) are able to use counting mechanisms to recognize specific formal languages. Finally, linguistic resources can also serve as proxy for evaluation. As recently shown (Agi´c et al., 2017), type-level information from dictionaries approximates PoS tagging accuracy in the absence of gold data for cross-lingual tagger evaluation. Their use of high-frequency word types inspired parts of our analysis. 6 Conclusions We analyze D S D S, a recently-proposed lowresource tagger that symbiotically leverages neural representations and symbolic linguistic knowledge by"
W19-6103,D18-1310,0,0.0232188,"ce prediction follows the commonly perceived wisdom that hand-crafted features are obsolete for deep learning methods. They rely on end-to-end training without resorting to additional linguistic resources. Our study contributes to the increasing literature to show the utility of linguistic resources for deep learning models by providing a deep analysis of a recently proposed model (Plank and Agi´c, 2018). Most prior work in this direction can be found on machine translation (Sennrich and Haddow, 2016; Chen et al., 2017; Li et al., 2017; Passban et al., 2018), work on named entity recognition (Wu et al., 2018) and PoS tagging (Sagot and Mart´ınez Alonso, 2017) who use lexicons, but as n-hot features and without examining the crosslingual aspect. Somewhat complementary to evaluating the utility of linguistic resources empirically is the increasing body of work that uses linguistic insights to try to understand what properties neural-based representations capture (K´ad´ar et al., 2017; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Hupkes et al., 2018). Shi et al. (2016) and Adi et al. (2017) introduced the idea of probing tasks (or ‘diagnostic classifiers’), see Belinkov and Glass fo"
W19-6141,cieri-etal-2004-fisher,0,0.019469,"ibed data and creating more data rather than correcting existing transcriptions provides better performance (Sperber et al., 2016; Novotney and Callison-Burch, 7 11 8 12 See https://github.com/facebookresearch/LASER See https://visl.sdu.dk/visl/da/tools/ 9 See http://opus.nlpl.eu/ 10 See www.clarin.eu/resource-families /parallel-corpora 13 14 See github.com/fnielsen/awesome-danish for links. github.com/kaldi-asr/kaldi/tree/master/egs/sprakbanken. Offline means it cannot recognise speech in real-time. https://www.dictus.dk 2010). This was used to create the Fisher corpus, a standard benchmark (Cieri et al., 2004). We recommend this approach, coupled with release of publicly-owned parallel data (e.g. subtitles & audio from Danmarks Radio archives; Danish parliament speeches with transcriptions). 5.2 Speech synthesis. The synthesisers available online are eSpeak and Responsive Voice.15 Spr˚akbanken contains a section of data that can be used to train a speech synthesiser. Recently, toolkits to train DNNbased speech synthesisers have become available online16 because they can be trained on aligned speech and text data like ASR systems, but we are not aware of any systems or recipes to train Danish speech"
W19-6141,W19-6138,1,0.835945,"Missing"
W19-6141,W17-4418,1,0.700825,"Missing"
W19-6141,E14-2016,1,0.813569,"thers, the technology is less advanced. It is a more coarse-grained task than sense tagging which has received attention in Danish (Alonso et al., 2015; Pedersen et al., 2015). Many NER results for Danish are outdated and based on closed, systems. E.g., Bick (2004) offers details of a system trained on 43K tokens but reports no F1. One has to pay for this tool and the data is not open. Johannessen et al. (2005) mention efforts in Danish NER but the research lies behind a paywall that the authors do not have access through, and we failed to find other artefacts of this research. More recently, Derczynski et al. (2014) describe a dataset used to train a recognizer that is openly available in GATE (Cunningham et al., 2012). Current efforts focus on addressing the problem of data sparsity and on providing accessible tools (Plank, 2019; Derczynski, 2019), including as part of the ITU Copenhagen open tool set for Danish NLP.5 In contrast, for English, F1 scores are in the mid-90s (e.g., 94.03 from Chiu and Nichols (2016)). Researchers have since moved on to more exotic challenges, such as nested entities, emerging entities, and clinical information extraction (Katiyar and Cardie, 2018; Derczynski et al., 2017;"
W19-6141,W14-2602,1,0.834,"xt, which presents its own hurdles (Balahur and Jacquet, 2015), leading to a series of shared tasks (Rosenthal et al., 2015). The afinn tool (Nielsen, 2011) performs sentiment analysis using a lexicon consisting of 3552 words, labelled with a value between -5 (very negative) and 5 (very positive). This approach only considers the individual words in the input, and therefore the context is lost. Full-text annotations for sentiment in Danish have appeared in previous multilingual work, including systems reaching F-scores of 0.924 on same-domain Trustpilot reviews and 0.462 going across domains (Elming et al., 2014). Alexandra Institute offer a model6 based on Facebook’s 6 See https://github.com/alexandrainst/danlp LASER multilingual sentiment tool.7 This is total of Danish sentiment text tools, and all are included incidentally as part of multilingual efforts. 4 Machine Translation Machine translation (MT) is the automatic translation from one language to another. MT typically thrives on sentence-aligned data, where sentences in the source language are paired with their translation in the target language. Tools specifically designed in Denmark for Danish are not open and often only translate one way;8 t"
W19-6141,C16-1084,0,0.0503077,"Missing"
W19-6141,N18-1079,0,0.0186264,"s research. More recently, Derczynski et al. (2014) describe a dataset used to train a recognizer that is openly available in GATE (Cunningham et al., 2012). Current efforts focus on addressing the problem of data sparsity and on providing accessible tools (Plank, 2019; Derczynski, 2019), including as part of the ITU Copenhagen open tool set for Danish NLP.5 In contrast, for English, F1 scores are in the mid-90s (e.g., 94.03 from Chiu and Nichols (2016)). Researchers have since moved on to more exotic challenges, such as nested entities, emerging entities, and clinical information extraction (Katiyar and Cardie, 2018; Derczynski et al., 2017; Wang et al., 2018). To improve Danish NER seems simple: we need open tools and annotated data. Fortunately, the landscape for Danish NER is somewhat barren, and so first movers have an advantage. Openly contributing such a dataset to a shared resource would mean that Danish NER would be included in multilingual NER exercises, thus enabling the rest of the world to also work on improving entity recognition for Danish. Danish clinical NLP lags behind that for other languages, even when English is taken out of the picture, with for example four times as many Pubmed refe"
W19-6141,L16-1262,0,0.0310059,"Missing"
W19-6141,N10-1024,0,0.0506236,"Missing"
W19-6141,P16-2067,1,0.864069,"Missing"
W19-6141,S15-2078,0,0.0151706,"bly be able to use belles lettres in English to better process belles lettres in Danish, the idiosyncracies in clinical notes mean that one language’s clinical note data is unlikely to hugely help understanding clinical notes in other languages. 5 See nlp.itu.dk/resources/ and github.com/ITUnlp Sentiment Extraction Sentiment analysis is a long-standing NLP task for predicting the sentiment of an utterance, in general or related to a target (Liu, 2012). It has been investigated for non-formal text, which presents its own hurdles (Balahur and Jacquet, 2015), leading to a series of shared tasks (Rosenthal et al., 2015). The afinn tool (Nielsen, 2011) performs sentiment analysis using a lexicon consisting of 3552 words, labelled with a value between -5 (very negative) and 5 (very positive). This approach only considers the individual words in the input, and therefore the context is lost. Full-text annotations for sentiment in Danish have appeared in previous multilingual work, including systems reaching F-scores of 0.924 on same-domain Trustpilot reviews and 0.462 going across domains (Elming et al., 2014). Alexandra Institute offer a model6 based on Facebook’s 6 See https://github.com/alexandrainst/danlp LA"
W19-6141,L16-1314,0,0.0127276,"nre is important for acoustic model performance, so language models trained on newswire, Wikipedia, Twitter data or similar will not work as well as language models trained on speech transcriptions. Dictus Sun has access to 11 years of transcribed speeches and so may work well for monologues in that domain, but we have not been able to test the system and cannot know its performance on spontaneous speech. A lot of medium quality transcribed data is better than a little perfectly transcribed data and creating more data rather than correcting existing transcriptions provides better performance (Sperber et al., 2016; Novotney and Callison-Burch, 7 11 8 12 See https://github.com/facebookresearch/LASER See https://visl.sdu.dk/visl/da/tools/ 9 See http://opus.nlpl.eu/ 10 See www.clarin.eu/resource-families /parallel-corpora 13 14 See github.com/fnielsen/awesome-danish for links. github.com/kaldi-asr/kaldi/tree/master/egs/sprakbanken. Offline means it cannot recognise speech in real-time. https://www.dictus.dk 2010). This was used to create the Fisher corpus, a standard benchmark (Cieri et al., 2004). We recommend this approach, coupled with release of publicly-owned parallel data (e.g. subtitles & audio fro"
W19-6141,D18-1334,0,0.0223992,", where sentences in the source language are paired with their translation in the target language. Tools specifically designed in Denmark for Danish are not open and often only translate one way;8 this makes them impossible to benchmark. On the other hand, it is rare that translation tools include Danish in evaluations. Popular pairs are en-fr, en-de, en-zh and en-ja, which tend to be present in most large-scale research exercises (Johnson et al., 2017; Chen et al., 2018). When Danish does appear, it is typically in order to make a linguistic point, rather than improve MT for Danish-speakers (Vanmassenhove et al., 2018). However, even given that, there is a relatively large amount of Danish parallel text (that MT relies on): Opus9 reports 63M sentences for English-Danish, 70M for English-Swedish, 117M for English-German, and 242M for EnglishFrench. A large amount of the Danish data comes from colloquial, crowdsourced sites like OpenSubtitles.net and Tatoeba. Just as it’s incidental that Danish is included in these (i.e. their translations is not purpose-created for Danish, which is a signal of quality), there are also no dedicated Danish parallel texts listed on CLARIN.eu.10 The result is thus that Danish MT"
W19-6141,K18-2001,0,0.0230321,"Missing"
W19-6143,W13-3520,0,0.0609577,"language understanding (NLU), and important for information extraction, relation extraction, question answering and even privacy protection. However, the scarcity of publicly-available human annotated datasets has resulted in a lack of evaluation for languages beyond a selected set (e.g., those covered in early shared tasks like Dutch, German, English, Spanish), despite the fact that NER tools exists or recently emerged for other languages. One such case is Danish, for which NER dates back as early as (Bick, 2004) and tools exist (Bick, 2004; Derczynski et al., 2014; Johannessen et al., 2005; Al-Rfou et al., 2013) but lack empirical evaluation. Contemporarily, there exists a surge of interest in porting NLU components quickly and cheaply to new languages. This includes cross-lingual transfer methods that exploit resources from existing high-resource languages for zero-shot or few-shot learning. This line of research is blooming, particularly since the advent of neural NER, which holds the state of the art (Yadav and Bethard, 2018). However, neither neural tagging nor crosslingual transfer has been explored for Danish NER, a gap we seek to fill in this paper. Contributions We present a) publiclyavailabl"
W19-6143,W15-4319,0,0.0599295,"Missing"
W19-6143,bick-2004-named,0,0.742687,"erformance of Danish NER. 1 Introduction Named entity recognition is a key step for natural language understanding (NLU), and important for information extraction, relation extraction, question answering and even privacy protection. However, the scarcity of publicly-available human annotated datasets has resulted in a lack of evaluation for languages beyond a selected set (e.g., those covered in early shared tasks like Dutch, German, English, Spanish), despite the fact that NER tools exists or recently emerged for other languages. One such case is Danish, for which NER dates back as early as (Bick, 2004) and tools exist (Bick, 2004; Derczynski et al., 2014; Johannessen et al., 2005; Al-Rfou et al., 2013) but lack empirical evaluation. Contemporarily, there exists a surge of interest in porting NLU components quickly and cheaply to new languages. This includes cross-lingual transfer methods that exploit resources from existing high-resource languages for zero-shot or few-shot learning. This line of research is blooming, particularly since the advent of neural NER, which holds the state of the art (Yadav and Bethard, 2018). However, neither neural tagging nor crosslingual transfer has been expl"
W19-6143,W98-1614,0,0.582113,"13), we add an annotation layer for Named Entities to the development and test sets of the Danish section of the Universal Dependencies (UD) treebank (Nivre et al., 2016; Johannsen et al., 2015). To answer RQ2, we further annotate a very small portion of the training data, i.e., the first 5,000 and 10,000 tokens. Examples are shown in Figure 1. Dataset statistics are provided in Table 2. The Danish UD treebank (Danish-DDT) is a conversion of the Copenhagen Dependency Treebank (CDT). CDT (Kromann et al., 2003) consists of 5,512 sentences and 100k tokens, originating from the PAROLE-DK project (Bilgram and Keson, 1998). In contrast to original CDT and the PAROLE tokenization scheme, starting from the Danish UD has the advantage that it is closer to everyday language, as it splits tokens which were originally joined (such as ‘i alt’). We follow the CoNLL 2003 annotation guidelines (Tjong Kim Sang and De Meulder, 2003) and annotate proper names of four types: person (PER), location (LOC), organization (ORG) and miscellaneous (MISC). MISC contains for example names of products, drinks or film titles. 2.2 Cross-lingual transfer We train a model on English (a medium and high resource setup, see details in Sectio"
W19-6143,A00-1031,0,0.686726,"Missing"
W19-6143,E14-2016,0,0.743955,"Named entity recognition is a key step for natural language understanding (NLU), and important for information extraction, relation extraction, question answering and even privacy protection. However, the scarcity of publicly-available human annotated datasets has resulted in a lack of evaluation for languages beyond a selected set (e.g., those covered in early shared tasks like Dutch, German, English, Spanish), despite the fact that NER tools exists or recently emerged for other languages. One such case is Danish, for which NER dates back as early as (Bick, 2004) and tools exist (Bick, 2004; Derczynski et al., 2014; Johannessen et al., 2005; Al-Rfou et al., 2013) but lack empirical evaluation. Contemporarily, there exists a surge of interest in porting NLU components quickly and cheaply to new languages. This includes cross-lingual transfer methods that exploit resources from existing high-resource languages for zero-shot or few-shot learning. This line of research is blooming, particularly since the advent of neural NER, which holds the state of the art (Yadav and Bethard, 2018). However, neither neural tagging nor crosslingual transfer has been explored for Danish NER, a gap we seek to fill in this pa"
W19-6143,N13-1014,0,0.0125477,"ach We investigate the following questions: RQ1: To what extent can we transfer a NER tagger to Danish from existing English resources? RQ2: How does cross-lingual transfer compare to annotating a very small amount of inlanguage data (zero-shot vs few-shot learning)? RQ3: How accurate are existing NER systems for Danish? 2.1 NER annotation To answer these questions, we need gold annotated data. Access to existing resources is limited as they are not available online or behind a paywall. Therefore, we annotate NERs on top of publicly available data.1 In line with limited budget for annotation (Garrette and Baldridge, 2013), we add an annotation layer for Named Entities to the development and test sets of the Danish section of the Universal Dependencies (UD) treebank (Nivre et al., 2016; Johannsen et al., 2015). To answer RQ2, we further annotate a very small portion of the training data, i.e., the first 5,000 and 10,000 tokens. Examples are shown in Figure 1. Dataset statistics are provided in Table 2. The Danish UD treebank (Danish-DDT) is a conversion of the Copenhagen Dependency Treebank (CDT). CDT (Kromann et al., 2003) consists of 5,512 sentences and 100k tokens, originating from the PAROLE-DK project (Bil"
W19-6143,X98-1012,0,0.112885,"paired with S MALL amounts of gold data. Per-Entity evaluation shows that ours outperforms Polyglot except for Location, which is consistent across evaluation data (Table 4). Overall we find that very little data paired with dense representations yields an effective NER quickly. 5 Related Work Named Entity Recognition has a long history in NLP research. While interest in NER originally arose mostly from a question answering perspective, it developed into an independent task through the pioneering shared task organized by the Message Understanding Conference (MUC) (Grishman and Sundheim, 1996; Grishman, 1998). Since then, many shared task for NER have been organized, including CoNLL (Tjong Kim Sang and De Meulder, 2003) for newswire and WNUT for social media data (Baldwin et al., 2015). While Danish NER tools and data exists (Bick, 2004; Derczynski et al., 2014; Johannessen et al., 2005; Al-Rfou et al., 2013), there was a lack of reporting F1 scores. Supersense tagging, a task close to NER has received attention (Mart´ınez Alonso et al., 2015). The range of methods that have been proposed for NER is broad. Early methods focused on hand-crated rule-based methods with lexicons and orthographic featu"
W19-6143,C96-1079,0,0.332559,"NER from M EDIUM source data paired with S MALL amounts of gold data. Per-Entity evaluation shows that ours outperforms Polyglot except for Location, which is consistent across evaluation data (Table 4). Overall we find that very little data paired with dense representations yields an effective NER quickly. 5 Related Work Named Entity Recognition has a long history in NLP research. While interest in NER originally arose mostly from a question answering perspective, it developed into an independent task through the pioneering shared task organized by the Message Understanding Conference (MUC) (Grishman and Sundheim, 1996; Grishman, 1998). Since then, many shared task for NER have been organized, including CoNLL (Tjong Kim Sang and De Meulder, 2003) for newswire and WNUT for social media data (Baldwin et al., 2015). While Danish NER tools and data exists (Bick, 2004; Derczynski et al., 2014; Johannessen et al., 2005; Al-Rfou et al., 2013), there was a lack of reporting F1 scores. Supersense tagging, a task close to NER has received attention (Mart´ınez Alonso et al., 2015). The range of methods that have been proposed for NER is broad. Early methods focused on hand-crated rule-based methods with lexicons and o"
W19-6143,N19-2023,0,0.0173921,"nt of deep learning and the seminal work by (Collobert et al., 2011), state-of-the-art NER systems typically rely on feature-inferring encoder-decoder models that extract dense embeddings from word and subword embeddings, including affixes (Yadav and Bethard, 2018), often outperforming neural architectures that include lexicon information such as gazetteers. Recently, there has been a surge of interest in cross-lingual transfer of NER models (Xie et al., 2018). This includes work on transfer between distant languages (Rahimi et al., 2019) and work on projecting from multiple source languages (Johnson et al., 2019). 6 Conclusions We contribute to the transfer learning literature by providing a first study on the effectiveness of exploiting English NER data to boost Danish NER performance.2 We presented a publicly-available evaluation dataset and compare our neural cross-lingual Danish NER tagger to existing systems. Our experiments show that a very small amount of in-language NER data pushes cross-lingual transfer, resulting in an effective Danish NER system. 2 Available at: https://github.com/ ITUnlp/transfer_ner Acknowledgements We kindly acknowledge the support of NVIDIA Corporation for the donation"
W19-6143,L16-1262,0,0.034861,"Missing"
W19-6143,W03-0419,0,0.600469,"Missing"
W19-6143,D18-1034,0,0.048355,"Missing"
W19-6143,C18-1182,0,0.112486,"d for other languages. One such case is Danish, for which NER dates back as early as (Bick, 2004) and tools exist (Bick, 2004; Derczynski et al., 2014; Johannessen et al., 2005; Al-Rfou et al., 2013) but lack empirical evaluation. Contemporarily, there exists a surge of interest in porting NLU components quickly and cheaply to new languages. This includes cross-lingual transfer methods that exploit resources from existing high-resource languages for zero-shot or few-shot learning. This line of research is blooming, particularly since the advent of neural NER, which holds the state of the art (Yadav and Bethard, 2018). However, neither neural tagging nor crosslingual transfer has been explored for Danish NER, a gap we seek to fill in this paper. Contributions We present a) publiclyavailable evaluation data to encourage research on Danish NER; b) an empirical comparison of two existing NER systems for Danish to a neural model; c) an empirical evaluation of learning an effective NER tagger for Danish via cross-lingual transfer paired with very little labeled data. 2 Approach We investigate the following questions: RQ1: To what extent can we transfer a NER tagger to Danish from existing English resources? RQ2"
