P17-3019,Fast Forward Through Opportunistic Incremental Meaning Representation Construction,2017,10,0,2,0,32551,petr babkin,"Proceedings of {ACL} 2017, Student Research Workshop",0,None
W16-3805,Extra-Specific Multiword Expressions for Language-Endowed Intelligent Agents,2016,-1,-1,2,0.81689,33676,marjorie mcshane,Proceedings of the Workshop on Grammar and Lexicon: interactions and interfaces ({G}ram{L}ex),0,"Language-endowed intelligent agents benefit from leveraging lexical knowledge falling at different points along a spectrum of compositionality. This means that robust computational lexicons should include not only the compositional expectations of argument-taking words, but also non-compositional collocations (idioms), semi-compositional collocations that might be difficult for an agent to interpret (e.g., standard metaphors), and even collocations that could be compositionally analyzed but are so frequently encountered that recording their meaning increases the efficiency of interpretation. In this paper we argue that yet another type of string-to-meaning mapping can also be useful to intelligent agents: remembered semantic analyses of actual text inputs. These can be viewed as super-specific multi-word expressions whose recorded interpretations mimic a person{'}s memories of knowledge previously learned from language input. These differ from typical annotated corpora in two ways. First, they provide a full, context-sensitive semantic interpretation rather than select features. Second, they are are formulated in the ontologically-grounded metalanguage used in a particular agent environment, meaning that the interpretations contribute to the dynamically evolving cognitive capabilites of agents configured in that environment."
W08-2214,The Idiom{--}Reference Connection,2008,8,1,2,0.969697,33676,marjorie mcshane,Semantics in Text Processing. {STEP} 2008 Conference Proceedings,0,"Idiom processing and reference resolution are two complex aspects of text processing that are commonly treated in isolation. However, closer study of the reference needs of some idioms suggests that these two phenomena will need to be treated together to support high-end NLP applications. Using evidence from Russian and English, this article describes a number of classes of idioms according to their reference needs and suggests a method of lexical encoding which, supplemented by procedural semantic routines, can adequately support the full semantic and referential interpretation of these idioms."
W08-2215,Resolving Paraphrases to Support Modeling Language Perception in an Intelligent Agent,2008,15,9,1,1,32552,sergei nirenburg,Semantics in Text Processing. {STEP} 2008 Conference Proceedings,0,"When interacting with humans, intelligent agents must be able not only to understand natural language inputs but also to remember them and link their content with the contents of their memory of event and object instances. As inputs can come in a variety of forms, linking to memory can be successful only when paraphrasing relations are established between the meaning of new input and the content of the agent's memory. This paper discusses a variety of types of paraphrases relevant to this task and describes the way we implement this capability in a virtual patient application."
W08-2225,Baseline Evaluation of {WSD} and Semantic Dependency in {O}nto{S}em,2008,6,9,1,1,32552,sergei nirenburg,Semantics in Text Processing. {STEP} 2008 Conference Proceedings,0,"This paper presents the evaluation of a subset of the capabilities of the On-toSem semantic analyzer conducted in the framework of the Shared Task for the STEP 2008 workshop. We very briefly describe OntoSem's components and knowledge resources, describe the work preparatory to the evaluation (the creation of gold standard basic text meaning representations) and present OntoSem's performance on word sense disambiguation and determination of semantic dependencies. The paper also contains elements of a methodological discussion."
W08-1507,Language Understanding in {M}aryland Virtual Patient,2008,2,2,1,1,32552,sergei nirenburg,Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications,0,"This paper discusses language understanding in the Maryland Virtual Patient environment. Language understanding is just one of many cognitive functions of the virtual patients in MVP, others including decision making about healthcare and lifestyle, and the experiencing and remembering of interoceptive events."
W05-0310,Semantically Rich Human-Aided Machine Annotation,2005,9,20,2,1,33676,marjorie mcshane,Proceedings of the Workshop on Frontiers in Corpus Annotations {II}: Pie in the Sky,0,"This paper describes a semantically rich, human-aided machine annotation system created within the Ontological Semantics (OntoSem) environment using the DEKADE toolset. In contrast to mainstream annotation efforts, this method of annotation provides more information at a lower cost and, for the most part, shifts the maintenance of consistency to the system itself. In addition, each tagging effort not only produces knowledge resources for that corpus, but also leads to improvements in the knowledge environment that will better support subsequent tagging efforts."
I05-7007,Increasing Understanding: Interpreting Events of Change,2005,0,1,1,1,32552,sergei nirenburg,Proceedings of {O}nto{L}ex 2005 - Ontologies and Lexical Resources,0,None
2005.mtsummit-papers.9,Document Authoring the {B}ible for Minority Language Translation,2005,5,5,2,0.937191,38698,stephen beale,Proceedings of Machine Translation Summit X: Papers,0,"This paper describes one approach to document authoring and natural language generation being pursued by the Summer Institute of Linguistics in cooperation with the University of Maryland, Baltimore County. We will describe the tools provided for document authoring, including a glimpse at the underlying controlled language and the semantic representation of the textual meaning. We will also introduce The Bible Translator{'}s Assistant{\copyright} (TBTA), which is used to elicit and enter target language data as well as perform the actual text generation process. We conclude with a discussion of the usefulness of this paradigm from a Bible translation perspective and suggest several ways in which this work will benefit the field of computational linguistics."
W04-2601,{O}nto{S}em Methods for Processing Semantic Ellipsis,2004,9,8,3,1,33676,marjorie mcshane,Proceedings of the Computational Lexical Semantics Workshop at {HLT}-{NAACL} 2004,0,"This paper describes various types of semantic ellipsis and underspecification in natural language, and the ways in which the meaning of semantically elided elements is reconstructed in the Ontological Semantics (OntoSem) text processing environment. The description covers phenomena whose treatment in OntoSem has reached various levels of advancement: fully implemented, partially implemented, and described algorithmically outside of implementation. We present these research results at this point -- prior to full implementation and extensive evaluation -- for two reasons: first, new descriptive material is being reported; second, some subclasses of the phenomena in question will require a truly long-term effort whose results are best reported in installments."
W04-0904,{O}nto{S}em and {SIMPLE}: Two multi-lingual world views,2004,4,7,3,1,33676,marjorie mcshane,Proceedings of the 2nd Workshop on Text Meaning and Interpretation,0,"In this paper we compare programs of work that aim to develop broad coverage cross-linguistic resources for NLP: Ontological Semantics (OntoSem) and SIMPLE. The approaches taken in these projects differ in three notable respects: the use of an ontology versus a word net as the semantic substrate; the development of knowledge resources inside of as opposed to outside of a processing environment; and the development of lexicons for multiple languages based on a single core lexicon or without such a core (i.e., in parallel fashion). In large part, these differences derive from project-driven, real-world requirements and available resources -- a reflection of their being practical rather than theoretical projects. However, that being said, we will suggest certain preferences regarding the content and development of NLP resources with a view toward both short- and long-term, high-level language processing goals."
W04-0905,Evaluating the performance of the {O}nto{S}em semantic analyzer,2004,3,16,1,1,32552,sergei nirenburg,Proceedings of the 2nd Workshop on Text Meaning and Interpretation,0,"This paper describes an innovative evaluation regimen developed for the text meaning representations (TMRs) produced by the Ontological Semantic (OntoSem) general purpose syntactic-semantic analyzer. The goal of evaluation is not only to determine the quality of TMRs for given texts, but also to assign blame for various classes of errors, thus suggesting directions for continued work on both knowledge resources and processors. The paper includes descriptions of the OntoSem processing environment, the evaluation regime itself and results from our first evaluation effort."
W04-0906,Question answering using ontological semantics,2004,8,26,4,1,38698,stephen beale,Proceedings of the 2nd Workshop on Text Meaning and Interpretation,0,"This paper describes the initial results of an experiment in integrating knowledge-based text processing with real-world reasoning in a question answering system. Our MOQA meaning-oriented question answering system seeks answers to questions not in open text but rather in a structured fact repository whose elements are instances of ontological concepts extracted from the text meaning representations (TMRs) produced by the OntoSem text analyzer. The query interpretation and answer content formulation modules of MOQA use the same knowledge representation substrate and the same static knowledge resources as the ontological semantic (OntoSem) semantic text analyzer. The same analyzer is used for deriving the meaning of questions and of texts from which the fact repository content is extracted. Inference processes in question answering rely on ontological scripts (complex events) that also support reasoning for purely NLP-related purposes, such as ambiguity resolution in its many guises."
nirenburg-etal-2004-rationale,The Rationale for Building an Ontology Expressly for {NLP},2004,11,12,1,1,32552,sergei nirenburg,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,None
mcshane-etal-2004-meaning,Some Meaning Procedures of Ontological Semantics,2004,1,13,3,1,33676,marjorie mcshane,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper presents implemented algorithms for interpreting the meaning of certain context-dependent lexical items within the Ontological Semantic text processing environment. We discuss the form, function and rationale behind three meaning procedures, all of which are, in a certain sense, numerically oriented. We show that only a knowledge-rich processing system can fully interpret such entities, and that an integrated combination of static resources and processors provides sufficient foundation for high-quality text interpretation."
W03-0904,Operative strategies in ontological semantics,2003,2,7,1,1,32552,sergei nirenburg,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Text Meaning,0,"In this paper, we briefly and informally illustrate, using a few annotated examples, the static and dynamic knowledge resources of ontological semantics. We then present the main motivations and desiderata of our approach and then discuss issues related to making ontological-semantic applications feasible through the judicious stepwise enhancement of static and dynamic knowledge sources while at all times maintaining a working system."
W03-0905,The genesis of a script for bankruptcy in ontological semantics,2003,19,18,2,0.994338,51619,victor raskin,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Text Meaning,0,"This paper describes the creation of a script in the framework of ontological semantics as the formal representation of the complex event BANKRUPTCY. This script for BANKRUPTCY serves as the exemplary basis for a discussion of the general motivations for including scripts in NLP, as well as the discovery process for, and format of, scripts for the purposes of processing coreference and inferencing which are required, for example, in high-end Q&A and IE applications."
W02-1303,Why {NLP} Should Move into {IAS},2002,9,10,2,1,51619,victor raskin,{COLING}-02: A Roadmap for Computational Linguistics,0,"The paper introduces the ways in which methods and resources of natural language processing (NLP) can be fruitfully employed in the domain of information assurance and security (IAS). IAS may soon claim a very prominent status both conceptually and in terms of future funding for NLP, alongside or even instead of established applications, such as machine translation (MT). After a brief summary of theoretical premises of NLP in general and of ontological semantics as a specific approach to NLP developed and/or practiced by the authors, the paper reports on the interaction between NLP and IAS through brief discussions of some implemented and planned NLP-enhanced IAS systems at the Center for Education and Research in Information Assurance and Security (CERIAS). The rest of the paper deals with the milestones and challenges in the future interaction between NLP and IAS as well as the role of a representational, meaning-based NLP approach in that future."
moreno-ortiz-etal-2002-new,New Developments in Ontological Semantics,2002,13,3,3,0,53431,antonio ortiz,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"In this paper we discuss ongoing activity within the approach to natural language processing known as ontological semantics, as defined in Nirenburg and Raskin (forthcoming). After a brief discussion of the principal tenets on which this approach is built, and a revision of extant implementations that have led toward its present form, we concentrate on some specific aspects that are key to the development of this approach, such as the acquisition of the semantics of lexical items and, intimately connected with this, the ontology, the central resource in this approach. Although we review the fundamentals of the approach, the focus is on practical aspects of implementation, such as the automation of static knowledge acquisition and the acquisition of scripts to enrich the ontology further. 1. Fundamentals of ontological semantics Ontological semantics is defined as xe2x80x9can integrated set of complex theories, methodologies, descriptions and implementationsxe2x80x9d (Nirenburg and Raskin, forthcoming), where a theory is the set of statements that determine the format of descriptions -obtained by applying certain methodologiesof the phenomena that the theory deals with. From this definition it follows that ontological semantics places strong emphasis on content rather than formalism. This characteristic has a strong impact on all four aspects of the approach, which is highly eclectic with regard to both representation and processing of content, linguistic or otherwise. It also implies, however, that a substantial effort in terms of acquisition of the resources is required prior to the successful implementation of the various processors that the approach requires. Historically, a number of research projects have contributed to bring ontological semantics into its current state. These projects have been aimed at producing robust large-scale natural language processing systems to be used in machine translation, information retrieval and extraction, text summarization, and other such tasks. Perhaps the most relevant ones are Dionysus (Monarch et al., 1989), Pangloss (Nirenburg, 1994), Mikrokosmos (Nirenburg et al., 1995), CAMBIO (Nirenburg, 2000a), and CREST (Nirenburg, 2000b). 1.1. Theory and methodologies We define theory as a set of statements that determine the format of descriptions of phenomena in the purview of the theory. A theory is effective if it comes with an explicit methodology for acquiring these descriptions. A theory associated with an application is interested in descriptions that support the work of an application. Figure 1 specifies how that schema applies to ontological semantics (the general notions are listed as headers in the four boxes; their interpretation for ontological semantics is given in the rest of the text in the boxes). Figure 1: Theory, methodologies, and applications The theory of ontological semantics includes the format and the semantics of dynamic knowledge sources, i.e., text meaning representations as well as static knowledge sources: the ontology, the fact database, the lexicons and the onomasticons as well as the generic processing architecture for analysis of meaning and its manipulation, including the generation of text off of it. The description part in ontological semantics includes all 1 The Text Meaning Representation (TMR) is the format used for representing text meaning in ontological semantics. The TMR is constructed compositionally from the meaning of the individual elements, defined either ontologically or procedurally, that constitute texts: words, bound morphemes, syntactic structures, and word, phrase, and clause order in the input text. A proper description of their construction process and format falls beyond the scope of this paper. See Nirenburg and Raskin (forthcoming) for an in-depth discussion. the knowledge sources, both static and dynamic (generic procedures for extraction, representation and manipulation of meaning), implemented to provide full coverage for a language (or languages) and the world. In practice, an ontological semantic description is always partial, covering only a subset of subject domains and sublanguages, and constantly under development, through the process of acquisition and as a side effect of the operation of any applications based on ontological semantics. The methodology of ontological semantics consists of acquisition of the static knowledge sources, discussed in section 3, and of the procedures for producing and manipulating dynamic knowledge structures."
2002.tmi-papers.4,Two experiments in situated {MT},2002,4,0,2,1,53660,jim cowie,Proceedings of the 9th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,"More often than not, MT these days is delivered as a component of a comprehensive end-toend NLP application. This paper presents two applications that integrate MT with other NLP processes. The first of the two combines MT with crosslingual information retrieval. The second environment uses MT, together with summarization and information extraction techniques, to generate monolingual (English) documents based on information extracted from documents in various languages. In particular, this application generates a time-stamped list of events connected to a particular person. One of the key factors in the document assembly process is the assignment of absolute dates to each sentence produced by the system. Both applications use a general purpose computational architecture that centers on an annotated document collection."
sheremetyeva-nirenburg-2000-towards,Towards A Universal Tool For {NLP} Resource Acquisition,2000,20,7,2,1,18939,svetlana sheremetyeva,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"This paper describes an approach to developing a universal tool for eliciting, from a non-expert human user, knowledge about any language L. The purpose of this elicitation is rapid development of NLP systems. The approach is described on the example of the syntax module of the Boas knowledge elicitation system for a quick ramp up of a standard transfer-based machine translation system from L into English. The preparation of knowledge for the MT system is carried out into two stages; the acquisition of descriptive knowledge about L and using the descriptive knowledge to derive operational knowledge for the system. Boas guides the acquisition process using data-driven, expectation-driven and goal-driven methodologies."
C00-2147,The Week at a Glance - Cross-language Cross-document Information Extraction and Translation,2000,1,2,4,1,53660,jim cowie,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"Work on the production of texts in English describing instances of a particular event type from multiple news sources will be described. A system has been developed which extracts events, such as meetings, from texts in English, Russian, Spanish, and Japanese. The extraction is currently carried out using only ontological information. The results of a set of such extractions were combined to produce a table of event instances, date stamped, with links back to the original documents. The original documents can then be summarized and translated by the system on demand.By using techniques from information retrieval, information extraction, summarization, and machine translation, in a multi-lingual environlment, new documents can be produced which provide at a glance access to news ell events from multiple sources.The paper concludes with a discussion of the key resources which need to be developed to enhance the accuracy and coverage of the techniques used in our experiment."
C00-2168,Acquisition of a Language Computational Model for {NLP},2000,8,2,2,1,18939,svetlana sheremetyeva,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,This paper describes an approach to actively acquire a language computational model. The purpose of this acquisition is rapid development of NLP systems. The model is created with the syntax module of the Boas knowledge elicitation system for a quick ramp up of a standard transfer-based machine translation system from L into English.
2000.bcs-1.23,Generating personal profiles,2000,-1,-1,2,1,53660,jim cowie,Proceedings of the International Conference on Machine Translation and Multilingual Applications in the new Millennium: MT 2000,0,None
2000.amta-tutorials.1,Ontological semantics,2000,-1,-1,1,1,32552,sergei nirenburg,Proceedings of the Fourth Conference of the Association for Machine Translation in the Americas: Tutorial Descriptions,0,None
W99-0703,Practical Bootstrapping of Morphological Analyzers,1999,17,13,2,0,6238,kemal oflazer,{EACL} 1999: {C}o{NLL}-99 Computational Natural Language Learning,0,"This paper presents a semi-automatic technique for developing broad-coverage finite-state morphological analyzers for any language. It consists of three components-elicitation of linguistic information from humans, a machine learning bootstrapping scheme and a testing environment. The three components are applied iteratively until a threshold of output quality is attained. The initial application of this technique is for morphology of low-density languages in the context of the Expedition project at NMSU CRL. This elicitbuild-test technique compiles lexical and inflectional information elicited from a human into a finite state transducer lexicon and combines this with a sequence of morphographemic rewrite rules that is induced using transformation-based learning from the elicited examples. The resulting morphological analyzer is then tested against a test suite, and any corrections are fed back into the learning procedure that builds an improved analyzer."
1999.mtsummit-1.47,Interactive {MT} as support for non-native language authoring,1999,14,1,2,1,18939,svetlana sheremetyeva,Proceedings of Machine Translation Summit VII,0,"The paper describes an approach to developing an interactive MT system for translating technical texts on the example of translating patent claims between Russian and English. The approach conforms to the human-aided machine translation paradigm. The system is meant for a source language (SL) speaker who does not know the target language (TL). It consists of i) an analysis module which includes a submodule of interactive syntactic analysis of SL text and a submodule of fully automated morphological analysis, ii) an automatic module for transferring the lexical and partially syntactic content of SL text into a similar content of the TL text and iii) a fully automated TL text generation module which relies on knowledge about the legal format of TL patent claims. An interactive analysis module guides the user through a sequence of SL analysis procedures, as a result of which the system produces a set of internal knowledge structures which serve as input to the TL text generation. Both analysis and generation rely heavily on the analysis of the sublanguage of patent claims. The model has been developed for English and Russian as both SLs and TLs but is readily extensible to other languages."
1999.mtsummit-1.61,Using a target language model for domain independent lexical disambiguation,1999,5,0,3,1,53660,jim cowie,Proceedings of Machine Translation Summit VII,0,"In this paper we describe a lexical disambiguation algorithm based on a statistical language model we call maximum likelihood disambiguation. The maximum likelihood method depends solely on the target language. The model was trained on a corpus of American English newspaper texts. Its performance was tested using output from a transfer based translation system between Turkish and English. The method is source language independent, and can be used for systems translating from any language into English."
W98-1406,De-Constraining Text Generation,1998,12,11,2,1,38698,stephen beale,Natural Language Generation,0,"We argue that the current, predominantly task-oriented, approaz~hes to modularizing text xe2x80xa2 generation, while plausible and useful conceptually, set up spurious conceptual and operational constraints. We propose a data-driven approach to modularization and illustrate how it eliminates xe2x80xa2 xe2x80xa2the previously ubiquitous constraints on combination of evidence across modules and on xe2x80xa2 control. We also briefly overview the constraint-based control architecture that enables such an approach and facilitates near linear-time processing with realistic texts."
W98-0713,Lexical Acquisition with {W}ord{N}et and the Mikrokosmos Ontology,1998,7,12,3,0,47361,tom ohara,Usage of {W}ord{N}et in Natural Language Processing Systems,0,None
P98-2160,Universal Grammar and Lexis for Quick Ramp-Up of {MT} Systems,1998,16,19,1,1,32552,sergei nirenburg,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"This paper introduces Boas, a semi-automatic knowledge elicitation system that guides a team of two people through the process of developing the static knowledge sources for a moderate-quality, broad-coverage MT system from any low-density language into English in about six months. The paper focuses on some issues in the elicitation of descriptive knowledge in Boas and also the issue of the principled reuse of pre-existing resources, such as a lexicon, an ontology, and an English generation module, among others, made possible by the fact that the client MT system is developed for a single target language."
P98-2216,The Computational Lexical Semantics of Syntagmatic Expressions,1998,15,4,3,0.535714,45501,evelyne viegas,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"In this paper, we address the issue of syntagmatic expressions from a computational lexical semantic perspective. From a representational viewpoint, we argue for a hybrid approach combining linguistic and conceptual paradigms, in order to account for the continuum we find in natural languages from free combining words to frozen expressions. In particular, we focus on the place of lexical and semantic restricted co-occurrences. From a processing viewpoint, we show how to generate/analyze syntagmatic expressions by using an efficient constraintbased processor, well fitted for a knowledge-driven approach."
C98-2155,Universal Grammar and Lexis for Quick Ramp-Up of {MT} Systems,1998,16,19,1,1,32552,sergei nirenburg,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"This paper introduces Boas, a semi-automatic knowledge elicitation system that guides a team of two people through the process of developing the static knowledge sources for a moderate-quality, broad-coverage MT system from any low-density language into English in about six months. The paper focuses on some issues in the elicitation of descriptive knowledge in Boas and also the issue of the principled reuse of pre-existing resources, such as a lexicon, an ontology, and an English generation module, among others, made possible by the fact that the client MT system is developed for a single target language."
C98-2211,The Computational Lexical Semantics of Syntagmatic Relations,1998,15,4,3,0.535714,45501,evelyne viegas,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"In this paper, we address the issue of syntagmatic expressions from a computational lexical semantic perspective. From a representational viewpoint, we argue for a hybrid approach combining linguistic and conceptual paradigms, in order to account for the continuum we find in natural languages from free combining words to frozen expressions. In particular, we focus on the place of lexical and semantic restricted co-occurrences. From a processing viewpoint, we show how to generate/analyze syntagmatic expressions by using an efficient constraintbased processor, well fitted for a knowledge-driven approach."
1998.amta-tutorials.4,Ontological semantics for knowledge-based {MT},1998,-1,-1,1,1,32552,sergei nirenburg,Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Tutorial Descriptions,0,None
1997.tmi-1.1,"If you have it, flaunt it: using full ontological knowledge for word sense disambiguation",1997,-1,-1,2,1,33381,kavi mahesh,Proceedings of the 7th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
1997.tmi-1.18,Word sense disambiguation: why statistics when we have these numbers?,1997,-1,-1,2,1,33381,kavi mahesh,Proceedings of the 7th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
W96-0506,{PICARD}: The Next Generator,1996,3,1,2,1,38698,stephen beale,Eighth International Natural Language Generation Workshop (Posters and Demonstrations),0,None
W96-0407,Generating Patent Claims from Interactive Input,1996,11,19,2,1,18939,svetlana sheremetyeva,Eighth International Natural Language Generation Workshop,0,"Patent claims are the subject of legal protection. They must be formulated according to a set of precise syntactic, lexical and stylistic guidelines. Composing patent claims is a complex task, even for experts. In this paper we report about an tmplemented system for supporting authoring claims for patents describing apparatuses. The system generates claim texts from the input specified partly by the stored conceptual text schemata and partly by the input from the user. The result of the interactive content acquisition stage is a shaUow-level representation which can be considered a draft to be automatically revised into the final text of the claim. Subjec t"
W96-0310,Lexical Rules for Deverbal Adjectives,1996,-1,-1,2,1,51619,victor raskin,Breadth and Depth of Semantic Lexicons,0,None
P96-1005,From Submit to Submitted via Submission: On Lexical Rules in Large-Scale Lexicon Acquisition,1996,17,27,4,0.535714,45501,evelyne viegas,34th Annual Meeting of the Association for Computational Linguistics,1,"This paper deals with the discovery, representation, and use of lexical rules (LRs) during large-scale semi-automatic computational lexicon acquisition. The analysis is based on a set of LRs implemented and tested on the basis of Spanish and English business- and finance-related corpora. We show that, though the use of LRs is justified, they do not come cost-free. Semi-automatic output checking is required, even with blocking and preemtion procedures built in. Nevertheless, large-scope LRs are justified because they facilitate the unavoidable process of large-scale semi-automatic lexical acquisition. We also argue that the place of LRs in the computational process is a complex issue."
C96-2142,Adjectival Modification in Text Meaning Representation,1996,10,14,2,1,51619,victor raskin,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"This work belongs to a family of research efforts, called microtheories and aimed at describing the static meaning of all lexical categories in several languages in the framework of the MikroKosmos project on computational semantics. The latter also involves other static microtheories describing world knowledge and syntax-semantics mapping as well as dynamic microtheories connected with the actual process of text analysis. This paper describes our approach to detecting and recording adjectival meaning, compares it with the body of knowledge on adjectives in literature and presents a detailed, practically tested methodology for the acquisition of lexical entries for adjectives. The work was based on the set of over 6,000 English and about 1,500 Spanish adjectives obtained from task-oriented corpora."
C96-1016,Measuring Semantic Coverage,1996,6,4,1,1,32552,sergei nirenburg,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"The development of natural language processing systems is currently driven to a large extent by measures of knowledge-base size and coverage of individual phenomena relative to a corpus. While these measures have led to significant advances for knowledge-lean applications, they do not adequately motivate progress in computational semantics leading to the development of large-scale, general purpose NLP systems. In this article, we argue that depth of semantic representation is essential for covering a broad range of phenomena in the computational treatment of language and propose depth as an important additional dimension for measuring the semantic coverage of NLP systems. We propose an operationalization of this measure and show how to characterize an NLP system along the dimensions of size, corpus coverage, and depth. The proposed framework is illustrated using several prominent NLP systems. We hope the preliminary proposals made in this article will lead to prolonged debates in the field and will continue to be refined."
1996.amta-1.10,Two principles and six techniques for rapid {MT} development,1996,-1,-1,1,1,32552,sergei nirenburg,Conference of the Association for Machine Translation in the Americas,0,None
1996.amta-1.24,Panel: The limits of automation: optimists vs skeptics.,1996,-1,-1,6,0.245976,1043,eduard hovy,Conference of the Association for Machine Translation in the Americas,0,None
1995.tmi-1.8,Apologiae Ontologiae,1995,-1,-1,1,1,32552,sergei nirenburg,Proceedings of the Sixth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
H94-1026,Toward Multi-Engine Machine Translation,1994,4,17,1,1,32552,sergei nirenburg,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"Current MT systems, whatever translation method they at present employ, do not reach an optimum output on free text. Our hypothesis for the experiment reported in this paper is that if an MT environment can use the best results from a variety of MT systems working simultaneously on the same text, the overall quality will improve. Using this novel approach to MT in the latest version of the Pangloss MT project, we submit an input text to a battery of machine translation systems (engines), collect their (possibly, incomplete) results in a joint chart-like data structure and select the overall best translation using a set of simple heuristics. This paper describes the simple mechanism we use for combining the findings of the various translation engines."
H94-1093,{P}angloss: A Machine Translation Project,1994,0,6,1,1,32552,sergei nirenburg,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"The project involves three sites (NMSU, USC, CMU) and is devoted to enhancing the state of the art in machine translation of natural language texts."
C94-1019,Two Types of Adaptive {MT} Environments,1994,7,7,1,1,32552,sergei nirenburg,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"A number of proposal have come up in recent years for hybridization of MT. Current MT projects --- both pure and hybrid, both predominantly technology-oriented and scientific (including those currently funded by NSF) are single-engine projects, capable of one particular type of source text analysis, one particular method of finding target language correspondences for source language elements and one prescribed method of generating the target language text. While such projects can be quite useful, we believe that it is time to make the next step in the design of machine translation systems and to move toward adaptive, multiple-engine systems. We describe the architecture of an adaptive multi-engine MT system which uses each of the engines under the circumstances which are most favorable for its success."
C94-1057,The Correct Place of Lexical Semantics in Interlingual {MT},1994,13,11,2,0,17380,lori levin,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
A94-1016,Three Heads are Better than One,1994,4,128,2,1,39652,robert frederking,Fourth Conference on Applied Natural Language Processing,0,"Machine translation (MT) systems do not currently achieve optimal quality translation on free text, whatever translation method they employ. Our hypothesis is that the quality of MT will improve if an MT environment uses output from a variety of MT systems working on the same text. In the latest version of the Pangloss MT project, we collect the results of three translation engines---typically, subsentential chunks---in a chart data structure. Since the individual MT systems operate completely independently, their results may be incomplete, conflicting, or redundant. We use simple scoring heuristics to estimate the quality of each chunk, and find the highest-score sequence of chunks (the best cover). This paper describes in detail the combining method, presenting the algorithm and illustrations of its progress on one of many actual translations it has produced. It uses dynamic programming to efficiently compare weighted averages of sets of adjacent scored component translations. The current system operates primarily in a human-aided MT mode. The translation delivery system and its associated post-editing aide are briefly described, as is an initial evaluation of the usefulness of this method. Individual MT engines will be reported separately and are not, therefore, described in detail here."
1994.amta-1.10,Integrating Translations from Multiple Sources within the {PANGLOSS} Mark {III} Machine Translation System,1994,-1,-1,2,1,39652,robert frederking,Proceedings of the First Conference of the Association for Machine Translation in the Americas,0,None
1994.amta-1.27,Is {MT} Research Doing Any Good?,1994,-1,-1,4,0,3453,kenneth church,Proceedings of the First Conference of the Association for Machine Translation in the Americas,0,None
1994.amta-1.41,{PANGLOSS},1994,0,0,8,0,10837,jaime carbonell,Proceedings of the First Conference of the Association for Machine Translation in the Americas,0,None
H93-1038,An {MAT} Tool and Its Effectiveness,1993,2,19,4,1,39652,robert frederking,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"Although automatic machine translation (MT) of unconstrained text is beyond the state of the art today, the need for increased translator productivity is urgent. The PANGLOSS system addresses this dilemma by integrating MT with machine-aided translation (MAT). The main measure of progress in the development of the PANGLOSS system is a gradual increase in the level of automation. The current PANGLOSS MT system typically generates sub-sentence-length units of the target text. Any remaining gaps are treated by lexicon lookup. A mixture of these two kinds of components is presented to the user using the CMAT (Component Machine-Aided Translation) editor, which was designed to facilitate the transformation of this output into a high-quality text. An experiment evaluating the utility of the CMAT editor demonstrated its usefulness in this task, and provides useful guidance for further development."
E93-1062,The {PANGLOSS MARK I} {MAT} system,1993,2,4,5,1,39652,robert frederking,Sixth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"The goal of the PANGLOSS projecd is to develop a system which will, from the very beginning, produce highquality translations of unconstrained text. This can only be attained currently by keeping the human in the translation loop, in our case via a software module called the A U O ~ R . The main measure of progress in the development of the Pangloss system will therefore be the gradual decrease in need for user assistance, as the level of automation increases. The analyzer used in the first version of PANGLOSS, PANGLOSS MARK I, is a version of the ULTRA Spanish analyzer from NMSU [Farwell 1990], while generation is carried out by the PENMAN generator from ISI [Mann 1983]. The Translator's Workstation (TWS) provides the user interface and the integration platform [Nirenburg 1992]. This paper focuses on this use of TWS as a substrate for PANGLOSS. PANOLOSS operates in the following mode: a) a fullyautomated translation of each full sentence is attempted; if it fails, then b) a fully-automated translation of smaller chunks of text is attempted (in the first PANGLOSS configuration, PANGLOSS MARK I, these were noun phrases); c) the material that does not get covered by noun phrases is treated in word-for-word mode, whereby translation suggestions for each word (or phrase) are sought in the system's MT lexicons, a machine-readable dictionary, and a set of user glossaries; d) The resulting list of translated noun phrases and translation suggestions for words and phrases is displayed in a special editor window of TWS, where the human user finalizes the translation. At stages a) and b) there is an option of the user being presented by the system with disambiguation questions via the AUGMENTOR. We provide an intelligent environment, the CMAT (Constituent Machine-Aided Translation) editor, for postediting. It allows the user to select, move, and delete words and phrases (constituents) quickly and easily, using dynamically-changing menus. As can be seen in Figure 1, each constituent in the target window is surrounded by  > characters. If the user clicks with the mouse anywhere within a constituent (between the  > symbols), a CMAT menu for that constituent appears. It contains the word or phrase in the source text if available, the functions Move and Delete, and alternative translations of the word or phrase from the source text if any. Using these popup menus, the user moves, replaces, or deletes a constituent with a single mouse action, rapidly turning the list of translated words"
1993.tmi-1.4,Two Approaches to Matching in Example-Based Machine Translation,1993,-1,-1,1,1,32552,sergei nirenburg,Proceedings of the Fifth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
1993.mtsummit-1.18,A Direction of {MT} Development,1993,6,1,1,1,32552,sergei nirenburg,Proceedings of Machine Translation Summit IV,0,"The two most recently popular technological paradigms in machine translation xe2x80x94 examplebased translation (EBMT) and statistics-based translation (SBMT) xe2x80x94 require knowledge about language only as an afterthought. While the representatives of the above paradigms are still at the stage of either building toy systems (e.g., Furuse and Iida, 1992; McLean, 1992, Jones, 1992, Maruyama and Watanabe, 1992) or struggling with the natural constraints of approaches that eschew the study of language as such (e.g., Brown et al., 1990), a number of proposals have come up for some hybridization of MT. In some such approaches, corpus analysis is used for tuning analysis and transfer grammars (e.g., Su and Chang, 1992). In others, a standard transfer-based approach (TBMT) is followed using traditional analysis and generation techniques but having transfer component based on aligned bilingual corpora (Grishman and Kosaka, 1992). Still in others, it was suggested that statistical information be used as the source of preference assignment during text disambiguation (e.g., the outline presented in Lehmann and Ott, 1992). Indeed, hybrid MT systems were a central topic of the latest Conference on Theoretical and Methodological Issues in MT."
H92-1052,Approximating an Interlingua in a Principled Way,1992,11,27,2,0.259557,1043,eduard hovy,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"We address the problem of constructing in a principled way an ontology of terms to be used in an interlingua for machine translation. Given our belief that the a true language-neutral ontology of terms can only be approached asymptotically, the construction method outlined involves a stepwise folding in of one language at a time. This is effected in three steps: first building for each language a taxonomy of the linguistic generalizations required to analyze and generate that language, then organizing the domain entities in terms of that taxonomy, and finally merging the result with the existing interlingua ontology in a well-defined way. This methodology is based not on intuitive grounds about what is and is not 'true' about the world, which is a question of language-independence, but instead on practical concerns, namely what information the analysis and generation programs require in order to perform their tasks, a question of language-neutrality. After each merging is complete, the resulting taxonomy contains, declaratively and explicitly represented, those distinctions required to control the analysis and generation of the linguistic phenomena. The paper is based on current work of the PANGLOSS MT project."
A92-1035,Practical World Modeling for {NLP} Applications,1992,4,2,2,0,53771,lynn carlson,Third Conference on Applied Natural Language Processing,0,None
A92-1045,Multi-Purpose Development and Operation Environments for Natural Language Applications,1992,5,11,1,1,32552,sergei nirenburg,Third Conference on Applied Natural Language Processing,0,"Interactive user environments have been a central efficiencyenhancing feature of many modem computer applications, including natural language processing. There are two major classes of users for whom NLP environments can be constructed developers and end users, such as technical writers and translators. Developers need help in the various knowledge acquisition tasks, such as dictionary and grammar writing for NLP applications. End users look for efficiency enhancements in document preparation beyond the level of word processing support. There are two approaches to the solution of this problem. A dedicated workstation can be developed for each of the required functionalities. Alternatively, workstations can be configured as sets of application routines attached to a universal user interface. In this report we describe a general-purpose user environment, under development at the Center for Machine Translation of Carnegie Mellon University, capable of supporting a number of dedicated workstation configurations. Among the types of end users whom this system will benefit are technical writers, text revisors and translators. In the framework of NLP system development this tool supports dictionary and ontology acquisition. A number of separate functionalities included in this system have been developed and used either in commercial word processing software packages or in NLP projects (e.g., the translator's tools described in Macklovitch, 1989, ; and the developer environments IRACQ (Ayuso et al., 1987), LUKE (Wrobiewski and Rich, 1988) or ONTOS (Monarch and Nirenburg, 1989), among many others). Our system allows a merge of the two directions in the tool development. One direct reason to put the two previously separate kinds of functionality into a single system was to support the knowledge-based machineaided translation environment which involves an interactive human editor who uses an interface to help the machine understand the source text. A standard Unixand X-windows-based workstation platform has been selected for our system, whose working name is Tws, for Translator's Workstation. We also used the C-based X11 toolkit called MOTIF (Motif, 1991) and its CommonLisp interface called CLM (Babatz et. al., 1991). In practice, TWS consists of a number of application (functionality) modules which are integrated through the central MOTIF-based user interface module. For reasons of uniformity, each of the applications uses the facilities of the user interface for display and input. Each module uses a standard window to interact with the user, and each window has standard menus which, among other functionalities, allow the user to invoke any other module. Each module also has special menus. The architecture of TWS is presented in Figure 1."
W91-0202,Syntax-Driven and Ontology-Driven Lexical Semantics,1991,18,28,1,1,32552,sergei nirenburg,Lexical Semantics and Knowledge Representation,0,"We describe the scopes of two schools in lexical semantics, which we call syntax-driven lexical semantics and ontology-driven lexical semantics, respectively. Both approaches are used in various applications at The Center for Machine Translation. We believe that a comparative analysis of these positions and clarification of claims and coverage is essential for the field as a whole."
1991.mtsummit-panels.4,Where Do Translators Fit Into {MT}?,1991,-1,-1,6,0,57445,alex gross,Proceedings of Machine Translation Summit III: Panels,0,None
W90-0120,Speaker Attitudes in Text Planning,1990,4,0,2,0,57482,christine defrise,Proceedings of the Fifth International Workshop on Natural Language Generation,0,None
H90-1072,Machine Translation Again?,1990,0,2,5,0,37311,yorick wilks,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"Machine translation (MT) remains the paradigm task for natural language processing (NLP) since its inception in the 1950s. Unless NLP can succeed with the central task of machine translation, it cannot be considered successful as a field. We maintain that the most profitable approach to MT at the present time is an interlingual and modular one. MT is one the precious few computational tasks falling broadly within artificial intelligence (AI) that combine a fundamental intellectual research challenge with enormous proven need. To establish the latter, one only has to note that in Japan alone the current MT requirement is for 20 billion pages a year (a market of some $66 billion a year)."
C90-3039,Meaning Representation and Text Planning,1990,8,4,2,0,57482,christine defrise,{COLING} 1990 Volume 3: Papers presented to the 13th International Conference on Computational Linguistics,0,"The data flow in natural language generation (NLG) starts with a 'world' state, represented by structures of an application program (e.g., an expert system) that has text generation needs and an impetus to produce a natural language text. The output of generation is a natural language text. The generation process involves the tasks of a) delimiting the content of the eventual text, b) plano ning its structure, c) selecting lexieal, syntactic and word order me,'ms of realizing this structure and d) actually realizing the textusing the latter. In advanced generation systems these processes are treated not in a monolithic way, but rather as components of a large, modular generator. NLG researchers experiment with various ways of delimiting the modules of the generation process and control architectures to drive these modules (see, for instance, McKeown, 1985, Hovy, 1987 or Meteer, 1989). But regardless of the decisions about general (intermodular) or local (intramodular) control flow, knowledge structures have to be defined to support processing and facilitate communication among the modules. The natural language generator DIOGENES(e.g., Nirenburg et al., 1989) has been originally designed for use in machine translation. This means that the content delimitation stage is unnecessary, as the set of meanings to be realized by the generator is obtained in machine translationas a result of source text analysis. The first processing component in DIOGENES is, therefore, its text planner which, takes as input a text meaning representation (TMR) and a set of static pragmatic factors (similar to Hovy's (1987) rhetorical goals) and produces a text plan (TP), a structure containing information about the order and boundaries of target language sentences; the decisions about reference realization and lexical selection, t At the next stage, a set of semantics-to-syntax mapping rules are used to produce a set of target-language syntactic structures (we are using the f-structures of LFG see, e.g., Nirenburg and Levin, 1989). Finally, a syntactic realizer produces a target language text from the set of f-structures. To produce texts of adequate quality, natural language generation needs a sufficiently expressive input language. In this paper we discuss several important aspects of the knowledge and the processing at the text planning stage of a generation system. First, we describe a comprehensive language processing paradigm which underlies work on both generation and analysis of natural language in our environment. Next, we illustrate the features of our meaning representation languages, the text meaning representation language TAMERLAN and the text plan representation language TPL. Finally, we describe the mechanism of text planning in DIOGENES and illustrate the formalism and the strategy for acquiring text planning rules."
1989.mtsummit-1.27,{KBMT}-89 - A knowledge-based {MT} project at {C}arnegie {M}ellon {U}niversity,1989,-1,-1,1,1,32552,sergei nirenburg,Proceedings of Machine Translation Summit II,0,None
C88-2100,A Framework for Lexical Selection in Natural Language Generation,1988,13,39,1,1,32552,sergei nirenburg,{C}oling {B}udapest 1988 Volume 2: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"This paper describes a procedure for lexical selection of open-class lexical items in a natural language generation system. An optimum lexical selection module must be able to make realization decisions under varying contextual circumstances. First, it must be able to operate without the influence of context, based on meaning correspondences between elements of conceptual input and the lexical inventory of the target language. Second, it must be able to use contextual constraints, as supported by collocational information in the generation lexicon. Third, there must be an option of realizing input representations pronominally or through definite descriptions. Finally, there must also be an option of using elliptical constactions. The nature of background knowledge and the algorithm we suggest for this task are described. The lexical selection procedure is a part of a comprehensive generation system, DIOGENES."
1988.tmi-1.4,Lexical realization in natural language generation,1988,-1,-1,1,1,32552,sergei nirenburg,Proceedings of the Second Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
P87-1028,Lexical Selection in the Process of Language Generation,1987,11,18,2,0,993,james pustejovsky,25th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we argue that lexical selection plays a more important role in the generation process than has commonly been assumed. To stress the importance of lexical-semantic input to generation, we explore the distinction and treatment of generating open and closed class lexical items, and suggest an additional classification of the latter into discourse-oriented and proposition-oriented items. Finally, we discuss how lexical selection is influenced by thematic (focus) information in the input."
J87-3007,The Subworld Concept Lexicon and the Lexicon Management System,1987,28,56,1,1,32552,sergei nirenburg,Computational Linguistics,0,"Natural language processing systems require three different types of lexicons: the concept lexicon that describes the (sub)world ontology and the analysis and generation lexicons for natural languages. We argue that the acquisition of the concept lexicon must precede any lexical work on natural language and that a comprehensive lexicon management system (LMS) is necessary for lexicon acquisition in large-scale applications. We describe the interactive concept lexicon acquisition module of the LMS for TRANSLATOR, a knowledge-based, sublanguage-oriented machine translation project."
C86-1043,Discourse and Cohesion in Expository Text,1986,5,10,2,0,58182,allen tucker,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,None
C86-1079,A Metric for Computational Analysis of Meaning: Toward an Applied Theory of Linguistic Semantics,1986,5,6,1,1,32552,sergei nirenburg,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,"A metric for assessing the complexity of semantic (and pragmatic) analysis in natural language processing is proposed as part of a general applied theory of linguistic semantics for NLP. The theory is intended as a complete projection of linguistic semantics onto NLP and is designed as an exhaustive list of possible choices among strategies of semantic analysis at each level, from the word to the entire text. The alternatives are summarized in a chart, which can be completed for each existing or projected NLP system. The remaining components of the applied theory are also outlined."
C86-1148,On Knowledge-Based Machine Translation,1986,2,11,1,1,32552,sergei nirenburg,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,"This paper describes the design of the knowledge representation medium used for representing concepts and assertions, respectively, in a subworld chosen for a knowledge-based machine translation system. This design is used in the TRANSLATOR machine translation project. The knowledge representation language, or interlingua, has two components, DIL and TIL. DIL stands for 'dictionary of interlingua' and descibes the semantics of a subworld. TIL stands for 'text of interlingua' and is responsible for producing an interlingua text, which represents the meaning of an input text in the terms of the interlingua. We maintain that involved analysis of various types of linguistic and encyclopaedic meaning is necessary for the task of automatic translation. The mechanisms for extracting and manipulating and reproducing the meaning of texts will be reported in detail elsewhere. The linguistic (including the syntactic) knowledge about source and target languages is used by the mechanisms that translate texts into and from the interlingua. Since interlingua is an artificial language, we can (and do, through TIL) control the syntax and semantics of the allowed interlingua elements. The interlingua suggested for TRANSLATOR has a broader coverage than other knowledge representation schemata for natural language. It involves the knowledge about discourse, speech acts, focus, time, space and other facets of the overall meaning of texts."
1985.tmi-1.15,Interlingua Design for translator,1985,-1,-1,1,1,32552,sergei nirenburg,Proceedings of the first Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
P84-1081,Interruptable Transition Networks,1984,3,1,1,1,32552,sergei nirenburg,10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics,1,"A specialized transition network mechanism, the interruptable transition network (ITN) is used to perform the last of three stages in a multiprocessor syntactic parser. This approach can be seen as an exercise in implementing a parsing procedure of the active chart parser family."
