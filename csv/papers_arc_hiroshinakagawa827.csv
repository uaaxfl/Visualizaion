D14-1143,Formalizing Word Sampling for Vocabulary Prediction as Graph-based Active Learning,2014,10,5,5,1,260,yo ehara,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Predicting vocabulary of second language learners is essential to support their language learning; however, because of the large size of language vocabularies, we cannot collect information on the entire vocabulary. For practical measurements, we need to sample a small portion of words from the entire vocabulary and predict the rest of the words. In this study, we propose a novel framework for this sampling method. Current methods rely on simple heuristic techniques involving inflexible manual tuning by educational experts. We formalize these heuristic techniques as a graph-based non-interactive active learning method as applied to a special graph. We show that by extending the graph, we can support additional functionality such as incorporating domain specificity and sampling from multiple corpora. In our experiments, we show that our extended methods outperform other methods in terms of vocabulary prediction accuracy when the number of samples is small."
W13-5007,Understanding seed selection in bootstrapping,2013,12,2,4,1,260,yo ehara,Proceedings of {T}ext{G}raphs-8 Graph-based Methods for Natural Language Processing,0,"Bootstrapping has recently become the focus of much attention in natural language processing to reduce labeling cost. In bootstrapping, unlabeled instances can be harvested from the initial labeled xe2x80x9cseedxe2x80x9d set. The selected seed set affects accuracy, but how to select a good seed set is not yet clear. Thus, an xe2x80x9citerative seedingxe2x80x9d framework is proposed for bootstrapping to reduce its labeling cost. Our framework iteratively selects the unlabeled instance that has the best xe2x80x9cgoodness of seedxe2x80x9d and labels the unlabeled instance in the seed set. Our framework deepens understanding of this seeding process in bootstrapping by deriving the dual problem. We propose a method called expected model rotation (EMR) that works well on not well-separated data which frequently occur as realistic data. Experimental results show that EMR can select seed sets that provide significantly higher mean reciprocal rank on realistic data than existing naive selection methods or random seed sets."
D13-1069,Automatically Determining a Proper Length for Multi-Document Summarization: A {B}ayesian Nonparametric Approach,2013,23,2,2,0,6725,tengfei ma,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Document summarization is an important task in the area of natural language processing, which aims to extract the most important information from a single document or a cluster of documents. In various summarization tasks, the summary length is manually defined. However, how to find the proper summary length is quite a problem; and keeping all summaries restricted to the same length is not always a good choice. It is obviously improper to generate summaries with the same length for two clusters of documents which contain quite different quantity of information. In this paper, we propose a Bayesian nonparametric model for multidocument summarization in order to automatically determine the proper lengths of summaries. Assuming that an original document can be reconstructed from its summary, we describe the xe2x80x9dreconstructionxe2x80x9d by a Bayesian framework which selects sentences to form a good summary. Experimental results on DUC2004 data sets and some expanded data demonstrate the good quality of our summaries and the rationality of the length determination."
Y12-1054,Cross-Lingual Topic Alignment in Time Series {J}apanese / {C}hinese News,2012,9,1,8,0,41984,shuo hu,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"Among various types of recent information explosion, that in news stream is also a kind of serious problems. This paper studies issues regarding topic modeling of information flow in multilingual news streams. If someone wants to find differences in the topics of Japanese news and Chinese news, it is usually necessary for him/her to carefully watch every article in Japanese and Chinese news streams at every moment. In such a situation, topic models such as LDA (Latent Dirichlet Allocation) and DTM (dynamic topic model) are quite effective in estimating distribution of topics over a document collection such as articles in a news stream. Especially, as a topic model, this paper employs DTM, but not LDA,since it canconsidercorrespondence between topics of consecutive dates. Based on the results of estimating distribution of topics in Japanese / Chinese news streams, this paper proposes how to analyze cross-lingual alignment of topics in time series Japanese / Chinese news streams."
P12-1076,Reducing Wrong Labels in Distant Supervision for Relation Extraction,2012,18,134,3,0,42707,shingo takamatsu,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In relation extraction, distant supervision seeks to extract relations between entities from text by using a knowledge base, such as Freebase, as a source of supervision. When a sentence and a knowledge base refer to the same entity pair, this approach heuristically labels the sentence with the corresponding relation in the knowledge base. However, this heuristic can fail with the result that some sentences are labeled wrongly. This noisy labeled data causes poor extraction performance. In this paper, we propose a method to reduce the number of wrong labels. We present a novel generative model that directly models the heuristic labeling process of distant supervision. The model predicts whether assigned labels are correct or wrong via its hidden variables. Our experimental results show that this model detected wrong labels with higher performance than baseline methods. In the experiment, we also found that our wrong label reduction boosted the performance of relation extraction."
C12-1049,Mining Words in the Minds of Second Language Learners: Learner-Specific Word Difficulty,2012,18,7,4,1,260,yo ehara,Proceedings of {COLING} 2012,0,"While there have been many studies on measuring the size of learnersxe2x80x99 vocabulary or the vocabulary they should learn, there have been few studies on what kind of words learners actually know. Therefore, we investigated theoretically and practically important models for predicting second language learnersxe2x80x99 vocabulary and propose another model for this vocabulary prediction task. With the current models, the same word difficulty measure is shared by all learners. This is unrealistic because some learners have special interests. A learner interested in music may know special music-related terms regardless of their difficulty. To solve this problem, our model can define a learner-specific word difficulty measure. Our model is also an extension of these current models in the sense that these models are special cases of our model. In a qualitative evaluation, we defined a measure for how learner-specific a word is. Interestingly, the word with the highest learner-specificity was xe2x80x9ctwitterxe2x80x9d. Although xe2x80x9ctwitterxe2x80x9d is a difficult English word, some low-ability learners presumably knew this word through the famous micro-blogging service. Our qualitative evaluation successfully extracted such interesting and suggestive examples. Our model achieved an accuracy competitive with the current models."
W10-3020,Features for Detecting Hedge Cues,2010,8,0,2,1,20306,nobuyuki shimizu,Proceedings of the Fourteenth Conference on Computational Natural Language Learning {--} Shared Task,0,"We present a sequential labeling approach to hedge cue detection submitted to the biological portion of task 1 for the CoNLL-2010 shared task. Our main approach is as follows. We make use of partial syntactic information together with features obtained from the unlabeled corpus, and convert the task into one of sequential BIO-tagging. If a cue is found, a sentence is classified as uncertain and certain otherwise. To examine a large number of feature combinations, we employ a genetic algorithm. While some features obtained by this method are difficult to interpret, they were shown to improve the performance of the final system."
E09-1069,Deterministic Shift-Reduce Parsing for Unification-Based Grammars by Using Default Unification,2009,30,6,4,0,3213,takashi ninomiya,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"Many parsing techniques including parameter estimation assume the use of a packed parse forest for efficient and accurate parsing. However, they have several inherent problems deriving from the restriction of locality in the packed parse forest. Deterministic parsing is one of solutions that can achieve simple and fast parsing without the mechanisms of the packed parse forest by accurately choosing search paths. We propose (i) deterministic shift-reduce parsing for unification-based grammars, and (ii) best-first shift-reduce parsing with beam thresholding for unification-based grammars. Deterministic parsing cannot simply be applied to unification-based grammar parsing, which often fails because of its hard constraints. Therefore, it is developed by using default unification, which almost always succeeds in unification by overwriting inconsistent constraints in grammars."
kiyota-etal-2008-automated,Automated Subject Induction from Query Keywords through {W}ikipedia Categories and Subject Headings,2008,2,2,4,1,41987,yoji kiyota,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper addresses a novel approach that integrates two different types of information resources: the World Wide Web and libraries. This approach is based on a hypothesis: advantages and disadvantages of the Web and libraries are complemental. The integration is based on correspondent conceptual label names between the Wikipedia categories and subject headings of library materials. The method enables us to find locations of bookshelves in a library easily, using any query keywords. Any keywords which are registered as Wikipedia items are acceptable. The advantages of the method are: the integrative approach makes subject access of library resources have broader coverage than an approach which only uses subject headings; and the approach navigates us to reliable information resources. We implemented the proposed method into an application system, and are now operating the system at several university libraries in Japan. We are planning to evaluate the method based on the query logs collected by the system."
C08-1044,Modeling {C}hinese Documents with Topical Word-Character Models,2008,4,5,3,0,10159,wei hu,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"As Chinese text is written without word boundaries, effectively recognizing Chinese words is like recognizing collocations in English, substituting characters for words and words for collocations. However, existing topical models that involve collocations have a common limitation. Instead of directly assigning a topic to a collocation, they take the topic of a word within the collocation as the topic of the whole collocation. This is unsatisfactory for topical modeling of Chinese documents. Thus, we propose a topical word-character model (TWC), which allows two distinct types of topics: word topic and character topic. We evaluated TWC both qualitatively and quantitatively to show that it is a powerful and a promising topic model."
C08-1100,Metric Learning for Synonym Acquisition,2008,31,6,5,1,20306,nobuyuki shimizu,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"The distance or similarity metric plays an important role in many natural language processing (NLP) tasks. Previous studies have demonstrated the effectiveness of a number of metrics such as the Jaccard coefficient, especially in synonym acquisition. While the existing metrics perform quite well, to further improve performance, we propose the use of a supervised machine learning algorithm that fine-tunes them. Given the known instances of similar or dissimilar words, we estimated the parameters of the Mahalanobis distance. We compared a number of metrics in our experiments, and the results show that the proposed metric has a higher mean average precision than other metrics."
D07-1044,{B}ayesian Document Generative Model with Explicit Multiple Topics,2007,3,3,2,1,40139,issei sato,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"In this paper, we proposed a novel probabilistic generative model to deal with explicit multiple-topic documents: Parametric Dirichlet Mixture Model(PDMM). PDMM is an expansion of an existing probabilistic generative model: Parametric Mixture Model(PMM) by hierarchical Bayes model. PMM models multiple-topic documents by mixing model parameters of each single topic with an equal mixture ratio. PDMM models multiple-topic documents by mixing model parameters of each single topic with mixture ratio following Dirichlet distribution. We evaluate PDMM and PMM by comparing F-measures using MEDLINE corpus. The evaluation showed that PDMM is more effective than PMM."
D07-1129,Structural Correspondence Learning for Dependency Parsing,2007,12,9,2,1,20306,nobuyuki shimizu,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"Following (Blitzer et al., 2006), we present an application of structural correspondence learning to non-projective dependency parsing (McDonald et al., 2005). To induce the correspondences among dependency edges from different domains, we looked at every two tokens in a sentence and examined whether or not there is a preposition, a determiner or a helping verb between them. Three binary linear classifiers were trained to predict the existence of a preposition, etc, on unlabeled data and we used singular value decomposition to induce new features. During the training, the parser was trained with these additional features in addition to these described in (McDonald et al., 2005). We discriminatively trained our parser in an on-line fashion using a variant of the voted perceptron (Collins, 2002; Collins and Roark, 2004; Crammer and Singer, 2003)."
2007.mtsummit-papers.35,Development of a {J}apanese-{C}hinese machine translation system,2007,-1,-1,5,0,15925,hitoshi isahara,Proceedings of Machine Translation Summit XI: Papers,0,None
kiyota-nakagawa-2006-domain,A Domain Ontology Production Tool Kit Based on Automatically Constructed Case Frames,2006,5,1,2,1,41987,yoji kiyota,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper proposes a tool kit to produce a domain ontology for text mining, based on case frames automatically constructed from a raw corpus of a specific domain. Since case frames are strongly related to implicit facts hidden in large domain-specific corpora, we can say that case frames are a promising device for text mining. The aim of the tool kit is to enable automatic analysis of event reports, from which implicit factors of the events are to be extracted. The tool kit enables us to produce a domain ontology by iterating associative retrieval of case frames and manual refinement. In this study, the tool kit is applied to the Japan Airlines pilot report collection, and a domain ontology of contributing factors in the civil aviation domain is experimentally produced. A lot of interesting examples are found in the ontology. In addition, a brief examination of the production process shows the efficiency of the tool kit."
W05-0203,A Real-Time Multiple-Choice Question Generation For Language Testing: A Preliminary Study,2005,2,47,2,0,50838,ayako hoshino,Proceedings of the Second Workshop on Building Educational Applications Using {NLP},0,"An automatic generation of multiple-choice questions is one of the promising examples of educational applications of NLP techniques. A machine learning approach seems to be useful for this purpose because some of the processes can be done by classification. Using basic machine learning algorithms as Naive Bayes and K-Nearest Neighbors, we have developed a real-time system which generates questions on English grammar and vocabulary from on-line news articles. This paper describes the current version of our system and discusses some of the issues on constructing this kind of system."
P05-3031,Reformatting Web Documents via Header Trees,2005,8,0,2,0,29589,minoru yoshida,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,We propose a new method for reformatting web documents by extracting semantic structures from web pages. Our approach is to extract trees that describe hierarchical relations in documents. We developed an algorithm for this task by employing the EM algorithm and clustering techniques. Preliminary experiments showed that our approach was more effective than baseline methods.
I05-1024,Automatic Term Extraction Based on Perplexity of Compound Words,2005,7,17,2,0,29589,minoru yoshida,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Many methods of term extraction have been discussed in terms of their accuracy on huge corpora. However, when we try to apply various methods that derive from frequency to a small corpus, we may not be able to achieve sufficient accuracy because of the shortage of statistical information on frequency. This paper reports a new way of extracting terms that is tuned for a very small corpus. It focuses on the structure of compound terms and calculates perplexity on the term unit's left-side and right-side. The results of our experiments revealed that the accuracy with the proposed method was not that advantageous. However, experimentation with the method combining perplexity and frequency information obtained the highest average-precision in comparison with other methods."
H05-2010,{W}eb{E}xperimenter for Multiple-Choice Question Generation,2005,3,6,2,0,50838,ayako hoshino,Proceedings of {HLT}/{EMNLP} 2005 Interactive Demonstrations,0,"Automatic generation of multiple-choice questions is an emerging topic in application of natural language processing. Particularly, applying it to language testing has been proved to be useful (Sumita et al., 2005)."
W04-1112,{C}hinese Term Extraction from Web Pages Based on Compound Term Productivity,2004,13,7,1,1,40140,hiroshi nakagawa,Proceedings of the Third {SIGHAN} Workshop on {C}hinese Language Processing,0,"In this paper, we propose an automatic term recognition system for Chinese. Our idea is based on the relation between a compound word and its constituents that are simple words or individual Chinese character. More precisely, we basically focus on how many words/characters adjoin the word/character in question to form compound words. We also take into account the frequency of term. We evaluated word based method and character based method with several Chinese Web pages, resulting in precision of 75% for top ten candidate terms."
nakagawa-etal-2004-terminal,Terminal Device Oriented Comparable Corpora and its Alignment- Towards Extracting Paraphrasing Patterns,2004,3,0,1,1,40140,hiroshi nakagawa,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Many terminal devices for mobile environment such as mobile phones have small and low resolution screens compared to the big and high resolution screen of personal computers. In this circumstance, Web pages for ordinary personal computer and mobile phones written in the same language are developed separately even though they describe the same topic or contents. In this research, we collected Web news articles aimed at displaying on personal computer screens and news articles aimed at mobile terminals for more than two years. Then we aligned these two kinds of news articles first in article level and then in sentence level. As the result, we got more than 88,000 pairs of aligned sentences. Next, we extract paraphrases of the final part of sentences from this aligned corpus. Actual results are the sentence final nouns of mobile article sentences and their counterpart expressions of Web article sentences. We extract character strings for paraphrases based on branching factor, frequency and length of string. The precision is 90% for highest ranked candidate and 80% for each top four candidates of 10 most frequently used nouns."
C04-1176,Automatic Construction of {J}apanese {KATAKANA} Variant List from Large Corpus,2004,3,13,3,0,1084,takeshi masuyama,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper presents a method to construct Japanese KATAKANA variant list from large corpus. Our method is useful for information retrieval, information extraction, question answering, and so on, because KATAKANA words tend to be used as loan words and the transliteration causes several variations of spelling. Our method consists of three steps. At step 1, our system collects KATAKANA words from large corpus. At step 2, our system collects candidate pairs of KATAKANA variants from the collected KATAKANA words using a spelling similarity which is based on the edit distance. At step 3, our system selects variant pairs from the candidate pairs using a semantic similarity which is calculated by a vector space model of a context of each KATAKANA word. We conducted experiments using 38 years of Japanese newspaper articles and constructed Japanese KATAKANA variant list with the performance of 97.4% recall and 89.1% precision. Estimating from this precision, our system can extract 178,569 variant pairs from the corpus."
P03-2016,{K}iwi: A Multilingual Usage Consultation Tool based on {I}nternet Searching,2003,14,5,3,0,22805,kumiko tanakaishii,The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,0,"We present a usage consultation tool based on Internet searching. When a user enters a string of words that he wants to find the usage for, the system sends a query to the search engine to obtain a corpus about the string. The corpus is statistically analyzed and results are displayed. As the system uses neither language-dependent analysis nor initial data, queries can be made in any language, even languages for which there are no well-established analytical methods. Also, since the corpus is dynamically obtained from the search engines, the usages provided to the user are always up to date. Kiwi can fill in the missing parts of the collocations frequently used by native speakers."
W02-1407,A Simple but Powerful Automatic Term Extraction Method,2002,7,99,1,1,40140,hiroshi nakagawa,{COLING}-02: {COMPUTERM} 2002: Second International Workshop on Computational Terminology,0,"In this paper, we propose a new idea for the automatic recognition of domain specific terms. Our idea is based on the statistics between a compound noun and its component single-nouns. More precisely, we focus basically on how many nouns adjoin the noun in question to form compound nouns. We propose several scoring methods based on this idea and experimentally evaluate them on the NTCIRI TMREC test collection. The results are very promising especially in the low recall area."
W00-1708,Alignment of Sound Track with Text in a {TV} Drama,2000,2,0,2,0,54171,seigo tanimura,Proceedings of the {COLING}-2000 Workshop on Semantic Annotation and Intelligent Content,0,"We propose a system to align a sound track and a part of TV drama video contents with a script. We first use the number of moras in each sentence of speech line, the sounding time in a sound track and the shot change time in the motion image to align them approximately. Then we perform DP matching to align a sequence of words obtained from a speech recognition system applied to a sound track with each sentence of speech lines in a script. Confident correspondences obtained from the DP matching of the words act as the pivots to improve alignment accuracy iteratively. The results show that around a half of the sentences in a script were aligned within the differences of up to two seconds."
W99-0203,Identification of Coreference Between Names and Faces,1999,16,2,4,0,51189,koichi yamada,Coreference and Its Applications,0,"To retrieve multimedia contents by their meaning, it is necessary to use not only the contents of distinct media, such as image or language, but also a certain semantic relation holding between them. For this purpose, in this paper, we propose a method to find coreferences between human names in the article of newspaper and human faces in the accompanying photograph. The method we proposed is based on the machine learning and the hypothesis driven combining method for identifying names and corresponding faces. Our experimental results show that the recall and precision rate of our method are better than those of the system which uses information exclusively from either text media or image media."
P98-2153,Hypertext Authoring for Linking Relevant Segments of Related Instruction Manuals,1998,4,1,1,1,40140,hiroshi nakagawa,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"Recently manuals of industrial products become large and often consist of separated volumes. In reading such individual but related manuals, we must consider the relation among segments, which contain explanations of sequences of operation. In this paper, we propose methods for linking relevant segments in hypertext authoring of a set of related manuals. Our method is based on the similarity calculation between two segments. Our experimental results show that the proposed method improves both recall and precision comparing with the conventional tf.idf based method."
C98-2148,Hypertext Authoring for Linking Relevant Segments of Related Instruction Manuals,1998,4,1,1,1,40140,hiroshi nakagawa,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"Recently manuals of industrial products become large and often consist of separated volumes. In reading such individual but related manuals, we must consider the relation among segments, which contain explanations of sequences of operation. In this paper, we propose methods for linking relevant segments in hypertext authoring of a set of related manuals. Our method is based on the similarity calculation between two segments. Our experimental results show that the proposed method improves both recall and precision comparing with the conventional tf.idf based method."
W97-1407,Scene Direction Based Reference In Drama Scenes,1997,3,0,1,1,40140,hiroshi nakagawa,Referring Phenomena in a Multimedia Context and their Computational Treatment,0,"Out research target is reference relations between descriptions of script and an actor/actress who actually plays in the drama scene that correspond to the scene direction which is a part of the script. In this paper, first we analyze sentences used as the scene directions, and classify them. Then we propose the rules to extract subjects and predicates from those sentences. With the extracted subjects and predicates, we build the existence/action map that explains the situations happening on each scene. The existence/action map we build describes scenes very correctly as for whether each player appears in each scene or not. Our experiment shows that the recall is around 80% and the precision is over 90%. This means that our system of inferring reference relations works well for scene directions. Then we develop the scene retrieval system in which this map is used to retrieve scenes from the drama video database according to the input query. We also show some experimental results of our retrieval system."
W97-1302,Constraints and defaults of zero pronouns in {J}apanese instruction manuals,1997,5,4,3,1,23407,tatsunori mori,"Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts",0,"In this paper, we propose a method for anaphora resolution of zero subjects in Japanese manual sentences based on both the nature of language expressions and the ontology of ordinary instruction manuals. In instruction manuals written in Japanese, zero subjects often introduce ambiguity into sentences. In order to resolve them, we consider the property of several types of expressions including some forms of verbal phrases and some conjunctives of clauses, and so on. As the result, we have a set of constraints and defaults for zero subject resolution. We examine the precision of the constraints and defaults with real manual sentences, and we have the result that they make a good estimate with precision of over 80%."
C96-2132,Zero Pronouns and Conditionals in {J}apanese Instruction Manuals,1996,3,3,2,1,23407,tatsunori mori,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"This paper proposes a method of the zero pronoun resolution, which is one of the essential processes in understanding systems for Japanese manual sentences. It is based on pragmatic properties of Japanese conditionals. We examined a number of sentences appearing in Japanese manuals according to the classification based on the types of agent and the types of verb phrase. As a result, we obtained the following pattern of usage in matrix clauses: 1) The connective particles TO and REBA have the same distribution of usage. TARA and NARA have the same distribution of usage. 2) The distribution of usage of TO and REBA, and that of TARA and NARA are complementary to each other. We show that these distributions of usage can be used for resolution of zero subjects."
C94-2107,Semantics of Complex Sentences in {J}apanese,1994,7,3,1,1,40140,hiroshi nakagawa,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The important part of semantics of complex sentence is captured as relations among semantic roles in subordinate and main clause respectively. However if there can be relations between every pair of semantic roles, the amount of computation to identify the relations that hold in the given sentence is extremely large. In this paper, for semantics of Japanese complex sentence, we introduce new pragmatic roles called observer and molivated respectively to bridge sementic roles of subordinate and those of main clauses. By these new roles constraints on the relations among semantic/pragmatic roles are known to be almost local within subordinate or main clause. In other words, as for the semantics of the whole complex sentence, the only role we should deal with is a molivated."
C92-1051,Zero Pronouns as Experiencer in {J}apanese Discourse,1992,3,3,1,1,40140,hiroshi nakagawa,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The process of finding the antecedent of zero pronoun, that is indispensable to Japanese language understanding, is the topic of this paper. Here we mainly concern with discourses comprising two sentences that are in a subordinate relation, especially one of them describes the agent's volitional action and the other describes the reason of the action. We propose basically two new principles: (1) The agent of an action should experience a certain psychological reason, (2) Predicates reporting someone's psychological state are categorized into 1) weakly or 2) strongly bound to the expected point of view. Combination of these principles accounts for some problematic Japanese zero anaphora, which cannot be accounted for by the theories so far proposed."
C88-2097,A Parser based on Connectionist Model,1988,10,9,1,1,40140,hiroshi nakagawa,{C}oling {B}udapest 1988 Volume 2: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"This paper proposes a parser based fully upon the connectionist model(called CM parser here after). In order to realize the CM parser, we use Sigma-Pi-Units to implement a constraint of grammatical category order or word order, and a copy mechanism of sub-parse trees. Further more, we suppose there exist weak suppressive connection links between every pair of CM units. By these suppressive links, our CM parser explains why garden path sentences and/or deeply nested sentences are hard to recognize. Our CM parser also explains the preference principles for syntactically ambiguous sentences."
