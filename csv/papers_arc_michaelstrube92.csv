2021.sustainlp-1.4,Countering the Influence of Essay Length in Neural Essay Scoring,2021,-1,-1,2,1,868,sungho jeon,Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing,0,"Previous work has shown that automated essay scoring systems, in particular machine learning-based systems, are not capable of assessing the quality of essays, but are relying on essay length, a factor irrelevant to writing proficiency. In this work, we first show that state-of-the-art systems, recent neural essay scoring systems, might be also influenced by the correlation between essay length and scores in a standard dataset. In our evaluation, a very simple neural model shows the state-of-the-art performance on the standard dataset. To consider essay content without taking essay length into account, we introduce a simple neural model assessing the similarity of content between an input essay and essays assigned different scores. This neural model achieves performance comparable to the state of the art on a standard dataset as well as on a second dataset. Our findings suggest that neural essay scoring systems should consider the characteristics of datasets to focus on text quality."
2021.newsum-1.5,A Novel {W}ikipedia based Dataset for Monolingual and Cross-Lingual Summarization,2021,-1,-1,2,0,3110,mehwish fatima,Proceedings of the Third Workshop on New Frontiers in Summarization,0,"Cross-lingual summarization is a challenging task for which there are no cross-lingual scientific resources currently available. To overcome the lack of a high-quality resource, we present a new dataset for monolingual and cross-lingual summarization considering the English-German pair. We collect high-quality, real-world cross-lingual data from Spektrum der Wissenschaft, which publishes human-written German scientific summaries of English science articles on various subjects. The generated Spektrum dataset is small; therefore, we harvest a similar dataset from the Wikipedia Science Portal to complement it. The Wikipedia dataset consists of English and German articles, which can be used for monolingual and cross-lingual summarization. Furthermore, we present a quantitative analysis of the datasets and results of empirical experiments with several existing extractive and abstractive summarization models. The results suggest the viability and usefulness of the proposed dataset for monolingual and cross-lingual summarization."
2021.codi-sharedtask.1,"The {CODI}-{CRAC} 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue",2021,-1,-1,6,0,2579,sopan khosla,"Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue",0,"In this paper, we provide an overview of the CODI-CRAC 2021 Shared-Task: Anaphora Resolution in Dialogue. The shared task focuses on detecting anaphoric relations in different genres of conversations. Using five conversational datasets, four of which have been newly annotated with a wide range of anaphoric relations: identity, bridging references and discourse deixis, we defined multiple subtasks focusing individually on these key relations. We discuss the evaluation scripts used to assess the system performance on these subtasks, and provide a brief summary of the participating systems and the results obtained across ?? runs from 5 teams, with most submissions achieving significantly better results than our baseline methods."
2020.sdp-1.9,Reconstructing Manual Information Extraction with {DB}-to-Document Backprojection: Experiments in the Life Science Domain,2020,-1,-1,6,1,12169,markchristoph muller,Proceedings of the First Workshop on Scholarly Document Processing,0,"We introduce a novel scientific document processing task for making previously inaccessible information in printed paper documents available to automatic processing. We describe our data set of scanned documents and data records from the biological database SABIO-RK, provide a definition of the task, and report findings from preliminary experiments. Rigorous evaluation proved challenging due to lack of gold-standard data and a difficult notion of correctness. Qualitative inspection of results, however, showed the feasibility and usefulness of the task"
2020.lrec-1.697,A Large Harvested Corpus of Location Metonymy,2020,-1,-1,2,0,18037,kevin mathews,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Metonymy is a figure of speech in which an entity is referred to by another related entity. The existing datasets of metonymy are either too small in size or lack sufficient coverage. We propose a new, labelled, high-quality corpus of location metonymy called WiMCor, which is large in size and has high coverage. The corpus is harvested semi-automatically from English Wikipedia. We use different labels of varying granularity to annotate the corpus. The corpus can directly be used for training and evaluating automatic metonymy resolution systems. We construct benchmarks for metonymy resolution, and evaluate baseline methods using the new corpus."
2020.findings-emnlp.42,A Fully Hyperbolic Neural Model for Hierarchical Multi-Class Classification,2020,-1,-1,2,0,19431,federico lopez,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Label inventories for fine-grained entity typing have grown in size and complexity. Nonetheless, they exhibit a hierarchical structure. Hyperbolic spaces offer a mathematically appealing approach for learning hierarchical representations of symbolic data. However, it is not clear how to integrate hyperbolic components into downstream tasks. This is the first work that proposes a fully hyperbolic model for multi-class multi-label classification, which performs all operations in hyperbolic space. We evaluate the proposed model on two challenging datasets and compare to different baselines that operate under Euclidean assumptions. Our hyperbolic model infers the latent hierarchy from the class distribution, captures implicit hyponymic relations in the inventory, and shows performance on par with state-of-the-art methods on fine-grained classification with remarkable reduction of the parameter size. A thorough analysis sheds light on the impact of each component in the final prediction and showcases its ease of integration with Euclidean layers."
2020.emnlp-main.604,Centering-based Neural Coherence Modeling with Hierarchical Discourse Segments,2020,-1,-1,2,1,868,sungho jeon,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Previous neural coherence models have focused on identifying semantic relations between adjacent sentences. However, they do not have the means to exploit structural information. In this work, we propose a coherence model which takes discourse structural information into account without relying on human annotations. We approximate a linguistic theory of coherence, Centering theory, which we use to track the changes of focus between discourse segments. Our model first identifies the focus of each sentence, recognized with regards to the context, and constructs the structural relationship for discourse segments by tracking the changes of the focus. The model then incorporates this structural information into a structure-aware transformer. We evaluate our model on two tasks, automated essay scoring and assessing writing quality. Our results demonstrate that our model, built on top of a pretrained language model, achieves state-of-the-art performance on both tasks. We next statistically examine the identified trees of texts assigned to different quality scores. Finally, we investigate what our model learns in terms of theoretical claims."
2020.coling-main.594,Incremental Neural Lexical Coherence Modeling,2020,-1,-1,2,1,868,sungho jeon,Proceedings of the 28th International Conference on Computational Linguistics,0,"Pretrained language models, neural models pretrained on massive amounts of data, have established the state of the art in a range of NLP tasks. They are based on a modern machine-learning technique, the Transformer which relates all items simultaneously to capture semantic relations in sequences. However, it differs from what humans do. Humans read sentences one-by-one, incrementally. Can neural models benefit by interpreting texts incrementally as humans do? We investigate this question in coherence modeling. We propose a coherence model which interprets sentences incrementally to capture lexical relations between them. We compare the state of the art in each task, simple neural models relying on a pretrained language model, and our model in two downstream tasks. Our findings suggest that interpreting texts incrementally as humans could be useful to design more advanced models."
2020.codi-1.16,Evaluation of Coreference Resolution Systems Under Adversarial Attacks,2020,-1,-1,4,0,21812,haixia chai,Proceedings of the First Workshop on Computational Approaches to Discourse,0,"A substantial overlap of coreferent mentions in the CoNLL dataset magnifies the recent progress on coreference resolution. This is because the CoNLL benchmark fails to evaluate the ability of coreference resolvers that requires linking novel mentions unseen at train time. In this work, we create a new dataset based on CoNLL, which largely decreases mention overlaps in the entire dataset and exposes the limitations of published resolvers on two aspects{---}lexical inference ability and understanding of low-level orthographic noise. Our findings show (1) the requirements for embeddings, used in resolvers, and for coreference resolutions are, by design, in conflict and (2) adversarial approaches are sometimes not legitimate to mitigate the obstacles, as they may falsely introduce mention overlaps in adversarial training and test sets, thus giving an inflated impression for the improvements."
W19-4319,Fine-Grained Entity Typing in Hyperbolic Space,2019,34,0,3,0,19431,federico lopez,Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019),0,How can we represent hierarchical information present in large type inventories for entity typing? We study the suitability of hyperbolic embeddings to capture hierarchical relations between mentions in context and their target types in a shared vector space. We evaluate on two datasets and propose two different techniques to extract hierarchical information from the type inventory: from an expert-generated ontology and by automatically mining the dataset. The hyperbolic model shows improvements in some but not all cases over its Euclidean counterpart. Our analysis suggests that the adequacy of this geometry depends on the granularity of the type inventory and the representation of its distribution.
P19-1027,Sequence Tagging with Contextual and Non-Contextual Subword Representations: A Multilingual Evaluation,2019,37,0,2,1,4152,benjamin heinzerling,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Pretrained contextual and non-contextual subword embeddings have become available in over 250 languages, allowing massively multilingual NLP. However, while there is no dearth of pretrained embeddings, the distinct lack of systematic evaluations makes it difficult for practitioners to choose between them. In this work, we conduct an extensive evaluation comparing non-contextual subword embeddings, namely FastText and BPEmb, and a contextual representation method, namely BERT, on multilingual named entity recognition and part-of-speech tagging. We find that overall, a combination of BERT, BPEmb, and character representations works best across languages and tasks. A more detailed analysis reveals different strengths and weaknesses: Multilingual BERT performs well in medium- to high-resource languages, but is outperformed by non-contextual subword embeddings in a low-resource setting."
P19-1408,Using Automatically Extracted Minimum Spans to Disentangle Coreference Evaluation from Boundary Detection,2019,18,0,4,1,4244,nafise moosavi,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"The common practice in coreference resolution is to identify and evaluate the maximum span of mentions. The use of maximum spans tangles coreference evaluation with the challenges of mention boundary detection like prepositional phrase attachment. To address this problem, minimum spans are manually annotated in smaller corpora. However, this additional annotation is costly and therefore, this solution does not scale to large corpora. In this paper, we propose the MINA algorithm for automatically extracting minimum spans to benefit from minimum span evaluation in all corpora. We show that the extracted minimum spans by MINA are consistent with those that are manually annotated by experts. Our experiments show that using minimum spans is in particular important in cross-dataset coreference evaluation, in which detected mention boundaries are noisier due to domain shift. We have integrated MINA into https://github.com/ns-moosavi/coval for reporting standard coreference scores based on both maximum and automatically detected minimum spans."
K19-1021,On the Importance of Subword Information for Morphological Tasks in Truly Low-Resource Languages,2019,46,1,4,0,6432,yi zhu,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"Recent work has validated the importance of subword information for word representation learning. Since subwords increase parameter sharing ability in neural models, their value should be even more pronounced in low-data regimes. In this work, we therefore provide a comprehensive analysis focused on the usefulness of subwords for word representation learning in truly low-resource scenarios and for three representative morphological tasks: fine-grained entity typing, morphological tagging, and named entity recognition. We conduct a systematic study that spans several dimensions of comparison: 1) type of data scarcity which can stem from the lack of task-specific training data, or even from the lack of unannotated data required to train word embeddings, or both; 2) language type by working with a sample of 16 typologically diverse languages including some truly low-resource ones (e.g. Rusyn, Buryat, and Zulu); 3) the choice of the subword-informed word representation method. Our main results show that subword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and task-based models, where having sufficient in-task data is a more critical requirement."
D19-5542,Adapting Deep Learning Methods for Mental Health Prediction on Social Media,2019,34,1,2,0,22657,ivan sekulic,Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),0,"Mental health poses a significant challenge for an individual{'}s well-being. Text analysis of rich resources, like social media, can contribute to deeper understanding of illnesses and provide means for their early detection. We tackle a challenge of detecting social media users{'} mental status through deep learning-based models, moving away from traditional approaches to the task. In a binary classification task on predicting if a user suffers from one of nine different disorders, a hierarchical attention network outperforms previously set benchmarks for four of the disorders. Furthermore, we explore the limitations of our model and analyze phrases relevant for classification by inspecting the model{'}s word-level attention weights."
L18-1473,{BPE}mb: Tokenization-free Pre-trained Subword Embeddings in 275 Languages,2018,-1,-1,2,1,4152,benjamin heinzerling,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
J18-2002,Unrestricted Bridging Resolution,2018,43,4,3,0.727746,2135,yufang hou,Computational Linguistics,0,"In contrast to identity anaphors, which indicate coreference between a noun phrase and its antecedent, bridging anaphors link to their antecedent(s) via lexico-semantic, frame, or encyclopedic relations. Bridging resolution involves recognizing bridging anaphors and finding links to antecedents. In contrast to most prior work, we tackle both problems. Our work also follows a more wide-ranging definition of bridging than most previous work and does not impose any restrictions on the type of bridging anaphora or relations between anaphor and antecedent. We create a corpus (ISNotes) annotated for information status (IS), bridging being one of the IS subcategories. The annotations reach high reliability for all categories and marginal reliability for the bridging subcategory. We use a two-stage statistical global inference method for bridging resolution. Given all mentions in a document, the first stage, bridging anaphora recognition, recognizes bridging anaphors as a subtask of learning fine-grained IS. We use a cascading collective classification method where (i) collective classification allows us to investigate relations among several mentions and autocorrelation among IS classes and (ii) cascaded classification allows us to tackle class imbalance, important for minority classes such as bridging. We show that our method outperforms current methods both for IS recognition overall as well as for bridging, specifically. The second stage, bridging antecedent selection, finds the antecedents for all predicted bridging anaphors. We investigate the phenomenon of semantically or syntactically related bridging anaphors that share the same antecedent, a phenomenon we call sibling anaphors. We show that taking sibling anaphors into account in a joint inference model improves antecedent selection performance. In addition, we develop semantic and salience features for antecedent selection and suggest a novel method to build the candidate antecedent list for an anaphor, using the discourse scope of the anaphor. Our model outperforms previous work significantly."
D18-1018,Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers,2018,0,5,2,1,4244,nafise moosavi,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Coreference resolution is an intermediate step for text understanding. It is used in tasks and domains for which we do not necessarily have coreference annotated corpora. Therefore, generalization is of special importance for coreference resolution. However, while recent coreference resolvers have notable improvements on the CoNLL dataset, they struggle to generalize properly to new domains or datasets. In this paper, we investigate the role of linguistic features in building more generalizable coreference resolvers. We show that generalization improves only slightly by merely using a set of additional linguistic features. However, employing features and subsets of their values that are informative for coreference resolution, considerably improves generalization. Thanks to better generalization, our system achieves state-of-the-art results in out-of-domain evaluations, e.g., on WikiCoref, our system, which is trained on CoNLL, achieves on-par performance with a system designed for this dataset."
D18-1464,A Neural Local Coherence Model for Text Quality Assessment,2018,0,9,2,1,6924,mohsen mesgar,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"We propose a local coherence model that captures the flow of what semantically connects adjacent sentences in a text. We represent the semantics of a sentence by a vector and capture its state at each word of the sentence. We model what relates two adjacent sentences based on the two most similar semantic states, each of which is in one of the sentences. We encode the perceived coherence of a text by a vector, which represents patterns of changes in salient information that relates adjacent sentences. Our experiments demonstrate that our approach is beneficial for two downstream tasks: Readability assessment, in which our model achieves new state-of-the-art results; and essay scoring, in which the combination of our coherence vectors and other task-dependent features significantly improves the performance of a strong essay scorer."
C18-2012,"Transparent, Efficient, and Robust Word Embedding Access with {WOMBAT}",2018,9,1,2,1,12169,markchristoph muller,Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations,0,"We present WOMBAT, a Python tool which supports NLP practitioners in accessing word embeddings from code. WOMBAT addresses common research problems, including unified access, scaling, and robust and reproducible preprocessing. Code that uses WOMBAT for accessing word embeddings is not only cleaner, more readable, and easier to reuse, but also much more efficient than code using standard in-memory methods: a Python script using WOMBAT for evaluating seven large word embedding collections (8.7M embedding vectors in total) on a simple SemEval sentence similarity task involving 250 raw sentence pairs completes in under ten seconds end-to-end on a standard notebook computer."
W17-4803,Using a Graph-based Coherence Model in Document-Level Machine Translation,2017,17,0,3,1,17056,leo born,Proceedings of the Third Workshop on Discourse in Machine Translation,0,"Although coherence is an important aspect of any text generation system, it has received little attention in the context of machine translation (MT) so far. We hypothesize that the quality of document-level translation can be improved if MT models take into account the semantic relations among sentences during translation. We integrate the graph-based coherence model proposed by Mesgar and Strube, (2016) with Docent (Hardmeier et al., 2012, Hardmeier, 2014) a document-level machine translation system. The application of this graph-based coherence modeling approach is novel in the context of machine translation. We evaluate the coherence model and its effects on the quality of the machine translation. The result of our experiments shows that our coherence model slightly improves the quality of translation in terms of the average Meteor score."
W17-1501,"Use Generalized Representations, But Do Not Forget Surface Features",2017,26,0,2,1,4244,nafise moosavi,Proceedings of the 2nd Workshop on Coreference Resolution Beyond {O}nto{N}otes ({CORBON} 2017),0,"Only a year ago, all state-of-the-art coreference resolvers were using an extensive amount of surface features. Recently, there was a paradigm shift towards using word embeddings and deep neural networks, where the use of surface features is very limited. In this paper, we show that a simple SVM model with surface features outperforms more complex neural models for detecting anaphoric mentions. Our analysis suggests that using generalized representations and surface features have different strength that should be both taken into account for improving coreference resolution."
P17-2003,Lexical Features in Coreference Resolution: To be Used With Caution,2017,1,10,2,1,4244,nafise moosavi,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Lexical features are a major source of information in state-of-the-art coreference resolvers. Lexical features implicitly model some of the linguistic phenomena at a fine granularity level. They are especially useful for representing the context of mentions. In this paper we investigate a drawback of using many lexical features in state-of-the-art coreference resolvers. We show that if coreference resolvers mainly rely on lexical features, they can hardly generalize to unseen domains. Furthermore, we show that the current coreference resolution evaluation is clearly flawed by only evaluating on a specific split of a specific dataset in which there is a notable overlap between the training, development and test sets."
I17-1083,Event Argument Identification on Dependency Graphs with Bidirectional {LSTM}s,2017,0,0,2,1,32915,alex judea,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"In this paper we investigate the performance of event argument identification. We show that the performance is tied to syntactic complexity. Based on this finding, we propose a novel and effective system for event argument identification. Recurrent Neural Networks learn to produce meaningful representations of long and short dependency paths. Convolutional Neural Networks learn to decompose the lexical context of argument candidates. They are combined into a simple system which outperforms a feature-based, state-of-the-art event argument identifier without any manual feature engineering."
E17-1078,"Trust, but Verify! Better Entity Linking through Automatic Verification",2017,0,1,2,1,4152,benjamin heinzerling,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"We introduce automatic verification as a post-processing step for entity linking (EL). The proposed method trusts EL system results collectively, by assuming entity mentions are mostly linked correctly, in order to create a semantic profile of the given text using geospatial and temporal information, as well as fine-grained entity types. This profile is then used to automatically verify each linked mention individually, i.e., to predict whether it has been linked correctly or not. Verification allows leveraging a rich set of global and pairwise features that would be prohibitively expensive for EL systems employing global inference. Evaluation shows consistent improvements across datasets and systems. In particular, when applied to state-of-the-art systems, our method yields an absolute improvement in linking performance of up to 1.7 F1 on AIDA/CoNLL{'}03 and up to 2.4 F1 on the English TAC KBP 2015 TEDL dataset."
D17-1138,Revisiting Selectional Preferences for Coreference Resolution,2017,21,1,3,1,4152,benjamin heinzerling,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Selectional preferences have long been claimed to be essential for coreference resolution. However, they are modeled only implicitly by current coreference resolvers. We propose a dependency-based embedding model of selectional preferences which allows fine-grained compatibility judgments with high coverage. Incorporating our model improves performance, matching state-of-the-art results of a more complex system. However, it comes with a cost that makes it debatable how worthwhile are such improvements."
W16-4317,"Microblog Emotion Classification by Computing Similarity in Text, Time, and Space",2016,0,4,3,0,33598,anja summa,"Proceedings of the Workshop on Computational Modeling of People{'}s Opinions, Personality, and Emotions in Social Media ({PEOPLES})",0,"Most work in NLP analysing microblogs focuses on textual content thus neglecting temporal and spatial information. We present a new interdisciplinary method for emotion classification that combines linguistic, temporal, and spatial information into a single metric. We create a graph of labeled and unlabeled tweets that encodes the relations between neighboring tweets with respect to their emotion labels. Graph-based semi-supervised learning labels all tweets with an emotion."
W16-0518,Feature-Rich Error Detection in Scientific Writing Using Logistic Regression,2016,5,1,3,0,34077,madeline remse,Proceedings of the 11th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"The goal of the Automatic Evaluation of Scientific Writing (AESW) Shared Task 2016 is to identify sentences in scientific articles which need editing to improve their correctness and readability or to make them better fit within the genre at hand. We encode many different types of errors occurring in the dataset by linguistic features. We use logistic regression to assign a probability indicating whether a sentence needs to be edited. We participate in both tracks at AESW 2016: binary prediction and probabilistic estimation. In the former track, our model (HITS) gets the fifth place and in the latter one, it ranks first according to the evaluation metric."
P16-1060,Which Coreference Evaluation Metric Do You Trust? A Proposal for a Link-based Entity Aware Metric,2016,14,20,2,1,4244,nafise moosavi,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
N16-1115,Search Space Pruning: A Simple Solution for Better Coreference Resolvers,2016,17,3,2,1,4244,nafise moosavi,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
N16-1167,Lexical Coherence Graph Modeling Using Word Embeddings,2016,20,3,2,1,6924,mohsen mesgar,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Coherence is established by semantic connections between sentences of a text which can be modeled by lexical relations. In this paper, we introduce the lexical coherence graph (LCG), a new graph-based model to represent lexical relations among sentences. The frequency of subgraphs (coherence patterns) of this graph captures the connectivity style of sentence nodes in this graph. The coherence of a text is encoded by a vector of these frequencies. We evaluate the LCG model on the readability ranking task. The results of the experiments show that the LCG model obtains higher accuracy than state-of-the-art coherence models. Using larger subgraphs yields higher accuracy, because they capture more structural information. However, larger subgraphs can be sparse. We adapt Kneser-Ney smoothing to smooth subgraphsxe2x80x99 frequencies. Smoothing improves performance."
D16-1074,Generating Coherent Summaries of Scientific Articles Using Coherence Patterns,2016,22,18,3,1,10283,daraksha parveen,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1215,Incremental Global Event Extraction,2016,17,3,2,1,32915,alex judea,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Event extraction is a difficult information extraction task. Li et al. (2014) explore the benefits of modeling event extraction and two related tasks, entity mention and relation extraction, jointly. This joint system achieves state-of-the-art performance in all tasks. However, as a system operating only at the sentence level, it misses valuable information from other parts of the document. In this paper, we present an incremental easy-first approach to make the global context of the entire document available to the intra-sentential, state-of-the-art event extractor. We show that our method robustly increases performance on two datasets, namely ACE 2005 and TAC 2015."
S15-1018,Event Extraction as Frame-Semantic Parsing,2015,13,4,2,1,32915,alex judea,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"Based on the hypothesis that frame-semantic parsing and event extraction are structurally identical tasks, we retrain SEMAFOR, a stateof-the-art frame-semantic parsing system to predict event triggers and arguments. We describe how we change SEMAFOR to be better suited for the new task and show that it performs comparable to one of the best systems in event extraction. We also describe a bias in one of its models and propose a feature factorization which is better suited for this model."
S15-1036,Graph-based Coherence Modeling For Assessing Readability,2015,23,8,2,1,6924,mohsen mesgar,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,Readability depends on many factors ranging from shallow features like word length to semantic ones like coherence. We introduce novel graph-based coherence features based on frequent subgraphs and compare their ability to assess the readability of Wall Street Journal articles. In contrast to Pitler and Nenkova (2008) some of our graph-based features are significantly correlated with human judgments. We outperform Pitler and Nenkova (2008) in the readability ranking task by more than 5% accuracy thus establishing a new state-of-the-art on this dataset.
Q15-1029,Latent Structures for Coreference Resolution,2015,41,45,2,1,30358,sebastian martschat,Transactions of the Association for Computational Linguistics,0,"Machine learning approaches to coreference resolution vary greatly in the modeling of the problem: while early approaches operated on the mention pair level, current research focuses on ranking architectures and antecedent trees. We propose a unified representation of different approaches to coreference resolution in terms of the structure they operate on. We represent several coreference resolution approaches proposed in the literature in our framework and evaluate their performance. Finally, we conduct a systematic analysis of the output of these approaches, highlighting differences and similarities."
P15-4007,Visual Error Analysis for Entity Linking,2015,11,1,2,1,4152,benjamin heinzerling,Proceedings of {ACL}-{IJCNLP} 2015 System Demonstrations,0,"We present the Visual Entity Explorer (VEX), an interactive tool for visually exploring and analyzing the output of entity linking systems. VEX is designed to aid developers in improving their systems by visualizing system results, gold annotations, and various mention detection and entity linking error types in a clear, concise, and customizable manner."
P15-4011,Plug Latent Structures and Play Coreference Resolution,2015,20,2,3,1,30358,sebastian martschat,Proceedings of {ACL}-{IJCNLP} 2015 System Demonstrations,0,"We present cort, a modular toolkit for devising, implementing, comparing and analyzing approaches to coreference resolution. The toolkit allows for a unified representation of popular coreference resolution approaches by making explicit the structures they operate on. Several of the implemented approaches achieve state-ofthe-art performance."
N15-3002,Analyzing and Visualizing Coreference Resolution Errors,2015,11,7,3,1,30358,sebastian martschat,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,We present a toolkit for coreference resolution error analysis. It implements a recently proposed analysis framework and contains rich components for analyzing and visualizing recall and precision errors.
D15-1226,Topical Coherence for Graph-based Extractive Summarization,2015,14,55,3,1,10283,daraksha parveen,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We present an approach for extractive single-document summarization. Our approach is based on a weighted graphical representation of documents obtained by topic modeling. We optimize importance, coherence and non-redundancy simultaneously using ILP. We compare ROUGE scores of our system with state-of-the-art results on scientific articles from PLOS Medicine and on DUC 2002 data. Human judges evaluate the coherence of summaries generated by our system in comparision to two baselines. Our approach obtains competitive performance."
W14-3701,Normalized Entity Graph for Computing Local Coherence,2014,15,1,2,1,6924,mohsen mesgar,Proceedings of {T}ext{G}raphs-9: the workshop on Graph-based Methods for Natural Language Processing,0,"Guinaudeau and Strube (2013) introduce a graph based model to compute local entity coherence. We propose a computationally efficient normalization method for these graphs and then evaluate it on three tasks: sentence ordering, summary coherence rating and readability assessment. In all tasks normalization improves the results."
W14-3703,Multi-document Summarization Using Bipartite Graphs,2014,-1,-1,2,1,10283,daraksha parveen,Proceedings of {T}ext{G}raphs-9: the workshop on Graph-based Methods for Natural Language Processing,0,None
P14-2006,Scoring Coreference Partitions of Predicted Mentions: A Reference Implementation,2014,19,67,6,0,11322,sameer pradhan,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,": The definitions of two coreference scoring metrics- B3 and CEAF-are underspecified with respect to predicted, as opposed to key (or gold) mentions. Several variations have been proposed that manipulate either, or both, the key and predicted mentions in order to get a one-to-one mapping. On the other hand, the metric BLANC was, until recently, limited to scoring partitions of key mentions. In this paper, we (i) argue that mention manipulation for scoring predicted mentions is unnecessary, and potentially harmful as it could produce unintuitive results; (ii) illustrate the application of all these measures to scoring predicted mentions; (iii) make available an open-source, thoroughly-tested reference implementation of the main coreference evaluation measures; and (iv) rescore the results of the CoNLL-2011/2012 shared task systems with this implementation. This will help the community accurately measure and compare new end-to-end coreference resolution algorithms."
E14-1052,A Latent Variable Model for Discourse-aware Concept and Entity Disambiguation,2014,39,6,2,1,40084,angela fahrni,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,This paper takes a discourse-oriented perspective for disambiguating common and proper noun mentions with respect to Wikipedia. Our novel approach models the relationship between disambiguation and aspects of cohesion using Markov Logic Networks with latent variables. Considering cohesive aspects consistently improves the disambiguation results on various commonly used data sets.
D14-1221,Recall Error Analysis for Coreference Resolution,2014,30,13,2,1,30358,sebastian martschat,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,We present a novel method for coreference resolution error analysis which we apply to perform a recall error analysis of four state-of-the-art English coreference resolution systems. Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems. We characterize this set of common challenging errors in terms of a broad range of lexical and semantic properties.
D14-1222,A Rule-Based System for Unrestricted Bridging Resolution: Recognizing Bridging Anaphora and Finding Links to Antecedents,2014,23,9,3,1,2135,yufang hou,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Bridging resolution plays an important role in establishing (local) entity coherence. This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution, where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations. The system consists of eight rules which target different relations based on linguistic insights. Our rule-based system significantly outperforms a reimplementation of a previous rule-based system (Vieira and Poesio, 2000). Furthermore, it performs better than a learning-based approach which has access to the same knowledge resources as the rule-based system. Additionally, incorporating the rules and more features into the learning-based system yields a minor improvement over the rule-based system."
C14-1061,Unsupervised Coreference Resolution by Utilizing the Most Informative Relations,2014,33,4,2,1,4244,nafise moosavi,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,In this paper we present a novel method for unsupervised coreference resolution. We introduce a precision-oriented inference method that scores a candidate entity of a mention based on the most informative mention pair relation between the given mention entity pair. We introduce an informativeness score for determining the most precise relation of a mention entity pair regarding the coreference decisions. The informativeness score is learned robustly during few iterations of the expectation maximization algorithm. The proposed unsupervised system outperforms existing unsupervised methods on all benchmark data sets.
P13-1010,Graph-based Local Coherence Modeling,2013,23,49,2,0,39845,camille guinaudeau,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose a computationally efficient graph-based approach for local coherence modeling. We evaluate our system on three tasks: sentence ordering, summary coherence rating and readability assessment. The performance is comparable to entity grid based approaches though these rely on a computationally expensive training phase and face data sparsity problems."
N13-1111,Global Inference for Bridging Anaphora Resolution,2013,27,19,3,1,2135,yufang hou,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,We present the first work on antecedent selection for bridging resolution without restrictions on anaphor or relation types. Our model integrates global constraints on top of a rich local feature set in the framework of Markov logic networks. The global model improves over the local one and both strongly outperform a reimplementation of prior work.
D13-1077,Cascading Collective Classification for Bridging Anaphora Recognition using a Rich Linguistic Feature Set,2013,23,4,3,1,2135,yufang hou,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Recognizing bridging anaphora is difficult due to the wide variation within the phenomenon, the resulting lack of easily identifiable surface markers and their relative rarity. We develop linguistically motivated discourse structure, lexico-semantic and genericity detection features and integrate these into a cascaded minority preference algorithm that models bridging recognition as a subtask of learning finegrained information status (IS). We substantially improve bridging recognition without impairing performance on other IS classes."
W12-4511,A Multigraph Model for Coreference Resolution,2012,6,22,5,1,30358,sebastian martschat,Joint Conference on {EMNLP} and {C}o{NLL} - Shared Task,0,"This paper presents HITS' coreference resolution system that participated in the CoNLL-2012 shared task on multilingual unrestricted coreference resolution. Our system employs a simple multigraph representation of the relation between mentions in a document, where the nodes correspond to mentions and the edges correspond to relations between the mentions. Entities are obtained via greedy clustering. We participated in the closed tasks for English and Chinese. Our system ranked second in the English closed task."
P12-1084,Collective Classification for Fine-grained Information Status,2012,38,14,3,0.496538,10765,katja markert,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Previous work on classifying information status (Nissim, 2006; Rahman and Ng, 2011) is restricted to coarse-grained classification and focuses on conversational dialogue. We here introduce the task of classifying fine-grained information status and work on written text. We add a fine-grained information status layer to the Wall Street Journal portion of the OntoNotes corpus. We claim that the information status of a mention depends not only on the mention itself but also on other mentions in the vicinity and solve the task by collectively classifying the information status of all mentions. Our approach strongly outperforms reimplementations of previous work."
judea-etal-2012-concept,Concept-based Selectional Preferences and Distributional Representations from {W}ikipedia Articles,2012,10,2,3,1,32915,alex judea,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes the derivation of distributional semantic representations for open class words relative to a concept inventory, and of concepts relative to open class words through grammatical relations extracted from Wikipedia articles. The concept inventory comes from WikiNet, a large-scale concept network derived from Wikipedia. The distinctive feature of these representations are their relation to a concept network, through which we can compute selectional preferences of open-class words relative to general concepts. The resource thus derived provides a meaning representation that complements the relational representation captured in the concept network. It covers English open-class words, but the concept base is language independent. The resource can be extended to other languages, with the use of language specific dependency parsers. Good results in metonymy resolution show the resource's potential use for NLP applications."
D12-1017,Local and Global Context for Supervised and Unsupervised Metonymy Resolution,2012,25,9,4,1,24179,vivi nastase,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Computational approaches to metonymy resolution have focused almost exclusively on the local context, especially the constraints placed on a potentially metonymic word by its grammatical collocates. We expand such approaches by taking into account the larger context. Our algorithm is tested on the data from the metonymy resolution task (Task 8) at SemEval 2007. The results show that incorporation of the global context can improve over the use of the local context alone, depending on the types of metonymies addressed. As a second contribution, we move towards unsupervised resolution of metonymies, made feasible by considering ontological relations as possible readings. We show that such an unsupervised approach delivers promising results: it beats the supervised most frequent sense baseline and performs close to a supervised approach using only standard lexico-syntactic features."
C12-1050,Jointly Disambiguating and Clustering Concepts and Entities with {M}arkov {L}ogic,2012,37,9,2,1,40084,angela fahrni,Proceedings of {COLING} 2012,0,"We present a novel approach for jointly disambiguating and clustering known and unknown concepts and entities with Markov Logic. Concept and entity disambiguation is the task of identifying the correct concept or entity in a knowledge base for a single- or multi-word noun (mention) given its context. Concept and entity clustering is the task of clustering mentions so that all mentions in one cluster refer to the same concept or entity. The proposed model (1) is global, i.e. a group of mentions in a text is disambiguated in one single step combining various global and local features, and (2) performs disambiguation, unknown concept and entity detection and clustering jointly. The disambiguation is performed with respect to Wikipedia. The model is trained once on Wikipedia articles and then applied to and evaluated on different data sets originating from news papers, audio transcripts and internet sources."
W11-1907,Unrestricted Coreference Resolution via Global Hypergraph Partitioning,2011,12,17,3,1,26970,jie cai,Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,0,"We present our end-to-end coreference resolution system, COPA, which implements a global decision via hypergraph partitioning. In constrast to almost all previous approaches, we do not rely on separate classification and clustering steps, but perform coreference resolution globally in one step. COPA represents each document as a hypergraph and partitions it with a spectral clustering algorithm. Various types of relational features can be easily incorporated in this framwork. COPA has participated in the open setting of the CoNLL shared task on modeling unrestricted coreference."
I11-2001,{W}iki{N}et{TK} {--} A Tool Kit for {E}mbedding{W}orld Knowledge in {NLP} Applications,2011,9,3,3,1,32915,alex judea,Proceedings of the {IJCNLP} 2011 System Demonstrations,0,"WikiNetTK is a Java-based open-source toolkit for facilitating the interaction with and the embedding of world knowledge in NLP applications. For user interaction we provide a visualization component, consisting of graphical and textual browsing tools. This allows the user to inspect the knowledge base to which WikiNetTK is applied. The application-oriented part of the toolkit provides various functionalities: access to various types of information in the knowledge base as well as methods for computing association paths and relatedness measures. The system is applied to a large-scale multilingual concept network obtained by extracting and combining various sources of information from Wikipedia."
I11-1038,Fine-Grained Sentiment Analysis with Structural Features,2011,29,74,4,0,37383,cacilia zirn,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Sentiment analysis is the problem of determining the polarity of a text with respect to a particular topic. For most applications, however, it is not only necessary to derive the polarity of a text as a whole but also to extract negative and positive utterances on a more finegrained level. Sentiment analysis systems working on the (sub-)sentence level, however, are difficult to develop since shorter textual segments rarely carry enough information to determine their polarity out of context. In this paper, therefore, we present a fully automatic framework for fine-grained sentiment analysis on the subsentence level combining multiple sentiment lexicons and neighborhood as well as discourse relations to overcome this problem. We use Markov logic to integrate polarity scores from different sentiment lexicons with information about relations between neighboring segments, and evaluate the approach on product reviews. The experiments show that the use of structural features improves the accuracy of polarity predictions achieving accuracy scores of up to 69%."
W10-4305,Evaluation Metrics For End-to-End Coreference Resolution Systems,2010,19,42,2,1,26970,jie cai,Proceedings of the {SIGDIAL} 2010 Conference,0,"Commonly used coreference resolution evaluation metrics can only be applied to key mentions, i.e. already annotated mentions. We here propose two variants of the B3 and CEAF coreference resolution evaluation algorithms which can be applied to coreference resolution systems dealing with system mentions, i.e. automatically determined mentions. Our experiments show that our variants lead to intuitive and reliable results."
nastase-etal-2010-wikinet,{W}iki{N}et: A Very Large Scale Multi-Lingual Concept Network,2010,19,81,2,1,24179,vivi nastase,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper describes a multi-lingual large-scale concept network obtained automatically by mining for concepts and relations and exploiting a variety of sources of knowledge from Wikipedia. Concepts and their lexicalizations are extracted from Wikipedia pages, in particular from article titles, hyperlinks, disambiguation pages and cross-language links. Relations are extracted from the category and page network, from the category names, from infoboxes and the body of the articles. The resulting network has two main components: (i) a central, language independent index of concepts, which serves to keep track of the concepts' lexicalizations both within a language and across languages, and to separate linguistic expressions of concepts from the relations in which they are involved (concepts themselves are represented as numeric IDs); (ii) a large network built on the basis of the relations extracted, represented as relations between concepts (more specifically, the numeric IDs). The various stages of obtaining the network were separately evaluated, and the results show a qualitative resource."
C10-1017,End-to-End Coreference Resolution via Hypergraph Partitioning,2010,22,37,2,1,26970,jie cai,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"We describe a novel approach to coreference resolution which implements a global decision via hypergraph partitioning. In constrast to almost all previous approaches, we do not rely on separate classification and clustering steps, but perform coreference resolution globally in one step. Our hypergraph-based global model implemented within an end-to-end coreference resolution system outperforms two strong baselines (Soon et al., 2001; Bengtson & Roth, 2008) using system mentions only."
W09-2814,Creating an Annotated Corpus for Generating Walking Directions,2009,16,7,4,0,46948,stephanie schuldes,Proceedings of the 2009 Workshop on Language Generation and Summarisation ({UCNLG}+{S}um 2009),0,"This work describes first steps towards building a system that synchronously generates multimodal (textual and visual) route directions for pedestrians. We pursue a corpus-based approach for building a generation model that produces natural instructions in multiple languages. We conducted an empirical study to collect verbal route directions, and annotated the acquired texts on different levels. Here we describe the experimental setting and an analysis of the collected data."
P09-2044,Finding Hedges by Chasing Weasels: Hedge Detection Using {W}ikipedia Tags and Shallow Linguistic Features,2009,10,52,2,0,47177,viola ganter,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"We investigate the automatic detection of sentences containing linguistic hedges using corpus statistics and syntactic patterns. We take Wikipedia as an already annotated corpus using its tagged weasel words which mark sentences and phrases as non-factual. We evaluate the quality of Wikipedia as training data for hedge detection, as well as shallow linguistic features."
N09-4004,Extracting World and Linguistic Knowledge from {W}ikipedia,2009,0,2,2,1,9871,simone ponzetto,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Tutorial Abstracts",0,"Many research efforts have been devoted to develop robust statistical modeling techniques for many NLP tasks. Our field is now moving towards more complex tasks (e.g. RTE, QA), which require to complement these methods with a semantically rich representation based on world and linguistic knowledge (i.e. annotated linguistic data). In this tutorial we show several approaches to extract this knowledge from Wikipedia. This resource has attracted the attention of much work in the AI community, mainly because it provides semi-structured information and a large amount of manual annotations. The purpose of this tutorial is to introduce Wikipedia as a resource to the NLP community and to provide an introduction for NLP researchers both from a scientific and a practical (i.e. data acquisition and processing issues) perspective."
N09-2057,Tree Linearization in {E}nglish: Improving Language Model Based Approaches,2009,12,31,2,1,9752,katja filippova,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,"We compare two approaches to dependency tree linearization, a task which arises in many NLP applications. The first one is the widely used 'overgenerate and rank' approach which relies exclusively on a trigram language model (LM); the second one combines language modeling with a maximum entropy classifier trained on a range of linguistic features. The results provide strong support for the combined method and show that trigram LMs are appropriate for phrase linearization while on the clause level a richer representation is necessary to achieve comparable performance."
D09-1095,"Combining Collocations, Lexical and Encyclopedic Knowledge for Metonymy Resolution",2009,24,11,2,1,24179,vivi nastase,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a supervised method for resolving metonymies. We enhance a commonly used feature set with features extracted based on collocation information from corpora, generalized using lexical and encyclopedic knowledge to determine the preferred sense of the potentially metonymic word using methods from unsupervised word sense disambiguation. The methodology developed addresses one issue related to metonymy resolution - the influence of local context. The method developed is applied to the metonymy resolution task from SemEval 2007. The results obtained, higher for the countries subtask, on a par for the companies subtask - compared to participating systems - confirm that lexical, encyclopedic and collocation information can be successfully combined for metonymy resolution."
W08-1105,Dependency Tree Based Sentence Compression,2008,17,93,2,1,9752,katja filippova,Proceedings of the Fifth International Natural Language Generation Conference,0,We present a novel unsupervised method for sentence compression which relies on a dependency tree representation and shortens sentences by removing subtrees. An automatic evaluation shows that our method obtains result comparable or superior to the state of the art. We demonstrate that the choice of the parser affects the performance of the system. We also apply the method to German and report the results of an evaluation with humans.
mueller-etal-2008-knowledge,Knowledge Sources for Bridging Resolution in Multi-Party Dialog,2008,15,0,3,0,23829,markchristoph mueller,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this paper we investigate the coverage of the two knowledge sources WordNet and Wikipedia for the task of bridging resolution. We report on an annotation experiment which yielded pairs of bridging anaphors and their antecedents in spoken multi-party dialog. Manual inspection of the two knowledge sources showed that, with some interesting exceptions, Wikipedia is superior to WordNet when it comes to the coverage of information necessary to resolve the bridging anaphors in our data set. We further describe a simple procedure for the automatic extraction of the required knowledge from Wikipedia by means of an API, and discuss some of the implications of the procedures performance."
mieskes-strube-2008-three,A Three-stage Disfluency Classifier for Multi Party Dialogues,2008,15,2,2,1,3132,margot mieskes,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,We present work on a three-stage system to detect and classify disfluencies in multi party dialogues. The system consists of a regular expression based module and two machine learning based modules. The results are compared to other work on multi party dialogues and we show that our system outperforms previously reported ones.
kassner-etal-2008-acquiring,Acquiring a Taxonomy from the {G}erman {W}ikipedia,2008,9,14,3,0,48187,laura kassner,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents the process of acquiring a large, domain independent, taxonomy from the German Wikipedia. We build upon a previously implemented platform that extracts a semantic network and taxonomy from the English version of the Wikipedia. We describe two accomplishments of our work: the semantic network for the German language in which isa links are identified and annotated, and an expansion of the platform for easy adaptation for a new language. We identify the platforms strengths and shortcomings, which stem from the scarcity of free processing resources for languages other than English. We show that the taxonomy induction process is highly reliable - evaluated against the German version of WordNet, GermaNet, the resource obtained shows an accuracy of 83.34{\%}."
mieskes-strube-2008-parameters,Parameters for Topic Boundary Detection in Multi-Party Dialogues,2008,26,0,2,1,3132,margot mieskes,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We present a topic boundary detection method that searches for connections between sequences of utterances in multi party dialogues. The connections are established based on word identity. We compare our method to a state-of-the art automatic Topic boundary detection method that was also used on multi party dialogues. We checked various methods of preprocessing of the data, including stemming, lemmatization and stopword filtering with a text-based as well as speech-based stopword lists. Using standard evaluation methods we found that our method outperformed the state-of-the art method."
D08-1019,Sentence Fusion via Dependency Graph Compression,2008,21,70,2,1,9752,katja filippova,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"We present a novel unsupervised sentence fusion method which we apply to a corpus of biographies in German. Given a group of related sentences, we align their dependency trees and build a dependency graph. Using integer linear programming we compress this graph to a new tree, which we then linearize. We use GermaNet and Wikipedia for checking semantic compatibility of co-arguments. In an evaluation with human judges our method outperforms the fusion approach of Barzilay & McKeown (2005) with respect to readability."
W07-2321,Extending the Entity-grid Coherence Model to Semantically Related Entities,2007,6,26,2,1,9752,katja filippova,Proceedings of the Eleventh {E}uropean Workshop on Natural Language Generation ({ENLG} 07),0,"This paper reports on work in progress on extending the entity-based approach on measuring coherence (Barzilay & Lapata, 2005; Lapata & Barzilay, 2005) from coreference to semantic relatedness. We use a corpus of manually annotated German newspaper text (TuBa-D/Z) and aim at improving the performance by grouping related entities with the WikiRelate! API (Strube & Ponzetto, 2006)."
P07-2013,An {API} for Measuring the Relatedness of Words in {W}ikipedia,2007,21,18,2,1,9871,simone ponzetto,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,None
P07-1041,Generating Constituent Order in {G}erman Clauses,2007,25,36,2,1,9752,katja filippova,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We investigate the factors which determine constituent order in German clauses and propose an algorithm which performs the task in two steps: First, the best candidate for the initial sentence position is chosen. Then, the order for the remaining constituents is determined. The rst task is more difcult than the second one because of properties of the German sentence-initial position. Experiments show a signicant improvement over competing approaches. Our algorithm is also more efcient than these."
W06-1632,Using linguistically motivated features for paragraph boundary identification,2006,20,11,2,1,9752,katja filippova,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"In this paper we propose a machine-learning approach to paragraph boundary identification which utilizes linguistically motivated features. We investigate the relation between paragraph boundaries and discourse cues, pronominalization and information structure. We test our algorithm on German data and report improvements over three baselines including a reimplementation of Sporleder & Lapata's (2006) work on paragraph segmentation. An analysis of the features' contribution suggests an interpretation of what paragraph boundaries indicate and what they depend on."
N06-1025,"Exploiting Semantic Role Labeling, {W}ord{N}et and {W}ikipedia for Coreference Resolution",2006,30,209,2,1,9871,simone ponzetto,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns."
mieskes-strube-2006-part,Part-of-Speech Tagging of Transcribed Speech,2006,15,9,2,1,3132,margot mieskes,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We used four Part-of-Speech taggers, which are available for research purposes and were originally trained on text to tag a corpus of transcribed multiparty spoken dialogues. The assigned tags were then manually corrected. The correction was first used to evaluate the four taggers, then to retrain them. Despite limited resources in time, money and annotators we reached results comparable to those reported for the taggers on text. Based on our experience we present guidelines to produce reliably POS tagged corpora of new domains."
E06-2015,Semantic Role Labeling for Coreference Resolution,2006,12,30,2,1,9871,simone ponzetto,Demonstrations,0,Extending a machine learning based coreference resolution system with a feature capturing automatically generated information about semantic roles improves its performance.
W05-1611,Discrete Optimization as an Alternative to Sequential Processing in {NLG},2005,28,6,2,0,50776,tomasz marciniak,Proceedings of the Tenth {E}uropean Workshop on Natural Language Generation ({ENLG}-05),0,We present an NLG system that uses Integer Linear Programming to integrate different decisions involved in the generation process. Our approach provides an alternative to pipeline-based sequential processing which has become prevalent in todayxe2x80x99s NLG applications.
W05-0618,Beyond the Pipeline: Discrete Optimization in {NLP},2005,19,44,2,0,50776,tomasz marciniak,Proceedings of the Ninth Conference on Computational Natural Language Learning ({C}o{NLL}-2005),0,"We present a discrete optimization model based on a linear programming formulation as an alternative to the cascade of classifiers implemented in many language processing systems. Since NLP tasks are correlated with one another, sequential processing does not guarantee optimal solutions. We apply our model in an NLG application and show that it performs better than a pipeline-based system."
W05-0633,Semantic Role Labeling Using Lexical Statistical Information,2005,6,3,2,1,9871,simone ponzetto,Proceedings of the Ninth Conference on Computational Natural Language Learning ({C}o{NLL}-2005),0,"Our system for semantic role labeling is multi-stage in nature, being based on tree pruning techniques, statistical methods for lexicalised feature encoding, and a C4.5 decision tree classifier. We use both shallow and deep syntactic information from automatically generated chunks and parse trees, and develop a model for learning the semantic arguments of predicates as a multi-class decision problem. We evaluate the performance on a set of relatively 'cheap' features and report an F1 score of 68.13% on the overall test set."
C04-1110,Semantic Similarity Applied to Spoken Dialogue Summarization,2004,12,43,2,0.512821,706,iryna gurevych,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We present a novel approach to spoken dialogue summarization. Our system employs a set of semantic similarity metrics using the noun portion of WordNet as a knowledge source. So far, the noun senses have been disambiguated manually. The algorithm aims to extract utterances carrying the essential content of dialogues. We evaluate the system on 20 Switchboard dialogues. The results show that our system outperforms LEAD, RANDOM and TF*IDF baselines."
W03-2117,Multi-Level Annotation in {MMAX},2003,9,48,2,1,49178,christoph muller,Proceedings of the Fourth {SIG}dial Workshop of Discourse and Dialogue,0,None
P03-1022,A Machine Learning Approach to Pronoun Resolution in Spoken Dialogue,2003,17,79,1,1,869,michael strube,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,We apply a decision tree based approach to pronoun resolution in spoken dialogue. Our system deals with pronouns with NP-and non-NP-antecedents. We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features. We evaluate the system on twenty Switchboard dialogues and show that it compares well to Byron's (2002) manually tuned system.
J03-4005,"Book Reviews: Learning to Classify Text Using Support Vector Machines: Methods, Theory and Algorithms by Thorsten Joachims; Anaphora Resolution by Ruslan Mitkov",2003,0,2,2,0,12620,roberto basili,Computational Linguistics,0,None
W02-1040,The Influence of Minimum Edit Distance on Reference Resolution,2002,15,69,1,1,869,michael strube,Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ({EMNLP} 2002),0,"We report on experiments in reference resolution using a decision tree approach. We started with a standard feature set used in previous work, which led to moderate results. A closer examination of the performance of the features for different forms of anaphoric expressions showed good results for pronouns, moderate results for proper names, and poor results for definite noun phrases. We then included a cheap, language and domain independent feature based on the minimum edit distance between strings. This feature yielded a significant improvement for data sets consisting of definite noun phrases and proper names, respectively. When applied to the whole data set the feature produced a smaller but still significant improvement."
W02-0207,Annotating the Semantic Consistency of Speech Recognition Hypotheses,2002,6,10,3,0.512821,706,iryna gurevych,Proceedings of the Third {SIG}dial Workshop on Discourse and Dialogue,0,"Recent work on natural language processing systems is aimed at more conversational, context-adaptive systems in multiple domains. An important requirement for such a system is the automatic detection of the domain and a domain consistency check of the given speech recognition hypotheses. We report a pilot study addressing these tasks, the underlying data collection and investigate the feasibility of annotating the data reliably by human annotators."
P02-1045,Applying Co-Training to Reference Resolution,2002,17,44,3,1,21923,christoph mueller,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we investigate the practical applicability of Co-Training for the task of building a classifier for reference resolution. We are concerned with the question if Co-Training can significantly reduce the amount of manual labeling work and still produce a classifier with an acceptable performance."
rapp-strube-2002-iterative,An Iterative Data Collection Approach for Multimodal Dialogue Systems,2002,6,19,2,0,53223,stefan rapp,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper deals with the way in which data for multimodal dialogue systems are collected. We argue that for multimodal data, an iterative data collection strategy should be followed. Instead of a single major data collection effort using a xe2x80x9cWizard of OZxe2x80x9d (WOZ) or xe2x80x9cpromptingxe2x80x9d experimental setup, several smaller data collections should accompany the system development. We also describe the xe2x80x9cscriptxe2x80x9d experimental setup we developed. It is in between the WOZ and prompting setup, and can be used as a cost effective design for the first data collection within the iterative data collection strategy."
muller-strube-2002-api,An {API} for Discourse-level Access to {XML}-encoded Corpora,2002,6,2,2,1,49178,christoph muller,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
W01-1612,Annotating Anaphoric and Bridging Relations with {MMAX},2001,8,28,2,1,21923,christoph mueller,Proceedings of the Second {SIG}dial Workshop on Discourse and Dialogue,0,"We present a tool for the annotation of anaphoric and bridging relations in a corpus of written texts. Based on differences as well as similarities between these phenomena, we define an annotation scheme. We then implement the scheme within an annotation tool and demonstrate its use."
A00-2003,A Probabilistic Genre-Independent Model of Pronominalization,2000,22,11,1,1,869,michael strube,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Our aim in this paper is to identify genreindependent factors that influence the decision to pronominalize. Results based on the annotation of twelve texts from four genres show that only a few factors have a strong influence on pronominalization across genres, i.e. distance from last mention, agreement, and form of the antecedent. Finally, we describe a probabilistic model of pronominalization derived from our data."
W99-0107,Building a Tool for Annotating Reference in Discourse,1999,21,6,2,0,54907,jonathan decristofaro,The Relation of Discourse/Dialogue Structure and Reference,0,"We discuss the development of a system for marking several types of reference to facilitate the analysis of reference in discourse. The tool is designed to be used in three applicationsi generating training data for machine learning of co-reference relations, evaluating iheories of referring expression generation and resolution in texts, and developing theories for understanding reference in dialogs. The need to mark any of a broad set of relations which may span several levels of discourse structure drives the system architecture. The system has the abilityto collect statistics over encoded relations and meastwe inter-coder reliability, and includes tools to increase the accuracy of the user's markings by highlighting the di.u:repancies between two sets of markings. Using parsed corpora as the input further reduces the human workload and increases reliability."
W99-0108,Generating Anaphoric Expressions: Pronoun or Definite Description?,1999,31,33,2,0,31670,kathleen mccoy,The Relation of Discourse/Dialogue Structure and Reference,0,"In order to produce coherent text. natural language generation systems must have the ability to generate pronouns in the appropriate places. In the past, pronoun usage was primarily investigated with respect to the accessibility of referents. We.argue that generating appropriate referring expressions requires looking at factors beyond accessibility. Also important are sentence boundaries, distance from last mention, discourse structure and ambiguity. We present an algorithm for generating appropriate anaphoric expressions which takes the tent poral structure of texts and knowledge about ambiguous contexts into account. We back up our hypotheses with some empirical results indicating that our algorithm chooses the right referring expression in 85% of the cases."
J99-3001,Functional Centering {--} Grounding Referential Coherence on Information Structure,1999,55,147,1,1,869,michael strube,Computational Linguistics,0,"Considering empirical evidence from a free-word-order language (German) we propose a revision of the principles guiding the ordering of discourse entities in the forward-looking center list within the centering model. We claim that grammatical role criteria should be replaced by criteria that reflect the functional information structure of the utterances. These new criteria are based on the distinction between hearer-old and hearer-new discourse entities. We demonstrate that such a functional model of centering can be successfully applied to the analysis of several forms of referential text phenomena, viz. pronominal, nominal, and functional anaphora. Our methodological and empirical claims are substantiated by two evaluation studies. In the first one, we compare success rates for the resolution of pronominal anaphora that result from a grammatical-role-driven centering algorithm and from a functional centering algorithm. The second study deals with a new cost-based evaluation methodology for the assessment of centering data, one which can be directly derived from and justified by the cognitive load premises of the centering model."
E99-1006,Resolving Discourse Deictic Anaphora in Dialogues,1999,33,33,2,0,45395,miriam eckert,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Most existing anaphora resolution algorithms are designed to account only for anaphors with NP-antecedents. This paper describes an algorithm for the resolution of discourse deictic anaphors, which constitute a large percentage of anaphors in spoken dialogues. The success of the resolution is dependent on the classification of all pronouns and demonstratives into individual, discourse deictic and vague anaphora. Finally, the empirical results of the application of the algorithm to a corpus of spoken dialogues are presented."
P98-2204,Never Look Back: An Alternative to Centering,1998,15,75,1,1,869,michael strube,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"I propose a model for determining the hearer's attentional state which depends solely on a list of salient discourse entities (S-list). The ordering among the elements of the S-list covers also the function of the backward-looking center in the centering model. The ranking criteria for the S-list are based on the distinction between hearer-old and hearer-new discourse entities and incorporate preferences for inter- and intra-sentential anaphora. The model is the basis for an algorithm which operates incrementally, word by word."
C98-2199,Never Look Back: An Alternative to Centering,1998,15,75,1,1,869,michael strube,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"I propose a model for determining the hearer's attentional state which depends solely on a list of salient discourse entities (S-list). The ordering among the elements of the S-list covers also the function of the backward-looking center in the centering model. The ranking criteria for the S-list are based on the distinction between hearer-old and hearer-new discourse entities and incorporate preferences for inter- and intra-sentential anaphora. The model is the basis for an algorithm which operates incrementally, word by word."
P97-1014,Centering in-the-Large: Computing Referential Discourse Segments,1997,15,19,2,0.664334,10102,udo hahn,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"We specify an algorithm that builds up a hierarchy of referential discourse segments from local centering data. The spatial extension and nesting of these discourse segments constrain the reachability of potential antecedents of an anaphoric expression beyond the local level of adjacent center pairs. Thus, the centering model is scaled up to the level of the global referential structure of discourse. An empirical evaluation of the algorithm is supplied."
P96-1036,Functional Centering,1996,22,73,1,1,869,michael strube,34th Annual Meeting of the Association for Computational Linguistics,1,"Based on empirical evidence from a free word order language (German) we propose a fundamental revision of the principles guiding the ordering of discourse entities in the forward-looking centers within the centering model. We claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances, i.e., the distinction between context-bound and unbound discourse elements. This claim is backed up by an empirical evaluation of functional centering."
P96-1057,Processing Complex Sentences in the Centering Framework,1996,10,3,1,1,869,michael strube,34th Annual Meeting of the Association for Computational Linguistics,1,We extend the centering model for the resolution of intra-sentential anaphora and specify how to handle complex sentences. An empirical evaluation indicates that the functional information structure guides the search for an antecedent within the sentence.
C96-1084,Bridging Textual Ellipses,1996,19,16,2,0.664334,10102,udo hahn,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"We present a hybrid text understanding methodology for the resolution of textual ellipsis. It integrates language-independent conceptual criteria and language-dependent functional constraints. The methodological framework for text ellipsis resolution is the centering model that has been adapted to constraints reflecting the functional information structure within utterances, i.e., the distinction between context-bound and unbound discourse elements."
E95-1033,{P}arse{T}alk about Sentence- and Text-Level Anaphora,1995,20,16,1,1,869,michael strube,Seventh Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We provide a unified account of sentence-level and text-level anaphora within the framework of a dependency-based grammar model. Criteria for anaphora resolution within sentence boundaries rephrase major concepts from GB's binding theory, while those for text-level anaphora incorporate an adapted version of a Grosz-Sidner-style focus model."
